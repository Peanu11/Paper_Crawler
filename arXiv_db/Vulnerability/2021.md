# 2021

## TOC

- [2021-01](#2021-01)
- [2021-02](#2021-02)
- [2021-03](#2021-03)
- [2021-04](#2021-04)
- [2021-05](#2021-05)
- [2021-06](#2021-06)
- [2021-07](#2021-07)
- [2021-08](#2021-08)
- [2021-09](#2021-09)
- [2021-10](#2021-10)
- [2021-11](#2021-11)
- [2021-12](#2021-12)

## 2021-01

<details>

<summary>2021-01-01 18:46:44 - Jamming Attacks and Anti-Jamming Strategies in Wireless Networks: A Comprehensive Survey</summary>

- *Hossein Pirayesh, Huacheng Zeng*

- `2101.00292v1` - [abs](http://arxiv.org/abs/2101.00292v1) - [pdf](http://arxiv.org/pdf/2101.00292v1)

> Wireless networks are a key component of the telecommunications infrastructure in our society, and wireless services become increasingly important as the applications of wireless devices have penetrated every aspect of our lives. Although wireless technologies have significantly advanced in the past decades, most wireless networks are still vulnerable to radio jamming attacks due to the openness nature of wireless channels, and the progress in the design of jamming-resistant wireless networking systems remains limited. This stagnation can be attributed to the lack of practical physical-layer wireless technologies that can efficiently decode data packets in the presence of jamming attacks. This article surveys existing jamming attacks and anti-jamming strategies in wireless local area networks (WLANs), cellular networks, cognitive radio networks (CRNs), ZigBee networks, Bluetooth networks, vehicular networks, LoRa networks, RFID networks, and GPS system, with the objective of offering a comprehensive knowledge landscape of existing jamming/anti-jamming strategies and stimulating more research efforts to secure wireless networks against jamming attacks. Different from prior survey papers, this article conducts a comprehensive, in-depth review on jamming and anti-jamming strategies, casting insights on the design of jamming-resilient wireless networking systems. An outlook on promising antijamming techniques is offered at the end of this article to delineate important research directions.

</details>

<details>

<summary>2021-01-01 22:38:54 - PHOENIX: Device-Centric Cellular Network Protocol Monitoring using Runtime Verification</summary>

- *Mitziu Echeverria, Zeeshan Ahmed, Bincheng Wang, M. Fareed Arif, Syed Rafiul Hussain, Omar Chowdhury*

- `2101.00328v1` - [abs](http://arxiv.org/abs/2101.00328v1) - [pdf](http://arxiv.org/pdf/2101.00328v1)

> End-user-devices in the current cellular ecosystem are prone to many different vulnerabilities across different generations and protocol layers. Fixing these vulnerabilities retrospectively can be expensive, challenging, or just infeasible. A pragmatic approach for dealing with such a diverse set of vulnerabilities would be to identify attack attempts at runtime on the device side, and thwart them with mitigating and corrective actions. Towards this goal, in the paper we propose a general and extendable approach called Phoenix for identifying n-day cellular network control-plane vulnerabilities as well as dangerous practices of network operators from the device vantage point. Phoenix monitors the device-side cellular network traffic for performing signature-based unexpected behavior detection through lightweight runtime verification techniques. Signatures in Phoenix can be manually-crafted by a cellular network security expert or can be automatically synthesized using an optional component of Phoenix, which reduces the signature synthesis problem to the language learning from the informant problem. Based on the corrective actions that are available to Phoenix when an undesired behavior is detected, different instantiations of Phoenix are possible: a full-fledged defense when deployed inside a baseband processor; a user warning system when deployed as a mobile application; a probe for identifying attacks in the wild. One such instantiation of Phoenix was able to identify all 15 representative n-day vulnerabilities and unsafe practices of 4G LTE networks considered in our evaluation with a high packet processing speed (~68000 packets/second) while inducing only a moderate amount of energy overhead (~4mW).

</details>

<details>

<summary>2021-01-02 23:16:26 - EEG-Based Brain-Computer Interfaces Are Vulnerable to Backdoor Attacks</summary>

- *Lubin Meng, Jian Huang, Zhigang Zeng, Xue Jiang, Shan Yu, Tzyy-Ping Jung, Chin-Teng Lin, Ricardo Chavarriaga, Dongrui Wu*

- `2011.00101v2` - [abs](http://arxiv.org/abs/2011.00101v2) - [pdf](http://arxiv.org/pdf/2011.00101v2)

> Research and development of electroencephalogram (EEG) based brain-computer interfaces (BCIs) have advanced rapidly, partly due to deeper understanding of the brain and wide adoption of sophisticated machine learning approaches for decoding the EEG signals. However, recent studies have shown that machine learning algorithms are vulnerable to adversarial attacks. This article proposes to use narrow period pulse for poisoning attack of EEG-based BCIs, which is implementable in practice and has never been considered before. One can create dangerous backdoors in the machine learning model by injecting poisoning samples into the training set. Test samples with the backdoor key will then be classified into the target class specified by the attacker. What most distinguishes our approach from previous ones is that the backdoor key does not need to be synchronized with the EEG trials, making it very easy to implement. The effectiveness and robustness of the backdoor attack approach is demonstrated, highlighting a critical security concern for EEG-based BCIs and calling for urgent attention to address it.

</details>

<details>

<summary>2021-01-03 19:05:24 - Imitation Attacks and Defenses for Black-box Machine Translation Systems</summary>

- *Eric Wallace, Mitchell Stern, Dawn Song*

- `2004.15015v3` - [abs](http://arxiv.org/abs/2004.15015v3) - [pdf](http://arxiv.org/pdf/2004.15015v3)

> Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploits of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary's BLEU score and attack success rate at some cost in the defender's BLEU and inference speed.

</details>

<details>

<summary>2021-01-03 19:58:55 - Universal Adversarial Triggers for Attacking and Analyzing NLP</summary>

- *Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh*

- `1908.07125v3` - [abs](http://arxiv.org/abs/1908.07125v3) - [pdf](http://arxiv.org/pdf/1908.07125v3)

> Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of "why" questions in SQuAD to be answered "to kill american people", and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.

</details>

<details>

<summary>2021-01-04 19:28:19 - Abusive Advertising: Scrutinizing socially relevant algorithms in a black box analysis to examine their impact on vulnerable patient groups in the health sector</summary>

- *Martin Reber*

- `2101.02018v1` - [abs](http://arxiv.org/abs/2101.02018v1) - [pdf](http://arxiv.org/pdf/2101.02018v1)

> The targeted direct-to-customer marketing of unapproved stem cell treatments by a questionable online industry is directed at vulnerable users who search the Internet in the hope of a cure. This behavior especially poses a threat to individuals who find themselves in hopeless and desperate phases in their lives. They might show low reluctance to try therapies that solely promise a cure but are not scientifically proven to do so. In the worst case, they suffer serious side-effects. Therefore, this thesis examines the display of advertisements of unapproved stem cell treatments for Parkinson's Disease, Multiple Sclerosis, Diabetes on Google's results page. The company announced a policy change in September 2019 that was meant to prohibit and ban the practices in question. However, there was evidence that those ads were still being delivered. A browser extension for Firefox and Chrome was developed and distributed to conduct a crowdsourced Black Box analysis. It was delivered to volunteers and virtual machines in Australia, Canada, the USA and the UK. Data on search results, advertisements and top stories was collected and analyzed. The results showed that there still is questionable advertising even though Google announced to purge it from its platform.

</details>

<details>

<summary>2021-01-04 20:06:56 - Robust Machine Learning Systems: Challenges, Current Trends, Perspectives, and the Road Ahead</summary>

- *Muhammad Shafique, Mahum Naseer, Theocharis Theocharides, Christos Kyrkou, Onur Mutlu, Lois Orosa, Jungwook Choi*

- `2101.02559v1` - [abs](http://arxiv.org/abs/2101.02559v1) - [pdf](http://arxiv.org/pdf/2101.02559v1)

> Machine Learning (ML) techniques have been rapidly adopted by smart Cyber-Physical Systems (CPS) and Internet-of-Things (IoT) due to their powerful decision-making capabilities. However, they are vulnerable to various security and reliability threats, at both hardware and software levels, that compromise their accuracy. These threats get aggravated in emerging edge ML devices that have stringent constraints in terms of resources (e.g., compute, memory, power/energy), and that therefore cannot employ costly security and reliability measures. Security, reliability, and vulnerability mitigation techniques span from network security measures to hardware protection, with an increased interest towards formal verification of trained ML models.   This paper summarizes the prominent vulnerabilities of modern ML systems, highlights successful defenses and mitigation techniques against these vulnerabilities, both at the cloud (i.e., during the ML training phase) and edge (i.e., during the ML inference stage), discusses the implications of a resource-constrained design on the reliability and security of the system, identifies verification methodologies to ensure correct system behavior, and describes open research challenges for building secure and reliable ML systems at both the edge and the cloud.

</details>

<details>

<summary>2021-01-04 22:42:28 - A Research Ecosystem for Secure Computing</summary>

- *Nadya Bliss, Lawrence A. Gordon, Daniel Lopresti, Fred Schneider, Suresh Venkatasubramanian*

- `2101.01264v1` - [abs](http://arxiv.org/abs/2101.01264v1) - [pdf](http://arxiv.org/pdf/2101.01264v1)

> Computing devices are vital to all areas of modern life and permeate every aspect of our society. The ubiquity of computing and our reliance on it has been accelerated and amplified by the COVID-19 pandemic. From education to work environments to healthcare to defense to entertainment - it is hard to imagine a segment of modern life that is not touched by computing. The security of computers, systems, and applications has been an active area of research in computer science for decades. However, with the confluence of both the scale of interconnected systems and increased adoption of artificial intelligence, there are many research challenges the community must face so that our society can continue to benefit and risks are minimized, not multiplied. Those challenges range from security and trust of the information ecosystem to adversarial artificial intelligence and machine learning.   Along with basic research challenges, more often than not, securing a system happens after the design or even deployment, meaning the security community is routinely playing catch-up and attempting to patch vulnerabilities that could be exploited any minute. While security measures such as encryption and authentication have been widely adopted, questions of security tend to be secondary to application capability. There needs to be a sea-change in the way we approach this critically important aspect of the problem: new incentives and education are at the core of this change. Now is the time to refocus research community efforts on developing interconnected technologies with security "baked in by design" and creating an ecosystem that ensures adoption of promising research developments. To realize this vision, two additional elements of the ecosystem are necessary - proper incentive structures for adoption and an educated citizenry that is well versed in vulnerabilities and risks.

</details>

<details>

<summary>2021-01-04 22:43:24 - Federated Learning-Based Risk-Aware Decision toMitigate Fake Task Impacts on CrowdsensingPlatforms</summary>

- *Zhiyan Chen, Murat Simsek, Burak Kantarci*

- `2101.01266v1` - [abs](http://arxiv.org/abs/2101.01266v1) - [pdf](http://arxiv.org/pdf/2101.01266v1)

> Mobile crowdsensing (MCS) leverages distributed and non-dedicated sensing concepts by utilizing sensors imbedded in a large number of mobile smart devices. However, the openness and distributed nature of MCS leads to various vulnerabilities and consequent challenges to address. A malicious user submitting fake sensing tasks to an MCS platform may be attempting to consume resources from any number of participants' devices; as well as attempting to clog the MCS server. In this paper, a novel approach that is based on horizontal federated learning is proposed to identify fake tasks that contain a number of independent detection devices and an aggregation entity. Detection devices are deployed to operate in parallel with each device equipped with a machine learning (ML) module, and an associated training dataset. Furthermore, the aggregation module collects the prediction results from individual devices and determines the final decision with the objective of minimizing the prediction loss. Loss measurement considers the lost task values with respect to misclassification, where the final decision utilizes a risk-aware approach where the risk is formulated as a function of the utility loss. Experimental results demonstrate that using federated learning-driven illegitimate task detection with a risk aware aggregation function improves the detection performance of the traditional centralized framework. Furthermore, the higher performance of detection and lower loss of utility can be achieved by the proposed framework. This scheme can even achieve 100%detection accuracy using small training datasets distributed across devices, while achieving slightly over an 8% increase in detection improvement over traditional approaches.

</details>

<details>

<summary>2021-01-05 07:31:34 - Alleviating Vulnerabilities of the Possible Outbreaks of Measles: A Data Trend Analysis and Prediction of Possible Cases</summary>

- *Hidear Talirongan, Markdy Y. Orong, Florence Jean B. Talirongan*

- `2101.01387v1` - [abs](http://arxiv.org/abs/2101.01387v1) - [pdf](http://arxiv.org/pdf/2101.01387v1)

> Measles is considered as a highly contagious disease that leads to serious complications around the world. Thus, the paper determined the trend and the five-year forecasted data of the Measles in the Philippines. This study utilized the time series data for trend analysis and data forecasting using the ARIMA model to visualize the measles cases. Figures for the time-series and forecasted results are individually presented with the use of GRETL software. Results showed that there was an increasing pattern of the disease from 2016 to 2019. However, there was a decreasing pattern of its occurrence in the next five years based on the five-year forecast. Nevertheless, with the results of the study, there is still a need to improve the different intervention plans of the authority in alleviating the occurrence of the disease though it yielded a decreasing pattern in the future since it is evident that the figure of the forecasted data is still approximately 15,000 and above.

</details>

<details>

<summary>2021-01-05 09:52:05 - Generating Informative CVE Description From ExploitDB Posts by Extractive Summarization</summary>

- *Jiamou Sun, Zhenchang Xing, Hao Guo, Deheng Ye, Xiaohong Li, Xiwei Xu, Liming Zhu*

- `2101.01431v1` - [abs](http://arxiv.org/abs/2101.01431v1) - [pdf](http://arxiv.org/pdf/2101.01431v1)

> ExploitDB is one of the important public websites, which contributes a large number of vulnerabilities to official CVE database. Over 60\% of these vulnerabilities have high- or critical-security risks. Unfortunately, over 73\% of exploits appear publicly earlier than the corresponding CVEs, and about 40\% of exploits do not even have CVEs. To assist in documenting CVEs for the ExploitDB posts, we propose an open information method to extract 9 key vulnerability aspects (vulnerable product/version/component, vulnerability type, vendor, attacker type, root cause, attack vector and impact) from the verbose and noisy ExploitDB posts. The extracted aspects from an ExploitDB post are then composed into a CVE description according to the suggested CVE description templates, which is must-provided information for requesting new CVEs. Through the evaluation on 13,017 manually labeled sentences and the statistically sampling of 3,456 extracted aspects, we confirm the high accuracy of our extraction method. Compared with 27,230 reference CVE descriptions. Our composed CVE descriptions achieve high ROUGH-L (0.38), a longest common subsequence based metric for evaluating text summarization methods.

</details>

<details>

<summary>2021-01-05 15:15:27 - Achieving Dalenius' Goal of Data Privacy with Practical Assumptions</summary>

- *Genqiang Wu, Xianyao Xia, Yeping He*

- `1703.07474v5` - [abs](http://arxiv.org/abs/1703.07474v5) - [pdf](http://arxiv.org/pdf/1703.07474v5)

> Recent studies show that differential privacy is vulnerable when different individuals' data in the dataset are correlated, and that there are many cases where differential privacy implies poor utility. In order to treat the two weaknesses, we traced the origin of differential privacy to Dalenius' goal, a more rigorous privacy measure. We formalized Dalenius' goal by using Shannon's perfect secrecy and tried to achieve Dalenius' goal with better utility. Our first result is that, if the independence assumption is true, then differential privacy is equivalent to Dalenius' goal, where the independence assumption assumes that each adversary has no knowledge of the correlation among different individuals' data in the dataset. This implies that the security of differential privacy is based on the independence assumption. Since the independence assumption is impractical, we introduced a new practical assumption, which assumes that each adversary is unknown to some data of the dataset if the dataset is large enough. Based on the assumption, we can achieve Dalenius' goal with better utility. Furthermore, we proved a useful result which can transplant results or approaches of information theory into data privacy protection. We then proved that several basic privacy mechanisms/channels satisfy Dalenuis' goal, such as the random response, the exponential, and the Gaussian privacy channels, which are respective counterparts of the random response, the exponential, and the Gaussian mechanisms of differential privacy. Moreover, the group and the composition properties were also proved. Finally, by using Yao's computational information theory, we extend our model to the computational-bounded case.

</details>

<details>

<summary>2021-01-05 18:24:13 - Label Augmentation via Time-based Knowledge Distillation for Financial Anomaly Detection</summary>

- *Hongda Shen, Eren Kursun*

- `2101.01689v1` - [abs](http://arxiv.org/abs/2101.01689v1) - [pdf](http://arxiv.org/pdf/2101.01689v1)

> Detecting anomalies has become increasingly critical to the financial service industry. Anomalous events are often indicative of illegal activities such as fraud, identity theft, network intrusion, account takeover, and money laundering. Financial anomaly detection use cases face serious challenges due to the dynamic nature of the underlying patterns especially in adversarial environments such as constantly changing fraud tactics. While retraining the models with the new patterns is absolutely essential; keeping up with the rapid changes introduces other challenges as it moves the model away from older patterns or continuously grows the size of the training data. The resulting data growth is hard to manage and it reduces the agility of the models' response to the latest attacks. Due to the data size limitations and the need to track the latest patterns, older time periods are often dropped in practice, which in turn, causes vulnerabilities. In this study, we propose a label augmentation approach to utilize the learning from older models to boost the latest. Experimental results show that the proposed approach provides a significant reduction in training time, while providing potential performance improvement.

</details>

<details>

<summary>2021-01-06 03:46:44 - Designing Actively Secure, Highly Available Industrial Automation Applications</summary>

- *Awais Tanveer, Roopak Sinha, Stephen G. MacDonell, Paulo Leitao, Valeriy Vyatkin*

- `2101.01856v1` - [abs](http://arxiv.org/abs/2101.01856v1) - [pdf](http://arxiv.org/pdf/2101.01856v1)

> Programmable Logic Controllers (PLCs) execute critical control software that drives Industrial Automation and Control Systems (IACS). PLCs can become easy targets for cyber-adversaries as they are resource-constrained and are usually built using legacy, less-capable security measures. Security attacks can significantly affect system availability, which is an essential requirement for IACS. We propose a method to make PLC applications more security-aware. Based on the well-known IEC 61499 function blocks standard for developing IACS software, our method allows designers to annotate critical parts of an application during design time. On deployment, these parts of the application are automatically secured using appropriate security mechanisms to detect and prevent attacks. We present a summary of availability attacks on distributed IACS applications that can be mitigated by our proposed method. Security mechanisms are achieved using IEC 61499 Service-Interface Function Blocks (SIFBs) embedding Intrusion Detection and Prevention System (IDPS), added to the application at compile time. This method is more amenable to providing active security protection from attacks on previously unknown (zero-day) vulnerabilities. We test our solution on an IEC 61499 application executing on Wago PFC200 PLCs. Experiments show that we can successfully log and prevent attacks at the application level as well as help the application to gracefully degrade into safe mode, subsequently improving availability.

</details>

<details>

<summary>2021-01-06 08:26:10 - sGUARD: Towards Fixing Vulnerable Smart Contracts Automatically</summary>

- *Tai D. Nguyen, Long H. Pham, Jun Sun*

- `2101.01917v1` - [abs](http://arxiv.org/abs/2101.01917v1) - [pdf](http://arxiv.org/pdf/2101.01917v1)

> Smart contracts are distributed, self-enforcing programs executing on top of blockchain networks. They have the potential to revolutionize many industries such as financial institutes and supply chains. However, smart contracts are subject to code-based vulnerabilities, which casts a shadow on its applications. As smart contracts are unpatchable (due to the immutability of blockchain), it is essential that smart contracts are guaranteed to be free of vulnerabilities. Unfortunately, smart contract languages such as Solidity are Turing-complete, which implies that verifying them statically is infeasible. Thus, alternative approaches must be developed to provide the guarantee. In this work, we develop an approach which automatically transforms smart contracts so that they are provably free of 4 common kinds of vulnerabilities. The key idea is to apply runtime verification in an efficient and provably correct manner. Experiment results with 5000 smart contracts show that our approach incurs minor run-time overhead in terms of time (i.e., 14.79%) and gas (i.e., 0.79%).

</details>

<details>

<summary>2021-01-06 15:57:00 - When Interactive Graphic Storytelling Fails</summary>

- *James Barela, Tiago Espinha Gasiba, Santiago Reinhard Suppan, Marc Berges, Kristian Beckers*

- `2101.02106v1` - [abs](http://arxiv.org/abs/2101.02106v1) - [pdf](http://arxiv.org/pdf/2101.02106v1)

> Many people are unaware of the digital dangers that lie around each cyber-corner. Teaching people how to recognize dangerous situations is crucial, especially for those who work on or with computers. We postulated that interactive graphic vignettes could be a great way to expose professionals to dangerous situations and demonstrate the effects of their choices in these situations. In that way, we aimed to inoculate employees against cybersecurity threats.   We used the Comic-BEE platform to create interactive security awareness vignettes and evaluated for how employees of a major industrial company perceived them. For analysing the potential of these comics, we ran an evaluation study as part of a capture-the-flag (CTF) event, an interactive exercise for hacking vulnerable software. We evaluated whether the comics fulfilled our requirements based on the responses of the participants. We showed the comics, on various cybersecurity concepts, to 20 volunteers. In the context of a CTF event, our requirements were not fulfilled. Most participants considered the images distracting, stating a preference for text-only material.

</details>

<details>

<summary>2021-01-06 16:01:49 - Design of Secure Coding Challenges for Cybersecurity Education in the Industry</summary>

- *Tiago Espinha Gasiba, Ulrike Lechner, Maria Pinto-Albuquerque, Alae Zouitni*

- `2101.02108v1` - [abs](http://arxiv.org/abs/2101.02108v1) - [pdf](http://arxiv.org/pdf/2101.02108v1)

> According to a recent survey with more than 4000 software developers, less than half of developers can spot security holes. As a result, software products present a low-security quality expressed by vulnerabilities that can be exploited by cyber-criminals. This lack of quality and security is particularly dangerous if the software which contains the vulnerabilities is deployed in critical infrastructures. Serious games, and in particular, Capture-the-Flag(CTF) events, have shown promising results in improving secure coding awareness of software developers in the industry. The challenges in the CTF event, to be useful, must be adequately designed to address the target group. This paper presents novel contributions by investigating which challenge types are adequate to improve software developers' ability to write secure code in an industrial context. We propose 1) six challenge types usable in the industry context, and 2) a structure for the CTF challenges. Our investigation also presents results on 3) how to include hints and penalties into the cyber-security challenges. We evaluated our work through a survey with security experts. While our results show that "traditional" challenge types seem to be adequate, they also reveal a new class of challenges based on code entry and interaction with an automated coach.

</details>

<details>

<summary>2021-01-07 01:59:00 - Understanding the Error in Evaluating Adversarial Robustness</summary>

- *Pengfei Xia, Ziqiang Li, Hongjing Niu, Bin Li*

- `2101.02325v1` - [abs](http://arxiv.org/abs/2101.02325v1) - [pdf](http://arxiv.org/pdf/2101.02325v1)

> Deep neural networks are easily misled by adversarial examples. Although lots of defense methods are proposed, many of them are demonstrated to lose effectiveness when against properly performed adaptive attacks. How to evaluate the adversarial robustness effectively is important for the realistic deployment of deep models, but yet still unclear. To provide a reasonable solution, one of the primary things is to understand the error (or gap) between the true adversarial robustness and the evaluated one, what is it and why it exists. Several works are done in this paper to make it clear. Firstly, we introduce an interesting phenomenon named gradient traps, which lead to incompetent adversaries and are demonstrated to be a manifestation of evaluation error. Then, we analyze the error and identify that there are three components. Each of them is caused by a specific compromise. Moreover, based on the above analysis, we present our evaluation suggestions. Experiments on adversarial training and its variations indicate that: (1) the error does exist empirically, and (2) these defenses are still vulnerable. We hope these analyses and results will help the community to develop more powerful defenses.

</details>

<details>

<summary>2021-01-07 16:52:19 - Reimagining City Configuration: Automated Urban Planning via Adversarial Learning</summary>

- *Dongjie Wang, Yanjie Fu, Pengyang Wang, Bo Huang, Chang-Tien Lu*

- `2008.09912v2` - [abs](http://arxiv.org/abs/2008.09912v2) - [pdf](http://arxiv.org/pdf/2008.09912v2)

> Urban planning refers to the efforts of designing land-use configurations. Effective urban planning can help to mitigate the operational and social vulnerability of a urban system, such as high tax, crimes, traffic congestion and accidents, pollution, depression, and anxiety. Due to the high complexity of urban systems, such tasks are mostly completed by professional planners. But, human planners take longer time. The recent advance of deep learning motivates us to ask: can machines learn at a human capability to automatically and quickly calculate land-use configuration, so human planners can finally adjust machine-generated plans for specific needs? To this end, we formulate the automated urban planning problem into a task of learning to configure land-uses, given the surrounding spatial contexts. To set up the task, we define a land-use configuration as a longitude-latitude-channel tensor, where each channel is a category of POIs and the value of an entry is the number of POIs. The objective is then to propose an adversarial learning framework that can automatically generate such tensor for an unplanned area. In particular, we first characterize the contexts of surrounding areas of an unplanned area by learning representations from spatial graphs using geographic and human mobility data. Second, we combine each unplanned area and its surrounding context representation as a tuple, and categorize all the tuples into positive (well-planned areas) and negative samples (poorly-planned areas). Third, we develop an adversarial land-use configuration approach, where the surrounding context representation is fed into a generator to generate a land-use configuration, and a discriminator learns to distinguish among positive and negative samples.

</details>

<details>

<summary>2021-01-07 17:52:17 - Adversarial Machine Learning for 5G Communications Security</summary>

- *Yalin E. Sagduyu, Tugba Erpek, Yi Shi*

- `2101.02656v1` - [abs](http://arxiv.org/abs/2101.02656v1) - [pdf](http://arxiv.org/pdf/2101.02656v1)

> Machine learning provides automated means to capture complex dynamics of wireless spectrum and support better understanding of spectrum resources and their efficient utilization. As communication systems become smarter with cognitive radio capabilities empowered by machine learning to perform critical tasks such as spectrum awareness and spectrum sharing, they also become susceptible to new vulnerabilities due to the attacks that target the machine learning applications. This paper identifies the emerging attack surface of adversarial machine learning and corresponding attacks launched against wireless communications in the context of 5G systems. The focus is on attacks against (i) spectrum sharing of 5G communications with incumbent users such as in the Citizens Broadband Radio Service (CBRS) band and (ii) physical layer authentication of 5G User Equipment (UE) to support network slicing. For the first attack, the adversary transmits during data transmission or spectrum sensing periods to manipulate the signal-level inputs to the deep learning classifier that is deployed at the Environmental Sensing Capability (ESC) to support the 5G system. For the second attack, the adversary spoofs wireless signals with the generative adversarial network (GAN) to infiltrate the physical layer authentication mechanism based on a deep learning classifier that is deployed at the 5G base station. Results indicate major vulnerabilities of 5G systems to adversarial machine learning. To sustain the 5G system operations in the presence of adversaries, a defense mechanism is presented to increase the uncertainty of the adversary in training the surrogate model used for launching its subsequent attacks.

</details>

<details>

<summary>2021-01-08 08:16:41 - Adversarial Attack Attribution: Discovering Attributable Signals in Adversarial ML Attacks</summary>

- *Marissa Dotter, Sherry Xie, Keith Manville, Josh Harguess, Colin Busho, Mikel Rodriguez*

- `2101.02899v1` - [abs](http://arxiv.org/abs/2101.02899v1) - [pdf](http://arxiv.org/pdf/2101.02899v1)

> Machine Learning (ML) models are known to be vulnerable to adversarial inputs and researchers have demonstrated that even production systems, such as self-driving cars and ML-as-a-service offerings, are susceptible. These systems represent a target for bad actors. Their disruption can cause real physical and economic harm. When attacks on production ML systems occur, the ability to attribute the attack to the responsible threat group is a critical step in formulating a response and holding the attackers accountable. We pose the following question: can adversarially perturbed inputs be attributed to the particular methods used to generate the attack? In other words, is there a way to find a signal in these attacks that exposes the attack algorithm, model architecture, or hyperparameters used in the attack? We introduce the concept of adversarial attack attribution and create a simple supervised learning experimental framework to examine the feasibility of discovering attributable signals in adversarial attacks. We find that it is possible to differentiate attacks generated with different attack algorithms, models, and hyperparameters on both the CIFAR-10 and MNIST datasets.

</details>

<details>

<summary>2021-01-08 09:57:47 - Eth2Vec: Learning Contract-Wide Code Representations for Vulnerability Detection on Ethereum Smart Contracts</summary>

- *Nami Ashizawa, Naoto Yanai, Jason Paul Cruz, Shingo Okamura*

- `2101.02377v2` - [abs](http://arxiv.org/abs/2101.02377v2) - [pdf](http://arxiv.org/pdf/2101.02377v2)

> Ethereum smart contracts are programs that run on the Ethereum blockchain, and many smart contract vulnerabilities have been discovered in the past decade. Many security analysis tools have been created to detect such vulnerabilities, but their performance decreases drastically when codes to be analyzed are being rewritten. In this paper, we propose Eth2Vec, a machine-learning-based static analysis tool for vulnerability detection, with robustness against code rewrites in smart contracts. Existing machine-learning-based static analysis tools for vulnerability detection need features, which analysts create manually, as inputs. In contrast, Eth2Vec automatically learns features of vulnerable Ethereum Virtual Machine (EVM) bytecodes with tacit knowledge through a neural network for language processing. Therefore, Eth2Vec can detect vulnerabilities in smart contracts by comparing the code similarity between target EVM bytecodes and the EVM bytecodes it already learned. We conducted experiments with existing open databases, such as Etherscan, and our results show that Eth2Vec outperforms the existing work in terms of well-known metrics, i.e., precision, recall, and F1-score. Moreover, Eth2Vec can detect vulnerabilities even in rewritten codes.

</details>

<details>

<summary>2021-01-08 19:39:30 - Attack-Resistant Federated Learning with Residual-based Reweighting</summary>

- *Shuhao Fu, Chulin Xie, Bo Li, Qifeng Chen*

- `1912.11464v3` - [abs](http://arxiv.org/abs/1912.11464v3) - [pdf](http://arxiv.org/pdf/1912.11464v3)

> Federated learning has a variety of applications in multiple domains by utilizing private training data stored on different devices. However, the aggregation process in federated learning is highly vulnerable to adversarial attacks so that the global model may behave abnormally under attacks. To tackle this challenge, we present a novel aggregation algorithm with residual-based reweighting to defend federated learning. Our aggregation algorithm combines repeated median regression with the reweighting scheme in iteratively reweighted least squares. Our experiments show that our aggregation algorithm outperforms other alternative algorithms in the presence of label-flipping and backdoor attacks. We also provide theoretical analysis for our aggregation algorithm.

</details>

<details>

<summary>2021-01-08 20:11:01 - Misspelling Correction with Pre-trained Contextual Language Model</summary>

- *Yifei Hu, Xiaonan Jing, Youlim Ko, Julia Taylor Rayz*

- `2101.03204v1` - [abs](http://arxiv.org/abs/2101.03204v1) - [pdf](http://arxiv.org/pdf/2101.03204v1)

> Spelling irregularities, known now as spelling mistakes, have been found for several centuries. As humans, we are able to understand most of the misspelled words based on their location in the sentence, perceived pronunciation, and context. Unlike humans, computer systems do not possess the convenient auto complete functionality of which human brains are capable. While many programs provide spelling correction functionality, many systems do not take context into account. Moreover, Artificial Intelligence systems function in the way they are trained on. With many current Natural Language Processing (NLP) systems trained on grammatically correct text data, many are vulnerable against adversarial examples, yet correctly spelled text processing is crucial for learning. In this paper, we investigate how spelling errors can be corrected in context, with a pre-trained language model BERT. We present two experiments, based on BERT and the edit distance algorithm, for ranking and selecting candidate corrections. The results of our experiments demonstrated that when combined properly, contextual word embeddings of BERT and edit distance are capable of effectively correcting spelling errors.

</details>

<details>

<summary>2021-01-09 00:27:23 - SyReNN: A Tool for Analyzing Deep Neural Networks</summary>

- *Matthew Sotoudeh, Aditya V. Thakur*

- `2101.03263v1` - [abs](http://arxiv.org/abs/2101.03263v1) - [pdf](http://arxiv.org/pdf/2101.03263v1)

> Deep Neural Networks (DNNs) are rapidly gaining popularity in a variety of important domains. Formally, DNNs are complicated vector-valued functions which come in a variety of sizes and applications. Unfortunately, modern DNNs have been shown to be vulnerable to a variety of attacks and buggy behavior. This has motivated recent work in formally analyzing the properties of such DNNs. This paper introduces SyReNN, a tool for understanding and analyzing a DNN by computing its symbolic representation. The key insight is to decompose the DNN into linear functions. Our tool is designed for analyses using low-dimensional subsets of the input space, a unique design point in the space of DNN analysis tools. We describe the tool and the underlying theory, then evaluate its use and performance on three case studies: computing Integrated Gradients, visualizing a DNN's decision boundaries, and patching a DNN.

</details>

<details>

<summary>2021-01-09 06:30:38 - Robust Blockchained Federated Learning with Model Validation and Proof-of-Stake Inspired Consensus</summary>

- *Hang Chen, Syed Ali Asif, Jihong Park, Chien-Chung Shen, Mehdi Bennis*

- `2101.03300v1` - [abs](http://arxiv.org/abs/2101.03300v1) - [pdf](http://arxiv.org/pdf/2101.03300v1)

> Federated learning (FL) is a promising distributed learning solution that only exchanges model parameters without revealing raw data. However, the centralized architecture of FL is vulnerable to the single point of failure. In addition, FL does not examine the legitimacy of local models, so even a small fraction of malicious devices can disrupt global training. To resolve these robustness issues of FL, in this paper, we propose a blockchain-based decentralized FL framework, termed VBFL, by exploiting two mechanisms in a blockchained architecture. First, we introduced a novel decentralized validation mechanism such that the legitimacy of local model updates is examined by individual validators. Second, we designed a dedicated proof-of-stake consensus mechanism where stake is more frequently rewarded to honest devices, which protects the legitimate local model updates by increasing their chances of dictating the blocks appended to the blockchain. Together, these solutions promote more federation within legitimate devices, enabling robust FL. Our emulation results of the MNIST classification corroborate that with 15% of malicious devices, VBFL achieves 87% accuracy, which is 7.4x higher than Vanilla FL.

</details>

<details>

<summary>2021-01-09 19:10:17 - General stochastic separation theorems with optimal bounds</summary>

- *Bogdan Grechuk, Alexander N. Gorban, Ivan Y. Tyukin*

- `2010.05241v2` - [abs](http://arxiv.org/abs/2010.05241v2) - [pdf](http://arxiv.org/pdf/2010.05241v2)

> Phenomenon of stochastic separability was revealed and used in machine learning to correct errors of Artificial Intelligence (AI) systems and analyze AI instabilities. In high-dimensional datasets under broad assumptions each point can be separated from the rest of the set by simple and robust Fisher's discriminant (is Fisher separable). Errors or clusters of errors can be separated from the rest of the data. The ability to correct an AI system also opens up the possibility of an attack on it, and the high dimensionality induces vulnerabilities caused by the same stochastic separability that holds the keys to understanding the fundamentals of robustness and adaptivity in high-dimensional data-driven AI. To manage errors and analyze vulnerabilities, the stochastic separation theorems should evaluate the probability that the dataset will be Fisher separable in given dimensionality and for a given class of distributions. Explicit and optimal estimates of these separation probabilities are required, and this problem is solved in present work. The general stochastic separation theorems with optimal probability estimates are obtained for important classes of distributions: log-concave distribution, their convex combinations and product distributions. The standard i.i.d. assumption was significantly relaxed. These theorems and estimates can be used both for correction of high-dimensional data driven AI systems and for analysis of their vulnerabilities. The third area of application is the emergence of memories in ensembles of neurons, the phenomena of grandmother's cells and sparse coding in the brain, and explanation of unexpected effectiveness of small neural ensembles in high-dimensional brain.

</details>

<details>

<summary>2021-01-09 22:37:43 - Correlated Data in Differential Privacy: Definition and Analysis</summary>

- *Tao Zhang, Tianqing Zhu, Renping Liu, Wanlei Zhou*

- `2008.00180v2` - [abs](http://arxiv.org/abs/2008.00180v2) - [pdf](http://arxiv.org/pdf/2008.00180v2)

> Differential privacy is a rigorous mathematical framework for evaluating and protecting data privacy. In most existing studies, there is a vulnerable assumption that records in a dataset are independent when differential privacy is applied. However, in real-world datasets, records are likely to be correlated, which may lead to unexpected data leakage. In this survey, we investigate the issue of privacy loss due to data correlation under differential privacy models. Roughly, we classify existing literature into three lines: 1) using parameters to describe data correlation in differential privacy, 2) using models to describe data correlation in differential privacy, and 3) describing data correlation based on the framework of Pufferfish. Firstly, a detailed example is given to illustrate the issue of privacy leakage on correlated data in real scenes. Then our main work is to analyze and compare these methods, and evaluate situations that these diverse studies are applied. Finally, we propose some future challenges on correlated differential privacy.

</details>

<details>

<summary>2021-01-10 02:51:56 - Beyond PS-LTE: Security Model Design Framework for PPDR Operational Environment</summary>

- *Daegeon Kim, Do Hyung Gu, Huy Kang Kim*

- `2009.12116v3` - [abs](http://arxiv.org/abs/2009.12116v3) - [pdf](http://arxiv.org/pdf/2009.12116v3)

> National disasters can threaten national security and require several organizations to integrate the functionalities to correspond to the event. Many countries are constructing a nationwide mobile communication network infrastructure to share information and promptly communicate with corresponding organizations. Public Safety Long-Term Evolution (PS-LTE) is a communication mechanism adopted in many countries to achieve such a purpose. Organizations can increase the efficiency of public protection and disaster relief (PPDR) operations by securely connecting the services run on their legacy networks to the PS-LTE infrastructure. This environment allows the organizations to continue facilitating the information and system functionalities provided by the legacy network. The vulnerabilities in the environment, which differ from commercial LTE, need to be resolved to connect the network securely. In this study, we propose a security model design framework to derive the system architecture and the security requirements targeting the restricted environment applied by certain technologies for a particular purpose. After analyzing the PPDR operation environment's characteristics under the PS-LTE infrastructure, we applied the framework to derive the security model for organizations using PPDR services operated in their legacy networks through this infrastructure. Although the proposed security model design framework is applied to the specific circumstance in this research, it can be generally adopted for the application environment.

</details>

<details>

<summary>2021-01-10 15:15:18 - Cybersecurity of Industrial Cyber-Physical Systems: A Review</summary>

- *Hakan Kayan, Matthew Nunes, Omer Rana, Pete Burnap, Charith Perera*

- `2101.03564v1` - [abs](http://arxiv.org/abs/2101.03564v1) - [pdf](http://arxiv.org/pdf/2101.03564v1)

> Industrial cyber-physical systems (ICPSs) manage critical infrastructures by controlling the processes based on the "physics" data gathered by edge sensor networks. Recent innovations in ubiquitous computing and communication technologies have prompted the rapid integration of highly interconnected systems to ICPSs. Hence, the "security by obscurity" principle provided by air-gapping is no longer followed. As the interconnectivity in ICPSs increases, so does the attack surface. Industrial vulnerability assessment reports have shown that a variety of new vulnerabilities have occurred due to this transition while the most common ones are related to weak boundary protection. Although there are existing surveys in this context, very little is mentioned regarding these reports. This paper bridges this gap by defining and reviewing ICPSs from a cybersecurity perspective. In particular, multi-dimensional adaptive attack taxonomy is presented and utilized for evaluating real-life ICPS cyber incidents. We also identify the general shortcomings and highlight the points that cause a gap in existing literature while defining future research directions.

</details>

<details>

<summary>2021-01-11 08:50:51 - Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey</summary>

- *Görkem Algan, Ilkay Ulusoy*

- `1912.05170v3` - [abs](http://arxiv.org/abs/1912.05170v3) - [pdf](http://arxiv.org/pdf/1912.05170v3)

> Image classification systems recently made a giant leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data to be adequately trained. Gathering a correctly annotated dataset is not always feasible due to several factors, such as the expensiveness of the labeling process or difficulty of correctly classifying data, even for the experts. Because of these practical challenges, label noise is a common problem in real-world datasets, and numerous methods to train deep neural networks with label noise are proposed in the literature. Although deep neural networks are known to be relatively robust to label noise, their tendency to overfit data makes them vulnerable to memorizing even random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse effects to train deep neural networks efficiently. Even though an extensive survey of machine learning techniques under label noise exists, the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the first group aim to estimate the noise structure and use this information to avoid the adverse effects of noisy labels. Differently, methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses, regularizers or other learning paradigms.

</details>

<details>

<summary>2021-01-11 12:27:42 - Understanding the Quality of Container Security Vulnerability Detection Tools</summary>

- *Omar Javed, Salman Toor*

- `2101.03844v1` - [abs](http://arxiv.org/abs/2101.03844v1) - [pdf](http://arxiv.org/pdf/2101.03844v1)

> Virtualization enables information and communications technology industry to better manage computing resources. In this regard, improvements in virtualization approaches together with the need for consistent runtime environment, lower overhead and smaller package size has led to the growing adoption of containers. This is a technology, which packages an application, its dependencies and Operating System (OS) to run as an isolated unit. However, the pressing concern with the use of containers is its susceptibility to security attacks. Consequently, a number of container scanning tools are available for detecting container security vulnerabilities. Therefore, in this study, we investigate the quality of existing container scanning tools by proposing two metrics that reflects coverage and accuracy. We analyze 59 popular public container images for Java applications hosted on DockerHub using different container scanning tools (such as Clair, Anchore, and Microscanner). Our findings show that existing container scanning approach does not detect application package vulnerabilities. Furthermore, existing tools do not have high accuracy, since 34% vulnerabilities are being missed by the best performing tool. Finally, we also demonstrate quality of Docker images for Java applications hosted on DockerHub by assessing complete vulnerability landscape i.e., number of vulnerabilities detected in images.

</details>

<details>

<summary>2021-01-12 00:04:44 - SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities</summary>

- *Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, Zhaoxuan Chen*

- `1807.06756v3` - [abs](http://arxiv.org/abs/1807.06756v3) - [pdf](http://arxiv.org/pdf/1807.06756v3)

> The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by the many vulnerabilities reported on a daily basis. This calls for machine learning methods for vulnerability detection. Deep learning is attractive for this purpose because it alleviates the requirement to manually define features. Despite the tremendous success of deep learning in other application domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities in C/C++ programs with source code. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations (SySeVR), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with 4 software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, 7 are unknown and have been reported to the vendors, and the other 8 have been "silently" patched by the vendors when releasing newer versions of the pertinent software products.

</details>

<details>

<summary>2021-01-12 06:22:17 - AI-GAN: Attack-Inspired Generation of Adversarial Examples</summary>

- *Tao Bai, Jun Zhao, Jinlin Zhu, Shoudong Han, Jiefeng Chen, Bo Li, Alex Kot*

- `2002.02196v2` - [abs](http://arxiv.org/abs/2002.02196v2) - [pdf](http://arxiv.org/pdf/2002.02196v2)

> Deep neural networks (DNNs) are vulnerable to adversarial examples, which are crafted by adding imperceptible perturbations to inputs. Recently different attacks and strategies have been proposed, but how to generate adversarial examples perceptually realistic and more efficiently remains unsolved. This paper proposes a novel framework called Attack-Inspired GAN (AI-GAN), where a generator, a discriminator, and an attacker are trained jointly. Once trained, it can generate adversarial perturbations efficiently given input images and target classes. Through extensive experiments on several popular datasets \eg MNIST and CIFAR-10, AI-GAN achieves high attack success rates and reduces generation time significantly in various settings. Moreover, for the first time, AI-GAN successfully scales to complicated datasets \eg CIFAR-100 with around $90\%$ success rates among all classes.

</details>

<details>

<summary>2021-01-12 06:42:45 - DeepiSign: Invisible Fragile Watermark to Protect the Integrityand Authenticity of CNN</summary>

- *Alsharif Abuadbba, Hyoungshick Kim, Surya Nepal*

- `2101.04319v1` - [abs](http://arxiv.org/abs/2101.04319v1) - [pdf](http://arxiv.org/pdf/2101.04319v1)

> Convolutional Neural Networks (CNNs) deployed in real-life applications such as autonomous vehicles have shown to be vulnerable to manipulation attacks, such as poisoning attacks and fine-tuning. Hence, it is essential to ensure the integrity and authenticity of CNNs because compromised models can produce incorrect outputs and behave maliciously. In this paper, we propose a self-contained tamper-proofing method, called DeepiSign, to ensure the integrity and authenticity of CNN models against such manipulation attacks. DeepiSign applies the idea of fragile invisible watermarking to securely embed a secret and its hash value into a CNN model. To verify the integrity and authenticity of the model, we retrieve the secret from the model, compute the hash value of the secret, and compare it with the embedded hash value. To minimize the effects of the embedded secret on the CNN model, we use a wavelet-based technique to transform weights into the frequency domain and embed the secret into less significant coefficients. Our theoretical analysis shows that DeepiSign can hide up to 1KB secret in each layer with minimal loss of the model's accuracy. To evaluate the security and performance of DeepiSign, we performed experiments on four pre-trained models (ResNet18, VGG16, AlexNet, and MobileNet) using three datasets (MNIST, CIFAR-10, and Imagenet) against three types of manipulation attacks (targeted input poisoning, output poisoning, and fine-tuning). The results demonstrate that DeepiSign is verifiable without degrading the classification accuracy, and robust against representative CNN manipulation attacks.

</details>

<details>

<summary>2021-01-12 09:02:33 - SpreadMeNot: A Provably Secure and Privacy-Preserving Contact Tracing Protocol</summary>

- *Pietro Tedeschi, Spiridon Bakiras, Roberto Di Pietro*

- `2011.07306v2` - [abs](http://arxiv.org/abs/2011.07306v2) - [pdf](http://arxiv.org/pdf/2011.07306v2)

> A plethora of contact tracing apps have been developed and deployed in several countries around the world in the battle against Covid-19. However, people are rightfully concerned about the security and privacy risks of such applications. To this end, the contribution of this work is twofold. First, we present an in-depth analysis of the security and privacy characteristics of the most prominent contact tracing protocols, under both passive and active adversaries. The results of our study indicate that all protocols are vulnerable to a variety of attacks, mainly due to the deterministic nature of the underlying cryptographic protocols. Our second contribution is the design and implementation of SpreadMeNot, a novel contact tracing protocol that can defend against most passive and active attacks, thus providing strong (provable) security and privacy guarantees that are necessary for such a sensitive application. Our detailed analysis, both formal and experimental, shows that SpreadMeNot satisfies security, privacy, and performance requirements, hence being an ideal candidate for building a contact tracing solution that can be adopted by the majority of the general public, as well as to serve as an open-source reference for further developments in the field.

</details>

<details>

<summary>2021-01-12 13:41:26 - Towards Cross-Modal Forgery Detection and Localization on Live Surveillance Videos</summary>

- *Yong Huang, Xiang Li, Wei Wang, Tao Jiang, Qian Zhang*

- `2101.00848v2` - [abs](http://arxiv.org/abs/2101.00848v2) - [pdf](http://arxiv.org/pdf/2101.00848v2)

> The cybersecurity breaches render surveillance systems vulnerable to video forgery attacks, under which authentic live video streams are tampered to conceal illegal human activities under surveillance cameras. Traditional video forensics approaches can detect and localize forgery traces in each video frame using computationally-expensive spatial-temporal analysis, while falling short in real-time verification of live video feeds. The recent work correlates time-series camera and wireless signals to recognize replayed surveillance videos using event-level timing information but it cannot realize fine-grained forgery detection and localization on each frame. To fill this gap, this paper proposes Secure-Pose, a novel cross-modal forgery detection and localization system for live surveillance videos using WiFi signals near the camera spot. We observe that coexisting camera and WiFi signals convey common human semantic information and the presence of forgery attacks on video frames will decouple such information correspondence. Secure-Pose extracts effective human pose features from synchronized multi-modal signals and detects and localizes forgery traces under both inter-frame and intra-frame attacks in each frame. We implement Secure-Pose using a commercial camera and two Intel 5300 NICs and evaluate it in real-world environments. Secure-Pose achieves a high detection accuracy of 95.1% and can effectively localize tampered objects under different forgery attacks.

</details>

<details>

<summary>2021-01-12 19:54:22 - Boundary thickness and robustness in learning models</summary>

- *Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E. Gonzalez, Kannan Ramchandran, Michael W. Mahoney*

- `2007.05086v2` - [abs](http://arxiv.org/abs/2007.05086v2) - [pdf](http://arxiv.org/pdf/2007.05086v2)

> Robustness of machine learning models to various adversarial and non-adversarial corruptions continues to be of interest. In this paper, we introduce the notion of the boundary thickness of a classifier, and we describe its connection with and usefulness for model robustness. Thick decision boundaries lead to improved performance, while thin decision boundaries lead to overfitting (e.g., measured by the robust generalization gap between training and testing) and lower robustness. We show that a thicker boundary helps improve robustness against adversarial examples (e.g., improving the robust test accuracy of adversarial training) as well as so-called out-of-distribution (OOD) transforms, and we show that many commonly-used regularization and data augmentation procedures can increase boundary thickness. On the theoretical side, we establish that maximizing boundary thickness during training is akin to the so-called mixup training. Using these observations, we show that noise-augmentation on mixup training further increases boundary thickness, thereby combating vulnerability to various forms of adversarial attacks and OOD transforms. We can also show that the performance improvement in several lines of recent work happens in conjunction with a thicker boundary.

</details>

<details>

<summary>2021-01-13 13:00:51 - Untargeted, Targeted and Universal Adversarial Attacks and Defenses on Time Series</summary>

- *Pradeep Rathore, Arghya Basak, Sri Harsha Nistala, Venkataramana Runkana*

- `2101.05639v1` - [abs](http://arxiv.org/abs/2101.05639v1) - [pdf](http://arxiv.org/pdf/2101.05639v1)

> Deep learning based models are vulnerable to adversarial attacks. These attacks can be much more harmful in case of targeted attacks, where an attacker tries not only to fool the deep learning model, but also to misguide the model to predict a specific class. Such targeted and untargeted attacks are specifically tailored for an individual sample and require addition of an imperceptible noise to the sample. In contrast, universal adversarial attack calculates a special imperceptible noise which can be added to any sample of the given dataset so that, the deep learning model is forced to predict a wrong class. To the best of our knowledge these targeted and universal attacks on time series data have not been studied in any of the previous works. In this work, we have performed untargeted, targeted and universal adversarial attacks on UCR time series datasets. Our results show that deep learning based time series classification models are vulnerable to these attacks. We also show that universal adversarial attacks have good generalization property as it need only a fraction of the training data. We have also performed adversarial training based adversarial defense. Our results show that models trained adversarially using Fast gradient sign method (FGSM), a single step attack, are able to defend against FGSM as well as Basic iterative method (BIM), a popular iterative attack.

</details>

<details>

<summary>2021-01-14 02:44:28 - Federated Learning: Opportunities and Challenges</summary>

- *Priyanka Mary Mammen*

- `2101.05428v1` - [abs](http://arxiv.org/abs/2101.05428v1) - [pdf](http://arxiv.org/pdf/2101.05428v1)

> Federated Learning (FL) is a concept first introduced by Google in 2016, in which multiple devices collaboratively learn a machine learning model without sharing their private data under the supervision of a central server. This offers ample opportunities in critical domains such as healthcare, finance etc, where it is risky to share private user information to other organisations or devices. While FL appears to be a promising Machine Learning (ML) technique to keep the local data private, it is also vulnerable to attacks like other ML models. Given the growing interest in the FL domain, this report discusses the opportunities and challenges in federated learning.

</details>

<details>

<summary>2021-01-14 08:39:50 - ECOL: Early Detection of COVID Lies Using Content, Prior Knowledge and Source Information</summary>

- *Ipek Baris, Zeyd Boukhers*

- `2101.05499v1` - [abs](http://arxiv.org/abs/2101.05499v1) - [pdf](http://arxiv.org/pdf/2101.05499v1)

> Social media platforms are vulnerable to fake news dissemination, which causes negative consequences such as panic and wrong medication in the healthcare domain. Therefore, it is important to automatically detect fake news in an early stage before they get widely spread. This paper analyzes the impact of incorporating content information, prior knowledge, and credibility of sources into models for the early detection of fake news. We propose a framework modeling those features by using BERT language model and external sources, namely Simple English Wikipedia and source reliability tags. The conducted experiments on CONSTRAINT datasets demonstrated the benefit of integrating these features for the early detection of fake news in the healthcare domain.

</details>

<details>

<summary>2021-01-14 18:33:19 - Time-Based CAN Intrusion Detection Benchmark</summary>

- *Deborah H. Blevins, Pablo Moriano, Robert A. Bridges, Miki E. Verma, Michael D. Iannacone, Samuel C Hollifield*

- `2101.05781v1` - [abs](http://arxiv.org/abs/2101.05781v1) - [pdf](http://arxiv.org/pdf/2101.05781v1)

> Modern vehicles are complex cyber-physical systems made of hundreds of electronic control units (ECUs) that communicate over controller area networks (CANs). This inherited complexity has expanded the CAN attack surface which is vulnerable to message injection attacks. These injections change the overall timing characteristics of messages on the bus, and thus, to detect these malicious messages, time-based intrusion detection systems (IDSs) have been proposed. However, time-based IDSs are usually trained and tested on low-fidelity datasets with unrealistic, labeled attacks. This makes difficult the task of evaluating, comparing, and validating IDSs. Here we detail and benchmark four time-based IDSs against the newly published ROAD dataset, the first open CAN IDS dataset with real (non-simulated) stealthy attacks with physically verified effects. We found that methods that perform hypothesis testing by explicitly estimating message timing distributions have lower performance than methods that seek anomalies in a distribution-related statistic. In particular, these "distribution-agnostic" based methods outperform "distribution-based" methods by at least 55% in area under the precision-recall curve (AUC-PR). Our results expand the body of knowledge of CAN time-based IDSs by providing details of these methods and reporting their results when tested on datasets with real advanced attacks. Finally, we develop an after-market plug-in detector using lightweight hardware, which can be used to deploy the best performing IDS method on nearly any vehicle.

</details>

<details>

<summary>2021-01-15 09:41:05 - RAICC: Revealing Atypical Inter-Component Communication in Android Apps</summary>

- *Jordan Samhi, Alexandre Bartel, Tegawendé F. Bissyandé, Jacques Klein*

- `2012.09916v2` - [abs](http://arxiv.org/abs/2012.09916v2) - [pdf](http://arxiv.org/pdf/2012.09916v2)

> Inter-Component Communication (ICC) is a key mechanism in Android. It enables developers to compose rich functionalities and explore reuse within and across apps. Unfortunately, as reported by a large body of literature, ICC is rather "complex and largely unconstrained", leaving room to a lack of precision in apps modeling. To address the challenge of tracking ICCs within apps, state of the art static approaches such as Epicc, IccTA and Amandroid have focused on the documented framework ICC methods (e.g., startActivity) to build their approaches. In this work we show that ICC models inferred in these state of the art tools may actually be incomplete: the framework provides other atypical ways of performing ICCs. To address this limitation in the state of the art, we propose RAICC a static approach for modeling new ICC links and thus boosting previous analysis tasks such as ICC vulnerability detection, privacy leaks detection, malware detection, etc. We have evaluated RAICC on 20 benchmark apps, demonstrating that it improves the precision and recall of uncovered leaks in state of the art tools. We have also performed a large empirical investigation showing that Atypical ICC methods are largely used in Android apps, although not necessarily for data transfer. We also show that RAICC increases the number of ICC links found by 61.6% on a dataset of real-world malicious apps, and that RAICC enables the detection of new ICC vulnerabilities.

</details>

<details>

<summary>2021-01-15 10:13:19 - Bulwark: Holistic and Verified Security Monitoring of Web Protocols</summary>

- *Lorenzo Veronese, Stefano Calzavara, Luca Compagna*

- `2101.06043v1` - [abs](http://arxiv.org/abs/2101.06043v1) - [pdf](http://arxiv.org/pdf/2101.06043v1)

> Modern web applications often rely on third-party services to provide their functionality to users. The secure integration of these services is a non-trivial task, as shown by the large number of attacks against Single Sign On and Cashier-as-a-Service protocols. In this paper we present Bulwark, a new automatic tool which generates formally verified security monitors from applied pi-calculus specifications of web protocols. The security monitors generated by Bulwark offer holistic protection, since they can be readily deployed both at the client side and at the server side, thus ensuring full visibility of the attack surface against web protocols. We evaluate the effectiveness of Bulwark by testing it against a pool of vulnerable web applications that use the OAuth 2.0 protocol or integrate the PayPal payment system.

</details>

<details>

<summary>2021-01-15 14:31:39 - Quantitative System-Level Security Verification of the IoV Infrastructure</summary>

- *Jan Lauinger, Mudassar Aslam, Mohammad Hamad, Shahid Raza, Sebastian Steinhorst*

- `2101.06137v1` - [abs](http://arxiv.org/abs/2101.06137v1) - [pdf](http://arxiv.org/pdf/2101.06137v1)

> The Internet of Vehicles (IoV) equips vehicles with connectivity to the Internet and the Internet of Things (IoT) to support modern applications such as autonomous driving. However, the consolidation of complex computing domains of vehicles, the Internet, and the IoT limits the applicability of tailored security solutions. In this paper, we propose a new methodology to quantitatively verify the security of single or system-level assets of the IoV infrastructure. In detail, our methodology decomposes assets of the IoV infrastructure with the help of reference sub-architectures and the 4+1 view model analysis to map identified assets into data, software, networking, and hardware categories. This analysis includes a custom threat modeling concept to perform parameterization of Common Vulnerability Scoring System (CVSS) scores per view model domain. As a result, our methodology is able to allocate assets from attack paths to view model domains. This equips assets of attack paths with our IoV-driven CVSS scores. Our CVSS scores assess the attack likelihood which we use for Markov Chain transition probabilities. This way, we quantitatively verify system-level security among a set of IoV assets. Our results show that our methodology applies to arbitrary IoV attack paths. Based on our parameterization of CVSS scores and our selection of use cases, remote attacks are less likely to compromise location data compared to attacks from close proximity for authorized and unauthorized attackers respectively.

</details>

<details>

<summary>2021-01-15 14:33:57 - TrustSECO: An Interview Survey into Software Trust</summary>

- *Floris Jansen, Slinger Jansen, Fang Hou*

- `2101.06138v1` - [abs](http://arxiv.org/abs/2101.06138v1) - [pdf](http://arxiv.org/pdf/2101.06138v1)

> The software ecosystem is a trust-rich part of the world. Collaboratively, software engineers trust major hubs in the ecosystem, such as package managers, repository services, and programming language ecosystems. This trust, however, is often broken by vulnerabilities, ransomware, and abuse from malignant actors.   But what is trust? In this paper we explore, through twelve in-depth interviews with software engineers, how they perceive trust in their daily work. From the interviews we conclude three things. First, software engineers make a distinction between an adoption factor and a trust factor when selecting a package. Secondly, while in literature mostly technical factors are considered as the main trust factors, the software engineers in this study conclude that organizational factors are more important. Finally, we find that different kinds of software engineers require different views on trust, and that it is impossible to create one unified perception of trust.   Keywords: software ecosystem trust, empirical software engineering, TrustSECO, external software adoption, cross-sectional exploratory interview analysis, trust perception.

</details>

<details>

<summary>2021-01-15 15:29:02 - A Framework for Enhancing Deep Neural Networks Against Adversarial Malware</summary>

- *Deqiang Li, Qianmu Li, Yanfang Ye, Shouhuai Xu*

- `2004.07919v3` - [abs](http://arxiv.org/abs/2004.07919v3) - [pdf](http://arxiv.org/pdf/2004.07919v3)

> Machine learning-based malware detection is known to be vulnerable to adversarial evasion attacks. The state-of-the-art is that there are no effective defenses against these attacks. As a response to the adversarial malware classification challenge organized by the MIT Lincoln Lab and associated with the AAAI-19 Workshop on Artificial Intelligence for Cyber Security (AICS'2019), we propose six guiding principles to enhance the robustness of deep neural networks. Some of these principles have been scattered in the literature, but the others are introduced in this paper for the first time. Under the guidance of these six principles, we propose a defense framework to enhance the robustness of deep neural networks against adversarial malware evasion attacks. By conducting experiments with the Drebin Android malware dataset, we show that the framework can achieve a 98.49\% accuracy (on average) against grey-box attacks, where the attacker knows some information about the defense and the defender knows some information about the attack, and an 89.14% accuracy (on average) against the more capable white-box attacks, where the attacker knows everything about the defense and the defender knows some information about the attack. The framework wins the AICS'2019 challenge by achieving a 76.02% accuracy, where neither the attacker (i.e., the challenge organizer) knows the framework or defense nor we (the defender) know the attacks. This gap highlights the importance of knowing about the attack.

</details>

<details>

<summary>2021-01-15 16:42:32 - The Eye of Horus: Spotting and Analyzing Attacks on Ethereum Smart Contracts</summary>

- *Christof Ferreira Torres, Antonio Ken Iannillo, Arthur Gervais, Radu State*

- `2101.06204v1` - [abs](http://arxiv.org/abs/2101.06204v1) - [pdf](http://arxiv.org/pdf/2101.06204v1)

> In recent years, Ethereum gained tremendously in popularity, growing from a daily transaction average of 10K in January 2016 to an average of 500K in January 2020. Similarly, smart contracts began to carry more value, making them appealing targets for attackers. As a result, they started to become victims of attacks, costing millions of dollars. In response to these attacks, both academia and industry proposed a plethora of tools to scan smart contracts for vulnerabilities before deploying them on the blockchain. However, most of these tools solely focus on detecting vulnerabilities and not attacks, let alone quantifying or tracing the number of stolen assets. In this paper, we present Horus, a framework that empowers the automated detection and investigation of smart contract attacks based on logic-driven and graph-driven analysis of transactions. Horus provides quick means to quantify and trace the flow of stolen assets across the Ethereum blockchain. We perform a large-scale analysis of all the smart contracts deployed on Ethereum until May 2020. We identified 1,888 attacked smart contracts and 8,095 adversarial transactions in the wild. Our investigation shows that the number of attacks did not necessarily decrease over the past few years, but for some vulnerabilities remained constant. Finally, we also demonstrate the practicality of our framework via an in-depth analysis on the recent Uniswap and Lendf.me attacks.

</details>

<details>

<summary>2021-01-15 21:09:57 - CARE: Lightweight Attack Resilient Secure Boot Architecturewith Onboard Recovery for RISC-V based SOC</summary>

- *Avani Dave, Nilanjan Banerjee, Chintan Patel*

- `2101.06300v1` - [abs](http://arxiv.org/abs/2101.06300v1) - [pdf](http://arxiv.org/pdf/2101.06300v1)

> Recent technological advancements have proliferated the use of small embedded devices for collecting, processing, and transferring the security-critical information. The Internet of Things (IoT) has enabled remote access and control of these network-connected devices. Consequently, an attacker can exploit security vulnerabilities and compromise these devices. In this context, the secure boot becomes a useful security mechanism to verify the integrity and authenticity of the software state of the devices. However, the current secure boot schemes focus on detecting the presence of potential malware on the device but not on disinfecting and restoring the soft-ware to a benign state. This manuscript presents CARE- the first secure boot framework that provides detection, resilience, and onboard recovery mechanism for the com-promised devices. The framework uses a prototype hybrid CARE: Code Authentication and Resilience Engine to verify the software state and restore it to a benign state. It uses Physical Memory Protection (PMP) and other security enchaining techniques of RISC-V processor to pro-vide resilience from modern attacks. The state-of-the-art comparison and performance analysis results indicate that the proposed secure boot framework provides a promising resilience and recovery mechanism with very little 8 % performance and resource overhead

</details>

<details>

<summary>2021-01-16 03:53:01 - AR-based Modern Healthcare: A Review</summary>

- *Jinat Ara, Hanif Bhuiyan, Yeasin Arafat Bhuiyan, Salma Begum Bhyan, Muhammad Ismail Bhuiyan*

- `2101.06364v1` - [abs](http://arxiv.org/abs/2101.06364v1) - [pdf](http://arxiv.org/pdf/2101.06364v1)

> The recent advances of Augmented Reality (AR) in healthcare have shown that technology is a significant part of the current healthcare system. In recent days, augmented reality has proposed numerous smart applications in healthcare domain including wearable access, telemedicine, remote surgery, diagnosis of medical reports, emergency medicine, etc. The aim of the developed augmented healthcare application is to improve patient care, increase efficiency, and decrease costs. This article puts on an effort to review the advances in AR-based healthcare technologies and goes to peek into the strategies that are being taken to further this branch of technology. This article explores the important services of augmented-based healthcare solutions and throws light on recently invented ones as well as their respective platforms. It also addresses concurrent concerns and their relevant future challenges. In addition, this paper analyzes distinct AR security and privacy including security requirements and attack terminologies. Furthermore, this paper proposes a security model to minimize security risks. Augmented reality advantages in healthcare, especially for operating surgery, emergency diagnosis, and medical training is being demonstrated here thorough proper analysis. To say the least, the article illustrates a complete overview of augmented reality technology in the modern healthcare sector by demonstrating its impacts, advancements, current vulnerabilities; future challenges, and concludes with recommendations to a new direction for further research.

</details>

<details>

<summary>2021-01-16 16:57:16 - Visual Analytics approach for finding spatiotemporal patterns from COVID19</summary>

- *Arunav Das*

- `2101.06476v1` - [abs](http://arxiv.org/abs/2101.06476v1) - [pdf](http://arxiv.org/pdf/2101.06476v1)

> Bounce Back Loan is amongst a number of UK business financial support schemes launched by UK Government in 2020 amidst pandemic lockdown. Through these schemes, struggling businesses are provided financial support to weather economic slowdown from pandemic lockdown. {\pounds}43.5bn loan value has been provided as of 17th Dec2020. However, with no major checks for granting these loans and looming prospect of loan losses from write-offs from failed businesses and fraud, this paper theorizes prospect of applying spatiotemporal modelling technique to explore if geospatial patterns and temporal analysis could aid design of loan grant criteria for schemes. Application of Clustering and Visual Analytics framework to business demographics, survival rate and Sector concentration shows Inner and Outer London spatial patterns which historic business failures and reversal of the patterns under COVID-19 implying sector influence on spatial clusters. Combination of unsupervised clustering technique with multinomial logistic regression modelling on research datasets complimented by additional datasets on other support schemes, business structure and financial crime, is recommended for modelling business vulnerability to certain types of financial market or economic condition. The limitations of clustering technique for high dimensional is discussed along with relevance of an applicable model for continuing the research through next steps.

</details>

<details>

<summary>2021-01-16 19:38:16 - Multi-objective Search of Robust Neural Architectures against Multiple Types of Adversarial Attacks</summary>

- *Jia Liu, Yaochu Jin*

- `2101.06507v1` - [abs](http://arxiv.org/abs/2101.06507v1) - [pdf](http://arxiv.org/pdf/2101.06507v1)

> Many existing deep learning models are vulnerable to adversarial examples that are imperceptible to humans. To address this issue, various methods have been proposed to design network architectures that are robust to one particular type of adversarial attacks. It is practically impossible, however, to predict beforehand which type of attacks a machine learn model may suffer from. To address this challenge, we propose to search for deep neural architectures that are robust to five types of well-known adversarial attacks using a multi-objective evolutionary algorithm. To reduce the computational cost, a normalized error rate of a randomly chosen attack is calculated as the robustness for each newly generated neural architecture at each generation. All non-dominated network architectures obtained by the proposed method are then fully trained against randomly chosen adversarial attacks and tested on two widely used datasets. Our experimental results demonstrate the superiority of optimized neural architectures found by the proposed approach over state-of-the-art networks that are widely used in the literature in terms of the classification accuracy under different adversarial attacks.

</details>

<details>

<summary>2021-01-17 19:25:56 - Adversarial Policies: Attacking Deep Reinforcement Learning</summary>

- *Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, Stuart Russell*

- `1905.10615v3` - [abs](http://arxiv.org/abs/1905.10615v3) - [pdf](http://arxiv.org/pdf/1905.10615v3)

> Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.

</details>

<details>

<summary>2021-01-18 06:29:30 - DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection</summary>

- *Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, Yunxin Liu*

- `2101.06896v1` - [abs](http://arxiv.org/abs/2101.06896v1) - [pdf](http://arxiv.org/pdf/2101.06896v1)

> Deep learning models are increasingly used in mobile applications as critical components. Unlike the program bytecode whose vulnerabilities and threats have been widely-discussed, whether and how the deep learning models deployed in the applications can be compromised are not well-understood since neural networks are usually viewed as a black box. In this paper, we introduce a highly practical backdoor attack achieved with a set of reverse-engineering techniques over compiled deep learning models. The core of the attack is a neural conditional branch constructed with a trigger detector and several operators and injected into the victim model as a malicious payload. The attack is effective as the conditional logic can be flexibly customized by the attacker, and scalable as it does not require any prior knowledge from the original model. We evaluated the attack effectiveness using 5 state-of-the-art deep learning models and real-world samples collected from 30 users. The results demonstrated that the injected backdoor can be triggered with a success rate of 93.5%, while only brought less than 2ms latency overhead and no more than 1.4% accuracy decrease. We further conducted an empirical study on real-world mobile deep learning apps collected from Google Play. We found 54 apps that were vulnerable to our attack, including popular and security-critical ones. The results call for the awareness of deep learning application developers and auditors to enhance the protection of deployed models.

</details>

<details>

<summary>2021-01-18 06:34:45 - Multi-Source Data Fusion for Cyberattack Detection in Power Systems</summary>

- *Abhijeet Sahu, Zeyu Mao, Patrick Wlazlo, Hao Huang, Katherine Davis, Ana Goulart, Saman Zonouz*

- `2101.06897v1` - [abs](http://arxiv.org/abs/2101.06897v1) - [pdf](http://arxiv.org/pdf/2101.06897v1)

> Cyberattacks can cause a severe impact on power systems unless detected early. However, accurate and timely detection in critical infrastructure systems presents challenges, e.g., due to zero-day vulnerability exploitations and the cyber-physical nature of the system coupled with the need for high reliability and resilience of the physical system. Conventional rule-based and anomaly-based intrusion detection system (IDS) tools are insufficient for detecting zero-day cyber intrusions in the industrial control system (ICS) networks. Hence, in this work, we show that fusing information from multiple data sources can help identify cyber-induced incidents and reduce false positives. Specifically, we present how to recognize and address the barriers that can prevent the accurate use of multiple data sources for fusion-based detection. We perform multi-source data fusion for training IDS in a cyber-physical power system testbed where we collect cyber and physical side data from multiple sensors emulating real-world data sources that would be found in a utility and synthesizes these into features for algorithms to detect intrusions. Results are presented using the proposed data fusion application to infer False Data and Command injection-based Man-in- The-Middle (MiTM) attacks. Post collection, the data fusion application uses time-synchronized merge and extracts features followed by pre-processing such as imputation and encoding before training supervised, semi-supervised, and unsupervised learning models to evaluate the performance of the IDS. A major finding is the improvement of detection accuracy by fusion of features from cyber, security, and physical domains. Additionally, we observed the co-training technique performs at par with supervised learning methods when fed with our features.

</details>

<details>

<summary>2021-01-18 08:01:06 - Detection of Insider Attacks in Distributed Projected Subgradient Algorithms</summary>

- *Sissi Xiaoxiao Wu, Gangqiang Li, Shengli Zhang, Xiaohui Lin*

- `2101.06917v1` - [abs](http://arxiv.org/abs/2101.06917v1) - [pdf](http://arxiv.org/pdf/2101.06917v1)

> The gossip-based distributed algorithms are widely used to solve decentralized optimization problems in various multi-agent applications, while they are generally vulnerable to data injection attacks by internal malicious agents as each agent locally estimates its decent direction without an authorized supervision. In this work, we explore the application of artificial intelligence (AI) technologies to detect internal attacks. We show that a general neural network is particularly suitable for detecting and localizing the malicious agents, as they can effectively explore nonlinear relationship underlying the collected data. Moreover, we propose to adopt one of the state-of-art approaches in federated learning, i.e., a collaborative peer-to-peer machine learning protocol, to facilitate training our neural network models by gossip exchanges. This advanced approach is expected to make our model more robust to challenges with insufficient training data, or mismatched test data. In our simulations, a least-squared problem is considered to verify the feasibility and effectiveness of AI-based methods. Simulation results demonstrate that the proposed AI-based methods are beneficial to improve performance of detecting and localizing malicious agents over score-based methods, and the peer-to-peer neural network model is indeed robust to target issues.

</details>

<details>

<summary>2021-01-18 18:21:42 - Leveraging AI to optimize website structure discovery during Penetration Testing</summary>

- *Diego Antonelli, Roberta Cascella, Gaetano Perrone, Simon Pietro Romano, Antonio Schiano*

- `2101.07223v1` - [abs](http://arxiv.org/abs/2101.07223v1) - [pdf](http://arxiv.org/pdf/2101.07223v1)

> Dirbusting is a technique used to brute force directories and file names on web servers while monitoring HTTP responses, in order to enumerate server contents. Such a technique uses lists of common words to discover the hidden structure of the target website. Dirbusting typically relies on response codes as discovery conditions to find new pages. It is widely used in web application penetration testing, an activity that allows companies to detect websites vulnerabilities. Dirbusting techniques are both time and resource consuming and innovative approaches have never been explored in this field. We hence propose an advanced technique to optimize the dirbusting process by leveraging Artificial Intelligence. More specifically, we use semantic clustering techniques in order to organize wordlist items in different groups according to their semantic meaning. The created clusters are used in an ad-hoc implemented next-word intelligent strategy. This paper demonstrates that the usage of clustering techniques outperforms the commonly used brute force methods. Performance is evaluated by testing eight different web applications. Results show a performance increase that is up to 50% for each of the conducted experiments.

</details>

<details>

<summary>2021-01-18 23:35:42 - Panel: Humans and Technology for Inclusive Privacy and Security</summary>

- *Sanchari Das, Robert S. Gutzwiller, Rod D. Roscoe, Prashanth Rajivan, Yang Wang, L. Jean Camp, Roberto Hoyle*

- `2101.07377v1` - [abs](http://arxiv.org/abs/2101.07377v1) - [pdf](http://arxiv.org/pdf/2101.07377v1)

> Computer security and user privacy are critical issues and concerns in the digital era due to both increasing users and threats to their data. Separate issues arise between generic cybersecurity guidance (i.e., protect all user data from malicious threats) and the individualistic approach of privacy (i.e., specific to users and dependent on user needs and risk perceptions). Research has shown that several security- and privacy-focused vulnerabilities are technological (e.g., software bugs (Streiff, Kenny, Das, Leeth, & Camp, 2018), insecure authentication (Das, Wang, Tingle, & Camp, 2019)), or behavioral (e.g., sharing passwords (Das, Dingman, & Camp, 2018); and compliance (Das, Dev, & Srinivasan, 2018) (Dev, Das, Rashidi, & Camp, 2019)). This panel proposal addresses a third category of sociotechnical vulnerabilities that can and sometimes do arise from non-inclusive design of security and privacy. In this panel, we will address users' needs and desires for privacy. The panel will engage in in-depth discussions about value-sensitive design while focusing on potentially vulnerable populations, such as older adults, teens, persons with disabilities, and others who are not typically emphasized in general security and privacy concerns. Human factors have a stake in and ability to facilitate improvements in these areas.

</details>

<details>

<summary>2021-01-19 19:01:56 - Internet of Predictable Things (IoPT) Framework to Increase Cyber-Physical System Resiliency</summary>

- *Umit Cali, Murat Kuzlu, Vinayak Sharma, Manisa Pipattanasomporn, Ferhat Ozgur Catak*

- `2101.07816v1` - [abs](http://arxiv.org/abs/2101.07816v1) - [pdf](http://arxiv.org/pdf/2101.07816v1)

> During the last two decades, distributed energy systems, especially renewable energy sources (RES), have become more economically viable with increasing market share and penetration levels on power systems. In addition to decarbonization and decentralization of energy systems, digitalization has also become very important. The use of artificial intelligence (AI), advanced optimization algorithms, Industrial Internet of Things (IIoT), and other digitalization frameworks makes modern power system assets more intelligent, while vulnerable to cybersecurity risks. This paper proposes the concept of the Internet of Predictable Things (IoPT) that incorporates advanced data analytics and machine learning methods to increase the resiliency of cyber-physical systems against cybersecurity risks. The proposed concept is demonstrated using a cyber-physical system testbed under a variety of cyber attack scenarios as a proof of concept (PoC).

</details>

<details>

<summary>2021-01-19 21:02:55 - An Empirical Study of C++ Vulnerabilities in Crowd-Sourced Code Examples</summary>

- *Morteza Verdi, Ashkan Sami, Jafar Akhondali, Foutse Khomh, Gias Uddin, Alireza Karami Motlagh*

- `1910.01321v2` - [abs](http://arxiv.org/abs/1910.01321v2) - [pdf](http://arxiv.org/pdf/1910.01321v2)

> Software developers share programming solutions in Q&A sites like Stack Overflow. The reuse of crowd-sourced code snippets can facilitate rapid prototyping. However, recent research shows that the shared code snippets may be of low quality and can even contain vulnerabilities. This paper aims to understand the nature and the prevalence of security vulnerabilities in crowd-sourced code examples. To achieve this goal, we investigate security vulnerabilities in the C++ code snippets shared on Stack Overflow over a period of 10 years. In collaborative sessions involving multiple human coders, we manually assessed each code snippet for security vulnerabilities following CWE (Common Weakness Enumeration) guidelines. From the 72,483 reviewed code snippets used in at least one project hosted on GitHub, we found a total of 69 vulnerable code snippets categorized into 29 types. Many of the investigated code snippets are still not corrected on Stack Overflow. The 69 vulnerable code snippets found in Stack Overflow were reused in a total of 2859 GitHub projects. To help improve the quality of code snippets shared on Stack Overflow, we developed a browser extension that allow Stack Overflow users to check for vulnerabilities in code snippets when they upload them on the platform.

</details>

<details>

<summary>2021-01-20 00:45:02 - Epidemic? The Attack Surface of German Hospitals during the COVID-19 Pandemic</summary>

- *Johannes Klick, Robert Koch, Thomas Brandstetter*

- `2101.07912v1` - [abs](http://arxiv.org/abs/2101.07912v1) - [pdf](http://arxiv.org/pdf/2101.07912v1)

> In our paper we analyze the attack surface of German hospitals and healthcare providers in 2020 during the COVID-19 Pandemic. The analysis looked at the publicly visible attack surface utilizing a Distributed Cyber Recon System, utilizing distributed Internet scanning, Big Data methods and scan data of 1,483 GB from more than 89 different global Internet scans. From the 1,555 identified German clinical entities, security posture analysis was conducted by looking at more than 13,000 service banners for version identification and subsequent CVE-based vulnerability identification. Primary analysis shows that 32 percent of the analyzed services were determined as vulnerable to various degrees and 36 percent of all hospitals showed numerous vulnerabilities. Further resulting vulnerability statistics were mapped against size of organization and hospital bed count.

</details>

<details>

<summary>2021-01-20 12:24:26 - Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI</summary>

- *Alon Jacovi, Ana Marasović, Tim Miller, Yoav Goldberg*

- `2010.07487v3` - [abs](http://arxiv.org/abs/2010.07487v3) - [pdf](http://arxiv.org/pdf/2010.07487v3)

> Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, sociology's interpersonal trust (i.e., trust between people). This model rests on two key properties of the vulnerability of the user and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (which detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We then present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.

</details>

<details>

<summary>2021-01-21 13:18:04 - Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning</summary>

- *Vedant Nanda, Samuel Dooley, Sahil Singla, Soheil Feizi, John P. Dickerson*

- `2006.12621v4` - [abs](http://arxiv.org/abs/2006.12621v4) - [pdf](http://arxiv.org/pdf/2006.12621v4)

> Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of \textit{robustness bias}. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: \url{https://github.com/nvedant07/Fairness-Through-Robustness}

</details>

<details>

<summary>2021-01-21 13:55:23 - Right and left, partisanship predicts (asymmetric) vulnerability to misinformation</summary>

- *Dimitar Nikolov, Alessandro Flammini, Filippo Menczer*

- `2010.01462v2` - [abs](http://arxiv.org/abs/2010.01462v2) - [pdf](http://arxiv.org/pdf/2010.01462v2)

> We analyze the relationship between partisanship, echo chambers, and vulnerability to online misinformation by studying news sharing behavior on Twitter. While our results confirm prior findings that online misinformation sharing is strongly correlated with right-leaning partisanship, we also uncover a similar, though weaker trend among left-leaning users. Because of the correlation between a user's partisanship and their position within a partisan echo chamber, these types of influence are confounded. To disentangle their effects, we perform a regression analysis and find that vulnerability to misinformation is most strongly influenced by partisanship for both left- and right-leaning users.

</details>

<details>

<summary>2021-01-21 16:55:14 - Copycat CNN: Are Random Non-Labeled Data Enough to Steal Knowledge from Black-box Models?</summary>

- *Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue, Alberto F. De Souza, Thiago Oliveira-Santos*

- `2101.08717v1` - [abs](http://arxiv.org/abs/2101.08717v1) - [pdf](http://arxiv.org/pdf/2101.08717v1)

> Convolutional neural networks have been successful lately enabling companies to develop neural-based products, which demand an expensive process, involving data acquisition and annotation; and model generation, usually requiring experts. With all these costs, companies are concerned about the security of their models against copies and deliver them as black-boxes accessed by APIs. Nonetheless, we argue that even black-box models still have some vulnerabilities. In a preliminary work, we presented a simple, yet powerful, method to copy black-box models by querying them with natural random images. In this work, we consolidate and extend the copycat method: (i) some constraints are waived; (ii) an extensive evaluation with several problems is performed; (iii) models are copied between different architectures; and, (iv) a deeper analysis is performed by looking at the copycat behavior. Results show that natural random images are effective to generate copycats for several problems.

</details>

<details>

<summary>2021-01-21 19:07:13 - Constrained optimisation of preliminary spacecraft configurations under the design-for-demise paradigm</summary>

- *Mirko Trisolini, Hugh G. Lewis, Camilla Colombo*

- `2101.01558v2` - [abs](http://arxiv.org/abs/2101.01558v2) - [pdf](http://arxiv.org/pdf/2101.01558v2)

> In the past few years, the interest towards the implementation of design-for-demise measures has increased steadily. Most mid-sized satellites currently launched and already in orbit fail to comply with the casualty risk threshold of 0.0001. Therefore, satellites manufacturers and mission operators need to perform a disposal through a controlled re-entry, which has a higher cost and increased complexity. Through the design-for-demise paradigm, this additional cost and complexity can be removed as the spacecraft is directly compliant with the casualty risk regulations. However, building a spacecraft such that most of its parts will demise may lead to designs that are more vulnerable to space debris impacts, thus compromising the reliability of the mission. In fact, the requirements connected to the demisability and the survivability are in general competing. Given this competing nature, trade-off solutions can be found, which favour the implementation of design-for-demise measures while still maintaining the spacecraft resilient to space debris impacts. A multi-objective optimisation framework has been developed by the authors in previous works. The framework's objective is to find preliminary design solutions considering the competing nature of the demisability and the survivability of a spacecraft since the early stages of the mission design. In this way, a more integrated design can be achieved. The present work focuses on the improvement of the multi-objective optimisation framework by including constraints. The paper shows the application of the constrained optimisation to two relevant examples: the optimisation of a tank assembly and the optimisation of a typical satellite configuration.

</details>

<details>

<summary>2021-01-22 14:46:21 - Neurocognitive and traffic based handover strategies</summary>

- *Andreas Otte, Jonas Vogt, Jens Staub, Niclas Wolniak, Horst Wieker*

- `2101.10186v1` - [abs](http://arxiv.org/abs/2101.10186v1) - [pdf](http://arxiv.org/pdf/2101.10186v1)

> The level of automation in vehicles will significantly increase over the next decade. As automation will become more and more common, vehicles will not be able to master all traffic related situations for a long time by themselves. In such situations, the driver must take over and steer the vehicle through the situation. One of the important questions is when the takeover should be performed. Many decisive factors must be considered. On the one hand, the current traffic situation including roads, traffic light and other road users, especially vulnerable road users, and on the other hand, the state of the driver must be considered. The goal is to combine neurocognitive measurement of the drivers state and the static and dynamic traffic related data to develop an interpretation of the current situation. This situation analysis should be the basis for the determination of the best takeover point.

</details>

<details>

<summary>2021-01-22 18:17:56 - Brazilian Favela Women: How Your Standard Solutions for Technology Abuse Might Actually Harm Them</summary>

- *Mirela Silva, Daniela Oliveira*

- `2101.09257v1` - [abs](http://arxiv.org/abs/2101.09257v1) - [pdf](http://arxiv.org/pdf/2101.09257v1)

> Brazil is home to over 200M people, the majority of which have access to the Internet. Over 11M Brazilians live in favelas, or informal settlements with no outside government regulation, often ruled by narcos or militias. Victims of intimate partner violence (IPV) in these communities are made extra vulnerable not only by lack of access to resources, but by the added layer of violence caused by criminal activity and police confrontations. In this paper, we use an unintended harms framework to analyze the unique online privacy needs of favela women and present research questions that we urge tech abuse researchers to consider.

</details>

<details>

<summary>2021-01-22 21:59:44 - On managing vulnerabilities in AI/ML systems</summary>

- *Jonathan M. Spring, April Galyardt, Allen D. Householder, Nathan VanHoudnos*

- `2101.10865v1` - [abs](http://arxiv.org/abs/2101.10865v1) - [pdf](http://arxiv.org/pdf/2101.10865v1)

> This paper explores how the current paradigm of vulnerability management might adapt to include machine learning systems through a thought experiment: what if flaws in machine learning (ML) were assigned Common Vulnerabilities and Exposures (CVE) identifiers (CVE-IDs)? We consider both ML algorithms and model objects. The hypothetical scenario is structured around exploring the changes to the six areas of vulnerability management: discovery, report intake, analysis, coordination, disclosure, and response. While algorithm flaws are well-known in the academic research community, there is no apparent clear line of communication between this research community and the operational communities that deploy and manage systems that use ML. The thought experiments identify some ways in which CVE-IDs may establish some useful lines of communication between these two communities. In particular, it would start to introduce the research community to operational security concepts, which appears to be a gap left by existing efforts.

</details>

<details>

<summary>2021-01-23 00:19:52 - Online Adversarial Purification based on Self-Supervision</summary>

- *Changhao Shi, Chester Holtz, Gal Mishne*

- `2101.09387v1` - [abs](http://arxiv.org/abs/2101.09387v1) - [pdf](http://arxiv.org/pdf/2101.09387v1)

> Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.

</details>

<details>

<summary>2021-01-23 22:43:28 - Resilient Virtualized Systems Using ReHype</summary>

- *Michael Le, Yuval Tamir*

- `2101.09282v1` - [abs](http://arxiv.org/abs/2101.09282v1) - [pdf](http://arxiv.org/pdf/2101.09282v1)

> System-level virtualization introduces critical vulnerabilities to failures of the software components that implement virtualization -- the virtualization infrastructure (VI). To mitigate the impact of such failures, we introduce a resilient VI (RVI) that can recover individual VI components from failure, caused by hardware or software faults, transparently to the hosted virtual machines (VMs). Much of the focus is on the ReHype mechanism for recovery from hypervisor failures, that can lead to state corruption and to inconsistencies among the states of system components. ReHype's implementation for the Xen hypervisor was done incrementally, using fault injection results to identify sources of critical corruption and inconsistencies. This implementation involved 900 LOC, with memory space overhead of 2.1MB. Fault injection campaigns, with a variety of fault types, show that ReHype can successfully recover, in less than 750ms, from over 88% of detected hypervisor failures. In addition to ReHype, recovery mechanisms for the other VI components are described. The overall effectiveness of our RVI is evaluated hosting a Web service application, on a cluster of VMs. With faults in any VI component, for over 87% of detected failures, our recovery mechanisms allow services provided by the application to be continuously maintained despite the resulting failures of VI components.

</details>

<details>

<summary>2021-01-23 23:16:41 - Concentrated Stopping Set Design for Coded Merkle Tree: Improving Security Against Data Availability Attacks in Blockchain Systems</summary>

- *Debarnab Mitra, Lev Tauz, Lara Dolecek*

- `2010.07363v2` - [abs](http://arxiv.org/abs/2010.07363v2) - [pdf](http://arxiv.org/pdf/2010.07363v2)

> In certain blockchain systems, light nodes are clients that download only a small portion of the block. Light nodes are vulnerable to data availability (DA) attacks where a malicious node hides an invalid portion of the block from the light nodes. Recently, a technique based on erasure codes called Coded Merkle Tree (CMT) was proposed by Yu et al. that enables light nodes to detect a DA attack with high probability. The CMT is constructed using LDPC codes for fast decoding but can fail to detect a DA attack if a malicious node hides a small stopping set of the code. To combat this, Yu et al. used well-studied techniques to design random LDPC codes with high minimum stopping set size. Although effective, these codes are not necessarily optimal for this application. In this paper, we demonstrate a more specialized LDPC code design to improve the security against DA attacks. We achieve this goal by providing a deterministic LDPC code construction that focuses on concentrating stopping sets to a small group of variable nodes rather than only eliminating stopping sets. We design these codes by modifying the Progressive Edge Growth algorithm into a technique called the entropy-constrained PEG (EC-PEG) algorithm. This new method demonstrates a higher probability of detecting DA attacks and allows for good codes at short lengths.

</details>

<details>

<summary>2021-01-23 23:32:39 - Improving Adversarial Robustness in Weight-quantized Neural Networks</summary>

- *Chang Song, Elias Fallon, Hai Li*

- `2012.14965v2` - [abs](http://arxiv.org/abs/2012.14965v2) - [pdf](http://arxiv.org/pdf/2012.14965v2)

> Neural networks are getting deeper and more computation-intensive nowadays. Quantization is a useful technique in deploying neural networks on hardware platforms and saving computation costs with negligible performance loss. However, recent research reveals that neural network models, no matter full-precision or quantized, are vulnerable to adversarial attacks. In this work, we analyze both adversarial and quantization losses and then introduce criteria to evaluate them. We propose a boundary-based retraining method to mitigate adversarial and quantization losses together and adopt a nonlinear mapping method to defend against white-box gradient-based adversarial attacks. The evaluations demonstrate that our method can better restore accuracy after quantization than other baseline methods on both black-box and white-box adversarial attacks. The results also show that adversarial training suffers quantization loss and does not cooperate well with other training methods.

</details>

<details>

<summary>2021-01-25 07:39:16 - Generalizing Adversarial Examples by AdaBelief Optimizer</summary>

- *Yixiang Wang, Jiqiang Liu, Xiaolin Chang*

- `2101.09930v1` - [abs](http://arxiv.org/abs/2101.09930v1) - [pdf](http://arxiv.org/pdf/2101.09930v1)

> Recent research has proved that deep neural networks (DNNs) are vulnerable to adversarial examples, the legitimate input added with imperceptible and well-designed perturbations can fool DNNs easily in the testing stage. However, most of the existing adversarial attacks are difficult to fool adversarially trained models. To solve this issue, we propose an AdaBelief iterative Fast Gradient Sign Method (AB-FGSM) to generalize adversarial examples. By integrating AdaBelief optimization algorithm to I-FGSM, we believe that the generalization of adversarial examples will be improved, relying on the strong generalization of AdaBelief optimizer. To validate the effectiveness and transferability of adversarial examples generated by our proposed AB-FGSM, we conduct the white-box and black-box attacks on various single models and ensemble models. Compared with state-of-the-art attack methods, our proposed method can generate adversarial examples effectively in the white-box setting, and the transfer rate is 7%-21% higher than latest attack methods.

</details>

<details>

<summary>2021-01-25 07:59:28 - Initializing Perturbations in Multiple Directions for Fast Adversarial Training</summary>

- *Xunguang Wang, Ship Peng Xu, Eric Ke Wang*

- `2005.07606v2` - [abs](http://arxiv.org/abs/2005.07606v2) - [pdf](http://arxiv.org/pdf/2005.07606v2)

> Recent developments in the filed of Deep Learning have demonstrated that Deep Neural Networks(DNNs) are vulnerable to adversarial examples. Specifically, in image classification, an adversarial example can fool the well trained deep neural networks by adding barely imperceptible perturbations to clean images. Adversarial Training, one of the most direct and effective methods, minimizes the losses of perturbed-data to learn robust deep networks against adversarial attacks. It has been proven that using the fast gradient sign method (FGSM) can achieve Fast Adversarial Training. However, FGSM-based adversarial training may finally obtain a failed model because of overfitting to FGSM samples. In this paper, we proposed the Diversified Initialized Perturbations Adversarial Training (DIP-FAT) which involves seeking the initialization of the perturbation via enlarging the output distances of the target model in a random directions. Due to the diversity of random directions, the embedded fast adversarial training using FGSM increases the information from the adversary and reduces the possibility of overfitting. In addition to preventing overfitting, the extensive results show that our proposed DIP-FAT technique can also improve the accuracy of the clean data. The biggest advantage of DIP-FAT method: achieving the best banlance among clean-data, perturbed-data and efficiency.

</details>

<details>

<summary>2021-01-26 05:16:16 - Investigating the significance of adversarial attacks and their relation to interpretability for radar-based human activity recognition systems</summary>

- *Utku Ozbulak, Baptist Vandersmissen, Azarakhsh Jalalvand, Ivo Couckuyt, Arnout Van Messem, Wesley De Neve*

- `2101.10562v1` - [abs](http://arxiv.org/abs/2101.10562v1) - [pdf](http://arxiv.org/pdf/2101.10562v1)

> Given their substantial success in addressing a wide range of computer vision challenges, Convolutional Neural Networks (CNNs) are increasingly being used in smart home applications, with many of these applications relying on the automatic recognition of human activities. In this context, low-power radar devices have recently gained in popularity as recording sensors, given that the usage of these devices allows mitigating a number of privacy concerns, a key issue when making use of conventional video cameras. Another concern that is often cited when designing smart home applications is the resilience of these applications against cyberattacks. It is, for instance, well-known that the combination of images and CNNs is vulnerable against adversarial examples, mischievous data points that force machine learning models to generate wrong classifications during testing time. In this paper, we investigate the vulnerability of radar-based CNNs to adversarial attacks, and where these radar-based CNNs have been designed to recognize human gestures. Through experiments with four unique threat models, we show that radar-based CNNs are susceptible to both white- and black-box adversarial attacks. We also expose the existence of an extreme adversarial attack case, where it is possible to change the prediction made by the radar-based CNNs by only perturbing the padding of the inputs, without touching the frames where the action itself occurs. Moreover, we observe that gradient-based attacks exercise perturbation not randomly, but on important features of the input data. We highlight these important features by making use of Grad-CAM, a popular neural network interpretability method, hereby showing the connection between adversarial perturbation and prediction interpretability.

</details>

<details>

<summary>2021-01-26 06:40:32 - SkeletonVis: Interactive Visualization for Understanding Adversarial Attacks on Human Action Recognition Models</summary>

- *Haekyu Park, Zijie J. Wang, Nilaksh Das, Anindya S. Paul, Pruthvi Perumalla, Zhiyan Zhou, Duen Horng Chau*

- `2101.10586v1` - [abs](http://arxiv.org/abs/2101.10586v1) - [pdf](http://arxiv.org/pdf/2101.10586v1)

> Skeleton-based human action recognition technologies are increasingly used in video based applications, such as home robotics, healthcare on aging population, and surveillance. However, such models are vulnerable to adversarial attacks, raising serious concerns for their use in safety-critical applications. To develop an effective defense against attacks, it is essential to understand how such attacks mislead the pose detection models into making incorrect predictions. We present SkeletonVis, the first interactive system that visualizes how the attacks work on the models to enhance human understanding of attacks.

</details>

<details>

<summary>2021-01-26 09:55:19 - Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples</summary>

- *Maximilian Mozes, Pontus Stenetorp, Bennett Kleinberg, Lewis D. Griffin*

- `2004.05887v2` - [abs](http://arxiv.org/abs/2004.05887v2) - [pdf](http://arxiv.org/pdf/2004.05887v2)

> Recent efforts have shown that neural text processing models are vulnerable to adversarial examples, but the nature of these examples is poorly understood. In this work, we show that adversarial attacks against CNN, LSTM and Transformer-based classification models perform word substitutions that are identifiable through frequency differences between replaced words and their corresponding substitutions. Based on these findings, we propose frequency-guided word substitutions (FGWS), a simple algorithm exploiting the frequency properties of adversarial word substitutions for the detection of adversarial examples. FGWS achieves strong performance by accurately detecting adversarial examples on the SST-2 and IMDb sentiment datasets, with F1 detection scores of up to 91.4% against RoBERTa-based classification models. We compare our approach against a recently proposed perturbation discrimination framework and show that we outperform it by up to 13.0% F1.

</details>

<details>

<summary>2021-01-26 11:14:12 - Sydr: Cutting Edge Dynamic Symbolic Execution</summary>

- *Alexey Vishnyakov, Andrey Fedotov, Daniil Kuts, Alexander Novikov, Darya Parygina, Eli Kobrin, Vlada Logunova, Pavel Belecky, Shamil Kurmangaleev*

- `2011.09269v2` - [abs](http://arxiv.org/abs/2011.09269v2) - [pdf](http://arxiv.org/pdf/2011.09269v2)

> The security development lifecycle (SDL) is becoming an industry standard. Dynamic symbolic execution (DSE) has enormous amount of applications in computer security (fuzzing, vulnerability discovery, reverse-engineering, etc.). We propose several performance and accuracy improvements for dynamic symbolic execution. Skipping non-symbolic instructions allows to build a path predicate 1.2--3.5 times faster. Symbolic engine simplifies formulas during symbolic execution. Path predicate slicing eliminates irrelevant conjuncts from solver queries. We handle each jump table (switch statement) as multiple branches and describe the method for symbolic execution of multi-threaded programs. The proposed solutions were implemented in Sydr tool. Sydr performs inversion of branches in path predicate. Sydr combines DynamoRIO dynamic binary instrumentation tool with Triton symbolic engine. We evaluated Sydr features on 64-bit Linux executables.

</details>

<details>

<summary>2021-01-26 14:07:09 - Adversarial Vulnerability of Active Transfer Learning</summary>

- *Nicolas M. Müller, Konstantin Böttinger*

- `2101.10792v1` - [abs](http://arxiv.org/abs/2101.10792v1) - [pdf](http://arxiv.org/pdf/2101.10792v1)

> Two widely used techniques for training supervised machine learning models on small datasets are Active Learning and Transfer Learning. The former helps to optimally use a limited budget to label new data. The latter uses large pre-trained models as feature extractors and enables the design of complex, non-linear models even on tiny datasets. Combining these two approaches is an effective, state-of-the-art method when dealing with small datasets.   In this paper, we share an intriguing observation: Namely, that the combination of these techniques is particularly susceptible to a new kind of data poisoning attack: By adding small adversarial noise on the input, it is possible to create a collision in the output space of the transfer learner. As a result, Active Learning algorithms no longer select the optimal instances, but almost exclusively the ones injected by the attacker. This allows an attacker to manipulate the active learner to select and include arbitrary images into the data set, even against an overwhelming majority of unpoisoned samples. We show that a model trained on such a poisoned dataset has a significantly deteriorated performance, dropping from 86\% to 34\% test accuracy. We evaluate this attack on both audio and image datasets and support our findings empirically. To the best of our knowledge, this weakness has not been described before in literature.

</details>

<details>

<summary>2021-01-26 16:02:40 - The Rise of Technology in Crime Prevention: Opportunities, Challenges and Practitioners Perspectives</summary>

- *Dario Ortega Anderez, Eiman Kanjo, Amna Amnwar, Shane Johnson, David Lucy*

- `2102.04204v1` - [abs](http://arxiv.org/abs/2102.04204v1) - [pdf](http://arxiv.org/pdf/2102.04204v1)

> Criminal activity is a prevalent issue in contemporary culture and society, with most nations facing unacceptable levels of crime. Technological innovation has been one of the main driving forces leading to the continuous improvement of crime control and crime prevention strategies (e.g. GPS tracking and tagging, video surveillance, etc.). Given this, it is a moral obligation for the research community to consider how the contemporary technological developments (i.e. Internet of Things (IoT), Machine Learning, Edge Computing)might help reduce crime worldwide. In line with this, this paper provides a discussion of how a sample of contemporary hardware and software-based technologies might help further reduce criminal actions. After a thorough analysis of a wide array of technologies and a number of workshops with organisations of interest, we believe that the adoption of novel technologies by vulnerable individuals, victim support organisations and law enforcement can help reduce the occurrence of criminal activity.

</details>

<details>

<summary>2021-01-26 23:37:30 - Uncertainty aware and explainable diagnosis of retinal disease</summary>

- *Amitojdeep Singh, Sourya Sengupta, Mohammed Abdul Rasheed, Varadharajan Jayakumar, Vasudevan Lakshminarayanan*

- `2101.12041v1` - [abs](http://arxiv.org/abs/2101.12041v1) - [pdf](http://arxiv.org/pdf/2101.12041v1)

> Deep learning methods for ophthalmic diagnosis have shown considerable success in tasks like segmentation and classification. However, their widespread application is limited due to the models being opaque and vulnerable to making a wrong decision in complicated cases. Explainability methods show the features that a system used to make prediction while uncertainty awareness is the ability of a system to highlight when it is not sure about the decision. This is one of the first studies using uncertainty and explanations for informed clinical decision making. We perform uncertainty analysis of a deep learning model for diagnosis of four retinal diseases - age-related macular degeneration (AMD), central serous retinopathy (CSR), diabetic retinopathy (DR), and macular hole (MH) using images from a publicly available (OCTID) dataset. Monte Carlo (MC) dropout is used at the test time to generate a distribution of parameters and the predictions approximate the predictive posterior of a Bayesian model. A threshold is computed using the distribution and uncertain cases can be referred to the ophthalmologist thus avoiding an erroneous diagnosis. The features learned by the model are visualized using a proven attribution method from a previous study. The effects of uncertainty on model performance and the relationship between uncertainty and explainability are discussed in terms of clinical significance. The uncertainty information along with the heatmaps make the system more trustworthy for use in clinical settings.

</details>

<details>

<summary>2021-01-27 03:44:44 - Learnings from Technological Interventions in a Low Resource Language: A Case-Study on Gondi</summary>

- *Devansh Mehta, Sebastin Santy, Ramaravind Kommiya Mothilal, Brij Mohan Lal Srivastava, Alok Sharma, Anurag Shukla, Vishnu Prasad, Venkanna U, Amit Sharma, Kalika Bali*

- `2004.10270v2` - [abs](http://arxiv.org/abs/2004.10270v2) - [pdf](http://arxiv.org/pdf/2004.10270v2)

> The primary obstacle to developing technologies for low-resource languages is the lack of usable data. In this paper, we report the adoption and deployment of 4 technology-driven methods of data collection for Gondi, a low-resource vulnerable language spoken by around 2.3 million tribal people in south and central India. In the process of data collection, we also help in its revival by expanding access to information in Gondi through the creation of linguistic resources that can be used by the community, such as a dictionary, children's stories, an app with Gondi content from multiple sources and an Interactive Voice Response (IVR) based mass awareness platform. At the end of these interventions, we collected a little less than 12,000 translated words and/or sentences and identified more than 650 community members whose help can be solicited for future translation efforts. The larger goal of the project is collecting enough data in Gondi to build and deploy viable language technologies like machine translation and speech to text systems that can help take the language onto the internet.

</details>

<details>

<summary>2021-01-27 06:23:25 - Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks</summary>

- *Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma*

- `2101.05930v2` - [abs](http://arxiv.org/abs/2101.05930v2) - [pdf](http://arxiv.org/pdf/2101.05930v2)

> Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a training time attack that injects a trigger pattern into a small proportion of training data so as to control the model's prediction at the test time. Backdoor attacks are notably dangerous since they do not affect the model's performance on clean examples, yet can fool the model to make incorrect prediction whenever the trigger pattern appears during testing. In this paper, we propose a novel defense framework Neural Attention Distillation (NAD) to erase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to guide the finetuning of the backdoored student network on a small clean subset of data such that the intermediate-layer attention of the student network aligns with that of the teacher network. The teacher network can be obtained by an independent finetuning process on the same clean subset. We empirically show, against 6 state-of-the-art backdoor attacks, NAD can effectively erase the backdoor triggers using only 5\% clean training data without causing obvious performance degradation on clean examples. Code is available in https://github.com/bboylyg/NAD.

</details>

<details>

<summary>2021-01-27 13:20:57 - Robust Ensemble Model Training via Random Layer Sampling Against Adversarial Attack</summary>

- *Hakmin Lee, Hong Joo Lee, Seong Tae Kim, Yong Man Ro*

- `2005.10757v2` - [abs](http://arxiv.org/abs/2005.10757v2) - [pdf](http://arxiv.org/pdf/2005.10757v2)

> Deep neural networks have achieved substantial achievements in several computer vision areas, but have vulnerabilities that are often fooled by adversarial examples that are not recognized by humans. This is an important issue for security or medical applications. In this paper, we propose an ensemble model training framework with random layer sampling to improve the robustness of deep neural networks. In the proposed training framework, we generate various sampled model through the random layer sampling and update the weight of the sampled model. After the ensemble models are trained, it can hide the gradient efficiently and avoid the gradient-based attack by the random layer sampling method. To evaluate our proposed method, comprehensive and comparative experiments have been conducted on three datasets. Experimental results show that the proposed method improves the adversarial robustness.

</details>

<details>

<summary>2021-01-27 16:07:07 - A Stealthy Hardware Trojan Exploiting the Architectural Vulnerability of Deep Learning Architectures: Input Interception Attack (IIA)</summary>

- *Tolulope A. Odetola, Hawzhin Raoof Mohammed, Syed Rafay Hasan*

- `1911.00783v2` - [abs](http://arxiv.org/abs/1911.00783v2) - [pdf](http://arxiv.org/pdf/1911.00783v2)

> Deep learning architectures (DLA) have shown impressive performance in computer vision, natural language processing and so on. Many DLA make use of cloud computing to achieve classification due to the high computation and memory requirements. Privacy and latency concerns resulting from cloud computing has inspired the deployment of DLA on embedded hardware accelerators. To achieve short time-to-market and have access to global experts, state-of-the-art techniques of DLA deployment on hardware accelerators are outsourced to untrusted third parties. This outsourcing raises security concerns as hardware Trojans can be inserted into the hardware design of the mapped DLA of the hardware accelerator. We argue that existing hardware Trojan attacks highlighted in literature have no qualitative means how definite they are of the triggering of the Trojan. Also, most inserted Trojans show a obvious spike in the number of hardware resources utilized on the accelerator at the time of triggering the Trojan or when the payload is active. In this paper, we introduce a hardware Trojan attack called Input Interception Attack (IIA). In this attack, we make use of the statistical properties of layer-by-layer output to ensure that asides from being stealthy. Our IIA is able to trigger with some measure of definiteness. Moreover, this IIA attack is tested on DLA used to classify MNIST and Cifar-10 data sets. The attacked design utilizes approximately up to 2% more LUTs respectively compared to the un-compromised designs. Finally, this paper discusses potential defensive mechanisms that could be used to combat such hardware Trojans based attack in hardware accelerators for DLA.

</details>

<details>

<summary>2021-01-27 17:39:13 - Privacy-Preserving Smart Parking System Using Blockchain and Private Information Retrieval</summary>

- *Wesam Al Amiri, Mohamed Baza, Karim Banawan, Mohamed Mahmoud, Waleed Alasmary, Kemal Akkaya*

- `1904.09703v3` - [abs](http://arxiv.org/abs/1904.09703v3) - [pdf](http://arxiv.org/pdf/1904.09703v3)

> Searching for available parking spaces is a major problem for drivers especially in big crowded cities, causing traffic congestion and air pollution, and wasting drivers' time. Smart parking systems are a novel solution to enable drivers to have real-time parking information for pre-booking. However, current smart parking requires drivers to disclose their private information, such as desired destinations. Moreover, the existing schemes are centralized and vulnerable to the bottleneck of the single point of failure and data breaches. In this paper, we propose a distributed privacy-preserving smart parking system using blockchain. A consortium blockchain created by different parking lot owners to ensure security, transparency, and availability is proposed to store their parking offers on the blockchain. To preserve drivers' location privacy, we adopt a private information retrieval (PIR) technique to enable drivers to retrieve parking offers from blockchain nodes privately, without revealing which parking offers are retrieved. Furthermore, a short randomizable signature is used to enable drivers to reserve available parking slots in an anonymous manner. Besides, we introduce an anonymous payment system that cannot link drivers' to specific parking locations. Finally, our performance evaluations demonstrate that the proposed scheme can preserve drivers' privacy with low communication and computation overhead.

</details>

<details>

<summary>2021-01-28 03:28:18 - Covert Model Poisoning Against Federated Learning: Algorithm Design and Optimization</summary>

- *Kang Wei, Jun Li, Ming Ding, Chuan Ma, Yo-Seb Jeon, H. Vincent Poor*

- `2101.11799v1` - [abs](http://arxiv.org/abs/2101.11799v1) - [pdf](http://arxiv.org/pdf/2101.11799v1)

> Federated learning (FL), as a type of distributed machine learning frameworks, is vulnerable to external attacks on FL models during parameters transmissions. An attacker in FL may control a number of participant clients, and purposely craft the uploaded model parameters to manipulate system outputs, namely, model poisoning (MP). In this paper, we aim to propose effective MP algorithms to combat state-of-the-art defensive aggregation mechanisms (e.g., Krum and Trimmed mean) implemented at the server without being noticed, i.e., covert MP (CMP). Specifically, we first formulate the MP as an optimization problem by minimizing the Euclidean distance between the manipulated model and designated one, constrained by a defensive aggregation rule. Then, we develop CMP algorithms against different defensive mechanisms based on the solutions of their corresponding optimization problems. Furthermore, to reduce the optimization complexity, we propose low complexity CMP algorithms with a slight performance degradation. In the case that the attacker does not know the defensive aggregation mechanism, we design a blind CMP algorithm, in which the manipulated model will be adjusted properly according to the aggregated model generated by the unknown defensive aggregation. Our experimental results demonstrate that the proposed CMP algorithms are effective and substantially outperform existing attack mechanisms.

</details>

<details>

<summary>2021-01-28 10:33:50 - Detecting Malicious Accounts showing Adversarial Behavior in Permissionless Blockchains</summary>

- *Rachit Agarwal, Tanmay Thapliyal, Sandeep K. Shukla*

- `2101.11915v1` - [abs](http://arxiv.org/abs/2101.11915v1) - [pdf](http://arxiv.org/pdf/2101.11915v1)

> Different types of malicious activities have been flagged in multiple permissionless blockchains such as bitcoin, Ethereum etc. While some malicious activities exploit vulnerabilities in the infrastructure of the blockchain, some target its users through social engineering techniques. To address these problems, we aim at automatically flagging blockchain accounts that originate such malicious exploitation of accounts of other participants. To that end, we identify a robust supervised machine learning (ML) algorithm that is resistant to any bias induced by an over representation of certain malicious activity in the available dataset, as well as is robust against adversarial attacks. We find that most of the malicious activities reported thus far, for example, in Ethereum blockchain ecosystem, behaves statistically similar. Further, the previously used ML algorithms for identifying malicious accounts show bias towards a particular malicious activity which is over-represented. In the sequel, we identify that Neural Networks (NN) holds up the best in the face of such bias inducing dataset at the same time being robust against certain adversarial attacks.

</details>

<details>

<summary>2021-01-28 16:01:20 - Neural Architecture Search by Estimation of Network Structure Distributions</summary>

- *Anton Muravev, Jenni Raitoharju, Moncef Gabbouj*

- `1908.06886v3` - [abs](http://arxiv.org/abs/1908.06886v3) - [pdf](http://arxiv.org/pdf/1908.06886v3)

> The influence of deep learning is continuously expanding across different domains, and its new applications are ubiquitous. The question of neural network design thus increases in importance, as traditional empirical approaches are reaching their limits. Manual design of network architectures from scratch relies heavily on trial and error, while using existing pretrained models can introduce redundancies or vulnerabilities. Automated neural architecture design is able to overcome these problems, but the most successful algorithms operate on significantly constrained design spaces, assuming the target network to consist of identical repeating blocks. While such approach allows for faster search, it does so at the cost of expressivity. We instead propose an alternative probabilistic representation of a whole neural network structure under the assumption of independence between layer types. Our matrix of probabilities is equivalent to the population of models, but allows for discovery of structural irregularities, while being simple to interpret and analyze. We construct an architecture search algorithm, inspired by the estimation of distribution algorithms, to take advantage of this representation. The probability matrix is tuned towards generating high-performance models by repeatedly sampling the architectures and evaluating the corresponding networks, while gradually increasing the model depth. Our algorithm is shown to discover non-regular models which cannot be expressed via blocks, but are competitive both in accuracy and computational cost, while not utilizing complex dataflows or advanced training techniques, as well as remaining conceptually simple and highly extensible.

</details>

<details>

<summary>2021-01-28 16:34:04 - Adversarial Machine Learning Attacks on Condition-Based Maintenance Capabilities</summary>

- *Hamidreza Habibollahi Najaf Abadi*

- `2101.12097v1` - [abs](http://arxiv.org/abs/2101.12097v1) - [pdf](http://arxiv.org/pdf/2101.12097v1)

> Condition-based maintenance (CBM) strategies exploit machine learning models to assess the health status of systems based on the collected data from the physical environment, while machine learning models are vulnerable to adversarial attacks. A malicious adversary can manipulate the collected data to deceive the machine learning model and affect the CBM system's performance. Adversarial machine learning techniques introduced in the computer vision domain can be used to make stealthy attacks on CBM systems by adding perturbation to data to confuse trained models. The stealthy nature causes difficulty and delay in detection of the attacks. In this paper, adversarial machine learning in the domain of CBM is introduced. A case study shows how adversarial machine learning can be used to attack CBM capabilities. Adversarial samples are crafted using the Fast Gradient Sign method, and the performance of a CBM system under attack is investigated. The obtained results reveal that CBM systems are vulnerable to adversarial machine learning attacks and defense strategies need to be considered.

</details>

<details>

<summary>2021-01-28 17:47:48 - Optimism in the Face of Adversity: Understanding and Improving Deep Learning through Adversarial Robustness</summary>

- *Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard*

- `2010.09624v2` - [abs](http://arxiv.org/abs/2010.09624v2) - [pdf](http://arxiv.org/pdf/2010.09624v2)

> Driven by massive amounts of data and important advances in computational resources, new deep learning systems have achieved outstanding results in a large spectrum of applications. Nevertheless, our current theoretical understanding on the mathematical foundations of deep learning lags far behind its empirical success. Towards solving the vulnerability of neural networks, however, the field of adversarial robustness has recently become one of the main sources of explanations of our deep models. In this article, we provide an in-depth review of the field of adversarial robustness in deep learning, and give a self-contained introduction to its main notions. But, in contrast to the mainstream pessimistic perspective of adversarial robustness, we focus on the main positive aspects that it entails. We highlight the intuitive connection between adversarial examples and the geometry of deep neural networks, and eventually explore how the geometric study of adversarial examples can serve as a powerful tool to understand deep learning. Furthermore, we demonstrate the broad applicability of adversarial robustness, providing an overview of the main emerging applications of adversarial robustness beyond security. The goal of this article is to provide readers with a set of new perspectives to understand deep learning, and to supply them with intuitive tools and insights on how to use adversarial robustness to improve it.

</details>

<details>

<summary>2021-01-28 21:07:23 - GF-Flush: A GF(2) Algebraic Attack on Secure Scan Chains</summary>

- *Dake Chen, Chunxiao Lin, Peter A. Beerel*

- `2101.12279v1` - [abs](http://arxiv.org/abs/2101.12279v1) - [pdf](http://arxiv.org/pdf/2101.12279v1)

> Scan chains provide increased controllability and observability for testing digital circuits. The increased testability, however, can also be a source of information leakage for sensitive designs. The state-of-the-art defenses to secure scan chains apply dynamic keys to pseudo-randomly invert the scan vectors. In this paper, we pinpoint an algebraic vulnerability of these dynamic defenses that involves creating and solving a system of linear equations over the finite field GF(2). In particular, we propose a novel GF(2)-based flush attack that breaks even the most rigorous version of state-of-the-art dynamic defenses. Our experimental results demonstrate that our attack recovers the key as long as 500 bits in less than 7 seconds, the attack times are about one hundredth of state-of-the-art SAT based attacks on the same defenses. We then demonstrate how our attacks can be extended to scan chains compressed with Multiple-Input Signature Registers (MISRs).

</details>

<details>

<summary>2021-01-29 07:31:09 - Finding the Sweet Spot for Data Anonymization: A Mechanism Design Perspective</summary>

- *Abdelrahman Eldosouky, Tapadhir Das, Anuraag Kotra, Shamik Sengupta*

- `2101.12442v1` - [abs](http://arxiv.org/abs/2101.12442v1) - [pdf](http://arxiv.org/pdf/2101.12442v1)

> Data sharing between different organizations is an essential process in today's connected world. However, recently there were many concerns about data sharing as sharing sensitive information can jeopardize users' privacy. To preserve the privacy, organizations use anonymization techniques to conceal users' sensitive data. However, these techniques are vulnerable to de-anonymization attacks which aim to identify individual records within a dataset. In this paper, a two-tier mathematical framework is proposed for analyzing and mitigating the de-anonymization attacks, by studying the interactions between sharing organizations, data collector, and a prospective attacker. In the first level, a game-theoretic model is proposed to enable sharing organizations to optimally select their anonymization levels for k-anonymization under two potential attacks: background-knowledge attack and homogeneity attack. In the second level, a contract-theoretic model is proposed to enable the data collector to optimally reward the organizations for their data. The formulated problems are studied under single-time sharing and repeated sharing scenarios. Different Nash equilibria for the proposed game and the optimal solution of the contract-based problem are analytically derived for both scenarios. Simulation results show that the organizations can optimally select their anonymization levels, while the data collector can benefit from incentivizing the organizations to share their data.

</details>

<details>

<summary>2021-01-29 16:06:58 - Improving Neural Network Robustness through Neighborhood Preserving Layers</summary>

- *Bingyuan Liu, Christopher Malon, Lingzhou Xue, Erik Kruus*

- `2101.11766v2` - [abs](http://arxiv.org/abs/2101.11766v2) - [pdf](http://arxiv.org/pdf/2101.11766v2)

> Robustness against adversarial attack in neural networks is an important research topic in the machine learning community. We observe one major source of vulnerability of neural nets is from overparameterized fully-connected layers. In this paper, we propose a new neighborhood preserving layer which can replace these fully connected layers to improve the network robustness. We demonstrate a novel neural network architecture which can incorporate such layers and also can be trained efficiently. We theoretically prove that our models are more robust against distortion because they effectively control the magnitude of gradients. Finally, we empirically show that our designed network architecture is more robust against state-of-art gradient descent based attacks, such as a PGD attack on the benchmark datasets MNIST and CIFAR10.

</details>

<details>

<summary>2021-01-29 18:54:23 - Improving VAEs' Robustness to Adversarial Attack</summary>

- *Matthew Willetts, Alexander Camuto, Tom Rainforth, Stephen Roberts, Chris Holmes*

- `1906.00230v6` - [abs](http://arxiv.org/abs/1906.00230v6) - [pdf](http://arxiv.org/pdf/1906.00230v6)

> Variational autoencoders (VAEs) have recently been shown to be vulnerable to adversarial attacks, wherein they are fooled into reconstructing a chosen target image. However, how to defend against such attacks remains an open problem. We make significant advances in addressing this issue by introducing methods for producing adversarially robust VAEs. Namely, we first demonstrate that methods proposed to obtain disentangled latent representations produce VAEs that are more robust to these attacks. However, this robustness comes at the cost of reducing the quality of the reconstructions. We ameliorate this by applying disentangling methods to hierarchical VAEs. The resulting models produce high-fidelity autoencoders that are also adversarially robust. We confirm their capabilities on several different datasets and with current state-of-the-art VAE adversarial attacks, and also show that they increase the robustness of downstream tasks to attack.

</details>

<details>

<summary>2021-01-29 20:16:45 - Model Adaptation for Image Reconstruction using Generalized Stein's Unbiased Risk Estimator</summary>

- *Hemant Kumar Aggarwal, Mathews Jacob*

- `2102.00047v1` - [abs](http://arxiv.org/abs/2102.00047v1) - [pdf](http://arxiv.org/pdf/2102.00047v1)

> Deep learning image reconstruction algorithms often suffer from model mismatches when the acquisition scheme differs significantly from the forward model used during training. We introduce a Generalized Stein's Unbiased Risk Estimate (GSURE) loss metric to adapt the network to the measured k-space data and minimize model misfit impact. Unlike current methods that rely on the mean square error in kspace, the proposed metric accounts for noise in the measurements. This makes the approach less vulnerable to overfitting, thus offering improved reconstruction quality compared to schemes that rely on mean-square error. This approach may be useful to rapidly adapt pre-trained models to new acquisition settings (e.g., multi-site) and different contrasts than training data

</details>

<details>

<summary>2021-01-31 00:24:32 - Efficient Certification of Spatial Robustness</summary>

- *Anian Ruoss, Maximilian Baader, Mislav Balunović, Martin Vechev*

- `2009.09318v2` - [abs](http://arxiv.org/abs/2009.09318v2) - [pdf](http://arxiv.org/pdf/2009.09318v2)

> Recent work has exposed the vulnerability of computer vision models to vector field attacks. Due to the widespread usage of such models in safety-critical applications, it is crucial to quantify their robustness against such spatial transformations. However, existing work only provides empirical robustness quantification against vector field deformations via adversarial attacks, which lack provable guarantees. In this work, we propose novel convex relaxations, enabling us, for the first time, to provide a certificate of robustness against vector field transformations. Our relaxations are model-agnostic and can be leveraged by a wide range of neural network verifiers. Experiments on various network architectures and different datasets demonstrate the effectiveness and scalability of our method.

</details>

<details>

<summary>2021-01-31 14:18:37 - Toward Blockchain-Enabled Supply Chain Anti-Counterfeiting and Traceability</summary>

- *Neo C. K. Yiu*

- `2102.00459v1` - [abs](http://arxiv.org/abs/2102.00459v1) - [pdf](http://arxiv.org/pdf/2102.00459v1)

> Innovative solutions addressing product anti-counterfeiting and record provenance have been deployed across today's internationally spanning supply chain networks. These product anti-counterfeiting solutions are developed and implemented with centralized system architecture relying on centralized authorities or any form of intermediaries. Vulnerabilities of centralized product anti-counterfeiting solutions could possibly lead to system failure or susceptibility of malicious modifications performed on product records or various potential attacks to the system components by dishonest participant nodes traversing along the supply chain. Blockchain technology has progressed from merely with a use case of immutable ledger for cryptocurrency transactions to a programmable interactive environment of developing decentralized and reliable applications addressing different use cases globally. In this research, so as to facilitate trustworthy data provenance retrieval, verification and management, as well as strengthening capability of product anti-counterfeiting, key areas of decentralization and feasible mechanisms of developing decentralized and distributed product anti-counterfeiting and traceability ecosystems utilizing blockchain technology, are identified via a series of security and threat analyses performed mainly against NFC-Enabled Anti-Counterfeiting System (NAS) which is one of the solutions currently implemented in the industry with centralized architecture. A set of fundamental system requirements are set out for developing a blockchain-enabled autonomous and decentralized solution for supply chain anti-counterfeiting and traceability, as a secure and immutable scientific data provenance tracking and management platform in which provenance records, providing compelling properties on data integrity of luxurious goods, are recorded and verified automatically, for supply chain industry.

</details>

<details>

<summary>2021-01-31 17:25:49 - Rethinking the Trigger of Backdoor Attack</summary>

- *Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shutao Xia*

- `2004.04692v3` - [abs](http://arxiv.org/abs/2004.04692v3) - [pdf](http://arxiv.org/pdf/2004.04692v3)

> Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of the infected model will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger, while it performs well on benign samples. Currently, most of existing backdoor attacks adopted the setting of \emph{static} trigger, $i.e.,$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing the characteristics of the static trigger. We demonstrate that such an attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. We further explore how to utilize this property for backdoor defense, and discuss how to alleviate such vulnerability of existing attacks.

</details>


## 2021-02

<details>

<summary>2021-02-01 11:42:30 - Designing constraint-based false data injection attacks against the unbalanced distribution smart grids</summary>

- *Nam N. Tran, Hemanshu R. Pota, Quang N. Tran, Jiankun Hu*

- `2003.05088v2` - [abs](http://arxiv.org/abs/2003.05088v2) - [pdf](http://arxiv.org/pdf/2003.05088v2)

> The advent of smart power grid which plays a vital role in the upcoming smart city era is accompanied with the implementation of a monitoring tool, called state estimation. For the case of the unbalanced residential distribution grid, the state estimating operation which is conducted at a regional scale is considered as an application of the edge computing-based Internet of Things (IoT). While the outcome of the state estimation is important to the subsequent control activities, its accuracy heavily depends on the data integrity of the information collected from the scattered measurement devices. This fact exposes the vulnerability of the state estimation module under the effect of data-driven attacks. Among these, false data injection attack (FDI) is attracting much attention due to its capability to interfere with the normal operation of the network without being detected. This paper presents an attack design scheme based on a nonlinear physical-constraint model that is able to produce an FDI attack with theoretically stealthy characteristic. To demonstrate the effectiveness of the proposed design scheme, simulations with the IEEE 13-node test feeder and the WSCC 9-bus system are conducted. The experimental results indicate that not only the false positive rate of the bad data detection mechanism is 100 per cent but the physical consequence of the attack is severe. These results pose a serious challenge for operators in maintaining the integrity of measurement data.

</details>

<details>

<summary>2021-02-01 15:36:40 - Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems</summary>

- *Alireza Bahramali, Milad Nasr, Amir Houmansadr, Dennis Goeckel, Don Towsley*

- `2102.00918v1` - [abs](http://arxiv.org/abs/2102.00918v1) - [pdf](http://arxiv.org/pdf/2102.00918v1)

> Deep Neural Networks (DNNs) have become prevalent in wireless communication systems due to their promising performance. However, similar to other DNN-based applications, they are vulnerable to adversarial examples. In this work, we propose an input-agnostic, undetectable, and robust adversarial attack against DNN-based wireless communication systems in both white-box and black-box scenarios. We design tailored Universal Adversarial Perturbations (UAPs) to perform the attack. We also use a Generative Adversarial Network (GAN) to enforce an undetectability constraint for our attack. Furthermore, we investigate the robustness of our attack against countermeasures. We show that in the presence of defense mechanisms deployed by the communicating parties, our attack performs significantly better compared to existing attacks against DNN-based wireless systems. In particular, the results demonstrate that even when employing well-considered defenses, DNN-based wireless communications are vulnerable to adversarial attacks.

</details>

<details>

<summary>2021-02-02 14:22:45 - PatternMonitor: a whole pipeline with a much higher level of automation for guessing Android lock pattern based on videos</summary>

- *Yangde Wang, Weidong Qiu, Yuming Xie, Yan Zha*

- `2102.01509v1` - [abs](http://arxiv.org/abs/2102.01509v1) - [pdf](http://arxiv.org/pdf/2102.01509v1)

> Pattern lock is a general technique used to realize identity authentication and access authorization on mobile terminal devices such as Android platform devices, but it is vulnerable to the attack proposed by recent researches that exploit information leaked by users while drawing patterns. However, the existing attacks on pattern lock are environmentally sensitive, and rely heavily on manual work, which constrains the practicability of these attack approaches. To attain a more practical attack, this paper designs the PatternMonitor, a whole pipeline with a much higher level of automation system againsts pattern lock, which extracts the guessed candidate patterns from a video containing pattern drawing: instead of manually cutting the target video and setting thresholds, it first employs recognition models to locate the target phone and keypoints of pattern drawing hand, which enables the gesture can be recognized even when the fingertips are shaded. Then, we extract the frames from the video where the drawing starts and ends. These pre-processed frames are inputs of target tracking model to generate trajectories, and further transformed into possible candidate patterns by performing our designed algorithm. To the best of our knowledge, our work is the first attack system to generate candidate patterns by only relying on hand movement instead of accurate fingertips capture. The experimental results demonstrates that our work is as accurate as previous work, which gives more than 90\% success rate within 20 attempts.

</details>

<details>

<summary>2021-02-02 19:35:50 - A Historical and Statistical Studyof the Software Vulnerability Landscape</summary>

- *Assane Gueye, Peter Mell*

- `2102.01722v1` - [abs](http://arxiv.org/abs/2102.01722v1) - [pdf](http://arxiv.org/pdf/2102.01722v1)

> Understanding the landscape of software vulnerabilities is key for developing effective security solutions. Fortunately, the evaluation of vulnerability databases that use a framework for communicating vulnerability attributes and their severity scores, such as the Common Vulnerability Scoring System (CVSS), can help shed light on the nature of publicly published vulnerabilities. In this paper, we characterize the software vulnerability landscape by performing a historical and statistical analysis of CVSS vulnerability metrics over the period of 2005 to 2019 through using data from the National Vulnerability Database. We conduct three studies analyzing the following: the distribution of CVSS scores (both empirical and theoretical), the distribution of CVSS metric values and how vulnerability characteristics change over time, and the relative rankings of the most frequent metric value over time. Our resulting analysis shows that the vulnerability threat landscape has been dominated by only a few vulnerability types and has changed little during the time period of the study. The overwhelming majority of vulnerabilities are exploitable over the network. The complexity to successfully exploit these vulnerabilities is dominantly low; very little authentication to the target victim is necessary for a successful attack. And most of the flaws require very limited interaction with users. However on the positive side, the damage of these vulnerabilities is mostly confined within the security scope of the impacted components. A discussion of lessons that could be learned from this analysis is presented.

</details>

<details>

<summary>2021-02-03 01:43:20 - Membership Inference Attacks and Defenses in Classification Models</summary>

- *Jiacheng Li, Ninghui Li, Bruno Ribeiro*

- `2002.12062v3` - [abs](http://arxiv.org/abs/2002.12062v3) - [pdf](http://arxiv.org/pdf/2002.12062v3)

> We study the membership inference (MI) attack against classifiers, where the attacker's goal is to determine whether a data instance was used for training the classifier. Through systematic cataloging of existing MI attacks and extensive experimental evaluations of them, we find that a model's vulnerability to MI attacks is tightly related to the generalization gap -- the difference between training accuracy and test accuracy. We then propose a defense against MI attacks that aims to close the gap by intentionally reduces the training accuracy. More specifically, the training process attempts to match the training and validation accuracies, by means of a new {\em set regularizer} using the Maximum Mean Discrepancy between the softmax output empirical distributions of the training and validation sets. Our experimental results show that combining this approach with another simple defense (mix-up training) significantly improves state-of-the-art defense against MI attacks, with minimal impact on testing accuracy.

</details>

<details>

<summary>2021-02-03 12:19:10 - On Robustness of Neural Semantic Parsers</summary>

- *Shuo Huang, Zhuang Li, Lizhen Qu, Lei Pan*

- `2102.01563v2` - [abs](http://arxiv.org/abs/2102.01563v2) - [pdf](http://arxiv.org/pdf/2102.01563v2)

> Semantic parsing maps natural language (NL) utterances into logical forms (LFs), which underpins many advanced NLP problems. Semantic parsers gain performance boosts with deep neural networks, but inherit vulnerabilities against adversarial examples. In this paper, we provide the empirical study on the robustness of semantic parsers in the presence of adversarial attacks. Formally, adversaries of semantic parsing are considered to be the perturbed utterance-LF pairs, whose utterances have exactly the same meanings as the original ones. A scalable methodology is proposed to construct robustness test sets based on existing benchmark corpora. Our results answered five research questions in measuring the sate-of-the-art parsers' performance on robustness test sets, and evaluating the effect of data augmentation.

</details>

<details>

<summary>2021-02-03 22:03:30 - Fuzzing Hardware Like Software</summary>

- *Timothy Trippel, Kang G. Shin, Alex Chernyakhovsky, Garret Kelly, Dominic Rizzo, Matthew Hicks*

- `2102.02308v1` - [abs](http://arxiv.org/abs/2102.02308v1) - [pdf](http://arxiv.org/pdf/2102.02308v1)

> Hardware flaws are permanent and potent: hardware cannot be patched once fabricated, and any flaws may undermine any software executing on top. Consequently, verification time dominates implementation time. The gold standard in hardware Design Verification (DV) is concentrated at two extremes: random dynamic verification and formal verification. Both struggle to root out the subtle flaws in complex hardware that often manifest as security vulnerabilities. The root problem with random verification is its undirected nature, making it inefficient, while formal verification is constrained by the state-space explosion problem, making it infeasible against complex designs. What is needed is a solution that is directed, yet under-constrained.   Instead of making incremental improvements to existing DV approaches, we leverage the observation that existing software fuzzers already provide such a solution, and adapt them for hardware DV. Specifically, we translate RTL hardware to a software model and fuzz that model. The central challenge we address is how best to mitigate the differences between the hardware execution model and software execution model. This includes: 1) how to represent test cases, 2) what is the hardware equivalent of a crash, 3) what is an appropriate coverage metric, and 4) how to create a general-purpose fuzzing harness for hardware.   To evaluate our approach, we fuzz four IP blocks from Google's OpenTitan SoC. Our experiments reveal a two orders-of-magnitude reduction in run time to achieve Finite State Machine (FSM) coverage over traditional dynamic verification schemes. Moreover, with our design-agnostic harness, we achieve over 88% HDL line coverage in three out of four of our designs -- even without any initial seeds.

</details>

<details>

<summary>2021-02-04 01:55:31 - Verifying Security Vulnerabilities in Large Software Systems using Multi-Core k-Induction</summary>

- *Thales Silva, Carmina Porto, Erickson Alves, Lucas Cordeiro, Herbert Rocha*

- `2102.02368v1` - [abs](http://arxiv.org/abs/2102.02368v1) - [pdf](http://arxiv.org/pdf/2102.02368v1)

> Computer-based systems have been used to solve several domain problems, such as industrial, military, education, and wearable. Those systems need high-quality software to guarantee security and safety. We advocate that Bounded Model Checking (BMC) techniques can detect security vulnerabilities in the early stages of development processes. However, this technique struggles to scale up and verify large software commonly found on computer-based systems. Here, we develop and evaluate a pragmatic approach to verify large software systems using a state-of-the-art bounded model checker. In particular, we pre-process the input source-code files and then guide the model checker to explore the code systematically. We also present a multi-core implementation of the k-induction proof algorithm to verify and falsify large software systems iteratively. Our experimental results using the Efficient SMT-based Model Checker (ESBMC) show that our approach can guide ESBMC to efficiently verify large software systems. We evaluate our approach using the PuTTY application to verify 136 files and 2803 functions in less than 86 minutes, and the SlimGuard allocator, where we have found real security vulnerabilities confirmed by the developers. We conclude that our approach can successfully guide a bounded model checker to verify large software systems systematically.

</details>

<details>

<summary>2021-02-04 18:03:06 - Comparing State-of-the-Art and Emerging Augmented Reality Interfaces for Autonomous Vehicle-to-Pedestrian Communication</summary>

- *F. Gabriele Pratticò, Fabrizio Lamberti, Alberto Cannavò, Lia Morra, Paolo Montuschi*

- `2102.02783v1` - [abs](http://arxiv.org/abs/2102.02783v1) - [pdf](http://arxiv.org/pdf/2102.02783v1)

> Providing pedestrians and other vulnerable road users with a clear indication about a fully autonomous vehicle status and intentions is crucial to make them coexist. In the last few years, a variety of external interfaces have been proposed, leveraging different paradigms and technologies including vehicle-mounted devices (like LED panels), short-range on-road projections, and road infrastructure interfaces (e.g., special asphalts with embedded displays). These designs were experimented in different settings, using mockups, specially prepared vehicles, or virtual environments, with heterogeneous evaluation metrics. Promising interfaces based on Augmented Reality (AR) have been proposed too, but their usability and effectiveness have not been tested yet. This paper aims to complement such body of literature by presenting a comparison of state-of-the-art interfaces and new designs under common conditions. To this aim, an immersive Virtual Reality-based simulation was developed, recreating a well-known scenario represented by pedestrians crossing in urban environments under non-regulated conditions. A user study was then performed to investigate the various dimensions of vehicle-to-pedestrian interaction leveraging objective and subjective metrics. Even though no interface clearly stood out over all the considered dimensions, one of the AR designs achieved state-of-the-art results in terms of safety and trust, at the cost of higher cognitive effort and lower intuitiveness compared to LED panels showing anthropomorphic features. Together with rankings on the various dimensions, indications about advantages and drawbacks of the various alternatives that emerged from this study could provide important information for next developments in the field.

</details>

<details>

<summary>2021-02-04 19:20:35 - Building Representative Corpora from Illiterate Communities: A Review of Challenges and Mitigation Strategies for Developing Countries</summary>

- *Stephanie Hirmer, Alycia Leonard, Josephine Tumwesige, Costanza Conforti*

- `2102.02841v1` - [abs](http://arxiv.org/abs/2102.02841v1) - [pdf](http://arxiv.org/pdf/2102.02841v1)

> Most well-established data collection methods currently adopted in NLP depend on the assumption of speaker literacy. Consequently, the collected corpora largely fail to represent swathes of the global population, which tend to be some of the most vulnerable and marginalised people in society, and often live in rural developing areas. Such underrepresented groups are thus not only ignored when making modeling and system design decisions, but also prevented from benefiting from development outcomes achieved through data-driven NLP. This paper aims to address the under-representation of illiterate communities in NLP corpora: we identify potential biases and ethical issues that might arise when collecting data from rural communities with high illiteracy rates in Low-Income Countries, and propose a set of practical mitigation strategies to help future work.

</details>

<details>

<summary>2021-02-05 12:15:06 - Over 100 Bugs in a Row: Security Analysis of the Top-Rated Joomla Extensions</summary>

- *Marcus Niemietz, Mario Korth, Christian Mainka, Juraj Somorovsky*

- `2102.03131v1` - [abs](http://arxiv.org/abs/2102.03131v1) - [pdf](http://arxiv.org/pdf/2102.03131v1)

> Nearly every second website is using a Content Management System (CMS) such as WordPress, Drupal, and Joomla. These systems help to create and modify digital data, typically within a collaborative environment. One common feature is to enrich their functionality by using extensions. Popular extensions allow developers to easily include payment gateways, backup tools, and social media components.   Due to the extended functionality, it is not surprising that such an expansion of complexity implies a bigger attack surface. In contrast to CMS core systems, extensions are usually not considered during public security audits. However, a Cross-Site Scripting (XSS) or SQL injection (SQLi) attack within an activated extension has the same effect on the security of a CMS as the same issue within the core itself. Therefore, vulnerabilities within extensions are a very attractive tool for malicious parties.   We study the security of CMS extensions using the example Joomla; one of the most popular systems. We discovered that nearly every second installation of such a system also includes Joomla's official top-10 rated extensions as a per se requirement. Moreover, we have detected that every single extension of the official top-10 rated extensions is vulnerable to XSS and 30% of them against SQLi. We show that our findings are not only relevant to Joomla; two of the analyzed extensions are available within systems like WordPress or Drupal, and introduce the same vulnerabilities. Finally, we pinpoint mitigation strategies that can be realized within extensions to achieve the same security level as the core CMS.

</details>

<details>

<summary>2021-02-05 15:57:48 - Analyzing Host-Viral Interactome of SARS-CoV-2 for Identifying Vulnerable Host Proteins during COVID-19 Pathogenesis</summary>

- *Jayanta Kumar Das, Swarup Roy, Pietro Hiram Guzzi*

- `2102.03253v1` - [abs](http://arxiv.org/abs/2102.03253v1) - [pdf](http://arxiv.org/pdf/2102.03253v1)

> The development of therapeutic targets for COVID-19 treatment is based on the understanding of the molecular mechanism of pathogenesis. The identification of genes and proteins involved in the infection mechanism is the key to shed out light into the complex molecular mechanisms. The combined effort of many laboratories distributed throughout the world has produced the accumulation of both protein and genetic interactions. In this work we integrate these available results and we obtain an host protein-protein interaction network composed by 1432 human proteins. We calculate network centrality measures to identify key proteins. Then we perform functional enrichment of central proteins. We observed that the identified proteins are mostly associated with several crucial pathways, including cellular process, signalling transduction, neurodegenerative disease. Finally, we focused on proteins involved in causing disease in the human respiratory tract. We conclude that COVID19 is a complex disease, and we highlighted many potential therapeutic targets including RBX1, HSPA5, ITCH, RAB7A, RAB5A, RAB8A, PSMC5, CAPZB, CANX, IGF2R, HSPA1A, which are central and also associated with multiple diseases

</details>

<details>

<summary>2021-02-06 09:37:09 - Convolutional Neural Network-based Intrusion Detection System for AVTP Streams in Automotive Ethernet-based Networks</summary>

- *Seonghoon Jeong, Boosun Jeon, Boheung Chung, Huy Kang Kim*

- `2102.03546v1` - [abs](http://arxiv.org/abs/2102.03546v1) - [pdf](http://arxiv.org/pdf/2102.03546v1)

> Connected and autonomous vehicles (CAVs) are an innovative form of traditional vehicles. Automotive Ethernet replaces the controller area network and FlexRay to support the large throughput required by high-definition applications. As CAVs have numerous functions, they exhibit a large attack surface and an increased vulnerability to attacks. However, no previous studies have focused on intrusion detection in automotive Ethernet-based networks. In this paper, we present an intrusion detection method for detecting audio-video transport protocol (AVTP) stream injection attacks in automotive Ethernet-based networks. To the best of our knowledge, this is the first such method developed for automotive Ethernet. The proposed intrusion detection model is based on feature generation and a convolutional neural network (CNN). To evaluate our intrusion detection system, we built a physical BroadR-Reach-based testbed and captured real AVTP packets. The experimental results show that the model exhibits outstanding performance: the F1-score and recall are greater than 0.9704 and 0.9949, respectively. In terms of the inference time per input and the generation intervals of AVTP traffic, our CNN model can readily be employed for real-time detection.

</details>

<details>

<summary>2021-02-06 14:46:56 - Adversarial Attacks against Face Recognition: A Comprehensive Study</summary>

- *Fatemeh Vakhshiteh, Ahmad Nickabadi, Raghavendra Ramachandra*

- `2007.11709v3` - [abs](http://arxiv.org/abs/2007.11709v3) - [pdf](http://arxiv.org/pdf/2007.11709v3)

> Face recognition (FR) systems have demonstrated outstanding verification performance, suggesting suitability for real-world applications ranging from photo tagging in social media to automated border control (ABC). In an advanced FR system with deep learning-based architecture, however, promoting the recognition efficiency alone is not sufficient, and the system should also withstand potential kinds of attacks designed to target its proficiency. Recent studies show that (deep) FR systems exhibit an intriguing vulnerability to imperceptible or perceptible but natural-looking adversarial input images that drive the model to incorrect output predictions. In this article, we present a comprehensive survey on adversarial attacks against FR systems and elaborate on the competence of new countermeasures against them. Further, we propose a taxonomy of existing attack and defense methods based on different criteria. We compare attack methods on the orientation and attributes and defense approaches on the category. Finally, we explore the challenges and potential research direction.

</details>

<details>

<summary>2021-02-07 17:59:13 - Enabling Fast and Universal Audio Adversarial Attack Using Generative Model</summary>

- *Yi Xie, Zhuohang Li, Cong Shi, Jian Liu, Yingying Chen, Bo Yuan*

- `2004.12261v2` - [abs](http://arxiv.org/abs/2004.12261v2) - [pdf](http://arxiv.org/pdf/2004.12261v2)

> Recently, the vulnerability of DNN-based audio systems to adversarial attacks has obtained the increasing attention. However, the existing audio adversarial attacks allow the adversary to possess the entire user's audio input as well as granting sufficient time budget to generate the adversarial perturbations. These idealized assumptions, however, makes the existing audio adversarial attacks mostly impossible to be launched in a timely fashion in practice (e.g., playing unnoticeable adversarial perturbations along with user's streaming input). To overcome these limitations, in this paper we propose fast audio adversarial perturbation generator (FAPG), which uses generative model to generate adversarial perturbations for the audio input in a single forward pass, thereby drastically improving the perturbation generation speed. Built on the top of FAPG, we further propose universal audio adversarial perturbation generator (UAPG), a scheme crafting universal adversarial perturbation that can be imposed on arbitrary benign audio input to cause misclassification. Extensive experiments show that our proposed FAPG can achieve up to 167X speedup over the state-of-the-art audio adversarial attack methods. Also our proposed UAPG can generate universal adversarial perturbation that achieves much better attack performance than the state-of-the-art solutions.

</details>

<details>

<summary>2021-02-07 19:58:40 - A deep learning approach to identify unhealthy advertisements in street view images</summary>

- *Gregory Palmer, Mark Green, Emma Boyland, Yales Stefano Rios Vasconcelos, Rahul Savani, Alex Singleton*

- `2007.04611v2` - [abs](http://arxiv.org/abs/2007.04611v2) - [pdf](http://arxiv.org/pdf/2007.04611v2)

> While outdoor advertisements are common features within towns and cities, they may reinforce social inequalities in health. Vulnerable populations in deprived areas may have greater exposure to fast food, gambling and alcohol advertisements encouraging their consumption. Understanding who is exposed and evaluating potential policy restrictions requires a substantial manual data collection effort. To address this problem we develop a deep learning workflow to automatically extract and classify unhealthy advertisements from street-level images. We introduce the Liverpool 360 Street View (LIV360SV) dataset for evaluating our workflow. The dataset contains 25,349, 360 degree, street-level images collected via cycling with a GoPro Fusion camera, recorded Jan 14th - 18th 2020. 10,106 advertisements were identified and classified as food (1335), alcohol (217), gambling (149) and other (8405) (e.g., cars and broadband). We find evidence of social inequalities with a larger proportion of food advertisements located within deprived areas and those frequented by students. Our project presents a novel implementation for the incidental classification of street view images for identifying unhealthy advertisements, providing a means through which to identify areas that can benefit from tougher advertisement restriction policies for tackling social inequalities.

</details>

<details>

<summary>2021-02-08 07:43:45 - Adversarial Attacks on Convolutional Neural Networks in Facial Recognition Domain</summary>

- *Yigit Alparslan, Ken Alparslan, Jeremy Keim-Shenk, Shweta Khade, Rachel Greenstadt*

- `2001.11137v3` - [abs](http://arxiv.org/abs/2001.11137v3) - [pdf](http://arxiv.org/pdf/2001.11137v3)

> Numerous recent studies have demonstrated how Deep Neural Network (DNN) classifiers can be fooled by adversarial examples, in which an attacker adds perturbations to an original sample, causing the classifier to misclassify the sample. Adversarial attacks that render DNNs vulnerable in real life represent a serious threat in autonomous vehicles, malware filters, or biometric authentication systems. In this paper, we apply Fast Gradient Sign Method to introduce perturbations to a facial image dataset and then test the output on a different classifier that we trained ourselves, to analyze transferability of this method. Next, we craft a variety of different black-box attack algorithms on a facial image dataset assuming minimal adversarial knowledge, to further assess the robustness of DNNs in facial recognition. While experimenting with different image distortion techniques, we focus on modifying single optimal pixels by a large amount, or modifying all pixels by a smaller amount, or combining these two attack approaches. While our single-pixel attacks achieved about a 15% average decrease in classifier confidence level for the actual class, the all-pixel attacks were more successful and achieved up to an 84% average decrease in confidence, along with an 81.6% misclassification rate, in the case of the attack that we tested with the highest levels of perturbation. Even with these high levels of perturbation, the face images remained identifiable to a human. Understanding how these noised and perturbed images baffle the classification algorithms can yield valuable advances in the training of DNNs against defense-aware adversarial attacks, as well as adaptive noise reduction techniques. We hope our research may help to advance the study of adversarial attacks on DNNs and defensive mechanisms to counteract them, particularly in the facial recognition domain.

</details>

<details>

<summary>2021-02-08 15:52:09 - A Real-time Defense against Website Fingerprinting Attacks</summary>

- *Shawn Shan, Arjun Nitin Bhagoji, Haitao Zheng, Ben Y. Zhao*

- `2102.04291v1` - [abs](http://arxiv.org/abs/2102.04291v1) - [pdf](http://arxiv.org/pdf/2102.04291v1)

> Anonymity systems like Tor are vulnerable to Website Fingerprinting (WF) attacks, where a local passive eavesdropper infers the victim's activity. Current WF attacks based on deep learning classifiers have successfully overcome numerous proposed defenses. While recent defenses leveraging adversarial examples offer promise, these adversarial examples can only be computed after the network session has concluded, thus offer users little protection in practical settings.   We propose Dolos, a system that modifies user network traffic in real time to successfully evade WF attacks. Dolos injects dummy packets into traffic traces by computing input-agnostic adversarial patches that disrupt deep learning classifiers used in WF attacks. Patches are then applied to alter and protect user traffic in real time. Importantly, these patches are parameterized by a user-side secret, ensuring that attackers cannot use adversarial training to defeat Dolos. We experimentally demonstrate that Dolos provides 94+% protection against state-of-the-art WF attacks under a variety of settings. Against prior defenses, Dolos outperforms in terms of higher protection performance and lower information leakage and bandwidth overhead. Finally, we show that Dolos is robust against a variety of adaptive countermeasures to detect or disrupt the defense.

</details>

<details>

<summary>2021-02-09 06:06:13 - Security and Privacy for Artificial Intelligence: Opportunities and Challenges</summary>

- *Ayodeji Oseni, Nour Moustafa, Helge Janicke, Peng Liu, Zahir Tari, Athanasios Vasilakos*

- `2102.04661v1` - [abs](http://arxiv.org/abs/2102.04661v1) - [pdf](http://arxiv.org/pdf/2102.04661v1)

> The increased adoption of Artificial Intelligence (AI) presents an opportunity to solve many socio-economic and environmental challenges; however, this cannot happen without securing AI-enabled technologies. In recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques. This challenge has motivated concerted research efforts into adversarial AI, with the aim of developing robust machine and deep learning models that are resilient to different types of adversarial scenarios. In this paper, we present a holistic cyber security review that demonstrates adversarial attacks against AI applications, including aspects such as adversarial knowledge and capabilities, as well as existing methods for generating adversarial examples and existing cyber defence models. We explain mathematical AI models, especially new variants of reinforcement and federated learning, to demonstrate how attack vectors would exploit vulnerabilities of AI models. We also propose a systematic framework for demonstrating attack techniques against AI applications and reviewed several cyber defences that would protect AI applications against those attacks. We also highlight the importance of understanding the adversarial goals and their capabilities, especially the recent attacks against industry applications, to develop adaptive defences that assess to secure AI applications. Finally, we describe the main challenges and future research directions in the domain of security and privacy of AI technologies.

</details>

<details>

<summary>2021-02-09 14:17:57 - Target Training Does Adversarial Training Without Adversarial Samples</summary>

- *Blerta Lindqvist*

- `2102.04836v1` - [abs](http://arxiv.org/abs/2102.04836v1) - [pdf](http://arxiv.org/pdf/2102.04836v1)

> Neural network classifiers are vulnerable to misclassification of adversarial samples, for which the current best defense trains classifiers with adversarial samples. However, adversarial samples are not optimal for steering attack convergence, based on the minimization at the core of adversarial attacks. The minimization perturbation term can be minimized towards $0$ by replacing adversarial samples in training with duplicated original samples, labeled differently only for training. Using only original samples, Target Training eliminates the need to generate adversarial samples for training against all attacks that minimize perturbation. In low-capacity classifiers and without using adversarial samples, Target Training exceeds both default CIFAR10 accuracy ($84.3$%) and current best defense accuracy (below $25$%) with $84.8$% against CW-L$_2$($\kappa=0$) attack, and $86.6$% against DeepFool. Using adversarial samples against attacks that do not minimize perturbation, Target Training exceeds current best defense ($69.1$%) with $76.4$% against CW-L$_2$($\kappa=40$) in CIFAR10.

</details>

<details>

<summary>2021-02-09 14:52:33 - Effect of Social Media Use on Mental Health during Lockdown in India</summary>

- *Sweta Swarnam*

- `2102.09369v1` - [abs](http://arxiv.org/abs/2102.09369v1) - [pdf](http://arxiv.org/pdf/2102.09369v1)

> This research paper studies about the role of social media use and increase the risk factor of mental health during covid 19 or lockdown. Although few studies have been conducted on the role about the effect of social media use on mental health during lockdown and impact on human reactive nature during lockdown. As a rapidly spreading pandemic, a biomedical disease has serious physical and tremendous mental health implications. An occupational community of internal migrant workers is one of the most vulnerable, but neglected, and is likely to develop psychological ill-effects due to COVID-19's double whammy impact. Mental health is a crucial aspect that needs to be addressed during this lock-down as all modes of communication revolve around the virus. There are many difficulties with the unprecedented changes that have occurred so quickly due to the pandemic and stay-at - home confinement to achieve social distance and mitigate the risk of infection. These include impaired health, well-being, and sleep as a result of daily routine disruption, anxiety, worry, isolation, greater stress on family and work, and excessive screen time. An essential part of our overall health and well-being is mental and emotional health. An important skill is managing emotions and maintaining emotional balance. It helps you face challenges and stress when you manage your emotional health. Lack of skills in emotional regulation may lead to poor mental health and relationship difficulties. It is as important to look after our mental health as it is to look after our physical health. For mental health professionals, the pandemic has also brought many ethical challenges.

</details>

<details>

<summary>2021-02-09 20:21:31 - Adversarial Perturbations Are Not So Weird: Entanglement of Robust and Non-Robust Features in Neural Network Classifiers</summary>

- *Jacob M. Springer, Melanie Mitchell, Garrett T. Kenyon*

- `2102.05110v1` - [abs](http://arxiv.org/abs/2102.05110v1) - [pdf](http://arxiv.org/pdf/2102.05110v1)

> Neural networks trained on visual data are well-known to be vulnerable to often imperceptible adversarial perturbations. The reasons for this vulnerability are still being debated in the literature. Recently Ilyas et al. (2019) showed that this vulnerability arises, in part, because neural network classifiers rely on highly predictive but brittle "non-robust" features. In this paper we extend the work of Ilyas et al. by investigating the nature of the input patterns that give rise to these features. In particular, we hypothesize that in a neural network trained in a standard way, non-robust features respond to small, "non-semantic" patterns that are typically entangled with larger, robust patterns, known to be more human-interpretable, as opposed to solely responding to statistical artifacts in a dataset. Thus, adversarial examples can be formed via minimal perturbations to these small, entangled patterns. In addition, we demonstrate a corollary of our hypothesis: robust classifiers are more effective than standard (non-robust) ones as a source for generating transferable adversarial examples in both the untargeted and targeted settings. The results we present in this paper provide new insight into the nature of the non-robust features responsible for adversarial vulnerability of neural network classifiers.

</details>

<details>

<summary>2021-02-10 00:15:29 - DOVE: A Data-Oblivious Virtual Environment</summary>

- *Hyun Bin Lee, Tushar M. Jois, Christopher W. Fletcher, Carl A. Gunter*

- `2102.05195v1` - [abs](http://arxiv.org/abs/2102.05195v1) - [pdf](http://arxiv.org/pdf/2102.05195v1)

> Users can improve the security of remote communications by using Trusted Execution Environments (TEEs) to protect against direct introspection and tampering of sensitive data. This can even be done with applications coded in high-level languages with complex programming stacks such as R, Python, and Ruby. However, this creates a trade-off between programming convenience versus the risk of attacks using microarchitectural side channels.   In this paper, we argue that it is possible to address this problem for important applications by instrumenting a complex programming environment (like R) to produce a Data-Oblivious Transcript (DOT) that is explicitly designed to support computation that excludes side channels. Such a transcript is then evaluated on a Trusted Execution Environment (TEE) containing the sensitive data using a small trusted computing base called the Data-Oblivious Virtual Environment (DOVE).   To motivate the problem, we demonstrate a number of subtle side-channel vulnerabilities in the R language. We then provide an illustrative design and implementation of DOVE for R, creating the first side-channel resistant R programming stack. We demonstrate that the two-phase architecture provided by DOT generation and DOVE evaluation can provide practical support for complex programming languages with usable performance and high security assurances against side channels.

</details>

<details>

<summary>2021-02-10 09:33:21 - Is Secure Coding Education in the Industry Needed? An Investigation Through a Large Scale Survey</summary>

- *Tiago Espinha Gasiba, Ulrike Lechner, Maria Pinto-Albuquerque, Daniel Mendez*

- `2102.05343v1` - [abs](http://arxiv.org/abs/2102.05343v1) - [pdf](http://arxiv.org/pdf/2102.05343v1)

> The Department of Homeland Security in the United States estimates that 90% of software vulnerabilities can be traced back to defects in design and software coding. The financial impact of these vulnerabilities has been shown to exceed 380 million USD in industrial control systems alone. Since software developers write software, they also introduce these vulnerabilities into the source code. However, secure coding guidelines exist to prevent software developers from writing vulnerable code. This study focuses on the human factor, the software developer, and secure coding, in particular secure coding guidelines. We want to understand the software developers' awareness and compliance to secure coding guidelines and why, if at all, they aren't compliant or aware. We base our results on a large-scale survey on secure coding guidelines, with more than 190 industrial software developers. Our work's main contribution motivates the need to educate industrial software developers on secure coding guidelines, and it gives a list of fifteen actionable items to be used by practitioners in the industry. We also make our raw data openly available for further research.

</details>

<details>

<summary>2021-02-10 13:51:54 - Node-Level Membership Inference Attacks Against Graph Neural Networks</summary>

- *Xinlei He, Rui Wen, Yixin Wu, Michael Backes, Yun Shen, Yang Zhang*

- `2102.05429v1` - [abs](http://arxiv.org/abs/2102.05429v1) - [pdf](http://arxiv.org/pdf/2102.05429v1)

> Many real-world data comes in the form of graphs, such as social networks and protein structure. To fully utilize the information contained in graph data, a new family of machine learning (ML) models, namely graph neural networks (GNNs), has been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on data from the Euclidean space, like images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied.   In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose three node-level membership inference attacks based on an adversary's background knowledge. Our evaluation on three GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that graph density and feature similarity have a major impact on the attack's success. We further investigate two defense mechanisms and the empirical results indicate that these defenses can reduce the attack performance but with moderate utility loss.

</details>

<details>

<summary>2021-02-10 15:42:03 - Enterprise-Driven Open Source Software: A Case Study on Security Automation</summary>

- *Florian Angermeir, Markus Voggenreiter, Fabiola Moyón, Daniel Mendez*

- `2102.05500v1` - [abs](http://arxiv.org/abs/2102.05500v1) - [pdf](http://arxiv.org/pdf/2102.05500v1)

> Agile and DevOps are widely adopted by the industry. Hence, integrating security activities with industrial practices, such as continuous integration (CI) pipelines, is necessary to detect security flaws and adhere to regulators' demands early. In this paper, we analyze automated security activities in CI pipelines of enterprise-driven open source software (OSS). This shall allow us, in the long-run, to better understand the extent to which security activities are (or should be) part of automated pipelines. In particular, we mine publicly available OSS repositories and survey a sample of project maintainers to better understand the role that security activities and their related tools play in their CI pipelines. To increase transparency and allow other researchers to replicate our study (and to take different perspectives), we further disclose our research artefacts. Our results indicate that security activities in enterprise-driven OSS projects are scarce and protection coverage is rather low. Only 6.83% of the analyzed 8,243 projects apply security automation in their CI pipelines, even though maintainers consider security to be rather important. This alerts industry to keep the focus on vulnerabilities of 3rd Party software and it opens space for other improvements of practice which we outline in this manuscript.

</details>

<details>

<summary>2021-02-10 16:48:32 - Meta Federated Learning</summary>

- *Omid Aramoon, Pin-Yu Chen, Gang Qu, Yuan Tian*

- `2102.05561v1` - [abs](http://arxiv.org/abs/2102.05561v1) - [pdf](http://arxiv.org/pdf/2102.05561v1)

> Due to its distributed methodology alongside its privacy-preserving features, Federated Learning (FL) is vulnerable to training time adversarial attacks. In this study, our focus is on backdoor attacks in which the adversary's goal is to cause targeted misclassifications for inputs embedded with an adversarial trigger while maintaining an acceptable performance on the main learning task at hand. Contemporary defenses against backdoor attacks in federated learning require direct access to each individual client's update which is not feasible in recent FL settings where Secure Aggregation is deployed. In this study, we seek to answer the following question, Is it possible to defend against backdoor attacks when secure aggregation is in place?, a question that has not been addressed by prior arts. To this end, we propose Meta Federated Learning (Meta-FL), a novel variant of federated learning which not only is compatible with secure aggregation protocol but also facilitates defense against backdoor attacks. We perform a systematic evaluation of Meta-FL on two classification datasets: SVHN and GTSRB. The results show that Meta-FL not only achieves better utility than classic FL, but also enhances the performance of contemporary defenses in terms of robustness against adversarial attacks.

</details>

<details>

<summary>2021-02-10 16:59:20 - Linking Threat Tactics, Techniques, and Patterns with Defensive Weaknesses, Vulnerabilities and Affected Platform Configurations for Cyber Hunting</summary>

- *Erik Hemberg, Jonathan Kelly, Michal Shlapentokh-Rothman, Bryn Reinstadler, Katherine Xu, Nick Rutar, Una-May O'Reilly*

- `2010.00533v2` - [abs](http://arxiv.org/abs/2010.00533v2) - [pdf](http://arxiv.org/pdf/2010.00533v2)

> Many public sources of cyber threat and vulnerability information exist to help defend cyber systems. This paper links MITRE's ATT&CK MATRIX of Tactics and Techniques, NIST's Common Weakness Enumerations (CWE), Common Vulnerabilities and Exposures (CVE), and Common Attack Pattern Enumeration and Classification list (CAPEC), to gain further insight from alerts, threats and vulnerabilities. We preserve all entries and relations of the sources, while enabling bi-directional, relational path tracing within an aggregate data graph called BRON. In one example, we use BRON to enhance the information derived from a list of the top 10 most frequently exploited CVEs. We identify attack patterns, tactics, and techniques that exploit these CVEs and also uncover a disparity in how much linked information exists for each of these CVEs. This prompts us to further inventory BRON's collection of sources to provide a view of the extent and range of the coverage and blind spots of public data sources.

</details>

<details>

<summary>2021-02-10 18:27:13 - An Ensemble Deep Convolutional Neural Network Model for Electricity Theft Detection in Smart Grids</summary>

- *Hossein Mohammadi Rouzbahani, Hadis Karimipour, Lei Lei*

- `2102.06039v1` - [abs](http://arxiv.org/abs/2102.06039v1) - [pdf](http://arxiv.org/pdf/2102.06039v1)

> Smart grids extremely rely on Information and Communications Technology (ICT) and smart meters to control and manage numerous parameters of the network. However, using these infrastructures make smart grids more vulnerable to cyber threats especially electricity theft. Electricity Theft Detection (EDT) algorithms are typically used for such purpose since this Non-Technical Loss (NTL) may lead to significant challenges in the power system. In this paper, an Ensemble Deep Convolutional Neural Network (EDCNN) algorithm for ETD in smart grids has been proposed. As the first layer of the model, a random under bagging technique is applied to deal with the imbalance data, and then Deep Convolutional Neural Networks (DCNN) are utilized on each subset. Finally, a voting system is embedded, in the last part. The evaluation results based on the Area Under Curve (AUC), precision, recall, f1-score, and accuracy verify the efficiency of the proposed method compared to the existing method in the literature.

</details>

<details>

<summary>2021-02-11 06:55:40 - Adversarial Poisoning Attacks and Defense for General Multi-Class Models Based On Synthetic Reduced Nearest Neighbors</summary>

- *Pooya Tavallali, Vahid Behzadan, Peyman Tavallali, Mukesh Singhal*

- `2102.05867v1` - [abs](http://arxiv.org/abs/2102.05867v1) - [pdf](http://arxiv.org/pdf/2102.05867v1)

> State-of-the-art machine learning models are vulnerable to data poisoning attacks whose purpose is to undermine the integrity of the model. However, the current literature on data poisoning attacks is mainly focused on ad hoc techniques that are only applicable to specific machine learning models. Additionally, the existing data poisoning attacks in the literature are limited to either binary classifiers or to gradient-based algorithms. To address these limitations, this paper first proposes a novel model-free label-flipping attack based on the multi-modality of the data, in which the adversary targets the clusters of classes while constrained by a label-flipping budget. The complexity of our proposed attack algorithm is linear in time over the size of the dataset. Also, the proposed attack can increase the error up to two times for the same attack budget. Second, a novel defense technique based on the Synthetic Reduced Nearest Neighbor (SRNN) model is proposed. The defense technique can detect and exclude flipped samples on the fly during the training procedure. Through extensive experimental analysis, we demonstrate that (i) the proposed attack technique can deteriorate the accuracy of several models drastically, and (ii) under the proposed attack, the proposed defense technique significantly outperforms other conventional machine learning models in recovering the accuracy of the targeted model.

</details>

<details>

<summary>2021-02-11 18:48:26 - CENTRIS: A Precise and Scalable Approach for Identifying Modified Open-Source Software Reuse</summary>

- *Seunghoon Woo, Sunghan Park, Seulbae Kim, Heejo Lee, Hakjoo Oh*

- `2102.06182v1` - [abs](http://arxiv.org/abs/2102.06182v1) - [pdf](http://arxiv.org/pdf/2102.06182v1)

> Open-source software (OSS) is widely reused as it provides convenience and efficiency in software development. Despite evident benefits, unmanaged OSS components can introduce threats, such as vulnerability propagation and license violation. Unfortunately, however, identifying reused OSS components is a challenge as the reused OSS is predominantly modified and nested. In this paper, we propose CENTRIS, a precise and scalable approach for identifying modified OSS reuse. By segmenting an OSS code base and detecting the reuse of a unique part of the OSS only, CENTRIS is capable of precisely identifying modified OSS reuse in the presence of nested OSS components. For scalability, CENTRIS eliminates redundant code comparisons and accelerates the search using hash functions. When we applied CENTRIS on 10,241 widely-employed GitHub projects, comprising 229,326 versions and 80 billion lines of code, we observed that modified OSS reuse is a norm in software development, occurring 20 times more frequently than exact reuse. Nonetheless, CENTRIS identified reused OSS components with 91% precision and 94% recall in less than a minute per application on average, whereas a recent clone detection technique, which does not take into account modified and nested OSS reuse, hardly reached 10% precision and 40% recall.

</details>

<details>

<summary>2021-02-11 20:22:33 - Why Don't Developers Detect Improper Input Validation?'; DROP TABLE Papers; --</summary>

- *Larissa Braz, Enrico Fregnan, Gül Çalikli, Alberto Bacchelli*

- `2102.06251v1` - [abs](http://arxiv.org/abs/2102.06251v1) - [pdf](http://arxiv.org/pdf/2102.06251v1)

> Improper Input Validation (IIV) is a software vulnerability that occurs when a system does not safely handle input data. Even though IIV is easy to detect and fix, it still commonly happens in practice. In this paper, we study to what extent developers can detect IIV and investigate underlying reasons. This knowledge is essential to better understand how to support developers in creating secure software systems. We conduct an online experiment with 146 participants, of which 105 report at least three years of professional software development experience. Our results show that the existence of a visible attack scenario facilitates the detection of IIV vulnerabilities and that a significant portion of developers who did not find the vulnerability initially could identify it when warned about its existence. Yet, a total of 60 participants could not detect the vulnerability even after the warning. Other factors, such as the frequency with which the participants perform code reviews, influence the detection of IIV. Data and materials: https://doi.org/10.5281/zenodo.3996696

</details>

<details>

<summary>2021-02-11 21:29:25 - Ethics as a service: a pragmatic operationalisation of AI Ethics</summary>

- *Jessica Morley, Anat Elhalal, Francesca Garcia, Libby Kinsey, Jakob Mokander, Luciano Floridi*

- `2102.09364v1` - [abs](http://arxiv.org/abs/2102.09364v1) - [pdf](http://arxiv.org/pdf/2102.09364v1)

> As the range of potential uses for Artificial Intelligence (AI), in particular machine learning (ML), has increased, so has awareness of the associated ethical issues. This increased awareness has led to the realisation that existing legislation and regulation provides insufficient protection to individuals, groups, society, and the environment from AI harms. In response to this realisation, there has been a proliferation of principle-based ethics codes, guidelines and frameworks. However, it has become increasingly clear that a significant gap exists between the theory of AI ethics principles and the practical design of AI systems. In previous work, we analysed whether it is possible to close this gap between the what and the how of AI ethics through the use of tools and methods designed to help AI developers, engineers, and designers translate principles into practice. We concluded that this method of closure is currently ineffective as almost all existing translational tools and methods are either too flexible (and thus vulnerable to ethics washing) or too strict (unresponsive to context). This raised the question: if, even with technical guidance, AI ethics is challenging to embed in the process of algorithmic design, is the entire pro-ethical design endeavour rendered futile? And, if no, then how can AI ethics be made useful for AI practitioners? This is the question we seek to address here by exploring why principles and technical translational tools are still needed even if they are limited, and how these limitations can be potentially overcome by providing theoretical grounding of a concept that has been termed Ethics as a Service.

</details>

<details>

<summary>2021-02-12 13:31:27 - A Non-Intrusive Machine Learning Solution for Malware Detection and Data Theft Classification in Smartphones</summary>

- *Sai Vishwanath Venkatesh, Prasanna D. Kumaran, Joish J Bosco, Pravin R. Kumaar, Vineeth Vijayaraghavan*

- `2102.06511v1` - [abs](http://arxiv.org/abs/2102.06511v1) - [pdf](http://arxiv.org/pdf/2102.06511v1)

> Smartphones contain information that is more sensitive and personal than those found on computers and laptops. With an increase in the versatility of smartphone functionality, more data has become vulnerable and exposed to attackers. Successful mobile malware attacks could steal a user's location, photos, or even banking information. Due to a lack of post-attack strategies firms also risk going out of business due to data theft. Thus, there is a need besides just detecting malware intrusion in smartphones but to also identify the data that has been stolen to assess, aid in recovery and prevent future attacks. In this paper, we propose an accessible, non-intrusive machine learning solution to not only detect malware intrusion but also identify the type of data stolen for any app under supervision. We do this with Android usage data obtained by utilising publicly available data collection framework- SherLock. We test the performance of our architecture for multiple users on real-world data collected using the same framework. Our architecture exhibits less than 9% inaccuracy in detecting malware and can classify with 83% certainty on the type of data that is being stolen.

</details>

<details>

<summary>2021-02-13 09:06:19 - GPSPiChain-Blockchain based Self-Contained Family Security System in Smart Home</summary>

- *Ali Raza, Lachlan Hardy, Erin Roehrer, Soonja Yeom, Byeong ho Kang*

- `2102.06884v1` - [abs](http://arxiv.org/abs/2102.06884v1) - [pdf](http://arxiv.org/pdf/2102.06884v1)

> With advancements in technology, personal computing devices are better adapted for and further integrated into people's lives and homes. The integration of technology into society also results in an increasing desire to control who and what has access to sensitive information, especially for vulnerable people including children and the elderly. With blockchain coming in to the picture as a technology that can revolutionise the world, it is now possible to have an immutable audit trail of locational data over time. By controlling the process through inexpensive equipment in the home, it is possible to control whom has access to such personal data. This paper presents a blockchain based family security system for tracking the location of consenting family members' smart phones. The locations of the family members' smart phones are logged and stored in a private blockchain which can be accessed through a node installed in the family home on a computer. The data for the whereabouts of family members stays within the family unit and does not go to any third party. The system is implemented in a small scale (one miner and two other nodes) and the technical feasibility is discussed along with the limitations of the system. Further research will cover the integration of the system into a smart home environment, and ethical implementations of tracking, especially of vulnerable people, using the immutability of blockchain.

</details>

<details>

<summary>2021-02-13 11:55:20 - Why Security Defects Go Unnoticed during Code Reviews? A Case-Control Study of the Chromium OS Project</summary>

- *Rajshakhar Paul, Asif Kamal Turzo, Amiangshu Bosu*

- `2102.06909v1` - [abs](http://arxiv.org/abs/2102.06909v1) - [pdf](http://arxiv.org/pdf/2102.06909v1)

> Peer code review has been found to be effective in identifying security vulnerabilities. However, despite practicing mandatory code reviews, many Open Source Software (OSS) projects still encounter a large number of post-release security vulnerabilities, as some security defects escape those. Therefore, a project manager may wonder if there was any weakness or inconsistency during a code review that missed a security vulnerability. Answers to this question may help a manager pinpointing areas of concern and taking measures to improve the effectiveness of his/her project's code reviews in identifying security defects. Therefore, this study aims to identify the factors that differentiate code reviews that successfully identified security defects from those that missed such defects. With this goal, we conduct a case-control study of Chromium OS project. Using multi-stage semi-automated approaches, we build a dataset of 516 code reviews that successfully identified security defects and 374 code reviews where security defects escaped. The results of our empirical study suggest that the are significant differences between the categories of security defects that are identified and that are missed during code reviews. A logistic regression model fitted on our dataset achieved an AUC score of 0.91 and has identified nine code review attributes that influence identifications of security defects. While time to complete a review, the number of mutual reviews between two developers, and if the review is for a bug fix have positive impacts on vulnerability identification, opposite effects are observed from the number of directories under review, the number of total reviews by a developer, and the total number of prior commits for the file under review.

</details>

<details>

<summary>2021-02-13 18:40:53 - Exploiting epistemic uncertainty of the deep learning models to generate adversarial samples</summary>

- *Omer Faruk Tuna, Ferhat Ozgur Catak, M. Taner Eskil*

- `2102.04150v2` - [abs](http://arxiv.org/abs/2102.04150v2) - [pdf](http://arxiv.org/pdf/2102.04150v2)

> Deep neural network architectures are considered to be robust to random perturbations. Nevertheless, it was shown that they could be severely vulnerable to slight but carefully crafted perturbations of the input, termed as adversarial samples. In recent years, numerous studies have been conducted in this new area called "Adversarial Machine Learning" to devise new adversarial attacks and to defend against these attacks with more robust DNN architectures. However, almost all the research work so far has been concentrated on utilising model loss function to craft adversarial examples or create robust models. This study explores the usage of quantified epistemic uncertainty obtained from Monte-Carlo Dropout Sampling for adversarial attack purposes by which we perturb the input to the areas where the model has not seen before. We proposed new attack ideas based on the epistemic uncertainty of the model. Our results show that our proposed hybrid attack approach increases the attack success rates from 82.59% to 85.40%, 82.86% to 89.92% and 88.06% to 90.03% on MNIST Digit, MNIST Fashion and CIFAR-10 datasets, respectively.

</details>

<details>

<summary>2021-02-13 20:00:23 - Data-Driven Vulnerability Detection and Repair in Java Code</summary>

- *Ying Zhang, Mahir Kabir, Ya Xiao, Danfeng, Yao, Na Meng*

- `2102.06994v1` - [abs](http://arxiv.org/abs/2102.06994v1) - [pdf](http://arxiv.org/pdf/2102.06994v1)

> Java platform provides various APIs to facilitate secure coding. However, correctly using security APIs is usually challenging for developers who lack cybersecurity training. Prior work shows that many developers misuse security APIs; such misuses can introduce vulnerabilities into software, void security protections, and present security exploits to hackers. To eliminate such API-related vulnerabilities, this paper presents SEADER -- our new approach that detects and repairs security API misuses. Given an exemplar, insecure code snippet, and its secure counterpart, SEADER compares the snippets and conducts data dependence analysis to infer the security API misuse templates and corresponding fixing operations. Based on the inferred information, given a program, SEADER performs inter-procedural static analysis to search for any security API misuse and to propose customized fixing suggestions for those vulnerabilities.   To evaluate SEADER, we applied it to 25 <insecure, secure> code pairs, and SEADER successfully inferred 18 unique API misuse templates and related fixes. With these vulnerability repair patterns, we further applied SEADER to 10 open-source projects that contain in total 32 known vulnerabilities. Our experiment shows that SEADER detected vulnerabilities with 100% precision, 84% recall, and 91% accuracy. Additionally, we applied SEADER to 100 Apache open-source projects and detected 988 vulnerabilities; SEADER always customized repair suggestions correctly. Based on SEADER's outputs, we filed 60 pull requests. Up till now, developers of 18 projects have offered positive feedbacks on SEADER's suggestions. Our results indicate that SEADER can effectively help developers detect and fix security API misuses. Whereas prior work either detects API misuses or suggests simple fixes, SEADER is the first tool to do both for nontrivial vulnerability repairs.

</details>

<details>

<summary>2021-02-14 21:09:49 - Reinforcement Learning for IoT Security: A Comprehensive Survey</summary>

- *Aashma Uprety, Danda B. Rawat*

- `2102.07247v1` - [abs](http://arxiv.org/abs/2102.07247v1) - [pdf](http://arxiv.org/pdf/2102.07247v1)

> The number of connected smart devices has been increasing exponentially for different Internet-of-Things (IoT) applications. Security has been a long run challenge in the IoT systems which has many attack vectors, security flaws and vulnerabilities. Securing billions of B connected devices in IoT is a must task to realize the full potential of IoT applications. Recently, researchers have proposed many security solutions for IoT. Machine learning has been proposed as one of the emerging solutions for IoT security and Reinforcement learning is gaining more popularity for securing IoT systems. Reinforcement learning, unlike other machine learning techniques, can learn the environment by having minimum information about the parameters to be learned. It solves the optimization problem by interacting with the environment adapting the parameters on the fly. In this paper, we present an comprehensive survey of different types of cyber-attacks against different IoT systems and then we present reinforcement learning and deep reinforcement learning based security solutions to combat those different types of attacks in different IoT systems. Furthermore, we present the Reinforcement learning for securing CPS systems (i.e., IoT with feedback and control) such as smart grid and smart transportation system. The recent important attacks and countermeasures using reinforcement learning B in IoT are also summarized in the form of tables. With this paper, readers can have a more thorough understanding of IoT security attacks and countermeasures using Reinforcement Learning, as well as research trends in this area.

</details>

<details>

<summary>2021-02-14 23:18:12 - Exploring Adversarial Robustness of Deep Metric Learning</summary>

- *Thomas Kobber Panum, Zi Wang, Pengyu Kan, Earlence Fernandes, Somesh Jha*

- `2102.07265v1` - [abs](http://arxiv.org/abs/2102.07265v1) - [pdf](http://arxiv.org/pdf/2102.07265v1)

> Deep Metric Learning (DML), a widely-used technique, involves learning a distance metric between pairs of samples. DML uses deep neural architectures to learn semantic embeddings of the input, where the distance between similar examples is small while dissimilar ones are far apart. Although the underlying neural networks produce good accuracy on naturally occurring samples, they are vulnerable to adversarially-perturbed samples that reduce performance. We take a first step towards training robust DML models and tackle the primary challenge of the metric losses being dependent on the samples in a mini-batch, unlike standard losses that only depend on the specific input-output pair. We analyze this dependence effect and contribute a robust optimization formulation. Using experiments on three commonly-used DML datasets, we demonstrate 5-76 fold increases in adversarial accuracy, and outperform an existing DML model that sought out to be robust.

</details>

<details>

<summary>2021-02-15 05:10:33 - A Targeted Attack on Black-Box Neural Machine Translation with Parallel Data Poisoning</summary>

- *Chang Xu, Jun Wang, Yuqing Tang, Francisco Guzman, Benjamin I. P. Rubinstein, Trevor Cohn*

- `2011.00675v2` - [abs](http://arxiv.org/abs/2011.00675v2) - [pdf](http://arxiv.org/pdf/2011.00675v2)

> As modern neural machine translation (NMT) systems have been widely deployed, their security vulnerabilities require close scrutiny. Most recently, NMT systems have been found vulnerable to targeted attacks which cause them to produce specific, unsolicited, and even harmful translations. These attacks are usually exploited in a white-box setting, where adversarial inputs causing targeted translations are discovered for a known target system. However, this approach is less viable when the target system is black-box and unknown to the adversary (e.g., secured commercial systems). In this paper, we show that targeted attacks on black-box NMT systems are feasible, based on poisoning a small fraction of their parallel training data. We show that this attack can be realised practically via targeted corruption of web documents crawled to form the system's training data. We then analyse the effectiveness of the targeted poisoning in two common NMT training scenarios: the from-scratch training and the pre-train & fine-tune paradigm. Our results are alarming: even on the state-of-the-art systems trained with massive parallel data (tens of millions), the attacks are still successful (over 50% success rate) under surprisingly low poisoning budgets (e.g., 0.006%). Lastly, we discuss potential defences to counter such attacks.

</details>

<details>

<summary>2021-02-15 09:49:10 - Dynamic Vulnerability Detection on Smart Contracts Using Machine Learning</summary>

- *Mojtaba Eshghie, Cyrille Artho, Dilian Gurov*

- `2102.07420v1` - [abs](http://arxiv.org/abs/2102.07420v1) - [pdf](http://arxiv.org/pdf/2102.07420v1)

> In this work we propose Dynamit, a monitoring framework to detect reentrancy vulnerabilities in Ethereum smart contracts. The novelty of our framework is that it relies only on transaction metadata and balance data from the blockchain system; our approach requires no domain knowledge, code instrumentation, or special execution environment. Dynamit extracts features from transaction data and uses a machine learning model to classify transactions as benign or harmful. Therefore, not only can we find the contracts that are vulnerable to reentrancy attacks, but we also get an execution trace that reproduces the attack.

</details>

<details>

<summary>2021-02-15 19:00:09 - Universal Adversarial Examples and Perturbations for Quantum Classifiers</summary>

- *Weiyuan Gong, Dong-Ling Deng*

- `2102.07788v1` - [abs](http://arxiv.org/abs/2102.07788v1) - [pdf](http://arxiv.org/pdf/2102.07788v1)

> Quantum machine learning explores the interplay between machine learning and quantum physics, which may lead to unprecedented perspectives for both fields. In fact, recent works have shown strong evidences that quantum computers could outperform classical computers in solving certain notable machine learning tasks. Yet, quantum learning systems may also suffer from the vulnerability problem: adding a tiny carefully-crafted perturbation to the legitimate input data would cause the systems to make incorrect predictions at a notably high confidence level. In this paper, we study the universality of adversarial examples and perturbations for quantum classifiers. Through concrete examples involving classifications of real-life images and quantum phases of matter, we show that there exist universal adversarial examples that can fool a set of different quantum classifiers. We prove that for a set of $k$ classifiers with each receiving input data of $n$ qubits, an $O(\frac{\ln k} {2^n})$ increase of the perturbation strength is enough to ensure a moderate universal adversarial risk. In addition, for a given quantum classifier we show that there exist universal adversarial perturbations, which can be added to different legitimate samples and make them to be adversarial examples for the classifier. Our results reveal the universality perspective of adversarial attacks for quantum machine learning systems, which would be crucial for practical applications of both near-term and future quantum technologies in solving machine learning problems.

</details>

<details>

<summary>2021-02-16 07:46:53 - D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis</summary>

- *Yunhui Zheng, Saurabh Pujar, Burn Lewis, Luca Buratti, Edward Epstein, Bo Yang, Jim Laredo, Alessandro Morari, Zhong Su*

- `2102.07995v1` - [abs](http://arxiv.org/abs/2102.07995v1) - [pdf](http://arxiv.org/pdf/2102.07995v1)

> Static analysis tools are widely used for vulnerability detection as they understand programs with complex behavior and millions of lines of code. Despite their popularity, static analysis tools are known to generate an excess of false positives. The recent ability of Machine Learning models to understand programming languages opens new possibilities when applied to static analysis. However, existing datasets to train models for vulnerability identification suffer from multiple limitations such as limited bug context, limited size, and synthetic and unrealistic source code. We propose D2A, a differential analysis based approach to label issues reported by static analysis tools. The D2A dataset is built by analyzing version pairs from multiple open source projects. From each project, we select bug fixing commits and we run static analysis on the versions before and after such commits. If some issues detected in a before-commit version disappear in the corresponding after-commit version, they are very likely to be real bugs that got fixed by the commit. We use D2A to generate a large labeled dataset to train models for vulnerability identification. We show that the dataset can be used to build a classifier to identify possible false alarms among the issues reported by static analysis, hence helping developers prioritize and investigate potential true positives first.

</details>

<details>

<summary>2021-02-16 14:49:34 - Automated Identification of Vulnerable Devices in Networks using Traffic Data and Deep Learning</summary>

- *Jakob Greis, Artem Yushchenko, Daniel Vogel, Michael Meier, Volker Steinhage*

- `2102.08199v1` - [abs](http://arxiv.org/abs/2102.08199v1) - [pdf](http://arxiv.org/pdf/2102.08199v1)

> Many IoT devices are vulnerable to attacks due to flawed security designs and lacking mechanisms for firmware updates or patches to eliminate the security vulnerabilities. Device-type identification combined with data from vulnerability databases can pinpoint vulnerable IoT devices in a network and can be used to constrain the communications of vulnerable devices for preventing damage. In this contribution, we present and evaluate two deep learning approaches to the reliable IoT device-type identification, namely a recurrent and a convolutional network architecture. Both deep learning approaches show accuracies of 97% and 98%, respectively, and thereby outperform an up-to-date IoT device-type identification approach using hand-crafted fingerprint features obtaining an accuracy of 82%. The runtime performance for the IoT identification of both deep learning approaches outperforms the hand-crafted approach by three magnitudes. Finally, importance metrics explain the results of both deep learning approaches in terms of the utilization of the analyzed traffic data flow.

</details>

<details>

<summary>2021-02-16 17:18:54 - A Federated Data-Driven Evolutionary Algorithm</summary>

- *Jinjin Xu, Yaochu Jin, Wenli Du, Sai Gu*

- `2102.08288v1` - [abs](http://arxiv.org/abs/2102.08288v1) - [pdf](http://arxiv.org/pdf/2102.08288v1)

> Data-driven evolutionary optimization has witnessed great success in solving complex real-world optimization problems. However, existing data-driven optimization algorithms require that all data are centrally stored, which is not always practical and may be vulnerable to privacy leakage and security threats if the data must be collected from different devices. To address the above issue, this paper proposes a federated data-driven evolutionary optimization framework that is able to perform data driven optimization when the data is distributed on multiple devices. On the basis of federated learning, a sorted model aggregation method is developed for aggregating local surrogates based on radial-basis-function networks. In addition, a federated surrogate management strategy is suggested by designing an acquisition function that takes into account the information of both the global and local surrogate models. Empirical studies on a set of widely used benchmark functions in the presence of various data distributions demonstrate the effectiveness of the proposed framework.

</details>

<details>

<summary>2021-02-16 18:45:01 - Adversarial Targeted Forgetting in Regularization and Generative Based Continual Learning Models</summary>

- *Muhammad Umer, Robi Polikar*

- `2102.08355v1` - [abs](http://arxiv.org/abs/2102.08355v1) - [pdf](http://arxiv.org/pdf/2102.08355v1)

> Continual (or "incremental") learning approaches are employed when additional knowledge or tasks need to be learned from subsequent batches or from streaming data. However these approaches are typically adversary agnostic, i.e., they do not consider the possibility of a malicious attack. In our prior work, we explored the vulnerabilities of Elastic Weight Consolidation (EWC) to the perceptible misinformation. We now explore the vulnerabilities of other regularization-based as well as generative replay-based continual learning algorithms, and also extend the attack to imperceptible misinformation. We show that an intelligent adversary can take advantage of a continual learning algorithm's capabilities of retaining existing knowledge over time, and force it to learn and retain deliberately introduced misinformation. To demonstrate this vulnerability, we inject backdoor attack samples into the training data. These attack samples constitute the misinformation, allowing the attacker to capture control of the model at test time. We evaluate the extent of this vulnerability on both rotated and split benchmark variants of the MNIST dataset under two important domain and class incremental learning scenarios. We show that the adversary can create a "false memory" about any task by inserting carefully-designed backdoor samples to the test instances of that task thereby controlling the amount of forgetting of any task of its choosing. Perhaps most importantly, we show this vulnerability to be very acute and damaging: the model memory can be easily compromised with the addition of backdoor samples into as little as 1\% of the training data, even when the misinformation is imperceptible to human eye.

</details>

<details>

<summary>2021-02-16 23:10:46 - Efficacy of Satisfiability Based Attacks in the Presence of Circuit Reverse Engineering Errors</summary>

- *Qinhan Tan, Seetal Potluri, Aydin Aysu*

- `2005.13048v2` - [abs](http://arxiv.org/abs/2005.13048v2) - [pdf](http://arxiv.org/pdf/2005.13048v2)

> Intellectual Property (IP) theft is a serious concern for the integrated circuit (IC) industry. To address this concern, logic locking countermeasure transforms a logic circuit to a different one to obfuscate its inner details. The transformation caused by obfuscation is reversed only upon application of the programmed secret key, thus preserving the circuit's original function. This technique is known to be vulnerable to Satisfiability (SAT)-based attacks. But in order to succeed, SAT-based attacks implicitly assume a perfectly reverse-engineered circuit, which is difficult to achieve in practice due to reverse engineering (RE) errors caused by automated circuit extraction. In this paper, we analyze the effects of random circuit RE-errors on the success of SAT-based attacks. Empirical evaluation on ISCAS, MCNC benchmarks as well as a fully-fledged RISC-V CPU reveals that the attack success degrades exponentially with increase in the number of random RE-errors. Therefore, the adversaries either have to equip RE-tools with near perfection or propose better SAT-based attacks that can work with RE-imperfections.

</details>

<details>

<summary>2021-02-17 02:11:57 - So you want to be a Super Researcher?</summary>

- *Sanjay Rathee, Sheah Lin Lee*

- `2103.03351v1` - [abs](http://arxiv.org/abs/2103.03351v1) - [pdf](http://arxiv.org/pdf/2103.03351v1)

> Publishing original scientific research is inherent to the work of a researcher. However, the pressure to maintain productivity and scientific impact can lead to research group publishing excessively, negatively affecting the mental health of a researcher. Ph.D. students and early career researchers are particularly susceptible to this pressure due to the inherent vulnerability of their positions. At present, there are no resources that concisely summarise the publication culture of a research group to help the researcher make an informed decision before joining. In this article, we present the 'Super Researcher' app, an R Shiny application(app) with a user-friendly interface. Using text-mining methodology to extract publicly available author data from Scopus, this pilot app has four fundamental functions to provide snapshot information that will help researchers grasp the publication culture of a research group within minutes. The 'Super Researcher' app provides information on: 1) institution data, 2) author's publication, 3) co-author network plots and 4) publication journals. The 'Super Researcher' app is built on R shiny which provides an interactive interface to users. This app utilizes the Big Data framework Apache Spark to mine relevant information from a huge author information database. The author's information is stored and manipulated using both SQL(SQLite) and NoSQL(HBase) databases. Hbase is used for local data storage and manipulation while SQLite feeds data to the R Shiny interface. In this paper, we introduce these functionalities and illustrate how this information can help guide a researcher to select a new Principle Investigator (PI) with better compatibility in terms of publication attitude using a case study.   Available: https://researchmind.co.uk/super-researcher/

</details>

<details>

<summary>2021-02-17 14:00:42 - Discriminative Multi-level Reconstruction under Compact Latent Space for One-Class Novelty Detection</summary>

- *Jaewoo Park, Yoon Gyo Jung, Andrew Beng Jin Teoh*

- `2003.01665v3` - [abs](http://arxiv.org/abs/2003.01665v3) - [pdf](http://arxiv.org/pdf/2003.01665v3)

> In one-class novelty detection, a model learns solely on the in-class data to single out out-class instances. Autoencoder (AE) variants aim to compactly model the in-class data to reconstruct it exclusively, thus differentiating the in-class from out-class by the reconstruction error. However, compact modeling in an improper way might collapse the latent representations of the in-class data and thus their reconstruction, which would lead to performance deterioration. Moreover, to properly measure the reconstruction error of high-dimensional data, a metric is required that captures high-level semantics of the data. To this end, we propose Discriminative Compact AE (DCAE) that learns both compact and collapse-free latent representations of the in-class data, thereby reconstructing them both finely and exclusively. In DCAE, (a) we force a compact latent space to bijectively represent the in-class data by reconstructing them through internal discriminative layers of generative adversarial nets. (b) Based on the deep encoder's vulnerability to open set risk, out-class instances are encoded into the same compact latent space and reconstructed poorly without sacrificing the quality of in-class data reconstruction. (c) In inference, the reconstruction error is measured by a novel metric that computes the dissimilarity between a query and its reconstruction based on the class semantics captured by the internal discriminator. Extensive experiments on public image datasets validate the effectiveness of our proposed model on both novelty and adversarial example detection, delivering state-of-the-art performance.

</details>

<details>

<summary>2021-02-18 15:43:31 - Functionality-preserving Black-box Optimization of Adversarial Windows Malware</summary>

- *Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, Alessandro Armando*

- `2003.13526v4` - [abs](http://arxiv.org/abs/2003.13526v4) - [pdf](http://arxiv.org/pdf/2003.13526v4)

> Windows malware detectors based on machine learning are vulnerable to adversarial examples, even if the attacker is only given black-box query access to the model. The main drawback of these attacks is that: (i) they are query-inefficient, as they rely on iteratively applying random transformations to the input malware; and (ii) they may also require executing the adversarial malware in a sandbox at each iteration of the optimization process, to ensure that its intrusive functionality is preserved. In this paper, we overcome these issues by presenting a novel family of black-box attacks that are both query-efficient and functionality-preserving, as they rely on the injection of benign content - which will never be executed - either at the end of the malicious file, or within some newly-created sections. Our attacks are formalized as a constrained minimization problem which also enables optimizing the trade-off between the probability of evading detection and the size of the injected payload. We empirically investigate this trade-off on two popular static Windows malware detectors, and show that our black-box attacks can bypass them with only few queries and small payloads, even when they only return the predicted labels. We also evaluate whether our attacks transfer to other commercial antivirus solutions, and surprisingly find that they can evade, on average, more than 12 commercial antivirus engines. We conclude by discussing the limitations of our approach, and its possible future extensions to target malware classifiers based on dynamic analysis.

</details>

<details>

<summary>2021-02-18 17:10:47 - Assessing the Use of Insecure ICS Protocols via IXP Network Traffic Analysis</summary>

- *Giovanni Barbieri, Mauro Conti, Nils Ole Tippenhauer, Federico Turrin*

- `2007.01114v2` - [abs](http://arxiv.org/abs/2007.01114v2) - [pdf](http://arxiv.org/pdf/2007.01114v2)

> Modern Industrial Control Systems (ICSs) allow remote communication through the Internet using industrial protocols that were not designed to work with external networks. To understand security issues related to this practice, prior work usually relies on active scans by researchers or services such as Shodan. While such scans can identify publicly open ports, they cannot identify legitimate use of insecure industrial traffic. In particular, source-based filtering in Network Address Translation or Firewalls prevent detection by active scanning, but do not ensure that insecure communication is not manipulated in transit. In this work, we compare Shodan-only analysis with large-scale traffic analysis at a local Internet Exchange Point (IXP), based on sFlow sampling. This setup allows us to identify ICS endpoints actually exchanging industrial traffic over the Internet. Besides, we are able to detect scanning activities and what other type of traffic is exchanged by the systems (i.e., IT traffic). We find that Shodan only listed less than 2% of hosts that we identified as exchanging industrial traffic, and only 7% of hosts identified by Shodan actually exchange industrial traffic. Therefore, Shodan do not allow to understand the actual use of insecure industrial protocols on the Internet and the current security practices in ICS communications. We show that 75.6% of ICS hosts still rely on unencrypted communications without integrity protection, leaving those critical systems vulnerable to malicious attacks.

</details>

<details>

<summary>2021-02-18 22:36:50 - Obfuscated Access and Search Patterns in Searchable Encryption</summary>

- *Zhiwei Shang, Simon Oya, Andreas Peter, Florian Kerschbaum*

- `2102.09651v1` - [abs](http://arxiv.org/abs/2102.09651v1) - [pdf](http://arxiv.org/pdf/2102.09651v1)

> Searchable Symmetric Encryption (SSE) allows a data owner to securely outsource its encrypted data to a cloud server while maintaining the ability to search over it and retrieve matched documents. Most existing SSE schemes leak which documents are accessed per query, i.e., the so-called access pattern, and thus are vulnerable to attacks that can recover the database or the queried keywords. Current techniques that fully hide access patterns, such as ORAM or PIR, suffer from heavy communication or computational costs, and are not designed with search capabilities in mind. Recently, Chen et al. (INFOCOM'18) proposed an obfuscation framework for SSE that protects the access pattern in a differentially private way with a reasonable utility cost. However, this scheme leaks the so-called search pattern, i.e., how many times a certain query is performed. This leakage makes the proposal vulnerable to certain database and query recovery attacks.   In this paper, we propose OSSE (Obfuscated SSE), an SSE scheme that obfuscates the access pattern independently for each query performed. This in turn hides the search pattern and makes our scheme resistant against attacks that rely on this leakage. Under certain reasonable assumptions, our scheme has smaller communication overhead than ORAM-based SSE. Furthermore, our scheme works in a single communication round and requires very small constant client-side storage. Our empirical evaluation shows that OSSE is highly effective at protecting against different query recovery attacks while keeping a reasonable utility level. Our protocol provides significantly more protection than the proposal by Chen et al.~against some state-of-the-art attacks, which demonstrates the importance of hiding search patterns in designing effective privacy-preserving SSE schemes.

</details>

<details>

<summary>2021-02-18 23:45:09 - NATOs Mission-Critical Space Capabilities under Threat: Cybersecurity Gaps in the Military Space Asset Supply Chain</summary>

- *Berenike Vollmer*

- `2102.09674v1` - [abs](http://arxiv.org/abs/2102.09674v1) - [pdf](http://arxiv.org/pdf/2102.09674v1)

> The North Atlantic Treaty Organizations (NATO) public-private Space Asset Supply Chain (SASC) currently exhibits significant cybersecurity gaps. It is well-established that data obtained from space assets is fundamental to NATO, as they allow for the facilitation of its missions, self-defence and effective deterrence of its adversaries. Any hostile cyber operation, suspending control over a space asset, severely impacts both NATO missions and allied Member States national security. This threat is exacerbated by NATOs mostly unregulated cyber SASC. Hence, this thesis answers a twofold research question: a) What are current cybersecurity gaps along NATOs global SASC; and b) How can NATO and its allied Member States gain greater control over such gaps to safeguard the supply of NATO mission-critical information? An ontological field study is carried out by conducting nineteen semi-structured interviews with high-level representatives from relevant public, private and academic organizations. This research was undertaken in collaboration with the NATO Cooperative Cyber Defence Centre of Excellence (CCDCOE) in Tallinn, Estonia. This thesis concludes that current cybersecurity gaps along NATOs SASC are caused by cyber vulnerabilities such as legacy systems or the use of Commercial-Off-the-Shelf (COTS) technology. Inadequate cyber SASC management is caused by hindrances such as misaligned classification levels and significant understaffing. On this basis, NATO should consider two major collaboration initiatives: a) Raising Awareness throughout the whole of the NATO system, and b) Pushing forward the creation of regulation through a standardized security framework on SASC cybersecurity. Doing so would enable NATO and its Member States to recognise cyberthreats to mission-critical data early on along its cyber SASC, and thus increase transparency, responsibility, and liability.

</details>

<details>

<summary>2021-02-19 10:28:15 - V-Gas: Generating High Gas Consumption Inputs to Avoid Out-of-Gas Vulnerability</summary>

- *Fuchen Ma, Ying Fu, Meng Ren, Wanting Sun, Houbing Song, Yu Jiang, Jun Sun, Jiaguang Sun*

- `1910.02945v2` - [abs](http://arxiv.org/abs/1910.02945v2) - [pdf](http://arxiv.org/pdf/1910.02945v2)

> The out-of-gas error occurs when smart contract programs are provided with inputs that cause excessive gas consumption, and would be easily exploited to make the DoS attack. Multiple approaches have been proposed to estimate the gas limit of a function in smart contracts to avoid such error. However, under estimation often happens when the contract is complicated. In this work, we propose V-Gas, which could automatically generate inputs that maximizes the gas cost and reduce the under estimation cases. V-Gas is designed based on feedback-directed mutational fuzz testing. First, V-Gas builds the gas weighted control flow graph (CFG) of functions in smart contracts. Then, V-Gas develops gas consumption guided selection and mutation strategies to generate the input that maximize the gas consumption. For evaluation, we implement V-Gas based on js-evm, a widely used ethereum virtual machine written in javascript, and conduct experiments on 736 real-world transactions recorded on Ethereum. 44.02\% of the transactions would have out-of-gas errors under the estimation results given by solc, means that the recorded real gas consumption for those recorded transactions is larger than the gas limit value estimated by solc. While V-Gas could reduce the under estimation ratio to 13.86\%. Furthermore, V-Gas has exposed 25 previously unknown out-of-gas vulnerabilities in those widely-used smart contracts, 5 of which have been assigned unique CVE identifiers in the US National Vulnerability Database.

</details>

<details>

<summary>2021-02-19 13:37:27 - Cyber-Physical Energy Systems Security: Threat Modeling, Risk Assessment, Resources, Metrics, and Case Studies</summary>

- *Ioannis Zografopoulos, Juan Ospina, XiaoRui Liu, Charalambos Konstantinou*

- `2101.10198v2` - [abs](http://arxiv.org/abs/2101.10198v2) - [pdf](http://arxiv.org/pdf/2101.10198v2)

> Cyber-physical systems (CPS) are interconnected architectures that employ analog, digital, and communication resources for their interaction with the physical environment. CPS are the backbone of enterprise, industrial, and critical infrastructure. Thus, their vital importance makes them prominent targets for malicious attacks aiming to disrupt their operations. Attacks targeting cyber-physical energy systems (CPES), given their mission-critical nature, can have disastrous consequences. The security of CPES can be enhanced leveraging testbed capabilities to replicate power system operations, discover vulnerabilities, develop security countermeasures, and evaluate grid operation under fault-induced or maliciously constructed scenarios. In this paper, we provide a comprehensive overview of the CPS security landscape with emphasis on CPES. Specifically, we demonstrate a threat modeling methodology to accurately represent the CPS elements, their interdependencies, as well as the possible attack entry points and system vulnerabilities. Leveraging the threat model formulation, we present a CPS framework designed to delineate the hardware, software, and modeling resources required to simulate the CPS and construct high-fidelity models which can be used to evaluate the system's performance under adverse scenarios. The system performance is assessed using scenario-specific metrics, while risk assessment enables system vulnerability prioritization factoring the impact on the system operation. The overarching framework for modeling, simulating, assessing, and mitigating attacks in a CPS is illustrated using four representative attack scenarios targeting CPES. The key objective of this paper is to demonstrate a step-by-step process that can be used to enact in-depth cybersecurity analyses, thus leading to more resilient and secure CPS.

</details>

<details>

<summary>2021-02-19 17:31:30 - PCaaD: Towards Automated Determination and Exploitation of Industrial Processes</summary>

- *B. Green, W. Knowles, M. Krotofil, R. Derbyshire, D. Prince, N. Suri*

- `2102.10049v1` - [abs](http://arxiv.org/abs/2102.10049v1) - [pdf](http://arxiv.org/pdf/2102.10049v1)

> Over the last decade, Programmable Logic Controllers (PLCs) have been increasingly targeted by attackers to obtain control over industrial processes that support critical services. Such targeted attacks typically require detailed knowledge of system-specific attributes, including hardware configurations, adopted protocols, and PLC control-logic, i.e. process comprehension. The consensus from both academics and practitioners suggests stealthy process comprehension obtained from a PLC alone, to conduct targeted attacks, is impractical. In contrast, we assert that current PLC programming practices open the door to a new vulnerability class based on control-logic constructs. To support this, we propose the concept of Process Comprehension at a Distance (PCaaD), as a novel methodological and automatable approach for system-agnostic exploitation of PLC library functions, leading to the targeted exfiltration of operational data, manipulation of control-logic behavior, and establishment of covert command and control channels through unused memory. We validate PCaaD on widely used PLCs, by identification of practical attacks.

</details>

<details>

<summary>2021-02-19 21:59:16 - Anomalous Example Detection in Deep Learning: A Survey</summary>

- *Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K. Varshney, Dawn Song*

- `2003.06979v2` - [abs](http://arxiv.org/abs/2003.06979v2) - [pdf](http://arxiv.org/pdf/2003.06979v2)

> Deep Learning (DL) is vulnerable to out-of-distribution and adversarial examples resulting in incorrect outputs. To make DL more robust, several posthoc (or runtime) anomaly detection techniques to detect (and discard) these anomalous samples have been proposed in the recent past. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection for DL based applications. We provide a taxonomy for existing techniques based on their underlying assumptions and adopted approaches. We discuss various techniques in each of the categories and provide the relative strengths and weaknesses of the approaches. Our goal in this survey is to provide an easier yet better understanding of the techniques belonging to different categories in which research has been done on this topic. Finally, we highlight the unsolved research challenges while applying anomaly detection techniques in DL systems and present some high-impact future research directions.

</details>

<details>

<summary>2021-02-20 12:15:25 - FoolHD: Fooling speaker identification by Highly imperceptible adversarial Disturbances</summary>

- *Ali Shahin Shamsabadi, Francisco Sepúlveda Teixeira, Alberto Abad, Bhiksha Raj, Andrea Cavallaro, Isabel Trancoso*

- `2011.08483v2` - [abs](http://arxiv.org/abs/2011.08483v2) - [pdf](http://arxiv.org/pdf/2011.08483v2)

> Speaker identification models are vulnerable to carefully designed adversarial perturbations of their input signals that induce misclassification. In this work, we propose a white-box steganography-inspired adversarial attack that generates imperceptible adversarial perturbations against a speaker identification model. Our approach, FoolHD, uses a Gated Convolutional Autoencoder that operates in the DCT domain and is trained with a multi-objective loss function, in order to generate and conceal the adversarial perturbation within the original audio files. In addition to hindering speaker identification performance, this multi-objective loss accounts for human perception through a frame-wise cosine similarity between MFCC feature vectors extracted from the original and adversarial audio files. We validate the effectiveness of FoolHD with a 250-speaker identification x-vector network, trained using VoxCeleb, in terms of accuracy, success rate, and imperceptibility. Our results show that FoolHD generates highly imperceptible adversarial audio files (average PESQ scores above 4.30), while achieving a success rate of 99.6% and 99.2% in misleading the speaker identification model, for untargeted and targeted settings, respectively.

</details>

<details>

<summary>2021-02-20 20:18:54 - Raising Security Awareness using Cybersecurity Challenges in Embedded Programming Courses</summary>

- *Tiago Espinha Gasiba, Samra Hodzic, Ulrike Lechner, Maria Pinto-Albuquerque*

- `2102.10436v1` - [abs](http://arxiv.org/abs/2102.10436v1) - [pdf](http://arxiv.org/pdf/2102.10436v1)

> Security bugs are errors in code that, when exploited, can lead to serious software vulnerabilities. These bugs could allow an attacker to take over an application and steal information. One of the ways to address this issue is by means of awareness training. The Sifu platform was developed in the industry, for the industry, with the aim to raise software developers' awareness of secure coding. This paper extends the Sifu platform with three challenges that specifically address embedded programming courses, and describes how to implement these challenges, while also evaluating the usefulness of these challenges to raise security awareness in an academic setting. Our work presents technical details on the detection mechanisms for software vulnerabilities and gives practical advice on how to implement them. The evaluation of the challenges is performed through two trial runs with a total of 16 participants. Our preliminary results show that the challenges are suitable for academia, and can even potentially be included in official teaching curricula. One major finding is an indicator of the lack of awareness of secure coding by undergraduates. Finally, we compare our results with previous work done in the industry and extract advice for practitioners.

</details>

<details>

<summary>2021-02-20 20:19:27 - Understanding and Enhancing the Use of Context for Machine Translation</summary>

- *Marzieh Fadaee*

- `2102.10437v1` - [abs](http://arxiv.org/abs/2102.10437v1) - [pdf](http://arxiv.org/pdf/2102.10437v1)

> To understand and infer meaning in language, neural models have to learn complicated nuances. Discovering distinctive linguistic phenomena from data is not an easy task. For instance, lexical ambiguity is a fundamental feature of language which is challenging to learn. Even more prominently, inferring the meaning of rare and unseen lexical units is difficult with neural networks. Meaning is often determined from context. With context, languages allow meaning to be conveyed even when the specific words used are not known by the reader. To model this learning process, a system has to learn from a few instances in context and be able to generalize well to unseen cases. The learning process is hindered when training data is scarce for a task. Even with sufficient data, learning patterns for the long tail of the lexical distribution is challenging. In this thesis, we focus on understanding certain potentials of contexts in neural models and design augmentation models to benefit from them. We focus on machine translation as an important instance of the more general language understanding problem. To translate from a source language to a target language, a neural model has to understand the meaning of constituents in the provided context and generate constituents with the same meanings in the target language. This task accentuates the value of capturing nuances of language and the necessity of generalization from few observations. The main problem we study in this thesis is what neural machine translation models learn from data and how we can devise more focused contexts to enhance this learning. Looking more in-depth into the role of context and the impact of data on learning models is essential to advance the NLP field. Moreover, it helps highlight the vulnerabilities of current neural networks and provides insights into designing more robust models.

</details>

<details>

<summary>2021-02-20 21:40:53 - Spotting Silent Buffer Overflows in Execution Trace through Graph Neural Network Assisted Data Flow Analysis</summary>

- *Zhilong Wang, Li Yu, Suhang Wang, Peng Liu*

- `2102.10452v1` - [abs](http://arxiv.org/abs/2102.10452v1) - [pdf](http://arxiv.org/pdf/2102.10452v1)

> A software vulnerability could be exploited without any visible symptoms. When no source code is available, although such silent program executions could cause very serious damage, the general problem of analyzing silent yet harmful executions is still an open problem. In this work, we propose a graph neural network (GNN) assisted data flow analysis method for spotting silent buffer overflows in execution traces. The new method combines a novel graph structure (denoted DFG+) beyond data-flow graphs, a tool to extract {\tt DFG+} from execution traces, and a modified Relational Graph Convolutional Network as the GNN model to be trained. The evaluation results show that a well-trained model can be used to analyze vulnerabilities in execution traces (of previously-unseen programs) without support of any source code. Our model achieves 94.39\% accuracy on the test data and successfully locates 29 out of 30 real-world silent buffer overflow vulnerabilities. Leveraging deep learning, the proposed method is, to our best knowledge, the first general-purpose analysis method for silent buffer overflows. It is also the first method to spot silent buffer overflows in global variables, stack variables, or heap variables without crossing the boundary of allocated chunks.

</details>

<details>

<summary>2021-02-21 03:13:27 - Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits</summary>

- *Jiawang Bai, Baoyuan Wu, Yong Zhang, Yiming Li, Zhifeng Li, Shu-Tao Xia*

- `2102.10496v1` - [abs](http://arxiv.org/abs/2102.10496v1) - [pdf](http://arxiv.org/pdf/2102.10496v1)

> To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits ($i.e.$, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.

</details>

<details>

<summary>2021-02-21 21:51:11 - Survey on Enterprise Internet-of-Things Systems (E-IoT): A Security Perspective</summary>

- *Luis Puche Rondon, Leonardo Babun, Ahmet Aris, Kemal Akkaya, A. Selcuk Uluagac*

- `2102.10695v1` - [abs](http://arxiv.org/abs/2102.10695v1) - [pdf](http://arxiv.org/pdf/2102.10695v1)

> As technology becomes more widely available, millions of users worldwide have installed some form of smart device in their homes or workplaces. These devices are often off-the-shelf commodity systems, such as Google Home or Samsung SmartThings, that are installed by end-users looking to automate a small deployment. In contrast to these "plug-and-play" systems, purpose-built Enterprise Internet-of-Things (E-IoT) systems such as Crestron, Control4, RTI, Savant offer a smart solution for more sophisticated applications (e.g., complete lighting control, A/V management, security). In contrast to commodity systems, E-IoT systems are usually closed source, costly, require certified installers, and are overall more robust for their use cases. Due to this, E-IoT systems are often found in expensive smart homes, government and academic conference rooms, yachts, and smart private offices. However, while there has been plenty of research on the topic of commodity systems, no current study exists that provides a complete picture of E-IoT systems, their components, and relevant threats. As such, lack of knowledge of E-IoT system threats, coupled with the cost of E-IoT systems has led many to assume that E-IoT systems are secure. To address this research gap, raise awareness on E-IoT security, and motivate further research, this work emphasizes E-IoT system components, E-IoT vulnerabilities, solutions, and their security implications. In order to systematically analyze the security of E-IoT systems, we divide E-IoT systems into four layers: E-IoT Devices Layer, Communications Layer, Monitoring and Applications Layer, and Business Layer. We survey attacks and defense mechanisms, considering the E-IoT components at each layer and the associated threats. In addition, we present key observations in state-of-the-art E-IoT security and provide a list of open research problems that need further research.

</details>

<details>

<summary>2021-02-22 06:49:59 - Clustering Algorithm to Detect Adversaries in Federated Learning</summary>

- *Krishna Yadav, B. B Gupta*

- `2102.10799v1` - [abs](http://arxiv.org/abs/2102.10799v1) - [pdf](http://arxiv.org/pdf/2102.10799v1)

> In recent times, federated machine learning has been very useful in building intelligent intrusion detection systems for IoT devices. As IoT devices are equipped with a security architecture vulnerable to various attacks, these security loopholes may bring a risk during federated training of decentralized IoT devices. Adversaries can take control over these IoT devices and inject false gradients to degrade the global model performance. In this paper, we have proposed an approach that detects the adversaries with the help of a clustering algorithm. After clustering, it further rewards the clients for detecting honest and malicious clients. Our proposed gradient filtration approach does not require any processing power from the client-side and does not use excessive bandwidth, making it very much feasible for IoT devices. Further, our approach has been very successful in boosting the global model accuracy, up to 99% even in the presence of 40% adversaries.

</details>

<details>

<summary>2021-02-22 23:09:12 - An Online Approach to Cyberattack Detection and Localization in Smart Grid</summary>

- *Dan Li, Nagi Gebraeel, Kamran Paynabar, A. P. Sakis Meliopoulos*

- `2102.11401v1` - [abs](http://arxiv.org/abs/2102.11401v1) - [pdf](http://arxiv.org/pdf/2102.11401v1)

> Complex interconnections between information technology and digital control systems have significantly increased cybersecurity vulnerabilities in smart grids. Cyberattacks involving data integrity can be very disruptive because of their potential to compromise physical control by manipulating measurement data. This is especially true in large and complex electric networks that often rely on traditional intrusion detection systems focused on monitoring network traffic. In this paper, we develop an online detection algorithm to detect and localize covert attacks on smart grids. Using a network system model, we develop a theoretical framework by characterizing a covert attack on a generator bus in the network as sparse features in the state-estimation residuals. We leverage such sparsity via a regularized linear regression method to detect and localize covert attacks based on the regression coefficients. We conduct a comprehensive numerical study on both linear and nonlinear system models to validate our proposed method. The results show that our method outperforms conventional methods in both detection delay and localization accuracy.

</details>

<details>

<summary>2021-02-23 05:16:57 - V2W-BERT: A Framework for Effective Hierarchical Multiclass Classification of Software Vulnerabilities</summary>

- *Siddhartha Shankar Das, Edoardo Serra, Mahantesh Halappanavar, Alex Pothen, Ehab Al-Shaer*

- `2102.11498v1` - [abs](http://arxiv.org/abs/2102.11498v1) - [pdf](http://arxiv.org/pdf/2102.11498v1)

> Weaknesses in computer systems such as faults, bugs and errors in the architecture, design or implementation of software provide vulnerabilities that can be exploited by attackers to compromise the security of a system. Common Weakness Enumerations (CWE) are a hierarchically designed dictionary of software weaknesses that provide a means to understand software flaws, potential impact of their exploitation, and means to mitigate these flaws. Common Vulnerabilities and Exposures (CVE) are brief low-level descriptions that uniquely identify vulnerabilities in a specific product or protocol. Classifying or mapping of CVEs to CWEs provides a means to understand the impact and mitigate the vulnerabilities. Since manual mapping of CVEs is not a viable option, automated approaches are desirable but challenging.   We present a novel Transformer-based learning framework (V2W-BERT) in this paper. By using ideas from natural language processing, link prediction and transfer learning, our method outperforms previous approaches not only for CWE instances with abundant data to train, but also rare CWE classes with little or no data to train. Our approach also shows significant improvements in using historical data to predict links for future instances of CVEs, and therefore, provides a viable approach for practical applications. Using data from MITRE and National Vulnerability Database, we achieve up to 97% prediction accuracy for randomly partitioned data and up to 94% prediction accuracy in temporally partitioned data. We believe that our work will influence the design of better methods and training models, as well as applications to solve increasingly harder problems in cybersecurity.

</details>

<details>

<summary>2021-02-23 09:47:45 - Enhancing Model Robustness By Incorporating Adversarial Knowledge Into Semantic Representation</summary>

- *Jinfeng Li, Tianyu Du, Xiangyu Liu, Rong Zhang, Hui Xue, Shouling Ji*

- `2102.11584v1` - [abs](http://arxiv.org/abs/2102.11584v1) - [pdf](http://arxiv.org/pdf/2102.11584v1)

> Despite that deep neural networks (DNNs) have achieved enormous success in many domains like natural language processing (NLP), they have also been proven to be vulnerable to maliciously generated adversarial examples. Such inherent vulnerability has threatened various real-world deployed DNNs-based applications. To strength the model robustness, several countermeasures have been proposed in the English NLP domain and obtained satisfactory performance. However, due to the unique language properties of Chinese, it is not trivial to extend existing defenses to the Chinese domain. Therefore, we propose AdvGraph, a novel defense which enhances the robustness of Chinese-based NLP models by incorporating adversarial knowledge into the semantic representation of the input. Extensive experiments on two real-world tasks show that AdvGraph exhibits better performance compared with previous work: (i) effective - it significantly strengthens the model robustness even under the adaptive attacks setting without negative impact on model performance over legitimate input; (ii) generic - its key component, i.e., the representation of connotative adversarial knowledge is task-agnostic, which can be reused in any Chinese-based NLP models without retraining; and (iii) efficient - it is a light-weight defense with sub-linear computational complexity, which can guarantee the efficiency required in practical scenarios.

</details>

<details>

<summary>2021-02-23 15:38:49 - Analyzing Deep Neural Networks with Symbolic Propagation: Towards Higher Precision and Faster Verification</summary>

- *Jianlin Li, Pengfei Yang, Jiangchao Liu, Liqian Chen, Xiaowei Huang, Lijun Zhang*

- `1902.09866v2` - [abs](http://arxiv.org/abs/1902.09866v2) - [pdf](http://arxiv.org/pdf/1902.09866v2)

> Deep neural networks (DNNs) have been shown lack of robustness for the vulnerability of their classification to small perturbations on the inputs. This has led to safety concerns of applying DNNs to safety-critical domains. Several verification approaches have been developed to automatically prove or disprove safety properties of DNNs. However, these approaches suffer from either the scalability problem, i.e., only small DNNs can be handled, or the precision problem, i.e., the obtained bounds are loose. This paper improves on a recent proposal of analyzing DNNs through the classic abstract interpretation technique, by a novel symbolic propagation technique. More specifically, the values of neurons are represented symbolically and propagated forwardly from the input layer to the output layer, on top of abstract domains. We show that our approach can achieve significantly higher precision and thus can prove more properties than using only abstract domains. Moreover, we show that the bounds derived from our approach on the hidden neurons, when applied to a state-of-the-art SMT based verification tool, can improve its performance. We implement our approach into a software tool and validate it over a few DNNs trained on benchmark datasets such as MNIST, etc.

</details>

<details>

<summary>2021-02-23 20:59:30 - Non-Singular Adversarial Robustness of Neural Networks</summary>

- *Yu-Lin Tsai, Chia-Yi Hsu, Chia-Mu Yu, Pin-Yu Chen*

- `2102.11935v1` - [abs](http://arxiv.org/abs/2102.11935v1) - [pdf](http://arxiv.org/pdf/2102.11935v1)

> Adversarial robustness has become an emerging challenge for neural network owing to its over-sensitivity to small input perturbations. While being critical, we argue that solving this singular issue alone fails to provide a comprehensive robustness assessment. Even worse, the conclusions drawn from singular robustness may give a false sense of overall model robustness. Specifically, our findings show that adversarially trained models that are robust to input perturbations are still (or even more) vulnerable to weight perturbations when compared to standard models. In this paper, we formalize the notion of non-singular adversarial robustness for neural networks through the lens of joint perturbations to data inputs as well as model weights. To our best knowledge, this study is the first work considering simultaneous input-weight adversarial perturbations. Based on a multi-layer feed-forward neural network model with ReLU activation functions and standard classification loss, we establish error analysis for quantifying the loss sensitivity subject to $\ell_\infty$-norm bounded perturbations on data inputs and model weights. Based on the error analysis, we propose novel regularization functions for robust training and demonstrate improved non-singular robustness against joint input-weight adversarial perturbations.

</details>

<details>

<summary>2021-02-24 03:42:08 - Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion</summary>

- *Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Yi Chang*

- `2004.14781v2` - [abs](http://arxiv.org/abs/2004.14781v2) - [pdf](http://arxiv.org/pdf/2004.14781v2)

> Human-curated knowledge graphs provide critical supportive information to various natural language processing tasks, but these graphs are usually incomplete, urging auto-completion of them. Prevalent graph embedding approaches, e.g., TransE, learn structured knowledge via representing graph elements into dense embeddings and capturing their triple-level relationship with spatial distance. However, they are hardly generalizable to the elements never visited in training and are intrinsically vulnerable to graph incompleteness. In contrast, textual encoding approaches, e.g., KG-BERT, resort to graph triple's text and triple-level contextualized representations. They are generalizable enough and robust to the incompleteness, especially when coupled with pre-trained encoders. But two major drawbacks limit the performance: (1) high overheads due to the costly scoring of all possible triples in inference, and (2) a lack of structured knowledge in the textual encoder. In this paper, we follow the textual encoding paradigm and aim to alleviate its drawbacks by augmenting it with graph embedding techniques -- a complementary hybrid of both paradigms. Specifically, we partition each triple into two asymmetric parts as in translation-based graph embedding approach, and encode both parts into contextualized representations by a Siamese-style textual encoder. Built upon the representations, our model employs both deterministic classifier and spatial measurement for representation and structure learning respectively. Moreover, we develop a self-adaptive ensemble scheme to further improve the performance by incorporating triple scores from an existing graph embedding model. In experiments, we achieve state-of-the-art performance on three benchmarks and a zero-shot dataset for link prediction, with highlights of inference costs reduced by 1-2 orders of magnitude compared to a textual encoding method.

</details>

<details>

<summary>2021-02-24 13:45:38 - Graphfool: Targeted Label Adversarial Attack on Graph Embedding</summary>

- *Jinyin Chen, Xiang Lin, Dunjie Zhang, Wenrong Jiang, Guohan Huang, Hui Xiong, Yun Xiang*

- `2102.12284v1` - [abs](http://arxiv.org/abs/2102.12284v1) - [pdf](http://arxiv.org/pdf/2102.12284v1)

> Deep learning is effective in graph analysis. It is widely applied in many related areas, such as link prediction, node classification, community detection, and graph classification etc. Graph embedding, which learns low-dimensional representations for vertices or edges in the graph, usually employs deep models to derive the embedding vector. However, these models are vulnerable. We envision that graph embedding methods based on deep models can be easily attacked using adversarial examples. Thus, in this paper, we propose Graphfool, a novel targeted label adversarial attack on graph embedding. It can generate adversarial graph to attack graph embedding methods via classifying boundary and gradient information in graph convolutional network (GCN). Specifically, we perform the following steps: 1),We first estimate the classification boundaries of different classes. 2), We calculate the minimal perturbation matrix to misclassify the attacked vertex according to the target classification boundary. 3), We modify the adjacency matrix according to the maximal absolute value of the disturbance matrix. This process is implemented iteratively. To the best of our knowledge, this is the first targeted label attack technique. The experiments on real-world graph networks demonstrate that Graphfool can derive better performance than state-of-art techniques. Compared with the second best algorithm, Graphfool can achieve an average improvement of 11.44% in attack success rate.

</details>

<details>

<summary>2021-02-24 17:52:11 - Integrated Reasoning Engine for Pointer-related Code Clone Detection</summary>

- *Hongfa Xue, Yongsheng Mei, Kailash Gogineni, Guru Venkataramani, Tian Lan*

- `2105.11933v1` - [abs](http://arxiv.org/abs/2105.11933v1) - [pdf](http://arxiv.org/pdf/2105.11933v1)

> Detecting similar code fragments, usually referred to as code clones, is an important task. In particular, code clone detection can have significant uses in the context of vulnerability discovery, refactoring and plagiarism detection. However, false positives are inevitable and always require manual reviews. In this paper, we propose Twin-Finder+, a novel closed-loop approach for pointer-related code clone detection that integrates machine learning and symbolic execution techniques to achieve precision. Twin-Finder+ introduces a formal verification mechanism to automate such manual reviews process. Our experimental results show Twin-Finder+ that can remove 91.69% false positives in average. We further conduct security analysis for memory safety using real-world applications, Links version 2.14 and libreOffice-6.0.0.1. Twin-Finder+ is able to find 6 unreported bugs in Links version 2.14 and one public patched bug in libreOffice-6.0.0.1.

</details>

<details>

<summary>2021-02-24 23:10:31 - Data Poisoning Attacks and Defenses to Crowdsourcing Systems</summary>

- *Minghong Fang, Minghao Sun, Qi Li, Neil Zhenqiang Gong, Jin Tian, Jia Liu*

- `2102.09171v2` - [abs](http://arxiv.org/abs/2102.09171v2) - [pdf](http://arxiv.org/pdf/2102.09171v2)

> A key challenge of big data analytics is how to collect a large volume of (labeled) data. Crowdsourcing aims to address this challenge via aggregating and estimating high-quality data (e.g., sentiment label for text) from pervasive clients/users. Existing studies on crowdsourcing focus on designing new methods to improve the aggregated data quality from unreliable/noisy clients. However, the security aspects of such crowdsourcing systems remain under-explored to date. We aim to bridge this gap in this work. Specifically, we show that crowdsourcing is vulnerable to data poisoning attacks, in which malicious clients provide carefully crafted data to corrupt the aggregated data. We formulate our proposed data poisoning attacks as an optimization problem that maximizes the error of the aggregated data. Our evaluation results on one synthetic and two real-world benchmark datasets demonstrate that the proposed attacks can substantially increase the estimation errors of the aggregated data. We also propose two defenses to reduce the impact of malicious clients. Our empirical results show that the proposed defenses can substantially reduce the estimation errors of the data poisoning attacks.

</details>

<details>

<summary>2021-02-25 01:45:19 - Memory-Safety Challenge Considered Solved? An In-Depth Study with All Rust CVEs</summary>

- *Hui Xu, Zhuangbin Chen, Mingshen Sun, Yangfan Zhou, Michael Lyu*

- `2003.03296v6` - [abs](http://arxiv.org/abs/2003.03296v6) - [pdf](http://arxiv.org/pdf/2003.03296v6)

> Rust is an emerging programing language that aims at preventing memory-safety bugs without sacrificing much efficiency. The claimed property is very attractive to developers, and many projects start using the language. However, can Rust achieve the memory-safety promise? This paper studies the question by surveying 186 real-world bug reports collected from several origins which contain all existing Rust CVEs (common vulnerability and exposures) of memory-safety issues by 2020-12-31. We manually analyze each bug and extract their culprit patterns. Our analysis result shows that Rust can keep its promise that all memory-safety bugs require unsafe code, and many memory-safety bugs in our dataset are mild soundness issues that only leave a possibility to write memory-safety bugs without unsafe code. Furthermore, we summarize three typical categories of memory-safety bugs, including automatic memory reclaim, unsound function, and unsound generic or trait. While automatic memory claim bugs are related to the side effect of Rust newly-adopted ownership-based resource management scheme, unsound function reveals the essential challenge of Rust development for avoiding unsound code, and unsound generic or trait intensifies the risk of introducing unsoundness. Based on these findings, we propose two promising directions towards improving the security of Rust development, including several best practices of using specific APIs and methods to detect particular bugs involving unsafe code. Our work intends to raise more discussions regarding the memory-safety issues of Rust and facilitate the maturity of the language.

</details>

<details>

<summary>2021-02-25 09:15:27 - A Survey on Industrial Control System Testbeds and Datasets for Security Research</summary>

- *Mauro Conti, Denis Donadel, Federico Turrin*

- `2102.05631v3` - [abs](http://arxiv.org/abs/2102.05631v3) - [pdf](http://arxiv.org/pdf/2102.05631v3)

> The increasing digitization and interconnection of legacy Industrial Control Systems (ICSs) open new vulnerability surfaces, exposing such systems to malicious attackers. Furthermore, since ICSs are often employed in critical infrastructures (e.g., nuclear plants) and manufacturing companies (e.g., chemical industries), attacks can lead to devastating physical damages. In dealing with this security requirement, the research community focuses on developing new security mechanisms such as Intrusion Detection Systems (IDSs), facilitated by leveraging modern machine learning techniques. However, these algorithms require a testing platform and a considerable amount of data to be trained and tested accurately. To satisfy this prerequisite, Academia, Industry, and Government are increasingly proposing testbed (i.e., scaled-down versions of ICSs or simulations) to test the performances of the IDSs. Furthermore, to enable researchers to cross-validate security systems (e.g., security-by-design concepts or anomaly detectors), several datasets have been collected from testbeds and shared with the community. In this paper, we provide a deep and comprehensive overview of ICSs, presenting the architecture design, the employed devices, and the security protocols implemented. We then collect, compare, and describe testbeds and datasets in the literature, highlighting key challenges and design guidelines to keep in mind in the design phases. Furthermore, we enrich our work by reporting the best performing IDS algorithms tested on every dataset to create a baseline in state of the art for this field. Finally, driven by knowledge accumulated during this survey's development, we report advice and good practices on the development, the choice, and the utilization of testbeds, datasets, and IDSs.

</details>

<details>

<summary>2021-02-25 17:29:00 - Deep Adversarial Learning on Google Home devices</summary>

- *Andrea Ranieri, Davide Caputo, Luca Verderame, Alessio Merlo, Luca Caviglione*

- `2102.13023v1` - [abs](http://arxiv.org/abs/2102.13023v1) - [pdf](http://arxiv.org/pdf/2102.13023v1)

> Smart speakers and voice-based virtual assistants are core components for the success of the IoT paradigm. Unfortunately, they are vulnerable to various privacy threats exploiting machine learning to analyze the generated encrypted traffic. To cope with that, deep adversarial learning approaches can be used to build black-box countermeasures altering the network traffic (e.g., via packet padding) and its statistical information. This letter showcases the inadequacy of such countermeasures against machine learning attacks with a dedicated experimental campaign on a real network dataset. Results indicate the need for a major re-engineering to guarantee the suitable protection of commercially available smart speakers.

</details>

<details>

<summary>2021-02-27 15:08:50 - Fuzzing Based on Function Importance by Interprocedural Control Flow Graph</summary>

- *Wenshuo Wang, Liang Cheng, Yang Zhang*

- `2010.03482v4` - [abs](http://arxiv.org/abs/2010.03482v4) - [pdf](http://arxiv.org/pdf/2010.03482v4)

> Coverage-based graybox fuzzer (CGF), such as AFL has gained great success in vulnerability detection thanks to its ease-of-use and bug-finding power. Since some code fragments such as memory allocation are more vulnerable than others, various improving techniques have been proposed to explore the more vulnerable areas by collecting extra information from the program under test or its executions. However, these improvements only consider limited types of information sources and ignore the fact that the priority a seed input to be fuzzed may be influenced by all the code it covers. Based on the above observations, we propose a fuzzing method based on the importance of functions. First, a data structure called Attributed Interprocedural Control Flow Graph (AICFG) is devised to combine different features of code fragments. Second, the importance of each node in the AICFG is calculated based on an improved PageRank algorithm, which also models the influence between connected nodes. During the fuzzing process, the node importance is updated periodically by a propagation algorithm. Then the seed selection and energy scheduling of a seed input are determined by the importance of its execution trace. We implement this approach on top of AFL in a tool named FunAFL and conduct an evaluation on 14 real-world programs against AFL and two of its improvements. FunAFL, with 17% higher branch coverage than others on average, finds 13 bugs and 3 of them are confirmed by CVE after 72 hours.

</details>

<details>

<summary>2021-02-28 18:49:25 - PyCG: Practical Call Graph Generation in Python</summary>

- *Vitalis Salis, Thodoris Sotiropoulos, Panos Louridas, Diomidis Spinellis, Dimitris Mitropoulos*

- `2103.00587v1` - [abs](http://arxiv.org/abs/2103.00587v1) - [pdf](http://arxiv.org/pdf/2103.00587v1)

> Call graphs play an important role in different contexts, such as profiling and vulnerability propagation analysis. Generating call graphs in an efficient manner can be a challenging task when it comes to high-level languages that are modular and incorporate dynamic features and higher-order functions.   Despite the language's popularity, there have been very few tools aiming to generate call graphs for Python programs. Worse, these tools suffer from several effectiveness issues that limit their practicality in realistic programs. We propose a pragmatic, static approach for call graph generation in Python. We compute all assignment relations between program identifiers of functions, variables, classes, and modules through an inter-procedural analysis. Based on these assignment relations, we produce the resulting call graph by resolving all calls to potentially invoked functions. Notably, the underlying analysis is designed to be efficient and scalable, handling several Python features, such as modules, generators, function closures, and multiple inheritance.   We have evaluated our prototype implementation, which we call PyCG, using two benchmarks: a micro-benchmark suite containing small Python programs and a set of macro-benchmarks with several popular real-world Python packages. Our results indicate that PyCG can efficiently handle thousands of lines of code in less than a second (0.38 seconds for 1k LoC on average). Further, it outperforms the state-of-the-art for Python in both precision and recall: PyCG achieves high rates of precision ~99.2%, and adequate recall ~69.9%. Finally, we demonstrate how PyCG can aid dependency impact analysis by showcasing a potential enhancement to GitHub's "security advisory" notification service using a real-world example.

</details>


## 2021-03

<details>

<summary>2021-03-01 09:22:59 - IntelliGen: Automatic Driver Synthesis for FuzzTesting</summary>

- *Mingrui Zhang, Jianzhong Liu, Fuchen Ma, Huafeng Zhang, Yu Jiang*

- `2103.00862v1` - [abs](http://arxiv.org/abs/2103.00862v1) - [pdf](http://arxiv.org/pdf/2103.00862v1)

> Fuzzing is a technique widely used in vulnerability detection. The process usually involves writing effective fuzz driver programs, which, when done manually, can be extremely labor intensive. Previous attempts at automation leave much to be desired, in either degree of automation or quality of output. In this paper, we propose IntelliGen, a framework that constructs valid fuzz drivers automatically. First, IntelliGen determines a set of entry functions and evaluates their respective chance of exhibiting a vulnerability. Then, IntelliGen generates fuzz drivers for the entry functions through hierarchical parameter replacement and type inference. We implemented IntelliGen and evaluated its effectiveness on real-world programs selected from the Android Open-Source Project, Google's fuzzer-test-suite and industrial collaborators. IntelliGen covered on average 1.08X-2.03X more basic blocks and 1.36X-2.06X more paths over state-of-the-art fuzz driver synthesizers FUDGE and FuzzGen. IntelliGen performed on par with manually written drivers and found 10 more bugs.

</details>

<details>

<summary>2021-03-01 20:45:00 - Robustness of classifiers to universal perturbations: a geometric perspective</summary>

- *Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard, Stefano Soatto*

- `1705.09554v2` - [abs](http://arxiv.org/abs/1705.09554v2) - [pdf](http://arxiv.org/pdf/1705.09554v2)

> Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we propose the first quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exists shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.

</details>

<details>

<summary>2021-03-01 21:37:54 - Adversarial training in communication constrained federated learning</summary>

- *Devansh Shah, Parijat Dube, Supriyo Chakraborty, Ashish Verma*

- `2103.01319v1` - [abs](http://arxiv.org/abs/2103.01319v1) - [pdf](http://arxiv.org/pdf/2103.01319v1)

> Federated learning enables model training over a distributed corpus of agent data. However, the trained model is vulnerable to adversarial examples, designed to elicit misclassification. We study the feasibility of using adversarial training (AT) in the federated learning setting. Furthermore, we do so assuming a fixed communication budget and non-iid data distribution between participating agents. We observe a significant drop in both natural and adversarial accuracies when AT is used in the federated setting as opposed to centralized training. We attribute this to the number of epochs of AT performed locally at the agents, which in turn effects (i) drift between local models; and (ii) convergence time (measured in number of communication rounds). Towards this end, we propose FedDynAT, a novel algorithm for performing AT in federated setting. Through extensive experimentation we show that FedDynAT significantly improves both natural and adversarial accuracy, as well as model convergence time by reducing the model drift.

</details>

<details>

<summary>2021-03-02 07:56:46 - Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web APIs under Deepfake Impersonation Attack</summary>

- *Shahroz Tariq, Sowon Jeon, Simon S. Woo*

- `2103.00847v2` - [abs](http://arxiv.org/abs/2103.00847v2) - [pdf](http://arxiv.org/pdf/2103.00847v2)

> Recently, significant advancements have been made in face recognition technologies using Deep Neural Networks. As a result, companies such as Microsoft, Amazon, and Naver offer highly accurate commercial face recognition web services for diverse applications to meet the end-user needs. Naturally, however, such technologies are threatened persistently, as virtually any individual can quickly implement impersonation attacks. In particular, these attacks can be a significant threat for authentication and identification services, which heavily rely on their underlying face recognition technologies' accuracy and robustness. Despite its gravity, the issue regarding deepfake abuse using commercial web APIs and their robustness has not yet been thoroughly investigated. This work provides a measurement study on the robustness of black-box commercial face recognition APIs against Deepfake Impersonation (DI) attacks using celebrity recognition APIs as an example case study. We use five deepfake datasets, two of which are created by us and planned to be released. More specifically, we measure attack performance based on two scenarios (targeted and non-targeted) and further analyze the differing system behaviors using fidelity, confidence, and similarity metrics. Accordingly, we demonstrate how vulnerable face recognition technologies from popular companies are to DI attack, achieving maximum success rates of 78.0% and 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with any celebrity) attacks, respectively. Moreover, we propose practical defense strategies to mitigate DI attacks, reducing the attack success rates to as low as 0% and 0.02% for targeted and non-targeted attacks, respectively.

</details>

<details>

<summary>2021-03-02 22:19:35 - Spam Prevention Using zk-SNARKs for Anonymous Peer-to-Peer Content Sharing Systems</summary>

- *Alberto Inselvini*

- `2103.02061v1` - [abs](http://arxiv.org/abs/2103.02061v1) - [pdf](http://arxiv.org/pdf/2103.02061v1)

> Decentralized unpermissioned peer-to-peer networks are inherently vulnerable to spam when they allow arbitrary participants to submit content to a common public index or registry; preventing this is difficult due to the absence of a central arbitrator who can act as a gate-keeper. For this reason indexing of new content even in otherwise decentralized networks (e.g. Bittorrent with DHT, IPFS) has generally been left to centralized services such as torrent sites.   Decentralized methods for spam prevention, such as Web of Trust, already exist[1][2] but they require submitters to assume pseudonymous identities and establish trust over time.   In this paper we present a method of spam prevention that works under the assumption that the participants are fully anonymous and do not want different submissions of theirs to be linked to each other.   By spam we do not specifically mean unsolicited advertising; rather it is the practice of adding a large amount of content in a short time, saturating the capacity of the network, and causing denial of service to peers. The purpose of our solution is to prevent users from saturating the system, and can be described as rate-limiting. The system should be censorship resistant: it should not be possible for submissions to be excluded because of the content itself, or for users to be excluded based on what they submit.   We first discuss a solution based on a single, centralized rate-limiter that is censorship resistant and anonymous, then we extend this to a fully decentralized blockchain-based system and present methods to make it economical and scalable.

</details>

<details>

<summary>2021-03-03 03:19:26 - Too Quiet in the Library: An Empirical Study of Security Updates in Android Apps' Native Code</summary>

- *Sumaya Almanee, Arda Unal, Mathias Payer, Joshua Garcia*

- `1911.09716v2` - [abs](http://arxiv.org/abs/1911.09716v2) - [pdf](http://arxiv.org/pdf/1911.09716v2)

> Android apps include third-party native libraries to increase performance and to reuse functionality. Native code is directly executed from apps through the Java Native Interface or the Android Native Development Kit. Android developers add precompiled native libraries to their projects, enabling their use. Unfortunately, developers often struggle or simply neglect to update these libraries in a timely manner. This results in the continuous use of outdated native libraries with unpatched security vulnerabilities years after patches became available.   To further understand such phenomena, we study the security updates in native libraries in the most popular 200 free apps on Google Play from Sept. 2013 to May 2020. A core difficulty we face in this study is the identification of libraries and their versions. Developers often rename or modify libraries, making their identification challenging. We create an approach called LibRARIAN (LibRAry veRsion IdentificAtioN) that accurately identifies native libraries and their versions as found in Android apps based on our novel similarity metric bin2sim. LibRARIAN leverages different features extracted from libraries based on their metadata and identifying strings in read-only sections.   We discovered 53/200 popular apps (26.5%) with vulnerable versions with known CVEs between Sept. 2013 and May 2020, with 14 of those apps remaining vulnerable. We find that app developers took, on average, 528.71 days to apply security patches, while library developers release a security patch after 54.59 days - a 10 times slower rate of update.

</details>

<details>

<summary>2021-03-03 06:45:05 - SciviK: A Versatile Framework for Specifying and Verifying Smart Contracts</summary>

- *Shaokai Lin, Xinyuan Sun, Jianan Yao, Ronghui Gu*

- `2103.02209v1` - [abs](http://arxiv.org/abs/2103.02209v1) - [pdf](http://arxiv.org/pdf/2103.02209v1)

> The growing adoption of smart contracts on blockchains poses new security risks that can lead to significant monetary loss, while existing approaches either provide no (or partial) security guarantees for smart contracts or require huge proof effort. To address this challenge, we present SciviK, a versatile framework for specifying and verifying industrial-grade smart contracts. SciviK's versatile approach extends previous efforts with three key contributions: (i) an expressive annotation system enabling built-in directives for vulnerability pattern checking, neural-based loop invariant inference, and the verification of rich properties of real-world smart contracts (ii) a fine-grained model for the Ethereum Virtual Machine (EVM) that provides low-level execution semantics, (iii) an IR-level verification framework integrating both SMT solvers and the Coq proof assistant.   We use SciviK to specify and verify security properties for 12 benchmark contracts and a real-world Decentralized Finance (DeFi) smart contract. Among all 158 specified security properties (in six types), 151 properties can be automatically verified within 2 seconds, five properties can be automatically verified after moderate modifications, and two properties are manually proved with around 200 lines of Coq code.

</details>

<details>

<summary>2021-03-03 19:59:00 - ACeD: Scalable Data Availability Oracle</summary>

- *Peiyao Sheng, Bowen Xue, Sreeram Kannan, Pramod Viswanath*

- `2011.00102v2` - [abs](http://arxiv.org/abs/2011.00102v2) - [pdf](http://arxiv.org/pdf/2011.00102v2)

> A popular method in practice offloads computation and storage in blockchains by relying on committing only hashes of off-chain data into the blockchain. This mechanism is acknowledged to be vulnerable to a stalling attack: the blocks corresponding to the committed hashes may be unavailable at any honest node. The straightforward solution of broadcasting all blocks to the entire network sidesteps this data availability attack, but it is not scalable. In this paper, we propose ACeD, a scalable solution to this data availability problem with $O(1)$ communication efficiency, the first to the best of our knowledge.   The key innovation is a new protocol that requires each of the $N$ nodes to receive only $O(1/N)$ of the block, such that the data is guaranteed to be available in a distributed manner in the network. Our solution creatively integrates coding-theoretic designs inside of Merkle tree commitments to guarantee efficient and tamper-proof reconstruction; this solution is distinct from Asynchronous Verifiable Information Dispersal (in guaranteeing efficient proofs of malformed coding) and Coded Merkle Tree (which only provides guarantees for random corruption as opposed to our guarantees for worst-case corruption). We implement ACeD with full functionality in 6000 lines of Rust code, integrate the functionality as a smart contract into Ethereum via a high-performance implementation demonstrating up to 10,000 transactions per second in throughput and 6000x reduction in gas cost on the Ethereum testnet Kovan.

</details>

<details>

<summary>2021-03-04 07:38:50 - An RL-Based Adaptive Detection Strategy to Secure Cyber-Physical Systems</summary>

- *Ipsita Koley, Sunandan Adhikary, Soumyajit Dey*

- `2103.02872v1` - [abs](http://arxiv.org/abs/2103.02872v1) - [pdf](http://arxiv.org/pdf/2103.02872v1)

> Increased dependence on networked, software based control has escalated the vulnerabilities of Cyber Physical Systems (CPSs). Detection and monitoring components developed leveraging dynamical systems theory are often employed as lightweight security measures for protecting such safety critical CPSs against false data injection attacks. However, existing approaches do not correlate attack scenarios with parameters of detection systems. In the present work, we propose a Reinforcement Learning (RL) based framework which adaptively sets the parameters of such detectors based on experience learned from attack scenarios, maximizing detection rate and minimizing false alarms in the process while attempting performance preserving control actions.

</details>

<details>

<summary>2021-03-04 07:41:12 - BLOCKEYE: Hunting For DeFi Attacks on Blockchain</summary>

- *Bin Wang, Han Liu, Chao Liu, Zhiqiang Yang, Qian Ren, Huixuan Zheng, Hong Lei*

- `2103.02873v1` - [abs](http://arxiv.org/abs/2103.02873v1) - [pdf](http://arxiv.org/pdf/2103.02873v1)

> Decentralized finance, i.e., DeFi, has become the most popular type of application on many public blockchains (e.g., Ethereum) in recent years. Compared to the traditional finance, DeFi allows customers to flexibly participate in diverse blockchain financial services (e.g., lending, borrowing, collateralizing, exchanging etc.) via smart contracts at a relatively low cost of trust. However, the open nature of DeFi inevitably introduces a large attack surface, which is a severe threat to the security of participants funds. In this paper, we proposed BLOCKEYE, a real-time attack detection system for DeFi projects on the Ethereum blockchain. Key capabilities provided by BLOCKEYE are twofold: (1) Potentially vulnerable DeFi projects are identified based on an automatic security analysis process, which performs symbolic reasoning on the data flow of important service states, e.g., asset price, and checks whether they can be externally manipulated. (2) Then, a transaction monitor is installed offchain for a vulnerable DeFi project. Transactions sent not only to that project but other associated projects as well are collected for further security analysis. A potential attack is flagged if a violation is detected on a critical invariant configured in BLOCKEYE, e.g., Benefit is achieved within a very short time and way much bigger than the cost. We applied BLOCKEYE in several popular DeFi projects and managed to discover potential security attacks that are unreported before. A video of BLOCKEYE is available at https://youtu.be/7DjsWBLdlQU.

</details>

<details>

<summary>2021-03-04 20:42:17 - Technical Leverage in a Software Ecosystem: Development Opportunities and Security Risks</summary>

- *Fabio Massacci, Ivan Pashchenko*

- `2103.03317v1` - [abs](http://arxiv.org/abs/2103.03317v1) - [pdf](http://arxiv.org/pdf/2103.03317v1)

> In finance, leverage is the ratio between assets borrowed from others and one's own assets. A matching situation is present in software: by using free open-source software (FOSS) libraries a developer leverages on other people's code to multiply the offered functionalities with a much smaller own codebase. In finance as in software, leverage magnifies profits when returns from borrowing exceed costs of integration, but it may also magnify losses, in particular in the presence of security vulnerabilities. We aim to understand the level of technical leverage in the FOSS ecosystem and whether it can be a potential source of security vulnerabilities. Also, we introduce two metrics change distance and change direction to capture the amount and the evolution of the dependency on third-party libraries.   The application of the proposed metrics on 8494 distinct library versions from the FOSS Maven-based Java libraries shows that small and medium libraries (less than 100KLoC) have disproportionately more leverage on FOSS dependencies in comparison to large libraries. We show that leverage pays off as leveraged libraries only add a 4% delay in the time interval between library releases while providing four times more code than their own. However, libraries with such leverage (i.e., 75% of libraries in our sample) also have 1.6 higher odds of being vulnerable in comparison to the libraries with lower leverage.   We provide an online demo for computing the proposed metrics for real-world software libraries available under the following URL: https://techleverage.eu/.

</details>

<details>

<summary>2021-03-05 02:44:20 - Lord of the Ring(s): Side Channel Attacks on the CPU On-Chip Ring Interconnect Are Practical</summary>

- *Riccardo Paccagnella, Licheng Luo, Christopher W. Fletcher*

- `2103.03443v1` - [abs](http://arxiv.org/abs/2103.03443v1) - [pdf](http://arxiv.org/pdf/2103.03443v1)

> We introduce the first microarchitectural side channel attacks that leverage contention on the CPU ring interconnect. There are two challenges that make it uniquely difficult to exploit this channel. First, little is known about the ring interconnect's functioning and architecture. Second, information that can be learned by an attacker through ring contention is noisy by nature and has coarse spatial granularity. To address the first challenge, we perform a thorough reverse engineering of the sophisticated protocols that handle communication on the ring interconnect. With this knowledge, we build a cross-core covert channel over the ring interconnect with a capacity of over 4 Mbps from a single thread, the largest to date for a cross-core channel not relying on shared memory. To address the second challenge, we leverage the fine-grained temporal patterns of ring contention to infer a victim program's secrets. We demonstrate our attack by extracting key bits from vulnerable EdDSA and RSA implementations, as well as inferring the precise timing of keystrokes typed by a victim user.

</details>

<details>

<summary>2021-03-05 10:46:44 - Store-to-Leak Forwarding: Leaking Data on Meltdown-resistant CPUs (Updated and Extended Version)</summary>

- *Michael Schwarz, Claudio Canella, Lukas Giner, Daniel Gruss*

- `1905.05725v2` - [abs](http://arxiv.org/abs/1905.05725v2) - [pdf](http://arxiv.org/pdf/1905.05725v2)

> Meltdown and Spectre exploit microarchitectural changes the CPU makes during transient out-of-order execution. Using side-channel techniques, these attacks enable leaking arbitrary data from memory. As state-of-the-art software mitigations for Meltdown may incur significant performance overheads, they are only seen as a temporary solution. Thus, software mitigations are disabled on more recent processors, which are not susceptible to Meltdown anymore.   In this paper, we show that Meltdown-like attacks are still possible on recent CPUs which are not vulnerable to the original Meltdown attack. We show that the store buffer - a microarchitectural optimization to reduce the latency for data stores - in combination with the TLB enables powerful attacks. We present several ASLRrelated attacks, including a KASLR break from unprivileged applications, and breaking ASLR from JavaScript. We can also mount side-channel attacks, breaking the atomicity of TSX, and monitoring control flow of the kernel. Furthermore, when combined with a simple Spectre gadget, we can leak arbitrary data from memory. Our paper shows that Meltdown-like attacks are still possible, and software fixes are still necessary to ensure proper isolation between the kernel and user space.   This updated extended version of the original paper includes new results and explanations on the root cause of the vulnerability and shows how it is different to MDS attacks like Fallout.

</details>

<details>

<summary>2021-03-05 13:04:35 - Evaluating the Robustness of Geometry-Aware Instance-Reweighted Adversarial Training</summary>

- *Dorjan Hitaj, Giulio Pagnotta, Iacopo Masi, Luigi V. Mancini*

- `2103.01914v2` - [abs](http://arxiv.org/abs/2103.01914v2) - [pdf](http://arxiv.org/pdf/2103.01914v2)

> In this technical report, we evaluate the adversarial robustness of a very recent method called "Geometry-aware Instance-reweighted Adversarial Training"[7]. GAIRAT reports state-of-the-art results on defenses to adversarial attacks on the CIFAR-10 dataset. In fact, we find that a network trained with this method, while showing an improvement over regular adversarial training (AT), is biasing the model towards certain samples by re-scaling the loss. Indeed, this leads the model to be susceptible to attacks that scale the logits. The original model shows an accuracy of 59% under AutoAttack - when trained with additional data with pseudo-labels. We provide an analysis that shows the opposite. In particular, we craft a PGD attack multiplying the logits by a positive scalar that decreases the GAIRAT accuracy from from 55% to 44%, when trained solely on CIFAR-10. In this report, we rigorously evaluate the model and provide insights into the reasons behind the vulnerability of GAIRAT to this adversarial attack. The code to reproduce our evaluation is made available at https://github.com/giuxhub/GAIRAT-LSA

</details>

<details>

<summary>2021-03-05 14:24:32 - Don't Forget to Sign the Gradients!</summary>

- *Omid Aramoon, Pin-Yu Chen, Gang Qu*

- `2103.03701v1` - [abs](http://arxiv.org/abs/2103.03701v1) - [pdf](http://arxiv.org/pdf/2103.03701v1)

> Engineering a top-notch deep learning model is an expensive procedure that involves collecting data, hiring human resources with expertise in machine learning, and providing high computational resources. For that reason, deep learning models are considered as valuable Intellectual Properties (IPs) of the model vendors. To ensure reliable commercialization of deep learning models, it is crucial to develop techniques to protect model vendors against IP infringements. One of such techniques that recently has shown great promise is digital watermarking. However, current watermarking approaches can embed very limited amount of information and are vulnerable against watermark removal attacks. In this paper, we present GradSigns, a novel watermarking framework for deep neural networks (DNNs). GradSigns embeds the owner's signature into the gradient of the cross-entropy cost function with respect to inputs to the model. Our approach has a negligible impact on the performance of the protected model and it allows model vendors to remotely verify the watermark through prediction APIs. We evaluate GradSigns on DNNs trained for different image classification tasks using CIFAR-10, SVHN, and YTF datasets. Experimental results show that GradSigns is robust against all known counter-watermark attacks and can embed a large amount of information into DNNs.

</details>

<details>

<summary>2021-03-06 09:44:58 - Fine with "1234"? An Analysis of SMS One-Time Password Randomness in Android Apps</summary>

- *Siqi Ma, Juanru Li, Hyoungshick Kim, Elisa Bertino, Surya Nepal, Diethelm Ostry, Cong Sun*

- `2103.05758v1` - [abs](http://arxiv.org/abs/2103.05758v1) - [pdf](http://arxiv.org/pdf/2103.05758v1)

> A fundamental premise of SMS One-Time Password (OTP) is that the used pseudo-random numbers (PRNs) are uniquely unpredictable for each login session. Hence, the process of generating PRNs is the most critical step in the OTP authentication. An improper implementation of the pseudo-random number generator (PRNG) will result in predictable or even static OTP values, making them vulnerable to potential attacks. In this paper, we present a vulnerability study against PRNGs implemented for Android apps. A key challenge is that PRNGs are typically implemented on the server-side, and thus the source code is not accessible. To resolve this issue, we build an analysis tool, \sysname, to assess implementations of the PRNGs in an automated manner without the source code requirement. Through reverse engineering, \sysname identifies the apps using SMS OTP and triggers each app's login functionality to retrieve OTP values. It further assesses the randomness of the OTP values to identify vulnerable PRNGs. By analyzing 6,431 commercially used Android apps downloaded from \tool{Google Play} and \tool{Tencent Myapp}, \sysname identified 399 vulnerable apps that generate predictable OTP values. Even worse, 194 vulnerable apps use the OTP authentication alone without any additional security mechanisms, leading to insecure authentication against guessing attacks and replay attacks.

</details>

<details>

<summary>2021-03-06 15:11:31 - Political audience diversity and news reliability in algorithmic ranking</summary>

- *Saumya Bhadani, Shun Yamaya, Alessandro Flammini, Filippo Menczer, Giovanni Luca Ciampaglia, Brendan Nyhan*

- `2007.08078v2` - [abs](http://arxiv.org/abs/2007.08078v2) - [pdf](http://arxiv.org/pdf/2007.08078v2)

> Newsfeed algorithms frequently amplify misinformation and other low-quality content. How can social media platforms more effectively promote reliable information? Existing approaches are difficult to scale and vulnerable to manipulation. In this paper, we propose using the political diversity of a website's audience as a quality signal. Using news source reliability ratings from domain experts and web browsing data from a diverse sample of 6,890 U.S. citizens, we first show that websites with more extreme and less politically diverse audiences have lower journalistic standards. We then incorporate audience diversity into a standard collaborative filtering framework and show that our improved algorithm increases the trustworthiness of websites suggested to users -- especially those who most frequently consume misinformation -- while keeping recommendations relevant. These findings suggest that partisan audience diversity is a valuable signal of higher journalistic standards that should be incorporated into algorithmic ranking decisions.

</details>

<details>

<summary>2021-03-07 01:29:57 - Threat Modeling of Cyber-Physical Systems in Practice</summary>

- *Ameerah-Muhsinah Jamil, Lotfi ben Othmane, Altaz Valani*

- `2103.04226v1` - [abs](http://arxiv.org/abs/2103.04226v1) - [pdf](http://arxiv.org/pdf/2103.04226v1)

> Traditional Cyber-physical Systems(CPSs) were not built with cybersecurity in mind. They operated on separate Operational Technology (OT) networks. As these systems now become more integrated with Information Technology (IT) networks based on IP, they expose vulnerabilities that can be exploited by the attackers through these IT networks. The attackers can control such systems and cause behavior that jeopardizes the performance and safety measures that were originally designed into the system. In this paper, we explore the approaches to identify threats to CPSs and ensure the quality of the created threat models. The study involves interviews with eleven security experts working in security consultation companies, software engineering companies, an Original Equipment Manufacturer (OEM),and ground and areal vehicles integrators. We found through these interviews that the practitioners use a combination of various threat modeling methods, approaches, and standards together when they perform threat modeling of given CPSs. key challenges practitioners face are: they cannot transfer the threat modeling knowledge that they acquire in a cyber-physical domain to other domains, threat models of modified systems are often not updated, and the reliance on mostly peer-evaluation and quality checklists to ensure the quality of threat models. The study warns about the difficulty to develop secure CPSs and calls for research on developing practical threat modeling methods for CPSs, techniques for continuous threat modeling, and techniques to ensure the quality of threat models.

</details>

<details>

<summary>2021-03-07 08:43:22 - Detecting Adversarial Examples from Sensitivity Inconsistency of Spatial-Transform Domain</summary>

- *Jinyu Tian, Jiantao Zhou, Yuanman Li, Jia Duan*

- `2103.04302v1` - [abs](http://arxiv.org/abs/2103.04302v1) - [pdf](http://arxiv.org/pdf/2103.04302v1)

> Deep neural networks (DNNs) have been shown to be vulnerable against adversarial examples (AEs), which are maliciously designed to cause dramatic model output errors. In this work, we reveal that normal examples (NEs) are insensitive to the fluctuations occurring at the highly-curved region of the decision boundary, while AEs typically designed over one single domain (mostly spatial domain) exhibit exorbitant sensitivity on such fluctuations. This phenomenon motivates us to design another classifier (called dual classifier) with transformed decision boundary, which can be collaboratively used with the original classifier (called primal classifier) to detect AEs, by virtue of the sensitivity inconsistency. When comparing with the state-of-the-art algorithms based on Local Intrinsic Dimensionality (LID), Mahalanobis Distance (MD), and Feature Squeezing (FS), our proposed Sensitivity Inconsistency Detector (SID) achieves improved AE detection performance and superior generalization capabilities, especially in the challenging cases where the adversarial perturbation levels are small. Intensive experimental results on ResNet and VGG validate the superiority of the proposed SID.

</details>

<details>

<summary>2021-03-08 02:18:24 - Improving Global Adversarial Robustness Generalization With Adversarially Trained GAN</summary>

- *Desheng Wang, Weidong Jin, Yunpu Wu, Aamir Khan*

- `2103.04513v1` - [abs](http://arxiv.org/abs/2103.04513v1) - [pdf](http://arxiv.org/pdf/2103.04513v1)

> Convolutional neural networks (CNNs) have achieved beyond human-level accuracy in the image classification task and are widely deployed in real-world environments. However, CNNs show vulnerability to adversarial perturbations that are well-designed noises aiming to mislead the classification models. In order to defend against the adversarial perturbations, adversarially trained GAN (ATGAN) is proposed to improve the adversarial robustness generalization of the state-of-the-art CNNs trained by adversarial training. ATGAN incorporates adversarial training into standard GAN training procedure to remove obfuscated gradients which can lead to a false sense in defending against the adversarial perturbations and are commonly observed in existing GANs-based adversarial defense methods. Moreover, ATGAN adopts the image-to-image generator as data augmentation to increase the sample complexity needed for adversarial robustness generalization in adversarial training. Experimental results in MNIST SVHN and CIFAR-10 datasets show that the proposed method doesn't rely on obfuscated gradients and achieves better global adversarial robustness generalization performance than the adversarially trained state-of-the-art CNNs.

</details>

<details>

<summary>2021-03-08 17:05:09 - Socio-Technical Root Cause Analysis of Cyber-enabled Theft of the U.S. Intellectual Property -- The Case of APT41</summary>

- *Mazaher Kianpour*

- `2103.04901v1` - [abs](http://arxiv.org/abs/2103.04901v1) - [pdf](http://arxiv.org/pdf/2103.04901v1)

> Increased connectivity has made us all more vulnerable. Cyberspace, besides all its benefits, spawned more devices to hack and more opportunities to commit cybercrime. Criminals have found it lucrative to target both individuals and businesses, by holding or stealing their assets via different types of cyber attacks. The cyber-enabled theft of Intellectual Property (IP), as one of the most important and critical intangible assets of nations, organizations and individuals, by foreign countries has been a devastating challenge of the United States (U.S.) in the past decades. In this study, we conduct a socio-technical root cause analysis to investigate one of the recent cases of IP theft by employing a holistic approach. It concludes with a list of root causes and some corrective actions to stop the impact and prevent the recurrence of the problem in the future. Building upon the findings of this study, the U.S. requires a detailed revision of IP strategies bringing the whole socio-technical regulatory system into focus and strengthen IP rights protection considering China's indigenous innovation policies. It is critical that businesses and other organizations take steps to reduce their exposure to cyber attacks. It is particularly important to train employees on how to spot potential threats, and to institute policies that encourage workers to report potential security failures so that action can be taken quickly. Finally, we discuss how cyber ranges can provide an efficient and safe platform for dealing with such challenges. The results of this study can be expanded to other countries in order to protect their IP rights and deter or prevent and respond to future incidents.

</details>

<details>

<summary>2021-03-08 21:39:14 - Structuring a Comprehensive Software Security Course Around the OWASP Application Security Verification Standard</summary>

- *Sarah Elder, Nusrat Zahan, Val Kozarev, Rui Shu, Tim Menzies, Laurie Williams*

- `2103.05088v1` - [abs](http://arxiv.org/abs/2103.05088v1) - [pdf](http://arxiv.org/pdf/2103.05088v1)

> Lack of security expertise among software practitioners is a problem with many implications. First, there is a deficit of security professionals to meet current needs. Additionally, even practitioners who do not plan to work in security may benefit from increased understanding of security. The goal of this paper is to aid software engineering educators in designing a comprehensive software security course by sharing an experience running a software security course for the eleventh time. Through all the eleven years of running the software security course, the course objectives have been comprehensive - ranging from security testing, to secure design and coding, to security requirements to security risk management. For the first time in this eleventh year, a theme of the course assignments was to map vulnerability discovery to the security controls of the Open Web Application Security Project (OWASP) Application Security Verification Standard (ASVS). Based upon student performance on a final exploratory penetration testing project, this mapping may have increased students' depth of understanding of a wider range of security topics. The students efficiently detected 191 unique and verified vulnerabilities of 28 different Common Weakness Enumeration (CWE) types during a three-hour period in the OpenMRS project, an electronic health record application in active use.

</details>

<details>

<summary>2021-03-09 01:03:03 - Vulnerability Detection is Just the Beginning</summary>

- *Sarah Elder*

- `2103.05160v1` - [abs](http://arxiv.org/abs/2103.05160v1) - [pdf](http://arxiv.org/pdf/2103.05160v1)

> Vulnerability detection plays a key role in secure software development. There are many different vulnerability detection tools and techniques to choose from, and insufficient information on which vulnerability detection techniques to use and when. The goal of this research is to assist managers and other decision-makers on software projects in making informed choices about the use of different software vulnerability detection techniques through empirical analysis of the efficiency and effectiveness of each technique. We will examine the relationships between the vulnerability detection technique used to find a vulnerability, the type of vulnerability found, the exploitability of the vulnerability, and the effort needed to fix a vulnerability on two projects where we ensure all vulnerabilities found have been fixed. We will then examine how these relationships are seen in Open Source Software more broadly where practitioners may use different vulnerability detection techniques, or may not fix all vulnerabilities found due to resource constraints.

</details>

<details>

<summary>2021-03-09 05:40:30 - Stabilized Medical Image Attacks</summary>

- *Gege Qi, Lijun Gong, Yibing Song, Kai Ma, Yefeng Zheng*

- `2103.05232v1` - [abs](http://arxiv.org/abs/2103.05232v1) - [pdf](http://arxiv.org/pdf/2103.05232v1)

> Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.

</details>

<details>

<summary>2021-03-09 10:39:11 - CrypTag: Thwarting Physical and Logical Memory Vulnerabilities using Cryptographically Colored Memory</summary>

- *Pascal Nasahl, Robert Schilling, Mario Werner, Jan Hoogerbrugge, Marcel Medwed, Stefan Mangard*

- `2012.06761v2` - [abs](http://arxiv.org/abs/2012.06761v2) - [pdf](http://arxiv.org/pdf/2012.06761v2)

> Memory vulnerabilities are a major threat to many computing systems. To effectively thwart spatial and temporal memory vulnerabilities, full logical memory safety is required. However, current mitigation techniques for memory safety are either too expensive or trade security against efficiency. One promising attempt to detect memory safety vulnerabilities in hardware is memory coloring, a security policy deployed on top of tagged memory architectures. However, due to the memory storage and bandwidth overhead of large tags, commodity tagged memory architectures usually only provide small tag sizes, thus limiting their use for security applications. Irrespective of logical memory safety, physical memory safety is a necessity in hostile environments prevalent for modern cloud computing and IoT devices. Architectures from Intel and AMD already implement transparent memory encryption to maintain confidentiality and integrity of all off-chip data. Surprisingly, the combination of both, logical and physical memory safety, has not yet been extensively studied in previous research, and a naive combination of both security strategies would accumulate both overheads. In this paper, we propose CrypTag, an efficient hardware/software co-design mitigating a large class of logical memory safety issues and providing full physical memory safety. At its core, CrypTag utilizes a transparent memory encryption engine not only for physical memory safety, but also for memory coloring at hardly any additional costs. The design avoids any overhead for tag storage by embedding memory colors in the upper bits of a pointer and using these bits as an additional input for the memory encryption. A custom compiler extension automatically leverages CrypTag to detect logical memory safety issues for commodity programs and is fully backward compatible.

</details>

<details>

<summary>2021-03-09 10:59:13 - HECTOR-V: A Heterogeneous CPU Architecture for a Secure RISC-V Execution Environment</summary>

- *Pascal Nasahl, Robert Schilling, Mario Werner, Stefan Mangard*

- `2009.05262v3` - [abs](http://arxiv.org/abs/2009.05262v3) - [pdf](http://arxiv.org/pdf/2009.05262v3)

> To ensure secure and trustworthy execution of applications, vendors frequently embed trusted execution environments into their systems. Here, applications are protected from adversaries, including a malicious operating system. TEEs are usually built by integrating protection mechanisms directly into the processor or by using dedicated external secure elements. However, both of these approaches only cover a narrow threat model resulting in limited security guarantees. Enclaves in the application processor typically provide weak isolation between the secure and non-secure domain, especially when considering side-channel attacks. Although secure elements do provide strong isolation, the slow communication interface to the application processor is exposed to adversaries and restricts the use cases. Independently of the used implementation approach, TEEs often lack the possibility to establish secure communication to external peripherals, and most operating systems executed inside TEEs do not provide state-of-the-art defense strategies, making them vulnerable against various attacks. We argue that TEEs implemented on the main application processor are insecure, especially when considering side-channel attacks. We demonstrate how a heterogeneous architecture can be utilized to realize a secure TEE design. We directly embed a processor into our architecture to provide strong isolation between the secure and non-secure domain. The tight coupling of TEE and REE enables HECTOR-V to provide mechanisms for establishing secure communication channels. We further introduce RISC-V Secure Co-Processor, a security-hardened processor tailored for TEEs. To secure applications executed inside the TEE, RVSCP provides control-flow integrity, rigorously restricts I/O accesses to certain execution states, and provides operating system services directly in hardware.

</details>

<details>

<summary>2021-03-09 13:53:52 - "What's in the box?!": Deflecting Adversarial Attacks by Randomly Deploying Adversarially-Disjoint Models</summary>

- *Sahar Abdelnabi, Mario Fritz*

- `2102.05104v2` - [abs](http://arxiv.org/abs/2102.05104v2) - [pdf](http://arxiv.org/pdf/2102.05104v2)

> Machine learning models are now widely deployed in real-world applications. However, the existence of adversarial examples has been long considered a real threat to such models. While numerous defenses aiming to improve the robustness have been proposed, many have been shown ineffective. As these vulnerabilities are still nowhere near being eliminated, we propose an alternative deployment-based defense paradigm that goes beyond the traditional white-box and black-box threat models. Instead of training a single partially-robust model, one could train a set of same-functionality, yet, adversarially-disjoint models with minimal in-between attack transferability. These models could then be randomly and individually deployed, such that accessing one of them minimally affects the others. Our experiments on CIFAR-10 and a wide range of attacks show that we achieve a significantly lower attack transferability across our disjoint models compared to a baseline of ensemble diversity. In addition, compared to an adversarially trained set, we achieve a higher average robust accuracy while maintaining the accuracy of clean examples.

</details>

<details>

<summary>2021-03-10 14:44:19 - Blindspots in Python and Java APIs Result in Vulnerable Code</summary>

- *Yuriy Brun, Tian Lin, Jessie Elise Somerville, Elisha Myers, Natalie C. Ebner*

- `2103.06091v1` - [abs](http://arxiv.org/abs/2103.06091v1) - [pdf](http://arxiv.org/pdf/2103.06091v1)

> Blindspots in APIs can cause software engineers to introduce vulnerabilities, but such blindspots are, unfortunately, common. We study the effect APIs with blindspots have on developers in two languages by replicating an 109-developer, 24-Java-API controlled experiment. Our replication applies to Python and involves 129 new developers and 22 new APIs. We find that using APIs with blindspots statistically significantly reduces the developers' ability to correctly reason about the APIs in both languages, but that the effect is more pronounced for Python. Interestingly, for Java, the effect increased with complexity of the code relying on the API, whereas for Python, the opposite was true. Whether the developers considered API uses to be more difficult, less clear, and less familiar did not have an effect on their ability to correctly reason about them. Developers with better long-term memory recall were more likely to correctly reason about APIs with blindspots, but short-term memory, processing speed, episodic memory, and memory span had no effect. Surprisingly, professional experience and expertice did not improve the developers' ability to reason about APIs with blindspots across both languages, with long-term professionals with many years of experience making mistakes as often as relative novices. Finally, personality traits did not significantly affect the Python developers' ability to reason about APIs with blindspots, but less extraverted and more open developers were better at reasoning about Java APIs with blindspots. Overall, our findings suggest that blindspots in APIs are a serious problem across languages, and that experience and education alone do not overcome that problem, suggesting that tools are needed to help developers recognize blindspots in APIs as they write code that uses those APIs.

</details>

<details>

<summary>2021-03-11 01:02:24 - T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification</summary>

- *Ahmadreza Azizi, Ibrahim Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng Pu, Mobin Javed, Chandan K. Reddy, Bimal Viswanath*

- `2103.04264v2` - [abs](http://arxiv.org/abs/2103.04264v2) - [pdf](http://arxiv.org/pdf/2103.04264v2)

> Deep Neural Network (DNN) classifiers are known to be vulnerable to Trojan or backdoor attacks, where the classifier is manipulated such that it misclassifies any input containing an attacker-determined Trojan trigger. Backdoors compromise a model's integrity, thereby posing a severe threat to the landscape of DNN-based classification. While multiple defenses against such attacks exist for classifiers in the image domain, there have been limited efforts to protect classifiers in the text domain.   We present Trojan-Miner (T-Miner) -- a defense framework for Trojan attacks on DNN-based text classifiers. T-Miner employs a sequence-to-sequence (seq-2-seq) generative model that probes the suspicious classifier and learns to produce text sequences that are likely to contain the Trojan trigger. T-Miner then analyzes the text produced by the generative model to determine if they contain trigger phrases, and correspondingly, whether the tested classifier has a backdoor. T-Miner requires no access to the training dataset or clean inputs of the suspicious classifier, and instead uses synthetically crafted "nonsensical" text inputs to train the generative model. We extensively evaluate T-Miner on 1100 model instances spanning 3 ubiquitous DNN model architectures, 5 different classification tasks, and a variety of trigger phrases. We show that T-Miner detects Trojan and clean models with a 98.75% overall accuracy, while achieving low false positives on clean models. We also show that T-Miner is robust against a variety of targeted, advanced attacks from an adaptive attacker.

</details>

<details>

<summary>2021-03-12 08:32:40 - A Law of Robustness for Weight-bounded Neural Networks</summary>

- *Hisham Husain, Borja Balle*

- `2102.08093v2` - [abs](http://arxiv.org/abs/2102.08093v2) - [pdf](http://arxiv.org/pdf/2102.08093v2)

> Robustness of deep neural networks against adversarial perturbations is a pressing concern motivated by recent findings showing the pervasive nature of such vulnerabilities. One method of characterizing the robustness of a neural network model is through its Lipschitz constant, which forms a robustness certificate. A natural question to ask is, for a fixed model class (such as neural networks) and a dataset of size $n$, what is the smallest achievable Lipschitz constant among all models that fit the dataset? Recently, (Bubeck et al., 2020) conjectured that when using two-layer networks with $k$ neurons to fit a generic dataset, the smallest Lipschitz constant is $\Omega(\sqrt{\frac{n}{k}})$. This implies that one would require one neuron per data point to robustly fit the data. In this work we derive a lower bound on the Lipschitz constant for any arbitrary model class with bounded Rademacher complexity. Our result coincides with that conjectured in (Bubeck et al., 2020) for two-layer networks under the assumption of bounded weights. However, due to our result's generality, we also derive bounds for multi-layer neural networks, discovering that one requires $\log n$ constant-sized layers to robustly fit the data. Thus, our work establishes a law of robustness for weight bounded neural networks and provides formal evidence on the necessity of over-parametrization in deep learning.

</details>

<details>

<summary>2021-03-12 09:52:28 - Audio-Visual Biometric Recognition and Presentation Attack Detection: A Comprehensive Survey</summary>

- *Hareesh Mandalapu, P N Aravinda Reddy, Raghavendra Ramachandra, K Sreenivasa Rao, Pabitra Mitra, S R Mahadeva Prasanna, Christoph Busch*

- `2101.09725v2` - [abs](http://arxiv.org/abs/2101.09725v2) - [pdf](http://arxiv.org/pdf/2101.09725v2)

> Biometric recognition is a trending technology that uses unique characteristics data to identify or verify/authenticate security applications. Amidst the classically used biometrics, voice and face attributes are the most propitious for prevalent applications in day-to-day life because they are easy to obtain through restrained and user-friendly procedures. The pervasiveness of low-cost audio and face capture sensors in smartphones, laptops, and tablets has made the advantage of voice and face biometrics more exceptional when compared to other biometrics. For many years, acoustic information alone has been a great success in automatic speaker verification applications. Meantime, the last decade or two has also witnessed a remarkable ascent in face recognition technologies. Nonetheless, in adverse unconstrained environments, neither of these techniques achieves optimal performance. Since audio-visual information carries correlated and complementary information, integrating them into one recognition system can increase the system's performance. The vulnerability of biometrics towards presentation attacks and audio-visual data usage for the detection of such attacks is also a hot topic of research. This paper made a comprehensive survey on existing state-of-the-art audio-visual recognition techniques, publicly available databases for benchmarking, and Presentation Attack Detection (PAD) algorithms. Further, a detailed discussion on challenges and open problems is presented in this field of biometrics.

</details>

<details>

<summary>2021-03-12 22:27:23 - Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification</summary>

- *Xiaochen Hou, Peng Qi, Guangtao Wang, Rex Ying, Jing Huang, Xiaodong He, Bowen Zhou*

- `2103.11794v1` - [abs](http://arxiv.org/abs/2103.11794v1) - [pdf](http://arxiv.org/pdf/2103.11794v1)

> Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks(GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from differ-ent parsers. Instead of assigning one set of model parameters to each dependency tree, we first combine the dependency relations from different parses before applying GNNs over the resulting graph. This allows GNN mod-els to be robust to parse errors at no additional computational cost, and helps avoid overparameterization and overfitting from GNN layer stacking by introducing more connectivity into the ensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter datasets show that our GraphMerge model not only outperforms models with single dependency tree, but also beats other ensemble mod-els without adding model parameters.

</details>

<details>

<summary>2021-03-13 02:03:53 - Learning Defense Transformers for Counterattacking Adversarial Examples</summary>

- *Jincheng Li, Jiezhang Cao, Yifan Zhang, Jian Chen, Mingkui Tan*

- `2103.07595v1` - [abs](http://arxiv.org/abs/2103.07595v1) - [pdf](http://arxiv.org/pdf/2103.07595v1)

> Deep neural networks (DNNs) are vulnerable to adversarial examples with small perturbations. Adversarial defense thus has been an important means which improves the robustness of DNNs by defending against adversarial examples. Existing defense methods focus on some specific types of adversarial examples and may fail to defend well in real-world applications. In practice, we may face many types of attacks where the exact type of adversarial examples in real-world applications can be even unknown. In this paper, motivated by that adversarial examples are more likely to appear near the classification boundary, we study adversarial examples from a new perspective that whether we can defend against adversarial examples by pulling them back to the original clean distribution. We theoretically and empirically verify the existence of defense affine transformations that restore adversarial examples. Relying on this, we learn a defense transformer to counterattack the adversarial examples by parameterizing the affine transformations and exploiting the boundary information of DNNs. Extensive experiments on both toy and real-world datasets demonstrate the effectiveness and generalization of our defense transformer.

</details>

<details>

<summary>2021-03-13 06:29:13 - Attack as Defense: Characterizing Adversarial Examples using Robustness</summary>

- *Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, Jun Sun*

- `2103.07633v1` - [abs](http://arxiv.org/abs/2103.07633v1) - [pdf](http://arxiv.org/pdf/2103.07633v1)

> As a new programming paradigm, deep learning has expanded its application to many real-world problems. At the same time, deep learning based software are found to be vulnerable to adversarial attacks. Though various defense mechanisms have been proposed to improve robustness of deep learning software, many of them are ineffective against adaptive attacks. In this work, we propose a novel characterization to distinguish adversarial examples from benign ones based on the observation that adversarial examples are significantly less robust than benign ones. As existing robustness measurement does not scale to large networks, we propose a novel defense framework, named attack as defense (A2D), to detect adversarial examples by effectively evaluating an example's robustness. A2D uses the cost of attacking an input for robustness evaluation and identifies those less robust examples as adversarial since less robust examples are easier to attack. Extensive experiment results on MNIST, CIFAR10 and ImageNet show that A2D is more effective than recent promising approaches. We also evaluate our defence against potential adaptive attacks and show that A2D is effective in defending carefully designed adaptive attacks, e.g., the attack success rate drops to 0% on CIFAR10.

</details>

<details>

<summary>2021-03-13 07:20:14 - Generating Unrestricted Adversarial Examples via Three Parameters</summary>

- *Hanieh Naderi, Leili Goli, Shohreh Kasaei*

- `2103.07640v1` - [abs](http://arxiv.org/abs/2103.07640v1) - [pdf](http://arxiv.org/pdf/2103.07640v1)

> Deep neural networks have been shown to be vulnerable to adversarial examples deliberately constructed to misclassify victim models. As most adversarial examples have restricted their perturbations to $L_{p}$-norm, existing defense methods have focused on these types of perturbations and less attention has been paid to unrestricted adversarial examples; which can create more realistic attacks, able to deceive models without affecting human predictions. To address this problem, the proposed adversarial attack generates an unrestricted adversarial example with a limited number of parameters. The attack selects three points on the input image and based on their locations transforms the image into an adversarial example. By limiting the range of movement and location of these three points and using a discriminatory network, the proposed unrestricted adversarial example preserves the image appearance. Experimental results show that the proposed adversarial examples obtain an average success rate of 93.5% in terms of human evaluation on the MNIST and SVHN datasets. It also reduces the model accuracy by an average of 73% on six datasets MNIST, FMNIST, SVHN, CIFAR10, CIFAR100, and ImageNet. It should be noted that, in the case of attacks, lower accuracy in the victim model denotes a more successful attack. The adversarial train of the attack also improves model robustness against a randomly transformed image.

</details>

<details>

<summary>2021-03-13 12:17:47 - Simeon -- Secure Federated Machine Learning Through Iterative Filtering</summary>

- *Nicholas Malecki, Hye-young Paik, Aleksandar Ignjatovic, Alan Blair, Elisa Bertino*

- `2103.07704v1` - [abs](http://arxiv.org/abs/2103.07704v1) - [pdf](http://arxiv.org/pdf/2103.07704v1)

> Federated learning enables a global machine learning model to be trained collaboratively by distributed, mutually non-trusting learning agents who desire to maintain the privacy of their training data and their hardware. A global model is distributed to clients, who perform training, and submit their newly-trained model to be aggregated into a superior model. However, federated learning systems are vulnerable to interference from malicious learning agents who may desire to prevent training or induce targeted misclassification in the resulting global model. A class of Byzantine-tolerant aggregation algorithms has emerged, offering varying degrees of robustness against these attacks, often with the caveat that the number of attackers is bounded by some quantity known prior to training. This paper presents Simeon: a novel approach to aggregation that applies a reputation-based iterative filtering technique to achieve robustness even in the presence of attackers who can exhibit arbitrary behaviour. We compare Simeon to state-of-the-art aggregation techniques and find that Simeon achieves comparable or superior robustness to a variety of attacks. Notably, we show that Simeon is tolerant to sybil attacks, where other algorithms are not, presenting a key advantage of our approach.

</details>

<details>

<summary>2021-03-13 19:31:59 - Adversarial Machine Learning Attacks and Defense Methods in the Cyber Security Domain</summary>

- *Ihai Rosenberg, Asaf Shabtai, Yuval Elovici, Lior Rokach*

- `2007.02407v3` - [abs](http://arxiv.org/abs/2007.02407v3) - [pdf](http://arxiv.org/pdf/2007.02407v3)

> In recent years machine learning algorithms, and more specifically deep learning algorithms, have been widely used in many fields, including cyber security. However, machine learning systems are vulnerable to adversarial attacks, and this limits the application of machine learning, especially in non-stationary, adversarial environments, such as the cyber security domain, where actual adversaries (e.g., malware developers) exist. This paper comprehensively summarizes the latest research on adversarial attacks against security solutions based on machine learning techniques and illuminates the risks they pose. First, the adversarial attack methods are characterized based on their stage of occurrence, and the attacker's goals and capabilities. Then, we categorize the applications of adversarial attack and defense methods in the cyber security domain. Finally, we highlight some characteristics identified in recent research and discuss the impact of recent advancements in other adversarial learning domains on future research directions in the cyber security domain. This paper is the first to discuss the unique challenges of implementing end-to-end adversarial attacks in the cyber security domain, map them in a unified taxonomy, and use the taxonomy to highlight future research directions.

</details>

<details>

<summary>2021-03-15 04:56:31 - Contextualized Perturbation for Textual Adversarial Attack</summary>

- *Dianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris Brockett, Ming-Ting Sun, Bill Dolan*

- `2009.07502v2` - [abs](http://arxiv.org/abs/2009.07502v2) - [pdf](http://arxiv.org/pdf/2009.07502v2)

> Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, allowing for generating outputs of varied lengths. With a richer range of available strategies, CLARE is able to attack a victim model more efficiently with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality.

</details>

<details>

<summary>2021-03-15 15:05:01 - Formal Modelling and Security Analysis of Bitcoin's Payment Protocol</summary>

- *Paolo Modesti, Siamak F. Shahandashti, Patrick McCorry, Feng Hao*

- `2103.08436v1` - [abs](http://arxiv.org/abs/2103.08436v1) - [pdf](http://arxiv.org/pdf/2103.08436v1)

> The Payment Protocol standard BIP70, specifying how payments in Bitcoin are performed by merchants and customers, is supported by the largest payment processors and most widely-used wallets. The protocol has been shown to be vulnerable to refund attacks due to lack of authentication of the refund addresses. In this paper, we give the first formal model of the protocol and formalise the refund address security goals for the protocol, namely refund address authentication and secrecy. The formal model utilises communication channels as abstractions conveying security goals on which the protocol modeller and verifier can rely. We analyse the Payment Protocol confirming that it is vulnerable to an attack violating the refund address authentication security goal. Moreover, we present a concrete protocol revision proposal supporting the merchant with publicly verifiable evidence that can mitigate the attack. We verify that the revised protocol meets the security goals defined for the refund address. Hence, we demonstrate that the revised protocol is secure, not only against the existing attacks, but also against any further attacks violating the formalised security goals.

</details>

<details>

<summary>2021-03-16 14:51:53 - EtherSolve: Computing an Accurate Control-Flow Graph from Ethereum Bytecode</summary>

- *Filippo Contro, Marco Crosara, Mariano Ceccato, Mila Dalla Preda*

- `2103.09113v1` - [abs](http://arxiv.org/abs/2103.09113v1) - [pdf](http://arxiv.org/pdf/2103.09113v1)

> Motivated by the immutable nature of Ethereum smart contracts and of their transactions, quite many approaches have been proposed to detect defects and security problems before smart contracts become persistent in the blockchain and they are granted control on substantial financial value.   Because smart contracts source code might not be available, static analysis approaches mostly face the challenge of analysing compiled Ethereum bytecode, that is available directly from the official blockchain. However, due to the intrinsic complexity of Ethereum bytecode (especially in jump resolution), static analysis encounters significant obstacles that reduce the accuracy of exiting automated tools.   This paper presents a novel static analysis algorithm based on the symbolic execution of the Ethereum operand stack that allows us to resolve jumps in Ethereum bytecode and to construct an accurate control-flow graph (CFG) of the compiled smart contracts. EtherSolve is a prototype implementation of our approach. Experimental results on a significant set of real world Ethereum smart contracts show that EtherSolve improves the accuracy of the execrated CFGs with respect to the state of the art available approaches.   Many static analysis techniques are based on the CFG representation of the code and would therefore benefit from the accurate extraction of the CFG. For example, we implemented a simple extension of EtherSolve that allows to detect instances of the re-entrancy vulnerability.

</details>

<details>

<summary>2021-03-16 18:20:29 - Bio-inspired Robustness: A Review</summary>

- *Harshitha Machiraju, Oh-Hyeon Choung, Pascal Frossard, Michael. H Herzog*

- `2103.09265v1` - [abs](http://arxiv.org/abs/2103.09265v1) - [pdf](http://arxiv.org/pdf/2103.09265v1)

> Deep convolutional neural networks (DCNNs) have revolutionized computer vision and are often advocated as good models of the human visual system. However, there are currently many shortcomings of DCNNs, which preclude them as a model of human vision. For example, in the case of adversarial attacks, where adding small amounts of noise to an image, including an object, can lead to strong misclassification of that object. But for humans, the noise is often invisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot be taken as serious models of human vision. Many studies have tried to add features of the human visual system to DCNNs to make them robust against adversarial attacks. However, it is not fully clear whether human vision inspired components increase robustness because performance evaluations of these novel components in DCNNs are often inconclusive. We propose a set of criteria for proper evaluation and analyze different models according to these criteria. We finally sketch future efforts to make DCCNs one step closer to the model of human vision.

</details>

<details>

<summary>2021-03-16 21:17:51 - An Attacker Modeling Framework for the Assessment of Cyber-Physical Systems Security</summary>

- *Christopher Deloglos, Carl Elks, Ashraf Tantawy*

- `2006.03930v2` - [abs](http://arxiv.org/abs/2006.03930v2) - [pdf](http://arxiv.org/pdf/2006.03930v2)

> Characterizing attacker behavior with respect to Cyber-Physical Systems is important to assuring the security posture and resilience of these systems. Classical cyber vulnerability assessment approaches rely on the knowledge and experience of cyber-security experts to conduct security analyses and can be inconsistent where the experts' knowledge and experience are lacking. This paper proposes a flexible attacker modeling framework that aids in the security analysis process by simulating a diverse set of attacker behaviors to predict attack progression and provide consistent system vulnerability analysis. The model proposes an expanded architecture of vulnerability databases to maximize its effectiveness and consistency in detecting CPS vulnerabilities while being compatible with existing vulnerability databases. The model has the power to be implemented and simulated against an actual or virtual CPS. Execution of the attacker model is demonstrated against a simulated industrial control system architecture, resulting in a probabilistic prediction of attacker behavior.

</details>

<details>

<summary>2021-03-17 02:21:05 - The Manufacture of Partisan Echo Chambers by Follow Train Abuse on Twitter</summary>

- *Christopher Torres-Lugo, Kai-Cheng Yang, Filippo Menczer*

- `2010.13691v2` - [abs](http://arxiv.org/abs/2010.13691v2) - [pdf](http://arxiv.org/pdf/2010.13691v2)

> A growing body of evidence points to critical vulnerabilities of social media, such as the emergence of partisan echo chambers and the viral spread of misinformation. We show that these vulnerabilities are amplified by abusive behaviors associated with so-called "follow trains" on Twitter, in which long lists of like-minded accounts are mentioned for others to follow. We present the first systematic analysis of a large U.S. hyper-partisan train network. We observe an artificial inflation of influence: accounts heavily promoted by follow trains profit from a median six-fold increase in daily follower growth. This catalyzes the formation of highly clustered echo chambers, hierarchically organized around a dense core of active accounts. Train accounts also engage in other behaviors that violate platform policies: we find evidence of activity by inauthentic automated accounts and abnormal content deletion, as well as amplification of toxic content from low-credibility and conspiratorial sources. Some train accounts have been active for years, suggesting that platforms need to pay greater attention to this kind of abuse.

</details>

<details>

<summary>2021-03-17 09:47:10 - A Bayes-Optimal View on Adversarial Examples</summary>

- *Eitan Richardson, Yair Weiss*

- `2002.08859v2` - [abs](http://arxiv.org/abs/2002.08859v2) - [pdf](http://arxiv.org/pdf/2002.08859v2)

> Since the discovery of adversarial examples - the ability to fool modern CNN classifiers with tiny perturbations of the input, there has been much discussion whether they are a "bug" that is specific to current neural architectures and training methods or an inevitable "feature" of high dimensional geometry. In this paper, we argue for examining adversarial examples from the perspective of Bayes-Optimal classification. We construct realistic image datasets for which the Bayes-Optimal classifier can be efficiently computed and derive analytic conditions on the distributions under which these classifiers are provably robust against any adversarial attack even in high dimensions. Our results show that even when these "gold standard" optimal classifiers are robust, CNNs trained on the same datasets consistently learn a vulnerable classifier, indicating that adversarial examples are often an avoidable "bug". We further show that RBF SVMs trained on the same data consistently learn a robust classifier. The same trend is observed in experiments with real images in different datasets.

</details>

<details>

<summary>2021-03-17 12:23:21 - Assessing Smart Contracts Security Technical Debts</summary>

- *Sabreen Ahmadjee, Carlos Mera-Gómez, Rami Bahsoon*

- `2103.09595v1` - [abs](http://arxiv.org/abs/2103.09595v1) - [pdf](http://arxiv.org/pdf/2103.09595v1)

> Smart contracts are self-enforcing agreements that are employed to exchange assets without the approval of trusted third parties. This feature has encouraged various sectors to make use of smart contracts when transacting. Experience shows that many deployed contracts are vulnerable to exploitation due to their poor design, which allows attackers to steal valuable assets from the involved parties. Therefore, an assessment approach that allows developers to recognise the consequences of deploying vulnerable contracts is needed. In this paper, we propose a debt-aware approach for assessing security design vulnerabilities in smart contracts. Our assessment approach involves two main steps: (i) identification of design vulnerabilities using security analysis techniques and (ii) an estimation of the ramifications of the identified vulnerabilities leveraging the technical debt metaphor, its principal and interest. We use examples of vulnerable contracts to demonstrate the applicability of our approach. The results show that our assessment approach increases the visibility of security design issues. It also allows developers to concentrate on resolving smart contract vulnerabilities through technical debt impact analysis and prioritisation. Developers can use our approach to inform the design of more secure contracts and for reducing unintentional debts caused by a lack of awareness of security issues.

</details>

<details>

<summary>2021-03-17 14:49:12 - DAFAR: Defending against Adversaries by Feedback-Autoencoder Reconstruction</summary>

- *Haowen Liu, Ping Yi, Hsiao-Ying Lin, Jie Shi, Weidong Qiu*

- `2103.06487v2` - [abs](http://arxiv.org/abs/2103.06487v2) - [pdf](http://arxiv.org/pdf/2103.06487v2)

> Deep learning has shown impressive performance on challenging perceptual tasks and has been widely used in software to provide intelligent services. However, researchers found deep neural networks vulnerable to adversarial examples. Since then, many methods are proposed to defend against adversaries in inputs, but they are either attack-dependent or shown to be ineffective with new attacks. And most of existing techniques have complicated structures or mechanisms that cause prohibitively high overhead or latency, impractical to apply on real software.   We propose DAFAR, a feedback framework that allows deep learning models to detect/purify adversarial examples in high effectiveness and universality, with low area and time overhead. DAFAR has a simple structure, containing a victim model, a plug-in feedback network, and a detector. The key idea is to import the high-level features from the victim model's feature extraction layers into the feedback network to reconstruct the input. This data stream forms a feedback autoencoder. For strong attacks, it transforms the imperceptible attack on the victim model into the obvious reconstruction-error attack on the feedback autoencoder directly, which is much easier to detect; for weak attacks, the reformation process destroys the structure of adversarial examples. Experiments are conducted on MNIST and CIFAR-10 data-sets, showing that DAFAR is effective against popular and arguably most advanced attacks without losing performance on legitimate samples, with high effectiveness and universality across attack methods and parameters.

</details>

<details>

<summary>2021-03-17 15:49:33 - An Analysis of Security Vulnerabilities in Container Images for Scientific Data Analysis</summary>

- *Bhupinder Kaur, Mathieu Dugré, Aiman Hanna, Tristan Glatard*

- `2010.13970v2` - [abs](http://arxiv.org/abs/2010.13970v2) - [pdf](http://arxiv.org/pdf/2010.13970v2)

> Software containers greatly facilitate the deployment and reproducibility of scientific data analyses in various platforms. However, container images often contain outdated or unnecessary software packages, which increases the number of security vulnerabilities in the images, widens the attack surface in the container host, and creates substantial security risks for computing infrastructures at large. This paper presents a vulnerability analysis of container images for scientific data analysis. We compare results obtained with four vulnerability scanners, focusing on the use case of neuroscience data analysis, and quantifying the effect of image update and minification on the number of vulnerabilities. We find that container images used for neuroscience data analysis contain hundreds of vulnerabilities, that software updates remove about two thirds of these vulnerabilities, and that removing unused packages is also effective. We conclude with recommendations on how to build container images with a reduced amount of vulnerabilities.

</details>

<details>

<summary>2021-03-18 01:36:39 - How to Better Distinguish Security Bug Reports (using Dual Hyperparameter Optimization</summary>

- *Rui Shu, Tianpei Xia, Jianfeng Chen, Laurie Williams, Tim Menzies*

- `1911.02476v3` - [abs](http://arxiv.org/abs/1911.02476v3) - [pdf](http://arxiv.org/pdf/1911.02476v3)

> Background: In order that the general public is not vulnerable to hackers, security bug reports need to be handled by small groups of engineers before being widely discussed. But learning how to distinguish the security bug reports from other bug reports is challenging since they may occur rarely. Data mining methods that can find such scarce targets require extensive optimization effort.   Goal: The goal of this research is to aid practitioners as they struggle to optimize methods that try to distinguish between rare security bug reports and other bug reports.   Method: Our proposed method, called Swift, is a dual optimizer that optimizes both learner and pre-processor options. Since this is a large space of options, Swift uses a technique called epsilon-dominance that learns how to avoid operations that do not significantly improve performance.   Result: When compared to recent state-of-the-art results (from FARSEC which is published in TSE'18), we find that the Swift's dual optimization of both pre-processor and learner is more useful than optimizing each of them individually. For example, in a study of security bug reports from the Chromium dataset, the median recalls of FARSEC and Swift were 15.7% and 77.4%, respectively. For another example, in experiments with data from the Ambari project, the median recalls improved from 21.5% to 85.7% (FARSEC to SWIFT).   Conclusion: Overall, our approach can quickly optimize models that achieve better recalls than the prior state-of-the-art. These increases in recall are associated with moderate increases in false positive rates (from 8% to 24%, median). For future work, these results suggest that dual optimization is both practical and useful.

</details>

<details>

<summary>2021-03-18 04:23:21 - Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!</summary>

- *Xuanli He, Lingjuan Lyu, Qiongkai Xu, Lichao Sun*

- `2103.10013v1` - [abs](http://arxiv.org/abs/2103.10013v1) - [pdf](http://arxiv.org/pdf/2103.10013v1)

> Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pre-trained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model and find that unless the performance of the victim model is sacrificed, both model ex-traction and adversarial transferability can effectively compromise the target models

</details>

<details>

<summary>2021-03-18 10:35:33 - Explainable Deep One-Class Classification</summary>

- *Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, Klaus-Robert Müller*

- `2007.01760v3` - [abs](http://arxiv.org/abs/2007.01760v3) - [pdf](http://arxiv.org/pdf/2007.01760v3)

> Deep one-class classification variants for anomaly detection learn a mapping that concentrates nominal samples in feature space causing anomalies to be mapped away. Because this transformation is highly non-linear, finding interpretations poses a significant challenge. In this paper we present an explainable deep one-class classification method, Fully Convolutional Data Description (FCDD), where the mapped samples are themselves also an explanation heatmap. FCDD yields competitive detection performance and provides reasonable explanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet. On MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps, FCDD sets a new state of the art in the unsupervised setting. Our method can incorporate ground-truth anomaly maps during training and using even a few of these (~5) improves performance significantly. Finally, using FCDD's explanations we demonstrate the vulnerability of deep one-class classification models to spurious image features such as image watermarks.

</details>

<details>

<summary>2021-03-18 12:33:27 - Stochastic Simulation Techniques for Inference and Sensitivity Analysis of Bayesian Attack Graphs</summary>

- *Isaac Matthews, Sadegh Soudjani, Aad van Moorsel*

- `2103.10212v1` - [abs](http://arxiv.org/abs/2103.10212v1) - [pdf](http://arxiv.org/pdf/2103.10212v1)

> A vulnerability scan combined with information about a computer network can be used to create an attack graph, a model of how the elements of a network could be used in an attack to reach specific states or goals in the network. These graphs can be understood probabilistically by turning them into Bayesian attack graphs, making it possible to quantitatively analyse the security of large networks. In the event of an attack, probabilities on the graph change depending on the evidence discovered (e.g., by an intrusion detection system or knowledge of a host's activity). Since such scenarios are difficult to solve through direct computation, we discuss and compare three stochastic simulation techniques for updating the probabilities dynamically based on the evidence and compare their speed and accuracy. From our experiments we conclude that likelihood weighting is most efficient for most uses. We also consider sensitivity analysis of BAGs, to identify the most critical nodes for protection of the network and solve the uncertainty problem in the assignment of priors to nodes. Since sensitivity analysis can easily become computationally expensive, we present and demonstrate an efficient sensitivity analysis approach that exploits a quantitative relation with stochastic inference.

</details>

<details>

<summary>2021-03-18 13:04:21 - Explainable Adversarial Attacks in Deep Neural Networks Using Activation Profiles</summary>

- *Gabriel D. Cantareira, Rodrigo F. Mello, Fernando V. Paulovich*

- `2103.10229v1` - [abs](http://arxiv.org/abs/2103.10229v1) - [pdf](http://arxiv.org/pdf/2103.10229v1)

> As neural networks become the tool of choice to solve an increasing variety of problems in our society, adversarial attacks become critical. The possibility of generating data instances deliberately designed to fool a network's analysis can have disastrous consequences. Recent work has shown that commonly used methods for model training often result in fragile abstract representations that are particularly vulnerable to such attacks. This paper presents a visual framework to investigate neural network models subjected to adversarial examples, revealing how models' perception of the adversarial data differs from regular data instances and their relationships with class perception. Through different use cases, we show how observing these elements can quickly pinpoint exploited areas in a model, allowing further study of vulnerable features in input data and serving as a guide to improving model training and architecture.

</details>

<details>

<summary>2021-03-18 14:13:30 - TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation</summary>

- *Todd Huster, Emmanuel Ekwedike*

- `2103.10274v1` - [abs](http://arxiv.org/abs/2103.10274v1) - [pdf](http://arxiv.org/pdf/2103.10274v1)

> Deep neural networks (DNNs) are vulnerable to "backdoor" poisoning attacks, in which an adversary implants a secret trigger into an otherwise normally functioning model. Detection of backdoors in trained models without access to the training data or example triggers is an important open problem. In this paper, we identify an interesting property of these models: adversarial perturbations transfer from image to image more readily in poisoned models than in clean models. This holds for a variety of model and trigger types, including triggers that are not linearly separable from clean data. We use this feature to detect poisoned models in the TrojAI benchmark, as well as additional models.

</details>

<details>

<summary>2021-03-18 19:39:26 - Stochastic Security: Adversarial Defense Using Long-Run Dynamics of Energy-Based Models</summary>

- *Mitch Hill, Jonathan Mitchell, Song-Chun Zhu*

- `2005.13525v2` - [abs](http://arxiv.org/abs/2005.13525v2) - [pdf](http://arxiv.org/pdf/2005.13525v2)

> The vulnerability of deep networks to adversarial attacks is a central problem for deep learning from the perspective of both cognition and security. The current most successful defense method is to train a classifier using adversarial images created during learning. Another defense approach involves transformation or purification of the original input to remove adversarial signals before the image is classified. We focus on defending naturally-trained classifiers using Markov Chain Monte Carlo (MCMC) sampling with an Energy-Based Model (EBM) for adversarial purification. In contrast to adversarial training, our approach is intended to secure pre-existing and highly vulnerable classifiers.   The memoryless behavior of long-run MCMC sampling will eventually remove adversarial signals, while metastable behavior preserves consistent appearance of MCMC samples after many steps to allow accurate long-run prediction. Balancing these factors can lead to effective purification and robust classification. We evaluate adversarial defense with an EBM using the strongest known attacks against purification. Our contributions are 1) an improved method for training EBM's with realistic long-run MCMC samples, 2) an Expectation-Over-Transformation (EOT) defense that resolves theoretical ambiguities for stochastic defenses and from which the EOT attack naturally follows, and 3) state-of-the-art adversarial defense for naturally-trained classifiers and competitive defense compared to adversarially-trained classifiers on Cifar-10, SVHN, and Cifar-100. Code and pre-trained models are available at https://github.com/point0bar1/ebm-defense.

</details>

<details>

<summary>2021-03-19 01:07:37 - An Experiment Study on Federated LearningTestbed</summary>

- *Cheng Shen, Wanli Xue*

- `2103.10579v1` - [abs](http://arxiv.org/abs/2103.10579v1) - [pdf](http://arxiv.org/pdf/2103.10579v1)

> While the Internet of Things (IoT) can benefit from machine learning by outsourcing model training on the cloud, user data exposure to an untrusted cloud service provider can pose threat to user privacy. Recently, federated learning is proposed as an approach for privacy-preserving machine learning (PPML) for the IoT, while its practicability remains unclear. This work presents the evaluation on the efficiency and privacy performance of a readily available federated learning framework based on PySyft, a Python library for distributed deep learning. It is observed that the training speed of the framework is significantly slower than of the centralized approach due to communication overhead. Meanwhile, the framework bears some vulnerability to potential man-in-the-middle attacks at the network level. The report serves as a starting point for PPML performance analysis and suggests the future direction for PPML framework development.

</details>

<details>

<summary>2021-03-19 02:15:11 - Comprehensive Survey and Taxonomies of False Injection Attacks in Smart Grid: Attack Models, Targets, and Impacts</summary>

- *Haftu Tasew Reda, Adnan Anwar, Abdun Mahmood*

- `2103.10594v1` - [abs](http://arxiv.org/abs/2103.10594v1) - [pdf](http://arxiv.org/pdf/2103.10594v1)

> Smart Grid has rapidly transformed the centrally controlled power system into a massively interconnected cyber-physical system that benefits from the revolutions happening in the communications (e.g. 5G) and the growing proliferation of the Internet of Things devices (such as smart metres and intelligent electronic devices). While the convergence of a significant number of cyber-physical elements has enabled the Smart Grid to be far more efficient and competitive in addressing the growing global energy challenges, it has also introduced a large number of vulnerabilities culminating in violations of data availability, integrity, and confidentiality. Recently, false data injection (FDI) has become one of the most critical cyberattacks, and appears to be a focal point of interest for both research and industry. To this end, this paper presents a comprehensive review in the recent advances of the FDI attacks, with particular emphasis on 1) adversarial models, 2) attack targets, and 3) impacts in the Smart Grid infrastructure. This review paper aims to provide a thorough understanding of the incumbent threats affecting the entire spectrum of the Smart Grid. Related literature are analysed and compared in terms of their theoretical and practical implications to the Smart Grid cybersecurity. In conclusion, a range of technical limitations of existing false data attack research is identified, and a number of future research directions is recommended.

</details>

<details>

<summary>2021-03-20 05:07:03 - Self-supervised Learning: Generative or Contrastive</summary>

- *Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, Jie Tang*

- `2006.08218v5` - [abs](http://arxiv.org/abs/2006.08218v5) - [pdf](http://arxiv.org/pdf/2006.08218v5)

> Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.

</details>

<details>

<summary>2021-03-21 06:28:45 - Automated Software Vulnerability Assessment with Concept Drift</summary>

- *Triet H. M. Le, Bushra Sabir, M. Ali Babar*

- `2103.11316v1` - [abs](http://arxiv.org/abs/2103.11316v1) - [pdf](http://arxiv.org/pdf/2103.11316v1)

> Software Engineering researchers are increasingly using Natural Language Processing (NLP) techniques to automate Software Vulnerabilities (SVs) assessment using the descriptions in public repositories. However, the existing NLP-based approaches suffer from concept drift. This problem is caused by a lack of proper treatment of new (out-of-vocabulary) terms for the evaluation of unseen SVs over time. To perform automated SVs assessment with concept drift using SVs' descriptions, we propose a systematic approach that combines both character and word features. The proposed approach is used to predict seven Vulnerability Characteristics (VCs). The optimal model of each VC is selected using our customized time-based cross-validation method from a list of eight NLP representations and six well-known Machine Learning models. We have used the proposed approach to conduct large-scale experiments on more than 100,000 SVs in the National Vulnerability Database (NVD). The results show that our approach can effectively tackle the concept drift issue of the SVs' descriptions reported from 2000 to 2018 in NVD even without retraining the model. In addition, our approach performs competitively compared to the existing word-only method. We also investigate how to build compact concept-drift-aware models with much fewer features and give some recommendations on the choice of classifiers and NLP representations for SVs assessment.

</details>

<details>

<summary>2021-03-21 23:45:09 - An Empirical Study of OSS-Fuzz Bugs</summary>

- *Zhen Yu Ding, Claire Le Goues*

- `2103.11518v1` - [abs](http://arxiv.org/abs/2103.11518v1) - [pdf](http://arxiv.org/pdf/2103.11518v1)

> Continuous fuzzing is an increasingly popular technique for automated quality and security assurance. Google maintains OSS-Fuzz: a continuous fuzzing service for open source software. We conduct the first empirical study of OSS-Fuzz, analyzing 23,907 bugs found in 316 projects. We examine the characteristics of fuzzer-found faults, the lifecycles of such faults, and the evolution of fuzzing campaigns over time. We find that OSS-Fuzz is often effective at quickly finding bugs, and developers are often quick to patch them. However, flaky bugs, timeouts, and out of memory errors are problematic, people rarely file CVEs for security vulnerabilities, and fuzzing campaigns often exhibit punctuated equilibria, where developers might be surprised by large spikes in bugs found. Our findings have implications on future fuzzing research and practice.

</details>

<details>

<summary>2021-03-22 00:53:07 - ExAD: An Ensemble Approach for Explanation-based Adversarial Detection</summary>

- *Raj Vardhan, Ninghao Liu, Phakpoom Chinprutthiwong, Weijie Fu, Zhenyu Hu, Xia Ben Hu, Guofei Gu*

- `2103.11526v1` - [abs](http://arxiv.org/abs/2103.11526v1) - [pdf](http://arxiv.org/pdf/2103.11526v1)

> Recent research has shown Deep Neural Networks (DNNs) to be vulnerable to adversarial examples that induce desired misclassifications in the models. Such risks impede the application of machine learning in security-sensitive domains. Several defense methods have been proposed against adversarial attacks to detect adversarial examples at test time or to make machine learning models more robust. However, while existing methods are quite effective under blackbox threat model, where the attacker is not aware of the defense, they are relatively ineffective under whitebox threat model, where the attacker has full knowledge of the defense.   In this paper, we propose ExAD, a framework to detect adversarial examples using an ensemble of explanation techniques. Each explanation technique in ExAD produces an explanation map identifying the relevance of input variables for the model's classification. For every class in a dataset, the system includes a detector network, corresponding to each explanation technique, which is trained to distinguish between normal and abnormal explanation maps. At test time, if the explanation map of an input is detected as abnormal by any detector model of the classified class, then we consider the input to be an adversarial example. We evaluate our approach using six state-of-the-art adversarial attacks on three image datasets. Our extensive evaluation shows that our mechanism can effectively detect these attacks under blackbox threat model with limited false-positives. Furthermore, we find that our approach achieves promising results in limiting the success rate of whitebox attacks.

</details>

<details>

<summary>2021-03-22 11:44:30 - InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective</summary>

- *Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, Jingjing Liu*

- `2010.02329v4` - [abs](http://arxiv.org/abs/2010.02329v4) - [pdf](http://arxiv.org/pdf/2010.02329v4)

> Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust fine-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) a Robust Feature regularizer, which increases the mutual information between local robust features and global features. We provide a principled way to theoretically analyze and improve the robustness of representation learning for language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks. Our code is available at https://github.com/AI-secure/InfoBERT.

</details>

<details>

<summary>2021-03-22 15:32:44 - Shallow or Deep? An Empirical Study on Detecting Vulnerabilities using Deep Learning</summary>

- *Alejandro Mazuera-Rozo, Anamaria Mojica-Hanke, Mario Linares-Vásquez, Gabriele Bavota*

- `2103.11940v1` - [abs](http://arxiv.org/abs/2103.11940v1) - [pdf](http://arxiv.org/pdf/2103.11940v1)

> Deep learning (DL) techniques are on the rise in the software engineering research community. More and more approaches have been developed on top of DL models, also due to the unprecedented amount of software-related data that can be used to train these models. One of the recent applications of DL in the software engineering domain concerns the automatic detection of software vulnerabilities. While several DL models have been developed to approach this problem, there is still limited empirical evidence concerning their actual effectiveness especially when compared with shallow machine learning techniques. In this paper, we partially fill this gap by presenting a large-scale empirical study using three vulnerability datasets and five different source code representations (i.e., the format in which the code is provided to the classifiers to assess whether it is vulnerable or not) to compare the effectiveness of two widely used DL-based models and of one shallow machine learning model in (i) classifying code functions as vulnerable or non-vulnerable (i.e., binary classification), and (ii) classifying code functions based on the specific type of vulnerability they contain (or "clean", if no vulnerability is there). As a baseline we include in our study the AutoML utility provided by the Google Cloud Platform. Our results show that the experimented models are still far from ensuring reliable vulnerability detection, and that a shallow learning classifier represents a competitive baseline for the newest DL-based models.

</details>

<details>

<summary>2021-03-23 04:42:51 - Tracing Vulnerable Code Lineage</summary>

- *David Reid, Kalvin Eng, Chris Bogart, Adam Tutko*

- `2103.12304v1` - [abs](http://arxiv.org/abs/2103.12304v1) - [pdf](http://arxiv.org/pdf/2103.12304v1)

> This paper presents results from the MSR 2021 Hackathon. Our team investigates files/projects that contain known security vulnerabilities and how widespread they are throughout repositories in open source software. These security vulnerabilities can potentially be propagated through code reuse even when the vulnerability is fixed in different versions of the code. We utilize the World of Code infrastructure to discover file-level duplication of code from a nearly complete collection of open source software. This paper describes a method and set of tools to find all open source projects that use known vulnerable files and any previous revisions of those files.

</details>

<details>

<summary>2021-03-23 05:51:29 - Security of Healthcare Data Using Blockchains: A Survey</summary>

- *Mayank Pandey, Rachit Agarwal, Sandeep K. Shukla, Nishchal K. Verma*

- `2103.12326v1` - [abs](http://arxiv.org/abs/2103.12326v1) - [pdf](http://arxiv.org/pdf/2103.12326v1)

> The advancement in the healthcare sector is entering into a new era in the form of Health 4.0. The integration of innovative technologies like Cyber-Physical Systems (CPS), Big Data, Cloud Computing, Machine Learning, and Blockchain with Healthcare services has led to improved performance and efficiency through data-based learning and interconnection of systems. On the other hand, it has also increased complexities and has brought its own share of vulnerabilities due to the heavy influx, sharing, and storage of healthcare data. The protection of the same from cyber-attacks along with privacy preservation through authenticated access is one of the significant challenges for the healthcare sector. For this purpose, the use of blockchain-based networks can lead to a considerable reduction in the vulnerabilities of the healthcare systems and secure their data. This chapter explores blockchain's role in strengthening healthcare data security by answering the questions related to what data use, when we need, why we need, who needs, and how state-of-the-art techniques use blockchains to secure healthcare data. As a case study, we also explore and analyze the state-of-the-art implementations for blockchain in healthcare data security for the COVID-19 pandemic. In order to provide a path to future research directions, we identify and discuss the technical limitations and regulatory challenges associated with blockchain-based healthcare data security implementation.

</details>

<details>

<summary>2021-03-23 11:45:41 - RPATTACK: Refined Patch Attack on General Object Detectors</summary>

- *Hao Huang, Yongtao Wang, Zhaoyu Chen, Zhi Tang, Wenqiang Zhang, Kai-Kuang Ma*

- `2103.12469v1` - [abs](http://arxiv.org/abs/2103.12469v1) - [pdf](http://arxiv.org/pdf/2103.12469v1)

> Nowadays, general object detectors like YOLO and Faster R-CNN as well as their variants are widely exploited in many applications. Many works have revealed that these detectors are extremely vulnerable to adversarial patch attacks. The perturbed regions generated by previous patch-based attack works on object detectors are very large which are not necessary for attacking and perceptible for human eyes. To generate much less but more efficient perturbation, we propose a novel patch-based method for attacking general object detectors. Firstly, we propose a patch selection and refining scheme to find the pixels which have the greatest importance for attack and remove the inconsequential perturbations gradually. Then, for a stable ensemble attack, we balance the gradients of detectors to avoid over-optimizing one of them during the training phase. Our RPAttack can achieve an amazing missed detection rate of 100% for both Yolo v4 and Faster R-CNN while only modifies 0.32% pixels on VOC 2007 test set. Our code is available at https://github.com/VDIGPKU/RPAttack.

</details>

<details>

<summary>2021-03-23 14:21:02 - Detection of Iterative Adversarial Attacks via Counter Attack</summary>

- *Matthias Rottmann, Kira Maag, Mathis Peyron, Natasa Krejic, Hanno Gottschalk*

- `2009.11397v2` - [abs](http://arxiv.org/abs/2009.11397v2) - [pdf](http://arxiv.org/pdf/2009.11397v2)

> Deep neural networks (DNNs) have proven to be powerful tools for processing unstructured data. However for high-dimensional data, like images, they are inherently vulnerable to adversarial attacks. Small almost invisible perturbations added to the input can be used to fool DNNs. Various attacks, hardening methods and detection methods have been introduced in recent years. Notoriously, Carlini-Wagner (CW) type attacks computed by iterative minimization belong to those that are most difficult to detect. In this work we outline a mathematical proof that the CW attack can be used as a detector itself. That is, under certain assumptions and in the limit of attack iterations this detector provides asymptotically optimal separation of original and attacked images. In numerical experiments, we experimentally validate this statement and furthermore obtain AUROC values up to 99.73% on CIFAR10 and ImageNet. This is in the upper part of the spectrum of current state-of-the-art detection rates for CW attacks.

</details>

<details>

<summary>2021-03-23 15:04:44 - ESCORT: Ethereum Smart COntRacTs Vulnerability Detection using Deep Neural Network and Transfer Learning</summary>

- *Oliver Lutz, Huili Chen, Hossein Fereidooni, Christoph Sendner, Alexandra Dmitrienko, Ahmad Reza Sadeghi, Farinaz Koushanfar*

- `2103.12607v1` - [abs](http://arxiv.org/abs/2103.12607v1) - [pdf](http://arxiv.org/pdf/2103.12607v1)

> Ethereum smart contracts are automated decentralized applications on the blockchain that describe the terms of the agreement between buyers and sellers, reducing the need for trusted intermediaries and arbitration. However, the deployment of smart contracts introduces new attack vectors into the cryptocurrency systems. In particular, programming flaws in smart contracts can be and have already been exploited to gain enormous financial profits. It is thus an emerging yet crucial issue to detect vulnerabilities of different classes in contracts in an efficient manner. Existing machine learning-based vulnerability detection methods are limited and only inspect whether the smart contract is vulnerable, or train individual classifiers for each specific vulnerability, or demonstrate multi-class vulnerability detection without extensibility consideration. To overcome the scalability and generalization limitations of existing works, we propose ESCORT, the first Deep Neural Network (DNN)-based vulnerability detection framework for Ethereum smart contracts that support lightweight transfer learning on unseen security vulnerabilities, thus is extensible and generalizable. ESCORT leverages a multi-output NN architecture that consists of two parts: (i) A common feature extractor that learns the semantics of the input contract; (ii) Multiple branch structures where each branch learns a specific vulnerability type based on features obtained from the feature extractor. Experimental results show that ESCORT achieves an average F1-score of 95% on six vulnerability types and the detection time is 0.02 seconds per contract. When extended to new vulnerability types, ESCORT yields an average F1-score of 93%. To the best of our knowledge, ESCORT is the first framework that enables transfer learning on new vulnerability types with minimal modification of the DNN model architecture and re-training overhead.

</details>

<details>

<summary>2021-03-23 16:24:15 - SoK: Attacks on Industrial Control Logic and Formal Verification-Based Defenses</summary>

- *Ruimin Sun, Alejandro Mera, Long Lu, David Choffnes*

- `2006.04806v3` - [abs](http://arxiv.org/abs/2006.04806v3) - [pdf](http://arxiv.org/pdf/2006.04806v3)

> Programmable Logic Controllers (PLCs) play a critical role in the industrial control systems. Vulnerabilities in PLC programs might lead to attacks causing devastating consequences to the critical infrastructure, as shown in Stuxnet and similar attacks. In recent years, we have seen an exponential increase in vulnerabilities reported for PLC control logic. Looking back on past research, we found extensive studies explored control logic modification attacks, as well as formal verification-based security solutions. We performed systematization on these studies, and found attacks that can compromise a full chain of control and evade detection. However, the majority of the formal verification research investigated ad-hoc techniques targeting PLC programs. We discovered challenges in every aspect of formal verification, rising from (1) the ever-expanding attack surface from evolved system design, (2) the real-time constraint during the program execution, and (3) the barrier in security evaluation given proprietary and vendor-specific dependencies on different techniques. Based on the knowledge systematization, we provide a set of recommendations for future research directions, and we highlight the need of defending security issues besides safety issues.

</details>

<details>

<summary>2021-03-23 20:38:47 - On the combination of static analysis for software security assessment -- a case study of an open-source e-government project</summary>

- *Anh Nguyen-Duc, Manh Viet Do, Quan Luong Hong, Kiem Nguyen Khac*

- `2103.08010v2` - [abs](http://arxiv.org/abs/2103.08010v2) - [pdf](http://arxiv.org/pdf/2103.08010v2)

> Static Application Security Testing (SAST) is a popular quality assurance technique in software engineering. However, integrating SAST tools into industry-level product development and security assessment poses various technical and managerial challenges. In this work, we reported a longitudinal case study of adopting SAST as a part of a human-driven security assessment for an open-source e-government project. We described how SASTs are selected, evaluated, and combined into a novel approach for software security assessment. The approach was preliminarily evaluated using semi-structured interviews. Our result shows that (1) while some SAST tools out-perform others, it is possible to achieve better performance by combining more than one SAST tools and (2) SAST tools should be used towards a practical performance and in the combination with triangulated approaches for human-driven vulnerability assessment in real-world projects.

</details>

<details>

<summary>2021-03-24 01:15:17 - MIRAGE: Mitigating Conflict-Based Cache Attacks with a Practical Fully-Associative Design</summary>

- *Gururaj Saileshwar, Moinuddin Qureshi*

- `2009.09090v3` - [abs](http://arxiv.org/abs/2009.09090v3) - [pdf](http://arxiv.org/pdf/2009.09090v3)

> Shared processor caches are vulnerable to conflict-based side-channel attacks, where an attacker can monitor access patterns of a victim by evicting victim cache lines using cache-set conflicts. Recent mitigations propose randomized mapping of addresses to cache lines to obfuscate the locations of set-conflicts. However, these are vulnerable to new attacks that discover conflicting sets of addresses despite such mitigations, because these designs select eviction-candidates from a small set of conflicting lines.   This paper presents Mirage, a practical design for a fully associative cache, wherein eviction candidates are selected randomly from all lines resident in the cache, to be immune to set-conflicts. A key challenge for enabling such designs in large shared caches (containing tens of thousands of cache lines) is the complexity of cache-lookup, as a naive design can require searching through all the resident lines. Mirage achieves full-associativity while retaining practical set-associative lookups by decoupling placement and replacement, using pointer-based indirection from tag-store to data-store to allow a newly installed address to globally evict the data of any random resident line. To eliminate set-conflicts, Mirage provisions extra invalid tags in a skewed-associative tag-store design where lines can be installed without set-conflict, along with a load-aware skew-selection policy that guarantees the availability of sets with invalid tags. Our analysis shows Mirage provides the global eviction property of a fully-associative cache throughout system lifetime (violations of full-associativity, i.e. set-conflicts, occur less than once in 10^4 to 10^17 years), thus offering a principled defense against any eviction-set discovery and any potential conflict based attacks. Mirage incurs limited slowdown (2%) and 17-20% extra storage compared to a non-secure cache.

</details>

<details>

<summary>2021-03-24 01:16:28 - Improved Estimation of Concentration Under $\ell_p$-Norm Distance Metrics Using Half Spaces</summary>

- *Jack Prescott, Xiao Zhang, David Evans*

- `2103.12913v1` - [abs](http://arxiv.org/abs/2103.12913v1) - [pdf](http://arxiv.org/pdf/2103.12913v1)

> Concentration of measure has been argued to be the fundamental cause of adversarial vulnerability. Mahloujifar et al. presented an empirical way to measure the concentration of a data distribution using samples, and employed it to find lower bounds on intrinsic robustness for several benchmark datasets. However, it remains unclear whether these lower bounds are tight enough to provide a useful approximation for the intrinsic robustness of a dataset. To gain a deeper understanding of the concentration of measure phenomenon, we first extend the Gaussian Isoperimetric Inequality to non-spherical Gaussian measures and arbitrary $\ell_p$-norms ($p \geq 2$). We leverage these theoretical insights to design a method that uses half-spaces to estimate the concentration of any empirical dataset under $\ell_p$-norm distance metrics. Our proposed algorithm is more efficient than Mahloujifar et al.'s, and our experiments on synthetic datasets and image benchmarks demonstrate that it is able to find much tighter intrinsic robustness bounds. These tighter estimates provide further evidence that rules out intrinsic dataset concentration as a possible explanation for the adversarial vulnerability of state-of-the-art classifiers.

</details>

<details>

<summary>2021-03-24 01:56:08 - DIALED: Data Integrity Attestation for Low-end Embedded Devices</summary>

- *Ivan De Oliveira Nunes, Sashidhar Jakkamsetti, Gene Tsudik*

- `2103.12928v1` - [abs](http://arxiv.org/abs/2103.12928v1) - [pdf](http://arxiv.org/pdf/2103.12928v1)

> Verifying integrity of software execution in low-end micro-controller units (MCUs) is a well-known open problem. The central challenge is how to securely detect software exploits with minimal overhead, since these MCUs are designed for low cost, low energy and small size. Some recent work yielded inexpensive hardware/software co-designs for remotely verifying code and execution integrity. In particular, a means of detecting unauthorized code modifications and control-flow attacks were proposed, referred to as Remote Attestation (RA) and Control-Flow Attestation (CFA), respectively. Despite this progress, detection of data-only attacks remains elusive. Such attacks exploit software vulnerabilities to corrupt intermediate computation results stored in data memory, changing neither the program code nor its control flow. Motivated by lack of any current techniques (for low-end MCUs) that detect these attacks, in this paper we propose, implement and evaluate DIALED, the first Data-Flow Attestation (DFA) technique applicable to the most resource-constrained embedded devices (e.g., TI MSP430). DIALED works in tandem with a companion CFA scheme to detect all (currently known) types of runtime software exploits at fairly low cost.

</details>

<details>

<summary>2021-03-24 12:06:40 - Black-box Detection of Backdoor Attacks with Limited Information and Data</summary>

- *Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, Jun Zhu*

- `2103.13127v1` - [abs](http://arxiv.org/abs/2103.13127v1) - [pdf](http://arxiv.org/pdf/2103.13127v1)

> Although deep neural networks (DNNs) have made rapid progress in recent years, they are vulnerable in adversarial environments. A malicious backdoor could be embedded in a model by poisoning the training dataset, whose intention is to make the infected model give wrong predictions during inference when the specific trigger appears. To mitigate the potential threats of backdoor attacks, various backdoor detection and defense methods have been proposed. However, the existing techniques usually require the poisoned training data or access to the white-box model, which is commonly unavailable in practice. In this paper, we propose a black-box backdoor detection (B3D) method to identify backdoor attacks with only query access to the model. We introduce a gradient-free optimization algorithm to reverse-engineer the potential trigger for each class, which helps to reveal the existence of backdoor attacks. In addition to backdoor detection, we also propose a simple strategy for reliable predictions using the identified backdoored models. Extensive experiments on hundreds of DNN models trained on several datasets corroborate the effectiveness of our method under the black-box setting against various backdoor attacks.

</details>

<details>

<summary>2021-03-25 02:42:05 - Quantifying the efficacy of childcare services on women employment</summary>

- *Jing-Yi Liao, Ying Kong, Tao Zhou*

- `2103.13570v1` - [abs](http://arxiv.org/abs/2103.13570v1) - [pdf](http://arxiv.org/pdf/2103.13570v1)

> Women are set back in the labor market after becoming mother. Intuitively, childcare services are able to promote women employment as they may reconciliate the motherhood penalty. However, most known studies concentrated on the effects of childcare services on fertility rate, instead of quantitative analyses about the effects on women employment. Using worldwide panel data and Chinese data at province level, this paper unfolds the quantitative relationship between childcare services and women employment, that is, the attendance rate of childcare services is positively correlated with the relative employment rate of women to men. Further analysis suggests that such a positive impact may largely resulted from breaking the vulnerable employment dilemma.

</details>

<details>

<summary>2021-03-25 06:55:22 - HufuNet: Embedding the Left Piece as Watermark and Keeping the Right Piece for Ownership Verification in Deep Neural Networks</summary>

- *Peizhuo Lv, Pan Li, Shengzhi Zhang, Kai Chen, Ruigang Liang, Yue Zhao, Yingjiu Li*

- `2103.13628v1` - [abs](http://arxiv.org/abs/2103.13628v1) - [pdf](http://arxiv.org/pdf/2103.13628v1)

> Due to the wide use of highly-valuable and large-scale deep neural networks (DNNs), it becomes crucial to protect the intellectual property of DNNs so that the ownership of disputed or stolen DNNs can be verified. Most existing solutions embed backdoors in DNN model training such that DNN ownership can be verified by triggering distinguishable model behaviors with a set of secret inputs. However, such solutions are vulnerable to model fine-tuning and pruning. They also suffer from fraudulent ownership claim as attackers can discover adversarial samples and use them as secret inputs to trigger distinguishable behaviors from stolen models. To address these problems, we propose a novel DNN watermarking solution, named HufuNet, for protecting the ownership of DNN models. We evaluate HufuNet rigorously on four benchmark datasets with five popular DNN models, including convolutional neural network (CNN) and recurrent neural network (RNN). The experiments demonstrate HufuNet is highly robust against model fine-tuning/pruning, kernels cutoff/supplement, functionality-equivalent attack, and fraudulent ownership claims, thus highly promising to protect large-scale DNN models in the real-world.

</details>

<details>

<summary>2021-03-25 08:34:02 - REFIT: A Unified Watermark Removal Framework For Deep Learning Systems With Limited Data</summary>

- *Xinyun Chen, Wenxiao Wang, Chris Bender, Yiming Ding, Ruoxi Jia, Bo Li, Dawn Song*

- `1911.07205v3` - [abs](http://arxiv.org/abs/1911.07205v3) - [pdf](http://arxiv.org/pdf/1911.07205v3)

> Training deep neural networks from scratch could be computationally expensive and requires a lot of training data. Recent work has explored different watermarking techniques to protect the pre-trained deep neural networks from potential copyright infringements. However, these techniques could be vulnerable to watermark removal attacks. In this work, we propose REFIT, a unified watermark removal framework based on fine-tuning, which does not rely on the knowledge of the watermarks, and is effective against a wide range of watermarking schemes. In particular, we conduct a comprehensive study of a realistic attack scenario where the adversary has limited training data, which has not been emphasized in prior work on attacks against watermarking schemes. To effectively remove the watermarks without compromising the model functionality under this weak threat model, we propose two techniques that are incorporated into our fine-tuning framework: (1) an adaption of the elastic weight consolidation (EWC) algorithm, which is originally proposed for mitigating the catastrophic forgetting phenomenon; and (2) unlabeled data augmentation (AU), where we leverage auxiliary unlabeled data from other sources. Our extensive evaluation shows the effectiveness of REFIT against diverse watermark embedding schemes. In particular, both EWC and AU significantly decrease the amount of labeled training data needed for effective watermark removal, and the unlabeled data samples used for AU do not necessarily need to be drawn from the same distribution as the benign data for model evaluation. The experimental results demonstrate that our fine-tuning based watermark removal attacks could pose real threats to the copyright of pre-trained models, and thus highlight the importance of further investigating the watermarking problem and proposing more robust watermark embedding schemes against the attacks.

</details>

<details>

<summary>2021-03-25 09:59:57 - MAD-HTLC: Because HTLC is Crazy-Cheap to Attack</summary>

- *Itay Tsabary, Matan Yechieli, Alex Manuskin, Ittay Eyal*

- `2006.12031v3` - [abs](http://arxiv.org/abs/2006.12031v3) - [pdf](http://arxiv.org/pdf/2006.12031v3)

> Smart Contracts and transactions allow users to implement elaborate constructions on cryptocurrency blockchains like Bitcoin and Ethereum. Many of these constructions, including operational payment channels and atomic swaps, use a building block called Hashed Time-Locked Contract (HTLC).   In this work, we distill from HTLC a specification (HTLC-Spec), and present an implementation called Mutual-Assured-Destruction Hashed Time-Locked Contract (MAD-HTLC). MAD-HTLC employs a novel approach of utilizing the existing blockchain operators, called miners, as part of the design. If a user misbehaves, MAD-HTLC incentivizes the miners to confiscate all her funds. We prove MAD-HTLC's security using the UC framework and game-theoretic analysis. We demonstrate MAD-HTLC's efficacy and analyze its overhead by instantiating it on Bitcoin's and Ethereum's operational blockchains.   Notably, current miner software makes only little effort to optimize revenue, since the advantage is relatively small. However, as the demand grows and other revenue components shrink, miners are more motivated to fully optimize their fund intake. By patching the standard Bitcoin client, we demonstrate such optimization is easy to implement, making the miners natural enforcers of MAD-HTLC.   Finally, we extend previous results regarding HTLC vulnerability to bribery attacks. An attacker can incentivize miners to prefer her transactions by offering high transaction fees. We demonstrate this attack can be easily implemented by patching the Bitcoin client, and use game-theoretic tools to qualitatively tighten the known cost bound of such bribery attacks in presence of rational miners. We identify bribe opportunities occurring on the Bitcoin and Ethereum main networks where a few dollars bribe could yield tens of thousands of dollars in reward (e.g., \$2 for over \$25K).

</details>

<details>

<summary>2021-03-25 13:21:43 - The Cost of OSCORE and EDHOC for Constrained Devices</summary>

- *Stefan Hristozov, Manuel Huber, Lei Xu, Jaro Fietz, Marco Liess, Georg Sigl*

- `2103.13832v1` - [abs](http://arxiv.org/abs/2103.13832v1) - [pdf](http://arxiv.org/pdf/2103.13832v1)

> Many modern IoT applications rely on the Constrained Application Protocol (CoAP) because of its efficiency and seamless integrability in the existing Internet infrastructure. One of the strategies that CoAP leverages to achieve these characteristics is the usage of proxies. Unfortunately, in order for a proxy to operate, it needs to terminate the (D)TLS channels between clients and servers. Therefore, end-to-end confidentiality, integrity and authenticity of the exchanged data cannot be achieved. In order to overcome this problem, an alternative to (D)TLS was recently proposed by the Internet Engineering Task Force (IETF). This alternative consists of two novel protocols: 1) Object Security for Constrained RESTful Environments (OSCORE) providing authenticated encryption for the payload data and 2) Ephemeral Diffie-Hellman Over COSE (EDHOC) providing the symmetric session keys required for OSCORE. In this paper, we present the design of four firmware libraries for these protocols especially targeted for constrained microcontrollers and their detailed evaluation. More precisely, we present the design of uOSCORE and uEDHOC libraries for regular microcontrollers and uOSCORE-TEE and uEDHOC-TEE libraries for microcontrollers with a Trusted Execution Environment (TEE), such as microcontrollers featuring ARM TrustZone-M. Our firmware design for the later class of devices concerns the fact that attackers may exploit common software vulnerabilities, e.g., buffer overflows in the protocol logic, OS or application to compromise the protocol security. uOSCORE-TEE and uEDHOC-TEE achieve separation of the cryptographic operations and keys from the remainder of the firmware, which could be vulnerable. We present an evaluation of our implementations in terms of RAM/FLASH requirements, execution speed and energy on a broad range of microcontrollers.

</details>

<details>

<summary>2021-03-26 03:30:29 - A Survey of Microarchitectural Side-channel Vulnerabilities, Attacks and Defenses in Cryptography</summary>

- *Xiaoxuan Lou, Tianwei Zhang, Jun Jiang, Yinqian Zhang*

- `2103.14244v1` - [abs](http://arxiv.org/abs/2103.14244v1) - [pdf](http://arxiv.org/pdf/2103.14244v1)

> Side-channel attacks have become a severe threat to the confidentiality of computer applications and systems. One popular type of such attacks is the microarchitectural attack, where the adversary exploits the hardware features to break the protection enforced by the operating system and steal the secrets from the program. In this paper, we systematize microarchitectural side channels with a focus on attacks and defenses in cryptographic applications. We make three contributions. (1) We survey past research literature to categorize microarchitectural side-channel attacks. Since these are hardware attacks targeting software, we summarize the vulnerable implementations in software, as well as flawed designs in hardware. (2) We identify common strategies to mitigate microarchitectural attacks, from the application, OS and hardware levels. (3) We conduct a large-scale evaluation on popular cryptographic applications in the real world, and analyze the severity, practicality and impact of side-channel vulnerabilities. This survey is expected to inspire side-channel research community to discover new attacks, and more importantly, propose new defense solutions against them.

</details>

<details>

<summary>2021-03-26 06:30:54 - Vulnerability Due to Training Order in Split Learning</summary>

- *Harshit Madaan, Manish Gawali, Viraj Kulkarni, Aniruddha Pant*

- `2103.14291v1` - [abs](http://arxiv.org/abs/2103.14291v1) - [pdf](http://arxiv.org/pdf/2103.14291v1)

> Split learning (SL) is a privacy-preserving distributed deep learning method used to train a collaborative model without the need for sharing of patient's raw data between clients. In split learning, an additional privacy-preserving algorithm called no-peek algorithm can be incorporated, which is robust to adversarial attacks. The privacy benefits offered by split learning make it suitable for practice in the healthcare domain. However, the split learning algorithm is flawed as the collaborative model is trained sequentially, i.e., one client trains after the other. We point out that the model trained using the split learning algorithm gets biased towards the data of the clients used for training towards the end of a round. This makes SL algorithms highly susceptible to the order in which clients are considered for training. We demonstrate that the model trained using the data of all clients does not perform well on the client's data which was considered earliest in a round for training the model. Moreover, we show that this effect becomes more prominent with the increase in the number of clients. We also demonstrate that the SplitFedv3 algorithm mitigates this problem while still leveraging the privacy benefits provided by split learning.

</details>

<details>

<summary>2021-03-26 10:53:55 - Stereopagnosia: Fooling Stereo Networks with Adversarial Perturbations</summary>

- *Alex Wong, Mukund Mundhra, Stefano Soatto*

- `2009.10142v3` - [abs](http://arxiv.org/abs/2009.10142v3) - [pdf](http://arxiv.org/pdf/2009.10142v3)

> We study the effect of adversarial perturbations of images on the estimates of disparity by deep learning models trained for stereo. We show that imperceptible additive perturbations can significantly alter the disparity map, and correspondingly the perceived geometry of the scene. These perturbations not only affect the specific model they are crafted for, but transfer to models with different architecture, trained with different loss functions. We show that, when used for adversarial data augmentation, our perturbations result in trained models that are more robust, without sacrificing overall accuracy of the model. This is unlike what has been observed in image classification, where adding the perturbed images to the training set makes the model less vulnerable to adversarial perturbations, but to the detriment of overall accuracy. We test our method using the most recent stereo networks and evaluate their performance on public benchmark datasets.

</details>

<details>

<summary>2021-03-26 15:56:58 - FastSpec: Scalable Generation and Detection of Spectre Gadgets Using Neural Embeddings</summary>

- *M. Caner Tol, Berk Gulmezoglu, Koray Yurtseven, Berk Sunar*

- `2006.14147v2` - [abs](http://arxiv.org/abs/2006.14147v2) - [pdf](http://arxiv.org/pdf/2006.14147v2)

> Several techniques have been proposed to detect vulnerable Spectre gadgets in widely deployed commercial software. Unfortunately, detection techniques proposed so far rely on hand-written rules which fall short in covering subtle variations of known Spectre gadgets as well as demand a huge amount of time to analyze each conditional branch in software. Moreover, detection tool evaluations are based only on a handful of these gadgets, as it requires arduous effort to craft new gadgets manually.   In this work, we employ both fuzzing and deep learning techniques to automate the generation and detection of Spectre gadgets. We first create a diverse set of Spectre-V1 gadgets by introducing perturbations to the known gadgets. Using mutational fuzzing, we produce a data set with more than 1 million Spectre-V1 gadgets which is the largest Spectre gadget data set built to date. Next, we conduct the first empirical usability study of Generative Adversarial Networks (GANs) in the context of assembly code generation without any human interaction. We introduce SpectreGAN which leverages masking implementation of GANs for both learning the gadget structures and generating new gadgets. This provides the first scalable solution to extend the variety of Spectre gadgets.   Finally, we propose FastSpec which builds a classifier with the generated Spectre gadgets based on a novel high dimensional Neural Embeddings technique (BERT). For the case studies, we demonstrate that FastSpec discovers potential gadgets with a high success rate in OpenSSL libraries and Phoronix benchmarks. Further, FastSpec offers much greater flexibility and time-related performance gain compared to the existing tools and therefore can be used for gadget detection in large-scale software.

</details>

<details>

<summary>2021-03-26 21:28:13 - Leaky Nets: Recovering Embedded Neural Network Models and Inputs through Simple Power and Timing Side-Channels -- Attacks and Defenses</summary>

- *Saurav Maji, Utsav Banerjee, Anantha P. Chandrakasan*

- `2103.14739v1` - [abs](http://arxiv.org/abs/2103.14739v1) - [pdf](http://arxiv.org/pdf/2103.14739v1)

> With the recent advancements in machine learning theory, many commercial embedded micro-processors use neural network models for a variety of signal processing applications. However, their associated side-channel security vulnerabilities pose a major concern. There have been several proof-of-concept attacks demonstrating the extraction of their model parameters and input data. But, many of these attacks involve specific assumptions, have limited applicability, or pose huge overheads to the attacker. In this work, we study the side-channel vulnerabilities of embedded neural network implementations by recovering their parameters using timing-based information leakage and simple power analysis side-channel attacks. We demonstrate our attacks on popular micro-controller platforms over networks of different precisions such as floating point, fixed point, binary networks. We are able to successfully recover not only the model parameters but also the inputs for the above networks. Countermeasures against timing-based attacks are implemented and their overheads are analyzed.

</details>

<details>

<summary>2021-03-27 03:13:03 - Ensemble-in-One: Learning Ensemble within Random Gated Networks for Enhanced Adversarial Robustness</summary>

- *Yi Cai, Xuefei Ning, Huazhong Yang, Yu Wang*

- `2103.14795v1` - [abs](http://arxiv.org/abs/2103.14795v1) - [pdf](http://arxiv.org/pdf/2103.14795v1)

> Adversarial attacks have rendered high security risks on modern deep learning systems. Adversarial training can significantly enhance the robustness of neural network models by suppressing the non-robust features. However, the models often suffer from significant accuracy loss on clean data. Ensemble training methods have emerged as promising solutions for defending against adversarial attacks by diversifying the vulnerabilities among the sub-models, simultaneously maintaining comparable accuracy as standard training. However, existing ensemble methods are with poor scalability, owing to the rapid complexity increase when including more sub-models in the ensemble. Moreover, in real-world applications, it is difficult to deploy an ensemble with multiple sub-models, owing to the tight hardware resource budget and latency requirement. In this work, we propose ensemble-in-one (EIO), a simple but efficient way to train an ensemble within one random gated network (RGN). EIO augments the original model by replacing the parameterized layers with multi-path random gated blocks (RGBs) to construct a RGN. By diversifying the vulnerability of the numerous paths within the RGN, better robustness can be achieved. It provides high scalability because the paths within an EIO network exponentially increase with the network depth. Our experiments demonstrate that EIO consistently outperforms previous ensemble training methods with even less computational overhead.

</details>

<details>

<summary>2021-03-27 03:36:02 - Discovering Robust Convolutional Architecture at Targeted Capacity: A Multi-Shot Approach</summary>

- *Xuefei Ning, Junbo Zhao, Wenshuo Li, Tianchen Zhao, Yin Zheng, Huazhong Yang, Yu Wang*

- `2012.11835v3` - [abs](http://arxiv.org/abs/2012.11835v3) - [pdf](http://arxiv.org/pdf/2012.11835v3)

> Convolutional neural networks (CNNs) are vulnerable to adversarial examples, and studies show that increasing the model capacity of an architecture topology (e.g., width expansion) can bring consistent robustness improvements. This reveals a clear robustness-efficiency trade-off that should be considered in architecture design. In this paper, considering scenarios with capacity budget, we aim to discover adversarially robust architecture at targeted capacities. Recent studies employed one-shot neural architecture search (NAS) to discover robust architectures. However, since the capacities of different topologies cannot be aligned in the search process, one-shot NAS methods favor topologies with larger capacities in the supernet. And the discovered topology might be suboptimal when augmented to the targeted capacity. We propose a novel multi-shot NAS method to address this issue and explicitly search for robust architectures at targeted capacities. At the targeted FLOPs of 2000M, the discovered MSRobNet-2000 outperforms the recent NAS-discovered architecture RobNet-large under various criteria by a large margin of 4%-7%. And at the targeted FLOPs of 1560M, MSRobNet-1560 surpasses another NAS-discovered architecture RobNet-free by 2.3% and 1.3% in the clean and PGD-7 accuracies, respectively. All codes are available at https://github.com/walkerning/aw\_nas.

</details>

<details>

<summary>2021-03-27 19:58:06 - On the benefits of robust models in modulation recognition</summary>

- *Javier Maroto, Gérôme Bovet, Pascal Frossard*

- `2103.14977v1` - [abs](http://arxiv.org/abs/2103.14977v1) - [pdf](http://arxiv.org/pdf/2103.14977v1)

> Given the rapid changes in telecommunication systems and their higher dependence on artificial intelligence, it is increasingly important to have models that can perform well under different, possibly adverse, conditions. Deep Neural Networks (DNNs) using convolutional layers are state-of-the-art in many tasks in communications. However, in other domains, like image classification, DNNs have been shown to be vulnerable to adversarial perturbations, which consist of imperceptible crafted noise that when added to the data fools the model into misclassification. This puts into question the security of DNNs in communication tasks, and in particular in modulation recognition. We propose a novel framework to test the robustness of current state-of-the-art models where the adversarial perturbation strength is dependent on the signal strength and measured with the "signal to perturbation ratio" (SPR). We show that current state-of-the-art models are susceptible to these perturbations. In contrast to current research on the topic of image classification, modulation recognition allows us to have easily accessible insights on the usefulness of the features learned by DNNs by looking at the constellation space. When analyzing these vulnerable models we found that adversarial perturbations do not shift the symbols towards the nearest classes in constellation space. This shows that DNNs do not base their decisions on signal statistics that are important for the Bayes-optimal modulation recognition model, but spurious correlations in the training data. Our feature analysis and proposed framework can help in the task of finding better models for communication systems.

</details>

<details>

<summary>2021-03-27 21:06:41 - Compositional Security for Reentrant Applications</summary>

- *Ethan Cecchetti, Siqiu Yao, Haobin Ni, Andrew C. Myers*

- `2103.08577v2` - [abs](http://arxiv.org/abs/2103.08577v2) - [pdf](http://arxiv.org/pdf/2103.08577v2)

> The disastrous vulnerabilities in smart contracts sharply remind us of our ignorance: we do not know how to write code that is secure in composition with malicious code. Information flow control has long been proposed as a way to achieve compositional security, offering strong guarantees even when combining software from different trust domains. Unfortunately, this appealing story breaks down in the presence of reentrancy attacks. We formalize a general definition of reentrancy and introduce a security condition that allows software modules like smart contracts to protect their key invariants while retaining the expressive power of safe forms of reentrancy. We present a security type system that provably enforces secure information flow; in conjunction with run-time mechanisms, it enforces secure reentrancy even in the presence of unknown code; and it helps locate and correct recent high-profile vulnerabilities.

</details>

<details>

<summary>2021-03-28 07:33:03 - A Survey on Ethical Hacking: Issues and Challenges</summary>

- *Jean-Paul A. Yaacoub, Hassan N. Noura, Ola Salman, Ali Chehab*

- `2103.15072v1` - [abs](http://arxiv.org/abs/2103.15072v1) - [pdf](http://arxiv.org/pdf/2103.15072v1)

> Security attacks are growing in an exponential manner and their impact on existing systems is seriously high and can lead to dangerous consequences. However, in order to reduce the effect of these attacks, penetration tests are highly required, and can be considered as a suitable solution for this task. Therefore, the main focus of this paper is to explain the technical and non-technical steps of penetration tests. The objective of penetration tests is to make existing systems and their corresponding data more secure, efficient and resilient. In other terms, pen testing is a simulated attack with the goal of identifying any exploitable vulnerability or/and a security gap. In fact, any identified exploitable vulnerability will be used to conduct attacks on systems, devices, or personnel. This growing problem should be solved and mitigated to reach better resistance against these attacks. Moreover, the advantages and limitations of penetration tests are also listed. The main issue of penetration tests that it is efficient to detect known vulnerabilities. Therefore, in order to resist unknown vulnerabilities, a new kind of modern penetration tests is required, in addition to reinforcing the use of shadows honeypots. This can also be done by reinforcing the anomaly detection of intrusion detection/prevention system. In fact, security is increased by designing an efficient cooperation between the different security elements and penetration tests.

</details>

<details>

<summary>2021-03-28 16:12:04 - Scalable Call Graph Constructor for Maven</summary>

- *Mehdi Keshani*

- `2103.15162v1` - [abs](http://arxiv.org/abs/2103.15162v1) - [pdf](http://arxiv.org/pdf/2103.15162v1)

> As a rich source of data, Call Graphs are used for various applications including security vulnerability detection. Despite multiple studies showing that Call Graphs can drastically improve the accuracy of analysis, existing ecosystem-scale tools like Dependabot do not use Call Graphs and work at the package-level. Using Call Graphs in ecosystem use cases is not practical because of the scalability problems that Call Graph generators have. Call Graph generation is usually considered to be a "full program analysis" resulting in large Call Graphs and expensive computation. To make an analysis applicable to ecosystem scale, this pragmatic approach does not work, because the number of possible combinations of how a particular artifact can be combined in a full program explodes. Therefore, it is necessary to make the analysis incremental. There are existing studies on different types of incremental program analysis. However, none of them focuses on Call Graph generation for an entire ecosystem. In this paper, we propose an incremental implementation of the CHA algorithm that can generate Call Graphs on-demand, by stitching together partial Call Graphs that have been extracted for libraries before. Our preliminary evaluation results show that the proposed approach scales well and outperforms the most scalable existing framework called OPAL.

</details>

<details>

<summary>2021-03-29 12:19:45 - Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models</summary>

- *Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, Bin He*

- `2103.15543v1` - [abs](http://arxiv.org/abs/2103.15543v1) - [pdf](http://arxiv.org/pdf/2103.15543v1)

> Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/lancopku/Embedding-Poisoning.

</details>

<details>

<summary>2021-03-29 14:15:00 - Generalized SAT-Attack-Resistant Logic Locking</summary>

- *Jingbo Zhou, Xinmiao Zhang*

- `1910.12142v2` - [abs](http://arxiv.org/abs/1910.12142v2) - [pdf](http://arxiv.org/pdf/1910.12142v2)

> Logic locking is used to protect integrated circuits (ICs) from piracy and counterfeiting. An encrypted IC implements the correct function only when the right key is input. Many existing logic-locking methods are subject to the powerful satisfiability (SAT)-based attack. Recently, an Anti-SAT scheme has been developed. By adopting two complementary logic blocks that consist of AND/NAND trees, it makes the number of iterations needed by the SAT attack exponential to the number of input bits. Nevertheless, the Anti-SAT scheme is vulnerable to the later AppSAT and removal attacks. This paper proposes a generalized (G-)Anti-SAT scheme. Different from the Anti-SAT scheme, a variety of complementary or non-complementary functions can be adopted for the two blocks in our G-Anti-SAT scheme. The Anti-SAT scheme is just a special case of our proposed design. Our design can achieve higher output corruptibility, which is also tunable, so that better resistance to the AppSAT and removal attacks is achieved. Meanwhile, unlike existing AppSAT-resilient designs, our design does not sacrifice the resistance to the SAT attack.

</details>

<details>

<summary>2021-03-29 19:07:55 - Automating Defense Against Adversarial Attacks: Discovery of Vulnerabilities and Application of Multi-INT Imagery to Protect Deployed Models</summary>

- *Josh Kalin, David Noever, Matthew Ciolino, Dominick Hambrick, Gerry Dozier*

- `2103.15897v1` - [abs](http://arxiv.org/abs/2103.15897v1) - [pdf](http://arxiv.org/pdf/2103.15897v1)

> Image classification is a common step in image recognition for machine learning in overhead applications. When applying popular model architectures like MobileNetV2, known vulnerabilities expose the model to counter-attacks, either mislabeling a known class or altering box location. This work proposes an automated approach to defend these models. We evaluate the use of multi-spectral image arrays and ensemble learners to combat adversarial attacks. The original contribution demonstrates the attack, proposes a remedy, and automates some key outcomes for protecting the model's predictions against adversaries. In rough analogy to defending cyber-networks, we combine techniques from both offensive ("red team") and defensive ("blue team") approaches, thus generating a hybrid protective outcome ("green team"). For machine learning, we demonstrate these methods with 3-color channels plus infrared for vehicles. The outcome uncovers vulnerabilities and corrects them with supplemental data inputs commonly found in overhead cases particularly.

</details>

<details>

<summary>2021-03-29 22:49:58 - Linking average- and worst-case perturbation robustness via class selectivity and dimensionality</summary>

- *Matthew L. Leavitt, Ari Morcos*

- `2010.07693v2` - [abs](http://arxiv.org/abs/2010.07693v2) - [pdf](http://arxiv.org/pdf/2010.07693v2)

> Representational sparsity is known to affect robustness to input perturbations in deep neural networks (DNNs), but less is known about how the semantic content of representations affects robustness. Class selectivity-the variability of a unit's responses across data classes or dimensions-is one way of quantifying the sparsity of semantic representations. Given recent evidence that class selectivity may not be necessary for, and in some cases can impair generalization, we investigate whether it also confers robustness (or vulnerability) to perturbations of input data. We found that networks regularized to have lower levels of class selectivity were more robust to average-case (naturalistic) perturbations, while networks with higher class selectivity are more vulnerable. In contrast, class selectivity increases robustness to multiple types of worst-case (i.e. white box adversarial) perturbations, suggesting that while decreasing class selectivity is helpful for average-case perturbations, it is harmful for worst-case perturbations. To explain this difference, we studied the dimensionality of the networks' representations: we found that the dimensionality of early-layer representations is inversely proportional to a network's class selectivity, and that adversarial samples cause a larger increase in early-layer dimensionality than corrupted samples. Furthermore, the input-unit gradient is more variable across samples and units in high-selectivity networks compared to low-selectivity networks. These results lead to the conclusion that units participate more consistently in low-selectivity regimes compared to high-selectivity regimes, effectively creating a larger attack surface and hence vulnerability to worst-case perturbations.

</details>

<details>

<summary>2021-03-30 02:19:45 - Certifiably-Robust Federated Adversarial Learning via Randomized Smoothing</summary>

- *Cheng Chen, Bhavya Kailkhura, Ryan Goldhahn, Yi Zhou*

- `2103.16031v1` - [abs](http://arxiv.org/abs/2103.16031v1) - [pdf](http://arxiv.org/pdf/2103.16031v1)

> Federated learning is an emerging data-private distributed learning framework, which, however, is vulnerable to adversarial attacks. Although several heuristic defenses are proposed to enhance the robustness of federated learning, they do not provide certifiable robustness guarantees. In this paper, we incorporate randomized smoothing techniques into federated adversarial training to enable data-private distributed learning with certifiable robustness to test-time adversarial perturbations. Our experiments show that such an advanced federated adversarial learning framework can deliver models as robust as those trained by the centralized training. Further, this enables provably-robust classifiers to $\ell_2$-bounded adversarial perturbations in a distributed setup. We also show that one-point gradient estimation based training approach is $2-3\times$ faster than popular stochastic estimator based approach without any noticeable certified robustness differences.

</details>

<details>

<summary>2021-03-30 05:36:09 - A Taxonomy of Cyber Defence Strategies Against False Data Attacks in Smart Grid</summary>

- *Haftu Tasew Reda, Adnan Anwar, Abdun Naser Mahmood, Zahir Tari*

- `2103.16085v1` - [abs](http://arxiv.org/abs/2103.16085v1) - [pdf](http://arxiv.org/pdf/2103.16085v1)

> Modern electric power grid, known as the Smart Grid, has fast transformed the isolated and centrally controlled power system to a fast and massively connected cyber-physical system that benefits from the revolutions happening in the communications and the fast adoption of Internet of Things devices. While the synergy of a vast number of cyber-physical entities has allowed the Smart Grid to be much more effective and sustainable in meeting the growing global energy challenges, it has also brought with it a large number of vulnerabilities resulting in breaches of data integrity, confidentiality and availability. False data injection (FDI) appears to be among the most critical cyberattacks and has been a focal point interest for both research and industry. To this end, this paper presents a comprehensive review in the recent advances of the defence countermeasures of the FDI attacks in the Smart Grid infrastructure. Relevant existing literature are evaluated and compared in terms of their theoretical and practical significance to the Smart Grid cybersecurity. In conclusion, a range of technical limitations of existing false data attack detection researches are identified, and a number of future research directions are recommended.

</details>

<details>

<summary>2021-03-30 05:36:31 - Dynamic Attention guided Multi-Trajectory Analysis for Single Object Tracking</summary>

- *Xiao Wang, Zhe Chen, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian, Feng Wu*

- `2103.16086v1` - [abs](http://arxiv.org/abs/2103.16086v1) - [pdf](http://arxiv.org/pdf/2103.16086v1)

> Most of the existing single object trackers track the target in a unitary local search window, making them particularly vulnerable to challenging factors such as heavy occlusions and out-of-view movements. Despite the attempts to further incorporate global search, prevailing mechanisms that cooperate local and global search are relatively static, thus are still sub-optimal for improving tracking performance. By further studying the local and global search results, we raise a question: can we allow more dynamics for cooperating both results? In this paper, we propose to introduce more dynamics by devising a dynamic attention-guided multi-trajectory tracking strategy. In particular, we construct dynamic appearance model that contains multiple target templates, each of which provides its own attention for locating the target in the new frame. Guided by different attention, we maintain diversified tracking results for the target to build multi-trajectory tracking history, allowing more candidates to represent the true target trajectory. After spanning the whole sequence, we introduce a multi-trajectory selection network to find the best trajectory that delivers improved tracking performance. Extensive experimental results show that our proposed tracking strategy achieves compelling performance on various large-scale tracking benchmarks. The project page of this paper can be found at https://sites.google.com/view/mt-track/.

</details>

<details>

<summary>2021-03-30 10:00:09 - App's Auto-Login Function Security Testing via Android OS-Level Virtualization</summary>

- *Wenna Song, Jiang Ming, Lin Jiang, Han Yan, Yi Xiang, Yuan Chen, Jianming Fu, Guojun Peng*

- `2103.03511v2` - [abs](http://arxiv.org/abs/2103.03511v2) - [pdf](http://arxiv.org/pdf/2103.03511v2)

> Limited by the small keyboard, most mobile apps support the automatic login feature for better user experience. Therefore, users avoid the inconvenience of retyping their ID and password when an app runs in the foreground again. However, this auto-login function can be exploited to launch the so-called "data-clone attack": once the locally-stored, auto-login depended data are cloned by attackers and placed into their own smartphones, attackers can break through the login-device number limit and log in to the victim's account stealthily. A natural countermeasure is to check the consistency of devicespecific attributes. As long as the new device shows different device fingerprints with the previous one, the app will disable the auto-login function and thus prevent data-clone attacks. In this paper, we develop VPDroid, a transparent Android OS-level virtualization platform tailored for security testing. With VPDroid, security analysts can customize different device artifacts, such as CPU model, Android ID, and phone number, in a virtual phone without user-level API hooking. VPDroid's isolation mechanism ensures that user-mode apps in the virtual phone cannot detect device-specific discrepancies. To assess Android apps' susceptibility to the data-clone attack, we use VPDroid to simulate data-clone attacks with 234 most-downloaded apps. Our experiments on five different virtual phone environments show that VPDroid's device attribute customization can deceive all tested apps that perform device-consistency checks, such as Twitter, WeChat, and PayPal. 19 vendors have confirmed our report as a zero-day vulnerability. Our findings paint a cautionary tale: only enforcing a device-consistency check at client side is still vulnerable to an advanced data-clone attack.

</details>

<details>

<summary>2021-03-30 10:35:57 - BLEKeeper: Response Time Behavior Based Man-In-The-Middle Attack Detection</summary>

- *Muhammed Ali Yurdagul, Husrev Taha Sencar*

- `2103.16235v1` - [abs](http://arxiv.org/abs/2103.16235v1) - [pdf](http://arxiv.org/pdf/2103.16235v1)

> Bluetooth Low Energy (BLE) has become one of the most popular wireless communication protocols and is used in billions of smart devices. Despite several security features, the hardware and software limitations of these devices makes them vulnerable to man-in-the-middle (MITM) attacks. Due to the use of these devices in increasingly diverse and safety-critical applications, the capability to detect MITM attacks has become more critical. To address this challenge, we propose the use of the response time behavior of a BLE device observed in relation to select read and write operations and introduce an activeMITM attack detection system that identifies changes in response time. Our measurements on several BLE devices show that theirresponse time behavior exhibits very high regularity, making it a very reliable attack indicator that cannot be concealed by an attacker. Test results show that our system can very accurately and quickly detect MITM attacks while requiring a simple learning approach.

</details>

<details>

<summary>2021-03-30 10:44:26 - IFDS Taint Analysis with Access Paths</summary>

- *Nicholas Allen, François Gauthier, Alexander Jordan*

- `2103.16240v1` - [abs](http://arxiv.org/abs/2103.16240v1) - [pdf](http://arxiv.org/pdf/2103.16240v1)

> Over the years, static taint analysis emerged as the analysis of choice to detect some of the most common web application vulnerabilities, such as SQL injection (SQLi) and cross-site scripting (XSS)~\cite{OWASP}. Furthermore, from an implementation perspective, the IFDS dataflow framework stood out as one of the most successful vehicles to implement static taint analysis for real-world Java applications. While existing approaches scale reasonably to medium-size applications (e.g. up to one hour analysis time for less than 100K lines of code), our experience suggests that no existing solution can scale to very large industrial code bases (e.g. more than 1M lines of code). In this paper, we present our novel IFDS-based solution to perform fast and precise static taint analysis of very large industrial Java web applications. Similar to state-of-the-art approaches to taint analysis, our IFDS-based taint analysis uses \textit{access paths} to abstract objects and fields in a program. However, contrary to existing approaches, our analysis is demand-driven, which restricts the amount of code to be analyzed, and does not rely on a computationally expensive alias analysis, thereby significantly improving scalability.

</details>

<details>

<summary>2021-03-30 17:00:39 - TREND: Transferability based Robust ENsemble Design</summary>

- *Deepak Ravikumar, Sangamesh Kodge, Isha Garg, Kaushik Roy*

- `2008.01524v2` - [abs](http://arxiv.org/abs/2008.01524v2) - [pdf](http://arxiv.org/pdf/2008.01524v2)

> Deep Learning models hold state-of-the-art performance in many fields, but their vulnerability to adversarial examples poses threat to their ubiquitous deployment in practical settings. Additionally, adversarial inputs generated on one classifier have been shown to transfer to other classifiers trained on similar data, which makes the attacks possible even if model parameters are not revealed to the adversary. This property of transferability has not yet been systematically studied, leading to a gap in our understanding of robustness of neural networks to adversarial inputs. In this work, we study the effect of network architecture, initialization, optimizer, input, weight and activation quantization on transferability of adversarial samples. We also study the effect of different attacks on transferability. Our experiments reveal that transferability is significantly hampered by input quantization and architectural mismatch between source and target, is unaffected by initialization but the choice of optimizer turns out to be critical. We observe that transferability is architecture-dependent for both weight and activation quantized models. To quantify transferability, we use simple metric and demonstrate the utility of the metric in designing a methodology to build ensembles with improved adversarial robustness. When attacking ensembles we observe that "gradient domination" by a single ensemble member model hampers existing attacks. To combat this we propose a new state-of-the-art ensemble attack. We compare the proposed attack with existing attack techniques to show its effectiveness. Finally, we show that an ensemble consisting of carefully chosen diverse networks achieves better adversarial robustness than would otherwise be possible with a single network.

</details>

<details>

<summary>2021-03-31 08:49:31 - Lags in the Release, Adoption, and Propagation of npm Vulnerability Fixes</summary>

- *Bodin Chinthanet, Raula Gaikovina Kula, Shane McIntosh, Takashi Ishio, Akinori Ihara, Kenichi Matsumoto*

- `1907.03407v5` - [abs](http://arxiv.org/abs/1907.03407v5) - [pdf](http://arxiv.org/pdf/1907.03407v5)

> Security vulnerability in third-party dependencies is a growing concern not only for developers of the affected software, but for the risks it poses to an entire software ecosystem, e.g., Heartbleed vulnerability. Recent studies show that developers are slow to respond to the threat of vulnerability, sometimes taking four to eleven months to act. To ensure quick adoption and propagation of a release that contains the fix (fixing release), we conduct an empirical investigation to identify lags that may occur between the vulnerable release and its fixing release (package-side fixing release). Through a preliminary study of 231 package-side fixing release of npm projects on GitHub, we observe that a fixing release is rarely released on its own, with up to 85.72% of the bundled commits being unrelated to a fix. We then compare the package-side fixing release with changes on a client-side (client-side fixing release). Through an empirical study of the adoption and propagation tendencies of 1,290 package-side fixing releases that impact throughout a network of 1,553,325 releases of npm packages, we find that stale clients require additional migration effort, even if the package-side fixing release was quick (i.e., package patch landing). Furthermore, we show the influence of factors such as the branch that the package-side fixing release lands on and the severity of vulnerability on its propagation. In addition to these lags we identify and characterize, this paper lays the groundwork for future research on how to mitigate lags in an ecosystem.

</details>

<details>

<summary>2021-03-31 09:18:06 - A Lightweight and Scalable Physical Layer Attack Detection Mechanism for the Internet of Things (IoT) Using Hybrid Security Schema</summary>

- *Reza Fotohi, Hossein Pakdel*

- `2103.16920v1` - [abs](http://arxiv.org/abs/2103.16920v1) - [pdf](http://arxiv.org/pdf/2103.16920v1)

> The Internet of Things, also known as the IoT, refers to the billions of devices around the world that are now connected to the Internet, collecting and sharing data. The amount of data collected through IoT sensors must be completely securely controlled. To protect the information collected by IoT sensors, a lightweight method called Discover the Flooding Attack-RPL (DFA-RPL) has been proposed. The proposed DFA-RPL method identifies intrusive nodes in several steps to exclude them from continuing routing operations. Thus, in the DFA-RPL method, it first builds a cluster and selects the most appropriate node as a cluster head in DODAG, then, due to the vulnerability of the RPL protocol to Flooding attacks, it uses an ant colony algorithm (ACO) using five steps to detect attacks. Use Flooding to prevent malicious activity on the IoT network. In other words, if it detects a node as malicious, it puts that node on the detention list and quarantines it for a certain period of time. The results obtained from the simulation show the superiority of the proposed method in terms of Packet Delivery Rate, Detection Rate, False Positive Rate, and False Negative Rate compared to IRAD and REATO methods.

</details>

<details>

<summary>2021-03-31 10:41:28 - A Vulnerability Study on Academic Collaboration Networks Based on Network Dynamics</summary>

- *Asier Gutiérrez-Fandiño, Jordi Armengol-Estapé, Marta Villegas*

- `2012.11699v2` - [abs](http://arxiv.org/abs/2012.11699v2) - [pdf](http://arxiv.org/pdf/2012.11699v2)

> Researchers that work for the same institution use their email as the main communication tool. Email can be one of the most fruitful attack vectors of research institutions as they also contain access to all accounts and thus to all private information. We propose an approach for analyzing in terms of security research institutions' communication networks. We first obtained institutions' communication networks as well as a method to analyze possible breaches of collected emails. We downloaded the network of 4 different research centers, three from Spain and one from Portugal. We then ran simulations of Susceptible-Exposed-Infected-Recovered (SEIR) complex network dynamics model for analyzing the vulnerability of the network. More than half of the nodes have more than one security breach, and our simulation results show that more than 90\% of the networks' nodes are vulnerable. This method can be employed for enhancing security of research centers and can make email accounts' use security-aware. It may additionally open new research lines in communication security. Finally, we manifest that, due to confidentiality reasons, the sources we utilized for obtaining communication networks should not be providing the information that we were able to gather.

</details>

<details>

<summary>2021-03-31 14:44:58 - Adversarial Attacks and Defenses for Speech Recognition Systems</summary>

- *Piotr Żelasko, Sonal Joshi, Yiwen Shao, Jesus Villalba, Jan Trmal, Najim Dehak, Sanjeev Khudanpur*

- `2103.17122v1` - [abs](http://arxiv.org/abs/2103.17122v1) - [pdf](http://arxiv.org/pdf/2103.17122v1)

> The ubiquitous presence of machine learning systems in our lives necessitates research into their vulnerabilities and appropriate countermeasures. In particular, we investigate the effectiveness of adversarial attacks and defenses against automatic speech recognition (ASR) systems. We select two ASR models - a thoroughly studied DeepSpeech model and a more recent Espresso framework Transformer encoder-decoder model. We investigate two threat models: a denial-of-service scenario where fast gradient-sign method (FGSM) or weak projected gradient descent (PGD) attacks are used to degrade the model's word error rate (WER); and a targeted scenario where a more potent imperceptible attack forces the system to recognize a specific phrase. We find that the attack transferability across the investigated ASR systems is limited. To defend the model, we use two preprocessing defenses: randomized smoothing and WaveGAN-based vocoder, and find that they significantly improve the model's adversarial robustness. We show that a WaveGAN vocoder can be a useful countermeasure to adversarial attacks on ASR systems - even when it is jointly attacked with the ASR, the target phrases' word error rate is high.

</details>

<details>

<summary>2021-03-31 19:31:26 - DeepBlur: A Simple and Effective Method for Natural Image Obfuscation</summary>

- *Tao Li, Min Soo Choi*

- `2104.02655v1` - [abs](http://arxiv.org/abs/2104.02655v1) - [pdf](http://arxiv.org/pdf/2104.02655v1)

> There is a growing privacy concern due to the popularity of social media and surveillance systems, along with advances in face recognition software. However, established image obfuscation techniques are either vulnerable to re-identification attacks by human or deep learning models, insufficient in preserving image fidelity, or too computationally intensive to be practical. To tackle these issues, we present DeepBlur, a simple yet effective method for image obfuscation by blurring in the latent space of an unconditionally pre-trained generative model that is able to synthesize photo-realistic facial images. We compare it with existing methods by efficiency and image quality, and evaluate against both state-of-the-art deep learning models and industrial products (e.g., Face++, Microsoft face service). Experiments show that our method produces high quality outputs and is the strongest defense for most test cases.

</details>

<details>

<summary>2021-03-31 22:21:34 - Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses</summary>

- *Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, Tom Goldstein*

- `2012.10544v4` - [abs](http://arxiv.org/abs/2012.10544v4) - [pdf](http://arxiv.org/pdf/2012.10544v4)

> As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space. In addition to describing various poisoning and backdoor threat models and the relationships among them, we develop their unified taxonomy.

</details>


## 2021-04

<details>

<summary>2021-04-02 02:39:53 - An NCAP-like Safety Indicator for Self-Driving Cars</summary>

- *Jimy Cai Huang, Hanna Kurniawati*

- `2104.00859v1` - [abs](http://arxiv.org/abs/2104.00859v1) - [pdf](http://arxiv.org/pdf/2104.00859v1)

> This paper proposes a mechanism to assess the safety of autonomous cars. It assesses the car's safety in scenarios where the car must avoid collision with an adversary. Core to this mechanism is a safety measure, called Safe-Kamikaze Distance (SKD), which computes the average similarity between sets of safe adversary's trajectories and kamikaze trajectories close to the safe trajectories. The kamikaze trajectories are generated based on planning under uncertainty techniques, namely the Partially Observable Markov Decision Processes, to account for the partially observed car policy from the point of view of the adversary. We found that SKD is inversely proportional to the upper bound on the probability that a small deformation changes a collision-free trajectory of the adversary into a colliding one. We perform systematic tests on a scenario where the adversary is a pedestrian crossing a single-lane road in front of the car being assessed --which is, one of the scenarios in the Euro-NCAP's Vulnerable Road User (VRU) tests on Autonomous Emergency Braking. Simulation results on assessing cars with basic controllers and a test on a Machine-Learning controller using a high-fidelity simulator indicates promising results for SKD to measure the safety of autonomous cars. Moreover, the time taken for each simulation test is under 11 seconds, enabling a sufficient statistics to compute SKD from simulation to be generated on a quad-core desktop in less than 25 minutes.

</details>

<details>

<summary>2021-04-02 15:48:17 - Machine Learning Clustering Techniques for Selective Mitigation of Critical Design Features</summary>

- *Thomas Lange, Aneesh Balakrishnan, Maximilien Glorieux, Dan Alexandrescu, Luca Sterpone*

- `2008.13664v2` - [abs](http://arxiv.org/abs/2008.13664v2) - [pdf](http://arxiv.org/pdf/2008.13664v2)

> Selective mitigation or selective hardening is an effective technique to obtain a good trade-off between the improvements in the overall reliability of a circuit and the hardware overhead induced by the hardening techniques. Selective mitigation relies on preferentially protecting circuit instances according to their susceptibility and criticality. However, ranking circuit parts in terms of vulnerability usually requires computationally intensive fault-injection simulation campaigns. This paper presents a new methodology which uses machine learning clustering techniques to group flip-flops with similar expected contributions to the overall functional failure rate, based on the analysis of a compact set of features combining attributes from static elements and dynamic elements. Fault simulation campaigns can then be executed on a per-group basis, significantly reducing the time and cost of the evaluation. The effectiveness of grouping similar sensitive flip-flops by machine learning clustering algorithms is evaluated on a practical example.Different clustering algorithms are applied and the results are compared to an ideal selective mitigation obtained by exhaustive fault-injection simulation.

</details>

<details>

<summary>2021-04-03 00:22:38 - The COVID-19 Infodemic: Twitter versus Facebook</summary>

- *Kai-Cheng Yang, Francesco Pierri, Pik-Mai Hui, David Axelrod, Christopher Torres-Lugo, John Bryden, Filippo Menczer*

- `2012.09353v2` - [abs](http://arxiv.org/abs/2012.09353v2) - [pdf](http://arxiv.org/pdf/2012.09353v2)

> The global spread of the novel coronavirus is affected by the spread of related misinformation -- the so-called COVID-19 Infodemic -- that makes populations more vulnerable to the disease through resistance to mitigation efforts. Here we analyze the prevalence and diffusion of links to low-credibility content about the pandemic across two major social media platforms, Twitter and Facebook. We characterize cross-platform similarities and differences in popular sources, diffusion patterns, influencers, coordination, and automation. Comparing the two platforms, we find divergence among the prevalence of popular low-credibility sources and suspicious videos. A minority of accounts and pages exert a strong influence on each platform. These misinformation "superspreaders" are often associated with the low-credibility sources and tend to be verified by the platforms. On both platforms, there is evidence of coordinated sharing of Infodemic content. The overt nature of this manipulation points to the need for societal-level solutions in addition to mitigation strategies within the platforms. However, we highlight limits imposed by inconsistent data-access policies on our capability to study harmful manipulations of information ecosystems.

</details>

<details>

<summary>2021-04-03 05:07:33 - Hidden Backdoor Attack against Semantic Segmentation Models</summary>

- *Yiming Li, Yanjie Li, Yalei Lv, Yong Jiang, Shu-Tao Xia*

- `2103.04038v3` - [abs](http://arxiv.org/abs/2103.04038v3) - [pdf](http://arxiv.org/pdf/2103.04038v3)

> Deep neural networks (DNNs) are vulnerable to the \emph{backdoor attack}, which intends to embed hidden backdoors in DNNs by poisoning training data. The attacked model behaves normally on benign samples, whereas its prediction will be changed to a particular target label if hidden backdoors are activated. So far, backdoor research has mostly been conducted towards classification tasks. In this paper, we reveal that this threat could also happen in semantic segmentation, which may further endanger many mission-critical applications ($e.g.$, autonomous driving). Except for extending the existing attack paradigm to maliciously manipulate the segmentation models from the image-level, we propose a novel attack paradigm, the \emph{fine-grained attack}, where we treat the target label ($i.e.$, annotation) from the object-level instead of the image-level to achieve more sophisticated manipulation. In the annotation of poisoned samples generated by the fine-grained attack, only pixels of specific objects will be labeled with the attacker-specified target class while others are still with their ground-truth ones. Experiments show that the proposed methods can successfully attack semantic segmentation models by poisoning only a small proportion of training data. Our method not only provides a new perspective for designing novel attacks but also serves as a strong baseline for improving the robustness of semantic segmentation methods.

</details>

<details>

<summary>2021-04-03 22:28:04 - Gradient-based Adversarial Deep Modulation Classification with Data-driven Subsampling</summary>

- *Jinho Yi, Aly El Gamal*

- `2104.06375v1` - [abs](http://arxiv.org/abs/2104.06375v1) - [pdf](http://arxiv.org/pdf/2104.06375v1)

> Automatic modulation classification can be a core component for intelligent spectrally efficient wireless communication networks, and deep learning techniques have recently been shown to deliver superior performance to conventional model-based strategies, particularly when distinguishing between a large number of modulation types. However, such deep learning techniques have also been recently shown to be vulnerable to gradient-based adversarial attacks that rely on subtle input perturbations, which would be particularly feasible in a wireless setting via jamming. One such potent attack is the one known as the Carlini-Wagner attack, which we consider in this work. We further consider a data-driven subsampling setting, where several recently introduced deep-learning-based algorithms are employed to select a subset of samples that lead to reducing the final classifier's training time with minimal loss in accuracy. In this setting, the attacker has to make an assumption about the employed subsampling strategy, in order to calculate the loss gradient. Based on state of the art techniques available to both the attacker and defender, we evaluate best strategies under various assumptions on the knowledge of the other party's strategy. Interestingly, in presence of knowledgeable attackers, we identify computational cost reduction opportunities for the defender with no or minimal loss in performance.

</details>

<details>

<summary>2021-04-04 02:17:58 - Program Behavior Analysis and Clustering using Performance Counters</summary>

- *Sai Praveen Kadiyala, Akella Kartheek, Tram Truong-Huu*

- `2104.01518v1` - [abs](http://arxiv.org/abs/2104.01518v1) - [pdf](http://arxiv.org/pdf/2104.01518v1)

> Understanding the dynamic behavior of computer programs during normal working conditions is an important task, which has multiple security benefits such as the development of behavior-based anomaly detection, vulnerability discovery, and patching. Existing works achieved this goal by collecting and analyzing various data including network traffic, system calls, instruction traces, etc. In this paper, we explore the use of a new type of data, performance counters, to analyze the dynamic behavior of programs. Using existing primitives, we develop a tool named perfextract to capture data from different performance counters for a program during its startup time, thus forming multiple time series to represent the dynamic behavior of the program. We analyze the collected data and develop a semi-supervised clustering algorithm that allows us to classify each program using its performance counter time series into a specific group and to identify the intrinsic behavior of that group. We carry out extensive experiments with 18 real-world programs that belong to 4 groups including web browsers, text editors, image viewers, and audio players. The experimental results show that the examined programs can be accurately differentiated based on their performance counter data regardless of whether programs are run in physical or virtual environments.

</details>

<details>

<summary>2021-04-05 01:13:11 - Sensor-based Proximity Detection in the Face of Active Adversaries</summary>

- *Babins Shrestha, Nitesh Saxena, Hien Thi Thu Truong, N. Asokan*

- `1511.00905v2` - [abs](http://arxiv.org/abs/1511.00905v2) - [pdf](http://arxiv.org/pdf/1511.00905v2)

> Contextual proximity detection (or, co-presence detection) is a promising approach to defend against relay attacks in many mobile authentication systems. We present a systematic assessment of co-presence detection in the presence of a context-manipulating attacker. First, we show that it is feasible to manipulate, consistently control and stabilize the readings of different acoustic and physical environment sensors (and even multiple sensors simultaneously) using low-cost, off-the-shelf equipment. Second, based on these capabilities, we show that an attacker who can manipulate the context gains a significant advantage in defeating context-based co-presence detection. For systems that use multiple sensors, we investigate two sensor fusion approaches based on machine learning techniques: features-fusion and decisions-fusion, and show that both are vulnerable to contextual attacks but the latter approach can be more resistant in some cases.

</details>

<details>

<summary>2021-04-05 05:32:56 - BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification</summary>

- *Ishani Mondal*

- `2104.01782v1` - [abs](http://arxiv.org/abs/2104.01782v1) - [pdf](http://arxiv.org/pdf/2104.01782v1)

> Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the ever increasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERTMLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work.

</details>

<details>

<summary>2021-04-05 08:51:24 - Semi-supervised Variational Temporal Convolutional Network for IoT Communication Multi-anomaly Detection</summary>

- *Yan Xu, Yongliang Cheng*

- `2104.01813v1` - [abs](http://arxiv.org/abs/2104.01813v1) - [pdf](http://arxiv.org/pdf/2104.01813v1)

> The consumer Internet of Things (IoT) have developed in recent years. Mass IoT devices are constructed to build a huge communications network. But these devices are insecure in reality, it means that the communications network are exposed by the attacker. Moreover, the IoT communication network also faces with variety of sudden errors. Therefore, it easily leads to that is vulnerable with the threat of attacker and system failure. The severe situation of IoT communication network motivates the development of new techniques to automatically detect multi-anomaly. In this paper, we propose SS-VTCN, a semi-supervised network for IoT multiple anomaly detection that works well effectively for IoT communication network. SS-VTCN is designed to capture the normal patterns of the IoT traffic data based on the distribution whether it is labeled or not by learning their representations with key techniques such as Variational Autoencoders and Temporal Convolutional Network. This network can use the encode data to predict preliminary result, and reconstruct input data to determine anomalies by the representations. Extensive evaluation experiments based on a benchmark dataset and a real consumer smart home dataset demonstrate that SS-VTCN is more suitable than supervised and unsupervised method with better performance when compared other state-of-art semi-supervised method.

</details>

<details>

<summary>2021-04-05 23:31:25 - Robust Classification Under $\ell_0$ Attack for the Gaussian Mixture Model</summary>

- *Payam Delgosha, Hamed Hassani, Ramtin Pedarsani*

- `2104.02189v1` - [abs](http://arxiv.org/abs/2104.02189v1) - [pdf](http://arxiv.org/pdf/2104.02189v1)

> It is well-known that machine learning models are vulnerable to small but cleverly-designed adversarial perturbations that can cause misclassification. While there has been major progress in designing attacks and defenses for various adversarial settings, many fundamental and theoretical problems are yet to be resolved. In this paper, we consider classification in the presence of $\ell_0$-bounded adversarial perturbations, a.k.a. sparse attacks. This setting is significantly different from other $\ell_p$-adversarial settings, with $p\geq 1$, as the $\ell_0$-ball is non-convex and highly non-smooth. Under the assumption that data is distributed according to the Gaussian mixture model, our goal is to characterize the optimal robust classifier and the corresponding robust classification error as well as a variety of trade-offs between robustness, accuracy, and the adversary's budget. To this end, we develop a novel classification algorithm called FilTrun that has two main modules: Filtration and Truncation. The key idea of our method is to first filter out the non-robust coordinates of the input and then apply a carefully-designed truncated inner product for classification. By analyzing the performance of FilTrun, we derive an upper bound on the optimal robust classification error. We also find a lower bound by designing a specific adversarial strategy that enables us to derive the corresponding robust classifier and its achieved error. For the case that the covariance matrix of the Gaussian mixtures is diagonal, we show that as the input's dimension gets large, the upper and lower bounds converge; i.e. we characterize the asymptotically-optimal robust classifier. Throughout, we discuss several examples that illustrate interesting behaviors such as the existence of a phase transition for adversary's budget determining whether the effect of adversarial perturbation can be fully neutralized.

</details>

<details>

<summary>2021-04-06 08:59:06 - Adversarial Turing Patterns from Cellular Automata</summary>

- *Nurislam Tursynbek, Ilya Vilkoviskiy, Maria Sindeeva, Ivan Oseledets*

- `2011.09393v3` - [abs](http://arxiv.org/abs/2011.09393v3) - [pdf](http://arxiv.org/pdf/2011.09393v3)

> State-of-the-art deep classifiers are intriguingly vulnerable to universal adversarial perturbations: single disturbances of small magnitude that lead to misclassification of most in-puts. This phenomena may potentially result in a serious security problem. Despite the extensive research in this area,there is a lack of theoretical understanding of the structure of these perturbations. In image domain, there is a certain visual similarity between patterns, that represent these perturbations, and classical Turing patterns, which appear as a solution of non-linear partial differential equations and are underlying concept of many processes in nature. In this paper,we provide a theoretical bridge between these two different theories, by mapping a simplified algorithm for crafting universal perturbations to (inhomogeneous) cellular automata,the latter is known to generate Turing patterns. Furthermore,we propose to use Turing patterns, generated by cellular automata, as universal perturbations, and experimentally show that they significantly degrade the performance of deep learning models. We found this method to be a fast and efficient way to create a data-agnostic quasi-imperceptible perturbation in the black-box scenario. The source code is available at https://github.com/NurislamT/advTuring.

</details>

<details>

<summary>2021-04-06 14:19:31 - On the Generalization Properties of Adversarial Training</summary>

- *Yue Xing, Qifan Song, Guang Cheng*

- `2008.06631v2` - [abs](http://arxiv.org/abs/2008.06631v2) - [pdf](http://arxiv.org/pdf/2008.06631v2)

> Modern machine learning and deep learning models are shown to be vulnerable when testing data are slightly perturbed. Existing theoretical studies of adversarial training algorithms mostly focus on either adversarial training losses or local convergence properties. In contrast, this paper studies the generalization performance of a generic adversarial training algorithm. Specifically, we consider linear regression models and two-layer neural networks (with lazy training) using squared loss under low-dimensional and high-dimensional regimes. In the former regime, after overcoming the non-smoothness of adversarial training, the adversarial risk of the trained models can converge to the minimal adversarial risk. In the latter regime, we discover that data interpolation prevents the adversarially robust estimator from being consistent. Therefore, inspired by successes of the least absolute shrinkage and selection operator (LASSO), we incorporate the L1 penalty in the high dimensional adversarial learning and show that it leads to consistent adversarially robust estimation. A series of numerical studies are conducted to demonstrate how the smoothness and L1 penalization help improve the adversarial robustness of DNN models.

</details>

<details>

<summary>2021-04-06 14:46:00 - RFQuack: A Universal Hardware-Software Toolkit for Wireless Protocol (Security) Analysis and Research</summary>

- *Federico Maggi, Andrea Guglielmini*

- `2104.02551v1` - [abs](http://arxiv.org/abs/2104.02551v1) - [pdf](http://arxiv.org/pdf/2104.02551v1)

> Software-defined radios (SDRs) are indispensable for signal reconnaissance and physical-layer dissection, but despite we have advanced tools like Universal Radio Hacker, SDR-based approaches require substantial effort.   Contrarily, RF dongles such as the popular Yard Stick One are easy to use and guarantee a deterministic physical-layer implementation. However, they're not very flexible, as each dongle is a static hardware system with a monolithic firmware.   We present RFquack, an open-source tool and library firmware that combines the flexibility of a software-based approach with the determinism and performance of embedded RF frontends. RFquack is based on a multi-radio hardware system with swappable RF frontends, and a firmware that exposes a uniform, hardware-agnostic API. RFquack focuses on a structured firmware architecture that allows high- and low-level interaction with the RF frontends. It facilitates the development of host-side scripts and firmware plug-ins, to implement efficient data-processing pipelines or interactive protocols, thanks to the multi-radio support. RFquack has an IPython shell and 9 firmware modules for: spectrum scanning, automatic carrier detection and bitrate estimation, headless operation with remote management, in-flight packet filtering and manipulation, MouseJack, and RollJam (as examples).   We used RFquack to setup RF hacking contests, analyze industrial-grade devices and key fobs, on which we found and reported 11 vulnerabilities in their RF protocols.

</details>

<details>

<summary>2021-04-06 16:30:22 - PriFi: Low-Latency Anonymity for Organizational Networks</summary>

- *Ludovic Barman, Italo Dacosta, Mahdi Zamani, Ennan Zhai, Apostolos Pyrgelis, Bryan Ford, Jean-Pierre Hubaux, Joan Feigenbaum*

- `1710.10237v9` - [abs](http://arxiv.org/abs/1710.10237v9) - [pdf](http://arxiv.org/pdf/1710.10237v9)

> Organizational networks are vulnerable to traffic-analysis attacks that enable adversaries to infer sensitive information from the network traffic - even if encryption is used. Typical anonymous communication networks are tailored to the Internet and are poorly suited for organizational networks. We present PriFi, an anonymous communication protocol for LANs, which protects users against eavesdroppers and provides high-performance traffic-analysis resistance. PriFi builds on Dining Cryptographers networks but reduces the high communication latency of prior work via a new client/relay/server architecture, in which a client's packets remain on their usual network path without additional hops, and in which a set of remote servers assist the anonymization process without adding latency. PriFi also solves the challenge of equivocation attacks, which are not addressed by related works, by encrypting the traffic based on the communication history. Our evaluation shows that PriFi introduces a small latency overhead (~100ms for 100 clients) and is compatible with delay-sensitive applications such as VoIP.

</details>

<details>

<summary>2021-04-06 18:36:44 - On Adversarial Robustness of 3D Point Cloud Classification under Adaptive Attacks</summary>

- *Jiachen Sun, Karl Koenig, Yulong Cao, Qi Alfred Chen, Z. Morley Mao*

- `2011.11922v2` - [abs](http://arxiv.org/abs/2011.11922v2) - [pdf](http://arxiv.org/pdf/2011.11922v2)

> 3D point clouds play pivotal roles in various safety-critical applications, such as autonomous driving, which desires the underlying deep neural networks to be robust to adversarial perturbations. Though a few defenses against adversarial point cloud classification have been proposed, it remains unknown whether they are truly robust to adaptive attacks. To this end, we perform the first security analysis of state-of-the-art defenses and design adaptive evaluations on them. Our 100% adaptive attack success rates show that current countermeasures are still vulnerable. Since adversarial training (AT) is believed as the most robust defense, we present the first in-depth study showing how AT behaves in point cloud classification and identify that the required symmetric function (pooling operation) is paramount to the 3D model's robustness under AT. Through our systematic analysis, we find that the default-used fixed pooling (e.g., MAX pooling) generally weakens AT's effectiveness in point cloud classification. Interestingly, we further discover that sorting-based parametric pooling can significantly improve the models' robustness. Based on above insights, we propose DeepSym, a deep symmetric pooling operation, to architecturally advance the robustness to 47.0% under AT without sacrificing nominal accuracy, outperforming the original design and a strong baseline by 28.5% ($\sim 2.6 \times$) and 6.5%, respectively, in PointNet.

</details>

<details>

<summary>2021-04-06 19:39:05 - Exploring Targeted Universal Adversarial Perturbations to End-to-end ASR Models</summary>

- *Zhiyun Lu, Wei Han, Yu Zhang, Liangliang Cao*

- `2104.02757v1` - [abs](http://arxiv.org/abs/2104.02757v1) - [pdf](http://arxiv.org/pdf/2104.02757v1)

> Although end-to-end automatic speech recognition (e2e ASR) models are widely deployed in many applications, there have been very few studies to understand models' robustness against adversarial perturbations. In this paper, we explore whether a targeted universal perturbation vector exists for e2e ASR models. Our goal is to find perturbations that can mislead the models to predict the given targeted transcript such as "thank you" or empty string on any input utterance. We study two different attacks, namely additive and prepending perturbations, and their performances on the state-of-the-art LAS, CTC and RNN-T models. We find that LAS is the most vulnerable to perturbations among the three models. RNN-T is more robust against additive perturbations, especially on long utterances. And CTC is robust against both additive and prepending perturbations. To attack RNN-T, we find prepending perturbation is more effective than the additive perturbation, and can mislead the models to predict the same short target on utterances of arbitrary length.

</details>

<details>

<summary>2021-04-07 09:05:49 - Universal Adversarial Training with Class-Wise Perturbations</summary>

- *Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon*

- `2104.03000v1` - [abs](http://arxiv.org/abs/2104.03000v1) - [pdf](http://arxiv.org/pdf/2104.03000v1)

> Despite their overwhelming success on a wide range of applications, convolutional neural networks (CNNs) are widely recognized to be vulnerable to adversarial examples. This intriguing phenomenon led to a competition between adversarial attacks and defense techniques. So far, adversarial training is the most widely used method for defending against adversarial attacks. It has also been extended to defend against universal adversarial perturbations (UAPs). The SOTA universal adversarial training (UAT) method optimizes a single perturbation for all training samples in the mini-batch. In this work, we find that a UAP does not attack all classes equally. Inspired by this observation, we identify it as the source of the model having unbalanced robustness. To this end, we improve the SOTA UAT by proposing to utilize class-wise UAPs during adversarial training. On multiple benchmark datasets, our class-wise UAT leads superior performance for both clean accuracy and adversarial robustness against universal attack.

</details>

<details>

<summary>2021-04-07 11:02:03 - MIPGAN -- Generating Strong and High Quality Morphing Attacks Using Identity Prior Driven GAN</summary>

- *Haoyu Zhang, Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja, Naser Damer, Christoph Busch*

- `2009.01729v3` - [abs](http://arxiv.org/abs/2009.01729v3) - [pdf](http://arxiv.org/pdf/2009.01729v3)

> Face morphing attacks target to circumvent Face Recognition Systems (FRS) by employing face images derived from multiple data subjects (e.g., accomplices and malicious actors). Morphed images can be verified against contributing data subjects with a reasonable success rate, given they have a high degree of facial resemblance. The success of morphing attacks is directly dependent on the quality of the generated morph images. We present a new approach for generating strong attacks extending our earlier framework for generating face morphs. We present a new approach using an Identity Prior Driven Generative Adversarial Network, which we refer to as MIPGAN (Morphing through Identity Prior driven GAN). The proposed MIPGAN is derived from the StyleGAN with a newly formulated loss function exploiting perceptual quality and identity factor to generate a high quality morphed facial image with minimal artefacts and with high resolution. We demonstrate the proposed approach's applicability to generate strong morphing attacks by evaluating its vulnerability against both commercial and deep learning based Face Recognition System (FRS) and demonstrate the success rate of attacks. Extensive experiments are carried out to assess the FRS's vulnerability against the proposed morphed face generation technique on three types of data such as digital images, re-digitized (printed and scanned) images, and compressed images after re-digitization from newly generated MIPGAN Face Morph Dataset. The obtained results demonstrate that the proposed approach of morph generation poses a high threat to FRS.

</details>

<details>

<summary>2021-04-07 19:08:24 - Universal Spectral Adversarial Attacks for Deformable Shapes</summary>

- *Arianna Rampini, Franco Pestarini, Luca Cosmo, Simone Melzi, Emanuele Rodolà*

- `2104.03356v1` - [abs](http://arxiv.org/abs/2104.03356v1) - [pdf](http://arxiv.org/pdf/2104.03356v1)

> Machine learning models are known to be vulnerable to adversarial attacks, namely perturbations of the data that lead to wrong predictions despite being imperceptible. However, the existence of "universal" attacks (i.e., unique perturbations that transfer across different data points) has only been demonstrated for images to date. Part of the reason lies in the lack of a common domain, for geometric data such as graphs, meshes, and point clouds, where a universal perturbation can be defined. In this paper, we offer a change in perspective and demonstrate the existence of universal attacks for geometric data (shapes). We introduce a computational procedure that operates entirely in the spectral domain, where the attacks take the form of small perturbations to short eigenvalue sequences; the resulting geometry is then synthesized via shape-from-spectrum recovery. Our attacks are universal, in that they transfer across different shapes, different representations (meshes and point clouds), and generalize to previously unseen data.

</details>

<details>

<summary>2021-04-07 21:03:23 - Universal Adversarial Attacks with Natural Triggers for Text Classification</summary>

- *Liwei Song, Xinwei Yu, Hsuan-Tung Peng, Karthik Narasimhan*

- `2005.00174v2` - [abs](http://arxiv.org/abs/2005.00174v2) - [pdf](http://arxiv.org/pdf/2005.00174v2)

> Recent work has demonstrated the vulnerability of modern text classifiers to universal adversarial attacks, which are input-agnostic sequences of words added to text processed by classifiers. Despite being successful, the word sequences produced in such attacks are often ungrammatical and can be easily distinguished from natural text. We develop adversarial attacks that appear closer to natural English phrases and yet confuse classification systems when added to benign inputs. We leverage an adversarially regularized autoencoder (ARAE) to generate triggers and propose a gradient-based search that aims to maximize the downstream classifier's prediction loss. Our attacks effectively reduce model accuracy on classification tasks while being less identifiable than prior models as per automatic detection metrics and human-subject studies. Our aim is to demonstrate that adversarial attacks can be made harder to detect than previously thought and to enable the development of appropriate defenses.

</details>

<details>

<summary>2021-04-08 04:18:45 - Half-Duplex Attack: An Effectual Attack Modelling in D2D Communication</summary>

- *Misbah Shafi, Rakesh Kumar Jha*

- `2104.03499v1` - [abs](http://arxiv.org/abs/2104.03499v1) - [pdf](http://arxiv.org/pdf/2104.03499v1)

> The visualization of future generation Wireless Communication Network WCN redirects the presumption of onward innovations, the fulfillment of user demands in the form of high data rates, energy efficiency, low latency, and long-range services. To content these demands, various technologies such as massive MIMO Multiple Input Multiple Output, UDN Ultra Dense Network, spectrum sharing, D2D Device to Device communication were improvised in the next generation WCN. In comparison to previous technologies, these technologies exhibit flat architecture, the involvement of clouds in the network, centralized architecture incorporating small cells which creates vulnerable breaches initiating menaces to the security of the network. The half-duplex attack is another threat to the WCN, where the resource spoofing mechanism is attained in the downlink phase of D2D communication. Instead of triggering an attack on both uplink and downlink, solely downlink is targeted by the attacker. This scheme allows the reduced failed attempt rate of the attacker as compared to the conventional attacks. The analysis is determined on the basis of Poissons distribution to determine the probability of failed attempts of half duplex attack in contrast to a full duplex attack

</details>

<details>

<summary>2021-04-08 20:56:58 - Adversarial Learning Inspired Emerging Side-Channel Attacks and Defenses</summary>

- *Abhijitt Dhavlle*

- `2104.04054v1` - [abs](http://arxiv.org/abs/2104.04054v1) - [pdf](http://arxiv.org/pdf/2104.04054v1)

> Evolving attacks on the vulnerabilities of the computing systems demand novel defense strategies to keep pace with newer attacks. This report discusses previous works on side-channel attacks (SCAs) and defenses for cache-targeted and physical proximity attacks. We then discuss the proposed Entropy-Shield as a defense against timing SCAs, and explain how we can extend the same to hardware-based implementations of crypto applications as "Entropy-Shield for FPGA". We then discuss why we want to build newer attacks with the hope of coming up with better defense strategies.

</details>

<details>

<summary>2021-04-08 23:00:25 - FACESEC: A Fine-grained Robustness Evaluation Framework for Face Recognition Systems</summary>

- *Liang Tong, Zhengzhang Chen, Jingchao Ni, Wei Cheng, Dongjin Song, Haifeng Chen, Yevgeniy Vorobeychik*

- `2104.04107v1` - [abs](http://arxiv.org/abs/2104.04107v1) - [pdf](http://arxiv.org/pdf/2104.04107v1)

> We present FACESEC, a framework for fine-grained robustness evaluation of face recognition systems. FACESEC evaluation is performed along four dimensions of adversarial modeling: the nature of perturbation (e.g., pixel-level or face accessories), the attacker's system knowledge (about training data and learning architecture), goals (dodging or impersonation), and capability (tailored to individual inputs or across sets of these). We use FACESEC to study five face recognition systems in both closed-set and open-set settings, and to evaluate the state-of-the-art approach for defending against physically realizable attacks on these. We find that accurate knowledge of neural architecture is significantly more important than knowledge of the training data in black-box attacks. Moreover, we observe that open-set face recognition systems are more vulnerable than closed-set systems under different types of attacks. The efficacy of attacks for other threat model variations, however, appears highly dependent on both the nature of perturbation and the neural network architecture. For example, attacks that involve adversarial face masks are usually more potent, even against adversarially trained models, and the ArcFace architecture tends to be more robust than the others.

</details>

<details>

<summary>2021-04-09 13:16:06 - SchedGuard: Protecting against Schedule Leaks Using Linux Containers</summary>

- *Jiyang Chen, Tomasz Kloda, Ayoosh Bansal, Rohan Tabish, Chien-Ying Chen, Bo Liu, Sibin Mohan, Marco Caccamo, Lui Sha*

- `2104.04528v1` - [abs](http://arxiv.org/abs/2104.04528v1) - [pdf](http://arxiv.org/pdf/2104.04528v1)

> Real-time systems have recently been shown to be vulnerable to timing inference attacks, mainly due to their predictable behavioral patterns. Existing solutions such as schedule randomization lack the ability to protect against such attacks, often limited by the system's real-time nature. This paper presents SchedGuard: a temporal protection framework for Linux-based hard real-time systems that protects against posterior scheduler side-channel attacks by preventing untrusted tasks from executing during specific time segments. SchedGuard is integrated into the Linux kernel using cgroups, making it amenable to use with container frameworks. We demonstrate the effectiveness of our system using a realistic radio-controlled rover platform and synthetically generated workloads. Not only is SchedGuard able to protect against the attacks mentioned above, but it also ensures that the real-time tasks/containers meet their temporal requirements.

</details>

<details>

<summary>2021-04-09 16:53:02 - The Motivated Can Encrypt (Even with PGP)</summary>

- *Glencora Borradaile, Kelsy Kretschmer, Michele Gretes, Alexandria LeClerc*

- `2104.04478v1` - [abs](http://arxiv.org/abs/2104.04478v1) - [pdf](http://arxiv.org/pdf/2104.04478v1)

> Existing end-to-end-encrypted (E2EE) email systems, mainly PGP, have long been evaluated in controlled lab settings. While these studies have exposed usability obstacles for the average user and offer design improvements, there exist users with an immediate need for private communication, who must cope with existing software and its limitations. We seek to understand whether individuals motivated by concrete privacy threats, such as those vulnerable to state surveillance, can overcome usability issues to adopt complex E2EE tools for long-term use. We surveyed regional activists, as surveillance of social movements is well-documented. Our study group includes individuals from 9 social movement groups in the US who had elected to participate in a workshop on using Thunderbird+Enigmail for email encryption. These workshops tool place prior to mid-2017, via a partnership with a non-profit which supports social movement groups. Six to 40 months after their PGP email encryption training, more than half of the study participants were continuing to use PGP email encryption despite intervening widespread deployment of simple E2EE messaging apps such as Signal. We study the interplay of usability with social factors such as motivation and the risks that individuals undertake through their activism. We find that while usability is an important factor, it is not enough to explain long term use. For example, we find that riskiness of one's activism is negatively correlated with long-term PGP use. This study represents the first long-term study, and the first in-the-wild study, of PGP email encryption adoption.

</details>

<details>

<summary>2021-04-09 23:07:07 - Automated Meta-Analysis: A Causal Learning Perspective</summary>

- *Lu Cheng, Dmitriy A. Katz-Rogozhnikov, Kush R. Varshney, Ioana Baldini*

- `2104.04633v1` - [abs](http://arxiv.org/abs/2104.04633v1) - [pdf](http://arxiv.org/pdf/2104.04633v1)

> Meta-analysis is a systematic approach for understanding a phenomenon by analyzing the results of many previously published experimental studies. It is central to deriving conclusions about the summary effect of treatments and interventions in medicine, poverty alleviation, and other applications with social impact. Unfortunately, meta-analysis involves great human effort, rendering a process that is extremely inefficient and vulnerable to human bias. To overcome these issues, we work toward automating meta-analysis with a focus on controlling for risks of bias. In particular, we first extract information from scientific publications written in natural language. From a novel causal learning perspective, we then propose to frame automated meta-analysis -- based on the input of the first step -- as a multiple-causal-inference problem where the summary effect is obtained through intervention. Built upon existing efforts for automating the initial steps of meta-analysis, the proposed approach achieves the goal of automated meta-analysis and largely reduces the human effort involved. Evaluations on synthetic and semi-synthetic datasets show that this approach can yield promising results.

</details>

<details>

<summary>2021-04-10 19:48:33 - Unified Pre-training for Program Understanding and Generation</summary>

- *Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang*

- `2103.06333v2` - [abs](http://arxiv.org/abs/2103.06333v2) - [pdf](http://arxiv.org/pdf/2103.06333v2)

> Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART's effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., if block inside an else block is equivalent to else if block) that are crucial to program semantics and thus excels even with limited annotations.

</details>

<details>

<summary>2021-04-12 03:12:53 - EvaLDA: Efficient Evasion Attacks Towards Latent Dirichlet Allocation</summary>

- *Qi Zhou, Haipeng Chen, Yitao Zheng, Zhen Wang*

- `2012.04864v2` - [abs](http://arxiv.org/abs/2012.04864v2) - [pdf](http://arxiv.org/pdf/2012.04864v2)

> As one of the most powerful topic models, Latent Dirichlet Allocation (LDA) has been used in a vast range of tasks, including document understanding, information retrieval and peer-reviewer assignment. Despite its tremendous popularity, the security of LDA has rarely been studied. This poses severe risks to security-critical tasks such as sentiment analysis and peer-reviewer assignment that are based on LDA. In this paper, we are interested in knowing whether LDA models are vulnerable to adversarial perturbations of benign document examples during inference time. We formalize the evasion attack to LDA models as an optimization problem and prove it to be NP-hard. We then propose a novel and efficient algorithm, EvaLDA to solve it. We show the effectiveness of EvaLDA via extensive empirical evaluations. For instance, in the NIPS dataset, EvaLDA can averagely promote the rank of a target topic from 10 to around 7 by only replacing 1% of the words with similar words in a victim document. Our work provides significant insights into the power and limitations of evasion attacks to LDA models.

</details>

<details>

<summary>2021-04-12 03:39:01 - Ethereum Name Service: the Good, the Bad, and the Ugly</summary>

- *Pengcheng Xia, Haoyu Wang, Zhou Yu, Xinyu Liu, Xiapu Luo, Guoai Xu*

- `2104.05185v1` - [abs](http://arxiv.org/abs/2104.05185v1) - [pdf](http://arxiv.org/pdf/2104.05185v1)

> DNS has always been criticized for its inherent design flaws, making the system vulnerable to kinds of attacks. Besides, DNS domain names are not fully controlled by the users, which can be easily taken down by the authorities and registrars. Since blockchain has its unique properties like immutability and decentralization, it seems to be promising to build a decentralized name service on blockchain. Ethereum Name Service (ENS), as a novel name service built atop Etheruem, has received great attention from the community. Yet, no existing work has systematically studied this emerging system, especially the security issues and misbehaviors in ENS. To fill the void, we present the first large-scale study of ENS by collecting and analyzing millions of event logs related to ENS. We characterize the ENS system from a number of perspectives. Our findings suggest that ENS is showing gradually popularity during its four years' evolution, mainly due to its distributed and open nature that ENS domain names can be set to any kinds of records, even censored and malicious contents. We have identified several security issues and misbehaviors including traditional DNS security issues and new issues introduced by ENS smart contracts. Attackers are abusing the system with thousands of squatting ENS names, a number of scam blockchain addresses and malicious websites, etc. Our exploration suggests that our community should invest more effort into the detection and mitigation of issues in Blockchain-based Name Services towards building an open and trustworthy name service.

</details>

<details>

<summary>2021-04-12 06:57:36 - Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation</summary>

- *Chong Zhang, Jieyu Zhao, Huan Zhang, Kai-Wei Chang, Cho-Jui Hsieh*

- `2104.05232v1` - [abs](http://arxiv.org/abs/2104.05232v1) - [pdf](http://arxiv.org/pdf/2104.05232v1)

> Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a "double perturbation" framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models' robustness and counterfactual bias in English. (1) For robustness, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed attack attains high success rates (96.0%-99.8%) in finding vulnerable examples on both original and robustly trained CNNs and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/nlp-second-order-attack.

</details>

<details>

<summary>2021-04-12 11:14:32 - Sparse Coding Frontend for Robust Neural Networks</summary>

- *Can Bakiskan, Metehan Cekic, Ahmet Dundar Sezer, Upamanyu Madhow*

- `2104.05353v1` - [abs](http://arxiv.org/abs/2104.05353v1) - [pdf](http://arxiv.org/pdf/2104.05353v1)

> Deep Neural Networks are known to be vulnerable to small, adversarially crafted, perturbations. The current most effective defense methods against these adversarial attacks are variants of adversarial training. In this paper, we introduce a radically different defense trained only on clean images: a sparse coding based frontend which significantly attenuates adversarial attacks before they reach the classifier. We evaluate our defense on CIFAR-10 dataset under a wide range of attack types (including Linf , L2, and L1 bounded attacks), demonstrating its promise as a general-purpose approach for defense.

</details>

<details>

<summary>2021-04-12 11:51:50 - Measurements of the Most Significant Software Security Weaknesses</summary>

- *Carlos Cardoso Galhardo, Peter Mell, Irena Bojanova, Assane Gueye*

- `2104.05375v1` - [abs](http://arxiv.org/abs/2104.05375v1) - [pdf](http://arxiv.org/pdf/2104.05375v1)

> In this work, we provide a metric to calculate the most significant software security weaknesses as defined by an aggregate metric of the frequency, exploitability, and impact of related vulnerabilities. The Common Weakness Enumeration (CWE) is a well-known and used list of software security weaknesses. The CWE community publishes such an aggregate metric to calculate the `Most Dangerous Software Errors'. However, we find that the published equation highly biases frequency and almost ignores exploitability and impact in generating top lists of varying sizes. This is due to the differences in the distributions of the component metric values. To mitigate this, we linearize the frequency distribution using a double log function. We then propose a variety of other improvements, provide top lists of the most significant CWEs for 2019, provide an analysis of the identified software security weaknesses, and compare them against previously published top lists.

</details>

<details>

<summary>2021-04-12 20:47:48 - A Backdoor Attack against 3D Point Cloud Classifiers</summary>

- *Zhen Xiang, David J. Miller, Siheng Chen, Xi Li, George Kesidis*

- `2104.05808v1` - [abs](http://arxiv.org/abs/2104.05808v1) - [pdf](http://arxiv.org/pdf/2104.05808v1)

> Vulnerability of 3D point cloud (PC) classifiers has become a grave concern due to the popularity of 3D sensors in safety-critical applications. Existing adversarial attacks against 3D PC classifiers are all test-time evasion (TTE) attacks that aim to induce test-time misclassifications using knowledge of the classifier. But since the victim classifier is usually not accessible to the attacker, the threat is largely diminished in practice, as PC TTEs typically have poor transferability. Here, we propose the first backdoor attack (BA) against PC classifiers. Originally proposed for images, BAs poison the victim classifier's training set so that the classifier learns to decide to the attacker's target class whenever the attacker's backdoor pattern is present in a given input sample. Significantly, BAs do not require knowledge of the victim classifier. Different from image BAs, we propose to insert a cluster of points into a PC as a robust backdoor pattern customized for 3D PCs. Such clusters are also consistent with a physical attack (i.e., with a captured object in a scene). We optimize the cluster's location using an independently trained surrogate classifier and choose the cluster's local geometry to evade possible PC preprocessing and PC anomaly detectors (ADs). Experimentally, our BA achieves a uniformly high success rate (> 87%) and shows evasiveness against state-of-the-art PC ADs.

</details>

<details>

<summary>2021-04-13 05:22:04 - Data-driven Design of Context-aware Monitors for Hazard Prediction in Artificial Pancreas Systems</summary>

- *Xugui Zhou, Bulbul Ahmed, James H. Aylor, Philip Asare, Homa Alemzadeh*

- `2104.02545v2` - [abs](http://arxiv.org/abs/2104.02545v2) - [pdf](http://arxiv.org/pdf/2104.02545v2)

> Medical Cyber-physical Systems (MCPS) are vulnerable to accidental or malicious faults that can target their controllers and cause safety hazards and harm to patients. This paper proposes a combined model and data-driven approach for designing context-aware monitors that can detect early signs of hazards and mitigate them in MCPS. We present a framework for formal specification of unsafe system context using Signal Temporal Logic (STL) combined with an optimization method for patient-specific refinement of STL formulas based on real or simulated faulty data from the closed-loop system for the generation of monitor logic. We evaluate our approach in simulation using two state-of-the-art closed-loop Artificial Pancreas Systems (APS). The results show the context-aware monitor achieves up to 1.4 times increase in average hazard prediction accuracy (F1-score) over several baseline monitors, reduces false-positive and false-negative rates, and enables hazard mitigation with a 54% success rate while decreasing the average risk for patients.

</details>

<details>

<summary>2021-04-13 12:22:10 - WAIT: Protecting the Integrity of Web Applications with Binary-Equivalent Transparency</summary>

- *Echo Meißner, Frank Kargl, Benjamin Erb*

- `2104.06136v1` - [abs](http://arxiv.org/abs/2104.06136v1) - [pdf](http://arxiv.org/pdf/2104.06136v1)

> Modern single page web applications require client-side executions of application logic, including critical functionality such as client-side cryptography. Existing mechanisms such as TLS and Subresource Integrity secure the communication and provide external resource integrity. However, the browser is unaware of modifications to the client-side application as provided by the server and the user remains vulnerable against malicious modifications carried out on the server side. Our solution makes such modifications transparent and empowers the browser to validate the integrity of a web application based on a publicly verifiable log. Our Web Application Integrity Transparency (WAIT) approach requires (1) an extension for browsers for local integrity validations, (2) a custom HTTP header for web servers that host the application, and (3) public log servers that serve the verifiable logs. With WAIT, the browser can disallow the execution of undisclosed application changes. Also, web application providers cannot dispute their authorship for published modifications anymore. Although our approach cannot prevent every conceivable attack on client-side web application integrity, it introduces a novel sense of transparency for users and an increased level of accountability for application providers particularly effective against targeted insider attacks.

</details>

<details>

<summary>2021-04-13 20:34:38 - A multiagent based framework secured with layered SVM-based IDS for remote healthcare systems</summary>

- *Mohammadreza Begli, Farnaz Derakhshan*

- `2104.06498v1` - [abs](http://arxiv.org/abs/2104.06498v1) - [pdf](http://arxiv.org/pdf/2104.06498v1)

> Since the number of elderly and patients who are in hospitals and healthcare centers are growing, providing efficient remote healthcare services seems very important. Currently, most such systems benefit from the distribution and autonomy features of multiagent systems and the structure of wireless sensor networks. On the one hand, securing the data of remote healthcare systems is one of the most significant concerns; particularly recent types of research about the security of remote healthcare systems keep them secure from eavesdropping and data modification. On the other hand, existing remote healthcare systems are still vulnerable against other common attacks of healthcare networks such as Denial of Service (DoS) and User to Root (U2R) attacks, because they are managed remotely and based on the Internet. Therefore, in this paper, we propose a secure framework for remote healthcare systems that consists of two phases. First, we design a healthcare system base on multiagent technology to collect data from a sensor network. Then, in the second phase, a layered architecture of intrusion detection systems that uses Support Vector Machine to learn the behavior of network traffic is applied. Based on our framework, we implement a secure remote healthcare system and evaluate this system against the frequent attacks of healthcare networks such as Smurf, Buffer overflow, Neptune, and Pod attacks. In the end, evaluation parameters of the layered architecture of intrusion detection systems prove the efficiency and correctness of our proposed framework.

</details>

<details>

<summary>2021-04-13 22:55:37 - Coinbugs: Enumerating Common Blockchain Implementation-Level Vulnerabilities</summary>

- *Aleksandar Kircanski, Terence Tarvis*

- `2104.06540v1` - [abs](http://arxiv.org/abs/2104.06540v1) - [pdf](http://arxiv.org/pdf/2104.06540v1)

> A good amount of effort has been dedicated to surveying and systematizing Ethereum smart contract security bug classes. There is, however, a gap in literature when it comes to surveying implementation-level security bugs that commonly occur in basic PoW blockchain node implementations, discovered during the first decade of Bitcoin's existence. This paper attempts to fill this void. In particular, if software which participates in a network by validating and generating new blocks is developed from scratch, WCGW - What Could Go Wrong?   Ten broad bug type categories are listed and for each category, known examples are linked. Blockchain, as designed by the Satoshi's paper is exciting and introduces several novel bug classes which are interesting to security researchers. The paper is aimed at security testers aiming to start out in blockchain security reviews and blockchain developers as a reference on common pitfalls.

</details>

<details>

<summary>2021-04-14 00:08:45 - Towards Causal Federated Learning For Enhanced Robustness and Privacy</summary>

- *Sreya Francis, Irene Tenison, Irina Rish*

- `2104.06557v1` - [abs](http://arxiv.org/abs/2104.06557v1) - [pdf](http://arxiv.org/pdf/2104.06557v1)

> Federated Learning is an emerging privacy-preserving distributed machine learning approach to building a shared model by performing distributed training locally on participating devices (clients) and aggregating the local models into a global one. As this approach prevents data collection and aggregation, it helps in reducing associated privacy risks to a great extent. However, the data samples across all participating clients are usually not independent and identically distributed (non-iid), and Out of Distribution(OOD) generalization for the learned models can be poor. Besides this challenge, federated learning also remains vulnerable to various attacks on security wherein a few malicious participating entities work towards inserting backdoors, degrading the generated aggregated model as well as inferring the data owned by participating entities. In this paper, we propose an approach for learning invariant (causal) features common to all participating clients in a federated learning setup and analyze empirically how it enhances the Out of Distribution (OOD) accuracy as well as the privacy of the final learned model.

</details>

<details>

<summary>2021-04-15 09:40:59 - SDN-Based Intrusion Detection System for Early Detection and Mitigation of DDoS Attacks</summary>

- *Pedro Manso, Jose Moura, Carlos Serrao*

- `2104.07332v1` - [abs](http://arxiv.org/abs/2104.07332v1) - [pdf](http://arxiv.org/pdf/2104.07332v1)

> The current paper addresses relevant network security vulnerabilities introduced by network devices within the emerging paradigm of Internet of Things (IoT) as well as the urgent need to mitigate the negative effects of some types of Distributed Denial of Service (DDoS) attacks that try to explore those security weaknesses. We design and implement a Software-Defined Intrusion Detection System (IDS) that reactively impairs the attacks at its origin, ensuring the normal operation of the network infrastructure. Our proposal includes an IDS that automatically detects several DDoS attacks, and then as an attack is detected, it notifies a Software Defined Networking (SDN) controller. The current proposal also downloads some convenient traffic forwarding decisions from the SDN controller to network devices. The evaluation results suggest that our proposal timely detects several types of cyber-attacks based on DDoS, mitigates their negative impacts on the network performance, and ensures the correct data delivery of normal traffic. Our work sheds light on the programming relevance over an abstracted view of the network infrastructure to timely detect a Botnet exploitation, mitigate malicious traffic at its source, and protect benign traffic.

</details>

<details>

<summary>2021-04-15 11:07:56 - UIT-E10dot3 at SemEval-2021 Task 5: Toxic Spans Detection with Named Entity Recognition and Question-Answering Approaches</summary>

- *Phu Gia Hoang, Luan Thanh Nguyen, Kiet Van Nguyen*

- `2104.07376v1` - [abs](http://arxiv.org/abs/2104.07376v1) - [pdf](http://arxiv.org/pdf/2104.07376v1)

> The increment of toxic comments on online space is causing tremendous effects on other vulnerable users. For this reason, considerable efforts are made to deal with this, and SemEval-2021 Task 5: Toxic Spans Detection is one of those. This task asks competitors to extract spans that have toxicity from the given texts, and we have done several analyses to understand its structure before doing experiments. We solve this task by two approaches, Named Entity Recognition with spaCy library and Question-Answering with RoBERTa combining with ToxicBERT, and the former gains the highest F1-score of 66.99%.

</details>

<details>

<summary>2021-04-15 13:56:54 - "Thought I'd Share First" and Other Conspiracy Theory Tweets from the COVID-19 Infodemic: Exploratory Study</summary>

- *Dax Gerts, Courtney D. Shelley, Nidhi Parikh, Travis Pitts, Chrysm Watson Ross, Geoffrey Fairchild, Nidia Yadria Vaquera Chavez, Ashlynn R. Daughton*

- `2012.07729v2` - [abs](http://arxiv.org/abs/2012.07729v2) - [pdf](http://arxiv.org/pdf/2012.07729v2)

> Background: The COVID-19 outbreak has left many people isolated within their homes; these people are turning to social media for news and social connection, which leaves them vulnerable to believing and sharing misinformation. Health-related misinformation threatens adherence to public health messaging, and monitoring its spread on social media is critical to understanding the evolution of ideas that have potentially negative public health impacts. Results: Analysis using model-labeled data was beneficial for increasing the proportion of data matching misinformation indicators. Random forest classifier metrics varied across the four conspiracy theories considered (F1 scores between 0.347 and 0.857); this performance increased as the given conspiracy theory was more narrowly defined. We showed that misinformation tweets demonstrate more negative sentiment when compared to nonmisinformation tweets and that theories evolve over time, incorporating details from unrelated conspiracy theories as well as real-world events. Conclusions: Although we focus here on health-related misinformation, this combination of approaches is not specific to public health and is valuable for characterizing misinformation in general, which is an important first step in creating targeted messaging to counteract its spread. Initial messaging should aim to preempt generalized misinformation before it becomes widespread, while later messaging will need to target evolving conspiracy theories and the new facets of each as they become incorporated.

</details>

<details>

<summary>2021-04-15 23:10:37 - Improve robustness of DNN for ECG signal classification:a noise-to-signal ratio perspective</summary>

- *Linhai Ma, Liang Liang*

- `2005.09134v3` - [abs](http://arxiv.org/abs/2005.09134v3) - [pdf](http://arxiv.org/pdf/2005.09134v3)

> Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor the condition of the cardiovascular system. Deep neural networks (DNNs), have been developed in many research labs for automatic interpretation of ECG signals to identify potential abnormalities in patient hearts. Studies have shown that given a sufficiently large amount of data, the classification accuracy of DNNs could reach human-expert cardiologist level. A DNN-based automated ECG diagnostic system would be an affordable solution for patients in developing countries where human-expert cardiologist are lacking. However, despite of the excellent performance in classification accuracy, it has been shown that DNNs are highly vulnerable to adversarial attacks: subtle changes in input of a DNN can lead to a wrong classification output with high confidence. Thus, it is challenging and essential to improve adversarial robustness of DNNs for ECG signal classification, a life-critical application. In this work, we proposed to improve DNN robustness from the perspective of noise-to-signal ratio (NSR) and developed two methods to minimize NSR during training process. We evaluated the proposed methods on PhysionNets MIT-BIH dataset, and the results show that our proposed methods lead to an enhancement in robustness against PGD adversarial attack and SPSA attack, with a minimal change in accuracy on clean data.

</details>

<details>

<summary>2021-04-16 01:57:11 - Adaptive Adversarial Logits Pairing</summary>

- *Shangxi Wu, Jitao Sang, Kaiyuan Xu, Guanhua Zheng, Changsheng Xu*

- `2005.11904v2` - [abs](http://arxiv.org/abs/2005.11904v2) - [pdf](http://arxiv.org/pdf/2005.11904v2)

> Adversarial examples provide an opportunity as well as impose a challenge for understanding image classification systems. Based on the analysis of the adversarial training solution Adversarial Logits Pairing (ALP), we observed in this work that: (1) The inference of adversarially robust model tends to rely on fewer high-contribution features compared with vulnerable ones. (2) The training target of ALP doesn't fit well to a noticeable part of samples, where the logits pairing loss is overemphasized and obstructs minimizing the classification loss. Motivated by these observations, we design an Adaptive Adversarial Logits Pairing (AALP) solution by modifying the training process and training target of ALP. Specifically, AALP consists of an adaptive feature optimization module with Guided Dropout to systematically pursue fewer high-contribution features, and an adaptive sample weighting module by setting sample-specific training weights to balance between logits pairing loss and classification loss. The proposed AALP solution demonstrates superior defense performance on multiple datasets with extensive experiments.

</details>

<details>

<summary>2021-04-16 09:00:44 - HACK3D: Crowdsourcing the Assessment of Cybersecurity in Digital Manufacturing</summary>

- *Michael Linares, Nishant Aswani, Gary Mac, Chenglu Jin, Fei Chen, Nikhil Gupta, Ramesh Karri*

- `2005.04368v2` - [abs](http://arxiv.org/abs/2005.04368v2) - [pdf](http://arxiv.org/pdf/2005.04368v2)

> Digital manufacturing (DM) cyber-physical system is vulnerable to both cyber and physical attacks. HACK3D is a series of crowdsourcing red-team-blue-team events hosted by the NYU Center for Cybersecurity to assess the strength of the security methods embedded in designs using DM. This study summarizes the lessons learned from the past three offerings of HACK3D, including ingenious ways in which skilled engineers can launch surprising attacks on DM designs not anticipated before. A key outcome is a taxonomy-guided creation of DM security benchmarks for use by the DM community.

</details>

<details>

<summary>2021-04-16 14:37:27 - Towards Variable-Length Textual Adversarial Attacks</summary>

- *Junliang Guo, Zhirui Zhang, Linlin Zhang, Linli Xu, Boxing Chen, Enhong Chen, Weihua Luo*

- `2104.08139v1` - [abs](http://arxiv.org/abs/2104.08139v1) - [pdf](http://arxiv.org/pdf/2104.08139v1)

> Adversarial attacks have shown the vulnerability of machine learning models, however, it is non-trivial to conduct textual adversarial attacks on natural language processing tasks due to the discreteness of data. Most previous approaches conduct attacks with the atomic \textit{replacement} operation, which usually leads to fixed-length adversarial examples and therefore limits the exploration on the decision space. In this paper, we propose variable-length textual adversarial attacks~(VL-Attack) and integrate three atomic operations, namely \textit{insertion}, \textit{deletion} and \textit{replacement}, into a unified framework, by introducing and manipulating a special \textit{blank} token while attacking. In this way, our approach is able to more comprehensively find adversarial examples around the decision boundary and effectively conduct adversarial attacks. Specifically, our method drops the accuracy of IMDB classification by $96\%$ with only editing $1.3\%$ tokens while attacking a pre-trained BERT model. In addition, fine-tuning the victim model with generated adversarial samples can improve the robustness of the model without hurting the performance, especially for length-sensitive models. On the task of non-autoregressive machine translation, our method can achieve $33.18$ BLEU score on IWSLT14 German-English translation, achieving an improvement of $1.47$ over the baseline model.

</details>

<details>

<summary>2021-04-18 13:19:04 - BaFFLe: Backdoor detection via Feedback-based Federated Learning</summary>

- *Sebastien Andreina, Giorgia Azzurra Marson, Helen Möllering, Ghassan Karame*

- `2011.02167v2` - [abs](http://arxiv.org/abs/2011.02167v2) - [pdf](http://arxiv.org/pdf/2011.02167v2)

> Recent studies have shown that federated learning (FL) is vulnerable to poisoning attacks that inject a backdoor into the global model. These attacks are effective even when performed by a single client, and undetectable by most existing defensive techniques. In this paper, we propose Backdoor detection via Feedback-based Federated Learning (BAFFLE), a novel defense to secure FL against backdoor attacks. The core idea behind BAFFLE is to leverage data of multiple clients not only for training but also for uncovering model poisoning. We exploit the availability of diverse datasets at the various clients by incorporating a feedback loop into the FL process, to integrate the views of those clients when deciding whether a given model update is genuine or not. We show that this powerful construct can achieve very high detection rates against state-of-the-art backdoor attacks, even when relying on straightforward methods to validate the model. Through empirical evaluation using the CIFAR-10 and FEMNIST datasets, we show that by combining the feedback loop with a method that suspects poisoning attempts by assessing the per-class classification performance of the updated model, BAFFLE reliably detects state-of-the-art backdoor attacks with a detection accuracy of 100% and a false-positive rate below 5%. Moreover, we show that our solution can detect adaptive attacks aimed at bypassing the defense.

</details>

<details>

<summary>2021-04-19 05:51:54 - Formal Analysis of Composable DeFi Protocols</summary>

- *Palina Tolmach, Yi Li, Shang-Wei Lin, Yang Liu*

- `2103.00540v2` - [abs](http://arxiv.org/abs/2103.00540v2) - [pdf](http://arxiv.org/pdf/2103.00540v2)

> Decentralized finance (DeFi) has become one of the most successful applications of blockchain and smart contracts. The DeFi ecosystem enables a wide range of crypto-financial activities, while the underlying smart contracts often contain bugs, with many vulnerabilities arising from the unforeseen consequences of composing DeFi protocols together. In this paper, we propose a formal process-algebraic technique that models DeFi protocols in a compositional manner to allow for efficient property verification. We also conduct a case study to demonstrate the proposed approach in analyzing the composition of two interacting DeFi protocols, namely, Curve and Compound. Finally, we discuss how the proposed modeling and verification approach can be used to analyze financial and security properties of interest.

</details>

<details>

<summary>2021-04-19 10:42:24 - Removing Adversarial Noise in Class Activation Feature Space</summary>

- *Dawei Zhou, Nannan Wang, Chunlei Peng, Xinbo Gao, Xiaoyu Wang, Jun Yu, Tongliang Liu*

- `2104.09197v1` - [abs](http://arxiv.org/abs/2104.09197v1) - [pdf](http://arxiv.org/pdf/2104.09197v1)

> Deep neural networks (DNNs) are vulnerable to adversarial noise. Preprocessing based defenses could largely remove adversarial noise by processing inputs. However, they are typically affected by the error amplification effect, especially in the front of continuously evolving attacks. To solve this problem, in this paper, we propose to remove adversarial noise by implementing a self-supervised adversarial training mechanism in a class activation feature space. To be specific, we first maximize the disruptions to class activation features of natural examples to craft adversarial examples. Then, we train a denoising model to minimize the distances between the adversarial examples and the natural examples in the class activation feature space. Empirical evaluations demonstrate that our method could significantly enhance adversarial robustness in comparison to previous state-of-the-art approaches, especially against unseen adversarial attacks and adaptive attacks.

</details>

<details>

<summary>2021-04-19 11:50:36 - Multi-context Attention Fusion Neural Network for Software Vulnerability Identification</summary>

- *Anshul Tanwar, Hariharan Manikandan, Krishna Sundaresan, Prasanna Ganesan, Sathish Kumar Chandrasekaran, Sriram Ravi*

- `2104.09225v1` - [abs](http://arxiv.org/abs/2104.09225v1) - [pdf](http://arxiv.org/pdf/2104.09225v1)

> Security issues in shipped code can lead to unforeseen device malfunction, system crashes or malicious exploitation by crackers, post-deployment. These vulnerabilities incur a cost of repair and foremost risk the credibility of the company. It is rewarding when these issues are detected and fixed well ahead of time, before release. Common Weakness Estimation (CWE) is a nomenclature describing general vulnerability patterns observed in C code. In this work, we propose a deep learning model that learns to detect some of the common categories of security vulnerabilities in source code efficiently. The AI architecture is an Attention Fusion model, that combines the effectiveness of recurrent, convolutional and self-attention networks towards decoding the vulnerability hotspots in code. Utilizing the code AST structure, our model builds an accurate understanding of code semantics with a lot less learnable parameters. Besides a novel way of efficiently detecting code vulnerability, an additional novelty in this model is to exactly point to the code sections, which were deemed vulnerable by the model. Thus helping a developer to quickly focus on the vulnerable code sections; and this becomes the "explainable" part of the vulnerability detection. The proposed AI achieves 98.40% F1-score on specific CWEs from the benchmarked NIST SARD dataset and compares well with state of the art.

</details>

<details>

<summary>2021-04-19 12:39:21 - Bypassing memory safety mechanisms through speculative control flow hijacks</summary>

- *Andrea Mambretti, Alexandra Sandulescu, Alessandro Sorniotti, William Robertson, Engin Kirda, Anil Kurmus*

- `2003.05503v3` - [abs](http://arxiv.org/abs/2003.05503v3) - [pdf](http://arxiv.org/pdf/2003.05503v3)

> The prevalence of memory corruption bugs in the past decades resulted in numerous defenses, such as stack canaries, control flow integrity (CFI), and memory safe languages. These defenses can prevent entire classes of vulnerabilities, and help increase the security posture of a program. In this paper, we show that memory corruption defenses can be bypassed using speculative execution attacks. We study the cases of stack protectors, CFI, and bounds checks in Go, demonstrating under which conditions they can be bypassed by a form of speculative control flow hijack, relying on speculative or architectural overwrites of control flow data. Information is leaked by redirecting the speculative control flow of the victim to a gadget accessing secret data and acting as a side channel send. We also demonstrate, for the first time, that this can be achieved by stitching together multiple gadgets, in a speculative return-oriented programming attack. We discuss and implement software mitigations, showing moderate performance impact.

</details>

<details>

<summary>2021-04-19 14:57:25 - Adversarial Diffusion Attacks on Graph-based Traffic Prediction Models</summary>

- *Lyuyi Zhu, Kairui Feng, Ziyuan Pu, Wei Ma*

- `2104.09369v1` - [abs](http://arxiv.org/abs/2104.09369v1) - [pdf](http://arxiv.org/pdf/2104.09369v1)

> Real-time traffic prediction models play a pivotal role in smart mobility systems and have been widely used in route guidance, emerging mobility services, and advanced traffic management systems. With the availability of massive traffic data, neural network-based deep learning methods, especially the graph convolutional networks (GCN) have demonstrated outstanding performance in mining spatio-temporal information and achieving high prediction accuracy. Recent studies reveal the vulnerability of GCN under adversarial attacks, while there is a lack of studies to understand the vulnerability issues of the GCN-based traffic prediction models. Given this, this paper proposes a new task -- diffusion attack, to study the robustness of GCN-based traffic prediction models. The diffusion attack aims to select and attack a small set of nodes to degrade the performance of the entire prediction model. To conduct the diffusion attack, we propose a novel attack algorithm, which consists of two major components: 1) approximating the gradient of the black-box prediction model with Simultaneous Perturbation Stochastic Approximation (SPSA); 2) adapting the knapsack greedy algorithm to select the attack nodes. The proposed algorithm is examined with three GCN-based traffic prediction models: St-Gcn, T-Gcn, and A3t-Gcn on two cities. The proposed algorithm demonstrates high efficiency in the adversarial attack tasks under various scenarios, and it can still generate adversarial samples under the drop regularization such as DropOut, DropNode, and DropEdge. The research outcomes could help to improve the robustness of the GCN-based traffic prediction models and better protect the smart mobility systems. Our code is available at https://github.com/LYZ98/Adversarial-Diffusion-Attacks-on-Graph-based-Traffic-Prediction-Models

</details>

<details>

<summary>2021-04-19 23:58:33 - Demystifying Regular Expression Bugs: A comprehensive study on regular expression bug causes, fixes, and testing</summary>

- *Peipei Wang, Chris Brown, Jamie A. Jennings, Kathryn T. Stolee*

- `2104.09693v1` - [abs](http://arxiv.org/abs/2104.09693v1) - [pdf](http://arxiv.org/pdf/2104.09693v1)

> Regular expressions cause string-related bugs and open security vulnerabilities for DOS attacks. However, beyond ReDoS (Regular expression Denial of Service), little is known about the extent to which regular expression issues affect software development and how these issues are addressed in practice. We conduct an empirical study of 356 merged regex-related pull request bugs from Apache, Mozilla, Facebook, and Google GitHub repositories. We identify and classify the nature of the regular expression problems, the fixes, and the related changes in the test code.   The most important findings in this paper are as follows: 1) incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3%). The remaining root causes are incorrect API usage (9.3%) and other code issues that require regular expression changes in the fix (29.5%), 2) fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests, 3) most (51%) of the regex-related pull requests do not contain test code changes. Certain regex bug types (e.g., compile error, performance issues, regex representation) are less likely to include test code changes than others, and 4) the dominant type of test code changes in regex-related pull requests is test case addition (75%). The results of this study contribute to a broader understanding of the practical problems faced by developers when using, fixing, and testing regular expressions.

</details>

<details>

<summary>2021-04-20 00:03:55 - Hard Class Rectification for Domain Adaptation</summary>

- *Yunlong Zhang, Changxing Jing, Huangxing Lin, Chaoqi Chen, Yue Huang, Xinghao Ding, Yang Zou*

- `2008.03455v2` - [abs](http://arxiv.org/abs/2008.03455v2) - [pdf](http://arxiv.org/pdf/2008.03455v2)

> Domain adaptation (DA) aims to transfer knowledge from a label-rich and related domain (source domain) to a label-scare domain (target domain). Pseudo-labeling has recently been widely explored and used in DA. However, this line of research is still confined to the inaccuracy of pseudo-labels. In this paper, we reveal an interesting observation that the target samples belonging to the classes with larger domain shift are easier to be misclassified compared with the other classes. These classes are called hard class, which deteriorates the performance of DA and restricts the applications of DA. We propose a novel framework, called Hard Class Rectification Pseudo-labeling (HCRPL), to alleviate the hard class problem from two aspects. First, as is difficult to identify the target samples as hard class, we propose a simple yet effective scheme, named Adaptive Prediction Calibration (APC), to calibrate the predictions of the target samples according to the difficulty degree for each class. Second, we further consider that the predictions of target samples belonging to the hard class are vulnerable to perturbations. To prevent these samples to be misclassified easily, we introduce Temporal-Ensembling (TE) and Self-Ensembling (SE) to obtain consistent predictions. The proposed method is evaluated in both unsupervised domain adaptation (UDA) and semi-supervised domain adaptation (SSDA). The experimental results on several real-world cross-domain benchmarks, including ImageCLEF, Office-31 and Office-Home, substantiates the superiority of the proposed method.

</details>

<details>

<summary>2021-04-20 07:52:58 - Addressing the Vulnerability of NMT in Input Perturbations</summary>

- *Weiwen Xu, Ai Ti Aw, Yang Ding, Kui Wu, Shafiq Joty*

- `2104.09810v1` - [abs](http://arxiv.org/abs/2104.09810v1) - [pdf](http://arxiv.org/pdf/2104.09810v1)

> Neural Machine Translation (NMT) has achieved significant breakthrough in performance but is known to suffer vulnerability to input perturbations. As real input noise is difficult to predict during training, robustness is a big issue for system deployment. In this paper, we improve the robustness of NMT models by reducing the effect of noisy words through a Context-Enhanced Reconstruction (CER) approach. CER trains the model to resist noise in two steps: (1) perturbation step that breaks the naturalness of input sequence with made-up words; (2) reconstruction step that defends the noise propagation by generating better and more robust contextual representation. Experimental results on Chinese-English (ZH-EN) and French-English (FR-EN) translation tasks demonstrate robustness improvement on both news and social media text. Further fine-tuning experiments on social media text show our approach can converge at a higher position and provide a better adaptation.

</details>

<details>

<summary>2021-04-20 09:36:24 - Adversarial Training for Deep Learning-based Intrusion Detection Systems</summary>

- *Islam Debicha, Thibault Debatty, Jean-Michel Dricot, Wim Mees*

- `2104.09852v1` - [abs](http://arxiv.org/abs/2104.09852v1) - [pdf](http://arxiv.org/pdf/2104.09852v1)

> Nowadays, Deep Neural Networks (DNNs) report state-of-the-art results in many machine learning areas, including intrusion detection. Nevertheless, recent studies in computer vision have shown that DNNs can be vulnerable to adversarial attacks that are capable of deceiving them into misclassification by injecting specially crafted data. In security-critical areas, such attacks can cause serious damage; therefore, in this paper, we examine the effect of adversarial attacks on deep learning-based intrusion detection. In addition, we investigate the effectiveness of adversarial training as a defense against such attacks. Experimental results show that with sufficient distortion, adversarial examples are able to mislead the detector and that the use of adversarial training can improve the robustness of intrusion detection.

</details>

<details>

<summary>2021-04-20 20:08:25 - Identifying botnet IP address clusters using natural language processing techniques on honeypot command logs</summary>

- *Valentino Crespi, Wes Hardaker, Sami Abu-El-Haija, Aram Galstyan*

- `2104.10232v1` - [abs](http://arxiv.org/abs/2104.10232v1) - [pdf](http://arxiv.org/pdf/2104.10232v1)

> Computer security has been plagued by increasing formidable, dynamic, hard-to-detect, hard-to-predict, and hard-to-characterize hacking techniques. Such techniques are very often deployed in self-propagating worms capable of automatically infecting vulnerable computer systems and then building large bot networks, which are then used to launch coordinated attacks on designated targets. In this work, we investigate novel applications of Natural Language Processing (NLP) methods to detect and correlate botnet behaviors through the analysis of honeypot data. In our approach we take observed behaviors in shell commands issued by intruders during captured internet sessions and reduce them to collections of stochastic processes that are, in turn, processed with machine learning techniques to build classifiers and predictors. Our technique results in a new ability to cluster botnet source IP address even in the face of their desire to obfuscate their penetration attempts through rapid or random permutation techniques.

</details>

<details>

<summary>2021-04-20 21:46:32 - TAD: Trigger Approximation based Black-box Trojan Detection for AI</summary>

- *Xinqiao Zhang, Huili Chen, Farinaz Koushanfar*

- `2102.01815v3` - [abs](http://arxiv.org/abs/2102.01815v3) - [pdf](http://arxiv.org/pdf/2102.01815v3)

> An emerging amount of intelligent applications have been developed with the surge of Machine Learning (ML). Deep Neural Networks (DNNs) have demonstrated unprecedented performance across various fields such as medical diagnosis and autonomous driving. While DNNs are widely employed in security-sensitive fields, they are identified to be vulnerable to Neural Trojan (NT) attacks that are controlled and activated by the stealthy trigger. We call this vulnerable model adversarial artificial intelligence (AI). In this paper, we target to design a robust Trojan detection scheme that inspects whether a pre-trained AI model has been Trojaned before its deployment. Prior works are oblivious of the intrinsic property of trigger distribution and try to reconstruct the trigger pattern using simple heuristics, i.e., stimulating the given model to incorrect outputs. As a result, their detection time and effectiveness are limited. We leverage the observation that the pixel trigger typically features spatial dependency and propose TAD, the first trigger approximation based Trojan detection framework that enables fast and scalable search of the trigger in the input space. Furthermore, TAD can also detect Trojans embedded in the feature space where certain filter transformations are used to activate the Trojan. We perform extensive experiments to investigate the performance of the TAD across various datasets and ML models. Empirical results show that TAD achieves a ROC-AUC score of 0:91 on the public TrojAI dataset 1 and the average detection time per model is 7:1 minutes.

</details>

<details>

<summary>2021-04-21 00:37:42 - Accumulating Risk Capital Through Investing in Cooperation</summary>

- *Charlotte Roman, Michael Dennis, Andrew Critch, Stuart Russell*

- `2101.10305v2` - [abs](http://arxiv.org/abs/2101.10305v2) - [pdf](http://arxiv.org/pdf/2101.10305v2)

> Recent work on promoting cooperation in multi-agent learning has resulted in many methods which successfully promote cooperation at the cost of becoming more vulnerable to exploitation by malicious actors. We show that this is an unavoidable trade-off and propose an objective which balances these concerns, promoting both safety and long-term cooperation. Moreover, the trade-off between safety and cooperation is not severe, and you can receive exponentially large returns through cooperation from a small amount of risk. We study both an exact solution method and propose a method for training policies that targets this objective, Accumulating Risk Capital Through Investing in Cooperation (ARCTIC), and evaluate them in iterated Prisoner's Dilemma and Stag Hunt.

</details>

<details>

<summary>2021-04-21 10:14:36 - Towards a Robust Deep Neural Network in Texts: A Survey</summary>

- *Wenqi Wang, Run Wang, Lina Wang, Zhibo Wang, Aoshuang Ye*

- `1902.07285v6` - [abs](http://arxiv.org/abs/1902.07285v6) - [pdf](http://arxiv.org/pdf/1902.07285v6)

> Deep neural networks (DNNs) have achieved remarkable success in various tasks (e.g., image classification, speech recognition, and natural language processing (NLP)). However, researchers have demonstrated that DNN-based models are vulnerable to adversarial examples, which cause erroneous predictions by adding imperceptible perturbations into legitimate inputs. Recently, studies have revealed adversarial examples in the text domain, which could effectively evade various DNN-based text analyzers and further bring the threats of the proliferation of disinformation. In this paper, we give a comprehensive survey on the existing studies of adversarial techniques for generating adversarial texts written by both English and Chinese characters and the corresponding defense methods. More importantly, we hope that our work could inspire future studies to develop more robust DNN-based text analyzers against known and unknown adversarial techniques.   We classify the existing adversarial techniques for crafting adversarial texts based on the perturbation units, helping to better understand the generation of adversarial texts and build robust models for defense. In presenting the taxonomy of adversarial attacks and defenses in the text domain, we introduce the adversarial techniques from the perspective of different NLP tasks. Finally, we discuss the existing challenges of adversarial attacks and defenses in texts and present the future research directions in this emerging and challenging field.

</details>

<details>

<summary>2021-04-21 11:03:48 - ForASec: Formal Analysis of Security Vulnerabilities in Sequential Circuits</summary>

- *Faiq Khalid, Imran Hafeez Abbassi, Semeen Rehman, Awais Mehmood Kamboh, Osman Hasan, Muhammad Shafique*

- `1812.05446v3` - [abs](http://arxiv.org/abs/1812.05446v3) - [pdf](http://arxiv.org/pdf/1812.05446v3)

> Security vulnerability analysis of Integrated Circuits using conventional design-time validation and verification techniques (like simulations, emulations, etc.) is generally a computationally intensive task and incomplete by nature, especially under limited resources and time constraints. To overcome this limitation, we propose a novel methodology based on model checking to formally analyze security vulnerabilities in sequential circuits while considering side-channel parameters like propagation delay, switching power, and leakage power. In particular, we present a novel algorithm to efficiently partition the state-space into corresponding smaller state-spaces to enable distributed security analysis of complex sequential circuits and thereby mitigating the associated state-space explosion due to their feedback loops. We analyze multiple ISCAS89 and trust-hub benchmarks to demonstrate the efficacy of our framework in identifying security vulnerabilities. The experimental results show that ForASec successfully performs the complete analysis of the given complex and large sequential circuits, and provides approximately 11x to 16x speedup in analysis time compared to state-of-the-art model checking-based techniques. Moreover, it also identifies the number of gates required by an HT that can go undetected for a given design and variability conditions.

</details>

<details>

<summary>2021-04-21 11:30:04 - HDR-Fuzz: Detecting Buffer Overruns using AddressSanitizer Instrumentation and Fuzzing</summary>

- *Raveendra Kumar Medicherla, Malathy Nagalakshmi, Tanya Sharma, Raghavan Komondoor*

- `2104.10466v1` - [abs](http://arxiv.org/abs/2104.10466v1) - [pdf](http://arxiv.org/pdf/2104.10466v1)

> Buffer-overruns are a prevalent vulnerability in software libraries and applications. Fuzz testing is one of the effective techniques to detect vulnerabilities in general. Greybox fuzzers such as AFL automatically generate a sequence of test inputs for a given program using a fitness-guided search process. A recently proposed approach in the literature introduced a buffer-overrun specific fitness metric called "headroom", which tracks how close each generated test input comes to exposing the vulnerabilities. That approach showed good initial promise, but is somewhat imprecise and expensive due to its reliance on conservative points-to analysis. Inspired by the approach above, in this paper we propose a new ground-up approach for detecting buffer-overrun vulnerabilities. This approach uses an extended version of ASAN (Address Sanitizer) that runs in parallel with the fuzzer, and reports back to the fuzzer test inputs that happen to come closer to exposing buffer-overrun vulnerabilities. The ASAN-style instrumentation is precise as it has no dependence on points-to analysis. We describe in this paper our approach, as well as an implementation and evaluation of the approach.

</details>

<details>

<summary>2021-04-21 12:05:48 - Generating Adversarial yet Inconspicuous Patches with a Single Image</summary>

- *Jinqi Luo, Tao Bai, Jun Zhao*

- `2009.09774v2` - [abs](http://arxiv.org/abs/2009.09774v2) - [pdf](http://arxiv.org/pdf/2009.09774v2)

> Deep neural networks have been shown vulnerable toadversarial patches, where exotic patterns can resultin models wrong prediction. Nevertheless, existing ap-proaches to adversarial patch generation hardly con-sider the contextual consistency between patches andthe image background, causing such patches to be eas-ily detected and adversarial attacks to fail. On the otherhand, these methods require a large amount of data fortraining, which is computationally expensive. To over-come these challenges, we propose an approach to gen-erate adversarial yet inconspicuous patches with onesingle image. In our approach, adversarial patches areproduced in a coarse-to-fine way with multiple scalesof generators and discriminators. Contextual informa-tion is encoded during the Min-Max training to makepatches consistent with surroundings. The selection ofpatch location is based on the perceptual sensitivity ofvictim models. Through extensive experiments, our ap-proach shows strong attacking ability in both the white-box and black-box setting. Experiments on saliency de-tection and user evaluation indicate that our adversar-ial patches can evade human observations, demonstratethe inconspicuousness of our approach. Lastly, we showthat our approach preserves the attack ability in thephysical world.

</details>

<details>

<summary>2021-04-21 13:55:45 - A Large-scale Study of Security Vulnerability Support on Developer Q&A Websites</summary>

- *Triet H. M. Le, Roland Croft, David Hin, M. Ali Babar*

- `2008.04176v2` - [abs](http://arxiv.org/abs/2008.04176v2) - [pdf](http://arxiv.org/pdf/2008.04176v2)

> Context: Security Vulnerabilities (SVs) pose many serious threats to software systems. Developers usually seek solutions to addressing these SVs on developer Question and Answer (Q&A) websites. However, there is still little known about on-going SV-specific discussions on different developer Q&A sites. Objective: We present a large-scale empirical study to understand developers' SV discussions and how these discussions are being supported by Q&A sites. Method: We first curate 71,329 SV posts from two large Q&A sites, namely Stack Overflow (SO) and Security StackExchange (SSE). We then use topic modeling to uncover the topics of SV-related discussions and analyze the popularity, difficulty, and level of expertise for each topic. We also perform a qualitative analysis to identify the types of solutions to SV-related questions. Results: We identify 13 main SV discussion topics on Q&A sites. Many topics do not follow the distributions and trends in expert-based security sources such as Common Weakness Enumeration (CWE) and Open Web Application Security Project (OWASP). We also discover that SV discussions attract more experts to answer than many other domains, but some difficult SV topics (e.g., Vulnerability Scanning Tools) still receive quite limited support from experts. Moreover, we identify seven key types of answers given to SV questions on Q&A sites, in which SO often provides code and instructions, while SSE usually gives experience-based advice and explanations. Conclusion: Our findings provide support for researchers and practitioners to effectively acquire, share and leverage SV knowledge on Q&A sites.

</details>

<details>

<summary>2021-04-21 19:39:20 - Assessing Validity of Static Analysis Warnings using Ensemble Learning</summary>

- *Anshul Tanwar, Hariharan Manikandan, Krishna Sundaresan, Prasanna Ganesan, Sathish Kumar Chandrasekaran, Sriram Ravi*

- `2104.11593v1` - [abs](http://arxiv.org/abs/2104.11593v1) - [pdf](http://arxiv.org/pdf/2104.11593v1)

> Static Analysis (SA) tools are used to identify potential weaknesses in code and fix them in advance, while the code is being developed. In legacy codebases with high complexity, these rules-based static analysis tools generally report a lot of false warnings along with the actual ones. Though the SA tools uncover many hidden bugs, they are lost in the volume of fake warnings reported. The developers expend large hours of time and effort in identifying the true warnings. Other than impacting the developer productivity, true bugs are also missed out due to this challenge. To address this problem, we propose a Machine Learning (ML)-based learning process that uses source codes, historic commit data, and classifier-ensembles to prioritize the True warnings from the given list of warnings. This tool is integrated into the development workflow to filter out the false warnings and prioritize actual bugs. We evaluated our approach on the networking C codes, from a large data pool of static analysis warnings reported by the tools. Time-to-time these warnings are addressed by the developers, labelling them as authentic bugs or fake alerts. The ML model is trained with full supervision over the code features. Our results confirm that applying deep learning over the traditional static analysis reports is an assuring approach for drastically reducing the false positive rates.

</details>

<details>

<summary>2021-04-22 05:44:40 - Patch Shortcuts: Interpretable Proxy Models Efficiently Find Black-Box Vulnerabilities</summary>

- *Julia Rosenzweig, Joachim Sicking, Sebastian Houben, Michael Mock, Maram Akila*

- `2104.11691v1` - [abs](http://arxiv.org/abs/2104.11691v1) - [pdf](http://arxiv.org/pdf/2104.11691v1)

> An important pillar for safe machine learning (ML) is the systematic mitigation of weaknesses in neural networks to afford their deployment in critical applications. An ubiquitous class of safety risks are learned shortcuts, i.e. spurious correlations a network exploits for its decisions that have no semantic connection to the actual task. Networks relying on such shortcuts bear the risk of not generalizing well to unseen inputs. Explainability methods help to uncover such network vulnerabilities. However, many of these techniques are not directly applicable if access to the network is constrained, in so-called black-box setups. These setups are prevalent when using third-party ML components. To address this constraint, we present an approach to detect learned shortcuts using an interpretable-by-design network as a proxy to the black-box model of interest. Leveraging the proxy's guarantees on introspection we automatically extract candidates for learned shortcuts. Their transferability to the black box is validated in a systematic fashion. Concretely, as proxy model we choose a BagNet, which bases its decisions purely on local image patches. We demonstrate on the autonomous driving dataset A2D2 that extracted patch shortcuts significantly influence the black box model. By efficiently identifying such patch-based vulnerabilities, we contribute to safer ML models.

</details>

<details>

<summary>2021-04-22 06:01:25 - Dual Head Adversarial Training</summary>

- *Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey*

- `2104.10377v2` - [abs](http://arxiv.org/abs/2104.10377v2) - [pdf](http://arxiv.org/pdf/2104.10377v2)

> Deep neural networks (DNNs) are known to be vulnerable to adversarial examples/attacks, raising concerns about their reliability in safety-critical applications. A number of defense methods have been proposed to train robust DNNs resistant to adversarial attacks, among which adversarial training has so far demonstrated the most promising results. However, recent studies have shown that there exists an inherent tradeoff between accuracy and robustness in adversarially-trained DNNs. In this paper, we propose a novel technique Dual Head Adversarial Training (DH-AT) to further improve the robustness of existing adversarial training methods. Different from existing improved variants of adversarial training, DH-AT modifies both the architecture of the network and the training strategy to seek more robustness. Specifically, DH-AT first attaches a second network head (or branch) to one intermediate layer of the network, then uses a lightweight convolutional neural network (CNN) to aggregate the outputs of the two heads. The training strategy is also adapted to reflect the relative importance of the two heads. We empirically show, on multiple benchmark datasets, that DH-AT can bring notable robustness improvements to existing adversarial training methods. Compared with TRADES, one state-of-the-art adversarial training method, our DH-AT can improve the robustness by 3.4% against PGD40 and 2.3% against AutoAttack, and also improve the clean accuracy by 1.8%.

</details>

<details>

<summary>2021-04-22 09:30:01 - An Extensive Study on Smell-Aware Bug Localization</summary>

- *Aoi Takahashi, Natthawute Sae-Lim, Shinpei Hayashi, Motoshi Saeki*

- `2104.10953v1` - [abs](http://arxiv.org/abs/2104.10953v1) - [pdf](http://arxiv.org/pdf/2104.10953v1)

> Bug localization is an important aspect of software maintenance because it can locate modules that should be changed to fix a specific bug. Our previous study showed that the accuracy of the information retrieval (IR)-based bug localization technique improved when used in combination with code smell information. Although this technique showed promise, the study showed limited usefulness because of the small number of: 1) projects in the dataset, 2) types of smell information, and 3) baseline bug localization techniques used for assessment. This paper presents an extension of our previous experiments on Bench4BL, the largest bug localization benchmark dataset available for bug localization. In addition, we generalized the smell-aware bug localization technique to allow different configurations of smell information, which were combined with various bug localization techniques. Our results confirmed that our technique can improve the performance of IR-based bug localization techniques for the class level even when large datasets are processed. Furthermore, because of the optimized configuration of the smell information, our technique can enhance the performance of most state-of-the-art bug localization techniques.

</details>

<details>

<summary>2021-04-22 09:51:53 - SpoC: Spoofing Camera Fingerprints</summary>

- *Davide Cozzolino, Justus Thies, Andreas Rössler, Matthias Nießner, Luisa Verdoliva*

- `1911.12069v2` - [abs](http://arxiv.org/abs/1911.12069v2) - [pdf](http://arxiv.org/pdf/1911.12069v2)

> Thanks to the fast progress in synthetic media generation, creating realistic false images has become very easy. Such images can be used to wrap "rich" fake news with enhanced credibility, spawning a new wave of high-impact, high-risk misinformation campaigns. Therefore, there is a fast-growing interest in reliable detectors of manipulated media. The most powerful detectors, to date, rely on the subtle traces left by any device on all images acquired by it. In particular, due to proprietary in-camera processes, like demosaicing or compression, each camera model leaves trademark traces that can be exploited for forensic analyses. The absence or distortion of such traces in the target image is a strong hint of manipulation. In this paper, we challenge such detectors to gain better insight into their vulnerabilities. This is an important study in order to build better forgery detectors able to face malicious attacks. Our proposal consists of a GAN-based approach that injects camera traces into synthetic images. Given a GAN-generated image, we insert the traces of a specific camera model into it and deceive state-of-the-art detectors into believing the image was acquired by that model. Likewise, we deceive independent detectors of synthetic GAN images into believing the image is real. Experiments prove the effectiveness of the proposed method in a wide array of conditions. Moreover, no prior information on the attacked detectors is needed, but only sample images from the target camera.

</details>

<details>

<summary>2021-04-22 11:34:28 - Intrinsic Propensity for Vulnerability in Computers? Arbitrary Code Execution in the Universal Turing Machine</summary>

- *Pontus Johnson*

- `2105.02124v1` - [abs](http://arxiv.org/abs/2105.02124v1) - [pdf](http://arxiv.org/pdf/2105.02124v1)

> The universal Turing machine is generally considered to be the simplest, most abstract model of a computer. This paper reports on the discovery of an accidental arbitrary code execution vulnerability in Marvin Minsky's 1967 implementation of the universal Turing machine. By submitting crafted data, the machine may be coerced into executing user-provided code. The article presents the discovered vulnerability in detail and discusses its potential implications. To the best of our knowledge, an arbitrary code execution vulnerability has not previously been reported for such a simple system.

</details>

<details>

<summary>2021-04-22 14:29:22 - Simpler Hyperparameter Optimization for Software Analytics: Why, How, When?</summary>

- *Amritanshu Agrawal, Xueqi Yang, Rishabh Agrawal, Rahul Yedida, Xipeng Shen, Tim Menzies*

- `1912.04061v5` - [abs](http://arxiv.org/abs/1912.04061v5) - [pdf](http://arxiv.org/pdf/1912.04061v5)

> How can we make software analytics simpler and faster? One method is to match the complexity of analysis to the intrinsic complexity of the data being explored. For example, hyperparameter optimizers find the control settings for data miners that improve the predictions generated via software analytics. Sometimes, very fast hyperparameter optimization can be achieved by "DODGE-ing"; i.e. simply steering way from settings that lead to similar conclusions. But when is it wise to use that simple approach and when must we use more complex (and much slower) optimizers?} To answer this, we applied hyperparameter optimization to 120 SE data sets that explored bad smell detection, predicting Github issue close time, bug report analysis, defect prediction, and dozens of other non-SE problems. We find that the simple DODGE works best for data sets with low "intrinsic dimensionality" (u ~ 3) and very poorly for higher-dimensional data (u > 8). Nearly all the SE data seen here was intrinsically low-dimensional, indicating that DODGE is applicable for many SE analytics tasks.

</details>

<details>

<summary>2021-04-22 22:00:35 - The Effect of Moderation on Online Mental Health Conversations</summary>

- *David Wadden, Tal August, Qisheng Li, Tim Althoff*

- `2005.09225v7` - [abs](http://arxiv.org/abs/2005.09225v7) - [pdf](http://arxiv.org/pdf/2005.09225v7)

> Many people struggling with mental health issues are unable to access adequate care due to high costs and a shortage of mental health professionals, leading to a global mental health crisis. Online mental health communities can help mitigate this crisis by offering a scalable, easily accessible alternative to in-person sessions with therapists or support groups. However, people seeking emotional or psychological support online may be especially vulnerable to the kinds of antisocial behavior that sometimes occur in online discussions. Moderation can improve online discourse quality, but we lack an understanding of its effects on online mental health conversations. In this work, we leveraged a natural experiment, occurring across 200,000 messages from 7,000 online mental health conversations, to evaluate the effects of moderation on online mental health discussions. We found that participation in group mental health discussions led to improvements in psychological perspective, and that these improvements were larger in moderated conversations. The presence of a moderator increased user engagement, encouraged users to discuss negative emotions more candidly, and dramatically reduced bad behavior among chat participants. Moderation also encouraged stronger linguistic coordination, which is indicative of trust building. In addition, moderators who remained active in conversations were especially successful in keeping conversations on topic. Our findings suggest that moderation can serve as a valuable tool to improve the efficacy and safety of online mental health conversations. Based on these findings, we discuss implications and trade-offs involved in designing effective online spaces for mental health support.

</details>

<details>

<summary>2021-04-23 01:02:05 - Simpler Hyperparameter Optimization for Software Analytics: Why, How, When?</summary>

- *Amritanshu Agrawal, Xueqi Yang, Rishabh Agrawal, Xipeng Shen, Tim Menzies*

- `2008.07334v2` - [abs](http://arxiv.org/abs/2008.07334v2) - [pdf](http://arxiv.org/pdf/2008.07334v2)

> How to make software analytics simpler and faster? One method is to match the complexity of analysis to the intrinsic complexity of the data being explored. For example, hyperparameter optimizers find the control settings for data miners that improve for improving the predictions generated via software analytics. Sometimes, very fast hyperparameter optimization can be achieved by just DODGE-ing away from things tried before. But when is it wise to use DODGE and when must we use more complex (and much slower) optimizers? To answer this, we applied hyperparameter optimization to 120 SE data sets that explored bad smell detection, predicting Github ssue close time, bug report analysis, defect prediction, and dozens of other non-SE problems. We find that DODGE works best for data sets with low "intrinsic dimensionality" (D = 3) and very poorly for higher-dimensional data (D over 8). Nearly all the SE data seen here was intrinsically low-dimensional, indicating that DODGE is applicable for many SE analytics tasks.

</details>

<details>

<summary>2021-04-23 03:16:51 - Literature review on vulnerability detection using NLP technology</summary>

- *Jiajie Wu*

- `2104.11230v1` - [abs](http://arxiv.org/abs/2104.11230v1) - [pdf](http://arxiv.org/pdf/2104.11230v1)

> Vulnerability detection has always been the most important task in the field of software security. With the development of technology, in the face of massive source code, automated analysis and detection of vulnerabilities has become a current research hotspot. For special text files such as source code, using some of the hottest NLP technologies to build models and realize the automatic analysis and detection of source code has become one of the most anticipated studies in the field of vulnerability detection. This article does a brief survey of some recent new documents and technologies, such as CodeBERT, and summarizes the previous technologies.

</details>

<details>

<summary>2021-04-23 04:20:19 - Analysing Cyberbullying using Natural Language Processing by Understanding Jargon in Social Media</summary>

- *Bhumika Bhatia, Anuj Verma, Anjum, Rahul Katarya*

- `2107.08902v1` - [abs](http://arxiv.org/abs/2107.08902v1) - [pdf](http://arxiv.org/pdf/2107.08902v1)

> Cyberbullying is of extreme prevalence today. Online-hate comments, toxicity, cyberbullying amongst children and other vulnerable groups are only growing over online classes, and increased access to social platforms, especially post COVID-19. It is paramount to detect and ensure minors' safety across social platforms so that any violence or hate-crime is automatically detected and strict action is taken against it. In our work, we explore binary classification by using a combination of datasets from various social media platforms that cover a wide range of cyberbullying such as sexism, racism, abusive, and hate-speech. We experiment through multiple models such as Bi-LSTM, GloVe, state-of-the-art models like BERT, and apply a unique preprocessing technique by introducing a slang-abusive corpus, achieving a higher precision in comparison to models without slang preprocessing.

</details>

<details>

<summary>2021-04-23 13:29:54 - Identifying and Modeling Security Threats for IoMT Edge Network using Markov Chain and Common Vulnerability Scoring System (CVSS)</summary>

- *Maha Ali Allouzi, Javed I. Khan*

- `2104.11580v1` - [abs](http://arxiv.org/abs/2104.11580v1) - [pdf](http://arxiv.org/pdf/2104.11580v1)

> In this work, we defined an attack vector for networks utilizing the Internet of Medical Things (IoMT) devices and compute the probability distribution of IoMT security threats based on Markov chain and Common Vulnerability Scoring System (CVSS). IoMT is an emerging technology that improves patients' quality of life by permitting personalized e-health services without restrictions on time and site. The IoMT consists of embedded objects, sensors, and actuators that transmit and receive medical data. These Medical devices are vulnerable to different types of security threats, and thus, they pose a significant risk to patient's privacy and safety. Because security is a critical factor for successfully merging IoMT into pervasive healthcare systems, there is an urgent need for new security mechanisms to prevent threats on the IoMT edge network. Toward this direction, the first step is defining an attack vector that an attacker or unauthorized user can take advantage of to penetrate and tamper with medical data. In this article, we specify a threat model for the IoMT edge network. We identify any vulnerabilities or weaknesses within the IoMT network that allow unauthorized privileges and threats that can utilize these weaknesses to compromise the IoMT edge network. Finally, we compute the probability distribution of IoMT threats based on the Markov transition probability matrix.

</details>

<details>

<summary>2021-04-23 16:30:22 - Speculative Interference Attacks: Breaking Invisible Speculation Schemes</summary>

- *Mohammad Behnia, Prateek Sahu, Riccardo Paccagnella, Jiyong Yu, Zirui Zhao, Xiang Zou, Thomas Unterluggauer, Josep Torrellas, Carlos Rozas, Adam Morrison, Frank Mckeen, Fangfei Liu, Ron Gabor, Christopher W. Fletcher, Abhishek Basak, Alaa Alameldeen*

- `2007.11818v4` - [abs](http://arxiv.org/abs/2007.11818v4) - [pdf](http://arxiv.org/pdf/2007.11818v4)

> Recent security vulnerabilities that target speculative execution (e.g., Spectre) present a significant challenge for processor design. The highly publicized vulnerability uses speculative execution to learn victim secrets by changing cache state. As a result, recent computer architecture research has focused on invisible speculation mechanisms that attempt to block changes in cache state due to speculative execution. Prior work has shown significant success in preventing Spectre and other vulnerabilities at modest performance costs. In this paper, we introduce speculative interference attacks, which show that prior invisible speculation mechanisms do not fully block these speculation-based attacks. We make two key observations. First, misspeculated younger instructions can change the timing of older, bound-to-retire instructions, including memory operations. Second, changing the timing of a memory operation can change the order of that memory operation relative to other memory operations, resulting in persistent changes to the cache state. Using these observations, we demonstrate (among other attack variants) that secret information accessed by mis-speculated instructions can change the order of bound-to-retire loads. Load timing changes can therefore leave secret-dependent changes in the cache, even in the presence of invisible speculation mechanisms. We show that this problem is not easy to fix: Speculative interference converts timing changes to persistent cache-state changes, and timing is typically ignored by many cache-based defenses. We develop a framework to understand the attack and demonstrate concrete proof-of-concept attacks against invisible speculation mechanisms. We provide security definitions sufficient to block speculative interference attacks; describe a simple defense mechanism with a high performance cost; and discuss how future research can improve its performance.

</details>

<details>

<summary>2021-04-23 16:33:38 - A Framework for Unsupervised Classificiation and Data Mining of Tweets about Cyber Vulnerabilities</summary>

- *Kenneth Alperin, Emily Joback, Leslie Shing, Gabe Elkin*

- `2104.11695v1` - [abs](http://arxiv.org/abs/2104.11695v1) - [pdf](http://arxiv.org/pdf/2104.11695v1)

> Many cyber network defense tools rely on the National Vulnerability Database (NVD) to provide timely information on known vulnerabilities that exist within systems on a given network. However, recent studies have indicated that the NVD is not always up to date, with known vulnerabilities being discussed publicly on social media platforms, like Twitter and Reddit, months before they are published to the NVD. To that end, we present a framework for unsupervised classification to filter tweets for relevance to cyber security. We consider and evaluate two unsupervised machine learning techniques for inclusion in our framework, and show that zero-shot classification using a Bidirectional and Auto-Regressive Transformers (BART) model outperforms the other technique with 83.52% accuracy and a F1 score of 83.88, allowing for accurate filtering of tweets without human intervention or labelled data for training. Additionally, we discuss different insights that can be derived from these cyber-relevant tweets, such as trending topics of tweets and the counts of Twitter mentions for Common Vulnerabilities and Exposures (CVEs), that can be used in an alert or report to augment current NVD-based risk assessment tools.

</details>

<details>

<summary>2021-04-24 16:40:13 - Backdoor Attack in the Physical World</summary>

- *Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, Shu-Tao Xia*

- `2104.02361v2` - [abs](http://arxiv.org/abs/2104.02361v2) - [pdf](http://arxiv.org/pdf/2104.02361v2)

> Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of infected models will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger. Currently, most existing backdoor attacks adopted the setting of static trigger, $i.e.,$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing trigger characteristics. We demonstrate that this attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. As such, those attacks are far less effective in the physical world, where the location and appearance of the trigger in the digitized image may be different from that of the one used for training. Moreover, we also discuss how to alleviate such vulnerability. We hope that this work could inspire more explorations on backdoor properties, to help the design of more advanced backdoor attack and defense methods.

</details>

<details>

<summary>2021-04-24 19:06:35 - Predicting the Number of Reported Bugs in a Software Repository</summary>

- *Hadi Jahanshahi, Mucahit Cevik, Ayşe Başar*

- `2104.12001v1` - [abs](http://arxiv.org/abs/2104.12001v1) - [pdf](http://arxiv.org/pdf/2104.12001v1)

> The bug growth pattern prediction is a complicated, unrelieved task, which needs considerable attention. Advance knowledge of the likely number of bugs discovered in the software system helps software developers in designating sufficient resources at a convenient time. The developers may also use such information to take necessary actions to increase the quality of the system and in turn customer satisfaction. In this study, we examine eight different time series forecasting models, including Long Short Term Memory Neural Networks (LSTM), auto-regressive integrated moving average (ARIMA), and Random Forest Regressor. Further, we assess the impact of exogenous variables such as software release dates by incorporating those into the prediction models. We analyze the quality of long-term prediction for each model based on different performance metrics. The assessment is conducted on Mozilla, which is a large open-source software application. The dataset is originally mined from Bugzilla and contains the number of bugs for the project between Jan 2010 and Dec 2019. Our numerical analysis provides insights on evaluating the trends in a bug repository. We observe that LSTM is effective when considering long-run predictions whereas Random Forest Regressor enriched by exogenous variables performs better for predicting the number of bugs in the short term.

</details>

<details>

<summary>2021-04-25 10:18:36 - Voting Classifier-based Intrusion Detection for IoT Networks</summary>

- *Muhammad Almas Khan, Muazzam A Khan, Shahid Latif, Awais Aziz Shah, Mujeeb Ur Rehman, Wadii Boulila, Maha Driss, Jawad Ahmad*

- `2104.10015v2` - [abs](http://arxiv.org/abs/2104.10015v2) - [pdf](http://arxiv.org/pdf/2104.10015v2)

> Internet of Things (IoT) is transforming human lives by paving the way for the management of physical devices on the edge. These interconnected IoT objects share data for remote accessibility and can be vulnerable to open attacks and illegal access. Intrusion detection methods are commonly used for the detection of such kinds of attacks but with these methods, the performance/accuracy is not optimal. This work introduces a novel intrusion detection approach based on an ensemble-based voting classifier that combines multiple traditional classifiers as a base learner and gives the vote to the predictions of the traditional classifier in order to get the final prediction. To test the effectiveness of the proposed approach, experiments are performed on a set of seven different IoT devices and tested for binary attack classification and multi-class attack classification. The results illustrate prominent accuracies on Global Positioning System (GPS) sensors and weather sensors to 96% and 97% and for other machine learning algorithms to 85% and 87%, respectively. Furthermore, comparison with other traditional machine learning methods validates the superiority of the proposed algorithm.

</details>

<details>

<summary>2021-04-25 11:15:06 - Exploring the links between software development task type, team attitudes and task completion performance: Insights from the Jazz repository</summary>

- *Sherlock A. Licorish, Stephen G. MacDonell*

- `2104.12131v1` - [abs](http://arxiv.org/abs/2104.12131v1) - [pdf](http://arxiv.org/pdf/2104.12131v1)

> Context: In seeking to better understand the impact of various human factors in software development, and how teams' attitudes relate to their performance, increasing attention is being given to the study of team-related artefacts. In particular, researchers have conducted numerous studies on a range of team communication channels to explore links between developers' language use and the incidence of software bugs in the products they delivered. Comparatively limited attention has been paid, however, to the full range of software tasks that are commonly performed during the development and delivery of software systems, in spite of compelling evidence pointing to the need to understand teams' attitudes more widely. Objective: We were therefore motivated to study the relationships between task type and team attitudes, and how attitudes expressed in teams' communications might be related to their task completion performance when undertaking a range of activities. Method: Our investigation involved artefacts from 474 IBM Jazz practitioners assembled in 149 teams working on around 30,000 software development tasks over a three-year period. We applied linguistic analysis, standard statistical techniques and directed content analysis to address our research objective. Results: Our evidence revealed that teams expressed different attitudes when working on various forms of software tasks, and they were particularly emotional when working to remedy defects. That said, teams' expression of attitudes was not found to be a strong predictor of their task completion performance. Conclusion: Efforts aimed at reducing bug incidence may positively limit teams' emotional disposition when resolving bugs, thereby reducing the otherwise high demand for emotionally stable members. (Abridged)

</details>

<details>

<summary>2021-04-26 09:15:28 - Model Guided Road Intersection Classification</summary>

- *Augusto Luis Ballardini, Álvaro Hernández, Miguel Ángel Sotelo*

- `2104.12417v1` - [abs](http://arxiv.org/abs/2104.12417v1) - [pdf](http://arxiv.org/pdf/2104.12417v1)

> Understanding complex scenarios from in-vehicle cameras is essential for safely operating autonomous driving systems in densely populated areas. Among these, intersection areas are one of the most critical as they concentrate a considerable number of traffic accidents and fatalities. Detecting and understanding the scene configuration of these usually crowded areas is then of extreme importance for both autonomous vehicles and modern ADAS aimed at preventing road crashes and increasing the safety of vulnerable road users. This work investigates inter-section classification from RGB images using well-consolidate neural network approaches along with a method to enhance the results based on the teacher/student training paradigm. An extensive experimental activity aimed at identifying the best input configuration and evaluating different network parameters on both the well-known KITTI dataset and the new KITTI-360 sequences shows that our method outperforms current state-of-the-art approaches on a per-frame basis and prove the effectiveness of the proposed learning scheme.

</details>

<details>

<summary>2021-04-26 09:36:29 - Launching Adversarial Attacks against Network Intrusion Detection Systems for IoT</summary>

- *Pavlos Papadopoulos, Oliver Thornewill von Essen, Nikolaos Pitropakis, Christos Chrysoulas, Alexios Mylonas, William J. Buchanan*

- `2104.12426v1` - [abs](http://arxiv.org/abs/2104.12426v1) - [pdf](http://arxiv.org/pdf/2104.12426v1)

> As the internet continues to be populated with new devices and emerging technologies, the attack surface grows exponentially. Technology is shifting towards a profit-driven Internet of Things market where security is an afterthought. Traditional defending approaches are no longer sufficient to detect both known and unknown attacks to high accuracy. Machine learning intrusion detection systems have proven their success in identifying unknown attacks with high precision. Nevertheless, machine learning models are also vulnerable to attacks. Adversarial examples can be used to evaluate the robustness of a designed model before it is deployed. Further, using adversarial examples is critical to creating a robust model designed for an adversarial environment. Our work evaluates both traditional machine learning and deep learning models' robustness using the Bot-IoT dataset. Our methodology included two main approaches. First, label poisoning, used to cause incorrect classification by the model. Second, the fast gradient sign method, used to evade detection measures. The experiments demonstrated that an attacker could manipulate or circumvent detection with significant probability.

</details>

<details>

<summary>2021-04-26 14:30:55 - Achieving robustness in classification using optimal transport with hinge regularization</summary>

- *Mathieu Serrurier, Franck Mamalet, Alberto González-Sanz, Thibaut Boissin, Jean-Michel Loubes, Eustasio del Barrio*

- `2006.06520v3` - [abs](http://arxiv.org/abs/2006.06520v3) - [pdf](http://arxiv.org/pdf/2006.06520v3)

> Adversarial examples have pointed out Deep Neural Networks vulnerability to small local noise. It has been shown that constraining their Lipschitz constant should enhance robustness, but make them harder to learn with classical loss functions. We propose a new framework for binary classification, based on optimal transport, which integrates this Lipschitz constraint as a theoretical requirement. We propose to learn 1-Lipschitz networks using a new loss that is an hinge regularized version of the Kantorovich-Rubinstein dual formulation for the Wasserstein distance estimation. This loss function has a direct interpretation in terms of adversarial robustness together with certifiable robustness bound. We also prove that this hinge regularized version is still the dual formulation of an optimal transportation problem, and has a solution. We also establish several geometrical properties of this optimal solution, and extend the approach to multi-class problems. Experiments show that the proposed approach provides the expected guarantees in terms of robustness without any significant accuracy drop. The adversarial examples, on the proposed models, visibly and meaningfully change the input providing an explanation for the classification.

</details>

<details>

<summary>2021-04-26 17:35:42 - DABT: A Dependency-aware Bug Triaging Method</summary>

- *Hadi Jahanshahi, Kritika Chhabra, Mucahit Cevik, Ayşe Başar*

- `2104.12744v1` - [abs](http://arxiv.org/abs/2104.12744v1) - [pdf](http://arxiv.org/pdf/2104.12744v1)

> In software engineering practice, fixing a bug promptly reduces the associated costs. On the other hand, the manual bug fixing process can be time-consuming, cumbersome, and error-prone. In this work, we introduce a bug triaging method, called Dependency-aware Bug Triaging (DABT), which leverages natural language processing and integer programming to assign bugs to appropriate developers. Unlike previous works that mainly focus on one aspect of the bug reports, DABT considers the textual information, cost associated with each bug, and dependency among them. Therefore, this comprehensive formulation covers the most important aspect of the previous works while considering the blocking effect of the bugs. We report the performance of the algorithm on three open-source software systems, i.e., EclipseJDT, LibreOffice, and Mozilla. Our result shows that DABT is able to reduce the number of overdue bugs up to 12\%. It also decreases the average fixing time of the bugs by half. Moreover, it reduces the complexity of the bug dependency graph by prioritizing blocking bugs.

</details>

<details>

<summary>2021-04-26 21:05:03 - Security, Privacy and Trust: Cognitive Internet of Vehicles</summary>

- *Khondokar Fida Hasan, Anthony Overall, Keyvan Ansari, Gowri Ramachandran, Raja Jurdak*

- `2104.12878v1` - [abs](http://arxiv.org/abs/2104.12878v1) - [pdf](http://arxiv.org/pdf/2104.12878v1)

> The recent advancement of cloud technology offers unparallel strength to support intelligent computations and advanced services to assist with automated decisions to improve road transportation safety and comfort. Besides, the rise of machine intelligence propels the technological evolution of transportation systems one step further and leads to a new framework known as Cognitive Internet of Vehicles (C-IoV). The redefined cognitive technology in this framework promises significant enhancements and optimized network capacities compared with its predecessor framework, the Internet of Vehicles (IoV). CIoV offers additional security measures and introduces security and privacy concerns, such as evasion attacks, additional threats of data poisoning, and learning errors, which may likely lead to system failure and road user fatalities. Similar to many other public enterprise systems, transportation has a significant impact on the population. Therefore, it is crucial to understand the evolution and equally essential to identify potential security vulnerabilities and issues to offer mitigation towards success. This chapter offers discussions framing answers to the following two questions, 1) how and in what ways the penetration of the latest technologies are reshaping the transportation system? 2) whether the evolved system is capable of addressing the concerns of cybersecurity? This chapter, therefore, starts presenting the evolution of the transportation system followed by a quick overview of the evolved CIoV, highlighting the evolved cognitive design. Later it presents how a cognitive engine can overcome legacy security concerns and also be subjected to further potential security, privacy, and trust issues that this cloud-based evolved transportation system may encounter.

</details>

<details>

<summary>2021-04-26 22:04:43 - Trex: Learning Execution Semantics from Micro-Traces for Binary Similarity</summary>

- *Kexin Pei, Zhou Xuan, Junfeng Yang, Suman Jana, Baishakhi Ray*

- `2012.08680v3` - [abs](http://arxiv.org/abs/2012.08680v3) - [pdf](http://arxiv.org/pdf/2012.08680v3)

> Detecting semantically similar functions -- a crucial analysis capability with broad real-world security usages including vulnerability detection, malware lineage, and forensics -- requires understanding function behaviors and intentions. This task is challenging as semantically similar functions can be implemented differently, run on different architectures, and compiled with diverse compiler optimizations or obfuscations. Most existing approaches match functions based on syntactic features without understanding the functions' execution semantics.   We present Trex, a transfer-learning-based framework, to automate learning execution semantics explicitly from functions' micro-traces and transfer the learned knowledge to match semantically similar functions. Our key insight is that these traces can be used to teach an ML model the execution semantics of different sequences of instructions. We thus train the model to learn execution semantics from the functions' micro-traces, without any manual labeling effort. We then develop a novel neural architecture to learn execution semantics from micro-traces, and we finetune the pretrained model to match semantically similar functions.   We evaluate Trex on 1,472,066 function binaries from 13 popular software projects. These functions are from different architectures and compiled with various optimizations and obfuscations. Trex outperforms the state-of-the-art systems by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively. Ablation studies show that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics.

</details>

<details>

<summary>2021-04-27 10:29:38 - Towards On-Device Federated Learning: A Direct Acyclic Graph-based Blockchain Approach</summary>

- *Mingrui Cao, Long Zhang, Bin Cao*

- `2104.13092v1` - [abs](http://arxiv.org/abs/2104.13092v1) - [pdf](http://arxiv.org/pdf/2104.13092v1)

> Due to the distributed characteristics of Federated Learning (FL), the vulnerability of global model and coordination of devices are the main obstacle. As a promising solution of decentralization, scalability and security, leveraging blockchain in FL has attracted much attention in recent years. However, the traditional consensus mechanisms designed for blockchain like Proof of Work (PoW) would cause extreme resource consumption, which reduces the efficiency of FL greatly, especially when the participating devices are wireless and resource-limited. In order to address device asynchrony and anomaly detection in FL while avoiding the extra resource consumption caused by blockchain, this paper introduces a framework for empowering FL using Direct Acyclic Graph (DAG)-based blockchain systematically (DAG-FL). Accordingly, DAG-FL is first introduced from a three-layer architecture in details, and then two algorithms DAG-FL Controlling and DAG-FL Updating are designed running on different nodes to elaborate the operation of DAG-FL consensus mechanism. After that, a Poisson process model is formulated to discuss that how to set deployment parameters to maintain DAG-FL stably in different federated learning tasks. The extensive simulations and experiments show that DAG-FL can achieve better performance in terms of training efficiency and model accuracy compared with the typical existing on-device federated learning systems as the benchmarks.

</details>

<details>

<summary>2021-04-27 15:28:30 - A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code</summary>

- *Nadezhda Chirkova, Sergey Troshin*

- `2010.12663v2` - [abs](http://arxiv.org/abs/2010.12663v2) - [pdf](http://arxiv.org/pdf/2010.12663v2)

> There is an emerging interest in the application of natural language processing models to source code processing tasks. One of the major problems in applying deep learning to software engineering is that source code often contains a lot of rare identifiers, resulting in huge vocabularies. We propose a simple, yet effective method, based on identifier anonymization, to handle out-of-vocabulary (OOV) identifiers. Our method can be treated as a preprocessing step and, therefore, allows for easy implementation. We show that the proposed OOV anonymization method significantly improves the performance of the Transformer in two code processing tasks: code completion and bug fixing.

</details>

<details>

<summary>2021-04-27 16:05:27 - On the Embeddings of Variables in Recurrent Neural Networks for Source Code</summary>

- *Nadezhda Chirkova*

- `2010.12693v2` - [abs](http://arxiv.org/abs/2010.12693v2) - [pdf](http://arxiv.org/pdf/2010.12693v2)

> Source code processing heavily relies on the methods widely used in natural language processing (NLP), but involves specifics that need to be taken into account to achieve higher quality. An example of this specificity is that the semantics of a variable is defined not only by its name but also by the contexts in which the variable occurs. In this work, we develop dynamic embeddings, a recurrent mechanism that adjusts the learned semantics of the variable when it obtains more information about the variable's role in the program. We show that using the proposed dynamic embeddings significantly improves the performance of the recurrent neural network, in code completion and bug fixing tasks.

</details>

<details>

<summary>2021-04-27 16:08:49 - Metamorphic Detection of Repackaged Malware</summary>

- *Shirish Singh, Gail Kaiser*

- `2104.13295v1` - [abs](http://arxiv.org/abs/2104.13295v1) - [pdf](http://arxiv.org/pdf/2104.13295v1)

> Machine learning-based malware detection systems are often vulnerable to evasion attacks, in which a malware developer manipulates their malicious software such that it is misclassified as benign. Such software hides some properties of the real class or adopts some properties of a different class by applying small perturbations. A special case of evasive malware hides by repackaging a bonafide benign mobile app to contain malware in addition to the original functionality of the app, thus retaining most of the benign properties of the original app. We present a novel malware detection system based on metamorphic testing principles that can detect such benign-seeming malware apps. We apply metamorphic testing to the feature representation of the mobile app rather than to the app itself. That is, the source input is the original feature vector for the app and the derived input is that vector with selected features removed. If the app was originally classified benign and is indeed benign, the output for the source and derived inputs should be the same class, i.e., benign, but if they differ, then the app is exposed as likely malware. Malware apps originally classified as malware should retain that classification since only features prevalent in benign apps are removed. This approach enables the machine learning model to classify repackaged malware with reasonably few false negatives and false positives. Our training pipeline is simpler than many existing ML-based malware detection methods, as the network is trained end-to-end to learn appropriate features and perform classification. We pre-trained our classifier model on 3 million apps collected from the widely-used AndroZoo dataset. We perform an extensive study on other publicly available datasets to show our approach's effectiveness in detecting repackaged malware with more than94% accuracy, 0.98 precision, 0.95 recall, and 0.96 F1 score.

</details>

<details>

<summary>2021-04-27 16:21:48 - Extending Isolation Forest for Anomaly Detection in Big Data via K-Means</summary>

- *Md Tahmid Rahman Laskar, Jimmy Huang, Vladan Smetana, Chris Stewart, Kees Pouw, Aijun An, Stephen Chan, Lei Liu*

- `2104.13190v1` - [abs](http://arxiv.org/abs/2104.13190v1) - [pdf](http://arxiv.org/pdf/2104.13190v1)

> Industrial Information Technology (IT) infrastructures are often vulnerable to cyberattacks. To ensure security to the computer systems in an industrial environment, it is required to build effective intrusion detection systems to monitor the cyber-physical systems (e.g., computer networks) in the industry for malicious activities. This paper aims to build such intrusion detection systems to protect the computer networks from cyberattacks. More specifically, we propose a novel unsupervised machine learning approach that combines the K-Means algorithm with the Isolation Forest for anomaly detection in industrial big data scenarios. Since our objective is to build the intrusion detection system for the big data scenario in the industrial domain, we utilize the Apache Spark framework to implement our proposed model which was trained in large network traffic data (about 123 million instances of network traffic) stored in Elasticsearch. Moreover, we evaluate our proposed model on the live streaming data and find that our proposed system can be used for real-time anomaly detection in the industrial setup. In addition, we address different challenges that we face while training our model on large datasets and explicitly describe how these issues were resolved. Based on our empirical evaluation in different use-cases for anomaly detection in real-world network traffic data, we observe that our proposed system is effective to detect anomalies in big data scenarios. Finally, we evaluate our proposed model on several academic datasets to compare with other models and find that it provides comparable performance with other state-of-the-art approaches.

</details>

<details>

<summary>2021-04-27 21:12:45 - Finding Security Vulnerabilities in IoT Cryptographic Protocol and Concurrent Implementations</summary>

- *Fatimah Aljaafari, Rafael Menezes, Mustafa A. Mustafa, Lucas C. Cordeiro*

- `2103.11363v2` - [abs](http://arxiv.org/abs/2103.11363v2) - [pdf](http://arxiv.org/pdf/2103.11363v2)

> Internet of Things (IoT) consists of a large number of devices connected through a network, which exchange a high volume of data, thereby posing new security, privacy, and trust issues. One way to address these issues is ensuring data confidentiality using lightweight encryption algorithms for IoT protocols. However, the design and implementation of such protocols is an error-prone task; flaws in the implementation can lead to devastating security vulnerabilities. Here we propose a new verification approach named Encryption-BMC and Fuzzing (EBF), which combines Bounded Model Checking (BMC) and Fuzzing techniques to check for security vulnerabilities that arise from concurrent implementations of cyrptographic protocols, which include data race, thread leak, arithmetic overflow, and memory safety. EBF models IoT protocols as a client and server using POSIX threads, thereby simulating both entities' communication. It also employs static and dynamic verification to cover the system's state-space exhaustively. We evaluate EBF against three benchmarks. First, we use the concurrency benchmark from SV-COMP and show that it outperforms other state-of-the-art tools such as ESBMC, AFL, Lazy-CSeq, and TSAN with respect to bug finding. Second, we evaluate an open-source implementation called WolfMQTT. It is an MQTT client implementation that uses the WolfSSL library. We show that \tool detects a data race bug, which other approaches are unable to find. Third, to show the effectiveness of EBF, we replicate some known vulnerabilities in OpenSSL and CyaSSL (lately WolfSSL) libraries. EBF can detect the bugs in minimum time.

</details>

<details>

<summary>2021-04-28 02:45:44 - Towards Robust Neural Networks via Close-loop Control</summary>

- *Zhuotong Chen, Qianxiao Li, Zheng Zhang*

- `2102.01862v2` - [abs](http://arxiv.org/abs/2102.01862v2) - [pdf](http://arxiv.org/pdf/2102.01862v2)

> Despite their success in massive engineering applications, deep neural networks are vulnerable to various perturbations due to their black-box nature. Recent study has shown that a deep neural network can misclassify the data even if the input data is perturbed by an imperceptible amount. In this paper, we address the robustness issue of neural networks by a novel close-loop control method from the perspective of dynamic systems. Instead of modifying the parameters in a fixed neural network architecture, a close-loop control process is added to generate control signals adaptively for the perturbed or corrupted data. We connect the robustness of neural networks with optimal control using the geometrical information of underlying data to design the control objective. The detailed analysis shows how the embedding manifolds of state trajectory affect error estimation of the proposed method. Our approach can simultaneously maintain the performance on clean data and improve the robustness against many types of data perturbations. It can also further improve the performance of robustly trained neural networks against different perturbations. To the best of our knowledge, this is the first work that improves the robustness of neural networks with close-loop control.

</details>

<details>

<summary>2021-04-28 06:13:47 - Deep Domain Generalization with Feature-norm Network</summary>

- *Mohammad Mahfujur Rahman, Clinton Fookes, Sridha Sridharan*

- `2104.13581v1` - [abs](http://arxiv.org/abs/2104.13581v1) - [pdf](http://arxiv.org/pdf/2104.13581v1)

> In this paper, we tackle the problem of training with multiple source domains with the aim to generalize to new domains at test time without an adaptation step. This is known as domain generalization (DG). Previous works on DG assume identical categories or label space across the source domains. In the case of category shift among the source domains, previous methods on DG are vulnerable to negative transfer due to the large mismatch among label spaces, decreasing the target classification accuracy. To tackle the aforementioned problem, we introduce an end-to-end feature-norm network (FNN) which is robust to negative transfer as it does not need to match the feature distribution among the source domains. We also introduce a collaborative feature-norm network (CFNN) to further improve the generalization capability of FNN. The CFNN matches the predictions of the next most likely categories for each training sample which increases each network's posterior entropy. We apply the proposed FNN and CFNN networks to the problem of DG for image classification tasks and demonstrate significant improvement over the state-of-the-art.

</details>

<details>

<summary>2021-04-28 23:20:06 - Generating Bug-Fixes Using Pretrained Transformers</summary>

- *Dawn Drain, Chen Wu, Alexey Svyatkovskiy, Neel Sundaresan*

- `2104.07896v2` - [abs](http://arxiv.org/abs/2104.07896v2) - [pdf](http://arxiv.org/pdf/2104.07896v2)

> Detecting and fixing bugs are two of the most important yet frustrating parts of the software development cycle. Existing bug detection tools are based mainly on static analyzers, which rely on mathematical logic and symbolic reasoning about the program execution to detect common types of bugs. Fixing bugs is typically left out to the developer. In this work we introduce DeepDebug: a data-driven program repair approach which learns to detect and fix bugs in Java methods mined from real-world GitHub repositories. We frame bug-patching as a sequence-to-sequence learning task consisting of two steps: (i) denoising pretraining, and (ii) supervised finetuning on the target translation task. We show that pretraining on source code programs improves the number of patches found by 33% as compared to supervised training from scratch, while domain-adaptive pretraining from natural language to code further improves the accuracy by another 32%. We refine the standard accuracy evaluation metric into non-deletion and deletion-only fixes, and show that our best model generates 75% more non-deletion fixes than the previous state of the art. In contrast to prior work, we attain our best results when generating raw code, as opposed to working with abstracted code that tends to only benefit smaller capacity models. Finally, we observe a subtle improvement from adding syntax embeddings along with the standard positional embeddings, as well as with adding an auxiliary task to predict each token's syntactic class. Despite focusing on Java, our approach is language agnostic, requiring only a general-purpose parser such as tree-sitter.

</details>

<details>

<summary>2021-04-29 01:47:30 - A comparative study of neural network techniques for automatic software vulnerability detection</summary>

- *Gaigai Tang, Lianxiao Meng, Shuangyin Ren, Weipeng Cao, Qiang Wang, Lin Yang*

- `2104.14978v1` - [abs](http://arxiv.org/abs/2104.14978v1) - [pdf](http://arxiv.org/pdf/2104.14978v1)

> Software vulnerabilities are usually caused by design flaws or implementation errors, which could be exploited to cause damage to the security of the system. At present, the most commonly used method for detecting software vulnerabilities is static analysis. Most of the related technologies work based on rules or code similarity (source code level) and rely on manually defined vulnerability features. However, these rules and vulnerability features are difficult to be defined and designed accurately, which makes static analysis face many challenges in practical applications. To alleviate this problem, some researchers have proposed to use neural networks that have the ability of automatic feature extraction to improve the intelligence of detection. However, there are many types of neural networks, and different data preprocessing methods will have a significant impact on model performance. It is a great challenge for engineers and researchers to choose a proper neural network and data preprocessing method for a given problem. To solve this problem, we have conducted extensive experiments to test the performance of the two most typical neural networks (i.e., Bi-LSTM and RVFL) with the two most classical data preprocessing methods (i.e., the vector representation and the program symbolization methods) on software vulnerability detection problems and obtained a series of interesting research conclusions, which can provide valuable guidelines for researchers and engineers. Specifically, we found that 1) the training speed of RVFL is always faster than BiLSTM, but the prediction accuracy of Bi-LSTM model is higher than RVFL; 2) using doc2vec for vector representation can make the model have faster training speed and generalization ability than using word2vec; and 3) multi-level symbolization is helpful to improve the precision of neural network models.

</details>

<details>

<summary>2021-04-29 08:56:47 - Self-Claimed Assumptions in Deep Learning Frameworks: An Exploratory Study</summary>

- *Chen Yang, Peng Liang, Liming Fu, Zengyang Li*

- `2104.14208v1` - [abs](http://arxiv.org/abs/2104.14208v1) - [pdf](http://arxiv.org/pdf/2104.14208v1)

> Deep learning (DL) frameworks have been extensively designed, implemented, and used in software projects across many domains. However, due to the lack of knowledge or information, time pressure, complex context, etc., various uncertainties emerge during the development, leading to assumptions made in DL frameworks. Though not all the assumptions are negative to the frameworks, being unaware of certain assumptions can result in critical problems (e.g., system vulnerability and failures, inconsistencies, and increased cost). As the first step of addressing the critical problems, there is a need to explore and understand the assumptions made in DL frameworks. To this end, we conducted an exploratory study to understand self-claimed assumptions (SCAs) about their distribution, classification, and impacts using code comments from nine popular DL framework projects on GitHub. The results are that: (1) 3,084 SCAs are scattered across 1,775 files in the nine DL frameworks, ranging from 1,460 (TensorFlow) to 8 (Keras) SCAs. (2) There are four types of validity of SCAs: Valid SCA, Invalid SCA, Conditional SCA, and Unknown SCA, and four types of SCAs based on their content: Configuration and Context SCA, Design SCA, Tensor and Variable SCA, and Miscellaneous SCA. (3) Both valid and invalid SCAs may have an impact within a specific scope (e.g., in a function) on the DL frameworks. Certain technical debt is induced when making SCAs. There are source code written and decisions made based on SCAs. This is the first study on investigating SCAs in DL frameworks, which helps researchers and practitioners to get a comprehensive understanding on the assumptions made. We also provide the first dataset of SCAs for further research and practice in this area.

</details>

<details>

<summary>2021-04-29 17:07:06 - Questioning causality on sex, gender and COVID-19, and identifying bias in large-scale data-driven analyses: the Bias Priority Recommendations and Bias Catalog for Pandemics</summary>

- *Natalia Díaz-Rodríguez, Rūta Binkytė-Sadauskienė, Wafae Bakkali, Sannidhi Bookseller, Paola Tubaro, Andrius Bacevicius, Raja Chatila*

- `2104.14492v1` - [abs](http://arxiv.org/abs/2104.14492v1) - [pdf](http://arxiv.org/pdf/2104.14492v1)

> The COVID-19 pandemic has spurred a large amount of observational studies reporting linkages between the risk of developing severe COVID-19 or dying from it, and sex and gender. By reviewing a large body of related literature and conducting a fine grained analysis based on sex-disaggregated data of 61 countries spanning 5 continents, we discover several confounding factors that could possibly explain the supposed male vulnerability to COVID-19. We thus highlight the challenge of making causal claims based on available data, given the lack of statistical significance and potential existence of biases. Informed by our findings on potential variables acting as confounders, we contribute a broad overview on the issues bias, explainability and fairness entail in data-driven analyses. Thus, we outline a set of discriminatory policy consequences that could, based on such results, lead to unintended discrimination. To raise awareness on the dimensionality of such foreseen impacts, we have compiled an encyclopedia-like reference guide, the Bias Catalog for Pandemics (BCP), to provide definitions and emphasize realistic examples of bias in general, and within the COVID-19 pandemic context. These are categorized within a division of bias families and a 2-level priority scale, together with preventive steps. In addition, we facilitate the Bias Priority Recommendations on how to best use and apply this catalog, and provide guidelines in order to address real world research questions. The objective is to anticipate and avoid disparate impact and discrimination, by considering causality, explainability, bias and techniques to mitigate the latter. With these, we hope to 1) contribute to designing and conducting fair and equitable data-driven studies and research; and 2) interpret and draw meaningful and actionable conclusions from these.

</details>

<details>

<summary>2021-04-29 21:01:53 - MAB-Malware: A Reinforcement Learning Framework for Attacking Static Malware Classifiers</summary>

- *Wei Song, Xuezixiang Li, Sadia Afroz, Deepali Garg, Dmitry Kuznetsov, Heng Yin*

- `2003.03100v3` - [abs](http://arxiv.org/abs/2003.03100v3) - [pdf](http://arxiv.org/pdf/2003.03100v3)

> Modern commercial antivirus systems increasingly rely on machine learning to keep up with the rampant inflation of new malware. However, it is well-known that machine learning models are vulnerable to adversarial examples (AEs). Previous works have shown that ML malware classifiers are fragile to the white-box adversarial attacks. However, ML models used in commercial antivirus products are usually not available to attackers and only return hard classification labels. Therefore, it is more practical to evaluate the robustness of ML models and real-world AVs in a pure black-box manner. We propose a black-box Reinforcement Learning (RL) based framework to generate AEs for PE malware classifiers and AV engines. It regards the adversarial attack problem as a multi-armed bandit problem, which finds an optimal balance between exploiting the successful patterns and exploring more varieties. Compared to other frameworks, our improvements lie in three points. 1) Limiting the exploration space by modeling the generation process as a stateless process to avoid combination explosions. 2) Due to the critical role of payload in AE generation, we design to reuse the successful payload in modeling. 3) Minimizing the changes on AE samples to correctly assign the rewards in RL learning. It also helps identify the root cause of evasions. As a result, our framework has much higher black-box evasion rates than other off-the-shelf frameworks. Results show it has over 74\%--97\% evasion rate for two state-of-the-art ML detectors and over 32\%--48\% evasion rate for commercial AVs in a pure black-box setting. We also demonstrate that the transferability of adversarial attacks among ML-based classifiers is higher than the attack transferability between purely ML-based and commercial AVs.

</details>

<details>

<summary>2021-04-30 09:37:14 - WELES: Policy-driven Runtime Integrity Enforcement of Virtual Machines</summary>

- *Wojciech Ozga, Do Le Quoc, Christof Fetzer*

- `2104.14862v1` - [abs](http://arxiv.org/abs/2104.14862v1) - [pdf](http://arxiv.org/pdf/2104.14862v1)

> Trust is of paramount concern for tenants to deploy their security-sensitive services in the cloud. The integrity of VMs in which these services are deployed needs to be ensured even in the presence of powerful adversaries with administrative access to the cloud. Traditional approaches for solving this challenge leverage trusted computing techniques, e.g., vTPM, or hardware CPU extensions, e.g., AMD SEV. But, they are vulnerable to powerful adversaries, or they provide only load time (not runtime) integrity measurements of VMs.   We propose WELES, a protocol allowing tenants to establish and maintain trust in VM runtime integrity of software and its configuration. WELES is transparent to the VM configuration and setup. It performs an implicit attestation of VMs during a secure login and binds the VM integrity state with the secure connection. Our prototype's evaluation shows that WELES is practical and incurs low performance overhead.

</details>

<details>

<summary>2021-04-30 10:18:43 - DeBayes: a Bayesian Method for Debiasing Network Embeddings</summary>

- *Maarten Buyl, Tijl De Bie*

- `2002.11442v3` - [abs](http://arxiv.org/abs/2002.11442v3) - [pdf](http://arxiv.org/pdf/2002.11442v3)

> As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.

</details>

<details>

<summary>2021-04-30 11:09:05 - LightIoT: Lightweight and Secure Communication for Energy-Efficient IoT in Health Informatics</summary>

- *Mian Ahmad Jan, Fazlullah Khan, Spyridon Mastorakis, Muhammad Adil, Aamir Akbar, Nicholas Stergiou*

- `2104.14906v1` - [abs](http://arxiv.org/abs/2104.14906v1) - [pdf](http://arxiv.org/pdf/2104.14906v1)

> Internet of Things (IoT) is considered as a key enabler of health informatics. IoT-enabled devices are used for in-hospital and in-home patient monitoring to collect and transfer biomedical data pertaining to blood pressure, electrocardiography (ECG), blood sugar levels, body temperature, etc. Among these devices, wearables have found their presence in a wide range of healthcare applications. These devices generate data in real-time and transmit them to nearby gateways and remote servers for processing and visualization. The data transmitted by these devices are vulnerable to a range of adversarial threats, and as such, privacy and integrity need to be preserved. In this paper, we present LightIoT, a lightweight and secure communication approach for data exchanged among the devices of a healthcare infrastructure. LightIoT operates in three phases: initialization, pairing, and authentication. These phases ensure the reliable transmission of data by establishing secure sessions among the communicating entities (wearables, gateways and a remote server). Statistical results exhibit that our scheme is lightweight, robust, and resilient against a wide range of adversarial attacks and incurs much lower computational and communication overhead for the transmitted data in the presence of existing approaches.

</details>

<details>

<summary>2021-04-30 13:27:09 - FIPAC: Thwarting Fault- and Software-Induced Control-Flow Attacks with ARM Pointer Authentication</summary>

- *Robert Schilling, Pascal Nasahl, Stefan Mangard*

- `2104.14993v1` - [abs](http://arxiv.org/abs/2104.14993v1) - [pdf](http://arxiv.org/pdf/2104.14993v1)

> With the improvements of computing technology, more and more applications embed powerful ARM processors into their devices. These systems can be attacked by redirecting the control-flow of a program to bypass critical pieces of code such as privilege checks or signature verifications. Control-flow hijacks can be performed using classical software vulnerabilities, physical fault attacks, or software-induced fault attacks. To cope with this threat and to protect the control-flow, dedicated countermeasures are needed. To counteract control-flow hijacks, control-flow integrity~(CFI) aims to be a generic solution. However, software-based CFI typically either protects against software or fault attacks, but not against both. While hardware-assisted CFI can mitigate both types of attacks, they require extensive hardware modifications. As hardware changes are unrealistic for existing ARM architectures, a wide range of systems remains unprotected and vulnerable to control-flow attacks.   In this work, we present FIPAC, an efficient software-based CFI scheme protecting the execution at basic block granularity of ARM-based devices against software and fault attacks. FIPAC exploits ARM pointer authentication of ARMv8.6-A to implement a cryptographically signed control-flow graph. We cryptographically link the correct sequence of executed basic blocks to enforce CFI at this granularity. We use an LLVM-based toolchain to automatically instrument programs. The evaluation on SPEC2017 with different security policies shows a code overhead between 54-97\% and a runtime overhead between 35-105%. While these overheads are higher than for countermeasures against software attacks, FIPAC outperforms related work protecting the control-flow against fault attacks. FIPAC is an efficient solution to provide protection against software- and fault-based CFI attacks on basic block level on modern ARM devices.

</details>

<details>

<summary>2021-04-30 15:38:16 - DeFiRanger: Detecting Price Manipulation Attacks on DeFi Applications</summary>

- *Siwei Wu, Dabao Wang, Jianting He, Yajin Zhou, Lei Wu, Xingliang Yuan, Qinming He, Kui Ren*

- `2104.15068v1` - [abs](http://arxiv.org/abs/2104.15068v1) - [pdf](http://arxiv.org/pdf/2104.15068v1)

> The rapid growth of Decentralized Finance (DeFi) boosts the Ethereum ecosystem. At the same time, attacks towards DeFi applications (apps) are increasing. However, to the best of our knowledge, existing smart contract vulnerability detection tools cannot be directly used to detect DeFi attacks. That's because they lack the capability to recover and understand high-level DeFi semantics, e.g., a user trades a token pair X and Y in a Decentralized EXchange (DEX).   In this work, we focus on the detection of two types of new attacks on DeFi apps, including direct and indirect price manipulation attacks. The former one means that an attacker directly manipulates the token price in DEX by performing an unwanted trade in the same DEX by attacking the vulnerable DeFi app. The latter one means that an attacker indirectly manipulates the token price of the vulnerable DeFi app (e.g., a lending app). To this end, we propose a platform-independent way to recover high-level DeFi semantics by first constructing the cash flow tree from raw Ethereum transactions and then lifting the low-level semantics to high-level ones, including token trade, liquidity mining, and liquidity cancel. Finally, we detect price manipulation attacks using the patterns expressed with the recovered DeFi semantics.   We have implemented a prototype named \tool{} and applied it to more than 350 million transactions. It successfully detected 432 real-world attacks in the wild. We confirm that they belong to four known security incidents and five zero-day ones. We reported our findings. Two CVEs have been assigned. We further performed an attack analysis to reveal the root cause of the vulnerability, the attack footprint, and the impact of the attack. Our work urges the need to secure the DeFi ecosystem.

</details>

<details>

<summary>2021-04-30 17:23:07 - Does "AI" stand for augmenting inequality in the era of covid-19 healthcare?</summary>

- *David Leslie, Anjali Mazumder, Aidan Peppin, Maria Wolters, Alexa Hagerty*

- `2105.07844v1` - [abs](http://arxiv.org/abs/2105.07844v1) - [pdf](http://arxiv.org/pdf/2105.07844v1)

> Among the most damaging characteristics of the covid-19 pandemic has been its disproportionate effect on disadvantaged communities. As the outbreak has spread globally, factors such as systemic racism, marginalisation, and structural inequality have created path dependencies that have led to poor health outcomes. These social determinants of infectious disease and vulnerability to disaster have converged to affect already disadvantaged communities with higher levels of economic instability, disease exposure, infection severity, and death. Artificial intelligence (AI) technologies are an important part of the health informatics toolkit used to fight contagious disease. AI is well known, however, to be susceptible to algorithmic biases that can entrench and augment existing inequality. Uncritically deploying AI in the fight against covid-19 thus risks amplifying the pandemic's adverse effects on vulnerable groups, exacerbating health inequity. In this paper, we claim that AI systems can introduce or reflect bias and discrimination in three ways: in patterns of health discrimination that become entrenched in datasets, in data representativeness, and in human choices made during the design, development, and deployment of these systems. We highlight how the use of AI technologies threaten to exacerbate the disparate effect of covid-19 on marginalised, under-represented, and vulnerable groups, particularly black, Asian, and other minoritised ethnic people, older populations, and those of lower socioeconomic status. We conclude that, to mitigate the compounding effects of AI on inequalities associated with covid-19, decision makers, technology developers, and health officials must account for the potential biases and inequities at all stages of the AI process.

</details>

<details>

<summary>2021-04-30 20:14:11 - Automated Symbolic Verification of Telegram's MTProto 2.0</summary>

- *Marino Miculan, Nicola Vitacolonna*

- `2012.03141v2` - [abs](http://arxiv.org/abs/2012.03141v2) - [pdf](http://arxiv.org/pdf/2012.03141v2)

> MTProto 2.0 is a suite of cryptographic protocols for instant messaging at the core of the popular Telegram messenger application. In this paper we analyse MTProto 2.0 using the symbolic verifier ProVerif. We provide fully automated proofs of the soundness of MTProto 2.0's authentication, normal chat, end-to-end encrypted chat, and rekeying mechanisms with respect to several security properties, including authentication, integrity, secrecy and perfect forward secrecy; at the same time, we discover that the rekeying protocol is vulnerable to an unknown key-share (UKS) attack. We proceed in an incremental way: each protocol is examined in isolation, relying only on the guarantees provided by the previous ones and the robustness of the basic cryptographic primitives. Our research proves the formal correctness of MTProto 2.0 w.r.t. most relevant security properties, and it can serve as a reference for implementation and analysis of clients and servers.

</details>


## 2021-05

<details>

<summary>2021-05-01 03:02:21 - Pedestrian Collision Avoidance for Autonomous Vehicles at Unsignalized Intersection Using Deep Q-Network</summary>

- *Kasra Mokhtari, Alan R. Wagner*

- `2105.00153v1` - [abs](http://arxiv.org/abs/2105.00153v1) - [pdf](http://arxiv.org/pdf/2105.00153v1)

> Prior research has extensively explored Autonomous Vehicle (AV) navigation in the presence of other vehicles, however, navigation among pedestrians, who are the most vulnerable element in urban environments, has been less examined. This paper explores AV navigation in crowded, unsignalized intersections. We compare the performance of different deep reinforcement learning methods trained on our reward function and state representation. The performance of these methods and a standard rule-based approach were evaluated in two ways, first at the unsignalized intersection on which the methods were trained, and secondly at an unknown unsignalized intersection with a different topology. For both scenarios, the rule-based method achieves less than 40\% collision-free episodes, whereas our methods result in a performance of approximately 100\%. Of the three methods used, DDQN/PER outperforms the other two methods while it also shows the smallest average intersection crossing time, the greatest average speed, and the greatest distance from the closest pedestrian.

</details>

<details>

<summary>2021-05-01 08:34:01 - VulDeeLocator: A Deep Learning-based Fine-grained Vulnerability Detector</summary>

- *Zhen Li, Deqing Zou, Shouhuai Xu, Zhaoxuan Chen, Yawei Zhu, Hai Jin*

- `2001.02350v2` - [abs](http://arxiv.org/abs/2001.02350v2) - [pdf](http://arxiv.org/pdf/2001.02350v2)

> Automatically detecting software vulnerabilities is an important problem that has attracted much attention from the academic research community. However, existing vulnerability detectors still cannot achieve the vulnerability detection capability and the locating precision that would warrant their adoption for real-world use. In this paper, we present a vulnerability detector that can simultaneously achieve a high detection capability and a high locating precision, dubbed Vulnerability Deep learning-based Locator (VulDeeLocator). In the course of designing VulDeeLocator, we encounter difficulties including how to accommodate semantic relations between the definitions of types as well as macros and their uses across files, how to accommodate accurate control flows and variable define-use relations, and how to achieve high locating precision. We solve these difficulties by using two innovative ideas: (i) leveraging intermediate code to accommodate extra semantic information, and (ii) using the notion of granularity refinement to pin down locations of vulnerabilities. When applied to 200 files randomly selected from three real-world software products, VulDeeLocator detects 18 confirmed vulnerabilities (i.e., true-positives). Among them, 16 vulnerabilities correspond to known vulnerabilities; the other two are not reported in the National Vulnerability Database (NVD) but have been "silently" patched by the vendor of Libav when releasing newer versions.

</details>

<details>

<summary>2021-05-01 19:33:28 - Privacy and Integrity Preserving Training Using Trusted Hardware</summary>

- *Hanieh Hashemi, Yongqin Wang, Murali Annavaram*

- `2105.00334v1` - [abs](http://arxiv.org/abs/2105.00334v1) - [pdf](http://arxiv.org/pdf/2105.00334v1)

> Privacy and security-related concerns are growing as machine learning reaches diverse application domains. The data holders want to train with private data while exploiting accelerators, such as GPUs, that are hosted in the cloud. However, Cloud systems are vulnerable to attackers that compromise the privacy of data and integrity of computations. This work presents DarKnight, a framework for large DNN training while protecting input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEE) and accelerators, where the TEE provides privacy and integrity verification, while accelerators perform the computation heavy linear algebraic operations.

</details>

<details>

<summary>2021-05-02 02:51:57 - A Systematic Review of Security in the LoRaWAN Network Protocol</summary>

- *Poliana de Moraes, Arlindo Flavio da Conceição*

- `2105.00384v1` - [abs](http://arxiv.org/abs/2105.00384v1) - [pdf](http://arxiv.org/pdf/2105.00384v1)

> The age of the Internet of Things is adding an increasing number of new devices to the Internet and is expected to have fifty billion connected units by 2021. These form an extensive network that may have multiple points where there is a risk of attacks that can compromise the entire system. This paper has conducted a systematic review of security in LoRaWAN protocol specification versions 1.0 and 1.1 by locating its vulnerabilities and determining what measures can be taken for improvement and how they can be checked or tested. The review identifies nineteen areas of vulnerability in the LoRaWAN protocol and shows that the current studies focus on specification version 1.0, key management, and authentication procedures.

</details>

<details>

<summary>2021-05-03 16:12:15 - Robust and Generalizable Visual Representation Learning via Random Convolutions</summary>

- *Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, Marc Niethammer*

- `2007.13003v3` - [abs](http://arxiv.org/abs/2007.13003v3) - [pdf](http://arxiv.org/pdf/2007.13003v3)

> While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local textures. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation.

</details>

<details>

<summary>2021-05-03 22:19:22 - Unreasonable Effectiveness of Rule-Based Heuristics in Solving Russian SuperGLUE Tasks</summary>

- *Tatyana Iazykova, Denis Kapelyushnik, Olga Bystrova, Andrey Kutuzov*

- `2105.01192v1` - [abs](http://arxiv.org/abs/2105.01192v1) - [pdf](http://arxiv.org/pdf/2105.01192v1)

> Leader-boards like SuperGLUE are seen as important incentives for active development of NLP, since they provide standard benchmarks for fair comparison of modern language models. They have driven the world's best engineering teams as well as their resources to collaborate and solve a set of tasks for general language understanding. Their performance scores are often claimed to be close to or even higher than the human performance. These results encouraged more thorough analysis of whether the benchmark datasets featured any statistical cues that machine learning based language models can exploit. For English datasets, it was shown that they often contain annotation artifacts. This allows solving certain tasks with very simple rules and achieving competitive rankings.   In this paper, a similar analysis was done for the Russian SuperGLUE (RSG), a recently published benchmark set and leader-board for Russian natural language understanding. We show that its test datasets are vulnerable to shallow heuristics. Often approaches based on simple rules outperform or come close to the results of the notorious pre-trained language models like GPT-3 or BERT. It is likely (as the simplest explanation) that a significant part of the SOTA models performance in the RSG leader-board is due to exploiting these shallow heuristics and that has nothing in common with real language understanding. We provide a set of recommendations on how to improve these datasets, making the RSG leader-board even more representative of the real progress in Russian NLU.

</details>

<details>

<summary>2021-05-04 08:05:52 - Interactive Static Software Performance Analysis in the IDE</summary>

- *Aaron Beigelbeck, Maurício Aniche, Jürgen Cito*

- `2105.02023v1` - [abs](http://arxiv.org/abs/2105.02023v1) - [pdf](http://arxiv.org/pdf/2105.02023v1)

> Detecting performance issues due to suboptimal code during the development process can be a daunting task, especially when it comes to localizing them after noticing performance degradation after deployment. Static analysis has the potential to provide early feedback on performance problems to developers without having to run profilers with expensive (and often unavailable) performance tests. We develop a VSCode tool that integrates the static performance analysis results from Infer via code annotations and decorations (surfacing complexity analysis results in context) and side panel views showing details and overviews (enabling explainability of the results). Additionally, we design our system for interactivity to allow for more responsiveness to code changes as they happen. We evaluate the efficacy of our tool by measuring the overhead that the static performance analysis integration introduces in the development workflow. Further, we report on a case study that illustrates how our system can be used to reason about software performance in the context of a real performance bug in the ElasticSearch open-source project.   Demo video: https://www.youtube.com/watch?v=-GqPb_YZMOs Repository: https://github.com/ipa-lab/vscode-infer-performance

</details>

<details>

<summary>2021-05-04 10:32:30 - An Overview of Laser Injection against Embedded Neural Network Models</summary>

- *Mathieu Dumont, Pierre-Alain Moellic, Raphael Viera, Jean-Max Dutertre, Rémi Bernhard*

- `2105.01403v1` - [abs](http://arxiv.org/abs/2105.01403v1) - [pdf](http://arxiv.org/pdf/2105.01403v1)

> For many IoT domains, Machine Learning and more particularly Deep Learning brings very efficient solutions to handle complex data and perform challenging and mostly critical tasks. However, the deployment of models in a large variety of devices faces several obstacles related to trust and security. The latest is particularly critical since the demonstrations of severe flaws impacting the integrity, confidentiality and accessibility of neural network models. However, the attack surface of such embedded systems cannot be reduced to abstract flaws but must encompass the physical threats related to the implementation of these models within hardware platforms (e.g., 32-bit microcontrollers). Among physical attacks, Fault Injection Analysis (FIA) are known to be very powerful with a large spectrum of attack vectors. Most importantly, highly focused FIA techniques such as laser beam injection enable very accurate evaluation of the vulnerabilities as well as the robustness of embedded systems. Here, we propose to discuss how laser injection with state-of-the-art equipment, combined with theoretical evidences from Adversarial Machine Learning, highlights worrying threats against the integrity of deep learning inference and claims that join efforts from the theoretical AI and Physical Security communities are a urgent need.

</details>

<details>

<summary>2021-05-04 10:42:38 - Hardness-Preserving Reductions via Cuckoo Hashing</summary>

- *Itay Berman, Iftach Haitner, Ilan Komargodski, Moni Naor*

- `2105.01409v1` - [abs](http://arxiv.org/abs/2105.01409v1) - [pdf](http://arxiv.org/pdf/2105.01409v1)

> The focus of this work is \emph{hardness-preserving} transformations of somewhat limited pseudorandom functions families (PRFs) into ones with more versatile characteristics. Consider the problem of \emph{domain extension} of pseudorandom functions: given a PRF that takes as input elements of some domain $U$, we would like to come up with a PRF over a larger domain. Can we do it with little work and without significantly impacting the security of the system? One approach is to first hash the larger domain into the smaller one and then apply the original PRF. Such a reduction, however, is vulnerable to a "birthday attack": after $\sqrt{\size{U}}$ queries to the resulting PRF, a collision (\ie two distinct inputs having the same hash value) is very likely to occur. As a consequence, the resulting PRF is \emph{insecure} against an attacker making this number of queries. In this work we show how to go beyond the aforementioned birthday attack barrier by replacing the above simple hashing approach with a variant of \textit{cuckoo hashing}, a hashing paradigm that resolves collisions in a table by using two hash functions and two tables, cleverly assigning each element to one of the two tables. We use this approach to obtain: (i) a domain extension method that requires {\em just two calls} to the original PRF, can withstand as many queries as the original domain size, and has a distinguishing probability that is exponentially small in the amount of non-cryptographic work; and (ii) a {\em security-preserving} reduction from non-adaptive to adaptive PRFs.

</details>

<details>

<summary>2021-05-04 12:28:09 - A Catalogue of Game-Specific Software Nuggets</summary>

- *Vartika Agrahari, Sridhar Chimalakonda*

- `2006.13129v2` - [abs](http://arxiv.org/abs/2006.13129v2) - [pdf](http://arxiv.org/pdf/2006.13129v2)

> With the ever-increasing use of games, game developers are expected to write efficient code supporting several qualities such as security, maintainability, and performance. However, the continuous need to update the features of games in less duration might compel the developers to use anti-patterns, code smells and quick-fix solutions that may affect the functional and non-functional requirements of the game. These bad practices may lead to technical debt, poor program comprehension, and can cause several issues during software maintenance. Here, in this paper, we introduce "Software Nuggets" as a concept that affects software quality in a negative way and as a superset of anti-patterns, code smells, bugs, software bad practices. We call these Software Nuggets as "G-Nuggets" in the context of games. While there exists empirical research on games, we are not aware of any work on understanding and cataloguing these G-Nuggets. Thus, we propose a catalogue of G-Nuggets by mining and analyzing 892 commits, 189 issues, and 104 pull requests from 100 open-source GitHub game repositories. We use regular expressions and thematic analysis on this dataset for cataloguing game-specific Software Nuggets. We present a catalogue of ten G-Nuggets and provide examples for them present online at: https://phoebs88.github.io/A-Catalogue-of-Game-Specific-Software-Nuggets. We believe this catalogue might be helpful for researchers for further empirical research in the domain of games as well as for game developers to improve quality of games.

</details>

<details>

<summary>2021-05-04 14:58:48 - The Synergy of Complex Event Processing and Tiny Machine Learning in Industrial IoT</summary>

- *Haoyu Ren, Darko Anicic, Thomas Runkler*

- `2105.03371v1` - [abs](http://arxiv.org/abs/2105.03371v1) - [pdf](http://arxiv.org/pdf/2105.03371v1)

> Focusing on comprehensive networking, big data, and artificial intelligence, the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness in factory operations. Various sensors and field devices play a central role, as they generate a vast amount of real-time data that can provide insights into manufacturing. The synergy of complex event processing (CEP) and machine learning (ML) has been developed actively in the last years in IIoT to identify patterns in heterogeneous data streams and fuse raw data into tangible facts. In a traditional compute-centric paradigm, the raw field data are continuously sent to the cloud and processed centrally. As IIoT devices become increasingly pervasive and ubiquitous, concerns are raised since transmitting such amount of data is energy-intensive, vulnerable to be intercepted, and subjected to high latency. The data-centric paradigm can essentially solve these problems by empowering IIoT to perform decentralized on-device ML and CEP, keeping data primarily on edge devices and minimizing communications. However, this is no mean feat because most IIoT edge devices are designed to be computationally constrained with low power consumption. This paper proposes a framework that exploits ML and CEP's synergy at the edge in distributed sensor networks. By leveraging tiny ML and micro CEP, we shift the computation from the cloud to the power-constrained IIoT devices and allow users to adapt the on-device ML model and the CEP reasoning logic flexibly on the fly without requiring to reupload the whole program. Lastly, we evaluate the proposed solution and show its effectiveness and feasibility using an industrial use case of machine safety monitoring.

</details>

<details>

<summary>2021-05-05 01:01:42 - Vulnerability of Blockchain Technologies to Quantum Attacks</summary>

- *Joseph J. Kearney, Carlos A. Perez-Delgado*

- `2105.01815v1` - [abs](http://arxiv.org/abs/2105.01815v1) - [pdf](http://arxiv.org/pdf/2105.01815v1)

> Quantum computation represents a threat to many cryptographic protocols in operation today. It has been estimated that by 2035, there will exist a quantum computer capable of breaking the vital cryptographic scheme RSA2048. Blockchain technologies rely on cryptographic protocols for many of their essential sub-routines. Some of these protocols, but not all, are open to quantum attacks. Here we analyze the major blockchain-based cryptocurrencies deployed today -- including Bitcoin, Ethereum, Litecoin and ZCash, and determine their risk exposure to quantum attacks. We finish with a comparative analysis of the studied cryptocurrencies and their underlying blockchain technologies and their relative levels of vulnerability to quantum attacks.

</details>

<details>

<summary>2021-05-05 04:18:54 - SemperFi: A Spoofer Eliminating GPS Receiver for UAVs</summary>

- *Harshad Sathaye, Gerald LaMountain, Pau Closas, Aanjhan Ranganathan*

- `2105.01860v1` - [abs](http://arxiv.org/abs/2105.01860v1) - [pdf](http://arxiv.org/pdf/2105.01860v1)

> It is well-known that GPS is vulnerable to signal spoofing attacks. Although several spoofing detection techniques exist, they are incapable of mitigation and recovery from stealthy attackers. In this work, we present SemperFi, a single antenna GPS receiver capable of tracking legitimate GPS satellite signals and estimating the true location even during a spoofing attack. The main challenge in building SemperFi is, unlike most wireless systems where \emph{the data} contained in the wireless signals is important, GPS relies on the time of arrival (ToA) of satellite signals. SemperFi is capable of distinguishing spoofing signals and recovering legitimate GPS signals that are even completely overshadowed by a strong adversary. We exploit the short-term stability of inertial sensors to identify the spoofing signal and extend the successive interference cancellation algorithm to preserve the legitimate signal's ToA. We implement SemperFi in GNSS-SDR, an open-source software-defined GNSS receiver, and evaluate its performance using UAV simulators, real drones, a variety of real-world GPS datasets, and various embedded platforms. Our evaluation results indicate that in many scenarios, SemperFi can identify adversarial peaks by executing flight patterns that are less than 50 m long and recover the true location within 10 seconds (Jetson Xavier). We show that our receiver is secure against stealthy attackers who exploit inertial sensor errors and execute seamless takeover attacks. We design SemperFi as a pluggable module capable of generating a spoofer-free GPS signal for processing on any commercial-off-the-shelf GPS receiver available today. Finally, we release our implementation to the community for usage and further research.

</details>

<details>

<summary>2021-05-05 08:11:03 - Exploiting Vulnerabilities in Deep Neural Networks: Adversarial and Fault-Injection Attacks</summary>

- *Faiq Khalid, Muhammad Abdullah Hanif, Muhammad Shafique*

- `2105.03251v1` - [abs](http://arxiv.org/abs/2105.03251v1) - [pdf](http://arxiv.org/pdf/2105.03251v1)

> From tiny pacemaker chips to aircraft collision avoidance systems, the state-of-the-art Cyber-Physical Systems (CPS) have increasingly started to rely on Deep Neural Networks (DNNs). However, as concluded in various studies, DNNs are highly susceptible to security threats, including adversarial attacks. In this paper, we first discuss different vulnerabilities that can be exploited for generating security attacks for neural network-based systems. We then provide an overview of existing adversarial and fault-injection-based attacks on DNNs. We also present a brief analysis to highlight different challenges in the practical implementation of adversarial attacks. Finally, we also discuss various prospective ways to develop robust DNN-based systems that are resilient to adversarial and fault-injection attacks.

</details>

<details>

<summary>2021-05-05 17:47:22 - Massive MIMO-NOMA Systems Secrecy in the Presence of Active Eavesdroppers</summary>

- *Marziyeh Soltani, Mahtab Mirmohseni, Panos Papadimitratos*

- `2105.02215v1` - [abs](http://arxiv.org/abs/2105.02215v1) - [pdf](http://arxiv.org/pdf/2105.02215v1)

> Non-orthogonal multiple access (NOMA) and massive multiple-input multiple-output (MIMO) systems are highly efficient. Massive MIMO systems are inherently resistant to passive attackers (eavesdroppers), thanks to transmissions directed to the desired users. However, active attackers can transmit a combination of legitimate user pilot signals during the channel estimation phase. This way they can mislead the base station (BS) to rotate the transmission in their direction, and allow them to eavesdrop during the downlink data transmission phase. In this paper, we analyse this vulnerability in an improved system model and stronger adversary assumptions, and investigate how physical layer security can mitigate such attacks and ensure secure (confidential) communication. We derive the secrecy outage probability (SOP) and a lower bound on the ergodic secrecy capacity, using stochastic geometry tools when the number of antennas in the BSs tends to infinity. We adapt the result to evaluate the secrecy performance in massive orthogonal multiple access (OMA). We find that appropriate power allocation allows NOMA to outperform OMA in terms of ergodic secrecy rate and SOP.

</details>

<details>

<summary>2021-05-06 01:28:21 - Security Vulnerability Detection Using Deep Learning Natural Language Processing</summary>

- *Noah Ziems, Shaoen Wu*

- `2105.02388v1` - [abs](http://arxiv.org/abs/2105.02388v1) - [pdf](http://arxiv.org/pdf/2105.02388v1)

> Detecting security vulnerabilities in software before they are exploited has been a challenging problem for decades. Traditional code analysis methods have been proposed, but are often ineffective and inefficient. In this work, we model software vulnerability detection as a natural language processing (NLP) problem with source code treated as texts, and address the automated software venerability detection with recent advanced deep learning NLP models assisted by transfer learning on written English. For training and testing, we have preprocessed the NIST NVD/SARD databases and built a dataset of over 100,000 files in $C$ programming language with 123 types of vulnerabilities. The extensive experiments generate the best performance of over 93\% accuracy in detecting security vulnerabilities.

</details>

<details>

<summary>2021-05-06 01:37:51 - Migrating Client Code without Change Examples</summary>

- *Hao Zhong, Na Meng*

- `2105.02389v1` - [abs](http://arxiv.org/abs/2105.02389v1) - [pdf](http://arxiv.org/pdf/2105.02389v1)

> API developers evolve software libraries to fix bugs, add new features, or refactor code. To benefit from such library evolution, the programmers of client projects have to repetitively upgrade their library usages and adapt their codebases to any library API breaking changes (e.g., API renaming). Such adaptive changes can be tedious and error-prone. Existing tools provide limited support to help programmers migrate client projects from old library versions to new ones. For instance, some tools extract API mappings be-tween library versions and only suggest simple adaptive changes (i.e., statement updates); other tools suggest or automate more complicated edits (e.g., statement insertions) based on user-provided exemplar code migrations. However, when new library versions are available, it is usually cumbersome and time-consuming for users to provide sufficient human-crafted samples in order to guide automatic migration. In this paper, we propose a novel approach, AutoUpdate, to further improve the state of the art. Instead of learning from change examples, we designed AutoUpdate to automate migration in a compiler-directed way. Namely, given a compilation error triggered by upgrading libraries, AutoUpdate exploits 13 migration opera-tors to generate candidate edits, and tentatively applies each edit until the error is resolved or all edits are explored. We conducted two experiments. The first experiment involves migrating 371 tutorial examples between versions of 5 popular libraries. AutoUpdate reduced migration-related compilation errors for 92.7% of tasks. It eliminated such errors for 32.4% of tasks, and 33.9% of the tasks have identical edits to manual migrations. In the second experiment, we applied AutoUpdate to migrate two real client projects of lucene. AutoUpdate successfully migrated both projects, and the migrated code passed all tests.

</details>

<details>

<summary>2021-05-06 08:41:15 - Diagnosing Vulnerability of Variational Auto-Encoders to Adversarial Attacks</summary>

- *Anna Kuzina, Max Welling, Jakub M. Tomczak*

- `2103.06701v3` - [abs](http://arxiv.org/abs/2103.06701v3) - [pdf](http://arxiv.org/pdf/2103.06701v3)

> In this work, we explore adversarial attacks on the Variational Autoencoders (VAE). We show how to modify data point to obtain a prescribed latent code (supervised attack) or just get a drastically different code (unsupervised attack). We examine the influence of model modifications ($\beta$-VAE, NVAE) on the robustness of VAEs and suggest metrics to quantify it.

</details>

<details>

<summary>2021-05-06 10:26:34 - Distributive Justice and Fairness Metrics in Automated Decision-making: How Much Overlap Is There?</summary>

- *Matthias Kuppler, Christoph Kern, Ruben L. Bach, Frauke Kreuter*

- `2105.01441v2` - [abs](http://arxiv.org/abs/2105.01441v2) - [pdf](http://arxiv.org/pdf/2105.01441v2)

> The advent of powerful prediction algorithms led to increased automation of high-stake decisions regarding the allocation of scarce resources such as government spending and welfare support. This automation bears the risk of perpetuating unwanted discrimination against vulnerable and historically disadvantaged groups. Research on algorithmic discrimination in computer science and other disciplines developed a plethora of fairness metrics to detect and correct discriminatory algorithms. Drawing on robust sociological and philosophical discourse on distributive justice, we identify the limitations and problematic implications of prominent fairness metrics. We show that metrics implementing equality of opportunity only apply when resource allocations are based on deservingness, but fail when allocations should reflect concerns about egalitarianism, sufficiency, and priority. We argue that by cleanly distinguishing between prediction tasks and decision tasks, research on fair machine learning could take better advantage of the rich literature on distributive justice.

</details>

<details>

<summary>2021-05-06 10:29:44 - A novel method of predictive collision risk area estimation for proactive pedestrian accident prevention system in urban surveillance infrastructure</summary>

- *Byeongjoon Noh, Hwasoo Yeo*

- `2105.02572v1` - [abs](http://arxiv.org/abs/2105.02572v1) - [pdf](http://arxiv.org/pdf/2105.02572v1)

> Road traffic accidents, especially vehicle pedestrian collisions in crosswalk, globally pose a severe threat to human lives and have become a leading cause of premature deaths. In order to protect such vulnerable road users from collisions, it is necessary to recognize possible conflict in advance and warn to road users, not post facto. A breakthrough for proactively preventing pedestrian collisions is to recognize pedestrian's potential risks based on vision sensors such as CCTVs. In this study, we propose a predictive collision risk area estimation system at unsignalized crosswalks. The proposed system applied trajectories of vehicles and pedestrians from video footage after preprocessing, and then predicted their trajectories by using deep LSTM networks. With use of predicted trajectories, this system can infer collision risk areas statistically, further severity of levels is divided as danger, warning, and relative safe. In order to validate the feasibility and applicability of the proposed system, we applied it and assess the severity of potential risks in two unsignalized spots in Osan city, Korea.

</details>

<details>

<summary>2021-05-06 14:15:27 - Adversarial Attack on Large Scale Graph</summary>

- *Jintang Li, Tao Xie, Liang Chen, Fenfang Xie, Xiangnan He, Zibin Zheng*

- `2009.03488v2` - [abs](http://arxiv.org/abs/2009.03488v2) - [pdf](http://arxiv.org/pdf/2009.03488v2)

> Recent studies have shown that graph neural networks (GNNs) are vulnerable against perturbations due to lack of robustness and can therefore be easily fooled. Currently, most works on attacking GNNs are mainly using gradient information to guide the attack and achieve outstanding performance. However, the high complexity of time and space makes them unmanageable for large scale graphs and becomes the major bottleneck that prevents the practical usage. We argue that the main reason is that they have to use the whole graph for attacks, resulting in the increasing time and space complexity as the data scale grows. In this work, we propose an efficient Simplified Gradient-based Attack (SGA) method to bridge this gap. SGA can cause the GNNs to misclassify specific target nodes through a multi-stage attack framework, which needs only a much smaller subgraph. In addition, we present a practical metric named Degree Assortativity Change (DAC) to measure the impacts of adversarial attacks on graph data. We evaluate our attack method on four real-world graph networks by attacking several commonly used GNNs. The experimental results demonstrate that SGA can achieve significant time and memory efficiency improvements while maintaining competitive attack performance compared to state-of-art attack techniques. Codes are available via: https://github.com/EdisonLeeeee/SGAttack.

</details>

<details>

<summary>2021-05-06 16:48:52 - Dynamic Defense Approach for Adversarial Robustness in Deep Neural Networks via Stochastic Ensemble Smoothed Model</summary>

- *Ruoxi Qin, Linyuan Wang, Xingyuan Chen, Xuehui Du, Bin Yan*

- `2105.02803v1` - [abs](http://arxiv.org/abs/2105.02803v1) - [pdf](http://arxiv.org/pdf/2105.02803v1)

> Deep neural networks have been shown to suffer from critical vulnerabilities under adversarial attacks. This phenomenon stimulated the creation of different attack and defense strategies similar to those adopted in cyberspace security. The dependence of such strategies on attack and defense mechanisms makes the associated algorithms on both sides appear as closely reciprocating processes. The defense strategies are particularly passive in these processes, and enhancing initiative of such strategies can be an effective way to get out of this arms race. Inspired by the dynamic defense approach in cyberspace, this paper builds upon stochastic ensemble smoothing based on defense method of random smoothing and model ensemble. Proposed method employs network architecture and smoothing parameters as ensemble attributes, and dynamically change attribute-based ensemble model before every inference prediction request. The proposed method handles the extreme transferability and vulnerability of ensemble models under white-box attacks. Experimental comparison of ASR-vs-distortion curves with different attack scenarios shows that even the attacker with the highest attack capability cannot easily exceed the attack success rate associated with the ensemble smoothed model, especially under untargeted attacks.

</details>

<details>

<summary>2021-05-06 17:44:11 - Reentrancy Vulnerability Identification in Ethereum Smart Contracts</summary>

- *Noama Fatima Samreen, Manar H. Alalfi*

- `2105.02881v1` - [abs](http://arxiv.org/abs/2105.02881v1) - [pdf](http://arxiv.org/pdf/2105.02881v1)

> Ethereum Smart contracts use blockchain to transfer values among peers on networks without central agency. These programs are deployed on decentralized applications running on top of the blockchain consensus protocol to enable people to make agreements in a transparent and conflict-free environment. The security vulnerabilities within those smart contracts are a potential threat to the applications and have caused huge financial losses to their users. In this paper, we present a framework that combines static and dynamic analysis to detect Reentrancy vulnerabilities in Ethereum smart contracts. This framework generates an attacker contract based on the ABI specifications of smart contracts under test and analyzes the contract interaction to precisely report Reentrancy vulnerability. We conducted a preliminary evaluation of our proposed framework on 5 modified smart contracts from Etherscan and our framework was able to detect the Reentrancy vulnerability in all our modified contracts. Our framework analyzes smart contracts statically to identify potentially vulnerable functions and then uses dynamic analysis to precisely confirm Reentrancy vulnerability, thus achieving increased performance and reduced false positives.

</details>

<details>

<summary>2021-05-06 20:10:34 - A Cybersecurity Guide for Using Fitness Devices</summary>

- *Maria Bada, Basie von Solms*

- `2105.02933v1` - [abs](http://arxiv.org/abs/2105.02933v1) - [pdf](http://arxiv.org/pdf/2105.02933v1)

> The popularity of wearable devices is growing exponentially, with consumers using these for a variety of services. Fitness devices are currently offering new services such as shopping or buying train tickets using contactless payment. In addition, fitness devices are collecting a number of personal information such as body temperature, pulse rate, food habits and body weight, steps-distance travelled, calories burned and sleep stage. Although these devices can offer convenience to consumers, more and more reports are warning of the cybersecurity risks of such devices, and the possibilities for such devices to be hacked and used as springboards to other systems. Due to their wireless transmissions, these devices can potentially be vulnerable to a malicious attack allowing the data collected to be exposed. The vulnerabilities of these devices stem from lack of authentication, disadvantages of Bluetooth connections, location tracking as well as third party vulnerabilities. Guidelines do exist for securing such devices, but most of such guidance is directed towards device manufacturers or IoT providers, while consumers are often unaware of potential risks. The aim of this paper is to provide cybersecurity guidelines for users in order to take measures to avoid risks when using fitness devices.

</details>

<details>

<summary>2021-05-07 03:33:00 - Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates</summary>

- *Yuqing Xie, Yi-an Lai, Yuanjun Xiong, Yi Zhang, Stefano Soatto*

- `2105.03048v1` - [abs](http://arxiv.org/abs/2105.03048v1) - [pdf](http://arxiv.org/pdf/2105.03048v1)

> Behavior of deep neural networks can be inconsistent between different versions. Regressions during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain. This work focuses on quantifying, reducing and analyzing regression errors in the NLP model updates. Using negative flip rate as regression measure, we show that regression has a prevalent presence across tasks in the GLUE benchmark. We formulate the regression-free model updates into a constrained optimization problem, and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method. We empirically analyze how model ensemble reduces regression. Finally, we conduct CheckList behavioral testing to understand the distribution of regressions across linguistic phenomena, and the efficacy of ensemble and distillation methods.

</details>

<details>

<summary>2021-05-07 08:10:00 - Error-Correcting Output Codes with Ensemble Diversity for Robust Learning in Neural Networks</summary>

- *Yang Song, Qiyu Kang, Wee Peng Tay*

- `1912.00181v4` - [abs](http://arxiv.org/abs/1912.00181v4) - [pdf](http://arxiv.org/pdf/1912.00181v4)

> Though deep learning has been applied successfully in many scenarios, malicious inputs with human-imperceptible perturbations can make it vulnerable in real applications. This paper proposes an error-correcting neural network (ECNN) that combines a set of binary classifiers to combat adversarial examples in the multi-class classification problem. To build an ECNN, we propose to design a code matrix so that the minimum Hamming distance between any two rows (i.e., two codewords) and the minimum shared information distance between any two columns (i.e., two partitions of class labels) are simultaneously maximized. Maximizing row distances can increase the system fault tolerance while maximizing column distances helps increase the diversity between binary classifiers. We propose an end-to-end training method for our ECNN, which allows further improvement of the diversity between binary classifiers. The end-to-end training renders our proposed ECNN different from the traditional error-correcting output code (ECOC) based methods that train binary classifiers independently. ECNN is complementary to other existing defense approaches such as adversarial training and can be applied in conjunction with them. We empirically demonstrate that our proposed ECNN is effective against the state-of-the-art white-box and black-box attacks on several datasets while maintaining good classification accuracy on normal examples.

</details>

<details>

<summary>2021-05-07 09:10:20 - Code2Image: Intelligent Code Analysis by Computer Vision Techniques and Application to Vulnerability Prediction</summary>

- *Zeki Bilgin*

- `2105.03131v1` - [abs](http://arxiv.org/abs/2105.03131v1) - [pdf](http://arxiv.org/pdf/2105.03131v1)

> Intelligent code analysis has received increasing attention in parallel with the remarkable advances in the field of machine learning (ML) in recent years. A major challenge in leveraging ML for this purpose is to represent source code in a useful form that ML algorithms can accept as input. In this study, we present a novel method to represent source code as image while preserving semantic and syntactic properties, which paves the way for leveraging computer vision techniques to use for code analysis. Indeed the method makes it possible to directly enter the resulting image representation of source codes into deep learning (DL) algorithms as input without requiring any further data pre-processing or feature extraction step. We demonstrate feasibility and effectiveness of our method by realizing a vulnerability prediction use case over a public dataset containing a large number of real-world source code samples with performance evaluation in comparison to the state-of-art solutions. Our implementation is publicly available.

</details>

<details>

<summary>2021-05-07 09:22:09 - argXtract: Deriving IoT Security Configurations via Automated Static Analysis of Stripped ARM Binaries</summary>

- *Pallavi Sivakumaran, Jorge Blasco*

- `2105.03135v1` - [abs](http://arxiv.org/abs/2105.03135v1) - [pdf](http://arxiv.org/pdf/2105.03135v1)

> Recent high-profile attacks on the Internet of Things (IoT) have brought to the forefront the vulnerability of "smart" devices, and have resulted in numerous IoT-focused security analyses. Many of the attacks had weak device configuration as the root cause. One potential source of rich and definitive information about the configuration of an IoT device is the device's firmware. However, firmware analysis is complex and automated firmware analyses have thus far been confined to devices with more traditional operating systems such as Linux or VxWorks. Most IoT peripherals, due to lacking traditional operating systems and implementing a wide variety of communication technologies, have only been the subject of smaller-scale analyses. Peripheral firmware analysis is further complicated by the fact that such firmware files are predominantly available as stripped binaries, without the ELF headers and symbol tables that would simplify reverse engineering.   In this paper, we present argXtract, an open-source automated static analysis tool, which extracts security-relevant configuration information from stripped IoT peripheral firmware. Specifically, we focus on binaries that target the ARM Cortex-M architecture, due to its growing popularity among IoT peripherals. argXtract overcomes the challenges associated with stripped Cortex-M analysis and is able to retrieve arguments to security-relevant supervisor and function calls, enabling automated bulk analysis of firmware files. We demonstrate this via three real-world case studies. The largest case study covers a dataset of 243 Bluetooth Low Energy binaries targeting Nordic Semiconductor chipsets, while the other two focus on Nordic ANT and STMicroelectronics BlueNRG binaries. The results reveal widespread lack of security and privacy controls in IoT, such as minimal or no protection for data, fixed passkeys and trackable device addresses.

</details>

<details>

<summary>2021-05-07 15:57:17 - Detecting Security Fixes in Open-Source Repositories using Static Code Analyzers</summary>

- *Therese Fehrer, Rocío Cabrera Lozoya, Antonino Sabetta, Dario Di Nucci, Damian A. Tamburri*

- `2105.03346v1` - [abs](http://arxiv.org/abs/2105.03346v1) - [pdf](http://arxiv.org/pdf/2105.03346v1)

> The sources of reliable, code-level information about vulnerabilities that affect open-source software (OSS) are scarce, which hinders a broad adoption of advanced tools that provide code-level detection and assessment of vulnerable OSS dependencies.   In this paper, we study the extent to which the output of off-the-shelf static code analyzers can be used as a source of features to represent commits in Machine Learning (ML) applications. In particular, we investigate how such features can be used to construct embeddings and train ML models to automatically identify source code commits that contain vulnerability fixes.   We analyze such embeddings for security-relevant and non-security-relevant commits, and we show that, although in isolation they are not different in a statistically significant manner, it is possible to use them to construct a ML pipeline that achieves results comparable with the state of the art.   We also found that the combination of our method with commit2vec represents a tangible improvement over the state of the art in the automatic identification of commits that fix vulnerabilities: the ML models we construct and commit2vec are complementary, the former being more generally applicable, albeit not as accurate.

</details>

<details>

<summary>2021-05-08 04:47:37 - De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks</summary>

- *Jian Chen, Xuxin Zhang, Rui Zhang, Chen Wang, Ling Liu*

- `2105.03592v1` - [abs](http://arxiv.org/abs/2105.03592v1) - [pdf](http://arxiv.org/pdf/2105.03592v1)

> Machine learning techniques have been widely applied to various applications. However, they are potentially vulnerable to data poisoning attacks, where sophisticated attackers can disrupt the learning procedure by injecting a fraction of malicious samples into the training dataset. Existing defense techniques against poisoning attacks are largely attack-specific: they are designed for one specific type of attacks but do not work for other types, mainly due to the distinct principles they follow. Yet few general defense strategies have been developed. In this paper, we propose De-Pois, an attack-agnostic defense against poisoning attacks. The key idea of De-Pois is to train a mimic model the purpose of which is to imitate the behavior of the target model trained by clean samples. We take advantage of Generative Adversarial Networks (GANs) to facilitate informative training data augmentation as well as the mimic model construction. By comparing the prediction differences between the mimic model and the target model, De-Pois is thus able to distinguish the poisoned samples from clean ones, without explicit knowledge of any ML algorithms or types of poisoning attacks. We implement four types of poisoning attacks and evaluate De-Pois with five typical defense methods on different realistic datasets. The results demonstrate that De-Pois is effective and efficient for detecting poisoned data against all the four types of poisoning attacks, with both the accuracy and F1-score over 0.9 on average.

</details>

<details>

<summary>2021-05-08 09:37:32 - Kubernetes Autoscaling: YoYo Attack Vulnerability and Mitigation</summary>

- *Ronen Ben David, Anat Bremler Barr*

- `2105.00542v2` - [abs](http://arxiv.org/abs/2105.00542v2) - [pdf](http://arxiv.org/pdf/2105.00542v2)

> In recent years, we have witnessed a new kind of DDoS attack, the burst attack(Chai, 2013; Dahan, 2018), where the attacker launches periodic bursts of traffic overload on online targets. Recent work presents a new kind of Burst attack, the YoYo attack (Bremler-Barr et al., 2017) that operates against the auto-scaling mechanism of VMs in the cloud. The periodic bursts of traffic loads cause the auto-scaling mechanism to oscillate between scale-up and scale-down phases. The auto-scaling mechanism translates the flat DDoS attacks into Economic Denial of Sustainability attacks (EDoS), where the victim suffers from economic damage accrued by paying for extra resources required to process the traffic generated by the attacker. However, it was shown that YoYo attack also causes significant performance degradation since it takes time to scale-up VMs. In this research, we analyze the resilience of Kubernetes auto-scaling against YoYo attacks. As containerized cloud applications using Kubernetes gain popularity and replace VM-based architecture in recent years. We present experimental results on Google Cloud Platform, showing that even though the scale-up time of containers is much lower than VM, Kubernetes is still vulnerable to the YoYo attack since VMs are still involved. Finally, we evaluate ML models that can accurately detect YoYo attack on a Kubernetes cluster.

</details>

<details>

<summary>2021-05-09 13:15:10 - Automated Decision-based Adversarial Attacks</summary>

- *Qi-An Fu, Yinpeng Dong, Hang Su, Jun Zhu*

- `2105.03931v1` - [abs](http://arxiv.org/abs/2105.03931v1) - [pdf](http://arxiv.org/pdf/2105.03931v1)

> Deep learning models are vulnerable to adversarial examples, which can fool a target classifier by imposing imperceptible perturbations onto natural examples. In this work, we consider the practical and challenging decision-based black-box adversarial setting, where the attacker can only acquire the final classification labels by querying the target model without access to the model's details. Under this setting, existing works often rely on heuristics and exhibit unsatisfactory performance. To better understand the rationality of these heuristics and the limitations of existing methods, we propose to automatically discover decision-based adversarial attack algorithms. In our approach, we construct a search space using basic mathematical operations as building blocks and develop a random search algorithm to efficiently explore this space by incorporating several pruning techniques and intuitive priors inspired by program synthesis works. Although we use a small and fast model to efficiently evaluate attack algorithms during the search, extensive experiments demonstrate that the discovered algorithms are simple yet query-efficient when transferred to larger normal and defensive models on the CIFAR-10 and ImageNet datasets. They achieve comparable or better performance than the state-of-the-art decision-based attack methods consistently.

</details>

<details>

<summary>2021-05-10 14:22:09 - Why Aren't Regular Expressions a Lingua Franca? An Empirical Study on the Re-use and Portability of Regular Expressions</summary>

- *James C. Davis, Louis G. Michael IV, Christy A. Coghlan, Francisco Servant, Dongyoon Lee*

- `2105.04397v1` - [abs](http://arxiv.org/abs/2105.04397v1) - [pdf](http://arxiv.org/pdf/2105.04397v1)

> This paper explores the extent to which regular expressions (regexes) are portable across programming languages. Many languages offer similar regex syntaxes, and it would be natural to assume that regexes can be ported across language boundaries. But can regexes be copy/pasted across language boundaries while retaining their semantic and performance characteristics?   In our survey of 158 professional software developers, most indicated that they re-use regexes across language boundaries and about half reported that they believe regexes are a universal language. We experimentally evaluated the riskiness of this practice using a novel regex corpus -- 537,806 regexes from 193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and Rust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex portability problems: logic errors due to semantic differences, and security vulnerabilities due to performance differences.   We report that developers' belief in a regex lingua franca is understandable but unfounded. Though most regexes compile across language boundaries, 15% exhibit semantic differences across languages and 10% exhibit performance differences across languages. We explained these differences using regex documentation, and further illuminate our findings by investigating regex engine implementations. Along the way we found bugs in the regex engines of JavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance regex bugs in thousands of modules.

</details>

<details>

<summary>2021-05-10 16:17:44 - APIScanner -- Towards Automated Detection of Deprecated APIs in Python Libraries</summary>

- *Aparna Vadlamani, Rishitha Kalicheti, Sridhar Chimalakonda*

- `2102.09251v4` - [abs](http://arxiv.org/abs/2102.09251v4) - [pdf](http://arxiv.org/pdf/2102.09251v4)

> Python libraries are widely used for machine learning and scientific computing tasks today. APIs in Python libraries are deprecated due to feature enhancements and bug fixes in the same way as in other languages. These deprecated APIs are discouraged from being used in further software development. Manually detecting and replacing deprecated APIs is a tedious and time-consuming task due to the large number of API calls used in the projects. Moreover, the lack of proper documentation for these deprecated APIs makes the task challenging. To address this challenge, we propose an algorithm and a tool APIScanner that automatically detects deprecated APIs in Python libraries. This algorithm parses the source code of the libraries using abstract syntax tree (ASTs) and identifies the deprecated APIs via decorator, hard-coded warning or comments. APIScanner is a Visual Studio Code Extension that highlights and warns the developer on the use of deprecated API elements while writing the source code. The tool can help developers to avoid using deprecated API elements without the execution of code. We tested our algorithm and tool on six popular Python libraries, which detected 838 of 871 deprecated API elements. Demo of APIScanner: https://youtu.be/1hy_ugf-iek. Documentation, tool, and source code can be found here: https://rishitha957.github.io/APIScanner.

</details>

<details>

<summary>2021-05-10 18:18:50 - A Vector Space Approach to Generate Dynamic Keys for Hill Cipher</summary>

- *Sunil Kumar, Sandeep Kumar, Gaurav Mittal, Shiv Narain*

- `1909.06781v4` - [abs](http://arxiv.org/abs/1909.06781v4) - [pdf](http://arxiv.org/pdf/1909.06781v4)

> In this paper, a variant of the Hill cipher is proposed. In the classical Hill cipher, an invertible matrix is used for encryption but the scheme is vulnerable to the known-plaintext attack which can reveal the matrix. In our proposed cryptosystem, each plaintext block is encrypted by a new invertible key matrix that thwarts the known-plaintext attack. To generate the invertible matrices which serve as the dynamic keys we make use of the vector spaces, randomly generated basis and non-singular linear transformation. Resulting cipher is secure against the known-plaintext attack.

</details>

<details>

<summary>2021-05-10 18:31:45 - The challenges and realities of retailing in a COVID-19 world: Identifying trending and Vital During Crisis keywords during Covid-19 using Machine Learning (Austria as a case study)</summary>

- *Reda Mastouri Et Al., Joseph Gilkey*

- `2105.07876v1` - [abs](http://arxiv.org/abs/2105.07876v1) - [pdf](http://arxiv.org/pdf/2105.07876v1)

> From global pandemics to geopolitical turmoil, leaders in logistics, product allocation, procurement and operations are facing increasing difficulty with safeguarding their organizations against supply chain vulnerabilities. It is recommended to opt for forecasting against trending based benchmark because auditing a future forecast puts more focus on seasonality. The forecasting models provide with end-to-end, real time oversight of the entire supply chain, while utilizing predictive analytics and artificial intelligence to identify potential disruptions before they occur. By combining internal and external data points, coming up with an AI-enabled modelling engine can greatly reduce risk by helping retail companies proactively respond to supply and demand variability. This research paper puts focus on creating an ingenious way to tackle the impact of COVID19 on Supply chain, product allocation, trending and seasonality.   Key words: Supply chain, covid-19, forecasting, coronavirus, manufacturing, seasonality, trending, retail.

</details>

<details>

<summary>2021-05-11 02:27:32 - Stable Adversarial Learning under Distributional Shifts</summary>

- *Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun Kuang, Bo Li, Yishi Lin*

- `2006.04414v2` - [abs](http://arxiv.org/abs/2006.04414v2) - [pdf](http://arxiv.org/pdf/2006.04414v2)

> Machine learning algorithms with empirical risk minimization are vulnerable under distributional shifts due to the greedy adoption of all the correlations found in training data. Recently, there are robust learning methods aiming at this problem by minimizing the worst-case risk over an uncertainty set. However, they equally treat all covariates to form the decision sets regardless of the stability of their correlations with the target, resulting in the overwhelmingly large set and low confidence of the learner.In this paper, we propose Stable Adversarial Learning (SAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct differentiated robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradient-based optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of uniformly good performance across unknown distributional shifts.

</details>

<details>

<summary>2021-05-11 03:57:37 - An Innovative Security Strategy using Reactive Web Application Honeypot</summary>

- *Rajat Gupta, Madhu Viswanatham V., Manikandan K*

- `2105.04773v1` - [abs](http://arxiv.org/abs/2105.04773v1) - [pdf](http://arxiv.org/pdf/2105.04773v1)

> Nowadays, web applications have become most prevalent in the industry, and the critical data of most organizations stored using web apps. Hence, web applications a much bigger target for diverse cyber-attacks, which varies from database injections-SQL injection, PHP object injection, template injection, XML external entity injection, unsanitized input attacks- Cross-Site Scripting(XSS), and many more. As mitigation for them, among many proposed solutions, web application honeypots are a much sophisticated and powerful protection mechanism.   In this paper, we propose a low interaction, adaptive, and dynamic web application honeypot that imitates the vulnerabilities through HTTP events. The honeypot is built with SNARE and TANNER; SNARE creates the attack surface and sends the requests to TANNER, which evaluates them and decides how SNARE should respond to the requests. TANNER is an analysis and classification tool, which analyzes and evaluates HTTP requests served by SNARE and to compose the response, it is powered by emulators, which are engines used for the emulation of vulnerabilities.

</details>

<details>

<summary>2021-05-11 12:52:52 - Graph Theory for Metro Traffic Modelling</summary>

- *Bruno Scalzo Dees, Yao Lei Xu, Anthony G. Constantinides, Danilo P. Mandic*

- `2105.04991v1` - [abs](http://arxiv.org/abs/2105.04991v1) - [pdf](http://arxiv.org/pdf/2105.04991v1)

> A unifying graph theoretic framework for the modelling of metro transportation networks is proposed. This is achieved by first introducing a basic graph framework for the modelling of the London underground system from a diffusion law point of view. This forms a basis for the analysis of both station importance and their vulnerability, whereby the concept of graph vertex centrality plays a key role. We next explore k-edge augmentation of a graph topology, and illustrate its usefulness both for improving the network robustness and as a planning tool. Upon establishing the graph theoretic attributes of the underlying graph topology, we proceed to introduce models for processing data on such a metro graph. Commuter movement is shown to obey the Fick's law of diffusion, where the graph Laplacian provides an analytical model for the diffusion process of commuter population dynamics. Finally, we also explore the application of modern deep learning models, such as graph neural networks and hyper-graph neural networks, as general purpose models for the modelling and forecasting of underground data, especially in the context of the morning and evening rush hours. Comprehensive simulations including the passenger in- and out-flows during the morning rush hour in London demonstrates the advantages of the graph models in metro planning and traffic management, a formal mathematical approach with wide economic implications.

</details>

<details>

<summary>2021-05-11 14:02:27 - Learning Dynamic Belief Graphs to Generalize on Text-Based Games</summary>

- *Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L. Hamilton*

- `2002.09127v4` - [abs](http://arxiv.org/abs/2002.09127v4) - [pdf](http://arxiv.org/pdf/2002.09127v4)

> Playing text-based games requires skills in processing natural language and sequential decision making. Achieving human-level performance on text-based games remains an open challenge, and prior research has largely relied on hand-crafted structured representations and heuristics. In this work, we investigate how an agent can plan and generalize in text-based games using graph-structured representations learned end-to-end from raw text. We propose a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics. GATA is trained using a combination of reinforcement and self-supervised learning. Our work demonstrates that the learned graph-based representations help agents converge to better policies than their text-only counterparts and facilitate effective generalization across game configurations. Experiments on 500+ unique games from the TextWorld suite show that our best agent outperforms text-based baselines by an average of 24.2%.

</details>

<details>

<summary>2021-05-11 14:16:04 - Estimating and Improving Fairness with Adversarial Learning</summary>

- *Xiaoxiao Li, Ziteng Cui, Yifan Wu, Lin Gu, Tatsuya Harada*

- `2103.04243v2` - [abs](http://arxiv.org/abs/2103.04243v2) - [pdf](http://arxiv.org/pdf/2103.04243v2)

> Fairness and accountability are two essential pillars for trustworthy Artificial Intelligence (AI) in healthcare. However, the existing AI model may be biased in its decision marking. To tackle this issue, we propose an adversarial multi-task training strategy to simultaneously mitigate and detect bias in the deep learning-based medical image analysis system. Specifically, we propose to add a discrimination module against bias and a critical module that predicts unfairness within the base classification model. We further impose an orthogonality regularization to force the two modules to be independent during training. Hence, we can keep these deep learning tasks distinct from one another, and avoid collapsing them into a singular point on the manifold. Through this adversarial training method, the data from the underprivileged group, which is vulnerable to bias because of attributes such as sex and skin tone, are transferred into a domain that is neutral relative to these attributes. Furthermore, the critical module can predict fairness scores for the data with unknown sensitive attributes. We evaluate our framework on a large-scale public-available skin lesion dataset under various fairness evaluation metrics. The experiments demonstrate the effectiveness of our proposed method for estimating and improving fairness in the deep learning-based medical image analysis system.

</details>

<details>

<summary>2021-05-12 17:27:21 - Adversarial Reinforcement Learning in Dynamic Channel Access and Power Control</summary>

- *Feng Wang, M. Cenk Gursoy, Senem Velipasalar*

- `2105.05817v1` - [abs](http://arxiv.org/abs/2105.05817v1) - [pdf](http://arxiv.org/pdf/2105.05817v1)

> Deep reinforcement learning (DRL) has recently been used to perform efficient resource allocation in wireless communications. In this paper, the vulnerabilities of such DRL agents to adversarial attacks is studied. In particular, we consider multiple DRL agents that perform both dynamic channel access and power control in wireless interference channels. For these victim DRL agents, we design a jammer, which is also a DRL agent. We propose an adversarial jamming attack scheme that utilizes a listening phase and significantly degrades the users' sum rate. Subsequently, we develop an ensemble policy defense strategy against such a jamming attacker by reloading models (saved during retraining) that have minimum transition correlation.

</details>

<details>

<summary>2021-05-12 20:28:30 - Conversational Code Analysis: The Future of Secure Coding</summary>

- *Fitzroy D. Nembhard, Marco M. Carvalho*

- `2105.03502v2` - [abs](http://arxiv.org/abs/2105.03502v2) - [pdf](http://arxiv.org/pdf/2105.03502v2)

> The area of software development and secure coding can benefit significantly from advancements in virtual assistants. Research has shown that many coders neglect security in favor of meeting deadlines. This shortcoming leaves systems vulnerable to attackers. While a plethora of tools are available for programmers to scan their code for vulnerabilities, finding the right tool can be challenging. It is therefore imperative to adopt measures to get programmers to utilize code analysis tools that will help them produce more secure code. This chapter looks at the limitations of existing approaches to secure coding and proposes a methodology that allows programmers to scan and fix vulnerabilities in program code by communicating with virtual assistants on their smart devices. With the ubiquitous move towards virtual assistants, it is important to design systems that are more reliant on voice than on standard point-and-click and keyboard-driven approaches. Consequently, we propose MyCodeAnalyzer, a Google Assistant app and code analysis framework, which was designed to interactively scan program code for vulnerabilities and flaws using voice commands during development. We describe the proposed methodology, implement a prototype, test it on a vulnerable project and present our results.

</details>

<details>

<summary>2021-05-12 20:50:33 - Guardian: symbolic validation of orderliness in SGX enclaves</summary>

- *Pedro Antonino, Wojciech Aleksander Wołoszyn, A. W. Roscoe*

- `2105.05962v1` - [abs](http://arxiv.org/abs/2105.05962v1) - [pdf](http://arxiv.org/pdf/2105.05962v1)

> Modern processors can offer hardware primitives that allow a process to run in isolation. These primitives implement a trusted execution environment (TEE) in which a program can run such that the integrity and confidentiality of its execution are guaranteed. Intel's Software Guard eXtensions (SGX) is an example of such primitives and its isolated processes are called \emph{enclaves}. These guarantees, however, can be easily thwarted if the enclave has not been properly designed. Its interface with the untrusted software stack is arguably the largest attack surface that adversaries can exploit; unintended interactions with untrusted code can expose the enclave to memory corruption attacks, for instance. In this paper, we propose a notion of an \emph{orderly} enclave which splits its behaviour into several execution phases each of which imposes a set of restrictions on accesses to untrusted memory, phase transitions and registers sanitisation. A violation to these restrictions indicates an undesired behaviour which could be harnessed to perpetrate attacks against the enclave. We also introduce \Analyser{}: a tool that uses symbolic execution to carry out the validation of an enclave against our notion of an orderly enclave; in this process, it also looks for some typical memory-corruption vulnerabilities. We discuss how our approach can prevent and flag enclave vulnerabilities that have been identified in the literature. Moreover, we have evaluated how our approach fares in the analysis of some practical enclaves. \Analyser{} was able to identify real vulnerabilities on these enclaves which have been acknowledged and fixed by their maintainers.

</details>

<details>

<summary>2021-05-13 06:51:51 - Trusted Authentication using hybrid security algorithm in VANET</summary>

- *Prasanna Venkatesan E, Kristen Titus W*

- `2105.06105v1` - [abs](http://arxiv.org/abs/2105.06105v1) - [pdf](http://arxiv.org/pdf/2105.06105v1)

> Vehicular Ad Hoc Networks (VANETs) improves traffic management and reduce the amount of road accidents by providing safety applications. However, VANETs are vulnerable to variety of security attacks from malicious entities. An authentication is an integral a neighborhood of trust establishment and secure communications between vehicles. The Road-side Unit (RSU) evaluates trust-value and the Agent Trusted Authority (ATA) helps in computing the trust-value of auto supported its reward-points. The communication between nodes is enhanced, this can reduce 50% of road accidents. The security of the VANET is improved. We propose the utilization of Elliptic Curve Cryptography in the design of an efficient data encryption/decryption system for sensor nodes in a wireless network. Elliptic Curve Cryptography can provide impressive levels of security standards while keeping down the cost of certain issues, primarily storage space. Sensors will benefit from having to store relatively smaller keys coupled with increased computational capability and this will be a stronger design as the bit-level security is improved. Thus, reducing the time delay between the nodes and to provide better results between them we have made use of this method. The implementation of this work is done with NS2 software.

</details>

<details>

<summary>2021-05-13 07:52:00 - High-level Intellectual Property Obfuscation via Decoy Constants</summary>

- *Levent Aksoy, Quang-Linh Nguyen, Felipe Almeida, Jaan Raik, Marie-Lise Flottes, Sophie Dupuis, Samuel Pagliarini*

- `2105.06122v1` - [abs](http://arxiv.org/abs/2105.06122v1) - [pdf](http://arxiv.org/pdf/2105.06122v1)

> This paper presents a high-level circuit obfuscation technique to prevent the theft of intellectual property (IP) of integrated circuits. In particular, our technique protects a class of circuits that relies on constant multiplications, such as filters and neural networks, where the constants themselves are the IP to be protected. By making use of decoy constants and a key-based scheme, a reverse engineer adversary at an untrusted foundry is rendered incapable of discerning true constants from decoy constants. The time-multiplexed constant multiplication (TMCM) block of such circuits, which realizes the multiplication of an input variable by a constant at a time, is considered as our case study for obfuscation. Furthermore, two TMCM design architectures are taken into account; an implementation using a multiplier and a multiplierless shift-adds implementation. Optimization methods are also applied to reduce the hardware complexity of these architectures. The well-known satisfiability (SAT) and automatic test pattern generation (ATPG) attacks are used to determine the vulnerability of the obfuscated designs. It is observed that the proposed technique incurs small overheads in area, power, and delay that are comparable to the hardware complexity of prominent logic locking methods. Yet, the advantage of our approach is in the insight that constants -- instead of arbitrary circuit nodes -- become key-protected.

</details>

<details>

<summary>2021-05-13 14:20:55 - Coverage Guided Testing for Recurrent Neural Networks</summary>

- *Wei Huang, Youcheng Sun, Xingyu Zhao, James Sharp, Wenjie Ruan, Jie Meng, Xiaowei Huang*

- `1911.01952v3` - [abs](http://arxiv.org/abs/1911.01952v3) - [pdf](http://arxiv.org/pdf/1911.01952v3)

> Recurrent neural networks (RNNs) have been applied to a broad range of applications, including natural language processing, drug discovery, and video recognition. Their vulnerability to input perturbation is also known. Aligning with a view from software defect detection, this paper aims to develop a coverage guided testing approach to systematically exploit the internal behaviour of RNNs, with the expectation that such testing can detect defects with high possibility. Technically, the long short term memory network (LSTM), a major class of RNNs, is thoroughly studied. A family of three test metrics are designed to quantify not only the values but also the temporal relations (including both step-wise and bounded-length) exhibited when LSTM processing inputs. A genetic algorithm is applied to efficiently generate test cases. The test metrics and test case generation algorithm are implemented into a tool TestRNN, which is then evaluated on a set of LSTM benchmarks. Experiments confirm that TestRNN has advantages over the state-of-art tool DeepStellar and attack-based defect detection methods, owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation. Furthermore, TestRNN enables meaningful information to be collected and exhibited for users to understand the testing results, which is an important step towards interpretable neural network testing.

</details>

<details>

<summary>2021-05-13 16:27:08 - Comprehensive Study of Security and Privacy of Emerging Non-Volatile Memories</summary>

- *Mohammad Nasim Imtiaz Khan, Swaroop Ghosh*

- `2105.06401v1` - [abs](http://arxiv.org/abs/2105.06401v1) - [pdf](http://arxiv.org/pdf/2105.06401v1)

> At the end of Silicon roadmap, keeping the leakage power in tolerable limit and bridging the bandwidth gap between processor and memory have become some of the biggest challenges. Several promising Non-Volatile Memories (NVMs) such as, Spin-Transfer Torque RAM (STTRAM), Magnetic RAM (MRAM), Phase Change Memory (PCM), Resistive RAM (RRAM) and Ferroelectric RAM (FeRAM) are being investigated to address the above issues since they offer high density and consumes zero leakage power. On one hand, the desirable properties of emerging NVMs make them suitable candidates for several applications including replacement of conventional memories. On the other hand, their unique characteristics such as, high and asymmetric read/write current and persistence bring new threats to data security and privacy. Some of these memories are already deployed in full systems and as discrete chips and are believed to become ubiquitous in future computing devices. Therefore, it is of utmost important to investigate their security and privacy issues. Note that these NVMs can be considered for cache, main memory or storage application. They are also suitable to implement in-memory computation which increases system throughput and eliminates Von-Neumann Bottleneck. Compute-capable NVMs impose new security and privacy challenges that are fundamentally different than their storage counterpart. This work identifies NVM vulnerabilities, attack vectors originating from device level all the way to circuits and systems considering both storage and compute applications. We also summarize the circuit/system level countermeasures to make the NVMs robust against security and privacy issues.

</details>

<details>

<summary>2021-05-13 18:59:15 - Stochastic-Shield: A Probabilistic Approach Towards Training-Free Adversarial Defense in Quantized CNNs</summary>

- *Lorena Qendro, Sangwon Ha, René de Jong, Partha Maji*

- `2105.06512v1` - [abs](http://arxiv.org/abs/2105.06512v1) - [pdf](http://arxiv.org/pdf/2105.06512v1)

> Quantized neural networks (NN) are the common standard to efficiently deploy deep learning models on tiny hardware platforms. However, we notice that quantized NNs are as vulnerable to adversarial attacks as the full-precision models. With the proliferation of neural networks on small devices that we carry or surround us, there is a need for efficient models without sacrificing trust in the prediction in presence of malign perturbations. Current mitigation approaches often need adversarial training or are bypassed when the strength of adversarial examples is increased.   In this work, we investigate how a probabilistic framework would assist in overcoming the aforementioned limitations for quantized deep learning models. We explore Stochastic-Shield: a flexible defense mechanism that leverages input filtering and a probabilistic deep learning approach materialized via Monte Carlo Dropout. We show that it is possible to jointly achieve efficiency and robustness by accurately enabling each module without the burden of re-retraining or ad hoc fine-tuning.

</details>

<details>

<summary>2021-05-14 01:44:55 - Consumer, Commercial and Industrial IoT (In)Security: Attack Taxonomy and Case Studies</summary>

- *Christos Xenofontos, Ioannis Zografopoulos, Charalambos Konstantinou, Alireza Jolfaei, Muhammad Khurram Khan, Kim-Kwang Raymond Choo*

- `2105.06612v1` - [abs](http://arxiv.org/abs/2105.06612v1) - [pdf](http://arxiv.org/pdf/2105.06612v1)

> Internet of Things (IoT) devices are becoming ubiquitous in our lives, with applications spanning from the consumer domain to commercial and industrial systems. The steep growth and vast adoption of IoT devices reinforce the importance of sound and robust cybersecurity practices during the device development life-cycles. IoT-related vulnerabilities, if successfully exploited can affect, not only the device itself, but also the application field in which the IoT device operates. Evidently, identifying and addressing every single vulnerability is an arduous, if not impossible, task. Attack taxonomies can assist in classifying attacks and their corresponding vulnerabilities. Security countermeasures and best practices can then be leveraged to mitigate threats and vulnerabilities before they emerge into catastrophic attacks and ensure overall secure IoT operation. Therefore, in this paper, we provide an attack taxonomy which takes into consideration the different layers of IoT stack, i.e., device, infrastructure, communication, and service, and each layer's designated characteristics which can be exploited by adversaries. Furthermore, using nine real-world cybersecurity incidents, that had targeted IoT devices deployed in the consumer, commercial, and industrial sectors, we describe the IoT-related vulnerabilities, exploitation procedures, attacks, impacts, and potential mitigation mechanisms and protection strategies. These (and many other) incidents highlight the underlying security concerns of IoT systems and demonstrate the potential attack impacts of such connected ecosystems, while the proposed taxonomy provides a systematic procedure to categorize attacks based on the affected layer and corresponding impact.

</details>

<details>

<summary>2021-05-14 12:56:06 - Salient Feature Extractor for Adversarial Defense on Deep Neural Networks</summary>

- *Jinyin Chen, Ruoxi Chen, Haibin Zheng, Zhaoyan Ming, Wenrong Jiang, Chen Cui*

- `2105.06807v1` - [abs](http://arxiv.org/abs/2105.06807v1) - [pdf](http://arxiv.org/pdf/2105.06807v1)

> Recent years have witnessed unprecedented success achieved by deep learning models in the field of computer vision. However, their vulnerability towards carefully crafted adversarial examples has also attracted the increasing attention of researchers. Motivated by the observation that adversarial examples are due to the non-robust feature learned from the original dataset by models, we propose the concepts of salient feature(SF) and trivial feature(TF). The former represents the class-related feature, while the latter is usually adopted to mislead the model. We extract these two features with coupled generative adversarial network model and put forward a novel detection and defense method named salient feature extractor (SFE) to defend against adversarial attacks. Concretely, detection is realized by separating and comparing the difference between SF and TF of the input. At the same time, correct labels are obtained by re-identifying SF to reach the purpose of defense. Extensive experiments are carried out on MNIST, CIFAR-10, and ImageNet datasets where SFE shows state-of-the-art results in effectiveness and efficiency compared with baselines. Furthermore, we provide an interpretable understanding of the defense and detection process.

</details>

<details>

<summary>2021-05-14 17:24:34 - A Survey of Security Vulnerabilities in Ethereum Smart Contracts</summary>

- *Noama Fatima Samreen, Manar H. Alalfi*

- `2105.06974v1` - [abs](http://arxiv.org/abs/2105.06974v1) - [pdf](http://arxiv.org/pdf/2105.06974v1)

> Ethereum Smart Contracts based on Blockchain Technology (BT)enables monetary transactions among peers on a blockchain network independent of a central authorizing agency. Ethereum smart contracts are programs that are deployed as decentralized applications, having the building blocks of the blockchain consensus protocol. This enables consumers to make agreements in a transparent and conflict-free environment. However, there exist some security vulnerabilities within these smart contracts that are a potential threat to the applications and their consumers and have shown in the past to cause huge financial losses. In this study, we review the existing literature and broadly classify the BT applications. As Ethereum smart contracts find their application mostly in e-commerce applications, we believe these are more commonly vulnerable to attacks. In these smart contracts, we mainly focus on identifying vulnerabilities that programmers and users of smart contracts must avoid. This paper aims at explaining eight vulnerabilities that are specific to the application level of BT by analyzing the past exploitation case scenarios of these security vulnerabilities. We also review some of the available tools and applications that detect these vulnerabilities in terms of their approach and effectiveness. We also investigated the availability of detection tools for identifying these security vulnerabilities and lack thereof to identify some of them

</details>

<details>

<summary>2021-05-14 18:58:44 - Information Theoretic Key Agreement Protocol based on ECG signals</summary>

- *Anna V. Guglielmi, Alberto Muraro, Giulia Cisotto, Nicola Laurenti*

- `2105.07037v1` - [abs](http://arxiv.org/abs/2105.07037v1) - [pdf](http://arxiv.org/pdf/2105.07037v1)

> Wireless body area networks (WBANs) are becoming increasingly popular as they allow individuals to continuously monitor their vitals and physiological parameters remotely from the hospital. With the spread of the SARS-CoV-2 pandemic, the availability of portable pulse-oximeters and wearable heart rate detectors has boomed in the market. At the same time, in 2020 we assisted to an unprecedented increase of healthcare breaches, revealing the extreme vulnerability of the current generation of WBANs. Therefore, the development of new security protocols to ensure data protection, authentication, integrity and privacy within WBANs are highly needed. Here, we targeted a WBAN collecting ECG signals from different sensor nodes on the individual's body, we extracted the inter-pulse interval (i.e., R-R interval) sequence from each of them, and we developed a new information theoretic key agreement protocol that exploits the inherent randomness of ECG to ensure authentication between sensor pairs within the WBAN. After proper pre-processing, we provide an analytical solution that ensures robust authentication; we provide a unique information reconciliation matrix, which gives good performance for all ECG sensor pairs; and we can show that a relationship between information reconciliation and privacy amplification matrices can be found. Finally, we show the trade-off between the level of security, in terms of key generation rate, and the complexity of the error correction scheme implemented in the system.

</details>

<details>

<summary>2021-05-16 06:37:27 - Oriole: Thwarting Privacy against Trustworthy Deep Learning Models</summary>

- *Liuqiao Chen, Hu Wang, Benjamin Zi Hao Zhao, Minhui Xue, Haifeng Qian*

- `2102.11502v2` - [abs](http://arxiv.org/abs/2102.11502v2) - [pdf](http://arxiv.org/pdf/2102.11502v2)

> Deep Neural Networks have achieved unprecedented success in the field of face recognition such that any individual can crawl the data of others from the Internet without their explicit permission for the purpose of training high-precision face recognition models, creating a serious violation of privacy. Recently, a well-known system named Fawkes (published in USENIX Security 2020) claimed this privacy threat can be neutralized by uploading cloaked user images instead of their original images. In this paper, we present Oriole, a system that combines the advantages of data poisoning attacks and evasion attacks, to thwart the protection offered by Fawkes, by training the attacker face recognition model with multi-cloaked images generated by Oriole. Consequently, the face recognition accuracy of the attack model is maintained and the weaknesses of Fawkes are revealed. Experimental results show that our proposed Oriole system is able to effectively interfere with the performance of the Fawkes system to achieve promising attacking results. Our ablation study highlights multiple principal factors that affect the performance of the Oriole system, including the DSSIM perturbation budget, the ratio of leaked clean user images, and the numbers of multi-cloaks for each uncloaked image. We also identify and discuss at length the vulnerabilities of Fawkes. We hope that the new methodology presented in this paper will inform the security community of a need to design more robust privacy-preserving deep learning models.

</details>

<details>

<summary>2021-05-16 19:35:16 - Analysis of Bitcoin Vulnerability to Bribery Attacks Launched Through Large Transactions</summary>

- *Ghader Ebrahimpour, Mohammad Sayad Haghighi*

- `2105.07501v1` - [abs](http://arxiv.org/abs/2105.07501v1) - [pdf](http://arxiv.org/pdf/2105.07501v1)

> Bitcoin uses blockchain technology to maintain transactions order and provides probabilistic guarantee to prevent double-spending, assuming that an attacker's computational power does not exceed %50 of the network power. In this paper, we design a novel bribery attack and show that this guarantee can be hugely undermined. Miners are assumed to be rational in this setup and they are given incentives that are dynamically calculated. In this attack, the adversary misuses the Bitcoin protocol to bribe miners and maximize their gained advantage. We will reformulate the bribery attack to propose a general mathematical foundation upon which we build multiple strategies. We show that, unlike Whale Attack, these strategies are practical. If the rationality assumption holds, this shows how vulnerable blockchain-based systems like Bitcoin are. We suggest a soft fork on Bitcoin to fix this issue at the end.

</details>

<details>

<summary>2021-05-16 22:05:48 - Improving Vulnerability Prediction of JavaScript Functions Using Process Metrics</summary>

- *Tamás Viszkok, Péter Hegedűs, Rudolf Ferenc*

- `2105.07527v1` - [abs](http://arxiv.org/abs/2105.07527v1) - [pdf](http://arxiv.org/pdf/2105.07527v1)

> Due to the growing number of cyber attacks against computer systems, we need to pay special attention to the security of our software systems. In order to maximize the effectiveness, excluding the human component from this process would be a huge breakthrough. The first step towards this is to automatically recognize the vulnerable parts in our code. Researchers put a lot of effort into creating machine learning models that could determine if a given piece of code, or to be more precise, a selected function, contains any vulnerabilities or not. We aim at improving the existing models, building on previous results in predicting vulnerabilities at the level of functions in JavaScript code using the well-known static source code metrics. In this work, we propose to include several so-called process metrics (e.g., code churn, number of developers modifying a file, or the age of the changed source code) into the set of features, and examine how they affect the performance of the function-level JavaScript vulnerability prediction models. We can confirm that process metrics significantly improve the prediction power of such models. On average, we observed a 8.4% improvement in terms of F-measure (from 0.764 to 0.848), 3.5% improvement in terms of precision (from 0.953 to 0.988) and a 6.3% improvement in terms of recall (from 0.697 to 0.760).

</details>

<details>

<summary>2021-05-17 10:05:08 - Hash-MAC-DSDV: Mutual Authentication for Intelligent IoT-Based Cyber-Physical Systems</summary>

- *Muhammad Adil, Mian Ahmad Jan, Spyridon Mastorakis, Houbing Song, Muhammad Mohsin Jadoon, Safia Abbas, Ahmed Farouk*

- `2105.07711v1` - [abs](http://arxiv.org/abs/2105.07711v1) - [pdf](http://arxiv.org/pdf/2105.07711v1)

> Cyber-Physical Systems (CPS) connected in the form of Internet of Things (IoT) are vulnerable to various security threats, due to the infrastructure-less deployment of IoT devices. Device-to-Device (D2D) authentication of these networks ensures the integrity, authenticity, and confidentiality of information in the deployed area. The literature suggests different approaches to address security issues in CPS technologies. However, they are mostly based on centralized techniques or specific system deployments with higher cost of computation and communication. It is therefore necessary to develop an effective scheme that can resolve the security problems in CPS technologies of IoT devices. In this paper, a lightweight Hash-MAC-DSDV (Hash Media Access Control Destination Sequence Distance Vector) routing scheme is proposed to resolve authentication issues in CPS technologies, connected in the form of IoT networks. For this purpose, a CPS of IoT devices (multi-WSNs) is developed from the local-chain and public chain, respectively. The proposed scheme ensures D2D authentication by the Hash-MAC-DSDV mutual scheme, where the MAC addresses of individual devices are registered in the first phase and advertised in the network in the second phase. The proposed scheme allows legitimate devices to modify their routing table and unicast the one-way hash authentication mechanism to transfer their captured data from source towards the destination. Our evaluation results demonstrate that Hash- MAC-DSDV outweighs the existing schemes in terms of attack detection, energy consumption and communication metrics.

</details>

<details>

<summary>2021-05-17 10:40:49 - Microservices in IoT Security: Current Solutions, Research Challenges, and Future Directions</summary>

- *Maha Driss, Daniah Hasan, Wadii Boulila, Jawad Ahmad*

- `2105.07722v1` - [abs](http://arxiv.org/abs/2105.07722v1) - [pdf](http://arxiv.org/pdf/2105.07722v1)

> In recent years, the Internet of Things (IoT) technology has led to the emergence of multiple smart applications in different vital sectors including healthcare, education, agriculture, energy management, etc. IoT aims to interconnect several intelligent devices over the Internet such as sensors, monitoring systems, and smart appliances to control, store, exchange, and analyze collected data. The main issue in IoT environments is that they can present potential vulnerabilities to be illegally accessed by malicious users, which threatens the safety and privacy of gathered data. To face this problem, several recent works have been conducted using microservices-based architecture to minimize the security threats and attacks related to IoT data. By employing microservices, these works offer extensible, reusable, and reconfigurable security features. In this paper, we aim to provide a survey about microservices-based approaches for securing IoT applications. This survey will help practitioners understand ongoing challenges and explore new and promising research opportunities in the IoT security field. To the best of our knowledge, this paper constitutes the first survey that investigates the use of microservices technology for securing IoT applications.

</details>

<details>

<summary>2021-05-17 16:10:54 - Gradient Masking and the Underestimated Robustness Threats of Differential Privacy in Deep Learning</summary>

- *Franziska Boenisch, Philip Sperl, Konstantin Böttinger*

- `2105.07985v1` - [abs](http://arxiv.org/abs/2105.07985v1) - [pdf](http://arxiv.org/pdf/2105.07985v1)

> An important problem in deep learning is the privacy and security of neural networks (NNs). Both aspects have long been considered separately. To date, it is still poorly understood how privacy enhancing training affects the robustness of NNs. This paper experimentally evaluates the impact of training with Differential Privacy (DP), a standard method for privacy preservation, on model vulnerability against a broad range of adversarial attacks. The results suggest that private models are less robust than their non-private counterparts, and that adversarial examples transfer better among DP models than between non-private and private ones. Furthermore, detailed analyses of DP and non-DP models suggest significant differences between their gradients. Additionally, this work is the first to observe that an unfavorable choice of parameters in DP training can lead to gradient masking, and, thereby, results in a wrong sense of security.

</details>

<details>

<summary>2021-05-17 23:37:29 - In Search of Socio-Technical Congruence: A Large-Scale Longitudinal Study</summary>

- *Wolfgang Mauerer, Mitchell Joblin, Damian A. Tamburri, Carlos Paradis, Rick Kazman, Sven Apel*

- `2105.08198v1` - [abs](http://arxiv.org/abs/2105.08198v1) - [pdf](http://arxiv.org/pdf/2105.08198v1)

> We report on a large-scale empirical study investigating the relevance of socio-technical congruence over key basic software quality metrics, namely, bugs and churn. In particular, we explore whether alignment or misalignment of social communication structures and technical dependencies in large software projects influences software quality. To this end, we have defined a quantitative and operational notion of socio-technical congruence, which we call socio-technical motif congruence (STMC). STMC is a measure of the degree to which developers working on the same file or on two related files, need to communicate. As socio-technical congruence is a complex and multi-faceted phenomenon, the interpretability of the results is one of our main concerns, so we have employed a careful mixed-methods statistical analysis. In particular, we provide analyses with similar techniques as employed by seminal work in the field to ensure comparability of our results with the existing body of work. The major result of our study, based on an analysis of 25 large open-source projects, is that STMC is not related to project quality measures -- software bugs and churn -- in any temporal scenario. That is, we find no statistical relationship between the alignment of developer tasks and developer communications on the one hand, and project outcomes on the other hand. We conclude that, wherefore congruence does matter as literature shows, then its measurable effect lies elsewhere.

</details>

<details>

<summary>2021-05-18 17:52:16 - HeapSafe: Securing Unprotected Heaps in RISC-V</summary>

- *Asmit De, Swaroop Ghosh*

- `2105.08712v1` - [abs](http://arxiv.org/abs/2105.08712v1) - [pdf](http://arxiv.org/pdf/2105.08712v1)

> RISC-V is a promising open-source architecture primarily targeted for embedded systems. Programs compiled using the RISC-V toolchain can run bare-metal on the system, and, as such, can be vulnerable to several memory corruption vulnerabilities. In this work, we present HeapSafe, a lightweight hardware assisted heap-buffer protection scheme to mitigate heap overflow and use-after-free vulnerabilities in a RISC-V SoC. The proposed scheme tags pointers associated with heap buffers with metadata indices and enforces tag propagation for commonly used pointer operations. The HeapSafe hardware is decoupled from the core and is designed as a configurable coprocessor and is responsible for validating the heap buffer accesses. Benchmark results show a 1.5X performance overhead and 1.59% area overhead, while being 22% faster than a software protection. We further implemented a HeapSafe-nb, an asynchronous validation design, which improves performance by 27% over the synchronous HeapSafe.

</details>

<details>

<summary>2021-05-18 21:13:24 - Robust Machine Learning via Privacy/Rate-Distortion Theory</summary>

- *Ye Wang, Shuchin Aeron, Adnan Siraj Rakin, Toshiaki Koike-Akino, Pierre Moulin*

- `2007.11693v2` - [abs](http://arxiv.org/abs/2007.11693v2) - [pdf](http://arxiv.org/pdf/2007.11693v2)

> Robust machine learning formulations have emerged to address the prevalent vulnerability of deep neural networks to adversarial examples. Our work draws the connection between optimal robust learning and the privacy-utility tradeoff problem, which is a generalization of the rate-distortion problem. The saddle point of the game between a robust classifier and an adversarial perturbation can be found via the solution of a maximum conditional entropy problem. This information-theoretic perspective sheds light on the fundamental tradeoff between robustness and clean data performance, which ultimately arises from the geometric structure of the underlying data distribution and perturbation constraints.

</details>

<details>

<summary>2021-05-19 13:38:09 - Towards Automating Code Review Activities</summary>

- *Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, Gabriele Bavota*

- `2101.02518v4` - [abs](http://arxiv.org/abs/2101.02518v4) - [pdf](http://arxiv.org/pdf/2101.02518v4)

> Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, since code review is a manual activity it comes at the cost of spending developers' time on reviewing their teammates' code.   Our goal is to make the first step towards partially automating the code review process, thus, possibly reducing the manual costs associated with it. We focus on both the contributor and the reviewer sides of the process, by training two different Deep Learning architectures. The first one learns code changes performed by developers during real code review activities, thus providing the contributor with a revised version of her code implementing code transformations usually recommended during code review before the code is even submitted for review. The second one automatically provides the reviewer commenting on a submitted code with the revised code implementing her comments expressed in natural language.   The empirical evaluation of the two models shows that, on the contributor side, the trained model succeeds in replicating the code transformations applied during code reviews in up to 16% of cases. On the reviewer side, the model can correctly implement a comment provided in natural language in up to 31% of cases. While these results are encouraging, more research is needed to make these models usable by developers.

</details>

<details>

<summary>2021-05-19 16:04:20 - Joint Calibrationless Reconstruction and Segmentation of Parallel MRI</summary>

- *Aniket Pramanik, Xiaodong Wu, Mathews Jacob*

- `2105.09220v1` - [abs](http://arxiv.org/abs/2105.09220v1) - [pdf](http://arxiv.org/pdf/2105.09220v1)

> The volume estimation of brain regions from MRI data is a key problem in many clinical applications, where the acquisition of data at high spatial resolution is desirable. While parallel MRI and constrained image reconstruction algorithms can accelerate the scans, image reconstruction artifacts are inevitable, especially at high acceleration factors. We introduce a novel image domain deep-learning framework for calibrationless parallel MRI reconstruction, coupled with a segmentation network to improve image quality and to reduce the vulnerability of current segmentation algorithms to image artifacts resulting from acceleration. The combination of the proposed image domain deep calibrationless approach with the segmentation algorithm offers improved image quality, while increasing the accuracy of the segmentations. The novel architecture with an encoder shared between the reconstruction and segmentation tasks is seen to reduce the need for segmented training datasets. In particular, the proposed few-shot training strategy requires only 10% of segmented datasets to offer good performance.

</details>

<details>

<summary>2021-05-19 18:40:16 - DeepDebug: Fixing Python Bugs Using Stack Traces, Backtranslation, and Code Skeletons</summary>

- *Dawn Drain, Colin B. Clement, Guillermo Serrato, Neel Sundaresan*

- `2105.09352v1` - [abs](http://arxiv.org/abs/2105.09352v1) - [pdf](http://arxiv.org/pdf/2105.09352v1)

> The joint task of bug localization and program repair is an integral part of the software development process. In this work we present DeepDebug, an approach to automated debugging using large, pretrained transformers. We begin by training a bug-creation model on reversed commit data for the purpose of generating synthetic bugs. We apply these synthetic bugs toward two ends. First, we directly train a backtranslation model on all functions from 200K repositories. Next, we focus on 10K repositories for which we can execute tests, and create buggy versions of all functions in those repositories that are covered by passing tests. This provides us with rich debugging information such as stack traces and print statements, which we use to finetune our model which was pretrained on raw source code. Finally, we strengthen all our models by expanding the context window beyond the buggy function itself, and adding a skeleton consisting of that function's parent class, imports, signatures, docstrings, and method bodies, in order of priority. On the QuixBugs benchmark, we increase the total number of fixes found by over 50%, while also decreasing the false positive rate from 35% to 5% and decreasing the timeout from six hours to one minute. On our own benchmark of executable tests, our model fixes 68% of all bugs on its first attempt without using traces, and after adding traces it fixes 75% on first attempt. We will open-source our framework and validation set for evaluating on executable tests.

</details>

<details>

<summary>2021-05-20 01:59:54 - DeepStrike: Remotely-Guided Fault Injection Attacks on DNN Accelerator in Cloud-FPGA</summary>

- *Yukui Luo, Cheng Gongye, Yunsi Fei, Xiaolin Xu*

- `2105.09453v1` - [abs](http://arxiv.org/abs/2105.09453v1) - [pdf](http://arxiv.org/pdf/2105.09453v1)

> As Field-programmable gate arrays (FPGAs) are widely adopted in clouds to accelerate Deep Neural Networks (DNN), such virtualization environments have posed many new security issues. This work investigates the integrity of DNN FPGA accelerators in clouds. It proposes DeepStrike, a remotely-guided attack based on power glitching fault injections targeting DNN execution. We characterize the vulnerabilities of different DNN layers against fault injections on FPGAs and leverage time-to-digital converter (TDC) sensors to precisely control the timing of fault injections. Experimental results show that our proposed attack can successfully disrupt the FPGA DSP kernel and misclassify the target victim DNN application.

</details>

<details>

<summary>2021-05-20 04:03:08 - SmartScan: An approach to detect Denial of Service Vulnerability in Ethereum Smart Contracts</summary>

- *Noama Fatima Samreen, Manar H. Alalfi*

- `2105.02852v3` - [abs](http://arxiv.org/abs/2105.02852v3) - [pdf](http://arxiv.org/pdf/2105.02852v3)

> Blockchain technology (BT) Ethereum Smart Contracts allows programmable transactions that involve the transfer of monetary assets among peers on a BT network independent of a central authorizing agency. Ethereum Smart Contracts are programs that are deployed as decentralized applications, having the building blocks of the blockchain consensus protocol. This technology enables consumers to make agreements in a transparent and conflict-free environment. However, the security vulnerabilities within these smart contracts are a potential threat to the applications and their consumers and have shown in the past to cause huge financial losses. In this paper, we propose a framework that combines static and dynamic analysis to detect Denial of Service (DoS) vulnerability due to an unexpected revert in Ethereum Smart Contracts. Our framework, SmartScan, statically scans smart contracts under test (SCUTs) to identify patterns that are potentially vulnerable in these SCUTs and then uses dynamic analysis to precisely confirm their exploitability of the DoS-Unexpected Revert vulnerability, thus achieving increased performance and more precise results. We evaluated SmartScan on a set of 500 smart contracts collected from the Etherscan. Our approach shows an improvement in precision and recall when compared to available state-of-the-art techniques.

</details>

<details>

<summary>2021-05-20 08:34:20 - KotlinDetector: Towards Understanding the Implications of Using Kotlin in Android Applications</summary>

- *Fadi Mohsen, Loran Oosterhaven, Fatih Turkmen*

- `2105.09591v1` - [abs](http://arxiv.org/abs/2105.09591v1) - [pdf](http://arxiv.org/pdf/2105.09591v1)

> Java programming language has been long used to develop native Android mobile applications. In the last few years many companies and freelancers have switched into using Kotlin partially or entirely. As such, many projects are released as binaries and employ a mix of Java and Kotlin language constructs. Yet, the true security and privacy implications of this shift have not been thoroughly studied. In this work, a state-of-the-art tool, KotlinDetector, is developed to directly extract any Kotlin presence, percentages, and numerous language features from Android Application Packages (APKs) by performing heuristic pattern scanning and invocation tracing. Our evaluation study shows that the tool is considerably efficient and accurate. We further provide a use case in which the output of the KotlinDetector is combined with the output of an existing vulnerability scanner tool called AndroBugs to infer any security and/or privacy implications.

</details>

<details>

<summary>2021-05-21 02:58:46 - Design and Prototype Implementation of a Blockchain-Enabled LoRa System With Edge Computing</summary>

- *Lu Hou, Kan Zheng, Zhiming Liu, Xiaojun Xu*

- `2105.10103v1` - [abs](http://arxiv.org/abs/2105.10103v1) - [pdf](http://arxiv.org/pdf/2105.10103v1)

> Efficiency and security have become critical issues during the development of the long-range (LoRa) system for Internet-of-Things (IoT) applications. The centralized work method in the LoRa system, where all packages are processed and kept in the central cloud, cannot well exploit the resources in LoRa gateways and also makes it vulnerable to security risks, such as data falsification or data loss. On the other hand, the blockchain has the potential to provide a decentralized and secure infrastructure for the LoRa system. However, there are significant challenges in deploying blockchain at LoRa gateways with limited edge computing abilities. This article proposes a design and implementation of the blockchain-enabled LoRa system with edge computing by using the open-source Hyperledger Fabric, which is called as HyperLoRa. According to different features of LoRa data, a blockchain network with multiple ledgers is designed, each of which stores a specific kind of LoRa data. LoRa gateways can participate in the operations of the blockchain and share the ledger that keep the time-critical network data with small size. Then, the edge computing abilities of LoRa gateways are utilized to handle the join procedure and application packages processing. Furthermore, a HyperLoRa prototype is implemented on embedded hardware, which demonstrates the feasibility of deploying the blockchain into LoRa gateways with limited computing and storage resources. Finally, various experiments are conducted to evaluate the performances of the proposed LoRa system.

</details>

<details>

<summary>2021-05-21 03:41:10 - TestRank: Bringing Order into Unlabeled Test Instances for Deep Learning Tasks</summary>

- *Yu Li, Min Li, Qiuxia Lai, Yannan Liu, Qiang Xu*

- `2105.10113v1` - [abs](http://arxiv.org/abs/2105.10113v1) - [pdf](http://arxiv.org/pdf/2105.10113v1)

> Deep learning (DL) has achieved unprecedented success in a variety of tasks. However, DL systems are notoriously difficult to test and debug due to the lack of explainability of DL models and the huge test input space to cover. Generally speaking, it is relatively easy to collect a massive amount of test data, but the labeling cost can be quite high. Consequently, it is essential to conduct test selection and label only those selected "high quality" bug-revealing test inputs for test cost reduction.   In this paper, we propose a novel test prioritization technique that brings order into the unlabeled test instances according to their bug-revealing capabilities, namely TestRank. Different from existing solutions, TestRank leverages both intrinsic attributes and contextual attributes of test instances when prioritizing them. To be specific, we first build a similarity graph on test instances and training samples, and we conduct graph-based semi-supervised learning to extract contextual features. Then, for a particular test instance, the contextual features extracted from the graph neural network (GNN) and the intrinsic features obtained with the DL model itself are combined to predict its bug-revealing probability. Finally, TestRank prioritizes unlabeled test instances in descending order of the above probability value. We evaluate the performance of TestRank on a variety of image classification datasets. Experimental results show that the debugging efficiency of our method significantly outperforms existing test prioritization techniques.

</details>

<details>

<summary>2021-05-21 06:26:34 - Snipuzz: Black-box Fuzzing of IoT Firmware via Message Snippet Inference</summary>

- *Xiaotao Feng, Ruoxi Sun, Xiaogang Zhu, Minhui Xue, Sheng Wen, Dongxi Liu, Surya Nepal, Yang Xiang*

- `2105.05445v2` - [abs](http://arxiv.org/abs/2105.05445v2) - [pdf](http://arxiv.org/pdf/2105.05445v2)

> The proliferation of Internet of Things (IoT) devices has made people's lives more convenient, but it has also raised many security concerns. Due to the difficulty of obtaining and emulating IoT firmware, the black-box fuzzing of IoT devices has become a viable option. However, existing black-box fuzzers cannot form effective mutation optimization mechanisms to guide their testing processes, mainly due to the lack of feedback. It is difficult or even impossible to apply existing grammar-based fuzzing strategies. Therefore, an efficient fuzzing approach with syntax inference is required in the IoT fuzzing domain. To address these critical problems, we propose a novel automatic black-box fuzzing for IoT firmware, termed Snipuzz. Snipuzz runs as a client communicating with the devices and infers message snippets for mutation based on the responses. Each snippet refers to a block of consecutive bytes that reflect the approximate code coverage in fuzzing. This mutation strategy based on message snippets considerably narrows down the search space to change the probing messages. We compared Snipuzz with four state-of-the-art IoT fuzzing approaches, i.e., IoTFuzzer, BooFuzz, Doona, and Nemesys. Snipuzz not only inherits the advantages of app-based fuzzing (e.g., IoTFuzzer, but also utilizes communication responses to perform efficient mutation. Furthermore, Snipuzz is lightweight as its execution does not rely on any prerequisite operations, such as reverse engineering of apps. We also evaluated Snipuzz on 20 popular real-world IoT devices. Our results show that Snipuzz could identify 5 zero-day vulnerabilities, and 3 of them could be exposed only by Snipuzz. All the newly discovered vulnerabilities have been confirmed by their vendors.

</details>

<details>

<summary>2021-05-21 07:30:58 - Dissecting contact tracing apps in the Android platform</summary>

- *Vasileios Kouliaridis, Georgios Kambourakis, Efstratios Chatzoglou, Dimitrios Geneiatakis, Hua Wang*

- `2008.00214v5` - [abs](http://arxiv.org/abs/2008.00214v5) - [pdf](http://arxiv.org/pdf/2008.00214v5)

> Contact tracing has historically been used to retard the spread of infectious diseases, but if it is exercised by hand in large-scale, it is known to be a resource-intensive and quite deficient process. Nowadays, digital contact tracing has promptly emerged as an indispensable asset in the global fight against the coronavirus pandemic. The work at hand offers a meticulous study of all the official Android contact tracing apps deployed hitherto by European countries. Each app is closely scrutinized both statically and dynamically by means of dynamic instrumentation. Depending on the level of examination, static analysis results are grouped in two axes. The first encompasses permissions, API calls, and possible connections to external URLs, while the second concentrates on potential security weaknesses and vulnerabilities, including the use of trackers, in-depth manifest analysis, shared software analysis, and taint analysis. Dynamic analysis on the other hand collects data pertaining to Java classes and network traffic. The results demonstrate that while overall these apps are well-engineered, they are not free of weaknesses, vulnerabilities, and misconfigurations that may ultimately put the user security and privacy at risk.

</details>

<details>

<summary>2021-05-21 09:37:54 - Random Hash Code Generation for Cancelable Fingerprint Templates using Vector Permutation and Shift-order Process</summary>

- *Sani M. Abdullahi, Sun Shuifa*

- `2105.10227v1` - [abs](http://arxiv.org/abs/2105.10227v1) - [pdf](http://arxiv.org/pdf/2105.10227v1)

> Cancelable biometric techniques have been used to prevent the compromise of biometric data by generating and using their corresponding cancelable templates for user authentication. However, the non-invertible distance preserving transformation methods employed in various schemes are often vulnerable to information leakage since matching is performed in the transformed domain. In this paper, we propose a non-invertible distance preserving scheme based on vector permutation and shift-order process. First, the dimension of feature vectors is reduced using kernelized principle component analysis (KPCA) prior to randomly permuting the extracted vector features. A shift-order process is then applied to the generated features in order to achieve non-invertibility and combat similarity-based attacks. The generated hash codes are resilient to different security and privacy attacks whilst fulfilling the major revocability and unlinkability requirements. Experimental evaluation conducted on 6 datasets of FVC2002 and FVC2004 reveals a high-performance accuracy of the proposed scheme better than other existing state-of-the-art schemes.

</details>

<details>

<summary>2021-05-21 15:51:38 - SCSGuard: Deep Scam Detection for Ethereum Smart Contracts</summary>

- *Huiwen Hu, Yuedong Xu*

- `2105.10426v1` - [abs](http://arxiv.org/abs/2105.10426v1) - [pdf](http://arxiv.org/pdf/2105.10426v1)

> Smart contract is the building block of blockchain systems that enables automated peer-to-peer transactions and decentralized services. With the increasing popularity of smart contracts, blockchain systems, in particular Ethereum, have been the "paradise" of versatile fraud activities in which Ponzi, Honeypot and Phishing are the prominent ones. Formal verification and symbolic analysis have been employed to combat these destructive scams by analyzing the codes and function calls, yet the vulnerability of each \emph{individual} scam should be predefined discreetly. In this work, we present SCSGuard, a novel deep learning scam detection framework that harnesses the automatically extractable bytecodes of smart contracts as their new features. We design a GRU network with attention mechanism to learn from the \emph{N-gram bytecode} patterns, and determines whether a smart contract is fraudulent or not. Our framework is advantageous over the baseline algorithms in three aspects. Firstly, SCSGuard provides a unified solution to different scam genres, thus relieving the need of code analysis skills. Secondly, the inference of SCSGuard is faster than the code analysis by several order of magnitudes. Thirdly, experimental results manifest that SCSGuard achieves high accuracy (0.92$\sim$0.94), precision (0.94$\sim$0.96\%) and recall (0.97$\sim$0.98) for both Ponzi and Honeypot scams under similar settings, and is potentially useful to detect new Phishing smart contracts.

</details>

<details>

<summary>2021-05-21 21:47:51 - No Crash, No Exploit: Automated Verification of Embedded Kernels</summary>

- *Olivier Nicole, Matthieu Lemerre, Sébastien Bardin, Xavier Rival*

- `2011.15065v2` - [abs](http://arxiv.org/abs/2011.15065v2) - [pdf](http://arxiv.org/pdf/2011.15065v2)

> The kernel is the most safety- and security-critical component of many computer systems, as the most severe bugs lead to complete system crash or exploit. It is thus desirable to guarantee that a kernel is free from these bugs using formal methods, but the high cost and expertise required to do so are deterrent to wide applicability. We propose a method that can verify both absence of runtime errors (i.e. crashes) and absence of privilege escalation (i.e. exploits) in embedded kernels from their binary executables. The method can verify the kernel runtime independently from the application, at the expense of only a few lines of simple annotations. When given a specific application, the method can verify simple kernels without any human intervention. We demonstrate our method on two different use cases: we use our tool to help the development of a new embedded real-time kernel, and we verify an existing industrial real-time kernel executable with no modification. Results show that the method is fast, simple to use, and can prevent real errors and security vulnerabilities.

</details>

<details>

<summary>2021-05-22 09:00:47 - A Formal Approach to Physics-Based Attacks in Cyber-Physical Systems (Extended Version)</summary>

- *Ruggero Lanotte, Massimo Merro, Andrei Munteanu, Luca Viganò*

- `1902.04572v3` - [abs](http://arxiv.org/abs/1902.04572v3) - [pdf](http://arxiv.org/pdf/1902.04572v3)

> We apply formal methods to lay and streamline theoretical foundations to reason about Cyber-Physical Systems (CPSs) and physics-based attacks, i.e., attacks targeting physical devices. We focus on a formal treatment of both integrity and denial of service attacks to sensors and actuators of CPSs, and on the timing aspects of these attacks. Our contributions are fourfold. (1)~We define a hybrid process calculus to model both CPSs and physics-based attacks. (2)~We formalise a threat model that specifies MITM attacks that can manipulate sensor readings or control commands in order to drive a CPS into an undesired state, and we provide the means to assess attack tolerance/vulnerability with respect to a given attack. (3)~We formalise how to estimate the impact of a successful attack on a CPS and investigate possible quantifications of the success chances of an attack. (4)~We illustrate our definitions and results by formalising a non-trivial running example in Uppaal SMC, the statistical extension of the Uppaal model checker; we use Uppaal SMC as an automatic tool for carrying out a static security analysis of our running example in isolation and when exposed to three different physics-based attacks with different impacts.

</details>

<details>

<summary>2021-05-22 09:23:19 - Simulating SQL Injection Vulnerability Exploitation Using Q-Learning Reinforcement Learning Agents</summary>

- *Laszlo Erdodi, Åvald Åslaugson Sommervoll, Fabio Massimo Zennaro*

- `2101.03118v2` - [abs](http://arxiv.org/abs/2101.03118v2) - [pdf](http://arxiv.org/pdf/2101.03118v2)

> In this paper, we propose a formalization of the process of exploitation of SQL injection vulnerabilities. We consider a simplification of the dynamics of SQL injection attacks by casting this problem as a security capture-the-flag challenge. We model it as a Markov decision process, and we implement it as a reinforcement learning problem. We then deploy reinforcement learning agents tasked with learning an effective policy to perform SQL injection; we design our training in such a way that the agent learns not just a specific strategy to solve an individual challenge but a more generic policy that may be applied to perform SQL injection attacks against any system instantiated randomly by our problem generator. We analyze the results in terms of the quality of the learned policy and in terms of convergence time as a function of the complexity of the challenge and the learning agent's complexity. Our work fits in the wider research on the development of intelligent agents for autonomous penetration testing and white-hat hacking, and our results aim to contribute to understanding the potential and the limits of reinforcement learning in a security environment.

</details>

<details>

<summary>2021-05-22 17:27:57 - Bin2vec: Learning Representations of Binary Executable Programs for Security Tasks</summary>

- *Shushan Arakelyan, Sima Arasteh, Christophe Hauser, Erik Kline, Aram Galstyan*

- `2002.03388v2` - [abs](http://arxiv.org/abs/2002.03388v2) - [pdf](http://arxiv.org/pdf/2002.03388v2)

> Tackling binary program analysis problems has traditionally implied manually defining rules and heuristics, a tedious and time-consuming task for human analysts. In order to improve automation and scalability, we propose an alternative direction based on distributed representations of binary programs with applicability to a number of downstream tasks. We introduce Bin2vec, a new approach leveraging Graph Convolutional Networks (GCN) along with computational program graphs in order to learn a high dimensional representation of binary executable programs. We demonstrate the versatility of this approach by using our representations to solve two semantically different binary analysis tasks - functional algorithm classification and vulnerability discovery. We compare the proposed approach to our own strong baseline as well as published results and demonstrate improvement over state-of-the-art methods for both tasks. We evaluated Bin2vec on 49191 binaries for the functional algorithm classification task, and on 30 different CWE-IDs including at least 100 CVE entries each for the vulnerability discovery task. We set a new state-of-the-art result by reducing the classification error by 40% compared to the source-code-based inst2vec approach, while working on binary code. For almost every vulnerability class in our dataset, our prediction accuracy is over 80% (and over 90% in multiple classes).

</details>

<details>

<summary>2021-05-22 23:33:20 - Real-time Detection of Practical Universal Adversarial Perturbations</summary>

- *Kenneth T. Co, Luis Muñoz-González, Leslie Kanthan, Emil C. Lupu*

- `2105.07334v2` - [abs](http://arxiv.org/abs/2105.07334v2) - [pdf](http://arxiv.org/pdf/2105.07334v2)

> Universal Adversarial Perturbations (UAPs) are a prominent class of adversarial examples that exploit the systemic vulnerabilities and enable physically realizable and robust attacks against Deep Neural Networks (DNNs). UAPs generalize across many different inputs; this leads to realistic and effective attacks that can be applied at scale. In this paper we propose HyperNeuron, an efficient and scalable algorithm that allows for the real-time detection of UAPs by identifying suspicious neuron hyper-activations. Our results show the effectiveness of HyperNeuron on multiple tasks (image classification, object detection), against a wide variety of universal attacks, and in realistic scenarios, like perceptual ad-blocking and adversarial patches. HyperNeuron is able to simultaneously detect both adversarial mask and patch UAPs with comparable or better performance than existing UAP defenses whilst introducing a significantly reduced latency of only 0.86 milliseconds per image. This suggests that many realistic and practical universal attacks can be reliably mitigated in real-time, which shows promise for the robust deployment of machine learning systems.

</details>

<details>

<summary>2021-05-23 14:34:47 - Regularization Can Help Mitigate Poisoning Attacks... with the Right Hyperparameters</summary>

- *Javier Carnerero-Cano, Luis Muñoz-González, Phillippa Spencer, Emil C. Lupu*

- `2105.10948v1` - [abs](http://arxiv.org/abs/2105.10948v1) - [pdf](http://arxiv.org/pdf/2105.10948v1)

> Machine learning algorithms are vulnerable to poisoning attacks, where a fraction of the training data is manipulated to degrade the algorithms' performance. We show that current approaches, which typically assume that regularization hyperparameters remain constant, lead to an overly pessimistic view of the algorithms' robustness and of the impact of regularization. We propose a novel optimal attack formulation that considers the effect of the attack on the hyperparameters, modelling the attack as a \emph{minimax bilevel optimization problem}. This allows to formulate optimal attacks, select hyperparameters and evaluate robustness under worst case conditions. We apply this formulation to logistic regression using $L_2$ regularization, empirically show the limitations of previous strategies and evidence the benefits of using $L_2$ regularization to dampen the effect of poisoning attacks.

</details>

<details>

<summary>2021-05-23 18:45:29 - Neuronal Jamming Cyberattack over Invasive BCI Affecting the Resolution of Tasks Requiring Visual Capabilities</summary>

- *Sergio López Bernal, Alberto Huertas Celdrán, Gregorio Martínez Pérez*

- `2105.10997v1` - [abs](http://arxiv.org/abs/2105.10997v1) - [pdf](http://arxiv.org/pdf/2105.10997v1)

> Invasive Brain-Computer Interfaces (BCI) are extensively used in medical application scenarios to record, stimulate, or inhibit neural activity with different purposes. An example is the stimulation of some brain areas to reduce the effects generated by Parkinson's disease. Despite the advances in recent years, cybersecurity on BCI is an open challenge since attackers can exploit the vulnerabilities of invasive BCIs to induce malicious stimulation or treatment disruption, affecting neuronal activity. In this work, we design and implement a novel neuronal cyberattack, called Neuronal Jamming (JAM), which prevents neurons from producing spikes. To implement and measure the JAM impact, and due to the lack of realistic neuronal topologies in mammalians, we have defined a use case with a Convolutional Neural Network (CNN) trained to allow a mouse to exit a particular maze. The resulting model has been translated to a neural topology, simulating a portion of a mouse's visual cortex. The impact of JAM on both biological and artificial networks is measured, analyzing how the attacks can both disrupt the spontaneous neural signaling and the mouse's capacity to exit the maze. Besides, we compare the impacts of both JAM and FLO (an existing neural cyberattack) demonstrating that JAM generates a higher impact in terms of neuronal spike rate. Finally, we discuss on whether and how JAM and FLO attacks could induce the effects of neurodegenerative diseases if the implanted BCI had a comprehensive electrode coverage of the targeted brain regions.

</details>

<details>

<summary>2021-05-24 02:12:05 - Automated Dynamic Concurrency Analysis for Go</summary>

- *Saeed Taheri, Ganesh Gopalakrishnan*

- `2105.11064v1` - [abs](http://arxiv.org/abs/2105.11064v1) - [pdf](http://arxiv.org/pdf/2105.11064v1)

> The concurrency features of the Go language have proven versatile in the development of a number of concurrency systems. However, correctness methods to address challenges in Go concurrency debugging have not received much attention. In this work, we present an automatic dynamic tracing mechanism that efficiently captures and helps analyze the whole-program concurrency model. Using an enhancement to the built-in tracer package of Go and a framework that collects dynamic traces from application execution, we enable thorough post-mortem analysis for concurrency debugging. Preliminary results about the effectiveness and scalability (up to more than 2K goroutines) of our proposed dynamic tracing for concurrent debugging are presented. We discuss the future direction for exploiting dynamic tracing towards accelerating concurrent bug exposure.

</details>

<details>

<summary>2021-05-24 19:17:38 - Technical Report: Insider-Resistant Context-Based Pairing for Multimodality Sleep Apnea Test</summary>

- *Yao Zheng, Shekh Md Mahmudul Islam, Yanjun Pan, Marionne Millan, Samson Aggelopoulos, Brian Lu, Alvin Yang, Thomas Yang, Stephanie Aelmore, Willy Chang, Alana Power, Ming Li, Olga Borić-Lubecke, Victor Lubecke, Wenhai Sun*

- `2105.00314v2` - [abs](http://arxiv.org/abs/2105.00314v2) - [pdf](http://arxiv.org/pdf/2105.00314v2)

> The increasingly sophisticated at-home screening systems for obstructive sleep apnea (OSA), integrated with both contactless and contact-based sensing modalities, bring convenience and reliability to remote chronic disease management. However, the device pairing processes between system components are vulnerable to wireless exploitation from a non-compliant user wishing to manipulate the test results. This work presents SIENNA, an insider-resistant context-based pairing protocol. SIENNA leverages JADE-ICA to uniquely identify a user's respiration pattern within a multi-person environment and fuzzy commitment for automatic device pairing, while using friendly jamming technique to prevents an insider with knowledge of respiration patterns from acquiring the pairing key. Our analysis and test results show that SIENNA can achieve reliable (> 90% success rate) device pairing under a noisy environment and is robust against the attacker with full knowledge of the context information.

</details>

<details>

<summary>2021-05-24 20:35:43 - Recommending Bug-fixing Comments from Issue Tracking Discussions in Support of Bug Repair</summary>

- *Rrezarta Krasniqi*

- `2105.11525v1` - [abs](http://arxiv.org/abs/2105.11525v1) - [pdf](http://arxiv.org/pdf/2105.11525v1)

> In practice, developers search for related earlier bugs and their associated discussion threads when faced with a new bug to repair. Typically, these discussion threads consist of comments and even bug-fixing comments intended to capture clues for facilitating the investigation and root cause of a new bug report. Over time, these discussions can become extensively lengthy and difficult to understand. Inevitably, these discussion threads lead to instances where bug-fixing comments intermingle with seemingly-unrelated comments. This task, however, poses further challenges when dealing with high volumes of bug reports. Large software systems are plagued by thousands of bug reports daily. Hence, it becomes time-consuming to investigate these bug reports efficiently. To address this gap, this paper builds a ranked-based automated tool that we refer it to as RETRORANK. Specifically, RETRORANK recommends bug-fixing comments from issue tracking discussion threads in the context of user query relevance, the use of positive language, and semantic relevance among comments. By using a combination of Vector Space Model (VSM), Sentiment Analysis (SA), and the TextRank Model (TR) we show how that past fixed bugs and their associated bug-fixing comments with relatively positive sentiments can semantically connect to investigate the root cause of a new bug. We evaluated our approach via a synthetic study and a user study. Results indicate that RETRORANK significantly improved performance when compared to the baseline VSM.

</details>

<details>

<summary>2021-05-25 07:28:56 - On the Need of Removing Last Releases of Data When Using or Validating Defect Prediction Models</summary>

- *Aalok Ahluwalia, Massimiliano Di Penta, Davide Falessi*

- `2003.14376v2` - [abs](http://arxiv.org/abs/2003.14376v2) - [pdf](http://arxiv.org/pdf/2003.14376v2)

> To develop and train defect prediction models, researchers rely on datasets in which a defect is attributed to an artifact, e.g., a class of a given release. However, the creation of such datasets is far from being perfect. It can happen that a defect is discovered several releases after its introduction: this phenomenon has been called "dormant defects". This means that, if we observe today the status of a class in its current version, it can be considered as defect-free while this is not the case. We call "snoring" the noise consisting of such classes, affected by dormant defects only. We conjecture that the presence of snoring negatively impacts the classifiers' accuracy and their evaluation. Moreover, earlier releases likely contain more snoring classes than older releases, thus, removing the most recent releases from a dataset could reduce the snoring effect and improve the accuracy of classifiers. In this paper we investigate the impact of the snoring noise on classifiers' accuracy and their evaluation, and the effectiveness of a possible countermeasure consisting in removing the last releases of data. We analyze the accuracy of 15 machine learning defect prediction classifiers on data from more than 4,000 bugs and 600 releases of 19 open source projects from the Apache ecosystem. Our results show that, on average across projects: (i) the presence of snoring decreases the recall of defect prediction classifiers; (ii) evaluations affected by snoring are likely unable to identify the best classifiers, and (iii) removing data from recent releases helps to significantly improve the accuracy of the classifiers. On summary, this paper provides insights on how to create a software defect dataset by mitigating the effect of snoring.

</details>

<details>

<summary>2021-05-25 13:41:18 - Blockchain Assisted Decentralized Federated Learning (BLADE-FL): Performance Analysis and Resource Allocation</summary>

- *Jun Li, Yumeng Shao, Kang Wei, Ming Ding, Chuan Ma, Long Shi, Zhu Han, H. Vincent Poor*

- `2101.06905v2` - [abs](http://arxiv.org/abs/2101.06905v2) - [pdf](http://arxiv.org/pdf/2101.06905v2)

> Federated learning (FL), as a distributed machine learning paradigm, promotes personal privacy by local data processing at each client. However, relying on a centralized server for model aggregation, standard FL is vulnerable to server malfunctions, untrustworthy server, and external attacks. To address this issue, we propose a decentralized FL framework by integrating blockchain into FL, namely, blockchain assisted decentralized federated learning (BLADE-FL). In a round of the proposed BLADE-FL, each client broadcasts the trained model to other clients, aggregates its own model with received ones, and then competes to generate a block before its local training of the next round. We evaluate the learning performance of BLADE-FL, and develop an upper bound on the global loss function. Then we verify that this bound is convex with respect to the number of overall aggregation rounds K, and optimize the computing resource allocation for minimizing the upper bound. We also note that there is a critical problem of training deficiency, caused by lazy clients who plagiarize others' trained models and add artificial noises to disguise their cheating behaviors. Focusing on this problem, we explore the impact of lazy clients on the learning performance of BLADE-FL, and characterize the relationship among the optimal K, the learning parameters, and the proportion of lazy clients. Based on MNIST and Fashion-MNIST datasets, we show that the experimental results are consistent with the analytical ones. To be specific, the gap between the developed upper bound and experimental results is lower than 5%, and the optimized K based on the upper bound can effectively minimize the loss function.

</details>

<details>

<summary>2021-05-25 17:19:20 - Extractive Summarization of Related Bug-fixing Comments in Support of Bug Repair</summary>

- *Rrezarta Krasniqi*

- `2103.15211v2` - [abs](http://arxiv.org/abs/2103.15211v2) - [pdf](http://arxiv.org/pdf/2103.15211v2)

> When developers investigate a new bug report, they search for similar previously fixed bug reports and discussion threads attached to them. These discussion threads convey important information about the behavior of the bug including relevant bug-fixing comments. Oftentimes, these discussion threads become extensively lengthy due to the severity of the reported bug. This adds another layer of complexity, especially if relevant bug-fixing comments intermingle with seemingly unrelated comments. To manually detect these relevant comments among various cross-cutting discussion threads can become a daunting task when dealing with high volume of bug reports. To automate this process, our focus is to initially extract and detect comments in the context of query relevance, the use of positive language, and semantic relevance. Then, we merge these comments in the form of a summary for easy understanding. Specifically, we combine Sentiment Analysis and the TextRank Model with the baseline Vector Space Model (VSM). Preliminary findings indicate that bug-fixing comments tend to be positive and there exists a semantic relevance with comments from other cross-cutting discussion threads. The results also indicate that our combined approach improves overall ranking performance against the baseline VSM.

</details>

<details>

<summary>2021-05-26 04:42:09 - Vulnerabilities and Open Issues of Smart Contracts: A Systematic Mapping</summary>

- *Gabriel de Sousa Matsumura, Luciana Brasil Rebelo dos Santos, Arlindo Flavio da Conceição, Nandamudi Lankalapalli Vijaykumar*

- `2104.12295v2` - [abs](http://arxiv.org/abs/2104.12295v2) - [pdf](http://arxiv.org/pdf/2104.12295v2)

> Smart Contracts (SCs) are programs stored in a Blockchain to ensure agreements between two or more parties. Due to the unchangeable essence of Blockchain, failures or errors in SCs become perpetual once published. The reliability of SCs is essential to avoid financial losses. So, SCs must be checked to ensure the absence of errors. Hence, many studies addressed new methods and tools for zero-bug software in SCs. This paper conducted a systematic literature mapping identifying initiatives and tools to analyze SCs and how to deal with the identified vulnerabilities. Besides, this work identifies gaps that may lead to research topics for future work.

</details>

<details>

<summary>2021-05-26 06:16:19 - Benchmarking Robustness of Machine Reading Comprehension Models</summary>

- *Chenglei Si, Ziqing Yang, Yiming Cui, Wentao Ma, Ting Liu, Shijin Wang*

- `2004.14004v2` - [abs](http://arxiv.org/abs/2004.14004v2) - [pdf](http://arxiv.org/pdf/2004.14004v2)

> Machine Reading Comprehension (MRC) is an important testbed for evaluating models' natural language understanding (NLU) ability. There has been rapid progress in this area, with new models achieving impressive performance on various benchmarks. However, existing benchmarks only evaluate models on in-domain test sets without considering their robustness under test-time perturbations or adversarial attacks. To fill this important gap, we construct AdvRACE (Adversarial RACE), a new model-agnostic benchmark for evaluating the robustness of MRC models under four different types of adversarial attacks, including our novel distractor extraction and generation attacks. We show that state-of-the-art (SOTA) models are vulnerable to all of these attacks. We conclude that there is substantial room for building more robust MRC models and our benchmark can help motivate and measure progress in this area. We release our data and code at https://github.com/NoviScl/AdvRACE .

</details>

<details>

<summary>2021-05-26 09:07:35 - Optimal Provable Robustness of Quantum Classification via Quantum Hypothesis Testing</summary>

- *Maurice Weber, Nana Liu, Bo Li, Ce Zhang, Zhikuan Zhao*

- `2009.10064v2` - [abs](http://arxiv.org/abs/2009.10064v2) - [pdf](http://arxiv.org/pdf/2009.10064v2)

> Quantum machine learning models have the potential to offer speedups and better predictive accuracy compared to their classical counterparts. However, these quantum algorithms, like their classical counterparts, have been shown to also be vulnerable to input perturbations, in particular for classification problems. These can arise either from noisy implementations or, as a worst-case type of noise, adversarial attacks. In order to develop defence mechanisms and to better understand the reliability of these algorithms, it is crucial to understand their robustness properties in presence of natural noise sources or adversarial manipulation. From the observation that measurements involved in quantum classification algorithms are naturally probabilistic, we uncover and formalize a fundamental link between binary quantum hypothesis testing and provably robust quantum classification. This link leads to a tight robustness condition which puts constraints on the amount of noise a classifier can tolerate, independent of whether the noise source is natural or adversarial. Based on this result, we develop practical protocols to optimally certify robustness. Finally, since this is a robustness condition against worst-case types of noise, our result naturally extends to scenarios where the noise source is known. Thus, we also provide a framework to study the reliability of quantum classification protocols beyond the adversarial, worst-case noise scenarios.

</details>

<details>

<summary>2021-05-26 17:36:35 - MTH-IDS: A Multi-Tiered Hybrid Intrusion Detection System for Internet of Vehicles</summary>

- *Li Yang, Abdallah Moubayed, Abdallah Shami*

- `2105.13289v1` - [abs](http://arxiv.org/abs/2105.13289v1) - [pdf](http://arxiv.org/pdf/2105.13289v1)

> Modern vehicles, including connected vehicles and autonomous vehicles, nowadays involve many electronic control units connected through intra-vehicle networks to implement various functionalities and perform actions. Modern vehicles are also connected to external networks through vehicle-to-everything technologies, enabling their communications with other vehicles, infrastructures, and smart devices. However, the improving functionality and connectivity of modern vehicles also increase their vulnerabilities to cyber-attacks targeting both intra-vehicle and external networks due to the large attack surfaces. To secure vehicular networks, many researchers have focused on developing intrusion detection systems (IDSs) that capitalize on machine learning methods to detect malicious cyber-attacks. In this paper, the vulnerabilities of intra-vehicle and external networks are discussed, and a multi-tiered hybrid IDS that incorporates a signature-based IDS and an anomaly-based IDS is proposed to detect both known and unknown attacks on vehicular networks. Experimental results illustrate that the proposed system can detect various types of known attacks with 99.99% accuracy on the CAN-intrusion-dataset representing the intra-vehicle network data and 99.88% accuracy on the CICIDS2017 dataset illustrating the external vehicular network data. For the zero-day attack detection, the proposed system achieves high F1-scores of 0.963 and 0.800 on the above two datasets, respectively. The average processing time of each data packet on a vehicle-level machine is less than 0.6 ms, which shows the feasibility of implementing the proposed system in real-time vehicle systems. This emphasizes the effectiveness and efficiency of the proposed IDS.

</details>

<details>

<summary>2021-05-27 02:51:02 - Wireless Charging Power Side-Channel Attacks</summary>

- *Alexander S. La Cour, Khurram K. Afridi, G. Edward Suh*

- `2105.12266v2` - [abs](http://arxiv.org/abs/2105.12266v2) - [pdf](http://arxiv.org/pdf/2105.12266v2)

> This paper shows that today's wireless charging interface is vulnerable to power side-channel attacks; a smartphone charging wirelessly leaks private information about its activity to the wireless charger (charging transmitter). We present a website fingerprinting attack through the wireless charging side-channel for both iOS and Android devices. The attack monitors the current drawn by the wireless charging transmitter while 20 webpages from the Alexa top sites list are loaded on a charging smartphone. We implement a classifier that correctly identifies unlabeled current traces with an accuracy of 87% on average for an iPhone 11 and 95% on average for a Google Pixel 4. This represents a considerable security threat because wireless charging does not require any user permission if the phone is within the range of a charging transmitter. To the best of our knowledge, this work represents the first to introduce and demonstrate a power side-channel attack through wireless charging. Additionally, this study compares the wireless charging side-channel with the wired USB charging power side-channel, showing that they are comparable. We find that the performance of the attack deteriorates as the contents of websites change over time. Furthermore, we discover that the amount of information leakage through both wireless and wired charging interfaces heavily depends on the battery level; minimal information is leaked at low battery levels.

</details>

<details>

<summary>2021-05-27 18:36:25 - What breach? Measuring online awareness of security incidents by studying real-world browsing behavior</summary>

- *Sruti Bhagavatula, Lujo Bauer, Apu Kapadia*

- `2010.09843v4` - [abs](http://arxiv.org/abs/2010.09843v4) - [pdf](http://arxiv.org/pdf/2010.09843v4)

> Awareness about security and privacy risks is important for developing good security habits. Learning about real-world security incidents and data breaches can alert people to the ways in which their information is vulnerable online, thus playing a significant role in encouraging safe security behavior. This paper examines 1) how often people read about security incidents online, 2) of those people, whether and to what extent they follow up with an action, e.g., by trying to read more about the incident, and 3) what influences the likelihood that they will read about an incident and take some action. We study this by quantitatively examining real-world internet-browsing data from 303 participants.   Our findings present a bleak view of awareness of security incidents. Only 16% of participants visited any web pages related to six widely publicized large-scale security incidents; few read about one even when an incident was likely to have affected them (e.g., the Equifax breach almost universally affected people with Equifax credit reports). We further found that more severe incidents as well as articles that constructively spoke about the incident inspired more action. We conclude with recommendations for specific future research and for enabling useful security incident information to reach more people.

</details>

<details>

<summary>2021-05-27 18:59:12 - Tactical Reframing of Online Disinformation Campaigns Against The Istanbul Convention</summary>

- *Tuğrulcan Elmas, Rebekah Overdorf, Karl Aberer*

- `2105.13398v1` - [abs](http://arxiv.org/abs/2105.13398v1) - [pdf](http://arxiv.org/pdf/2105.13398v1)

> In March 2021, Turkey withdrew from The Istanbul Convention, a human-rights treaty that addresses violence against women, citing issues with the convention's implicit recognition of sexual and gender minorities. In this work, we trace disinformation campaigns related to the Istanbul Convention and its associated Turkish law that circulate on divorced men's rights Facebook groups. We find that these groups adjusted the narrative and focus of the campaigns to appeal to a larger audience, which we refer to as "tactical reframing." Initially, the men organized in a grass-roots manner to campaign against the Turkish law that was passed to codify the convention, focusing on one-sided custody of children and indefinite alimony. Later, they reframed their campaign and began attacking the Istanbul Convention, highlighting its acknowledgment of homosexuality. This case study highlights how disinformation campaigns can be used to weaponize homophobia in order to limit the rights of women. To the best of our knowledge, this is the first case study that analyzes a narrative reframing in the context of a disinformation campaign on social media.

</details>

<details>

<summary>2021-05-28 11:47:59 - The Unpatchable Silicon: A Full Break of the Bitstream Encryption of Xilinx 7-Series FPGAs</summary>

- *Maik Ender, Amir Moradi, Christof Paar*

- `2105.13756v1` - [abs](http://arxiv.org/abs/2105.13756v1) - [pdf](http://arxiv.org/pdf/2105.13756v1)

> The security of FPGAs is a crucial topic, as any vulnerability within the hardware can have severe consequences, if they are used in a secure design. Since FPGA designs are encoded in a bitstream, securing the bitstream is of the utmost importance. Adversaries have many motivations to recover and manipulate the bitstream, including design cloning, IP theft, manipulation of the design, or design subversions e.g., through hardware Trojans. Given that FPGAs are often part of cyber-physical systems e.g., in aviation, medical, or industrial devices, this can even lead to physical harm. Consequently, vendors have introduced bitstream encryption, offering authenticity and confidentiality. Even though attacks against bitstream encryption have been proposed in the past, e.g., side-channel analysis and probing, these attacks require sophisticated equipment and considerable technical expertise. In this paper, we introduce novel low-cost attacks against the Xilinx 7-Series (and Virtex-6) bitstream encryption, resulting in the total loss of authenticity and confidentiality. We exploit a design flaw which piecewise leaks the decrypted bitstream. In the attack, the FPGA is used as a decryption oracle, while only access to a configuration interface is needed. The attack does not require any sophisticated tools and, depending on the target system, can potentially be launched remotely. In addition to the attacks, we discuss several countermeasures.

</details>

<details>

<summary>2021-05-28 12:21:41 - ARMORY: Fully Automated and Exhaustive Fault Simulation on ARM-M Binaries</summary>

- *Max Hoffmann, Falk Schellenberg, Christof Paar*

- `2105.13769v1` - [abs](http://arxiv.org/abs/2105.13769v1) - [pdf](http://arxiv.org/pdf/2105.13769v1)

> Embedded systems are ubiquitous. However, physical access of users and likewise attackers makes them often threatened by fault attacks: a single fault during the computation of a cryptographic primitive can lead to a total loss of system security. This can have serious consequences, e.g., in safetycritical systems, including bodily harm and catastrophic technical failures. However, countermeasures often focus on isolated fault models and high layers of abstraction. This leads to a dangerous sense of security, because exploitable faults that are only visible at machine code level might not be covered by countermeasures. In this work we present ARMORY, a fully automated open source framework for exhaustive fault simulation on binaries of the ubiquitous ARM-M class. It allows engineers and analysts to efficiently scan a binary for potential weaknesses against arbitrary combinations of multi-variate fault injections under a large variety of fault models. Using ARMORY, we demonstrate the power of fully automated fault analysis and the dangerous implications of applying countermeasures without knowledge of physical addresses and offsets. We exemplarily analyze two case studies, which are highly relevant for practice: a DFA on AES (cryptographic) and a secure bootloader (non-cryptographic). Our results show that indeed numerous exploitable faults found by ARMORY which occur in the actual implementations are easily missed in manual inspection. Crucially, most faults are only visible when taking machine code information, i.e., addresses and offsets, into account. Surprisingly, we show that a countermeasure that protects against one type of fault can actually largely increase the vulnerability to other fault models. Our work demonstrates the need for countermeasures that, at least in their evaluation, are not restricted to isolated fault models and consider low-level information [...].

</details>

<details>

<summary>2021-05-28 12:58:24 - Social Engineering in Cybersecurity: A Domain Ontology and Knowledge Graph Application Examples</summary>

- *Zuoguang Wang, Hongsong Zhu, Peipei Liu, Limin Sun*

- `2106.01157v1` - [abs](http://arxiv.org/abs/2106.01157v1) - [pdf](http://arxiv.org/pdf/2106.01157v1)

> Social engineering has posed a serious threat to cyberspace security. To protect against social engineering attacks, a fundamental work is to know what constitutes social engineering. This paper first develops a domain ontology of social engineering in cybersecurity and conducts ontology evaluation by its knowledge graph application. The domain ontology defines 11 concepts of core entities that significantly constitute or affect social engineering domain, together with 22 kinds of relations describing how these entities related to each other. It provides a formal and explicit knowledge schema to understand, analyze, reuse and share domain knowledge of social engineering. Furthermore, this paper builds a knowledge graph based on 15 social engineering attack incidents and scenarios. 7 knowledge graph application examples (in 6 analysis patterns) demonstrate that the ontology together with knowledge graph is useful to 1) understand and analyze social engineering attack scenario and incident, 2) find the top ranked social engineering threat elements (e.g. the most exploited human vulnerabilities and most used attack mediums), 3) find potential social engineering threats to victims, 4) find potential targets for social engineering attackers, 5) find potential attack paths from specific attacker to specific target, and 6) analyze the same origin attacks.

</details>

<details>

<summary>2021-05-28 13:05:06 - Unsupervised Adversarially-Robust Representation Learning on Graphs</summary>

- *Jiarong Xu, Yang Yang, Junru Chen, Chunping Wang, Xin Jiang, Jiangang Lu, Yizhou Sun*

- `2012.02486v2` - [abs](http://arxiv.org/abs/2012.02486v2) - [pdf](http://arxiv.org/pdf/2012.02486v2)

> Unsupervised/self-supervised pre-training methods for graph representation learning have recently attracted increasing research interests, and they are shown to be able to generalize to various downstream applications. Yet, the adversarial robustness of such pre-trained graph learning models remains largely unexplored. More importantly, most existing defense techniques designed for end-to-end graph representation learning methods require pre-specified label definitions, and thus cannot be directly applied to the pre-training methods. In this paper, we propose an unsupervised defense technique to robustify pre-trained deep graph models, so that the perturbations on the input graph can be successfully identified and blocked before the model is applied to different downstream tasks. Specifically, we introduce a mutual information-based measure, \textit{graph representation vulnerability (GRV)}, to quantify the robustness of graph encoders on the representation space. We then formulate an optimization problem to learn the graph representation by carefully balancing the trade-off between the expressive power and the robustness (\emph{i.e.}, GRV) of the graph encoder. The discrete nature of graph topology and the joint space of graph data make the optimization problem intractable to solve. To handle the above difficulty and to reduce computational expense, we further relax the problem and thus provide an approximate solution. Additionally, we explore a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks. Extensive experiments demonstrate that even without access to labels and tasks, our model is still able to enhance robustness against adversarial attacks on three downstream tasks (node classification, link prediction, and community detection) by an average of +16.5% compared with existing methods.

</details>

<details>

<summary>2021-05-28 13:35:59 - SEVerity: Code Injection Attacks against Encrypted Virtual Machines</summary>

- *Mathias Morbitzer, Sergej Proskurin, Martin Radev, Marko Dorfhuber, Erick Quintanar Salas*

- `2105.13824v1` - [abs](http://arxiv.org/abs/2105.13824v1) - [pdf](http://arxiv.org/pdf/2105.13824v1)

> Modern enterprises increasingly take advantage of cloud infrastructures. Yet, outsourcing code and data into the cloud requires enterprises to trust cloud providers not to meddle with their data. To reduce the level of trust towards cloud providers, AMD has introduced Secure Encrypted Virtualization (SEV). By encrypting Virtual Machines (VMs), SEV aims to ensure data confidentiality, despite a compromised or curious Hypervisor. The SEV Encrypted State (SEV-ES) extension additionally protects the VM's register state from unauthorized access. Yet, both extensions do not provide integrity of the VM's memory, which has already been abused to leak the protected data or to alter the VM's control-flow. In this paper, we introduce the SEVerity attack; a missing puzzle piece in the series of attacks against the AMD SEV family. Specifically, we abuse the system's lack of memory integrity protection to inject and execute arbitrary code within SEV-ES-protected VMs. Contrary to previous code execution attacks against the AMD SEV family, SEVerity neither relies on a specific CPU version nor on any code gadgets inside the VM. Instead, SEVerity abuses the fact that SEV-ES prohibits direct memory access into the encrypted memory. Specifically, SEVerity injects arbitrary code into the encrypted VM through I/O channels and uses the Hypervisor to locate and trigger the execution of the encrypted payload. This allows us to sidestep the protection mechanisms of SEV-ES. Overall, our results demonstrate a success rate of 100% and hence highlight that memory integrity protection is an obligation when encrypting VMs. Consequently, our work presents the final stroke in a series of attacks against AMD SEV and SEV-ES and renders the present implementation as incapable of protecting against a curious, vulnerable, or malicious Hypervisor.

</details>

<details>

<summary>2021-05-28 17:21:23 - Hedging Against Sore Loser Attacks in Cross-Chain Transactions</summary>

- *Yingjie Xue, Maurice Herlihy*

- `2105.06322v2` - [abs](http://arxiv.org/abs/2105.06322v2) - [pdf](http://arxiv.org/pdf/2105.06322v2)

> A *sore loser attack* in cross-blockchain commerce rises when one party decides to halt participation partway through, leaving other parties' assets locked up for a long duration. Although vulnerability to sore loser attacks cannot be entirely eliminated, it can be reduced to an arbitrarily low level. This paper proposes new distributed protocols for hedging a range of cross-chain transactions in a synchronous communication model, such as two-party swaps, $n$-party swaps, brokered transactions, and auctions.

</details>

<details>

<summary>2021-05-28 21:34:02 - Visualizing Representations of Adversarially Perturbed Inputs</summary>

- *Daniel Steinberg, Paul Munro*

- `2105.14116v1` - [abs](http://arxiv.org/abs/2105.14116v1) - [pdf](http://arxiv.org/pdf/2105.14116v1)

> It has been shown that deep learning models are vulnerable to adversarial attacks. We seek to further understand the consequence of such attacks on the intermediate activations of neural networks. We present an evaluation metric, POP-N, which scores the effectiveness of projecting data to N dimensions under the context of visualizing representations of adversarially perturbed inputs. We conduct experiments on CIFAR-10 to compare the POP-2 score of several dimensionality reduction algorithms across various adversarial attacks. Finally, we utilize the 2D data corresponding to high POP-2 scores to generate example visualizations.

</details>

<details>

<summary>2021-05-29 13:33:52 - A Measurement Study on the (In)security of End-of-Life (EoL) Embedded Devices</summary>

- *Dingding Wang, Muhui Jiang, Rui Chang, Yajin Zhou, Baolei Hou, Xiapu Luo, Lei Wu, Kui Ren*

- `2105.14298v1` - [abs](http://arxiv.org/abs/2105.14298v1) - [pdf](http://arxiv.org/pdf/2105.14298v1)

> Embedded devices are becoming popular. Meanwhile, researchers are actively working on improving the security of embedded devices. However, previous work ignores the insecurity caused by a special category of devices, i.e., the End-of-Life (EoL in short) devices. Once a product becomes End-of-Life, vendors tend to no longer maintain its firmware or software, including providing bug fixes and security patches. This makes EoL devices susceptible to attacks. For instance, a report showed that an EoL model with thousands of active devices was exploited to redirect web traffic for malicious purposes. In this paper, we conduct the first measurement study to shed light on the (in)security of EoL devices. To this end, our study performs two types of analysis, including the aliveness analysis and the vulnerability analysis. The first one aims to detect the scale of EoL devices that are still alive. The second one is to evaluate the vulnerabilities existing in (active) EoL devices. We have applied our approach to a large number of EoL models from three vendors (i.e., D-Link, Tp-Link, and Netgear) and detect the alive devices in a time period of ten months. Our study reveals some worrisome facts that were unknown by the community. For instance, there exist more than 2 million active EoL devices. Nearly 300,000 of them are still alive even after five years since they became EoL. Although vendors may release security patches after the EoL date, however, the process is ad hoc and incomplete. As a result, more than 1 million active EoL devices are vulnerable, and nearly half of them are threatened by high-risk vulnerabilities. Attackers can achieve a minimum of 2.79 Tbps DDoS attack by compromising a large number of active EoL devices. We believe these facts pose a clear call for more attention to deal with the security issues of EoL devices.

</details>

<details>

<summary>2021-05-29 14:00:58 - SCOUT: Socially-COnsistent and UndersTandable Graph Attention Network for Trajectory Prediction of Vehicles and VRUs</summary>

- *Sandra Carrasco, David Fernández Llorca, Miguel Ángel Sotelo*

- `2102.06361v2` - [abs](http://arxiv.org/abs/2102.06361v2) - [pdf](http://arxiv.org/pdf/2102.06361v2)

> Autonomous vehicles navigate in dynamically changing environments under a wide variety of conditions, being continuously influenced by surrounding objects. Modelling interactions among agents is essential for accurately forecasting other agents' behaviour and achieving safe and comfortable motion planning. In this work, we propose SCOUT, a novel Attention-based Graph Neural Network that uses a flexible and generic representation of the scene as a graph for modelling interactions, and predicts socially-consistent trajectories of vehicles and Vulnerable Road Users (VRUs) under mixed traffic conditions. We explore three different attention mechanisms and test our scheme with both bird-eye-view and on-vehicle urban data, achieving superior performance than existing state-of-the-art approaches on InD and ApolloScape Trajectory benchmarks. Additionally, we evaluate our model's flexibility and transferability by testing it under completely new scenarios on RounD dataset. The importance and influence of each interaction in the final prediction is explored by means of Integrated Gradients technique and the visualization of the attention learned.

</details>

<details>

<summary>2021-05-29 19:06:35 - Constructing Flow Graphs from Procedural Cybersecurity Texts</summary>

- *Kuntal Kumar Pal, Kazuaki Kashihara, Pratyay Banerjee, Swaroop Mishra, Ruoyu Wang, Chitta Baral*

- `2105.14357v1` - [abs](http://arxiv.org/abs/2105.14357v1) - [pdf](http://arxiv.org/pdf/2105.14357v1)

> Following procedural texts written in natural languages is challenging. We must read the whole text to identify the relevant information or identify the instruction flows to complete a task, which is prone to failures. If such texts are structured, we can readily visualize instruction-flows, reason or infer a particular step, or even build automated systems to help novice agents achieve a goal. However, this structure recovery task is a challenge because of such texts' diverse nature. This paper proposes to identify relevant information from such texts and generate information flows between sentences. We built a large annotated procedural text dataset (CTFW) in the cybersecurity domain (3154 documents). This dataset contains valuable instructions regarding software vulnerability analysis experiences. We performed extensive experiments on CTFW with our LM-GNN model variants in multiple settings. To show the generalizability of both this task and our method, we also experimented with procedural texts from two other domains (Maintenance Manual and Cooking), which are substantially different from cybersecurity. Our experiments show that Graph Convolution Network with BERT sentence embeddings outperforms BERT in all three domains

</details>

<details>

<summary>2021-05-29 23:58:08 - Targeting the Weakest Link: Social Engineering Attacks in Ethereum Smart Contracts</summary>

- *Nikolay Ivanov, Jianzhi Lou, Ting Chen, Jin Li, Qiben Yan*

- `2105.00132v2` - [abs](http://arxiv.org/abs/2105.00132v2) - [pdf](http://arxiv.org/pdf/2105.00132v2)

> Ethereum holds multiple billions of U.S. dollars in the form of Ether cryptocurrency and ERC-20 tokens, with millions of deployed smart contracts algorithmically operating these funds. Unsurprisingly, the security of Ethereum smart contracts has been under rigorous scrutiny. In recent years, numerous defense tools have been developed to detect different types of smart contract code vulnerabilities. When opportunities for exploiting code vulnerabilities diminish, the attackers start resorting to social engineering attacks, which aim to influence humans -- often the weakest link in the system. The only known class of social engineering attacks in Ethereum are honeypots, which plant hidden traps for attackers attempting to exploit existing vulnerabilities, thereby targeting only a small population of potential victims.   In this work, we explore the possibility and existence of new social engineering attacks beyond smart contract honeypots. We present two novel classes of Ethereum social engineering attacks - Address Manipulation and Homograph - and develop six zero-day social engineering attacks. To show how the attacks can be used in popular programming patterns, we conduct a case study of five popular smart contracts with combined market capitalization exceeding $29 billion, and integrate our attack patterns in their source codes without altering their existing functionality. Moreover, we show that these attacks remain dormant during the test phase but activate their malicious logic only at the final production deployment. We further analyze 85,656 open-source smart contracts, and discover that 1,027 of them can be used for the proposed social engineering attacks. We conduct a professional opinion survey with experts from seven smart contract auditing firms, corroborating that the exposed social engineering attacks bring a major threat to the smart contract systems.

</details>

<details>

<summary>2021-05-30 13:13:55 - On the benefits of representation regularization in invariance based domain generalization</summary>

- *Changjian Shui, Boyu Wang, Christian Gagné*

- `2105.14529v1` - [abs](http://arxiv.org/abs/2105.14529v1) - [pdf](http://arxiv.org/pdf/2105.14529v1)

> A crucial aspect in reliable machine learning is to design a deployable system in generalizing new related but unobserved environments. Domain generalization aims to alleviate such a prediction gap between the observed and unseen environments. Previous approaches commonly incorporated learning invariant representation for achieving good empirical performance. In this paper, we reveal that merely learning invariant representation is vulnerable to the unseen environment. To this end, we derive novel theoretical analysis to control the unseen test environment error in the representation learning, which highlights the importance of controlling the smoothness of representation. In practice, our analysis further inspires an efficient regularization method to improve the robustness in domain generalization. Our regularization is orthogonal to and can be straightforwardly adopted in existing domain generalization algorithms for invariant representation learning. Empirical results show that our algorithm outperforms the base versions in various dataset and invariance criteria.

</details>

<details>

<summary>2021-05-31 03:00:29 - A Bytecode-based Approach for Smart Contract Classification</summary>

- *Chaochen Shi, Yong Xiang, Robin Ram Mohan Doss, Jiangshan Yu, Keshav Sood, Longxiang Gao*

- `2106.15497v1` - [abs](http://arxiv.org/abs/2106.15497v1) - [pdf](http://arxiv.org/pdf/2106.15497v1)

> With the development of blockchain technologies, the number of smart contracts deployed on blockchain platforms is growing exponentially, which makes it difficult for users to find desired services by manual screening. The automatic classification of smart contracts can provide blockchain users with keyword-based contract searching and helps to manage smart contracts effectively. Current research on smart contract classification focuses on Natural Language Processing (NLP) solutions which are based on contract source code. However, more than 94% of smart contracts are not open-source, so the application scenarios of NLP methods are very limited. Meanwhile, NLP models are vulnerable to adversarial attacks. This paper proposes a classification model based on features from contract bytecode instead of source code to solve these problems. We also use feature selection and ensemble learning to optimize the model. Our experimental studies on over 3,300 real-world Ethereum smart contracts show that our model can classify smart contracts without source code and has better performance than baseline models. Our model also has good resistance to adversarial attacks compared with NLP-based models. In addition, our analysis reveals that account features used in many smart contract classification models have little effect on classification and can be excluded.

</details>

<details>

<summary>2021-05-31 06:42:21 - LiBRe: A Practical Bayesian Approach to Adversarial Detection</summary>

- *Zhijie Deng, Xiao Yang, Shizhen Xu, Hang Su, Jun Zhu*

- `2103.14835v2` - [abs](http://arxiv.org/abs/2103.14835v2) - [pdf](http://arxiv.org/pdf/2103.14835v2)

> Despite their appealing flexibility, deep neural networks (DNNs) are vulnerable against adversarial examples. Various adversarial defense strategies have been proposed to resolve this problem, but they typically demonstrate restricted practicability owing to unsurmountable compromise on universality, effectiveness, or efficiency. In this work, we propose a more practical approach, Lightweight Bayesian Refinement (LiBRe), in the spirit of leveraging Bayesian neural networks (BNNs) for adversarial detection. Empowered by the task and attack agnostic modeling under Bayes principle, LiBRe can endow a variety of pre-trained task-dependent DNNs with the ability of defending heterogeneous adversarial attacks at a low cost. We develop and integrate advanced learning techniques to make LiBRe appropriate for adversarial detection. Concretely, we build the few-layer deep ensemble variational and adopt the pre-training & fine-tuning workflow to boost the effectiveness and efficiency of LiBRe. We further provide a novel insight to realise adversarial detection-oriented uncertainty quantification without inefficiently crafting adversarial examples during training. Extensive empirical studies covering a wide range of scenarios verify the practicability of LiBRe. We also conduct thorough ablation studies to evidence the superiority of our modeling and learning strategies.

</details>

<details>

<summary>2021-05-31 08:01:41 - Securing IoT Devices by Exploiting Backscatter Propagation Signatures</summary>

- *Zhiqing Luo, Wei Wang, Qianyi Huang, Tao Jiang, Qian Zhang*

- `2105.14768v1` - [abs](http://arxiv.org/abs/2105.14768v1) - [pdf](http://arxiv.org/pdf/2105.14768v1)

> The low-power radio technologies open up many opportunities to facilitate Internet-of-Things (IoT) into our daily life, while their minimalist design also makes IoT devices vulnerable to many active attacks. Recent advances use an antenna array to extract fine-grained physical-layer signatures to identify the attackers, which adds burdens in terms of energy and hardware cost to IoT devices. In this paper, we present ShieldScatter, a lightweight system that attaches low-cost tags to single-antenna devices to shield the system from active attacks. The key insight of ShieldScatter is to intentionally create multi-path propagation signatures with the careful deployment of tags. These signatures can be used to construct a sensitive profile to identify the location of the signals' arrival, and thus detect the threat. In addition, we also design a tag-random scheme and a multiple receivers combination approach to detect a powerful attacker who has the strong priori knowledge of the legitimate user. We prototype ShieldScatter with USRPs and tags to evaluate our system in various environments. The results show that even when the powerful attacker is close to the legitimate device, ShieldScatter can mitigate 95% of attack attempts while triggering false alarms on just 7% of legitimate traffic.

</details>

<details>

<summary>2021-05-31 13:41:47 - Leveraging Mobile Phone Data for Migration Flows</summary>

- *Massimiliano Luca, Gianni Barlacchi, Nuria Oliver, Bruno Lepri*

- `2105.14956v1` - [abs](http://arxiv.org/abs/2105.14956v1) - [pdf](http://arxiv.org/pdf/2105.14956v1)

> Statistics on migration flows are often derived from census data, which suffer from intrinsic limitations, including costs and infrequent sampling. When censuses are used, there is typically a time gap - up to a few years - between the data collection process and the computation and publication of relevant statistics. This gap is a significant drawback for the analysis of a phenomenon that is continuously and rapidly changing. Alternative data sources, such as surveys and field observations, also suffer from reliability, costs, and scale limitations. The ubiquity of mobile phones enables an accurate and efficient collection of up-to-date data related to migration. Indeed, passively collected data by the mobile network infrastructure via aggregated, pseudonymized Call Detail Records (CDRs) is of great value to understand human migrations. Through the analysis of mobile phone data, we can shed light on the mobility patterns of migrants, detect spontaneous settlements and understand the daily habits, levels of integration, and human connections of such vulnerable social groups. This Chapter discusses the importance of leveraging mobile phone data as an alternative data source to gather precious and previously unavailable insights on various aspects of migration. Also, we highlight pending challenges that would need to be addressed before we can effectively benefit from the availability of mobile phone data to help make better decisions that would ultimately improve millions of people's lives.

</details>

<details>

<summary>2021-05-31 19:35:23 - GRAVITAS: Graphical Reticulated Attack Vectors for Internet-of-Things Aggregate Security</summary>

- *Jacob Brown, Tanujay Saha, Niraj K. Jha*

- `2106.00073v1` - [abs](http://arxiv.org/abs/2106.00073v1) - [pdf](http://arxiv.org/pdf/2106.00073v1)

> Internet-of-Things (IoT) and cyber-physical systems (CPSs) may consist of thousands of devices connected in a complex network topology. The diversity and complexity of these components present an enormous attack surface, allowing an adversary to exploit security vulnerabilities of different devices to execute a potent attack. Though significant efforts have been made to improve the security of individual devices in these systems, little attention has been paid to security at the aggregate level. In this article, we describe a comprehensive risk management system, called GRAVITAS, for IoT/CPS that can identify undiscovered attack vectors and optimize the placement of defenses within the system for optimal performance and cost. While existing risk management systems consider only known attacks, our model employs a machine learning approach to extrapolate undiscovered exploits, enabling us to identify attacks overlooked by manual penetration testing (pen-testing). The model is flexible enough to analyze practically any IoT/CPS and provide the system administrator with a concrete list of suggested defenses that can reduce system vulnerability at optimal cost. GRAVITAS can be employed by governments, companies, and system administrators to design secure IoT/CPS at scale, providing a quantitative measure of security and efficiency in a world where IoT/CPS devices will soon be ubiquitous.

</details>


## 2021-06

<details>

<summary>2021-06-01 05:27:51 - "Why wouldn't someone think of democracy as a target?": Security practices & challenges of people involved with U.S. political campaigns</summary>

- *Sunny Consolvo, Patrick Gage Kelley, Tara Matthews, Kurt Thomas, Lee Dunn, Elie Bursztein*

- `2106.00236v1` - [abs](http://arxiv.org/abs/2106.00236v1) - [pdf](http://arxiv.org/pdf/2106.00236v1)

> People who are involved with political campaigns face increased digital security threats from well-funded, sophisticated attackers, especially nation-states. Improving political campaign security is a vital part of protecting democracy. To identify campaign security issues, we conducted qualitative research with 28 participants across the U.S. political spectrum to understand the digital security practices, challenges, and perceptions of people involved in campaigns. A main, overarching finding is that a unique combination of threats, constraints, and work culture lead people involved with political campaigns to use technologies from across platforms and domains in ways that leave them--and democracy--vulnerable to security attacks. Sensitive data was kept in a plethora of personal and work accounts, with ad hoc adoption of strong passwords, two-factor authentication, encryption, and access controls. No individual company, committee, organization, campaign, or academic institution can solve the identified problems on their own. To this end, we provide an initial understanding of this complex problem space and recommendations for how a diverse group of experts can begin working together to improve security for political campaigns.

</details>

<details>

<summary>2021-06-01 14:29:57 - Discovering ePassport Vulnerabilities using Bisimilarity</summary>

- *Ross Horne, Sjouke Mauw*

- `2002.07309v5` - [abs](http://arxiv.org/abs/2002.07309v5) - [pdf](http://arxiv.org/pdf/2002.07309v5)

> We uncover privacy vulnerabilities in the ICAO 9303 standard implemented by ePassports worldwide. These vulnerabilities, confirmed by ICAO, enable an ePassport holder who recently passed through a checkpoint to be reidentified without opening their ePassport. This paper explains how bisimilarity was used to discover these vulnerabilities, which exploit the BAC protocol - the original ICAO 9303 standard ePassport authentication protocol - and remains valid for the PACE protocol, which improves on the security of BAC in the latest ICAO 9303 standards. In order to tackle such bisimilarity problems, we develop here a chain of methods for the applied $\pi$-calculus including a symbolic under-approximation of bisimilarity, called open bisimilarity, and a modal logic, called classical FM, for describing and certifying attacks. Evidence is provided to argue for a new scheme for specifying such unlinkability problems that more accurately reflects the capabilities of an attacker.

</details>

<details>

<summary>2021-06-01 21:18:23 - On using distributed representations of source code for the detection of C security vulnerabilities</summary>

- *David Coimbra, Sofia Reis, Rui Abreu, Corina Păsăreanu, Hakan Erdogmus*

- `2106.01367v1` - [abs](http://arxiv.org/abs/2106.01367v1) - [pdf](http://arxiv.org/pdf/2106.01367v1)

> This paper presents an evaluation of the code representation model Code2vec when trained on the task of detecting security vulnerabilities in C source code. We leverage the open-source library astminer to extract path-contexts from the abstract syntax trees of a corpus of labeled C functions. Code2vec is trained on the resulting path-contexts with the task of classifying a function as vulnerable or non-vulnerable. Using the CodeXGLUE benchmark, we show that the accuracy of Code2vec for this task is comparable to simple transformer-based methods such as pre-trained RoBERTa, and outperforms more naive NLP-based methods. We achieved an accuracy of 61.43% while maintaining low computational requirements relative to larger models.

</details>

<details>

<summary>2021-06-01 23:53:49 - A Continuous Liveness Detection for Voice Authentication on Smart Devices</summary>

- *Linghan Zhang, Jie Yang*

- `2106.00859v1` - [abs](http://arxiv.org/abs/2106.00859v1) - [pdf](http://arxiv.org/pdf/2106.00859v1)

> Voice biometrics is drawing increasing attention as it is a promising alternative to legacy passwords for user authentication. Recently, a growing body of work shows that voice biometrics is vulnerable to spoofing through replay attacks, where an adversary tries to spoof voice authentication systems by using a pre-recorded voice sample collected from a genuine user. To this end, we propose VoiceGesture, a liveness detection solution for voice authentication on smart devices such as smartphones and smart speakers. VoiceGesture detects a live user by leveraging both the unique articulatory gesture of the user when speaking a passphrase and the audio hardware advances on these smart devices. Specifically, our system re-uses a pair of built-in speaker and microphone on a smart device as a Doppler radar, which transmits a high-frequency acoustic sound from the speaker and listens to the reflections at the microphone when a user speaks a passphrase. Then we extract Doppler shifts resulted from the user's articulatory gestures for liveness detection. VoiceGesture is practical as it requires neither cumbersome operations nor additional hardware but a speaker and a microphone commonly available on smart devices that support voice input. Our experimental evaluation with 21 participants and different smart devices shows that VoiceGesture achieves over 99% and around 98% detection accuracy for text-dependent and text-independent liveness detection, respectively. Results also show that VoiceGesture is robust to different device placements, low audio sampling frequency, and supports medium range liveness detection on smart speakers in various use scenarios like smart homes and smart vehicles.

</details>

<details>

<summary>2021-06-02 00:32:40 - A Continuous Liveness Detection System for Text-independent Speaker Verification</summary>

- *Linghan Zhang, Jie Yang*

- `2106.01840v1` - [abs](http://arxiv.org/abs/2106.01840v1) - [pdf](http://arxiv.org/pdf/2106.01840v1)

> Voice authentication is drawing increasing attention and becomes an attractive alternative to passwords for mobile authentication. Recent advances in mobile technology further accelerate the adoption of voice biometrics in an array of diverse mobile applications. However, recent studies show that voice authentication is vulnerable to replay attacks, where an adversary can spoof a voice authentication system using a pre-recorded voice sample collected from the victim. In this paper, we propose VoiceLive, a liveness detection system for both text-dependent and text-independent voice authentication on smartphones. VoiceLive detects a live user by leveraging the user's unique vocal system and the stereo recording of smartphones. In particular, utilizing the built-in gyroscope, loudspeaker, and microphone, VoiceLive first measures the smartphone's distance and angle from the user, then it captures the position-specific time-difference-of-arrival (TDoA) changes in a sequence of phoneme sounds to the two microphones of the phone, and uses such unique TDoA dynamic which doesn't exist under replay attacks for liveness detection. VoiceLive is practical as it doesn't require additional hardware but two-channel stereo recording that is supported by virtually all smartphones. Our experimental evaluation with 12 participants and different types of phones shows that VoiceLive achieves over 99% detection accuracy at around 1% Equal Error Rate (EER) on the text-dependent system and around 99% accuracy and 2% EER on the text-independent one. Results also show that VoiceLive is robust to different phone positions, i.e. the user is free to hold the smartphone with distinct distances and angles.

</details>

<details>

<summary>2021-06-02 03:07:41 - Enhanced Universal Dependency Parsing with Second-Order Inference and Mixture of Training Data</summary>

- *Xinyu Wang, Yong Jiang, Kewei Tu*

- `2006.01414v3` - [abs](http://arxiv.org/abs/2006.01414v3) - [pdf](http://arxiv.org/pdf/2006.01414v3)

> This paper presents the system used in our submission to the \textit{IWPT 2020 Shared Task}. Our system is a graph-based parser with second-order inference. For the low-resource Tamil corpus, we specially mixed the training data of Tamil with other languages and significantly improved the performance of Tamil. Due to our misunderstanding of the submission requirements, we submitted graphs that are not connected, which makes our system only rank \textbf{6th} over 10 teams. However, after we fixed this problem, our system is 0.6 ELAS higher than the team that ranked \textbf{1st} in the official results.

</details>

<details>

<summary>2021-06-02 04:41:17 - Fidelity and Privacy of Synthetic Medical Data</summary>

- *Ofer Mendelevitch, Michael D. Lesh*

- `2101.08658v2` - [abs](http://arxiv.org/abs/2101.08658v2) - [pdf](http://arxiv.org/pdf/2101.08658v2)

> The digitization of medical records ushered in a new era of big data to clinical science, and with it the possibility that data could be shared, to multiply insights beyond what investigators could abstract from paper records. The need to share individual-level medical data to accelerate innovation in precision medicine continues to grow, and has never been more urgent, as scientists grapple with the COVID-19 pandemic. However, enthusiasm for the use of big data has been tempered by a fully appropriate concern for patient autonomy and privacy. That is, the ability to extract private or confidential information about an individual, in practice, renders it difficult to share data, since significant infrastructure and data governance must be established before data can be shared. Although HIPAA provided de-identification as an approved mechanism for data sharing, linkage attacks were identified as a major vulnerability. A variety of mechanisms have been established to avoid leaking private information, such as field suppression or abstraction, strictly limiting the amount of information that can be shared, or employing mathematical techniques such as differential privacy. Another approach, which we focus on here, is creating synthetic data that mimics the underlying data. For synthetic data to be a useful mechanism in support of medical innovation and a proxy for real-world evidence, one must demonstrate two properties of the synthetic dataset: (1) any analysis on the real data must be matched by analysis of the synthetic data (statistical fidelity) and (2) the synthetic data must preserve privacy, with minimal risk of re-identification (privacy guarantee). In this paper we propose a framework for quantifying the statistical fidelity and privacy preservation properties of synthetic datasets and demonstrate these metrics for synthetic data generated by Syntegra technology.

</details>

<details>

<summary>2021-06-02 05:02:43 - Learning by Semantic Similarity Makes Abstractive Summarization Better</summary>

- *Wonjin Yoon, Yoon Sun Yeo, Minbyul Jeong, Bong-Jun Yi, Jaewoo Kang*

- `2002.07767v2` - [abs](http://arxiv.org/abs/2002.07767v2) - [pdf](http://arxiv.org/pdf/2002.07767v2)

> By harnessing pre-trained language models, summarization models had rapid progress recently. However, the models are mainly assessed by automatic evaluation metrics such as ROUGE. Although ROUGE is known for having a positive correlation with human evaluation scores, it has been criticized for its vulnerability and the gap between actual qualities. In this paper, we compare the generated summaries from recent LM, BART, and the reference summaries from a benchmark dataset, CNN/DM, using a crowd-sourced human evaluation metric. Interestingly, model-generated summaries receive higher scores relative to reference summaries. Stemming from our experimental results, we first argue the intrinsic characteristics of the CNN/DM dataset, the progress of pre-trained language models, and their ability to generalize on the training data. Finally, we share our insights into the model-generated summaries and presents our thought on learning methods for abstractive summarization.

</details>

<details>

<summary>2021-06-02 09:35:41 - SpectralDefense: Detecting Adversarial Attacks on CNNs in the Fourier Domain</summary>

- *Paula Harder, Franz-Josef Pfreundt, Margret Keuper, Janis Keuper*

- `2103.03000v2` - [abs](http://arxiv.org/abs/2103.03000v2) - [pdf](http://arxiv.org/pdf/2103.03000v2)

> Despite the success of convolutional neural networks (CNNs) in many computer vision and image analysis tasks, they remain vulnerable against so-called adversarial attacks: Small, crafted perturbations in the input images can lead to false predictions. A possible defense is to detect adversarial examples. In this work, we show how analysis in the Fourier domain of input images and feature maps can be used to distinguish benign test samples from adversarial images. We propose two novel detection methods: Our first method employs the magnitude spectrum of the input images to detect an adversarial attack. This simple and robust classifier can successfully detect adversarial perturbations of three commonly used attack methods. The second method builds upon the first and additionally extracts the phase of Fourier coefficients of feature-maps at different layers of the network. With this extension, we are able to improve adversarial detection rates compared to state-of-the-art detectors on five different attack methods.

</details>

<details>

<summary>2021-06-02 10:02:47 - Closeness and Uncertainty Aware Adversarial Examples Detection in Adversarial Machine Learning</summary>

- *Omer Faruk Tuna, Ferhat Ozgur Catak, M. Taner Eskil*

- `2012.06390v2` - [abs](http://arxiv.org/abs/2012.06390v2) - [pdf](http://arxiv.org/pdf/2012.06390v2)

> While state-of-the-art Deep Neural Network (DNN) models are considered to be robust to random perturbations, it was shown that these architectures are highly vulnerable to deliberately crafted perturbations, albeit being quasi-imperceptible. These vulnerabilities make it challenging to deploy DNN models in security-critical areas. In recent years, many research studies have been conducted to develop new attack methods and come up with new defense techniques that enable more robust and reliable models. In this work, we explore and assess the usage of different type of metrics for detecting adversarial samples. We first leverage the usage of moment-based predictive uncertainty estimates of a DNN classifier obtained using Monte-Carlo Dropout Sampling. And we also introduce a new method that operates in the subspace of deep features extracted by the model. We verified the effectiveness of our approach on a range of standard datasets like MNIST (Digit), MNIST (Fashion) and CIFAR-10. Our experiments show that these two different approaches complement each other, and the combined usage of all the proposed metrics yields up to 99 \% ROC-AUC scores regardless of the attack algorithm.

</details>

<details>

<summary>2021-06-02 13:44:59 - Controlled Update of Software Components using Concurrent Exection of Patched and Unpatched Versions</summary>

- *Stjepan Groš, Ivan Kovačević, Ivan Dujmić, Matej Petrinović*

- `2106.01154v1` - [abs](http://arxiv.org/abs/2106.01154v1) - [pdf](http://arxiv.org/pdf/2106.01154v1)

> Software patching is a common method of removing vulnerabilities in software components to make IT systems more secure. However, there are many cases where software patching is not possible due to the critical nature of the application, especially when the vendor providing the application guarantees correct operation only in a specific configuration. In this paper, we propose a method to solve this problem. The idea is to run unpatched and patched application instances concurrently, with the unpatched one having complete control and the output of the patched one being used only for comparison, to watch for differences that are consequences of introduced bugs. To test this idea, we developed a system that allows us to run web applications in parallel and tested three web applications. The experiments have shown that the idea is promising for web applications from the technical side. Furthermore, we discuss the potential limitations of this system and the idea in general, how long two instances should run in order to be able to claim with some probability that the patched version has not introduced any new bugs, other potential use cases of the proposed system where two application instances run concurrently, and finally the potential uses of this system with different types of applications, such as SCADA systems.

</details>

<details>

<summary>2021-06-02 15:43:12 - Phoenix: A Formally Verified Regenerating Vault</summary>

- *Uri Kirstein, Shelly Grossman, Michael Mirkin, James Wilcox, Ittay Eyal, Mooly Sagiv*

- `2106.01240v1` - [abs](http://arxiv.org/abs/2106.01240v1) - [pdf](http://arxiv.org/pdf/2106.01240v1)

> An attacker that gains access to a cryptocurrency user's private keys can perform any operation in her stead. Due to the decentralized nature of most cryptocurrencies, no entity can revert those operations. This is a central challenge for decentralized systems, illustrated by numerous high-profile heists. Vault contracts reduce this risk by introducing artificial delay on operations, allowing abortion by the contract owner during the delay. However, the theft of a key still renders the vault unusable and puts funds at risk.   We introduce Phoenix, a novel contract architecture that allows the user to restore its security properties after key loss. Phoenix takes advantage of users' ability to store keys in easily-available but less secure storage (tier-two) as well as more secure storage that is harder to access (tier-one). Unlike previous solutions, the user can restore Phoenix security after the theft of tier-two keys and does not lose funds despite losing keys in either tier. Phoenix also introduces a mechanism to reduce the damage an attacker can cause in case of a tier-one compromise.   We formally specify Phoenix's required behavior and provide a prototype implementation of Phoenix as an Ethereum contract. Since such an implementation is highly sensitive and vulnerable to subtle bugs, we apply a formal verification tool to prove specific code properties and identify faults. We highlight a bug identified by the tool that could be exploited by an attacker to compromise Phoenix. After fixing the bug, the tool proved the low-level executable code's correctness.

</details>

<details>

<summary>2021-06-02 15:46:27 - A Privacy-Preserving and Trustable Multi-agent Learning Framework</summary>

- *Anudit Nagar, Cuong Tran, Ferdinando Fioretto*

- `2106.01242v1` - [abs](http://arxiv.org/abs/2106.01242v1) - [pdf](http://arxiv.org/pdf/2106.01242v1)

> Distributed multi-agent learning enables agents to cooperatively train a model without requiring to share their datasets. While this setting ensures some level of privacy, it has been shown that, even when data is not directly shared, the training process is vulnerable to privacy attacks including data reconstruction and model inversion attacks. Additionally, malicious agents that train on inverted labels or random data, may arbitrarily weaken the accuracy of the global model. This paper addresses these challenges and presents Privacy-preserving and trustable Distributed Learning (PT-DL), a fully decentralized framework that relies on Differential Privacy to guarantee strong privacy protections of the agents' data, and Ethereum smart contracts to ensure trustability. The paper shows that PT-DL is resilient up to a 50% collusion attack, with high probability, in a malicious trust model and the experimental evaluation illustrates the benefits of the proposed model as a privacy-preserving and trustable distributed multi-agent learning system on several classification tasks.

</details>

<details>

<summary>2021-06-03 01:45:48 - PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack</summary>

- *Alexander Matyasko, Lap-Pui Chau*

- `2106.01538v1` - [abs](http://arxiv.org/abs/2106.01538v1) - [pdf](http://arxiv.org/pdf/2106.01538v1)

> State-of-the-art deep neural networks are sensitive to small input perturbations. Since the discovery of this intriguing vulnerability, many defence methods have been proposed that attempt to improve robustness to adversarial noise. Fast and accurate attacks are required to compare various defence methods. However, evaluating adversarial robustness has proven to be extremely challenging. Existing norm minimisation adversarial attacks require thousands of iterations (e.g. Carlini & Wagner attack), are limited to the specific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results (e.g. Brendel & Bethge attack). On the other hand, PGD attack, which is fast, general and accurate, ignores the norm minimisation penalty and solves a simpler perturbation-constrained problem. In this work, we introduce a fast, general and accurate adversarial attack that optimises the original non-convex constrained minimisation problem. We interpret optimising the Lagrangian of the adversarial attack optimisation problem as a two-player game: the first player minimises the Lagrangian wrt the adversarial noise; the second player maximises the Lagrangian wrt the regularisation penalty. Our attack algorithm simultaneously optimises primal and dual variables to find the minimal adversarial perturbation. In addition, for non-smooth $l_p$-norm minimisation, such as $l_{\infty}$-, $l_1$-, and $l_0$-norms, we introduce primal-dual proximal gradient descent attack. We show in the experiments that our attack outperforms current state-of-the-art $l_{\infty}$-, $l_2$-, $l_1$-, and $l_0$-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against unregularised and adversarially trained models.

</details>

<details>

<summary>2021-06-03 06:25:04 - Imperceptible Adversarial Examples for Fake Image Detection</summary>

- *Quanyu Liao, Yuezun Li, Xin Wang, Bin Kong, Bin Zhu, Siwei Lyu, Youbing Yin, Qi Song, Xi Wu*

- `2106.01615v1` - [abs](http://arxiv.org/abs/2106.01615v1) - [pdf](http://arxiv.org/pdf/2106.01615v1)

> Fooling people with highly realistic fake images generated with Deepfake or GANs brings a great social disturbance to our society. Many methods have been proposed to detect fake images, but they are vulnerable to adversarial perturbations -- intentionally designed noises that can lead to the wrong prediction. Existing methods of attacking fake image detectors usually generate adversarial perturbations to perturb almost the entire image. This is redundant and increases the perceptibility of perturbations. In this paper, we propose a novel method to disrupt the fake image detection by determining key pixels to a fake image detector and attacking only the key pixels, which results in the $L_0$ and the $L_2$ norms of adversarial perturbations much less than those of existing works. Experiments on two public datasets with three fake image detectors indicate that our proposed method achieves state-of-the-art performance in both white-box and black-box attacks.

</details>

<details>

<summary>2021-06-03 12:37:58 - Frontrunner Jones and the Raiders of the Dark Forest: An Empirical Study of Frontrunning on the Ethereum Blockchain</summary>

- *Christof Ferreira Torres, Ramiro Camino, Radu State*

- `2102.03347v2` - [abs](http://arxiv.org/abs/2102.03347v2) - [pdf](http://arxiv.org/pdf/2102.03347v2)

> Ethereum prospered the inception of a plethora of smart contract applications, ranging from gambling games to decentralized finance. However, Ethereum is also considered a highly adversarial environment, where vulnerable smart contracts will eventually be exploited. Recently, Ethereum's pool of pending transaction has become a far more aggressive environment. In the hope of making some profit, attackers continuously monitor the transaction pool and try to frontrun their victims' transactions by either displacing or suppressing them, or strategically inserting their transactions. This paper aims to shed some light into what is known as a dark forest and uncover these predators' actions. We present a methodology to efficiently measure the three types of frontrunning: displacement, insertion, and suppression. We perform a large-scale analysis on more than 11M blocks and identify almost 200K attacks with an accumulated profit of 18.41M USD for the attackers, providing evidence that frontrunning is both, lucrative and a prevalent issue.

</details>

<details>

<summary>2021-06-03 23:59:39 - Distributed Symbolic Execution using Test-Depth Partitioning</summary>

- *Shikhar Singh, Sarfraz Khurshid*

- `2106.02179v1` - [abs](http://arxiv.org/abs/2106.02179v1) - [pdf](http://arxiv.org/pdf/2106.02179v1)

> Symbolic execution is a classic technique for systematic bug finding, which has seen many applications in recent years but remains hard to scale. Recent work introduced ranged symbolic execution to distribute the symbolic execution task among different workers with minimal communication overhead using test inputs. However, each worker was restricted to perform only a depth-first search. This paper introduces a new approach to ranging, called test-depth partitioning, that allows the workers to employ different search strategies without compromising the completeness of the overall search. Experimental results show that the proposed approach provides a more flexible ranging solution for distributed symbolic execution. The core idea behind test-depth partitioning is to use a test-depth pair to define a region in the execution space. Such a pair represents a partial path or a prefix, and it obviates the need for complete tests to determine boundaries as was the case in the previous ranging scheme. Moreover, different workers have the freedom to select the search strategy of choice without affecting the functioning of the overall system. Test-depth partitioning is implemented using KLEE, a well-known symbolic execution tool. The preliminary results show that the proposed scheme can prove to be an efficient tool to speed up symbolic execution.

</details>

<details>

<summary>2021-06-04 09:44:06 - Efficient Predictive Monitoring of Linear Time-Invariant Systems Under Stealthy Attacks</summary>

- *Mazen Azzam, Liliana Pasquale, Gregory Provan, Bashar Nuseibeh*

- `2106.02378v1` - [abs](http://arxiv.org/abs/2106.02378v1) - [pdf](http://arxiv.org/pdf/2106.02378v1)

> Attacks on Industrial Control Systems (ICS) can lead to significant physical damage. While offline safety and security assessments can provide insight into vulnerable system components, they may not account for stealthy attacks designed to evade anomaly detectors during long operational transients. In this paper, we propose a predictive online monitoring approach to check the safety of the system under potential stealthy attacks. Specifically, we adapt previous results in reachability analysis for attack impact assessment to provide an efficient algorithm for online safety monitoring for Linear Time-Invariant (LTI) systems. The proposed approach relies on an offline computation of symbolic reachable sets in terms of the estimated physical state of the system. These sets are then instantiated online, and safety checks are performed by leveraging ideas from ellipsoidal calculus. We illustrate and evaluate our approach using the Tennessee-Eastman process. We also compare our approach with the baseline monitoring approaches proposed in previous work and assess its efficiency and scalability. Our evaluation results demonstrate that our approach can predict in a timely manner if a false data injection attack will be able to cause damage, while remaining undetected. Thus, our approach can be used to provide operators with real-time early warnings about stealthy attacks.

</details>

<details>

<summary>2021-06-04 17:33:10 - Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports</summary>

- *Aleksandra Edwards, David Rogers, Jose Camacho-Collados, Hélène de Ribaupierre, Alun Preece*

- `2010.14584v3` - [abs](http://arxiv.org/abs/2010.14584v3) - [pdf](http://arxiv.org/pdf/2010.14584v3)

> The task of text and sentence classification is associated with the need for large amounts of labelled training data. The acquisition of high volumes of labelled datasets can be expensive or unfeasible, especially for highly-specialised domains for which documents are hard to obtain. Research on the application of supervised classification based on small amounts of training data is limited. In this paper, we address the combination of state-of-the-art deep learning and classification methods and provide an insight into what combination of methods fit the needs of small, domain-specific, and terminologically-rich corpora. We focus on a real-world scenario related to a collection of safeguarding reports comprising learning experiences and reflections on tackling serious incidents involving children and vulnerable adults. The relatively small volume of available reports and their use of highly domain-specific terminology makes the application of automated approaches difficult. We focus on the problem of automatically identifying the main themes in a safeguarding report using supervised classification approaches. Our results show the potential of deep learning models to simulate subject-expert behaviour even for complex tasks with limited labelled data.

</details>

<details>

<summary>2021-06-05 02:32:50 - PEEL: A Provable Removal Attack on Deep Hiding</summary>

- *Tao Xiang, Hangcheng Liu, Shangwei Guo, Tianwei Zhang*

- `2106.02779v1` - [abs](http://arxiv.org/abs/2106.02779v1) - [pdf](http://arxiv.org/pdf/2106.02779v1)

> Deep hiding, embedding images into another using deep neural networks, has shown its great power in increasing the message capacity and robustness. In this paper, we conduct an in-depth study of state-of-the-art deep hiding schemes and analyze their hidden vulnerabilities. Then, according to our observations and analysis, we propose a novel ProvablE rEmovaL attack (PEEL) using image inpainting to remove secret images from containers without any prior knowledge about the deep hiding scheme. We also propose a systemic methodology to improve the efficiency and image quality of PEEL by carefully designing a removal strategy and fully utilizing the visual information of containers. Extensive evaluations show our attacks can completely remove secret images and has negligible impact on the quality of containers.

</details>

<details>

<summary>2021-06-05 10:22:15 - Manipulating SGD with Data Ordering Attacks</summary>

- *Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A. Erdogdu, Ross Anderson*

- `2104.09667v2` - [abs](http://arxiv.org/abs/2104.09667v2) - [pdf](http://arxiv.org/pdf/2104.09667v2)

> Machine learning is vulnerable to a wide variety of attacks. It is now well understood that by changing the underlying data distribution, an adversary can poison the model trained with it or introduce backdoors. In this paper we present a novel class of training-time attacks that require no changes to the underlying dataset or model architecture, but instead only change the order in which data are supplied to the model. In particular, we find that the attacker can either prevent the model from learning, or poison it to learn behaviours specified by the attacker. Furthermore, we find that even a single adversarially-ordered epoch can be enough to slow down model learning, or even to reset all of the learning progress. Indeed, the attacks presented here are not specific to the model or dataset, but rather target the stochastic nature of modern learning procedures. We extensively evaluate our attacks on computer vision and natural language benchmarks to find that the adversary can disrupt model training and even introduce backdoors.

</details>

<details>

<summary>2021-06-06 03:30:29 - Constrained episodic reinforcement learning in concave-convex and knapsack settings</summary>

- *Kianté Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, Wen Sun*

- `2006.05051v2` - [abs](http://arxiv.org/abs/2006.05051v2) - [pdf](http://arxiv.org/pdf/2006.05051v2)

> We propose an algorithm for tabular episodic reinforcement learning with constraints. We provide a modular analysis with strong theoretical guarantees for settings with concave rewards and convex constraints, and for settings with hard constraints (knapsacks). Most of the previous work in constrained reinforcement learning is limited to linear constraints, and the remaining work focuses on either the feasibility question or settings with a single episode. Our experiments demonstrate that the proposed algorithm significantly outperforms these approaches in existing constrained episodic environments.

</details>

<details>

<summary>2021-06-06 11:59:27 - A Primer on Multi-Neuron Relaxation-based Adversarial Robustness Certification</summary>

- *Kevin Roth*

- `2106.03099v1` - [abs](http://arxiv.org/abs/2106.03099v1) - [pdf](http://arxiv.org/pdf/2106.03099v1)

> The existence of adversarial examples poses a real danger when deep neural networks are deployed in the real world. The go-to strategy to quantify this vulnerability is to evaluate the model against specific attack algorithms. This approach is however inherently limited, as it says little about the robustness of the model against more powerful attacks not included in the evaluation. We develop a unified mathematical framework to describe relaxation-based robustness certification methods, which go beyond adversary-specific robustness evaluation and instead provide provable robustness guarantees against attacks by any adversary. We discuss the fundamental limitations posed by single-neuron relaxations and show how the recent ``k-ReLU'' multi-neuron relaxation framework of Singh et al. (2019) obtains tighter correlation-aware activation bounds by leveraging additional relational constraints among groups of neurons. Specifically, we show how additional pre-activation bounds can be mapped to corresponding post-activation bounds and how they can in turn be used to obtain tighter robustness certificates. We also present an intuitive way to visualize different relaxation-based certification methods. By approximating multiple non-linearities jointly instead of separately, the k-ReLU method is able to bypass the convex barrier imposed by single neuron relaxations.

</details>

<details>

<summary>2021-06-06 14:00:38 - SPI: Automated Identification of Security Patches via Commits</summary>

- *Yaqin Zhou, Jing Kai Siow, Chenyu Wang, Shangqing Liu, Yang Liu*

- `2105.14565v2` - [abs](http://arxiv.org/abs/2105.14565v2) - [pdf](http://arxiv.org/pdf/2105.14565v2)

> Security patches in open-source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyberattacks. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open-source libraries that are heavily relied on by developers. An extensive security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researchers, e.g., aiding in vulnerability research. To curate security patches including undisclosed patches at a large scale and low cost, we propose a deep neural-network-based approach built upon commits of open-source repositories. We build security patch datasets that include 38,291 security-related commits and 1,045 CVE patches from four C libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security-related. We devise a deep learning-based security patch identification system that consists of two neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset; and one code-revision neural network that takes code before and after revision and learns the distinction on the statement level. Our evaluation results show that our system outperforms SVM and K-fold stacking algorithm, achieving as high as 87.93% F1-score and precision of 86.24%. We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionality. Our experiment results and observation proved that our approach identifies security patches effectively among open-sourced projects.

</details>

<details>

<summary>2021-06-06 18:43:28 - Adversarial Classification of the Attacks on Smart Grids Using Game Theory and Deep Learning</summary>

- *Kian Hamedani, Lingjia Liu, Jithin Jagannath, Yang, Yi*

- `2106.03209v1` - [abs](http://arxiv.org/abs/2106.03209v1) - [pdf](http://arxiv.org/pdf/2106.03209v1)

> Smart grids are vulnerable to cyber-attacks. This paper proposes a game-theoretic approach to evaluate the variations caused by an attacker on the power measurements. Adversaries can gain financial benefits through the manipulation of the meters of smart grids. On the other hand, there is a defender that tries to maintain the accuracy of the meters. A zero-sum game is used to model the interactions between the attacker and defender. In this paper, two different defenders are used and the effectiveness of each defender in different scenarios is evaluated. Multi-layer perceptrons (MLPs) and traditional state estimators are the two defenders that are studied in this paper. The utility of the defender is also investigated in adversary-aware and adversary-unaware situations. Our simulations suggest that the utility which is gained by the adversary drops significantly when the MLP is used as the defender. It will be shown that the utility of the defender is variant in different scenarios, based on the defender that is being used. In the end, we will show that this zero-sum game does not yield a pure strategy, and the mixed strategy of the game is calculated.

</details>

<details>

<summary>2021-06-06 21:59:49 - Understand and Improve Contrastive Learning Methods for Visual Representation: A Review</summary>

- *Ran Liu*

- `2106.03259v1` - [abs](http://arxiv.org/abs/2106.03259v1) - [pdf](http://arxiv.org/pdf/2106.03259v1)

> Traditional supervised learning methods are hitting a bottleneck because of their dependency on expensive manually labeled data and their weaknesses such as limited generalization ability and vulnerability to adversarial attacks. A promising alternative, self-supervised learning, as a type of unsupervised learning, has gained popularity because of its potential to learn effective data representations without manual labeling. Among self-supervised learning algorithms, contrastive learning has achieved state-of-the-art performance in several fields of research. This literature review aims to provide an up-to-date analysis of the efforts of researchers to understand the key components and the limitations of self-supervised learning.

</details>

<details>

<summary>2021-06-07 07:14:39 - Mutualized oblivious DNS ($μ$ODNS): Hiding a tree in the wild forest</summary>

- *Jun Kurihara, Takeshi Kubo*

- `2104.13785v3` - [abs](http://arxiv.org/abs/2104.13785v3) - [pdf](http://arxiv.org/pdf/2104.13785v3)

> The traditional Domain Name System (DNS) lacks fundamental features of security and privacy in its design. As concerns of privacy increased on the Internet, security and privacy enhancements of DNS have been actively investigated and deployed. Specially for user's privacy in DNS queries, several relay-based anonymization schemes have been recently introduced, however, they are vulnerable to the collusion of a relay with a full-service resolver, i.e., identities of users cannot be hidden to the resolver. This paper introduces a new concept of a multiple-relay-based DNS for user anonymity in DNS queries, called the mutualized oblivious DNS ($\mu$ODNS), by extending the concept of existing relay-based schemes. The $\mu$ODNS introduces a small and reasonable assumption that each user has at least one trusted/dedicated relay in a network and mutually shares the dedicated one with others. The user just sets the dedicated one as his next-hop, first relay, conveying his queries to the resolver, and randomly chooses its $0$ or more subsequent relays shared by other entities. Under this small assumption, the user's identity is concealed to a target resolver in the $\mu$ODNS even if a certain (unknown) subset of relays collude with the resolver. That is, in $\mu$ODNS, users can preserve their privacy and anonymity just by paying a small cost of sharing its resource. Moreover, we present a PoC implementation of $\mu$ODNS that is publicly available on the Internet. We also show that by measurement of round-trip-time for queries, and our PoC implementation of $\mu$ODNS achieves the performance comparable to existing relay-based schemes.

</details>

<details>

<summary>2021-06-07 10:31:42 - The Closer You Look, The More You Learn: A Grey-box Approach to Protocol State Machine Learning</summary>

- *Chris McMahon Stone, Sam L. Thomas, Mathy Vanhoef, James Henderson, Nicolas Bailluet, Tom Chothia*

- `2106.02623v2` - [abs](http://arxiv.org/abs/2106.02623v2) - [pdf](http://arxiv.org/pdf/2106.02623v2)

> In this paper, we propose a new approach to infer state machine models from protocol implementations. Our method, STATEINSPECTOR, learns protocol states by using novel program analyses to combine observations of run-time memory and I/O. It requires no access to source code and only lightweight execution monitoring of the implementation under test. We demonstrate and evaluate STATEINSPECTOR's effectiveness on numerous TLS and WPA/2 implementations. In the process, we show STATEINSPECTOR enables deeper state discovery, increased learning efficiency, and more insightful post-mortem analyses than existing approaches. Further to improved learning, our method led us to discover several concerning deviations from the standards and a high impact vulnerability in a prominent Wi-Fi implementation.

</details>

<details>

<summary>2021-06-07 13:41:45 - Adversarial Attack and Defense in Deep Ranking</summary>

- *Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Nanning Zheng, Gang Hua*

- `2106.03614v1` - [abs](http://arxiv.org/abs/2106.03614v1) - [pdf](http://arxiv.org/pdf/2106.03614v1)

> Deep Neural Network classifiers are vulnerable to adversarial attack, where an imperceptible perturbation could result in misclassification. However, the vulnerability of DNN-based image ranking systems remains under-explored. In this paper, we propose two attacks against deep ranking systems, i.e., Candidate Attack and Query Attack, that can raise or lower the rank of chosen candidates by adversarial perturbations. Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like objective function is designed to obtain the optimal perturbation. Conversely, an anti-collapse triplet defense is proposed to improve the ranking model robustness against all proposed attacks, where the model learns to prevent the positive and negative samples being pulled close to each other by adversarial attack. To comprehensively measure the empirical adversarial robustness of a ranking model with our defense, we propose an empirical robustness score, which involves a set of representative attacks against ranking models. Our adversarial ranking attacks and defenses are evaluated on MNIST, Fashion-MNIST, CUB200-2011, CARS196 and Stanford Online Products datasets. Experimental results demonstrate that a typical deep ranking system can be effectively compromised by our attacks. Nevertheless, our defense can significantly improve the ranking system robustness, and simultaneously mitigate a wide range of attacks.

</details>

<details>

<summary>2021-06-07 17:16:12 - 3DB: A Framework for Debugging Computer Vision Models</summary>

- *Guillaume Leclerc, Hadi Salman, Andrew Ilyas, Sai Vemprala, Logan Engstrom, Vibhav Vineet, Kai Xiao, Pengchuan Zhang, Shibani Santurkar, Greg Yang, Ashish Kapoor, Aleksander Madry*

- `2106.03805v1` - [abs](http://arxiv.org/abs/2106.03805v1) - [pdf](http://arxiv.org/pdf/2106.03805v1)

> We introduce 3DB: an extendable, unified framework for testing and debugging vision models using photorealistic simulation. We demonstrate, through a wide range of use cases, that 3DB allows users to discover vulnerabilities in computer vision systems and gain insights into how models make decisions. 3DB captures and generalizes many robustness analyses from prior work, and enables one to study their interplay. Finally, we find that the insights generated by the system transfer to the physical world.   We are releasing 3DB as a library (https://github.com/3db/3db) alongside a set of example analyses, guides, and documentation: https://3db.github.io/3db/ .

</details>

<details>

<summary>2021-06-08 03:41:13 - Multi-agent Modeling of Hazard-Household-Infrastructure Nexus for Equitable Resilience Assessment</summary>

- *Amir Esmalian, Wanqiu Wang, Ali Mostafavi*

- `2106.03160v2` - [abs](http://arxiv.org/abs/2106.03160v2) - [pdf](http://arxiv.org/pdf/2106.03160v2)

> To enable integrating social equity considerations in infrastructure resilience assessments, this study created a new computational multi-agent simulation model which enables integrated assessment of hazard, infrastructure system, and household elements and their interactions. With a focus on hurricane-induced power outages, the model consists of three elements: 1) the hazard component simulates exposure of the community to a hurricane with varying intensity levels; 2) the physical infrastructure component simulates the power network and its probabilistic failures and restoration under different hazard scenarios; and 3) the households component captures the dynamic processes related to preparation, information seeking, and response actions of households facing hurricane-induced power outages. We used empirical data from household surveys in conjunction with theoretical decision-making models to abstract and simulate the underlying mechanisms affecting experienced hardship of households. The multi-agent simulation model was then tested in the context of Harris County, Texas, and verified and validated using empirical results from Hurricane Harvey in 2017. Then, the model was used to examine effects of different factors such as forewarning durations, social network types, and restoration and resource allocation strategies on reducing the societal impacts of service disruptions in an equitable manner. The results show that improving the restoration prioritization strategy to focus on vulnerable populations is an effective approach, especially during high-intensity events. The results show the capability of the proposed computational model for capturing the dynamic and complex interactions in the nexus of humans, hazards, and infrastructure systems to better integrate human-centric aspects in resilience planning and into assessment of infrastructure systems in disasters.

</details>

<details>

<summary>2021-06-08 07:25:11 - Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors</summary>

- *Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu, Xingang Shi, Xia Yin*

- `2005.07519v4` - [abs](http://arxiv.org/abs/2005.07519v4) - [pdf](http://arxiv.org/pdf/2005.07519v4)

> Machine learning (ML), especially deep learning (DL) techniques have been increasingly used in anomaly-based network intrusion detection systems (NIDS). However, ML/DL has shown to be extremely vulnerable to adversarial attacks, especially in such security-sensitive systems. Many adversarial attacks have been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately, existing attacks mostly focused on feature-space and/or white-box attacks, which make impractical assumptions in real-world scenarios, leaving the study on practical gray/black-box attacks largely unexplored.   To bridge this gap, we conduct the first systematic study of the gray/black-box traffic-space adversarial attacks to evaluate the robustness of ML-based NIDSs. Our work outperforms previous ones in the following aspects: (i) practical-the proposed attack can automatically mutate original traffic with extremely limited knowledge and affordable overhead while preserving its functionality; (ii) generic-the proposed attack is effective for evaluating the robustness of various NIDSs using diverse ML/DL models and non-payload-based features; (iii) explainable-we propose an explanation method for the fragile robustness of ML-based NIDSs. Based on this, we also propose a defense scheme against adversarial attacks to improve system robustness. We extensively evaluate the robustness of various NIDSs using diverse feature sets and ML/DL models. Experimental results show our attack is effective (e.g., >97% evasion rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable execution cost and the proposed defense method can effectively mitigate such attacks (evasion rate is reduced by >50% in most cases).

</details>

<details>

<summary>2021-06-08 15:12:31 - Enhancing Robustness of Neural Networks through Fourier Stabilization</summary>

- *Netanel Raviv, Aidan Kelley, Michael Guo, Yevgeny Vorobeychik*

- `2106.04435v1` - [abs](http://arxiv.org/abs/2106.04435v1) - [pdf](http://arxiv.org/pdf/2106.04435v1)

> Despite the considerable success of neural networks in security settings such as malware detection, such models have proved vulnerable to evasion attacks, in which attackers make slight changes to inputs (e.g., malware) to bypass detection. We propose a novel approach, \emph{Fourier stabilization}, for designing evasion-robust neural networks with binary inputs. This approach, which is complementary to other forms of defense, replaces the weights of individual neurons with robust analogs derived using Fourier analytic tools. The choice of which neurons to stabilize in a neural network is then a combinatorial optimization problem, and we propose several methods for approximately solving it. We provide a formal bound on the per-neuron drop in accuracy due to Fourier stabilization, and experimentally demonstrate the effectiveness of the proposed approach in boosting robustness of neural networks in several detection settings. Moreover, we show that our approach effectively composes with adversarial training.

</details>

<details>

<summary>2021-06-08 18:00:03 - Investigating Memorization of Conspiracy Theories in Text Generation</summary>

- *Sharon Levy, Michael Saxon, William Yang Wang*

- `2101.00379v3` - [abs](http://arxiv.org/abs/2101.00379v3) - [pdf](http://arxiv.org/pdf/2101.00379v3)

> The adoption of natural language generation (NLG) models can leave individuals vulnerable to the generation of harmful information memorized by the models, such as conspiracy theories. While previous studies examine conspiracy theories in the context of social media, they have not evaluated their presence in the new space of generative language models. In this work, we investigate the capability of language models to generate conspiracy theory text. Specifically, we aim to answer: can we test pretrained generative language models for the memorization and elicitation of conspiracy theories without access to the model's training data? We highlight the difficulties of this task and discuss it in the context of memorization, generalization, and hallucination. Utilizing a new dataset consisting of conspiracy theory topics and machine-generated conspiracy theories helps us discover that many conspiracy theories are deeply rooted in the pretrained language models. Our experiments demonstrate a relationship between model parameters such as size and temperature and their propensity to generate conspiracy theory text. These results indicate the need for a more thorough review of NLG applications before release and an in-depth discussion of the drawbacks of memorization in generative language models.

</details>

<details>

<summary>2021-06-09 06:18:04 - Verification of a Merkle Patricia Tree Library Using F*</summary>

- *Sota Sato, Ryotaro Banno, Jun Furuse, Kohei Suenaga, Atsushi Igarashi*

- `2106.04826v1` - [abs](http://arxiv.org/abs/2106.04826v1) - [pdf](http://arxiv.org/pdf/2106.04826v1)

> A Merkle tree is a data structure for representing a key-value store as a tree. Each node of a Merkle tree is equipped with a hash value computed from those of their descendants. A Merkle tree is often used for representing a state of a blockchain system since it can be used for efficiently auditing the state in a trustless manner. Due to the safety-critical nature of blockchains, ensuring the correctness of their implementation is paramount.   We show our formally verified implementation of the core part of Plebeia using F*. Plebeia is a library to manipulate an extension of Merkle trees (called Plebeia trees). It is being implemented as a part of the storage system of the Tezos blockchain system. To this end, we gradually ported Plebeia to F*; the OCaml code extracted from the modules ported to F* is linked with the unverified part of Plebeia. By this gradual porting process, we can obtain a working code from our partially verified implementation of Plebeia; we confirmed that the binary passes all the unit tests of Plebeia.   More specifically, we verified the following properties on the implementation of Plebeia: (1) Each tree-manipulating function preserves the invariants on the data structure of a Plebeia tree and satisfies the functional requirements as a nested key-value store; (2) Each function for serializing/deserializing a Plebeia tree to/from the low-level storage is implemented correctly; and (3) The hash function for a Plebeia tree is relatively collision-resistant with respect to the cryptographic safety of the blake2b hash function. During porting Plebeia to F*, we found a bug in an old version of Plebeia, which was overlooked by the tests bundled with the original implementation. To the best of our knowledge, this is the first work that verifies a production-level implementation of a Merkle-tree library by F*.

</details>

<details>

<summary>2021-06-09 07:13:56 - Analysis and Improvement of Heterogeneous Hardware Support in Docker Images</summary>

- *Panagiotis Gkikopoulos, Valerio Schiavoni, Josef Spillner*

- `2105.02606v2` - [abs](http://arxiv.org/abs/2105.02606v2) - [pdf](http://arxiv.org/pdf/2105.02606v2)

> Docker images are used to distribute and deploy cloud-native applications in containerised form. A container engine runs them with separated privileges according to namespaces. Recent studies have investigated security vulnerabilities and runtime characteristics of Docker images. In contrast, little is known about the extent of hardware-dependent features in them such as processor-specific trusted execution environments, graphics acceleration or extension boards. This problem can be generalised to missing knowledge about the extent of any hardware-bound instructions within the images that may require elevated privileges. We first conduct a systematic one-year evolution analysis of a sample of Docker images concerning their use of hardware-specific features. To improve the state of technology, we contribute novel tools to manage such images. Our heuristic hardware dependency detector and a hardware-aware Docker executor give early warnings upon missing dependencies instead of leading to silent or untimely failures. Our dataset and tools are released to the research community.

</details>

<details>

<summary>2021-06-09 07:55:25 - Preventing Manipulation Attack in Local Differential Privacy using Verifiable Randomization Mechanism</summary>

- *Fumiyuki Kato, Yang Cao, Masatoshi Yoshikawa*

- `2104.06569v2` - [abs](http://arxiv.org/abs/2104.06569v2) - [pdf](http://arxiv.org/pdf/2104.06569v2)

> Several randomization mechanisms for local differential privacy (LDP) (e.g., randomized response) are well-studied to improve the utility. However, recent studies show that LDP is generally vulnerable to malicious data providers in nature. Because a data collector has to estimate background data distribution only from already randomized data, malicious data providers can manipulate their output before sending, i.e., randomization would provide them plausible deniability. Attackers can skew the estimations effectively since they are calculated by normalizing with randomization probability defined in the LDP protocol, and can even control the estimations. In this paper, we show how we prevent malicious attackers from compromising LDP protocol. Our approach is to utilize a verifiable randomization mechanism. The data collector can verify the completeness of executing an agreed randomization mechanism for every data provider. Our proposed method completely protects the LDP protocol from output-manipulations, and significantly mitigates the expected damage from attacks. We do not assume any specific attacks, and it works effectively against general output-manipulation, and thus is more powerful than previously proposed countermeasures. We describe the secure version of three state-of-the-art LDP protocols and empirically show they cause acceptable overheads according to several parameters.

</details>

<details>

<summary>2021-06-09 08:52:33 - Erratum: Leveraging Flexible Tree Matching to Repair Broken Locators in Web Automation Scripts</summary>

- *Sacha Brisset, Romain Rouvoy, Lionel Seinturier, Renaud Pawlak*

- `2106.04916v1` - [abs](http://arxiv.org/abs/2106.04916v1) - [pdf](http://arxiv.org/pdf/2106.04916v1)

> Web applications are constantly evolving to integrate new features and fix reported bugs. Even an imperceptible change can sometimes entail significant modifications of the Document Object Model (DOM), which is the underlying model used by browsers to render all the elements included in a web application. Scripts that interact with web applications (e.g. web test scripts, crawlers, or robotic process automation) rely on this continuously evolving DOM which means they are often particularly fragile. More precisely, the major cause of breakages observed in automation scripts are element locators, which are identifiers used by automation scripts to navigate across the DOM. When the DOM evolves, these identifiers tend to break, thus causing the related scripts to no longer locate the intended target elements. For this reason, several contributions explored the idea of automatically repairing broken locators on a page. These works attempt to repair a given broken locator by scanning all elements in the new DOM to find the most similar one. Unfortunately, this approach fails to scale when the complexity of web pages grows, leading either to long computation times or incorrect element repairs. This article, therefore, adopts a different perspective on this problem by introducing a new locator repair solution that leverages tree matching algorithms to relocate broken locators. This solution, named Erratum, implements a holistic approach to reduce the element search space, which greatly eases the locator repair task and drastically improves repair accuracy. We compare the robustness of Erratum on a large-scale benchmark composed of realistic and synthetic mutations applied to popular web applications currently deployed in production. Our empirical results demonstrate that Erratum outperforms the accuracy of WATER, a state-of-the-art solution, by 67%.

</details>

<details>

<summary>2021-06-09 09:31:10 - Attacking Adversarial Attacks as A Defense</summary>

- *Boxi Wu, Heng Pan, Li Shen, Jindong Gu, Shuai Zhao, Zhifeng Li, Deng Cai, Xiaofei He, Wei Liu*

- `2106.04938v1` - [abs](http://arxiv.org/abs/2106.04938v1) - [pdf](http://arxiv.org/pdf/2106.04938v1)

> It is well known that adversarial attacks can fool deep neural networks with imperceptible perturbations. Although adversarial training significantly improves model robustness, failure cases of defense still broadly exist. In this work, we find that the adversarial attacks can also be vulnerable to small perturbations. Namely, on adversarially-trained models, perturbing adversarial examples with a small random noise may invalidate their misled predictions. After carefully examining state-of-the-art attacks of various kinds, we find that all these attacks have this deficiency to different extents. Enlightened by this finding, we propose to counter attacks by crafting more effective defensive perturbations. Our defensive perturbations leverage the advantage that adversarial training endows the ground-truth class with smaller local Lipschitzness. By simultaneously attacking all the classes, the misled predictions with larger Lipschitzness can be flipped into correct ones. We verify our defensive perturbation with both empirical experiments and theoretical analyses on a linear model. On CIFAR10, it boosts the state-of-the-art model from 66.16% to 72.66% against the four attacks of AutoAttack, including 71.76% to 83.30% against the Square attack. On ImageNet, the top-1 robust accuracy of FastAT is improved from 33.18% to 38.54% under the 100-step PGD attack.

</details>

<details>

<summary>2021-06-09 09:51:02 - Information flow based defensive chain for data leakage detection and prevention: a survey</summary>

- *Ning Xi, Chao Chen, Jun Zhang, Cong Sun, Shigang Liu, Pengbin Feng, Jianfeng Ma*

- `2106.04951v1` - [abs](http://arxiv.org/abs/2106.04951v1) - [pdf](http://arxiv.org/pdf/2106.04951v1)

> Mobile and IoT applications have greatly enriched our daily life by providing convenient and intelligent services. However, these smart applications have been a prime target of adversaries for stealing sensitive data. It poses a crucial threat to users' identity security, financial security, or even life security. Research communities and industries have proposed many Information Flow Control (IFC) techniques for data leakage detection and prevention, including secure modeling, type system, static analysis, dynamic analysis, \textit{etc}. According to the application's development life cycle, although most attacks are conducted during the application's execution phase, data leakage vulnerabilities have been introduced since the design phase. With a focus on lifecycle protection, this survey reviews the recent representative works adopted in different phases. We propose an information flow based defensive chain, which provides a new framework to systematically understand various IFC techniques for data leakage detection and prevention in Mobile and IoT applications. In line with the phases of the application life cycle, each reviewed work is comprehensively studied in terms of technique, performance, and limitation. Research challenges and future directions are also pointed out by consideration of the integrity of the defensive chain.

</details>

<details>

<summary>2021-06-09 10:26:46 - Eight Reasons Why Cybersecurity on Novel Generations of Brain-Computer Interfaces Must Be Prioritized</summary>

- *Sergio López Bernal, Alberto Huertas Celdrán, Gregorio Martínez Pérez*

- `2106.04968v1` - [abs](http://arxiv.org/abs/2106.04968v1) - [pdf](http://arxiv.org/pdf/2106.04968v1)

> This article presents eight neural cyberattacks affecting spontaneous neural activity, inspired by well-known cyberattacks from the computer science domain: Neural Flooding, Neural Jamming, Neural Scanning, Neural Selective Forwarding, Neural Spoofing, Neural Sybil, Neural Sinkhole and Neural Nonce. These cyberattacks are based on the exploitation of vulnerabilities existing in the new generation of Brain-Computer Interfaces. After presenting their formal definitions, the cyberattacks have been implemented over a neuronal simulation. To evaluate the impact of each cyberattack, they have been implemented in a Convolutional Neural Network (CNN) simulating a portion of a mouse's visual cortex. This implementation is based on existing literature indicating the similarities that CNNs have with neuronal structures from the visual cortex. Some conclusions are also provided, indicating that Neural Nonce and Neural Jamming are the most impactful cyberattacks for short-term effects, while Neural Scanning and Neural Nonce are the most damaging for long-term effects.

</details>

<details>

<summary>2021-06-09 15:28:23 - Influence-Augmented Online Planning for Complex Environments</summary>

- *Jinke He, Miguel Suau, Frans A. Oliehoek*

- `2010.11038v2` - [abs](http://arxiv.org/abs/2010.11038v2) - [pdf](http://arxiv.org/pdf/2010.11038v2)

> How can we plan efficiently in real time to control an agent in a complex environment that may involve many other agents? While existing sample-based planners have enjoyed empirical success in large POMDPs, their performance heavily relies on a fast simulator. However, real-world scenarios are complex in nature and their simulators are often computationally demanding, which severely limits the performance of online planners. In this work, we propose influence-augmented online planning, a principled method to transform a factored simulator of the entire environment into a local simulator that samples only the state variables that are most relevant to the observation and reward of the planning agent and captures the incoming influence from the rest of the environment using machine learning methods. Our main experimental results show that planning on this less accurate but much faster local simulator with POMCP leads to higher real-time planning performance than planning on the simulator that models the entire environment.

</details>

<details>

<summary>2021-06-09 16:23:04 - Adversarial Evaluation of Multimodal Models under Realistic Gray Box Assumption</summary>

- *Ivan Evtimov, Russel Howes, Brian Dolhansky, Hamed Firooz, Cristian Canton Ferrer*

- `2011.12902v3` - [abs](http://arxiv.org/abs/2011.12902v3) - [pdf](http://arxiv.org/pdf/2011.12902v3)

> This work examines the vulnerability of multimodal (image + text) models to adversarial threats similar to those discussed in previous literature on unimodal (image- or text-only) models. We introduce realistic assumptions of partial model knowledge and access, and discuss how these assumptions differ from the standard "black-box"/"white-box" dichotomy common in current literature on adversarial attacks. Working under various levels of these "gray-box" assumptions, we develop new attack methodologies unique to multimodal classification and evaluate them on the Hateful Memes Challenge classification task. We find that attacking multiple modalities yields stronger attacks than unimodal attacks alone (inducing errors in up to 73% of cases), and that the unimodal image attacks on multimodal classifiers we explored were stronger than character-based text augmentation attacks (inducing errors on average in 45% and 30% of cases, respectively).

</details>

<details>

<summary>2021-06-09 16:55:18 - Near-Optimal Privacy-Utility Tradeoff in Genomic Studies Using Selective SNP Hiding</summary>

- *Nour Almadhoun Alserr, Gulce Kale, Onur Mutlu, Oznur Tastan, Erman Ayday*

- `2106.05211v1` - [abs](http://arxiv.org/abs/2106.05211v1) - [pdf](http://arxiv.org/pdf/2106.05211v1)

> Motivation: Researchers need a rich trove of genomic datasets that they can leverage to gain a better understanding of the genetic basis of the human genome and identify associations between phenotypes and specific parts of DNA. However, sharing genomic datasets that include sensitive genetic or medical information of individuals can lead to serious privacy-related consequences if data lands in the wrong hands. Restricting access to genomic datasets is one solution, but this greatly reduces their usefulness for research purposes. To allow sharing of genomic datasets while addressing these privacy concerns, several studies propose privacy-preserving mechanisms for data sharing. Differential privacy (DP) is one of such mechanisms that formalize rigorous mathematical foundations to provide privacy guarantees while sharing aggregated statistical information about a dataset. However, it has been shown that the original privacy guarantees of DP-based solutions degrade when there are dependent tuples in the dataset, which is a common scenario for genomic datasets (due to the existence of family members). Results: In this work, we introduce a near-optimal mechanism to mitigate the vulnerabilities of the inference attacks on differentially private query results from genomic datasets including dependent tuples. We propose a utility-maximizing and privacy-preserving approach for sharing statistics by hiding selective SNPs of the family members as they participate in a genomic dataset. By evaluating our mechanism on a real-world genomic dataset, we empirically demonstrate that our proposed mechanism can achieve up to 40% better privacy than state-of-the-art DP-based solutions, while near-optimally minimizing the utility loss.

</details>

<details>

<summary>2021-06-10 03:31:29 - Semantic-aware Binary Code Representation with BERT</summary>

- *Hyungjoon Koo, Soyeon Park, Daejin Choi, Taesoo Kim*

- `2106.05478v1` - [abs](http://arxiv.org/abs/2106.05478v1) - [pdf](http://arxiv.org/pdf/2106.05478v1)

> A wide range of binary analysis applications, such as bug discovery, malware analysis and code clone detection, require recovery of contextual meanings on a binary code. Recently, binary analysis techniques based on machine learning have been proposed to automatically reconstruct the code representation of a binary instead of manually crafting specifics of the analysis algorithm. However, the existing approaches utilizing machine learning are still specialized to solve one domain of problems, rendering recreation of models for different types of binary analysis. In this paper, we propose DeepSemantic utilizing BERT in producing the semantic-aware code representation of a binary code.   To this end, we introduce well-balanced instruction normalization that holds rich information for each of instructions yet minimizing an out-of-vocabulary (OOV) problem. DeepSemantic has been carefully designed based on our study with large swaths of binaries. Besides, DeepSemantic leverages the essence of the BERT architecture into re-purposing a pre-trained generic model that is readily available as a one-time processing, followed by quickly applying specific downstream tasks with a fine-tuning process. We demonstrate DeepSemantic with two downstream tasks, namely, binary similarity comparison and compiler provenance (i.e., compiler and optimization level) prediction. Our experimental results show that the binary similarity model outperforms two state-of-the-art binary similarity tools, DeepBinDiff and SAFE, 49.84% and 15.83% on average, respectively.

</details>

<details>

<summary>2021-06-10 07:38:23 - We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature</summary>

- *Bin Liang, Jiachun Li, Jianjun Huang*

- `2106.05261v2` - [abs](http://arxiv.org/abs/2106.05261v2) - [pdf](http://arxiv.org/pdf/2106.05261v2)

> Recently, the object detection based on deep learning has proven to be vulnerable to adversarial patch attacks. The attackers holding a specially crafted patch can hide themselves from the state-of-the-art person detectors, e.g., YOLO, even in the physical world. This kind of attack can bring serious security threats, such as escaping from surveillance cameras. In this paper, we deeply explore the detection problems about the adversarial patch attacks to the object detection. First, we identify a leverageable signature of existing adversarial patches from the point of the visualization explanation. A fast signature-based defense method is proposed and demonstrated to be effective. Second, we design an improved patch generation algorithm to reveal the risk that the signature-based way may be bypassed by the techniques emerging in the future. The newly generated adversarial patches can successfully evade the proposed signature-based defense. Finally, we present a novel signature-independent detection method based on the internal content semantics consistency rather than any attack-specific prior knowledge. The fundamental intuition is that the adversarial object can appear locally but disappear globally in an input image. The experiments demonstrate that the signature-independent method can effectively detect the existing and improved attacks. It has also proven to be a general method by detecting unforeseen and even other types of attacks without any attack-specific prior knowledge. The two proposed detection methods can be adopted in different scenarios, and we believe that combining them can offer a comprehensive protection.

</details>

<details>

<summary>2021-06-10 11:06:17 - Deep neural network loses attention to adversarial images</summary>

- *Shashank Kotyan, Danilo Vasconcellos Vargas*

- `2106.05657v1` - [abs](http://arxiv.org/abs/2106.05657v1) - [pdf](http://arxiv.org/pdf/2106.05657v1)

> Adversarial algorithms have shown to be effective against neural networks for a variety of tasks. Some adversarial algorithms perturb all the pixels in the image minimally for the image classification task in image classification. In contrast, some algorithms perturb few pixels strongly. However, very little information is available regarding why these adversarial samples so diverse from each other exist. Recently, Vargas et al. showed that the existence of these adversarial samples might be due to conflicting saliency within the neural network. We test this hypothesis of conflicting saliency by analysing the Saliency Maps (SM) and Gradient-weighted Class Activation Maps (Grad-CAM) of original and few different types of adversarial samples. We also analyse how different adversarial samples distort the attention of the neural network compared to original samples. We show that in the case of Pixel Attack, perturbed pixels either calls the network attention to themselves or divert the attention from them. Simultaneously, the Projected Gradient Descent Attack perturbs pixels so that intermediate layers inside the neural network lose attention for the correct class. We also show that both attacks affect the saliency map and activation maps differently. Thus, shedding light on why some defences successful against some attacks remain vulnerable against other attacks. We hope that this analysis will improve understanding of the existence and the effect of adversarial samples and enable the community to develop more robust neural networks.

</details>

<details>

<summary>2021-06-10 20:11:36 - Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm</summary>

- *Mingkang Zhu, Tianlong Chen, Zhangyang Wang*

- `2106.06027v1` - [abs](http://arxiv.org/abs/2106.06027v1) - [pdf](http://arxiv.org/pdf/2106.06027v1)

> Sparse adversarial attacks can fool deep neural networks (DNNs) by only perturbing a few pixels (regularized by l_0 norm). Recent efforts combine it with another l_infty imperceptible on the perturbation magnitudes. The resultant sparse and imperceptible attacks are practically relevant, and indicate an even higher vulnerability of DNNs that we usually imagined. However, such attacks are more challenging to generate due to the optimization difficulty by coupling the l_0 regularizer and box constraints with a non-convex objective. In this paper, we address this challenge by proposing a homotopy algorithm, to jointly tackle the sparsity and the perturbation bound in one unified framework. Each iteration, the main step of our algorithm is to optimize an l_0-regularized adversarial loss, by leveraging the nonmonotone Accelerated Proximal Gradient Method (nmAPG) for nonconvex programming; it is followed by an l_0 change control step, and an optional post-attack step designed to escape bad local minima. We also extend the algorithm to handling the structural sparsity regularizer. We extensively examine the effectiveness of our proposed homotopy attack for both targeted and non-targeted attack scenarios, on CIFAR-10 and ImageNet datasets. Compared to state-of-the-art methods, our homotopy attack leads to significantly fewer perturbations, e.g., reducing 42.91% on CIFAR-10 and 75.03% on ImageNet (average case, targeted attack), at similar maximal perturbation magnitudes, when still achieving 100% attack success rates. Our codes are available at: https://github.com/VITA-Group/SparseADV_Homotopy.

</details>

<details>

<summary>2021-06-10 20:28:01 - Security testing using JUnit and Perl scripts</summary>

- *Julian Harty*

- `2106.07497v1` - [abs](http://arxiv.org/abs/2106.07497v1) - [pdf](http://arxiv.org/pdf/2106.07497v1)

> In this paper, I describe a recent practical experience where JUnit was used for testing security bugs in addition to functional bugs. Perl scripts were also used during the exploration phase. The application being tested was mature, but insecure.

</details>

<details>

<summary>2021-06-10 21:05:34 - Identifying and Supporting Financially Vulnerable Consumers in a Privacy-Preserving Manner: A Use Case Using Decentralised Identifiers and Verifiable Credentials</summary>

- *Tasos Spiliotopoulos, Dave Horsfall, Magdalene Ng, Kovila Coopamootoo, Aad van Moorsel, Karen Elliott*

- `2106.06053v1` - [abs](http://arxiv.org/abs/2106.06053v1) - [pdf](http://arxiv.org/pdf/2106.06053v1)

> Vulnerable individuals have a limited ability to make reasonable financial decisions and choices and, thus, the level of care that is appropriate to be provided to them by financial institutions may be different from that required for other consumers. Therefore, identifying vulnerability is of central importance for the design and effective provision of financial services and products. However, validating the information that customers share and respecting their privacy are both particularly important in finance and this poses a challenge for identifying and caring for vulnerable populations. This position paper examines the potential of the combination of two emerging technologies, Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs), for the identification of vulnerable consumers in finance in an efficient and privacy-preserving manner.

</details>

<details>

<summary>2021-06-10 21:50:19 - Man-in-the-Middle Attack Resistant Secret Key Generation via Channel Randomization</summary>

- *Yanjun Pan, Ziqi Xu, Ming Li, Loukas Lazos*

- `2106.02731v2` - [abs](http://arxiv.org/abs/2106.02731v2) - [pdf](http://arxiv.org/pdf/2106.02731v2)

> Physical-layer based key generation schemes exploit the channel reciprocity for secret key extraction, which can achieve information-theoretic secrecy against eavesdroppers. Such methods, although practical, have been shown to be vulnerable against man-in-the-middle (MitM) attacks, where an active adversary, Mallory, can influence and infer part of the secret key generated between Alice and Bob by injecting her own packet upon observing highly correlated channel/RSS measurements from Alice and Bob. As all the channels remain stable within the channel coherence time, Mallory's injected packets cause Alice and Bob to measure similar RSS, which allows Mallory to successfully predict the derived key bits. To defend against such a MitM attack, we propose to utilize a reconfigurable antenna at one of the legitimate transceivers to proactively randomize the channel state across different channel probing rounds. The randomization of the antenna mode at every probing round breaks the temporal correlation of the channels from the adversary to the legitimate devices, while preserving the reciprocity of the channel between the latter. This prevents key injection from the adversary without affecting Alice and Bob's ability to measure common randomness. We theoretically analyze the security of the protocol and conduct extensive simulations and real-world experiments to evaluate its performance. Our results show that our approach eliminates the advantage of an active MitM attack by driving down the probability of successfully guessing bits of the secret key to a random guess.

</details>

<details>

<summary>2021-06-11 00:42:30 - Finding Physical Adversarial Examples for Autonomous Driving with Fast and Differentiable Image Compositing</summary>

- *Jinghan Yang, Adith Boloor, Ayan Chakrabarti, Xuan Zhang, Yevgeniy Vorobeychik*

- `2010.08844v2` - [abs](http://arxiv.org/abs/2010.08844v2) - [pdf](http://arxiv.org/pdf/2010.08844v2)

> There is considerable evidence that deep neural networks are vulnerable to adversarial perturbations applied directly to their digital inputs. However, it remains an open question whether this translates to vulnerabilities in real systems. For example, an attack on self-driving cars would in practice entail modifying the driving environment, which then impacts the video inputs to the car's controller, thereby indirectly leading to incorrect driving decisions. Such attacks require accounting for system dynamics and tracking viewpoint changes. We propose a scalable approach for finding adversarial modifications of a simulated autonomous driving environment using a differentiable approximation for the mapping from environmental modifications (rectangles on the road) to the corresponding video inputs to the controller neural network. Given the parameters of the rectangles, our proposed differentiable mapping composites them onto pre-recorded video streams of the original environment, accounting for geometric and color variations. Moreover, we propose a multiple trajectory sampling approach that enables our attacks to be robust to a car's self-correcting behavior. When combined with a neural network-based controller, our approach allows the design of adversarial modifications through end-to-end gradient-based optimization. Using the Carla autonomous driving simulator, we show that our approach is significantly more scalable and far more effective at identifying autonomous vehicle vulnerabilities in simulation experiments than a state-of-the-art approach based on Bayesian Optimization.

</details>

<details>

<summary>2021-06-11 13:03:17 - Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution</summary>

- *Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun*

- `2106.06361v1` - [abs](http://arxiv.org/abs/2106.06361v1) - [pdf](http://arxiv.org/pdf/2106.06361v1)

> Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https://github.com/thunlp/BkdAtk-LWS.

</details>

<details>

<summary>2021-06-11 15:16:18 - ModelDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection</summary>

- *Yuanchun Li, Ziqi Zhang, Bingyan Liu, Ziyue Yang, Yunxin Liu*

- `2106.08890v1` - [abs](http://arxiv.org/abs/2106.08890v1) - [pdf](http://arxiv.org/pdf/2106.08890v1)

> The knowledge of a deep learning model may be transferred to a student model, leading to intellectual property infringement or vulnerability propagation. Detecting such knowledge reuse is nontrivial because the suspect models may not be white-box accessible and/or may serve different tasks. In this paper, we propose ModelDiff, a testing-based approach to deep learning model similarity comparison. Instead of directly comparing the weights, activations, or outputs of two models, we compare their behavioral patterns on the same set of test inputs. Specifically, the behavioral pattern of a model is represented as a decision distance vector (DDV), in which each element is the distance between the model's reactions to a pair of inputs. The knowledge similarity between two models is measured with the cosine similarity between their DDVs. To evaluate ModelDiff, we created a benchmark that contains 144 pairs of models that cover most popular model reuse methods, including transfer learning, model compression, and model stealing. Our method achieved 91.7% correctness on the benchmark, which demonstrates the effectiveness of using ModelDiff for model reuse detection. A study on mobile deep learning apps has shown the feasibility of ModelDiff on real-world models.

</details>

<details>

<summary>2021-06-12 12:53:38 - Towards a Privacy-preserving Deep Learning-based Network Intrusion Detection in Data Distribution Services</summary>

- *Stanislav Abaimov*

- `2106.06765v1` - [abs](http://arxiv.org/abs/2106.06765v1) - [pdf](http://arxiv.org/pdf/2106.06765v1)

> Data Distribution Service (DDS) is an innovative approach towards communication in ICS/IoT infrastructure and robotics. Being based on the cross-platform and cross-language API to be applicable in any computerised device, it offers the benefits of modern programming languages and the opportunities to develop more complex and advanced systems. However, the DDS complexity equally increases its vulnerability, while the existing security measures are limited to plug-ins and static rules, with the rest of the security provided by third-party applications and operating system. Specifically, traditional intrusion detection systems (IDS) do not detect any anomalies in the publish/subscribe method. With the exponentially growing global communication exchange, securing DDS is of the utmost importance to futureproofing industrial, public, and even personal devices and systems. This report presents an experimental work on the simulation of several specific attacks against DDS, and the application of Deep Learning for their detection. The findings show that even though Deep Learning allows to detect all simulated attacks using only metadata analysis, their detection level varies, with some of the advanced attacks being harder to detect. The limitations imposed by the attempts to preserve privacy significantly decrease the detection rate. The report also reviews the drawbacks and limitations of the Deep Learning approach and proposes a set of selected solutions and configurations, that can further improve the DDS security.

</details>

<details>

<summary>2021-06-13 05:35:04 - Information Obfuscation of Graph Neural Networks</summary>

- *Peiyuan Liao, Han Zhao, Keyulu Xu, Tommi Jaakkola, Geoffrey Gordon, Stefanie Jegelka, Ruslan Salakhutdinov*

- `2009.13504v5` - [abs](http://arxiv.org/abs/2009.13504v5) - [pdf](http://arxiv.org/pdf/2009.13504v5)

> While the advent of Graph Neural Networks (GNNs) has greatly improved node and graph representation learning in many applications, the neighborhood aggregation scheme exposes additional vulnerabilities to adversaries seeking to extract node-level information about sensitive attributes. In this paper, we study the problem of protecting sensitive attributes by information obfuscation when learning with graph structured data. We propose a framework to locally filter out pre-determined sensitive attributes via adversarial training with the total variation and the Wasserstein distance. Our method creates a strong defense against inference attacks, while only suffering small loss in task performance. Theoretically, we analyze the effectiveness of our framework against a worst-case adversary, and characterize an inherent trade-off between maximizing predictive accuracy and minimizing information leakage. Experiments across multiple datasets from recommender systems, knowledge graphs and quantum chemistry demonstrate that the proposed approach provides a robust defense across various graph structures and tasks, while producing competitive GNN encoders for downstream tasks.

</details>

<details>

<summary>2021-06-13 16:00:26 - Single Event Transient Fault Analysis of ELEPHANT cipher</summary>

- *Priyanka Joshi, Bodhistwa Mazumdar*

- `2106.09536v1` - [abs](http://arxiv.org/abs/2106.09536v1) - [pdf](http://arxiv.org/pdf/2106.09536v1)

> In this paper, we propose a novel fault attack termed as Single Event Transient Fault Analysis (SETFA) attack, which is well suited for hardware implementations. The proposed approach pinpoints hotspots in the cypher's Sbox combinational logic circuit that significantly reduce the key entropy when subjected to faults. ELEPHANT is a parallel authenticated encryption and associated data (AEAD) scheme targeted to hardware implementations, a finalist in the Lightweight cryptography (LWC) competition launched by NIST. In this work, we investigate vulnerabilities of ELEPHANT against fault analysis. We observe that the use of 128-bit random nonce makes it resistant against many cryptanalysis techniques like differential, linear, etc., and their variants. However, the relaxed nature of Statistical Fault Analysis (SFA) methods makes them widely applicable in restrictive environments. We propose a SETFA-based key recovery attack on Elephant. We performed Single experiments with random plaintexts and keys, on Dumbo, a Sponge-based instance of the Elephant-AEAD scheme. Our proposed approach could recover the secret key in 85-250 ciphertexts. In essence, this work investigates new vulnerabilities towards fault analysis that may require to be addressed to ensure secure computations and communications in IoT scenarios.

</details>

<details>

<summary>2021-06-13 17:18:19 - Target Model Agnostic Adversarial Attacks with Query Budgets on Language Understanding Models</summary>

- *Jatin Chauhan, Karan Bhukar, Manohar Kaul*

- `2106.07047v1` - [abs](http://arxiv.org/abs/2106.07047v1) - [pdf](http://arxiv.org/pdf/2106.07047v1)

> Despite significant improvements in natural language understanding models with the advent of models like BERT and XLNet, these neural-network based classifiers are vulnerable to blackbox adversarial attacks, where the attacker is only allowed to query the target model outputs. We add two more realistic restrictions on the attack methods, namely limiting the number of queries allowed (query budget) and crafting attacks that easily transfer across different pre-trained models (transferability), which render previous attack models impractical and ineffective. Here, we propose a target model agnostic adversarial attack method with a high degree of attack transferability across the attacked models. Our empirical studies show that in comparison to baseline methods, our method generates highly transferable adversarial sentences under the restriction of limited query budgets.

</details>

<details>

<summary>2021-06-13 23:02:14 - SPADE: A Spectral Method for Black-Box Adversarial Robustness Evaluation</summary>

- *Wuxinlin Cheng, Chenhui Deng, Zhiqiang Zhao, Yaohui Cai, Zhiru Zhang, Zhuo Feng*

- `2102.03716v3` - [abs](http://arxiv.org/abs/2102.03716v3) - [pdf](http://arxiv.org/pdf/2102.03716v3)

> A black-box spectral method is introduced for evaluating the adversarial robustness of a given machine learning (ML) model. Our approach, named SPADE, exploits bijective distance mapping between the input/output graphs constructed for approximating the manifolds corresponding to the input/output data. By leveraging the generalized Courant-Fischer theorem, we propose a SPADE score for evaluating the adversarial robustness of a given model, which is proved to be an upper bound of the best Lipschitz constant under the manifold setting. To reveal the most non-robust data samples highly vulnerable to adversarial attacks, we develop a spectral graph embedding procedure leveraging dominant generalized eigenvectors. This embedding step allows assigning each data sample a robustness score that can be further harnessed for more effective adversarial training. Our experiments show the proposed SPADE method leads to promising empirical results for neural network models that are adversarially trained with the MNIST and CIFAR-10 data sets.

</details>

<details>

<summary>2021-06-14 10:40:48 - Towards Certifying L-infinity Robustness using Neural Networks with L-inf-dist Neurons</summary>

- *Bohang Zhang, Tianle Cai, Zhou Lu, Di He, Liwei Wang*

- `2102.05363v4` - [abs](http://arxiv.org/abs/2102.05363v4) - [pdf](http://arxiv.org/pdf/2102.05363v4)

> It is well-known that standard neural networks, even with a high classification accuracy, are vulnerable to small $\ell_\infty$-norm bounded adversarial perturbations. Although many attempts have been made, most previous works either can only provide empirical verification of the defense to a particular attack method, or can only develop a certified guarantee of the model robustness in limited scenarios. In this paper, we seek for a new approach to develop a theoretically principled neural network that inherently resists $\ell_\infty$ perturbations. In particular, we design a novel neuron that uses $\ell_\infty$-distance as its basic operation (which we call $\ell_\infty$-dist neuron), and show that any neural network constructed with $\ell_\infty$-dist neurons (called $\ell_{\infty}$-dist net) is naturally a 1-Lipschitz function with respect to $\ell_\infty$-norm. This directly provides a rigorous guarantee of the certified robustness based on the margin of prediction outputs. We then prove that such networks have enough expressive power to approximate any 1-Lipschitz function with robust generalization guarantee. We further provide a holistic training strategy that can greatly alleviate optimization difficulties. Experimental results show that using $\ell_{\infty}$-dist nets as basic building blocks, we consistently achieve state-of-the-art performance on commonly used datasets: 93.09% certified accuracy on MNIST ($\epsilon=0.3$), 35.42% on CIFAR-10 ($\epsilon=8/255$) and 16.31% on TinyImageNet ($\epsilon=1/255$).

</details>

<details>

<summary>2021-06-14 14:13:12 - PopSkipJump: Decision-Based Attack for Probabilistic Classifiers</summary>

- *Carl-Johann Simon-Gabriel, Noman Ahmed Sheikh, Andreas Krause*

- `2106.07445v1` - [abs](http://arxiv.org/abs/2106.07445v1) - [pdf](http://arxiv.org/pdf/2106.07445v1)

> Most current classifiers are vulnerable to adversarial examples, small input perturbations that change the classification output. Many existing attack algorithms cover various settings, from white-box to black-box classifiers, but typically assume that the answers are deterministic and often fail when they are not. We therefore propose a new adversarial decision-based attack specifically designed for classifiers with probabilistic outputs. It is based on the HopSkipJump attack by Chen et al. (2019, arXiv:1904.02144v5 ), a strong and query efficient decision-based attack originally designed for deterministic classifiers. Our P(robabilisticH)opSkipJump attack adapts its amount of queries to maintain HopSkipJump's original output quality across various noise levels, while converging to its query efficiency as the noise level decreases. We test our attack on various noise models, including state-of-the-art off-the-shelf randomized defenses, and show that they offer almost no extra robustness to decision-based attacks. Code is available at https://github.com/cjsg/PopSkipJump .

</details>

<details>

<summary>2021-06-15 02:08:45 - Natural Language Adversarial Defense through Synonym Encoding</summary>

- *Xiaosen Wang, Hao Jin, Yichen Yang, Kun He*

- `1909.06723v4` - [abs](http://arxiv.org/abs/1909.06723v4) - [pdf](http://arxiv.org/pdf/1909.06723v4)

> In the area of natural language processing, deep learning models are recently known to be vulnerable to various types of adversarial perturbations, but relatively few works are done on the defense side. Especially, there exists few effective defense method against the successful synonym substitution based attacks that preserve the syntactic structure and semantic information of the original text while fooling the deep learning models. We contribute in this direction and propose a novel adversarial defense method called Synonym Encoding Method (SEM). Specifically, SEM inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding and trains the model to eliminate possible adversarial perturbations without modifying the network architecture or adding extra data. Extensive experiments demonstrate that SEM can effectively defend the current synonym substitution based attacks and block the transferability of adversarial examples. SEM is also easy and efficient to scale to large models and big datasets.

</details>

<details>

<summary>2021-06-15 02:57:39 - NNrepair: Constraint-based Repair of Neural Network Classifiers</summary>

- *Muhammad Usman, Divya Gopinath, Youcheng Sun, Yannic Noller, Corina Pasareanu*

- `2103.12535v2` - [abs](http://arxiv.org/abs/2103.12535v2) - [pdf](http://arxiv.org/pdf/2103.12535v2)

> We present NNrepair, a constraint-based technique for repairing neural network classifiers. The technique aims to fix the logic of the network at an intermediate layer or at the last layer. NNrepair first uses fault localization to find potentially faulty network parameters (such as the weights) and then performs repair using constraint solving to apply small modifications to the parameters to remedy the defects. We present novel strategies to enable precise yet efficient repair such as inferring correctness specifications to act as oracles for intermediate layer repair, and generation of experts for each class. We demonstrate the technique in the context of three different scenarios: (1) Improving the overall accuracy of a model, (2) Fixing security vulnerabilities caused by poisoning of training data and (3) Improving the robustness of the network against adversarial attacks. Our evaluation on MNIST and CIFAR-10 models shows that NNrepair can improve the accuracy by 45.56 percentage points on poisoned data and 10.40 percentage points on adversarial data. NNrepair also provides small improvement in the overall accuracy of models, without requiring new data or re-training.

</details>

<details>

<summary>2021-06-15 06:12:33 - CAN-LOC: Spoofing Detection and Physical Intrusion Localization on an In-Vehicle CAN Bus Based on Deep Features of Voltage Signals</summary>

- *Efrat Levy, Asaf Shabtai, Bogdan Groza, Pal-Stefan Murvay, Yuval Elovici*

- `2106.07895v1` - [abs](http://arxiv.org/abs/2106.07895v1) - [pdf](http://arxiv.org/pdf/2106.07895v1)

> The Controller Area Network (CAN) is used for communication between in-vehicle devices. The CAN bus has been shown to be vulnerable to remote attacks. To harden vehicles against such attacks, vehicle manufacturers have divided in-vehicle networks into sub-networks, logically isolating critical devices. However, attackers may still have physical access to various sub-networks where they can connect a malicious device. This threat has not been adequately addressed, as methods proposed to determine physical intrusion points have shown weak results, emphasizing the need to develop more advanced techniques. To address this type of threat, we propose a security hardening system for in-vehicle networks. The proposed system includes two mechanisms that process deep features extracted from voltage signals measured on the CAN bus. The first mechanism uses data augmentation and deep learning to detect and locate physical intrusions when the vehicle starts; this mechanism can detect and locate intrusions, even when the connected malicious devices are silent. This mechanism's effectiveness (100% accuracy) is demonstrated in a wide variety of insertion scenarios on a CAN bus prototype. The second mechanism is a continuous device authentication mechanism, which is also based on deep learning; this mechanism's robustness (99.8% accuracy) is demonstrated on a real moving vehicle.

</details>

<details>

<summary>2021-06-15 07:27:39 - Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks</summary>

- *Byunggill Joe, Akshay Mehra, Insik Shin, Jihun Hamm*

- `2106.07925v1` - [abs](http://arxiv.org/abs/2106.07925v1) - [pdf](http://arxiv.org/pdf/2106.07925v1)

> Electronic Health Records (EHRs) provide a wealth of information for machine learning algorithms to predict the patient outcome from the data including diagnostic information, vital signals, lab tests, drug administration, and demographic information. Machine learning models can be built, for example, to evaluate patients based on their predicted mortality or morbidity and to predict required resources for efficient resource management in hospitals. In this paper, we demonstrate that an attacker can manipulate the machine learning predictions with EHRs easily and selectively at test time by backdoor attacks with the poisoned training data. Furthermore, the poison we create has statistically similar features to the original data making it hard to detect, and can also attack multiple machine learning models without any knowledge of the models. With less than 5% of the raw EHR data poisoned, we achieve average attack success rates of 97% on mortality prediction tasks with MIMIC-III database against Logistic Regression, Multilayer Perceptron, and Long Short-term Memory models simultaneously.

</details>

<details>

<summary>2021-06-15 09:46:46 - Securing Face Liveness Detection Using Unforgeable Lip Motion Patterns</summary>

- *Man Zhou, Qian Wang, Qi Li, Peipei Jiang, Jingxiao Yang, Chao Shen, Cong Wang, Shouhong Ding*

- `2106.08013v1` - [abs](http://arxiv.org/abs/2106.08013v1) - [pdf](http://arxiv.org/pdf/2106.08013v1)

> Face authentication usually utilizes deep learning models to verify users with high recognition accuracy. However, face authentication systems are vulnerable to various attacks that cheat the models by manipulating the digital counterparts of human faces. So far, lots of liveness detection schemes have been developed to prevent such attacks. Unfortunately, the attacker can still bypass these schemes by constructing wide-ranging sophisticated attacks. We study the security of existing face authentication services (e.g., Microsoft, Amazon, and Face++) and typical liveness detection approaches. Particularly, we develop a new type of attack, i.e., the low-cost 3D projection attack that projects manipulated face videos on a 3D face model, which can easily evade these face authentication services and liveness detection approaches. To this end, we propose FaceLip, a novel liveness detection scheme for face authentication, which utilizes unforgeable lip motion patterns built upon well-designed acoustic signals to enable a strong security guarantee. The unique lip motion patterns for each user are unforgeable because FaceLip verifies the patterns by capturing and analyzing the acoustic signals that are dynamically generated according to random challenges, which ensures that our signals for liveness detection cannot be manipulated. Specially, we develop robust algorithms for FaceLip to eliminate the impact of noisy signals in the environment and thus can accurately infer the lip motions at larger distances. We prototype FaceLip on off-the-shelf smartphones and conduct extensive experiments under different settings. Our evaluation with 44 participants validates the effectiveness and robustness of FaceLip.

</details>

<details>

<summary>2021-06-15 10:25:58 - Best Practices for Notification Studies for Security and Privacy Issues on the Internet</summary>

- *Max Maass, Henning Pridöhl, Dominik Herrmann, Matthias Hollick*

- `2106.08029v1` - [abs](http://arxiv.org/abs/2106.08029v1) - [pdf](http://arxiv.org/pdf/2106.08029v1)

> Researchers help operators of vulnerable and non-compliant internet services by individually notifying them about security and privacy issues uncovered in their research. To improve efficiency and effectiveness of such efforts, dedicated notification studies are imperative. As of today, there is no comprehensive documentation of pitfalls and best practices for conducting such notification studies, which limits validity of results and impedes reproducibility. Drawing on our experience with such studies and guidance from related work, we present a set of guidelines and practical recommendations, including initial data collection, sending of notifications, interacting with the recipients, and publishing the results. We note that future studies can especially benefit from extensive planning and automation of crucial processes, i.e., activities that take place well before the first notifications are sent.

</details>

<details>

<summary>2021-06-15 12:45:22 - Detect and remove watermark in deep neural networks via generative adversarial networks</summary>

- *Haoqi Wang, Mingfu Xue, Shichang Sun, Yushu Zhang, Jian Wang, Weiqiang Liu*

- `2106.08104v1` - [abs](http://arxiv.org/abs/2106.08104v1) - [pdf](http://arxiv.org/pdf/2106.08104v1)

> Deep neural networks (DNN) have achieved remarkable performance in various fields. However, training a DNN model from scratch requires a lot of computing resources and training data. It is difficult for most individual users to obtain such computing resources and training data. Model copyright infringement is an emerging problem in recent years. For instance, pre-trained models may be stolen or abuse by illegal users without the authorization of the model owner. Recently, many works on protecting the intellectual property of DNN models have been proposed. In these works, embedding watermarks into DNN based on backdoor is one of the widely used methods. However, when the DNN model is stolen, the backdoor-based watermark may face the risk of being detected and removed by an adversary. In this paper, we propose a scheme to detect and remove watermark in deep neural networks via generative adversarial networks (GAN). We demonstrate that the backdoor-based DNN watermarks are vulnerable to the proposed GAN-based watermark removal attack. The proposed attack method includes two phases. In the first phase, we use the GAN and few clean images to detect and reverse the watermark in the DNN model. In the second phase, we fine-tune the watermarked DNN based on the reversed backdoor images. Experimental evaluations on the MNIST and CIFAR10 datasets demonstrate that, the proposed method can effectively remove about 98% of the watermark in DNN models, as the watermark retention rate reduces from 100% to less than 2% after applying the proposed attack. In the meantime, the proposed attack hardly affects the model's performance. The test accuracy of the watermarked DNN on the MNIST and the CIFAR10 datasets drops by less than 1% and 3%, respectively.

</details>

<details>

<summary>2021-06-15 17:20:56 - Model Extraction and Adversarial Attacks on Neural Networks using Switching Power Information</summary>

- *Tommy Li, Cory Merkel*

- `2106.08299v1` - [abs](http://arxiv.org/abs/2106.08299v1) - [pdf](http://arxiv.org/pdf/2106.08299v1)

> Artificial neural networks (ANNs) have gained significant popularity in the last decade for solving narrow AI problems in domains such as healthcare, transportation, and defense. As ANNs become more ubiquitous, it is imperative to understand their associated safety, security, and privacy vulnerabilities. Recently, it has been shown that ANNs are susceptible to a number of adversarial evasion attacks--inputs that cause the ANN to make high-confidence misclassifications despite being almost indistinguishable from the data used to train and test the network. This work explores to what degree finding these examples maybe aided by using side-channel information, specifically switching power consumption, of hardware implementations of ANNs. A black-box threat scenario is assumed, where an attacker has access to the ANN hardware's input, outputs, and topology, but the trained model parameters are unknown. Then, a surrogate model is trained to have similar functional (i.e. input-output mapping) and switching power characteristics as the oracle (black-box) model. Our results indicate that the inclusion of power consumption data increases the fidelity of the model extraction by up to 30 percent based on a mean square error comparison of the oracle and surrogate weights. However, transferability of adversarial examples from the surrogate to the oracle model was not significantly affected.

</details>

<details>

<summary>2021-06-15 17:45:26 - Extracting Training Data from Large Language Models</summary>

- *Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel*

- `2012.07805v2` - [abs](http://arxiv.org/abs/2012.07805v2) - [pdf](http://arxiv.org/pdf/2012.07805v2)

> It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.   We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.   We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.

</details>

<details>

<summary>2021-06-15 18:15:26 - Adversarial Attacks on Deep Models for Financial Transaction Records</summary>

- *Ivan Fursov, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, Evgeny Burnaev*

- `2106.08361v1` - [abs](http://arxiv.org/abs/2106.08361v1) - [pdf](http://arxiv.org/pdf/2106.08361v1)

> Machine learning models using transaction records as inputs are popular among financial institutions. The most efficient models use deep-learning architectures similar to those in the NLP community, posing a challenge due to their tremendous number of parameters and limited robustness. In particular, deep-learning models are vulnerable to adversarial attacks: a little change in the input harms the model's output.   In this work, we examine adversarial attacks on transaction records data and defences from these attacks. The transaction records data have a different structure than the canonical NLP or time series data, as neighbouring records are less connected than words in sentences, and each record consists of both discrete merchant code and continuous transaction amount. We consider a black-box attack scenario, where the attack doesn't know the true decision model, and pay special attention to adding transaction tokens to the end of a sequence. These limitations provide more realistic scenario, previously unexplored in NLP world.   The proposed adversarial attacks and the respective defences demonstrate remarkable performance using relevant datasets from the financial industry. Our results show that a couple of generated transactions are sufficient to fool a deep-learning model. Further, we improve model robustness via adversarial training or separate adversarial examples detection. This work shows that embedding protection from adversarial attacks improves model robustness, allowing a wider adoption of deep models for transaction records in banking and finance.

</details>

<details>

<summary>2021-06-15 20:00:22 - AndroR2: A Dataset of Manually Reproduced Bug Reports for Android Applications</summary>

- *Tyler Wendland, Jingyang Sun, Junayed Mahmud, S. M. Hasan Mansur, Steven Huang, Kevin Moran, Julia Rubin, Mattia Fazzini*

- `2106.08403v1` - [abs](http://arxiv.org/abs/2106.08403v1) - [pdf](http://arxiv.org/pdf/2106.08403v1)

> Software maintenance constitutes a large portion of the software development lifecycle. To carry out maintenance tasks, developers often need to understand and reproduce bug reports. As such, there has been increasing research activity coalescing around the notion of automating various activities related to bug reporting. A sizable portion of this research interest has focused on the domain of mobile apps. However, as research around mobile app bug reporting progresses, there is a clear need for a manually vetted and reproducible set of real-world bug reports that can serve as a benchmark for future work. This paper presents ANDROR2: a dataset of 90 manually reproduced bug reports for Android apps listed on Google Play and hosted on GitHub, systematically collected via an in-depth analysis of 459 reports extracted from the GitHub issue tracker. For each reproduced report, ANDROR2 includes the original bug report, an apk file for the buggy version of the app, an executable reproduction script, and metadata regarding the quality of the reproduction steps associated with the original report. We believe that the ANDROR2 dataset can be used to facilitate research in automatically analyzing, understanding, reproducing, localizing, and fixing bugs for mobile applications as well as other software maintenance activities more broadly.

</details>

<details>

<summary>2021-06-16 01:56:02 - Virtual Classrooms and Real Harms: Remote Learning at U.S. Universities</summary>

- *Shaanan Cohney, Ross Teixeira, Anne Kohlbrenner, Arvind Narayanan, Mihir Kshirsagar, Yan Shvartzshnaider, Madelyn Sanfilippo*

- `2012.05867v3` - [abs](http://arxiv.org/abs/2012.05867v3) - [pdf](http://arxiv.org/pdf/2012.05867v3)

> Universities have been forced to rely on remote educational technology to facilitate the rapid shift to online learning. In doing so, they acquire new risks of security vulnerabilities and privacy violations. To help universities navigate this landscape, we develop a model that describes the actors, incentives, and risks, informed by surveying 49 educators and 14 administrators at U.S. universities. Next, we develop a methodology for administrators to assess security and privacy risks of these products. We then conduct a privacy and security analysis of 23 popular platforms using a combination of sociological analyses of privacy policies and 129 state laws, alongside a technical assessment of platform software. Based on our findings, we develop recommendations for universities to mitigate the risks to their stakeholders.

</details>

<details>

<summary>2021-06-16 06:19:50 - Towards Automated Attack Simulations of BPMN-based Processes</summary>

- *Simon Hacks, Robert Lagerström, Daniel Ritter*

- `2106.08568v1` - [abs](http://arxiv.org/abs/2106.08568v1) - [pdf](http://arxiv.org/pdf/2106.08568v1)

> Process digitization and integration is an increasing need for enterprises, while cyber-attacks denote a growing threat. Using the Business Process Management Notation (BPMN) is common to handle the digital and integration focus within and across organizations. In other parts of the same companies, threat modeling and attack graphs are used for analyzing the security posture and resilience.   In this paper, we propose a novel approach to use attack graph simulations on processes represented in BPMN. Our contributions are the identification of BPMN's attack surface, a mapping of BPMN elements to concepts in a Meta Attack Language (MAL)-based Domain-Specific Language (DSL), called coreLang, and a prototype to demonstrate our approach in a case study using a real-world invoice integration process. The study shows that non-invasively enriching BPMN instances with cybersecurity analysis through attack graphs is possible without much human expert input. The resulting insights into potential vulnerabilities could be beneficial for the process modelers.

</details>

<details>

<summary>2021-06-16 10:51:58 - Detecting message modification attacks on the CAN bus with Temporal Convolutional Networks</summary>

- *Irina Chiscop, András Gazdag, Joost Bosman, Gergely Biczók*

- `2106.08692v1` - [abs](http://arxiv.org/abs/2106.08692v1) - [pdf](http://arxiv.org/pdf/2106.08692v1)

> Multiple attacks have shown that in-vehicle networks have vulnerabilities which can be exploited. Securing the Controller Area Network (CAN) for modern vehicles has become a necessary task for car manufacturers. Some attacks inject potentially large amount of fake messages into the CAN network; however, such attacks are relatively easy to detect. In more sophisticated attacks, the original messages are modified, making the detection a more complex problem. In this paper, we present a novel machine learning based intrusion detection method for CAN networks. We focus on detecting message modification attacks, which do not change the timing patterns of communications. Our proposed temporal convolutional network-based solution can learn the normal behavior of CAN signals and differentiate them from malicious ones. The method is evaluated on multiple CAN-bus message IDs from two public datasets including different types of attacks. Performance results show that our lightweight approach compares favorably to the state-of-the-art unsupervised learning approach, achieving similar or better accuracy for a wide range of scenarios with a significantly lower false positive rate.

</details>

<details>

<summary>2021-06-16 15:51:00 - Side-Channel Attacks on RISC-V Processors: Current Progress, Challenges, and Opportunities</summary>

- *Mahya Morid Ahmadi, Faiq Khalid, Muhammad Shafique*

- `2106.08877v1` - [abs](http://arxiv.org/abs/2106.08877v1) - [pdf](http://arxiv.org/pdf/2106.08877v1)

> Side-channel attacks on microprocessors, like the RISC-V, exhibit security vulnerabilities that lead to several design challenges. Hence, it is imperative to study and analyze these security vulnerabilities comprehensively. In this paper, we present a brief yet comprehensive study of the security vulnerabilities in modern microprocessors with respect to side-channel attacks and their respective mitigation techniques. The focus of this paper is to analyze the hardware-exploitable side-channel attack using power consumption and software-exploitable side-channel attacks to manipulate cache. Towards this, we perform an in-depth analysis of the applicability and practical implications of cache attacks on RISC-V microprocessors and their associated challenges. Finally, based on the comparative study and our analysis, we highlight some key research directions to develop robust RISC-V microprocessors that are resilient to side-channel attacks.

</details>

<details>

<summary>2021-06-16 20:19:04 - Explainable AI for Natural Adversarial Images</summary>

- *Tomas Folke, ZhaoBin Li, Ravi B. Sojitra, Scott Cheng-Hsin Yang, Patrick Shafto*

- `2106.09106v1` - [abs](http://arxiv.org/abs/2106.09106v1) - [pdf](http://arxiv.org/pdf/2106.09106v1)

> Adversarial images highlight how vulnerable modern image classifiers are to perturbations outside of their training set. Human oversight might mitigate this weakness, but depends on humans understanding the AI well enough to predict when it is likely to make a mistake. In previous work we have found that humans tend to assume that the AI's decision process mirrors their own. Here we evaluate if methods from explainable AI can disrupt this assumption to help participants predict AI classifications for adversarial and standard images. We find that both saliency maps and examples facilitate catching AI errors, but their effects are not additive, and saliency maps are more effective than examples.

</details>

<details>

<summary>2021-06-16 20:34:31 - Now You See It, Now You Dont: Adversarial Vulnerabilities in Computational Pathology</summary>

- *Alex Foote, Amina Asif, Ayesha Azam, Tim Marshall-Cox, Nasir Rajpoot, Fayyaz Minhas*

- `2106.08153v2` - [abs](http://arxiv.org/abs/2106.08153v2) - [pdf](http://arxiv.org/pdf/2106.08153v2)

> Deep learning models are routinely employed in computational pathology (CPath) for solving problems of diagnostic and prognostic significance. Typically, the generalization performance of CPath models is analyzed using evaluation protocols such as cross-validation and testing on multi-centric cohorts. However, to ensure that such CPath solutions are robust and safe for use in a clinical setting, a critical analysis of their predictive performance and vulnerability to adversarial attacks is required, which is the focus of this paper. Specifically, we show that a highly accurate model for classification of tumour patches in pathology images (AUC > 0.95) can easily be attacked with minimal perturbations which are imperceptible to lay humans and trained pathologists alike. Our analytical results show that it is possible to generate single-instance white-box attacks on specific input images with high success rate and low perturbation energy. Furthermore, we have also generated a single universal perturbation matrix using the training dataset only which, when added to unseen test images, results in forcing the trained neural network to flip its prediction labels with high confidence at a success rate of > 84%. We systematically analyze the relationship between perturbation energy of an adversarial attack, its impact on morphological constructs of clinical significance, their perceptibility by a trained pathologist and saliency maps obtained using deep learning models. Based on our analysis, we strongly recommend that computational pathology models be critically analyzed using the proposed adversarial validation strategy prior to clinical adoption.

</details>

<details>

<summary>2021-06-17 01:41:29 - AN-GCN: An Anonymous Graph Convolutional Network Defense Against Edge-Perturbing Attack</summary>

- *Ao Liu, Beibei Li, Tao Li, Pan Zhou, Rui wang*

- `2005.03482v6` - [abs](http://arxiv.org/abs/2005.03482v6) - [pdf](http://arxiv.org/pdf/2005.03482v6)

> Recent studies have revealed the vulnerability of graph convolutional networks (GCNs) to edge-perturbing attacks, such as maliciously inserting or deleting graph edges. However, a theoretical proof of such vulnerability remains a big challenge, and effective defense schemes are still open issues. In this paper, we first generalize the formulation of edge-perturbing attacks and strictly prove the vulnerability of GCNs to such attacks in node classification tasks. Following this, an anonymous graph convolutional network, named AN-GCN, is proposed to counter against edge-perturbing attacks. Specifically, we present a node localization theorem to demonstrate how the GCN locates nodes during its training phase. In addition, we design a staggered Gaussian noise based node position generator, and devise a spectral graph convolution based discriminator in detecting the generated node positions. Further, we give the optimization of the above generator and discriminator. AN-GCN can classify nodes without taking their position as input. It is demonstrated that the AN-GCN is secure against edge-perturbing attacks in node classification tasks, as AN-GCN classifies nodes without the edge information and thus makes it impossible for attackers to perturb edges anymore. Extensive evaluations demonstrated the effectiveness of the general edge-perturbing attack model in manipulating the classification results of the target nodes. More importantly, the proposed AN-GCN can achieve 82.7% in node classification accuracy without the edge-reading permission, which outperforms the state-of-the-art GCN.

</details>

<details>

<summary>2021-06-17 02:24:45 - Improving adversarial robustness of deep neural networks by using semantic information</summary>

- *Lina Wang, Rui Tang, Yawei Yue, Xingshu Chen, Wei Wang, Yi Zhu, Xuemei Zeng*

- `2008.07838v2` - [abs](http://arxiv.org/abs/2008.07838v2) - [pdf](http://arxiv.org/pdf/2008.07838v2)

> The vulnerability of deep neural networks (DNNs) to adversarial attack, which is an attack that can mislead state-of-the-art classifiers into making an incorrect classification with high confidence by deliberately perturbing the original inputs, raises concerns about the robustness of DNNs to such attacks. Adversarial training, which is the main heuristic method for improving adversarial robustness and the first line of defense against adversarial attacks, requires many sample-by-sample calculations to increase training size and is usually insufficiently strong for an entire network. This paper provides a new perspective on the issue of adversarial robustness, one that shifts the focus from the network as a whole to the critical part of the region close to the decision boundary corresponding to a given class. From this perspective, we propose a method to generate a single but image-agnostic adversarial perturbation that carries the semantic information implying the directions to the fragile parts on the decision boundary and causes inputs to be misclassified as a specified target. We call the adversarial training based on such perturbations "region adversarial training" (RAT), which resembles classical adversarial training but is distinguished in that it reinforces the semantic information missing in the relevant regions. Experimental results on the MNIST and CIFAR-10 datasets show that this approach greatly improves adversarial robustness even using a very small dataset from the training data; moreover, it can defend against FGSM adversarial attacks that have a completely different pattern from the model seen during retraining.

</details>

<details>

<summary>2021-06-17 04:10:37 - Towards Assurance-Driven Architectural Decomposition of Software Systems</summary>

- *Ramy Shahin*

- `2106.09237v1` - [abs](http://arxiv.org/abs/2106.09237v1) - [pdf](http://arxiv.org/pdf/2106.09237v1)

> Computer systems are so complex, so they are usually designed and analyzed in terms of layers of abstraction. Complexity is still a challenge facing logical reasoning tools that are used to find software design flaws and implementation bugs. Abstraction is also a common technique for scaling those tools to more complex systems. However, the abstractions used in the design phase of systems are in many cases different from those used for assurance. In this paper we argue that different software quality assurance techniques operate on different aspects of software systems. To facilitate assurance, and for a smooth integration of assurance tools into the Software Development Lifecycle (SDLC), we present a 4-dimensional meta-architecture that separates computational, coordination, and stateful software artifacts early on in the design stage. We enumerate some of the design and assurance challenges that can be addressed by this meta-architecture, and demonstrate it on the high-level design of a simple file system.

</details>

<details>

<summary>2021-06-17 07:12:13 - Smart Contract Vulnerability Detection: From Pure Neural Network to Interpretable Graph Feature and Expert Pattern Fusion</summary>

- *Zhenguang Liu, Peng Qian, Xiang Wang, Lei Zhu, Qinming He, Shouling Ji*

- `2106.09282v1` - [abs](http://arxiv.org/abs/2106.09282v1) - [pdf](http://arxiv.org/pdf/2106.09282v1)

> Smart contracts hold digital coins worth billions of dollars, their security issues have drawn extensive attention in the past years. Towards smart contract vulnerability detection, conventional methods heavily rely on fixed expert rules, leading to low accuracy and poor scalability. Recent deep learning approaches alleviate this issue but fail to encode useful expert knowledge. In this paper, we explore combining deep learning with expert patterns in an explainable fashion. Specifically, we develop automatic tools to extract expert patterns from the source code. We then cast the code into a semantic graph to extract deep graph features. Thereafter, the global graph feature and local expert patterns are fused to cooperate and approach the final prediction, while yielding their interpretable weights. Experiments are conducted on all available smart contracts with source code in two platforms, Ethereum and VNT Chain. Empirically, our system significantly outperforms state-of-the-art methods. Our code is released.

</details>

<details>

<summary>2021-06-17 10:52:42 - Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems</summary>

- *Giovanni Apruzzese, Mauro Andreolini, Luca Ferretti, Mirco Marchetti, Michele Colajanni*

- `2106.09380v1` - [abs](http://arxiv.org/abs/2106.09380v1) - [pdf](http://arxiv.org/pdf/2106.09380v1)

> The incremental diffusion of machine learning algorithms in supporting cybersecurity is creating novel defensive opportunities but also new types of risks. Multiple researches have shown that machine learning methods are vulnerable to adversarial attacks that create tiny perturbations aimed at decreasing the effectiveness of detecting threats. We observe that existing literature assumes threat models that are inappropriate for realistic cybersecurity scenarios because they consider opponents with complete knowledge about the cyber detector or that can freely interact with the target systems. By focusing on Network Intrusion Detection Systems based on machine learning, we identify and model the real capabilities and circumstances required by attackers to carry out feasible and successful adversarial attacks. We then apply our model to several adversarial attacks proposed in literature and highlight the limits and merits that can result in actual adversarial attacks. The contributions of this paper can help hardening defensive systems by letting cyber defenders address the most critical and real issues, and can benefit researchers by allowing them to devise novel forms of adversarial attacks based on realistic threat models.

</details>

<details>

<summary>2021-06-17 12:50:56 - Adversarial Attack Vulnerability of Medical Image Analysis Systems: Unexplored Factors</summary>

- *Gerda Bortsova, Cristina González-Gonzalo, Suzanne C. Wetstein, Florian Dubost, Ioannis Katramados, Laurens Hogeweg, Bart Liefers, Bram van Ginneken, Josien P. W. Pluim, Mitko Veta, Clara I. Sánchez, Marleen de Bruijne*

- `2006.06356v3` - [abs](http://arxiv.org/abs/2006.06356v3) - [pdf](http://arxiv.org/pdf/2006.06356v3)

> Adversarial attacks are considered a potentially serious security threat for machine learning systems. Medical image analysis (MedIA) systems have recently been argued to be vulnerable to adversarial attacks due to strong financial incentives and the associated technological infrastructure.   In this paper, we study previously unexplored factors affecting adversarial attack vulnerability of deep learning MedIA systems in three medical domains: ophthalmology, radiology, and pathology. We focus on adversarial black-box settings, in which the attacker does not have full access to the target model and usually uses another model, commonly referred to as surrogate model, to craft adversarial examples. We consider this to be the most realistic scenario for MedIA systems.   Firstly, we study the effect of weight initialization (ImageNet vs. random) on the transferability of adversarial attacks from the surrogate model to the target model. Secondly, we study the influence of differences in development data between target and surrogate models. We further study the interaction of weight initialization and data differences with differences in model architecture. All experiments were done with a perturbation degree tuned to ensure maximal transferability at minimal visual perceptibility of the attacks.   Our experiments show that pre-training may dramatically increase the transferability of adversarial examples, even when the target and surrogate's architectures are different: the larger the performance gain using pre-training, the larger the transferability. Differences in the development data between target and surrogate models considerably decrease the performance of the attack; this decrease is further amplified by difference in the model architecture. We believe these factors should be considered when developing security-critical MedIA systems planned to be deployed in clinical practice.

</details>

<details>

<summary>2021-06-17 14:22:20 - Boosting the Bounds of Symbolic QED for Effective Pre-Silicon Verification of Processor Cores</summary>

- *Karthik Ganesan, Srinivasa Shashank Nuthakki*

- `1908.06757v4` - [abs](http://arxiv.org/abs/1908.06757v4) - [pdf](http://arxiv.org/pdf/1908.06757v4)

> Existing techniques to ensure functional correctness and hardware trust during pre-silicon verification face severe limitations. In this work, we systematically leverage two key ideas: 1) Symbolic Quick Error Detection (Symbolic QED or SQED), a recent bug detection and localization technique using Bounded Model Checking (BMC); and 2) Symbolic starting states, to present a method that: i) Effectively detects both "difficult" logic bugs and Hardware Trojans, even with long activation sequences where traditional BMC techniques fail; and ii) Does not need skilled manual guidance for writing testbenches, writing design-specific assertions, or debugging spurious counter-examples. Using open-source RISC-V cores, we demonstrate the following: 1. Quick (<5 minutes for an in-order scalar core and <2.5 hours for an out-of-order superscalar core) detection of 100% of hundreds of logic bug and hardware Trojan scenarios from commercial chips and research literature, and 97.9% of "extremal" bugs (randomly-generated bugs requiring ~100,000 activation instructions taken from random test programs). 2. Quick (~1 minute) detection of several previously unknown bugs in open-source RISC-V designs.

</details>

<details>

<summary>2021-06-17 20:33:56 - Intentional Forgetting</summary>

- *Deborah Shands, Carolyn Talcott*

- `2106.09802v1` - [abs](http://arxiv.org/abs/2106.09802v1) - [pdf](http://arxiv.org/pdf/2106.09802v1)

> Many damaging cybersecurity attacks are enabled when an attacker can access residual sensitive information (e.g. cryptographic keys, personal identifiers) left behind from earlier computation. Attackers can sometimes use residual information to take control of a system, impersonate a user, or manipulate data. Current approaches to addressing access to residual sensitive information aim to patch individual software or hardware vulnerabilities. While such patching approaches are necessary to mitigate sometimes serious security vulnerabilities in the near term, they cannot address the underlying issue: explicit requirements for adequately eliminating residual information and explicit representations of the erasure capabilities of systems are necessary to ensure that sensitive information is handled as expected.   This position paper introduces the concept of intentional forgetting and the capabilities that are needed to achieve it. Intentional forgetting enables software and hardware system designers at every level of abstraction to clearly specify and rigorously reason about the forgetting capabilities required of and provided by a system. We identify related work that may help to illuminate challenges or contribute to solutions and consider conceptual and engineering tradeoffs in implementations of forgetting capabilities. We discuss approaches to modeling intentional forgetting and then modeling the strength of a system's forgetting capability by its resistance to disclosing information to different types of detectors. Research is needed in a variety of domains to advance the theory, specification techniques, system foundations, implementation tools, and methodologies for effective, practical forgetting. We highlight research challenges in several domains and encourage cross-disciplinary collaboration to one day create a robust theory and practice of intentional forgetting.

</details>

<details>

<summary>2021-06-17 23:14:32 - Conclusion Stability for Natural Language Based Mining of Design Discussions</summary>

- *Alvi Mahadi, Neil A. Ernst, Karan Tongay*

- `2106.09844v1` - [abs](http://arxiv.org/abs/2106.09844v1) - [pdf](http://arxiv.org/pdf/2106.09844v1)

> Developer discussions range from in-person hallway chats to comment chains on bug reports. Being able to identify discussions that touch on software design would be helpful in documentation and refactoring software. Design mining is the application of machine learning techniques to correctly label a given discussion artifact, such as a pull request, as pertaining (or not) to design. In this paper we demonstrate a simple example of how design mining works. We then show how conclusion stability is poor on different artifact types and different projects. We show two techniques -- augmentation and context specificity -- that greatly improve the conclusion stability and cross-project relevance of design mining. Our new approach achieves AUC of 0.88 on within dataset classification and 0.80 on the cross-dataset classification task.

</details>

<details>

<summary>2021-06-18 07:58:20 - A Grounded Theory of the Role of Coordination in Software Security Patch Management</summary>

- *Nesara Dissanayake, Mansooreh Zahedi, Asangi Jayatilaka, Muhammad Ali Babar*

- `2106.03458v2` - [abs](http://arxiv.org/abs/2106.03458v2) - [pdf](http://arxiv.org/pdf/2106.03458v2)

> Several disastrous security attacks can be attributed to delays in patching software vulnerabilities. While researchers and practitioners have paid significant attention to automate vulnerabilities identification and patch development activities of software security patch management, there has been relatively little effort dedicated to gain an in-depth understanding of the socio-technical aspects, e.g., coordination of interdependent activities of the patching process and patching decisions, that may cause delays in applying security patches. We report on a Grounded Theory study of the role of coordination in security patch management. The reported theory consists of four inter-related dimensions, i.e., causes, breakdowns, constraints, and mechanisms. The theory explains the causes that define the need for coordination among interdependent software and hardware components and multiple stakeholders' decisions, the constraints that can negatively impact coordination, the breakdowns in coordination, and the potential corrective measures. This study provides potentially useful insights for researchers and practitioners who can carefully consider the needs of and devise suitable solutions for supporting the coordination of interdependencies involved in security patch management.

</details>

<details>

<summary>2021-06-18 11:56:37 - Self-supervised Graph Learning for Recommendation</summary>

- *Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie*

- `2010.10783v4` - [abs](http://arxiv.org/abs/2010.10783v4) - [pdf](http://arxiv.org/pdf/2010.10783v4)

> Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges.   In this work, we explore self-supervised learning on user-item graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary self-supervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise three operators to generate the views -- node dropout, edge dropout, and random walk -- that change the graph structure in different manners. We term this new learning paradigm as \textit{Self-supervised Graph Learning} (SGL), implementing it on the state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL has the ability of automatically mining hard negatives. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the recommendation accuracy, especially on long-tail items, and the robustness against interaction noises. Our implementations are available at \url{https://github.com/wujcan/SGL}.

</details>

<details>

<summary>2021-06-18 14:42:09 - A Survey of Privacy Vulnerabilities of Mobile Device Sensors</summary>

- *Paula Delgado-Santos, Giuseppe Stragapede, Ruben Tolosana, Richard Guest, Farzin Deravi, Ruben Vera-Rodriguez*

- `2106.10154v1` - [abs](http://arxiv.org/abs/2106.10154v1) - [pdf](http://arxiv.org/pdf/2106.10154v1)

> The number of mobile devices, such as smartphones and smartwatches, is relentlessly increasing to almost 6.8 billion by 2022, and along with it, the amount of personal and sensitive data captured by them. This survey overviews the state of the art of what personal and sensitive user attributes can be extracted from mobile device sensors, emphasising critical aspects such as demographics, health and body features, activity and behaviour recognition, etc. In addition, we review popular metrics in the literature to quantify the degree of privacy, and discuss powerful privacy methods to protect the sensitive data while preserving data utility for analysis. Finally, open research questions a represented for further advancements in the field.

</details>

<details>

<summary>2021-06-19 10:02:51 - Towards Robustness of Text-to-SQL Models against Synonym Substitution</summary>

- *Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie, Pengsheng Huang*

- `2106.01065v2` - [abs](http://arxiv.org/abs/2106.01065v2) - [pdf](http://arxiv.org/pdf/2106.01065v2)

> Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries. Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. In this work, we investigate the robustness of text-to-SQL models to synonym substitution. In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases. We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case adversarial attacks. Finally, we present two categories of approaches to improve the model robustness. The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training. We demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective.

</details>

<details>

<summary>2021-06-19 11:53:49 - Vulnerability Detection with Fine-grained Interpretations</summary>

- *Yi Li, Shaohua Wang, Tien N. Nguyen*

- `2106.10478v1` - [abs](http://arxiv.org/abs/2106.10478v1) - [pdf](http://arxiv.org/pdf/2106.10478v1)

> Despite the successes of machine learning (ML) and deep learning (DL) based vulnerability detectors (VD), they are limited to providing only the decision on whether a given code is vulnerable or not, without details on what part of the code is relevant to the detected vulnerability. We present IVDetect an interpretable vulnerability detector with the philosophy of using Artificial Intelligence (AI) to detect vulnerabilities, while using Intelligence Assistant (IA) via providing VD interpretations in terms of vulnerable statements.   For vulnerability detection, we separately consider the vulnerable statements and their surrounding contexts via data and control dependencies. This allows our model better discriminate vulnerable statements than using the mixture of vulnerable code and~contextual code as in existing approaches. In addition to the coarse-grained vulnerability detection result, we leverage interpretable AI to provide users with fine-grained interpretations that include the sub-graph in the Program Dependency Graph (PDG) with the crucial statements that are relevant to the detected vulnerability. Our empirical evaluation on vulnerability databases shows that IVDetect outperforms the existing DL-based approaches by 43%--84% and 105%--255% in top-10 nDCG and MAP ranking scores. IVDetect correctly points out the vulnerable statements relevant to the vulnerability via its interpretation~in 67% of the cases with a top-5 ranked list. It improves over baseline interpretation models by 12.3%--400% and 9%--400% in accuracy.

</details>

<details>

<summary>2021-06-19 19:37:20 - gazel: Supporting Source Code Edits in Eye-Tracking Studies</summary>

- *Sarah Fakhoury, Devjeet Roy, Harry Pines, Tyler Cleveland, Cole Peterson, Venera Arnaoudova, Bonita Sharif, Jonathan Maletic*

- `2106.10563v1` - [abs](http://arxiv.org/abs/2106.10563v1) - [pdf](http://arxiv.org/pdf/2106.10563v1)

> Eye tracking tools are used in software engineering research to study various software development activities. However, a major limitation of these tools is their inability to track gaze data for activities that involve source code editing. We present a novel solution to support eye tracking experiments for tasks involving source code edits as an extension of the iTrace community infrastructure. We introduce the iTrace-Atom plugin and gazel -- a Python data processing pipeline that maps gaze information to changing source code elements and provides researchers with a way to query this dynamic data. iTrace-Atom is evaluated via a series of simulations and is over 99% accurate at high eye-tracking speeds of over 1,000Hz. iTrace and gazel completely revolutionize the way eye tracking studies are conducted in realistic settings with the presence of scrolling, context switching, and now editing. This opens the doors to support many day-to-day software engineering tasks such as bug fixing, adding new features, and refactoring.

</details>

<details>

<summary>2021-06-21 01:05:07 - An empirical evaluation of the usefulness of Tree Kernels for Commit-time Defect Detection in large software systems</summary>

- *Hareem Sahar, Yuxin Liu, Abram Hindle, Denilson Barbosa*

- `2106.10789v1` - [abs](http://arxiv.org/abs/2106.10789v1) - [pdf](http://arxiv.org/pdf/2106.10789v1)

> Defect detection at commit check-in time prevents the introduction of defects into software systems. Current defect detection approaches rely on metric-based models which are not very accurate and whose results are not directly useful for developers. We propose a method to detect bug-inducing commits by comparing the incoming changes with all past commits in the project, considering both those that introduced defects and those that did not. Our method considers individual changes in the commit separately, at the method-level granularity. Doing so helps developers as they are informed of specific methods that need further attention instead of being told that the entire commit is problematic. Our approach represents source code as abstract syntax trees and uses tree kernels to estimate the similarity of the code with previous commits. We experiment with subtree kernels (STK), subset tree kernels (SSTK), or partial tree kernels (PTK). An incoming change is then classified using a K-NN classifier on the past changes. We evaluate our approach on the BigCloneBench benchmark and on the Technical Debt dataset, using the NiCad clone detector as the baseline. Our experiments with the BigCloneBench benchmark show that the tree kernel approach can detect clones with a comparable MAP to that of NiCad. Also, on defect detection with the Technical Debt dataset, tree kernels are least as effective as NiCad with MRR, F-score, and Accuracy of 0.87, 0.80, and 0.82 respectively.

</details>

<details>

<summary>2021-06-21 14:24:22 - An Exploratory Study on Architectural Knowledge in Issue Tracking Systems</summary>

- *Mohamed Soliman, Matthias Galster, Paris Avgeriou*

- `2106.11140v1` - [abs](http://arxiv.org/abs/2106.11140v1) - [pdf](http://arxiv.org/pdf/2106.11140v1)

> Software developers use issue trackers (e.g. Jira) to manage defects, bugs, tasks, change requests, etc. In this paper we explore (a) how architectural knowledge concepts (e.g. architectural component behavior, contextual constraints) are textually represented in issues (e.g. as adjectives), (b) which architectural knowledge concepts commonly occur in issues, and (c) which architectural knowledge concepts appear together. We analyzed issues in the Jira issue trackers of three large Apache projects. To identify ``architecturally relevant'' issues, we linked issues to architecturally relevant source code changes in the studied systems. We then developed a code book by manually labeling a subset of issues. After reaching conceptual saturation, we coded remaining issues. Our findings support empirically-grounded search tools to identify architectural knowledge concepts in issues for future reuse.

</details>

<details>

<summary>2021-06-21 15:21:49 - Revisiting Model's Uncertainty and Confidences for Adversarial Example Detection</summary>

- *Ahmed Aldahdooh, Wassim Hamidouche, Olivier Déforges*

- `2103.05354v2` - [abs](http://arxiv.org/abs/2103.05354v2) - [pdf](http://arxiv.org/pdf/2103.05354v2)

> Security-sensitive applications that rely on Deep Neural Networks (DNNs) are vulnerable to small perturbations that are crafted to generate Adversarial Examples(AEs). The AEs are imperceptible to humans and cause DNN to misclassify them. Many defense and detection techniques have been proposed. Model's confidences and Dropout, as a popular way to estimate the model's uncertainty, have been used for AE detection but they showed limited success against black- and gray-box attacks. Moreover, the state-of-the-art detection techniques have been designed for specific attacks or broken by others, need knowledge about the attacks, are not consistent, increase model parameters overhead, are time-consuming, or have latency in inference time. To trade off these factors, we revisit the model's uncertainty and confidences and propose a novel unsupervised ensemble AE detection mechanism that 1) uses the uncertainty method called SelectiveNet, 2) processes model layers outputs, i.e.feature maps, to generate new confidence probabilities. The detection method is called Selective and Feature based Adversarial Detection (SFAD). Experimental results show that the proposed approach achieves better performance against black- and gray-box attacks than the state-of-the-art methods and achieves comparable performance against white-box attacks. Moreover, results show that SFAD is fully robust against High Confidence Attacks (HCAs) for MNIST and partially robust for CIFAR10 datasets.

</details>

<details>

<summary>2021-06-21 15:52:10 - HFContractFuzzer: Fuzzing Hyperledger Fabric Smart Contracts for Vulnerability Detection</summary>

- *Mengjie Ding, Peiru Li, Shanshan Li, He Zhang*

- `2106.11210v1` - [abs](http://arxiv.org/abs/2106.11210v1) - [pdf](http://arxiv.org/pdf/2106.11210v1)

> With its unique advantages such as decentralization and immutability, blockchain technology has been widely used in various fields in recent years. The smart contract running on the blockchain is also playing an increasingly important role in decentralized application scenarios. Therefore, the automatic detection of security vulnerabilities in smart contracts has become an urgent problem in the application of blockchain technology. Hyperledger Fabric is a smart contract platform based on enterprise-level licensed distributed ledger technology. However, the research on the vulnerability detection technology of Hyperledger Fabric smart contracts is still in its infancy. In this paper, we propose HFContractFuzzer, a method based on Fuzzing technology to detect Hyperledger Fabric smart contracts, which combines a Fuzzing tool for golang named go-fuzz and smart contracts written by golang. We use HFContractFuzzer to detect vulnerabilities in five contracts from typical sources and discover that four of them have security vulnerabilities, proving the effectiveness of the proposed method.

</details>

<details>

<summary>2021-06-21 16:13:32 - Defending against Backdoor Attack on Deep Neural Networks</summary>

- *Kaidi Xu, Sijia Liu, Pin-Yu Chen, Pu Zhao, Xue Lin*

- `2002.12162v2` - [abs](http://arxiv.org/abs/2002.12162v2) - [pdf](http://arxiv.org/pdf/2002.12162v2)

> Although deep neural networks (DNNs) have achieved a great success in various computer vision tasks, it is recently found that they are vulnerable to adversarial attacks. In this paper, we focus on the so-called \textit{backdoor attack}, which injects a backdoor trigger to a small portion of training data (also known as data poisoning) such that the trained DNN induces misclassification while facing examples with this trigger. To be specific, we carefully study the effect of both real and synthetic backdoor attacks on the internal response of vanilla and backdoored DNNs through the lens of Gard-CAM. Moreover, we show that the backdoor attack induces a significant bias in neuron activation in terms of the $\ell_\infty$ norm of an activation map compared to its $\ell_1$ and $\ell_2$ norm. Spurred by our results, we propose the \textit{$\ell_\infty$-based neuron pruning} to remove the backdoor from the backdoored DNN. Experiments show that our method could effectively decrease the attack success rate, and also hold a high classification accuracy for clean images.

</details>

<details>

<summary>2021-06-21 16:25:55 - Why flatness does and does not correlate with generalization for deep neural networks</summary>

- *Shuofeng Zhang, Isaac Reid, Guillermo Valle Pérez, Ard Louis*

- `2103.06219v2` - [abs](http://arxiv.org/abs/2103.06219v2) - [pdf](http://arxiv.org/pdf/2103.06219v2)

> The intuition that local flatness of the loss landscape is correlated with better generalization for deep neural networks (DNNs) has been explored for decades, spawning many different flatness measures. Recently, this link with generalization has been called into question by a demonstration that many measures of flatness are vulnerable to parameter re-scaling which arbitrarily changes their value without changing neural network outputs.   Here we show that, in addition, some popular variants of SGD such as Adam and Entropy-SGD, can also break the flatness-generalization correlation. As an alternative to flatness measures, we use a function based picture and propose using the log of Bayesian prior upon initialization, $\log P(f)$, as a predictor of the generalization when a DNN converges on function $f$ after training to zero error. The prior is directly proportional to the Bayesian posterior for functions that give zero error on a test set. For the case of image classification, we show that $\log P(f)$ is a significantly more robust predictor of generalization than flatness measures are.   Whilst local flatness measures fail under parameter re-scaling, the prior/posterior, which is global quantity, remains invariant under re-scaling. Moreover, the correlation with generalization as a function of data complexity remains good for different variants of SGD.

</details>

<details>

<summary>2021-06-21 19:37:06 - Membership Inference on Word Embedding and Beyond</summary>

- *Saeed Mahloujifar, Huseyin A. Inan, Melissa Chase, Esha Ghosh, Marcello Hasegawa*

- `2106.11384v1` - [abs](http://arxiv.org/abs/2106.11384v1) - [pdf](http://arxiv.org/pdf/2106.11384v1)

> In the text processing context, most ML models are built on word embeddings. These embeddings are themselves trained on some datasets, potentially containing sensitive data. In some cases this training is done independently, in other cases, it occurs as part of training a larger, task-specific model. In either case, it is of interest to consider membership inference attacks based on the embedding layer as a way of understanding sensitive information leakage. But, somewhat surprisingly, membership inference attacks on word embeddings and their effect in other natural language processing (NLP) tasks that use these embeddings, have remained relatively unexplored.   In this work, we show that word embeddings are vulnerable to black-box membership inference attacks under realistic assumptions. Furthermore, we show that this leakage persists through two other major NLP applications: classification and text-generation, even when the embedding layer is not exposed to the attacker. We show that our MI attack achieves high attack accuracy against a classifier model and an LSTM-based language model. Indeed, our attack is a cheaper membership inference attack on text-generative models, which does not require the knowledge of the target model or any expensive training of text-generative models as shadow models.

</details>

<details>

<summary>2021-06-22 00:40:54 - Assertion Based Functional Verification of March Algorithm Based MBIST Controller</summary>

- *Ashwani Kumar*

- `2106.11461v1` - [abs](http://arxiv.org/abs/2106.11461v1) - [pdf](http://arxiv.org/pdf/2106.11461v1)

> The thesis work presents assertion based functional verification of RTL representation of a digital design. The MBIST controller is designed based on a memory testing March algorithm. This March algorithm is a little modified March C algorithm which is modified by adding a paused element to test memory data retention faults. In assertion based functional verification, creation of verification plan, for MBIST controller RTL model and the implementation & simulation of the verification plan using System-Verilog and Synopsys-VCS are done. In ABV, verification plan includes the MBIST controller design and functional specification, functional coverage goals, code coverage goals, and assertions. Assertions are used to check the errors in RTL model of MBIST controller and to provide the functionality coverage. Functional coverage metrics are used to track the level or quality of verification. Most of the functional metrics score approximately reached the planned goal of 100 % which is planned in the verification plan. The designed MBIST controller is verified against the intended features. ABV approach helped to make the verification and design process efficient and less time-consuming by finding the bugs, exercising the corner cases in the design, and using the directed test cases in a small design. ABV helped to write directed and efficient test cases (25) which are approx 32 % less than the use of maximum possible random test cases (88) for designed MBIST controller with 100% assertion coverage and approximately equal total functional coverage, i.e., 97 % approx. In this way, ABV helped to fasten the design and verification process with better quality and assurance of correct functionality of MBIST controller after the integration in MBIST architecture.

</details>

<details>

<summary>2021-06-22 03:00:11 - An Accurate Non-accelerometer-based PPG Motion Artifact Removal Technique using CycleGAN</summary>

- *Amir Hosein Afandizadeh Zargari, Seyed Amir Hossein Aqajari, Hadi Khodabandeh, Amir M. Rahmani, Fadi Kurdahi*

- `2106.11512v1` - [abs](http://arxiv.org/abs/2106.11512v1) - [pdf](http://arxiv.org/pdf/2106.11512v1)

> A photoplethysmography (PPG) is an uncomplicated and inexpensive optical technique widely used in the healthcare domain to extract valuable health-related information, e.g., heart rate variability, blood pressure, and respiration rate. PPG signals can easily be collected continuously and remotely using portable wearable devices. However, these measuring devices are vulnerable to motion artifacts caused by daily life activities. The most common ways to eliminate motion artifacts use extra accelerometer sensors, which suffer from two limitations: i) high power consumption and ii) the need to integrate an accelerometer sensor in a wearable device (which is not required in certain wearables). This paper proposes a low-power non-accelerometer-based PPG motion artifacts removal method outperforming the accuracy of the existing methods. We use Cycle Generative Adversarial Network to reconstruct clean PPG signals from noisy PPG signals. Our novel machine-learning-based technique achieves 9.5 times improvement in motion artifact removal compared to the state-of-the-art without using extra sensors such as an accelerometer.

</details>

<details>

<summary>2021-06-22 14:07:54 - Meta Adversarial Training against Universal Patches</summary>

- *Jan Hendrik Metzen, Nicole Finnie, Robin Hutmacher*

- `2101.11453v2` - [abs](http://arxiv.org/abs/2101.11453v2) - [pdf](http://arxiv.org/pdf/2101.11453v2)

> Recently demonstrated physical-world adversarial attacks have exposed vulnerabilities in perception systems that pose severe risks for safety-critical applications such as autonomous driving. These attacks place adversarial artifacts in the physical world that indirectly cause the addition of a universal patch to inputs of a model that can fool it in a variety of contexts. Adversarial training is the most effective defense against image-dependent adversarial attacks. However, tailoring adversarial training to universal patches is computationally expensive since the optimal universal patch depends on the model weights which change during training. We propose meta adversarial training (MAT), a novel combination of adversarial training with meta-learning, which overcomes this challenge by meta-learning universal patches along with model training. MAT requires little extra computation while continuously adapting a large set of patches to the current model. MAT considerably increases robustness against universal patch attacks on image classification and traffic-light detection.

</details>

<details>

<summary>2021-06-22 15:16:04 - Adversarial Robustness vs Model Compression, or Both?</summary>

- *Shaokai Ye, Kaidi Xu, Sijia Liu, Jan-Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, Xue Lin*

- `1903.12561v5` - [abs](http://arxiv.org/abs/1903.12561v5) - [pdf](http://arxiv.org/pdf/1903.12561v5)

> It is well known that deep neural networks (DNNs) are vulnerable to adversarial attacks, which are implemented by adding crafted perturbations onto benign examples. Min-max robust optimization based adversarial training can provide a notion of security against adversarial attacks. However, adversarial robustness requires a significantly larger capacity of the network than that for the natural training with only benign examples. This paper proposes a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training. Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and finds that weight pruning is essential for reducing the network model size in the adversarial setting, training a small model from scratch even with inherited initialization from the large model cannot achieve both adversarial robustness and high standard accuracy. Code is available at https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.

</details>

<details>

<summary>2021-06-22 19:09:03 - DetectX -- Adversarial Input Detection using Current Signatures in Memristive XBar Arrays</summary>

- *Abhishek Moitra, Priyadarshini Panda*

- `2106.12021v1` - [abs](http://arxiv.org/abs/2106.12021v1) - [pdf](http://arxiv.org/pdf/2106.12021v1)

> Adversarial input detection has emerged as a prominent technique to harden Deep Neural Networks(DNNs) against adversarial attacks. Most prior works use neural network-based detectors or complex statistical analysis for adversarial detection. These approaches are computationally intensive and vulnerable to adversarial attacks. To this end, we propose DetectX - a hardware friendly adversarial detection mechanism using hardware signatures like Sum of column Currents (SoI) in memristive crossbars (XBar). We show that adversarial inputs have higher SoI compared to clean inputs. However, the difference is too small for reliable adversarial detection. Hence, we propose a dual-phase training methodology: Phase1 training is geared towards increasing the separation between clean and adversarial SoIs; Phase2 training improves the overall robustness against different strengths of adversarial attacks. For hardware-based adversarial detection, we implement the DetectX module using 32nm CMOS circuits and integrate it with a Neurosim-like analog crossbar architecture. We perform hardware evaluation of the Neurosim+DetectX system on the Neurosim platform using datasets-CIFAR10(VGG8), CIFAR100(VGG16) and TinyImagenet(ResNet18). Our experiments show that DetectX is 10x-25x more energy efficient and immune to dynamic adversarial attacks compared to previous state-of-the-art works. Moreover, we achieve high detection performance (ROC-AUC > 0.95) for strong white-box and black-box attacks. The code has been released at https://github.com/Intelligent-Computing-Lab-Yale/DetectX

</details>

<details>

<summary>2021-06-22 19:45:04 - Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout</summary>

- *Pengfei Xie, Linyuan Wang, Ruoxi Qin, Kai Qiao, Shuhao Shi, Guoen Hu, Bin Yan*

- `2106.01617v2` - [abs](http://arxiv.org/abs/2106.01617v2) - [pdf](http://arxiv.org/pdf/2106.01617v2)

> Deep neural networks(DNNs) is vulnerable to be attacked by adversarial examples. Black-box attack is the most threatening attack. At present, black-box attack methods mainly adopt gradient-based iterative attack methods, which usually limit the relationship between the iteration step size, the number of iterations, and the maximum perturbation. In this paper, we propose a new gradient iteration framework, which redefines the relationship between the above three. Under this framework, we easily improve the attack success rate of DI-TI-MIM. In addition, we propose a gradient iterative attack method based on input dropout, which can be well combined with our framework. We further propose a multi dropout rate version of this method. Experimental results show that our best method can achieve attack success rate of 96.2\% for defense model on average, which is higher than the state-of-the-art gradient-based attacks.

</details>

<details>

<summary>2021-06-22 21:44:44 - Probing Model Signal-Awareness via Prediction-Preserving Input Minimization</summary>

- *Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim Laredo, Alessandro Morari*

- `2011.14934v2` - [abs](http://arxiv.org/abs/2011.14934v2) - [pdf](http://arxiv.org/pdf/2011.14934v2)

> This work explores the signal awareness of AI models for source code understanding. Using a software vulnerability detection use case, we evaluate the models' ability to capture the correct vulnerability signals to produce their predictions. Our prediction-preserving input minimization (P2IM) approach systematically reduces the original source code to a minimal snippet which a model needs to maintain its prediction. The model's reliance on incorrect signals is then uncovered when the vulnerability in the original code is missing in the minimal snippet, both of which the model however predicts as being vulnerable. We measure the signal awareness of models using a new metric we propose- Signal-aware Recall (SAR). We apply P2IM on three different neural network architectures across multiple datasets. The results show a sharp drop in the model's Recall from the high 90s to sub-60s with the new metric, highlighting that the models are presumably picking up a lot of noise or dataset nuances while learning their vulnerability detection logic. Although the drop in model performance may be perceived as an adversarial attack, but this isn't P2IM's objective. The idea is rather to uncover the signal-awareness of a black-box model in a data-driven manner via controlled queries. SAR's purpose is to measure the impact of task-agnostic model training, and not to suggest a shortcoming in the Recall metric. The expectation, in fact, is for SAR to match Recall in the ideal scenario where the model truly captures task-specific signals.

</details>

<details>

<summary>2021-06-23 18:51:16 - Introducing Orthogonal Constraint in Structural Probes</summary>

- *Tomasz Limisiewicz, David Mareček*

- `2012.15228v2` - [abs](http://arxiv.org/abs/2012.15228v2) - [pdf](http://arxiv.org/pdf/2012.15228v2)

> With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. isomorphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization.

</details>

<details>

<summary>2021-06-24 02:03:24 - Universal Adversarial Perturbations for CNN Classifiers in EEG-Based BCIs</summary>

- *Zihan Liu, Lubin Meng, Xiao Zhang, Weili Fang, Dongrui Wu*

- `1912.01171v5` - [abs](http://arxiv.org/abs/1912.01171v5) - [pdf](http://arxiv.org/pdf/1912.01171v5)

> Multiple convolutional neural network (CNN) classifiers have been proposed for electroencephalogram (EEG) based brain-computer interfaces (BCIs). However, CNN models have been found vulnerable to universal adversarial perturbations (UAPs), which are small and example-independent, yet powerful enough to degrade the performance of a CNN model, when added to a benign example. This paper proposes a novel total loss minimization (TLM) approach to generate UAPs for EEG-based BCIs. Experimental results demonstrated the effectiveness of TLM on three popular CNN classifiers for both target and non-target attacks. We also verified the transferability of UAPs in EEG-based BCI systems. To our knowledge, this is the first study on UAPs of CNN classifiers in EEG-based BCIs. UAPs are easy to construct, and can attack BCIs in real-time, exposing a potentially critical security concern of BCIs.

</details>

<details>

<summary>2021-06-24 11:32:30 - Empirical Study of Transformers for Source Code</summary>

- *Nadezhda Chirkova, Sergey Troshin*

- `2010.07987v2` - [abs](http://arxiv.org/abs/2010.07987v2) - [pdf](http://arxiv.org/pdf/2010.07987v2)

> Initially developed for natural language processing (NLP), Transformers are now widely used for source code processing, due to the format similarity between source code and text. In contrast to natural language, source code is strictly structured, i.e., it follows the syntax of the programming language. Several recent works develop Transformer modifications for capturing syntactic information in source code. The drawback of these works is that they do not compare to each other and consider different tasks. In this work, we conduct a thorough empirical study of the capabilities of Transformers to utilize syntactic information in different tasks. We consider three tasks (code completion, function naming and bug fixing) and re-implement different syntax-capturing modifications in a unified framework. We show that Transformers are able to make meaningful predictions based purely on syntactic information and underline the best practices of taking the syntactic information into account for improving the performance of the model.

</details>

<details>

<summary>2021-06-24 13:07:03 - A Survey on Human and Personality Vulnerability Assessment in Cyber-security: Challenges, Approaches, and Open Issues</summary>

- *Dimitra Papatsaroucha, Yannis Nikoloudakis, Ioannis Kefaloukos, Evangelos Pallis, Evangelos K. Markakis*

- `2106.09986v2` - [abs](http://arxiv.org/abs/2106.09986v2) - [pdf](http://arxiv.org/pdf/2106.09986v2)

> These days, cyber-criminals target humans rather than machines since they try to accomplish their malicious intentions by exploiting the weaknesses of end users. Thus, human vulnerabilities pose a serious threat to the security and integrity of computer systems and data. The human tendency to trust and help others, as well as personal, social, and cultural characteristics, are indicative of the level of susceptibility that one may exhibit towards certain attack types and deception strategies. This work aims to investigate the factors that affect human susceptibility by studying the existing literature related to this subject. The objective is also to explore and describe state of the art human vulnerability assessment models, current prevention, and mitigation approaches regarding user susceptibility, as well as educational and awareness raising training strategies. Following the review of the literature, several conclusions are reached. Among them, Human Vulnerability Assessment has been included in various frameworks aiming to assess the cyber security capacity of organizations, but it concerns a one time assessment rather than a continuous practice. Moreover, human maliciousness is still neglected from current Human Vulnerability Assessment frameworks; thus, insider threat actors evade identification, which may lead to an increased cyber security risk. Finally, this work proposes a user susceptibility profile according to the factors stemming from our research.

</details>

<details>

<summary>2021-06-24 18:09:28 - AKER: A Design and Verification Framework for Safe andSecure SoC Access Control</summary>

- *Francesco Restuccia, Andres Meza, Ryan Kastner*

- `2106.13263v1` - [abs](http://arxiv.org/abs/2106.13263v1) - [pdf](http://arxiv.org/pdf/2106.13263v1)

> Modern systems on a chip (SoCs) utilize heterogeneous architectures where multiple IP cores have concurrent access to on-chip shared resources. In security-critical applications, IP cores have different privilege levels for accessing shared resources, which must be regulated by an access control system. AKER is a design and verification framework for SoC access control. AKER builds upon the Access Control Wrapper (ACW) -- a high performance and easy-to-integrate hardware module that dynamically manages access to shared resources. To build an SoC access control system, AKER distributes the ACWs throughout the SoC, wrapping controller IP cores, and configuring the ACWs to perform local access control. To ensure the access control system is functioning correctly and securely, AKER provides a property-driven security verification using MITRE common weakness enumerations. AKER verifies the SoC access control at the IP level to ensure the absence of bugs in the functionalities of the ACW module, at the firmware level to confirm the secure operation of the ACW when integrated with a hardware root-of-trust (HRoT), and at the system level to evaluate security threats due to the interactions among shared resources. The performance, resource usage, and security of access control systems implemented through AKER is experimentally evaluated on a Xilinx UltraScale+ programmable SoC, it is integrated with the OpenTitan hardware root-of-trust, and it is used to design an access control system for the OpenPULP multicore architecture.

</details>

<details>

<summary>2021-06-25 04:19:14 - Identifying malicious accounts in Blockchains using Domain Names and associated temporal properties</summary>

- *Rohit Kumar Sachan, Rachit Agarwal, Sandeep Kumar Shukla*

- `2106.13420v1` - [abs](http://arxiv.org/abs/2106.13420v1) - [pdf](http://arxiv.org/pdf/2106.13420v1)

> The rise in the adoption of blockchain technology has led to increased illegal activities by cyber-criminals costing billions of dollars. Many machine learning algorithms are applied to detect such illegal behavior. These algorithms are often trained on the transaction behavior and, in some cases, trained on the vulnerabilities that exist in the system. In our approach, we study the feasibility of using metadata such as Domain Name (DN) associated with the account in the blockchain and identify whether an account should be tagged malicious or not. Here, we leverage the temporal aspects attached to the DNs. Our results identify 144930 DNs that show malicious behavior, and out of these, 54114 DNs show persistent malicious behavior over time. Nonetheless, none of these identified malicious DNs were reported in new officially tagged malicious blockchain DNs.

</details>

<details>

<summary>2021-06-25 04:25:23 - Vulnerability and Transaction behavior based detection of Malicious Smart Contracts</summary>

- *Rachit Agarwal, Tanmay Thapliyal, Sandeep Kumar Shukla*

- `2106.13422v1` - [abs](http://arxiv.org/abs/2106.13422v1) - [pdf](http://arxiv.org/pdf/2106.13422v1)

> Smart Contracts (SCs) in Ethereum can automate tasks and provide different functionalities to a user. Such automation is enabled by the `Turing-complete' nature of the programming language (Solidity) in which SCs are written. This also opens up different vulnerabilities and bugs in SCs that malicious actors exploit to carry out malicious or illegal activities on the cryptocurrency platform. In this work, we study the correlation between malicious activities and the vulnerabilities present in SCs and find that some malicious activities are correlated with certain types of vulnerabilities. We then develop and study the feasibility of a scoring mechanism that corresponds to the severity of the vulnerabilities present in SCs to determine if it is a relevant feature to identify suspicious SCs. We analyze the utility of severity score towards detection of suspicious SCs using unsupervised machine learning (ML) algorithms across different temporal granularities and identify behavioral changes. In our experiments with on-chain SCs, we were able to find a total of 1094 benign SCs across different granularities which behave similar to malicious SCs, with the inclusion of the smart contract vulnerability scores in the feature set.

</details>

<details>

<summary>2021-06-25 09:36:21 - NSL: Hybrid Interpretable Learning From Noisy Raw Data</summary>

- *Daniel Cunnington, Alessandra Russo, Mark Law, Jorge Lobo, Lance Kaplan*

- `2012.05023v2` - [abs](http://arxiv.org/abs/2012.05023v2) - [pdf](http://arxiv.org/pdf/2012.05023v2)

> Inductive Logic Programming (ILP) systems learn generalised, interpretable rules in a data-efficient manner utilising existing background knowledge. However, current ILP systems require training examples to be specified in a structured logical format. Neural networks learn from unstructured data, although their learned models may be difficult to interpret and are vulnerable to data perturbations at run-time. This paper introduces a hybrid neural-symbolic learning framework, called NSL, that learns interpretable rules from labelled unstructured data. NSL combines pre-trained neural networks for feature extraction with FastLAS, a state-of-the-art ILP system for rule learning under the answer set semantics. Features extracted by the neural components define the structured context of labelled examples and the confidence of the neural predictions determines the level of noise of the examples. Using the scoring function of FastLAS, NSL searches for short, interpretable rules that generalise over such noisy examples. We evaluate our framework on propositional and first-order classification tasks using the MNIST dataset as raw data. Specifically, we demonstrate that NSL is able to learn robust rules from perturbed MNIST data and achieve comparable or superior accuracy when compared to neural network and random forest baselines whilst being more general and interpretable.

</details>

<details>

<summary>2021-06-25 14:45:44 - DAEMON: Dataset-Agnostic Explainable Malware Classification Using Multi-Stage Feature Mining</summary>

- *Ron Korine, Danny Hendler*

- `2008.01855v2` - [abs](http://arxiv.org/abs/2008.01855v2) - [pdf](http://arxiv.org/pdf/2008.01855v2)

> Numerous metamorphic and polymorphic malicious variants are generated automatically on a daily basis by mutation engines that transform the code of a malicious program while retaining its functionality, in order to evade signature-based detection. These automatic processes have greatly increased the number of malware variants, deeming their fully-manual analysis impossible. Malware classification is the task of determining to which family a new malicious variant belongs. Variants of the same malware family show similar behavioral patterns. Thus, classifying newly discovered malicious programs and applications helps assess the risks they pose. Moreover, malware classification facilitates determining which of the newly discovered variants should undergo manual analysis by a security expert, in order to determine whether they belong to a new family (e.g., one whose members exploit a zero-day vulnerability) or are simply the result of a concept drift within a known malicious family. This motivated intense research in recent years on devising high-accuracy automatic tools for malware classification. In this work, we present DAEMON - a novel dataset-agnostic malware classifier. A key property of DAEMON is that the type of features it uses and the manner in which they are mined facilitate understanding the distinctive behavior of malware families, making its classification decisions explainable. We've optimized DAEMON using a large-scale dataset of x86 binaries, belonging to a mix of several malware families targeting computers running Windows. We then re-trained it and applied it, without any algorithmic change, feature re-engineering or parameter tuning, to two other large-scale datasets of malicious Android applications consisting of numerous malware families. DAEMON obtained highly accurate classification results on all datasets, establishing that it is also platform-agnostic.

</details>

<details>

<summary>2021-06-25 17:38:22 - Programmable RO (PRO): A Multipurpose Countermeasure against Side-channel and Fault Injection Attack</summary>

- *Yuan Yao, Pantea Kiaei, Richa Singh, Shahin Tajik, Patrick Schaumont*

- `2106.13784v1` - [abs](http://arxiv.org/abs/2106.13784v1) - [pdf](http://arxiv.org/pdf/2106.13784v1)

> Side-channel and fault injection attacks reveal secret information by monitoring or manipulating the physical effects of computations involving secret variables. Circuit-level countermeasures help to deter these attacks, and traditionally such countermeasures have been developed for each attack vector separately. We demonstrate a multipurpose ring oscillator design - Programmable Ring Oscillator (PRO) to address both fault attacks and side-channel attacks in a generic, application-independent manner. PRO, as an integrated primitive, can provide on-chip side-channel resistance, power monitoring, and fault detection capabilities to a secure design. We present a grid of PROs monitoring the on-chip power network to detect anomalies. Such power anomalies may be caused by external factors such as electromagnetic fault injection and power glitches, as well as by internal factors such as hardware Trojans. By monitoring the frequency of the ring oscillators, we are able to detect the on-chip power anomaly in time as well as in location. Moreover, we show that the PROs can also inject a random noise pattern into a design's power consumption. By randomly switching the frequency of a ring oscillator, the resulting power-noise pattern significantly reduces the power-based side-channel leakage of a cipher. We discuss the design of PRO and present measurement results on a Xilinx Spartan-6 FPGA prototype, and we show that side-channel and fault vulnerabilities can be addressed at a low cost by introducing PRO to the design. We conclude that PRO can serve as an application-independent, multipurpose countermeasure.

</details>

<details>

<summary>2021-06-25 18:08:39 - Feature Attributions and Counterfactual Explanations Can Be Manipulated</summary>

- *Dylan Slack, Sophie Hilgard, Sameer Singh, Himabindu Lakkaraju*

- `2106.12563v2` - [abs](http://arxiv.org/abs/2106.12563v2) - [pdf](http://arxiv.org/pdf/2106.12563v2)

> As machine learning models are increasingly used in critical decision-making settings (e.g., healthcare, finance), there has been a growing emphasis on developing methods to explain model predictions. Such \textit{explanations} are used to understand and establish trust in models and are vital components in machine learning pipelines. Though explanations are a critical piece in these systems, there is little understanding about how they are vulnerable to manipulation by adversaries. In this paper, we discuss how two broad classes of explanations are vulnerable to manipulation. We demonstrate how adversaries can design biased models that manipulate model agnostic feature attribution methods (e.g., LIME \& SHAP) and counterfactual explanations that hill-climb during the counterfactual search (e.g., Wachter's Algorithm \& DiCE) into \textit{concealing} the model's biases. These vulnerabilities allow an adversary to deploy a biased model, yet explanations will not reveal this bias, thereby deceiving stakeholders into trusting the model. We evaluate the manipulations on real world data sets, including COMPAS and Communities \& Crime, and find explanations can be manipulated in practice.

</details>

<details>

<summary>2021-06-27 06:09:04 - Who is Responsible for Adversarial Defense?</summary>

- *Kishor Datta Gupta, Dipankar Dasgupta*

- `2106.14152v1` - [abs](http://arxiv.org/abs/2106.14152v1) - [pdf](http://arxiv.org/pdf/2106.14152v1)

> We have seen a surge in research aims toward adversarial attacks and defenses in AI/ML systems. While it is crucial to formulate new attack methods and devise novel defense strategies for robustness, it is also imperative to recognize who is responsible for implementing, validating, and justifying the necessity of these defenses. In particular, which components of the system are vulnerable to what type of adversarial attacks, and the expertise needed to realize the severity of adversarial attacks. Also how to evaluate and address the adversarial challenges in order to recommend defense strategies for different applications. This paper opened a discussion on who should examine and implement the adversarial defenses and the reason behind such efforts.

</details>

<details>

<summary>2021-06-27 10:34:48 - Open, Sesame! Introducing Access Control to Voice Services</summary>

- *Dominika Woszczyk, Alvin Lee, Soteris Demetriou*

- `2106.14191v1` - [abs](http://arxiv.org/abs/2106.14191v1) - [pdf](http://arxiv.org/pdf/2106.14191v1)

> Personal voice assistants (VAs) are shown to be vulnerable against record-and-replay, and other acoustic attacks which allow an adversary to gain unauthorized control of connected devices within a smart home. Existing defenses either lack detection and management capabilities or are too coarse-grained to enable flexible policies on par with other computing interfaces. In this work, we present Sesame, a lightweight framework for edge devices which is the first to enable fine-grained access control of smart-home voice commands. Sesame combines three components: Automatic Speech Recognition, Natural Language Understanding (NLU) and a Policy module. We implemented Sesame on Android devices and demonstrate that our system can enforce security policies for both Alexa and Google Home in real-time (362ms end-to-end inference time), with a lightweight (<25MB) NLU model which exhibits minimal accuracy loss compared to its non-compact equivalent.

</details>

<details>

<summary>2021-06-27 15:28:00 - Evaluating adversarial robustness in simulated cerebellum</summary>

- *Liu Yuezhang, Bo Li, Qifeng Chen*

- `2012.02976v2` - [abs](http://arxiv.org/abs/2012.02976v2) - [pdf](http://arxiv.org/pdf/2012.02976v2)

> It is well known that artificial neural networks are vulnerable to adversarial examples, in which great efforts have been made to improve the robustness. However, such examples are usually imperceptible to humans, and thus their effect on biological neural circuits is largely unknown. This paper will investigate the adversarial robustness in a simulated cerebellum, a well-studied supervised learning system in computational neuroscience. Specifically, we propose to study three unique characteristics revealed in the cerebellum: (i) network width; (ii) long-term depression on the parallel fiber-Purkinje cell synapses; (iii) sparse connectivity in the granule layer, and hypothesize that they will be beneficial for improving robustness. To the best of our knowledge, this is the first attempt to examine the adversarial robustness in simulated cerebellum models.   The results are negative in the experimental phase -- no significant improvements in robustness are discovered from the proposed three mechanisms. Consequently, the cerebellum is expected to be vulnerable to adversarial examples as the deep neural networks under batch training. Neuroscientists are encouraged to fool the biological system in experiments with adversarial attacks.

</details>

<details>

<summary>2021-06-27 16:37:11 - A Systematic Review of Bio-Cyber Interface Technologies and Security Issues for Internet of Bio-Nano Things</summary>

- *Sidra Zafar, Mohsin Nazir, Taimur Bakhshi, Hasan Ali Khattak, Sarmadullah Khan, Muhammad Bilal, Kim-Kwang Raymond Choo, Kyung-Sup Kwak7, Aneeqa Sabah*

- `2106.14273v1` - [abs](http://arxiv.org/abs/2106.14273v1) - [pdf](http://arxiv.org/pdf/2106.14273v1)

> Advances in synthetic biology and nanotechnology have contributed to the design of tools that can be used to control, reuse, modify, and re-engineer cells' structure, as well as enabling engineers to effectively use biological cells as programmable substrates to realize Bio-Nano Things (biological embedded computing devices). Bio-NanoThings are generally tiny, non-intrusive, and concealable devices that can be used for in-vivo applications such as intra-body sensing and actuation networks, where the use of artificial devices can be detrimental. Such (nano-scale) devices can be used in various healthcare settings such as continuous health monitoring, targeted drug delivery, and nano-surgeries. These services can also be grouped to form a collaborative network (i.e., nanonetwork), whose performance can potentially be improved when connected to higher bandwidth external networks such as the Internet, say via 5G. However, to realize the IoBNT paradigm, it is also important to seamlessly connect the biological environment with the technological landscape by having a dynamic interface design to convert biochemical signals from the human body into an equivalent electromagnetic signal (and vice versa). This, unfortunately, risks the exposure of internal biological mechanisms to cyber-based sensing and medical actuation, with potential security and privacy implications. This paper comprehensively reviews bio-cyber interface for IoBNT architecture, focusing on bio-cyber interfacing options for IoBNT like biologically inspired bio-electronic devices, RFID enabled implantable chips, and electronic tattoos. This study also identifies known and potential security and privacy vulnerabilities and mitigation strategies for consideration in future IoBNT designs and implementations.

</details>

<details>

<summary>2021-06-28 00:23:21 - Revelio: ML-Generated Debugging Queries for Distributed Systems</summary>

- *Pradeep Dogga, Karthik Narasimhan, Anirudh Sivaraman, Shiv Kumar Saini, George Varghese, Ravi Netravali*

- `2106.14347v1` - [abs](http://arxiv.org/abs/2106.14347v1) - [pdf](http://arxiv.org/pdf/2106.14347v1)

> A major difficulty in debugging distributed systems lies in manually determining which of the many available debugging tools to use and how to query its logs. Our own study of a production debugging workflow confirms the magnitude of this burden. This paper explores whether a machine-learning model can assist developers in distributed systems debugging. We present Revelio, a debugging assistant which takes user reports and system logs as input, and outputs debugging queries that developers can use to find a bug's root cause. The key challenges lie in (1) combining inputs of different types (e.g., natural language reports and quantitative logs) and (2) generalizing to unseen faults. Revelio addresses these by employing deep neural networks to uniformly embed diverse input sources and potential queries into a high-dimensional vector space. In addition, it exploits observations from production systems to factorize query generation into two computationally and statistically simpler learning tasks. To evaluate Revelio, we built a testbed with multiple distributed applications and debugging tools. By injecting faults and training on logs and reports from 800 Mechanical Turkers, we show that Revelio includes the most helpful query in its predicted list of top-3 relevant queries 96% of the time. Our developer study confirms the utility of Revelio.

</details>

<details>

<summary>2021-06-28 08:53:24 - Automatically Determining a Network Reconnaissance Scope Using Passive Scanning Techniques</summary>

- *Stefan Marksteiner, Bernhard Jandl-Scherf, Harald Lernbeiß*

- `2106.14484v1` - [abs](http://arxiv.org/abs/2106.14484v1) - [pdf](http://arxiv.org/pdf/2106.14484v1)

> The starting point of securing a network is having a concise overview of it. As networks are becoming more and more complex both in general and with the introduction of IoT technology and their topological peculiarities in particular, this is increasingly difficult to achieve. Especially in cyber-physical environments, such as smart factories, gaining a reliable picture of the network can be, due to intertwining of a vast amount of devices and different protocols, a tedious task. Nevertheless, this work is necessary to conduct security audits, compare documentation with actual conditions or found vulnerabilities using an attacker's view, for all of which a reliable topology overview is pivotal. For security auditors, however, there might not much information, such as asset management access, be available beforehand, which is why this paper assumes network to audit as a complete black box. The goal is therefore to set security auditors in a condition of, without having any a priori knowledge at all, automatically gaining a topology oversight. This paper describes, in the context of a bigger system that uses active scanning to determine the network topology, an approach to automate the first steps of this procedure: passively scanning the network and determining the network's scope, as well as gaining a valid address to perform the active scanning. This allows for bootstrapping an automatic network discovery process without prior knowledge.

</details>

<details>

<summary>2021-06-28 11:08:14 - Privacy-Preserving Image Acquisition Using Trainable Optical Kernel</summary>

- *Yamin Sepehri, Pedram Pad, Pascal Frossard, L. Andrea Dunbar*

- `2106.14577v1` - [abs](http://arxiv.org/abs/2106.14577v1) - [pdf](http://arxiv.org/pdf/2106.14577v1)

> Preserving privacy is a growing concern in our society where sensors and cameras are ubiquitous. In this work, for the first time, we propose a trainable image acquisition method that removes the sensitive identity revealing information in the optical domain before it reaches the image sensor. The method benefits from a trainable optical convolution kernel which transmits the desired information while filters out the sensitive content. As the sensitive content is suppressed before it reaches the image sensor, it does not enter the digital domain therefore is unretrievable by any sort of privacy attack. This is in contrast with the current digital privacy-preserving methods that are all vulnerable to direct access attack. Also, in contrast with the previous optical privacy-preserving methods that cannot be trained, our method is data-driven and optimized for the specific application at hand. Moreover, there is no additional computation, memory, or power burden on the acquisition system since this processing happens passively in the optical domain and can even be used together and on top of the fully digital privacy-preserving systems. The proposed approach is adaptable to different digital neural networks and content. We demonstrate it for several scenarios such as smile detection as the desired attribute while the gender is filtered out as the sensitive content. We trained the optical kernel in conjunction with two adversarial neural networks where the analysis network tries to detect the desired attribute and the adversarial network tries to detect the sensitive content. We show that this method can reduce 65.1% of sensitive content when it is selected to be the gender and it only loses 7.3% of the desired content. Moreover, we reconstruct the original faces using the deep reconstruction method that confirms the ineffectiveness of reconstruction attacks to obtain the sensitive content.

</details>

<details>

<summary>2021-06-28 19:51:14 - Avis: In-Situ Model Checking for Unmanned Aerial Vehicles</summary>

- *Max Taylor, Haicheng Chen, Feng Qin, Christopher Stewart*

- `2106.14959v1` - [abs](http://arxiv.org/abs/2106.14959v1) - [pdf](http://arxiv.org/pdf/2106.14959v1)

> Control firmware in unmanned aerial vehicles (UAVs) uses sensors to model and manage flight operations, from takeoff to landing to flying between waypoints. However, sensors can fail at any time during a flight. If control firmware mishandles sensor failures, UAVs can crash, fly away, or suffer other unsafe conditions. In-situ model checking finds sensor failures that could lead to unsafe conditions by systematically failing sensors. However, the type of sensor failure and its timing within a flight affect its manifestation, creating a large search space. We propose Avis, an in-situ model checker to quickly uncover UAV sensor failures that lead to unsafe conditions. Widely used control firmware already support operating modes. Avis injects sensor failures as the control firmware transitions between modes - a key execution point where mishandled software exceptions can trigger unsafe conditions. We implemented Avis and applied it to ArduPilot and PX4. Avis found unsafe conditions 2.4X faster than Bayesian Fault Injection, the leading, state-of-the-art approach. Within the current code base of ArduPilot and PX4, Avis discovered 10 previously unknown software bugs that lead to unsafe conditions. Additionally, we reinserted 5 known bugs that caused serious, unsafe conditions and Avis correctly reported all of them.

</details>

<details>

<summary>2021-06-28 22:07:48 - Validating Static Warnings via Testing Code Fragments</summary>

- *Ashwin Kallingal Joshy, Xueyuan Chen, Benjamin Steenhoek, Wei Le*

- `2106.04735v3` - [abs](http://arxiv.org/abs/2106.04735v3) - [pdf](http://arxiv.org/pdf/2106.04735v3)

> Static analysis is an important approach for finding bugs and vulnerabilities in software. However, inspecting and confirming static warnings are challenging and time-consuming. In this paper, we present a novel solution that automatically generates test cases based on static warnings to validate true and false positives. We designed a syntactic patching algorithm that can generate syntactically valid, semantic preserving executable code fragments from static warnings. We developed a build and testing system to automatically test code fragments using fuzzers, KLEE and Valgrind. We evaluated our techniques using 12 real-world C projects and 1955 warnings from two commercial static analysis tools. We successfully built 68.5% code fragments and generated 1003 test cases. Through automatic testing, we identified 48 true positives and 27 false positives, and 205 likely false positives. We matched 4 CVE and real-world bugs using Helium, and they are only triggered by our tool but not other baseline tools. We found that testing code fragments is scalable and useful; it can trigger bugs that testing entire programs or testing procedures failed to trigger.

</details>

<details>

<summary>2021-06-29 02:42:31 - Efficient Top-k Vulnerable Nodes Detection in Uncertain Graphs</summary>

- *Dawei Cheng, Chen Chen, Xiaoyang Wang, Sheng Xiang*

- `1912.12383v5` - [abs](http://arxiv.org/abs/1912.12383v5) - [pdf](http://arxiv.org/pdf/1912.12383v5)

> Uncertain graphs have been widely used to model complex linked data in many real-world applications, such as guaranteed-loan networks and power grids, where a node or edge may be associated with a probability. In these networks, a node usually has a certain chance of default or breakdown due to self-factors or the influence from upstream nodes. For regulatory authorities and companies, it is critical to efficiently identify the vulnerable nodes, i.e., nodes with high default risks, such that they could pay more attention to these nodes for the purpose of risk management. In this paper, we propose and investigate the problem of top-$k$ vulnerable nodes detection in uncertain graphs. We formally define the problem and prove its hardness. To identify the $k$ most vulnerable nodes, a sampling-based approach is proposed. Rigorous theoretical analysis is conducted to bound the quality of returned results. Novel optimization techniques and a bottom-$k$ sketch based approach are further developed in order to scale for large networks. In the experiments, we demonstrate the performance of proposed techniques on 3 real financial networks and 5 benchmark networks. The evaluation results show that the proposed methods can achieve up to 2 orders of magnitudes speedup compared with the baseline approach. Moreover, to further verify the advantages of our model in real-life scenarios, we integrate the proposed techniques with our current loan risk control system, which is deployed in the collaborated bank, for more evaluation. Particularly, we show that our proposed new model has superior performance on real-life guaranteed-loan network data, which can better predict the default risks of enterprises compared to the state-of-the-art techniques.

</details>

<details>

<summary>2021-06-29 07:00:25 - Analysis and Applications of Class-wise Robustness in Adversarial Training</summary>

- *Qi Tian, Kun Kuang, Kelu Jiang, Fei Wu, Yisen Wang*

- `2105.14240v5` - [abs](http://arxiv.org/abs/2105.14240v5) - [pdf](http://arxiv.org/pdf/2105.14240v5)

> Adversarial training is one of the most effective approaches to improve model robustness against adversarial examples. However, previous works mainly focus on the overall robustness of the model, and the in-depth analysis on the role of each class involved in adversarial training is still missing. In this paper, we propose to analyze the class-wise robustness in adversarial training. First, we provide a detailed diagnosis of adversarial training on six benchmark datasets, i.e., MNIST, CIFAR-10, CIFAR-100, SVHN, STL-10 and ImageNet. Surprisingly, we find that there are remarkable robustness discrepancies among classes, leading to unbalance/unfair class-wise robustness in the robust models. Furthermore, we keep investigating the relations between classes and find that the unbalanced class-wise robustness is pretty consistent among different attack and defense methods. Moreover, we observe that the stronger attack methods in adversarial learning achieve performance improvement mainly from a more successful attack on the vulnerable classes (i.e., classes with less robustness). Inspired by these interesting findings, we design a simple but effective attack method based on the traditional PGD attack, named Temperature-PGD attack, which proposes to enlarge the robustness disparity among classes with a temperature factor on the confidence distribution of each image. Experiments demonstrate our method can achieve a higher attack rate than the PGD attack. Furthermore, from the defense perspective, we also make some modifications in the training and inference phase to improve the robustness of the most vulnerable class, so as to mitigate the large difference in class-wise robustness. We believe our work can contribute to a more comprehensive understanding of adversarial training as well as rethinking the class-wise properties in robust models.

</details>

<details>

<summary>2021-06-29 10:20:20 - Electromagnetic Analysis of an Ultra-Lightweight Cipher: PRESENT</summary>

- *Nilupulee A. Gunathilake, Ahmed Al-Dubai, William J. Buchanan, Owen Lo*

- `2106.15225v1` - [abs](http://arxiv.org/abs/2106.15225v1) - [pdf](http://arxiv.org/pdf/2106.15225v1)

> Side-channel attacks are an unpredictable risk factor in cryptography. Therefore, continuous observations of physical leakages are essential to minimise vulnerabilities associated with cryptographic functions. Lightweight cryptography is a novel approach in progress towards internet-of-things (IoT) security. Thus, it would provide sufficient data and privacy protection in such a constrained ecosystem. IoT devices are resource-limited in terms of data rates (in kbps), power maintainability (battery) as well as hardware and software footprints (physical size, internal memory, RAM/ROM). Due to the difficulty in handling conventional cryptographic algorithms, lightweight ciphers consist of small key sizes, block sizes and few operational rounds. Unlike in the past, affordability to perform side-channel attacks using inexpensive electronic circuitries is becoming a reality. Hence, cryptanalysis of physical leakage in these emerging ciphers is crucial. Among existing studies, power analysis seems to have enough attention in research, whereas other aspects such as electromagnetic, timing, cache and optical attacks continue to be appropriately evaluated to play a role in forensic analysis.   As a result, we started analysing electromagnetic emission leakage of an ultra-lightweight block cipher, PRESENT. According to the literature, PRESENT promises to be adequate for IoT devices, and there still seems not to exist any work regarding correlation electromagnetic analysis (CEMA) of it. Firstly, we conducted simple electromagnetic analysis in both time and frequency domains and then proceeded towards CEMA attack modelling. This paper provides a summary of the related literature (IoT, lightweight cryptography, side-channel attacks and EMA), our methodology, current outcomes and future plans for the optimised results.

</details>

<details>

<summary>2021-06-29 12:50:20 - Attack Transferability Characterization for Adversarially Robust Multi-label Classification</summary>

- *Zhuo Yang, Yufei Han, Xiangliang Zhang*

- `2106.15360v1` - [abs](http://arxiv.org/abs/2106.15360v1) - [pdf](http://arxiv.org/pdf/2106.15360v1)

> Despite of the pervasive existence of multi-label evasion attack, it is an open yet essential problem to characterize the origin of the adversarial vulnerability of a multi-label learning system and assess its attackability. In this study, we focus on non-targeted evasion attack against multi-label classifiers. The goal of the threat is to cause miss-classification with respect to as many labels as possible, with the same input perturbation. Our work gains in-depth understanding about the multi-label adversarial attack by first characterizing the transferability of the attack based on the functional properties of the multi-label classifier. We unveil how the transferability level of the attack determines the attackability of the classifier via establishing an information-theoretic analysis of the adversarial risk. Furthermore, we propose a transferability-centered attackability assessment, named Soft Attackability Estimator (SAE), to evaluate the intrinsic vulnerability level of the targeted multi-label classifier. This estimator is then integrated as a transferability-tuning regularization term into the multi-label learning paradigm to achieve adversarially robust classification. The experimental study on real-world data echos the theoretical analysis and verify the validity of the transferability-regularized multi-label learning method.

</details>

<details>

<summary>2021-06-30 02:11:09 - Local Reweighting for Adversarial Training</summary>

- *Ruize Gao, Feng Liu, Kaiwen Zhou, Gang Niu, Bo Han, James Cheng*

- `2106.15776v1` - [abs](http://arxiv.org/abs/2106.15776v1) - [pdf](http://arxiv.org/pdf/2106.15776v1)

> Instances-reweighted adversarial training (IRAT) can significantly boost the robustness of trained models, where data being less/more vulnerable to the given attack are assigned smaller/larger weights during training. However, when tested on attacks different from the given attack simulated in training, the robustness may drop significantly (e.g., even worse than no reweighting). In this paper, we study this problem and propose our solution--locally reweighted adversarial training (LRAT). The rationale behind IRAT is that we do not need to pay much attention to an instance that is already safe under the attack. We argue that the safeness should be attack-dependent, so that for the same instance, its weight can change given different attacks based on the same model. Thus, if the attack simulated in training is mis-specified, the weights of IRAT are misleading. To this end, LRAT pairs each instance with its adversarial variants and performs local reweighting inside each pair, while performing no global reweighting--the rationale is to fit the instance itself if it is immune to the attack, but not to skip the pair, in order to passively defend different attacks in future. Experiments show that LRAT works better than both IRAT (i.e., global reweighting) and the standard AT (i.e., no reweighting) when trained with an attack and tested on different attacks.

</details>

<details>

<summary>2021-06-30 02:38:17 - Understanding Adversarial Examples Through Deep Neural Network's Response Surface and Uncertainty Regions</summary>

- *Juan Shu, Bowei Xi, Charles Kamhoua*

- `2107.00003v1` - [abs](http://arxiv.org/abs/2107.00003v1) - [pdf](http://arxiv.org/pdf/2107.00003v1)

> Deep neural network (DNN) is a popular model implemented in many systems to handle complex tasks such as image classification, object recognition, natural language processing etc. Consequently DNN structural vulnerabilities become part of the security vulnerabilities in those systems. In this paper we study the root cause of DNN adversarial examples. We examine the DNN response surface to understand its classification boundary. Our study reveals the structural problem of DNN classification boundary that leads to the adversarial examples. Existing attack algorithms can generate from a handful to a few hundred adversarial examples given one clean image. We show there are infinitely many adversarial images given one clean sample, all within a small neighborhood of the clean sample. We then define DNN uncertainty regions and show transferability of adversarial examples is not universal. We also argue that generalization error, the large sample theoretical guarantee established for DNN, cannot adequately capture the phenomenon of adversarial examples. We need new theory to measure DNN robustness.

</details>

<details>

<summary>2021-06-30 03:05:58 - Adversarial Machine Learning for Cybersecurity and Computer Vision: Current Developments and Challenges</summary>

- *Bowei Xi*

- `2107.02894v1` - [abs](http://arxiv.org/abs/2107.02894v1) - [pdf](http://arxiv.org/pdf/2107.02894v1)

> We provide a comprehensive overview of adversarial machine learning focusing on two application domains, i.e., cybersecurity and computer vision. Research in adversarial machine learning addresses a significant threat to the wide application of machine learning techniques -- they are vulnerable to carefully crafted attacks from malicious adversaries. For example, deep neural networks fail to correctly classify adversarial images, which are generated by adding imperceptible perturbations to clean images.We first discuss three main categories of attacks against machine learning techniques -- poisoning attacks, evasion attacks, and privacy attacks. Then the corresponding defense approaches are introduced along with the weakness and limitations of the existing defense approaches. We notice adversarial samples in cybersecurity and computer vision are fundamentally different. While adversarial samples in cybersecurity often have different properties/distributions compared with training data, adversarial images in computer vision are created with minor input perturbations. This further complicates the development of robust learning techniques, because a robust learning technique must withstand different types of attacks.

</details>

<details>

<summary>2021-06-30 08:35:49 - Whose Opinions Matter? Perspective-aware Models to Identify Opinions of Hate Speech Victims in Abusive Language Detection</summary>

- *Sohail Akhtar, Valerio Basile, Viviana Patti*

- `2106.15896v1` - [abs](http://arxiv.org/abs/2106.15896v1) - [pdf](http://arxiv.org/pdf/2106.15896v1)

> Social media platforms provide users the freedom of expression and a medium to exchange information and express diverse opinions. Unfortunately, this has also resulted in the growth of abusive content with the purpose of discriminating people and targeting the most vulnerable communities such as immigrants, LGBT, Muslims, Jews and women. Because abusive language is subjective in nature, there might be highly polarizing topics or events involved in the annotation of abusive contents such as hate speech (HS). Therefore, we need novel approaches to model conflicting perspectives and opinions coming from people with different personal and demographic backgrounds. In this paper, we present an in-depth study to model polarized opinions coming from different communities under the hypothesis that similar characteristics (ethnicity, social background, culture etc.) can influence the perspectives of annotators on a certain phenomenon. We believe that by relying on this information, we can divide the annotators into groups sharing similar perspectives. We can create separate gold standards, one for each group, to train state-of-the-art deep learning models. We can employ an ensemble approach to combine the perspective-aware classifiers from different groups to an inclusive model. We also propose a novel resource, a multi-perspective English language dataset annotated according to different sub-categories relevant for characterising online abuse: hate speech, aggressiveness, offensiveness and stereotype. By training state-of-the-art deep learning models on this novel resource, we show how our approach improves the prediction performance of a state-of-the-art supervised classifier.

</details>

<details>

<summary>2021-06-30 23:44:27 - A Survey on Trust Metrics for Autonomous Robotic Systems</summary>

- *Vincenzo DiLuoffo, William R. Michalson*

- `2106.15015v2` - [abs](http://arxiv.org/abs/2106.15015v2) - [pdf](http://arxiv.org/pdf/2106.15015v2)

> This paper surveys the area of Trust Metrics related to security for autonomous robotic systems. As the robotics industry undergoes a transformation from programmed, task oriented, systems to Artificial Intelligence-enabled learning, these autonomous systems become vulnerable to several security risks, making a security assessment of these systems of critical importance. Therefore, our focus is on a holistic approach for assessing system trust which requires incorporating system, hardware, software, cognitive robustness, and supplier level trust metrics into a unified model of trust. We set out to determine if there were already trust metrics that defined such a holistic system approach. While there are extensive writings related to various aspects of robotic systems such as, risk management, safety, security assurance and so on, each source only covered subsets of an overall system and did not consistently incorporate the relevant costs in their metrics. This paper attempts to put this prior work into perspective, and to show how it might be extended to develop useful system-level trust metrics for evaluating complex robotic (and other) systems.

</details>


## 2021-07

<details>

<summary>2021-07-01 00:40:01 - Bi-Level Poisoning Attack Model and Countermeasure for Appliance Consumption Data of Smart Homes</summary>

- *Mustain Billah, Adnan Anwar, Ziaur Rahman, Syed Md. Galib*

- `2107.02897v1` - [abs](http://arxiv.org/abs/2107.02897v1) - [pdf](http://arxiv.org/pdf/2107.02897v1)

> Accurate building energy prediction is useful in various applications starting from building energy automation and management to optimal storage control. However, vulnerabilities should be considered when designing building energy prediction models, as intelligent attackers can deliberately influence the model performance using sophisticated attack models. These may consequently degrade the prediction accuracy, which may affect the efficiency and performance of the building energy management systems. In this paper, we investigate the impact of bi-level poisoning attacks on regression models of energy usage obtained from household appliances. Furthermore, an effective countermeasure against the poisoning attacks on the prediction model is proposed in this paper. Attacks and defenses are evaluated on a benchmark dataset. Experimental results show that an intelligent cyber-attacker can poison the prediction model to manipulate the decision. However, our proposed solution successfully ensures defense against such poisoning attacks effectively compared to other benchmark techniques.

</details>

<details>

<summary>2021-07-01 02:31:22 - Attention Meets Perturbations: Robust and Interpretable Attention with Adversarial Training</summary>

- *Shunsuke Kitada, Hitoshi Iyatomi*

- `2009.12064v2` - [abs](http://arxiv.org/abs/2009.12064v2) - [pdf](http://arxiv.org/pdf/2009.12064v2)

> Although attention mechanisms have been applied to a variety of deep learning models and have been shown to improve the prediction performance, it has been reported to be vulnerable to perturbations to the mechanism. To overcome the vulnerability to perturbations in the mechanism, we are inspired by adversarial training (AT), which is a powerful regularization technique for enhancing the robustness of the models. In this paper, we propose a general training technique for natural language processing tasks, including AT for attention (Attention AT) and more interpretable AT for attention (Attention iAT). The proposed techniques improved the prediction performance and the model interpretability by exploiting the mechanisms with AT. In particular, Attention iAT boosts those advantages by introducing adversarial perturbation, which enhances the difference in the attention of the sentences. Evaluation experiments with ten open datasets revealed that AT for attention mechanisms, especially Attention iAT, demonstrated (1) the best performance in nine out of ten tasks and (2) more interpretable attention (i.e., the resulting attention correlated more strongly with gradient-based word importance) for all tasks. Additionally, the proposed techniques are (3) much less dependent on perturbation size in AT. Our code is available at https://github.com/shunk031/attention-meets-perturbation

</details>

<details>

<summary>2021-07-01 06:02:24 - Compression Implies Generalization</summary>

- *Allan Grønlund, Mikael Høgsgaard, Lior Kamma, Kasper Green Larsen*

- `2106.07989v2` - [abs](http://arxiv.org/abs/2106.07989v2) - [pdf](http://arxiv.org/pdf/2106.07989v2)

> Explaining the surprising generalization performance of deep neural networks is an active and important line of research in theoretical machine learning. Influential work by Arora et al. (ICML'18) showed that, noise stability properties of deep nets occurring in practice can be used to provably compress model representations. They then argued that the small representations of compressed networks imply good generalization performance albeit only of the compressed nets. Extending their compression framework to yield generalization bounds for the original uncompressed networks remains elusive.   Our main contribution is the establishment of a compression-based framework for proving generalization bounds. The framework is simple and powerful enough to extend the generalization bounds by Arora et al. to also hold for the original network. To demonstrate the flexibility of the framework, we also show that it allows us to give simple proofs of the strongest known generalization bounds for other popular machine learning models, namely Support Vector Machines and Boosting.

</details>

<details>

<summary>2021-07-01 09:02:16 - Generalized Dirichlet-process-means for $f$-separable distortion measures</summary>

- *Masahiro Kobayashi, Kazuho Watanabe*

- `1901.11331v3` - [abs](http://arxiv.org/abs/1901.11331v3) - [pdf](http://arxiv.org/pdf/1901.11331v3)

> DP-means clustering was obtained as an extension of $K$-means clustering. While it is implemented with a simple and efficient algorithm, it can estimate the number of clusters simultaneously. However, DP-means is specifically designed for the average distortion measure. Therefore, it is vulnerable to outliers in data, and can cause large maximum distortion in clusters. In this work, we extend the objective function of the DP-means to $f$-separable distortion measures and propose a unified learning algorithm to overcome the above problems by selecting the function $f$. Further, the influence function of the estimated cluster center is analyzed to evaluate the robustness against outliers. We demonstrate the performance of the generalized method by numerical experiments using real datasets.

</details>

<details>

<summary>2021-07-01 12:56:36 - DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking Neural Networks</summary>

- *Alberto Marchisio, Giacomo Pira, Maurizio Martina, Guido Masera, Muhammad Shafique*

- `2107.00415v1` - [abs](http://arxiv.org/abs/2107.00415v1) - [pdf](http://arxiv.org/pdf/2107.00415v1)

> Spiking Neural Networks (SNNs), despite being energy-efficient when implemented on neuromorphic hardware and coupled with event-based Dynamic Vision Sensors (DVS), are vulnerable to security threats, such as adversarial attacks, i.e., small perturbations added to the input for inducing a misclassification. Toward this, we propose DVS-Attacks, a set of stealthy yet efficient adversarial attack methodologies targeted to perturb the event sequences that compose the input of the SNNs. First, we show that noise filters for DVS can be used as defense mechanisms against adversarial attacks. Afterwards, we implement several attacks and test them in the presence of two types of noise filters for DVS cameras. The experimental results show that the filters can only partially defend the SNNs against our proposed DVS-Attacks. Using the best settings for the noise filters, our proposed Mask Filter-Aware Dash Attack reduces the accuracy by more than 20% on the DVS-Gesture dataset and by more than 65% on the MNIST dataset, compared to the original clean frames. The source code of all the proposed DVS-Attacks and noise filters is released at https://github.com/albertomarchisio/DVS-Attacks.

</details>

<details>

<summary>2021-07-01 13:34:12 - CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding</summary>

- *Dong Wang, Ning Ding, Piji Li, Hai-Tao Zheng*

- `2107.00440v1` - [abs](http://arxiv.org/abs/2107.00440v1) - [pdf](http://arxiv.org/pdf/2107.00440v1)

> Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes. To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes. To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking. By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations. Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks. And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level.

</details>

<details>

<summary>2021-07-01 19:59:32 - Verifying Verified Code</summary>

- *Siddharth Priya, Xiang Zhou, Yusen Su, Yakir Vizel, Yuyan Bao, Arie Gurfinkel*

- `2107.00723v1` - [abs](http://arxiv.org/abs/2107.00723v1) - [pdf](http://arxiv.org/pdf/2107.00723v1)

> A recent case study from AWS by Chong et al. proposes an effective methodology for Bounded Model Checking in industry. In this paper, we report on a follow up case study that explores the methodology from the perspective of three research questions: (a) can proof artifacts be used across verification tools; (b) are there bugs in verified code; and (c) can specifications be improved. To study these questions, we port the verification tasks for $\texttt{aws-c-common}$ library to SEAHORN and KLEE. We show the benefits of using compiler semantics and cross-checking specifications with different verification techniques, and call for standardizing proof library extensions to increase specification reuse. The verification tasks discussed are publicly available online.

</details>

<details>

<summary>2021-07-02 06:49:44 - An Analysis of Speculative Type Confusion Vulnerabilities in the Wild</summary>

- *Ofek Kirzner, Adam Morrison*

- `2106.15601v2` - [abs](http://arxiv.org/abs/2106.15601v2) - [pdf](http://arxiv.org/pdf/2106.15601v2)

> Spectre v1 attacks, which exploit conditional branch misprediction, are often identified with attacks that bypass array bounds checking to leak data from a victim's memory. Generally, however, Spectre v1 attacks can exploit any conditional branch misprediction that makes the victim execute code incorrectly. In this paper, we investigate speculative type confusion, a Spectre v1 attack vector in which branch mispredictions make the victim execute with variables holding values of the wrong type and thereby leak memory content.   We observe that speculative type confusion can be inadvertently introduced by a compiler, making it extremely hard for programmers to reason about security and manually apply Spectre mitigations. We thus set out to determine the extent to which speculative type confusion affects the Linux kernel. Our analysis finds exploitable and potentially-exploitable arbitrary memory disclosure vulnerabilities. We also find many latent vulnerabilities, which could become exploitable due to innocuous system changes, such as coding style changes.   Our results suggest that Spectre mitigations which rely on statically/manually identifying "bad" code patterns need to be rethought, and more comprehensive mitigations are needed.

</details>

<details>

<summary>2021-07-02 09:02:55 - PointGuard: Provably Robust 3D Point Cloud Classification</summary>

- *Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong*

- `2103.03046v2` - [abs](http://arxiv.org/abs/2103.03046v2) - [pdf](http://arxiv.org/pdf/2103.03046v2)

> 3D point cloud classification has many safety-critical applications such as autonomous driving and robotic grasping. However, several studies showed that it is vulnerable to adversarial attacks. In particular, an attacker can make a classifier predict an incorrect label for a 3D point cloud via carefully modifying, adding, and/or deleting a small number of its points. Randomized smoothing is state-of-the-art technique to build certifiably robust 2D image classifiers. However, when applied to 3D point cloud classification, randomized smoothing can only certify robustness against adversarially modified points.   In this work, we propose PointGuard, the first defense that has provable robustness guarantees against adversarially modified, added, and/or deleted points. Specifically, given a 3D point cloud and an arbitrary point cloud classifier, our PointGuard first creates multiple subsampled point clouds, each of which contains a random subset of the points in the original point cloud; then our PointGuard predicts the label of the original point cloud as the majority vote among the labels of the subsampled point clouds predicted by the point cloud classifier. Our first major theoretical contribution is that we show PointGuard provably predicts the same label for a 3D point cloud when the number of adversarially modified, added, and/or deleted points is bounded. Our second major theoretical contribution is that we prove the tightness of our derived bound when no assumptions on the point cloud classifier are made. Moreover, we design an efficient algorithm to compute our certified robustness guarantees. We also empirically evaluate PointGuard on ModelNet40 and ScanNet benchmark datasets.

</details>

<details>

<summary>2021-07-02 13:39:13 - Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation</summary>

- *Chen Chen, Kerstin Hammernik, Cheng Ouyang, Chen Qin, Wenjia Bai, Daniel Rueckert*

- `2107.01079v1` - [abs](http://arxiv.org/abs/2107.01079v1) - [pdf](http://arxiv.org/pdf/2107.01079v1)

> Deep learning-based segmentation methods are vulnerable to unforeseen data distribution shifts during deployment, e.g. change of image appearances or contrasts caused by different scanners, unexpected imaging artifacts etc. In this paper, we present a cooperative framework for training image segmentation models and a latent space augmentation method for generating hard examples. Both contributions improve model generalization and robustness with limited data. The cooperative training framework consists of a fast-thinking network (FTN) and a slow-thinking network (STN). The FTN learns decoupled image features and shape features for image reconstruction and segmentation tasks. The STN learns shape priors for segmentation correction and refinement. The two networks are trained in a cooperative manner. The latent space augmentation generates challenging examples for training by masking the decoupled latent space in both channel-wise and spatial-wise manners. We performed extensive experiments on public cardiac imaging datasets. Using only 10 subjects from a single site for training, we demonstrated improved cross-site segmentation performance and increased robustness against various unforeseen imaging artifacts compared to strong baseline methods. Particularly, cooperative training with latent space data augmentation yields 15% improvement in terms of average Dice score when compared to a standard training method.

</details>

<details>

<summary>2021-07-02 13:47:24 - Leveraging Team Dynamics to Predict Open-source Software Projects' Susceptibility to Social Engineering Attacks</summary>

- *Luiz Giovanini, Daniela Oliveira, Huascar Sanchez, Deborah Shands*

- `2106.16067v3` - [abs](http://arxiv.org/abs/2106.16067v3) - [pdf](http://arxiv.org/pdf/2106.16067v3)

> Open-source software (OSS) is a critical part of the software supply chain. Recent social engineering attacks against OSS development teams have enabled attackers to become code contributors and later inject malicious code or vulnerabilities into the project with the goal of compromising dependent software. The attackers have exploited interactions among development team members and the social dynamics of team behavior to enable their attacks. We introduce a security approach that leverages signatures and patterns of team dynamics to predict the susceptibility of a software development team to social engineering attacks that enable access to the OSS project code. The proposed approach is programming language-, platform-, and vulnerability-agnostic because it assesses the artifacts of OSS team interactions, rather than OSS code.

</details>

<details>

<summary>2021-07-02 14:14:21 - Model Checking C++ Programs</summary>

- *Felipe R. Monteiro, Mikhail R. Gadelha, Lucas C. Cordeiro*

- `2107.01093v1` - [abs](http://arxiv.org/abs/2107.01093v1) - [pdf](http://arxiv.org/pdf/2107.01093v1)

> In the last three decades, memory safety issues in system programming languages such as C or C++ have been one of the significant sources of security vulnerabilities. However, there exist only a few attempts with limited success to cope with the complexity of C++ program verification. Here we describe and evaluate a novel verification approach based on bounded model checking (BMC) and satisfiability modulo theories (SMT) to verify C++ programs formally. Our verification approach analyzes bounded C++ programs by encoding into SMT various sophisticated features that the C++ programming language offers, such as templates, inheritance, polymorphism, exception handling, and the Standard C++ Libraries. We formalize these features within our formal verification framework using a decidable fragment of first-order logic and then show how state-of-the-art SMT solvers can efficiently handle that. We implemented our verification approach on top of ESBMC. We compare ESBMC to LLBMC and DIVINE, which are state-of-the-art verifiers to check C++ programs directly from the LLVM bitcode. Experimental results show that ESBMC can handle a wide range of C++ programs, presenting a higher number of correct verification results. At the same time, it reduces the verification time if compared to LLBMC and DIVINE tools. Additionally, ESBMC has been applied to a commercial C++ application in the telecommunication domain and successfully detected arithmetic overflow errors, potentially leading to security vulnerabilities.

</details>

<details>

<summary>2021-07-02 22:37:17 - Unknown Presentation Attack Detection against Rational Attackers</summary>

- *Ali Khodabakhsh, Zahid Akhtar*

- `2010.01592v2` - [abs](http://arxiv.org/abs/2010.01592v2) - [pdf](http://arxiv.org/pdf/2010.01592v2)

> Despite the impressive progress in the field of presentation attack detection and multimedia forensics over the last decade, these systems are still vulnerable to attacks in real-life settings. Some of the challenges for existing solutions are the detection of unknown attacks, the ability to perform in adversarial settings, few-shot learning, and explainability. In this study, these limitations are approached by reliance on a game-theoretic view for modeling the interactions between the attacker and the detector. Consequently, a new optimization criterion is proposed and a set of requirements are defined for improving the performance of these systems in real-life settings. Furthermore, a novel detection technique is proposed using generator-based feature sets that are not biased towards any specific attack species. To further optimize the performance on known attacks, a new loss function coined categorical margin maximization loss (C-marmax) is proposed which gradually improves the performance against the most powerful attack. The proposed approach provides a more balanced performance across known and unknown attacks and achieves state-of-the-art performance in known and unknown attack detection cases against rational attackers. Lastly, the few-shot learning potential of the proposed approach is studied as well as its ability to provide pixel-level explainability.

</details>

<details>

<summary>2021-07-03 08:47:41 - Too Expensive to Attack: Enlarge the Attack Expense through Joint Defense at the Edge</summary>

- *Jianhua Li, Ximeng Liu, Jiong JIn, Shui Yu*

- `2107.01382v1` - [abs](http://arxiv.org/abs/2107.01382v1) - [pdf](http://arxiv.org/pdf/2107.01382v1)

> The distributed denial of service (DDoS) attack is detrimental to businesses and individuals as people are heavily relying on the Internet. Due to remarkable profits, crackers favor DDoS as cybersecurity weapons to attack a victim. Even worse, edge servers are more vulnerable. Current solutions lack adequate consideration to the expense of attackers and inter-defender collaborations. Hence, we revisit the DDoS attack and defense, clarifying the advantages and disadvantages of both parties. We further propose a joint defense framework to defeat attackers by incurring a significant increment of required bots and enlarging attack expenses. The quantitative evaluation and experimental assessment showcase that such expense can surge up to thousands of times. The skyrocket of expenses leads to heavy loss to the cracker, which prevents further attacks.

</details>

<details>

<summary>2021-07-03 10:02:51 - Multi-view Graph Learning by Joint Modeling of Consistency and Inconsistency</summary>

- *Youwei Liang, Dong Huang, Chang-Dong Wang, Philip S. Yu*

- `2008.10208v2` - [abs](http://arxiv.org/abs/2008.10208v2) - [pdf](http://arxiv.org/pdf/2008.10208v2)

> Graph learning has emerged as a promising technique for multi-view clustering with its ability to learn a unified and robust graph from multiple views. However, existing graph learning methods mostly focus on the multi-view consistency issue, yet often neglect the inconsistency across multiple views, which makes them vulnerable to possibly low-quality or noisy datasets. To overcome this limitation, we propose a new multi-view graph learning framework, which for the first time simultaneously and explicitly models multi-view consistency and multi-view inconsistency in a unified objective function, through which the consistent and inconsistent parts of each single-view graph as well as the unified graph that fuses the consistent parts can be iteratively learned. Though optimizing the objective function is NP-hard, we design a highly efficient optimization algorithm which is able to obtain an approximate solution with linear time complexity in the number of edges in the unified graph. Furthermore, our multi-view graph learning approach can be applied to both similarity graphs and dissimilarity graphs, which lead to two graph fusion-based variants in our framework. Experiments on twelve multi-view datasets have demonstrated the robustness and efficiency of the proposed approach.

</details>

<details>

<summary>2021-07-03 10:14:01 - Demiguise Attack: Crafting Invisible Semantic Adversarial Perturbations with Perceptual Similarity</summary>

- *Yajie Wang, Shangbo Wu, Wenyi Jiang, Shengang Hao, Yu-an Tan, Quanxin Zhang*

- `2107.01396v1` - [abs](http://arxiv.org/abs/2107.01396v1) - [pdf](http://arxiv.org/pdf/2107.01396v1)

> Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples. Adversarial examples are malicious images with visually imperceptible perturbations. While these carefully crafted perturbations restricted with tight $\Lp$ norm bounds are small, they are still easily perceivable by humans. These perturbations also have limited success rates when attacking black-box models or models with defenses like noise reduction filters. To solve these problems, we propose Demiguise Attack, crafting ``unrestricted'' perturbations with Perceptual Similarity. Specifically, we can create powerful and photorealistic adversarial examples by manipulating semantic information based on Perceptual Similarity. Adversarial examples we generate are friendly to the human visual system (HVS), although the perturbations are of large magnitudes. We extend widely-used attacks with our approach, enhancing adversarial effectiveness impressively while contributing to imperceptibility. Extensive experiments show that the proposed method not only outperforms various state-of-the-art attacks in terms of fooling rate, transferability, and robustness against defenses but can also improve attacks effectively. In addition, we also notice that our implementation can simulate illumination and contrast changes that occur in real-world scenarios, which will contribute to exposing the blind spots of DNNs.

</details>

<details>

<summary>2021-07-04 13:49:31 - Against Membership Inference Attack: Pruning is All You Need</summary>

- *Yijue Wang, Chenghong Wang, Zigeng Wang, Shanglin Zhou, Hang Liu, Jinbo Bi, Caiwen Ding, Sanguthevar Rajasekaran*

- `2008.13578v4` - [abs](http://arxiv.org/abs/2008.13578v4) - [pdf](http://arxiv.org/pdf/2008.13578v4)

> The large model size, high computational operations, and vulnerability against membership inference attack (MIA) have impeded deep learning or deep neural networks (DNNs) popularity, especially on mobile devices. To address the challenge, we envision that the weight pruning technique will help DNNs against MIA while reducing model storage and computational operation. In this work, we propose a pruning algorithm, and we show that the proposed algorithm can find a subnetwork that can prevent privacy leakage from MIA and achieves competitive accuracy with the original DNNs. We also verify our theoretical insights with experiments. Our experimental results illustrate that the attack accuracy using model compression is up to 13.6% and 10% lower than that of the baseline and Min-Max game, accordingly.

</details>

<details>

<summary>2021-07-04 16:54:46 - From Library Portability to Para-rehosting: Natively Executing Microcontroller Software on Commodity Hardware</summary>

- *Wenqiang Li, Le Guan, Jingqiang Lin, Jiameng Shi, Fengjun Li*

- `2107.12867v1` - [abs](http://arxiv.org/abs/2107.12867v1) - [pdf](http://arxiv.org/pdf/2107.12867v1)

> Finding bugs in microcontroller (MCU) firmware is challenging, even for device manufacturers who own the source code. The MCU runs different instruction sets than x86 and exposes a very different development environment. This invalidates many existing sophisticated software testing tools on x86. To maintain a unified developing and testing environment, a straightforward way is to re-compile the source code into the native executable for a commodity machine (called rehosting). However, ad-hoc re-hosting is a daunting and tedious task and subject to many issues (library-dependence, kernel-dependence and hardware-dependence). In this work, we systematically explore the portability problem of MCU software and propose pararehosting to ease the porting process. Specifically, we abstract and implement a portable MCU (PMCU) using the POSIX interface. It models common functions of the MCU cores. For peripheral specific logic, we propose HAL-based peripheral function replacement, in which high-level hardware functions are replaced with an equivalent backend driver on the host. These backend drivers are invoked by well-designed para-APIs and can be reused across many MCU OSs. We categorize common HAL functions into four types and implement templates for quick backend development. Using the proposed approach, we have successfully rehosted nine MCU OSs including the widely deployed Amazon FreeRTOS, ARM Mbed OS, Zephyr and LiteOS. To demonstrate the superiority of our approach in terms of security testing, we used off-the-shelf dynamic analysis tools (AFL and ASAN) against the rehosted programs and discovered 28 previously-unknown bugs, among which 5 were confirmed by CVE and the other 19 were confirmed by vendors at the time of writing.

</details>

<details>

<summary>2021-07-05 05:08:30 - Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models</summary>

- *Mingyue Han, Yinglin Wang*

- `2107.01791v1` - [abs](http://arxiv.org/abs/2107.01791v1) - [pdf](http://arxiv.org/pdf/2107.01791v1)

> Pretrained language models (PLM) achieve surprising performance on the Choice of Plausible Alternatives (COPA) task. However, whether PLMs have truly acquired the ability of causal reasoning remains a question. In this paper, we investigate the problem of semantic similarity bias and reveal the vulnerability of current COPA models by certain attacks. Previous solutions that tackle the superficial cues of unbalanced token distribution still encounter the same problem of semantic bias, even more seriously due to the utilization of more training data. We mitigate this problem by simply adding a regularization loss and experimental results show that this solution not only improves the model's generalization ability, but also assists the models to perform more robustly on a challenging dataset, BCOPA-CE, which has unbiased token distribution and is more difficult for models to distinguish cause and effect.

</details>

<details>

<summary>2021-07-05 06:27:49 - Comparative Analysis of Impact of Cryptography Algorithms on Wireless Sensor Networks</summary>

- *Bilwasiva Basu Mallick, Ashutosh Bhatia*

- `2107.01810v1` - [abs](http://arxiv.org/abs/2107.01810v1) - [pdf](http://arxiv.org/pdf/2107.01810v1)

> Cryptography techniques are essential for a robust and stable security design of a system to mitigate the risk of external attacks and thus improve its efficiency. Wireless Sensor Networks (WSNs) play a pivotal role in sensing, monitoring, processing, and accumulating raw data to enhance the performance of the actuators, micro-controllers, embedded architectures, IoT devices, and computing machines to which they are connected. With so much threat of potential adversaries, it is essential to scale up the security level of WSN without affecting its primary goal of seamless data collection and communication with relay devices. This paper intends to explore the past and ongoing research activities in this domain. An extensive study of these algorithms referred here, are studied and analyzed. Based on these findings this paper will illustrate the best possible cryptography algorithms which will be most suited to implement the security aspects of the WSN and protect it from any threat and reduce its vulnerabilities. This study will pave the way for future research on this topic since it will provide a comprehensive and holistic view of the subject.

</details>

<details>

<summary>2021-07-05 09:38:44 - Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data</summary>

- *Pooya Rostami Mazrae, Maliheh Izadi, Abbas Heydarnoori*

- `2107.01894v1` - [abs](http://arxiv.org/abs/2107.01894v1) - [pdf](http://arxiv.org/pdf/2107.01894v1)

> An issue documents discussions around required changes in issue-tracking systems, while a commit contains the change itself in the version control systems. Recovering links between issues and commits can facilitate many software evolution tasks such as bug localization, and software documentation. A previous study on over half a million issues from GitHub reports only about 42.2% of issues are manually linked by developers to their pertinent commits. Automating the linking of commit-issue pairs can contribute to the improvement of the said tasks. By far, current state-of-the-art approaches for automated commit-issue linking suffer from low precision, leading to unreliable results, sometimes to the point that imposes human supervision on the predicted links. The low performance gets even more severe when there is a lack of textual information in either commits or issues. Current approaches are also proven computationally expensive.   We propose Hybrid-Linker to overcome such limitations by exploiting two information channels; (1) a non-textual-based component that operates on non-textual, automatically recorded information of the commit-issue pairs to predict a link, and (2) a textual-based one which does the same using textual information of the commit-issue pairs. Then, combining the results from the two classifiers, Hybrid-Linker makes the final prediction. Thus, every time one component falls short in predicting a link, the other component fills the gap and improves the results. We evaluate Hybrid-Linker against competing approaches, namely FRLink and DeepLink on a dataset of 12 projects. Hybrid-Linker achieves 90.1%, 87.8%, and 88.9% based on recall, precision, and F-measure, respectively. It also outperforms FRLink and DeepLink by 31.3%, and 41.3%, regarding the F-measure. Moreover, Hybrid-Linker exhibits extensive improvements in terms of performance as well.

</details>

<details>

<summary>2021-07-05 11:07:35 - Adversarial Robustness of Probabilistic Network Embedding for Link Prediction</summary>

- *Xi Chen, Bo Kang, Jefrey Lijffijt, Tijl De Bie*

- `2107.01936v1` - [abs](http://arxiv.org/abs/2107.01936v1) - [pdf](http://arxiv.org/pdf/2107.01936v1)

> In today's networked society, many real-world problems can be formalized as predicting links in networks, such as Facebook friendship suggestions, e-commerce recommendations, and the prediction of scientific collaborations in citation networks. Increasingly often, link prediction problem is tackled by means of network embedding methods, owing to their state-of-the-art performance. However, these methods lack transparency when compared to simpler baselines, and as a result their robustness against adversarial attacks is a possible point of concern: could one or a few small adversarial modifications to the network have a large impact on the link prediction performance when using a network embedding model? Prior research has already investigated adversarial robustness for network embedding models, focused on classification at the node and graph level. Robustness with respect to the link prediction downstream task, on the other hand, has been explored much less.   This paper contributes to filling this gap, by studying adversarial robustness of Conditional Network Embedding (CNE), a state-of-the-art probabilistic network embedding model, for link prediction. More specifically, given CNE and a network, we measure the sensitivity of the link predictions of the model to small adversarial perturbations of the network, namely changes of the link status of a node pair. Thus, our approach allows one to identify the links and non-links in the network that are most vulnerable to such perturbations, for further investigation by an analyst. We analyze the characteristics of the most and least sensitive perturbations, and empirically confirm that our approach not only succeeds in identifying the most vulnerable links and non-links, but also that it does so in a time-efficient manner thanks to an effective approximation.

</details>

<details>

<summary>2021-07-06 00:07:09 - Pedestrian Emergence Estimation and Occlusion-Aware Risk Assessment for Urban Autonomous Driving</summary>

- *Mert Koc, Ekim Yurtsever, Keith Redmill, Umit Ozguner*

- `2107.02326v1` - [abs](http://arxiv.org/abs/2107.02326v1) - [pdf](http://arxiv.org/pdf/2107.02326v1)

> Avoiding unseen or partially occluded vulnerable road users (VRUs) is a major challenge for fully autonomous driving in urban scenes. However, occlusion-aware risk assessment systems have not been widely studied. Here, we propose a pedestrian emergence estimation and occlusion-aware risk assessment system for urban autonomous driving. First, the proposed system utilizes available contextual information, such as visible cars and pedestrians, to estimate pedestrian emergence probabilities in occluded regions. These probabilities are then used in a risk assessment framework, and incorporated into a longitudinal motion controller. The proposed controller is tested against several baseline controllers that recapitulate some commonly observed driving styles. The simulated test scenarios include randomly placed parked cars and pedestrians, most of whom are occluded from the ego vehicle's view and emerges randomly. The proposed controller outperformed the baselines in terms of safety and comfort measures.

</details>

<details>

<summary>2021-07-06 06:57:40 - GradDiv: Adversarial Robustness of Randomized Neural Networks via Gradient Diversity Regularization</summary>

- *Sungyoon Lee, Hoki Kim, Jaewook Lee*

- `2107.02425v1` - [abs](http://arxiv.org/abs/2107.02425v1) - [pdf](http://arxiv.org/pdf/2107.02425v1)

> Deep learning is vulnerable to adversarial examples. Many defenses based on randomized neural networks have been proposed to solve the problem, but fail to achieve robustness against attacks using proxy gradients such as the Expectation over Transformation (EOT) attack. We investigate the effect of the adversarial attacks using proxy gradients on randomized neural networks and demonstrate that it highly relies on the directional distribution of the loss gradients of the randomized neural network. We show in particular that proxy gradients are less effective when the gradients are more scattered. To this end, we propose Gradient Diversity (GradDiv) regularizations that minimize the concentration of the gradients to build a robust randomized neural network. Our experiments on MNIST, CIFAR10, and STL10 show that our proposed GradDiv regularizations improve the adversarial robustness of randomized neural networks against a variety of state-of-the-art attack methods. Moreover, our method efficiently reduces the transferability among sample models of randomized neural networks.

</details>

<details>

<summary>2021-07-06 07:25:14 - ETHOS: an Online Hate Speech Detection Dataset</summary>

- *Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos, Grigorios Tsoumakas*

- `2006.08328v2` - [abs](http://arxiv.org/abs/2006.08328v2) - [pdf](http://arxiv.org/pdf/2006.08328v2)

> Online hate speech is a recent problem in our society that is rising at a steady pace by leveraging the vulnerabilities of the corresponding regimes that characterise most social media platforms. This phenomenon is primarily fostered by offensive comments, either during user interaction or in the form of a posted multimedia context. Nowadays, giant corporations own platforms where millions of users log in every day, and protection from exposure to similar phenomena appears to be necessary in order to comply with the corresponding legislation and maintain a high level of service quality. A robust and reliable system for detecting and preventing the uploading of relevant content will have a significant impact on our digitally interconnected society. Several aspects of our daily lives are undeniably linked to our social profiles, making us vulnerable to abusive behaviours. As a result, the lack of accurate hate speech detection mechanisms would severely degrade the overall user experience, although its erroneous operation would pose many ethical concerns. In this paper, we present 'ETHOS', a textual dataset with two variants: binary and multi-label, based on YouTube and Reddit comments validated using the Figure-Eight crowdsourcing platform. Furthermore, we present the annotation protocol used to create this dataset: an active sampling procedure for balancing our data in relation to the various aspects defined. Our key assumption is that, even gaining a small amount of labelled data from such a time-consuming process, we can guarantee hate speech occurrences in the examined material.

</details>

<details>

<summary>2021-07-06 12:32:48 - Effectiveness of MPC-friendly Softmax Replacement</summary>

- *Marcel Keller, Ke Sun*

- `2011.11202v2` - [abs](http://arxiv.org/abs/2011.11202v2) - [pdf](http://arxiv.org/pdf/2011.11202v2)

> Softmax is widely used in deep learning to map some representation to a probability distribution. As it is based on exp/log functions that are relatively expensive in multi-party computation, Mohassel and Zhang (2017) proposed a simpler replacement based on ReLU to be used in secure computation. However, we could not reproduce the accuracy they reported for training on MNIST with three fully connected layers. Later works (e.g., Wagh et al., 2019 and 2021) used the softmax replacement not for computing the output probability distribution but for approximating the gradient in back-propagation. In this work, we analyze the two uses of the replacement and compare them to softmax, both in terms of accuracy and cost in multi-party computation. We found that the replacement only provides a significant speed-up for a one-layer network while it always reduces accuracy, sometimes significantly. Thus we conclude that its usefulness is limited and one should use the original softmax function instead.

</details>

<details>

<summary>2021-07-07 03:33:50 - TokenHook: Secure ERC-20 smart contract</summary>

- *Reza Rahimian, Jeremy Clark*

- `2107.02997v1` - [abs](http://arxiv.org/abs/2107.02997v1) - [pdf](http://arxiv.org/pdf/2107.02997v1)

> ERC-20 is the most prominent Ethereum standard for fungible tokens. Tokens implementing the ERC-20 interface can interoperate with a large number of already deployed internet-based services and Ethereum-based smart contracts. In recent years, security vulnerabilities in ERC-20 have received special attention due to their widespread use and increased value. We systemize these vulnerabilities and their applicability to ERC-20 tokens, which has not been done before. Next, we use our domain expertise to provide a new implementation of the ERC-20 interface that is freely available in Vyper and Solidity, and has enhanced security properties and stronger compliance with best practices compared to the sole surviving reference implementation (from OpenZeppelin) in the ERC-20 specification. Finally, we use our implementation to study the effectiveness of seven static analysis tools, designed for general smart contracts, for identifying ERC-20 specific vulnerabilities. We find large inconsistencies across the tools and a high number of false positives which shows there is room for further improvement of these tools.

</details>

<details>

<summary>2021-07-07 06:59:07 - Refined Grey-Box Fuzzing with SIVO</summary>

- *Ivica Nikolic, Radu Mantu, Shiqi Shen, Prateek Saxena*

- `2102.02394v2` - [abs](http://arxiv.org/abs/2102.02394v2) - [pdf](http://arxiv.org/pdf/2102.02394v2)

> We design and implement from scratch a new fuzzer called SIVO that refines multiple stages of grey-box fuzzing. First, SIVO refines data-flow fuzzing in two ways: (a) it provides a new taint inference engine that requires only logarithmic in the input size number of tests to infer the dependency of all program branches on the input bytes, and (b) it deploys a novel method for inverting branches by solving directly and efficiently systems of inequalities. Second, our fuzzer refines accurate tracking and detection of code coverage with simple and easily implementable methods. Finally, SIVO refines selection of parameters and strategies by parameterizing all stages of fuzzing and then dynamically selecting optimal values during fuzzing. Thus the fuzzer can easily adapt to a target program and rapidly increase coverage. We compare our fuzzer to 11 other state-of-the-art grey-box fuzzers on 27 popular benchmarks. Our evaluation shows that SIVO scores the highest both in terms of code coverage and in terms of number of found vulnerabilities.

</details>

<details>

<summary>2021-07-07 07:22:41 - Controlled Caption Generation for Images Through Adversarial Attacks</summary>

- *Nayyer Aafaq, Naveed Akhtar, Wei Liu, Mubarak Shah, Ajmal Mian*

- `2107.03050v1` - [abs](http://arxiv.org/abs/2107.03050v1) - [pdf](http://arxiv.org/pdf/2107.03050v1)

> Deep learning is found to be vulnerable to adversarial examples. However, its adversarial susceptibility in image caption generation is under-explored. We study adversarial examples for vision and language models, which typically adopt an encoder-decoder framework consisting of two major components: a Convolutional Neural Network (i.e., CNN) for image feature extraction and a Recurrent Neural Network (RNN) for caption generation. In particular, we investigate attacks on the visual encoder's hidden layer that is fed to the subsequent recurrent network. The existing methods either attack the classification layer of the visual encoder or they back-propagate the gradients from the language model. In contrast, we propose a GAN-based algorithm for crafting adversarial examples for neural image captioning that mimics the internal representation of the CNN such that the resulting deep features of the input image enable a controlled incorrect caption generation through the recurrent network. Our contribution provides new insights for understanding adversarial attacks on vision systems with language component. The proposed method employs two strategies for a comprehensive evaluation. The first examines if a neural image captioning system can be misled to output targeted image captions. The second analyzes the possibility of keywords into the predicted captions. Experiments show that our algorithm can craft effective adversarial images based on the CNN hidden layers to fool captioning framework. Moreover, we discover the proposed attack to be highly transferable. Our work leads to new robustness implications for neural image captioning.

</details>

<details>

<summary>2021-07-07 13:57:06 - A Modified Drake Equation for Assessing Adversarial Risk to Machine Learning Models</summary>

- *Josh Kalin, David Noever, Matthew Ciolino*

- `2103.02718v2` - [abs](http://arxiv.org/abs/2103.02718v2) - [pdf](http://arxiv.org/pdf/2103.02718v2)

> Machine learning models present a risk of adversarial attack when deployed in production. Quantifying the contributing factors and uncertainties using empirical measures could assist the industry with assessing the risk of downloading and deploying common model types. This work proposes modifying the traditional Drake Equation's formalism to estimate the number of potentially successful adversarial attacks on a deployed model. The Drake Equation is famously used for parameterizing uncertainties and it has been used in many research fields outside of its original intentions to estimate the number of radio-capable extra-terrestrial civilizations. While previous work has outlined methods for discovering vulnerabilities in public model architectures, the proposed equation seeks to provide a semi-quantitative benchmark for evaluating and estimating the potential risk factors for adversarial attacks.

</details>

<details>

<summary>2021-07-08 02:46:14 - QFuzz: Quantitative Fuzzing for Side Channels</summary>

- *Yannic Noller, Saeid Tizpaz-Niari*

- `2106.03346v3` - [abs](http://arxiv.org/abs/2106.03346v3) - [pdf](http://arxiv.org/pdf/2106.03346v3)

> Side channels pose a significant threat to the confidentiality of software systems. Such vulnerabilities are challenging to detect and evaluate because they arise from non-functional properties of software such as execution times and require reasoning on multiple execution traces. Recently, noninterference notions have been adapted in static analysis, symbolic execution, and greybox fuzzing techniques. However, noninterference is a strict notion and may reject security even if the strength of information leaks are weak. A quantitative notion of security allows for the relaxation of noninterference and tolerates small (unavoidable) leaks. Despite progress in recent years, the existing quantitative approaches have scalability limitations in practice. In this work, we present QFuzz, a greybox fuzzing technique to quantitatively evaluate the strength of side channels with a focus on min entropy. Min entropy is a measure based on the number of distinguishable observations (partitions) to assess the resulting threat from an attacker who tries to compromise secrets in one try. We develop a novel greybox fuzzing equipped with two partitioning algorithms that try to maximize the number of distinguishable observations and the cost differences between them. We evaluate QFuzz on a large set of benchmarks from existing work and real-world libraries (with a total of 70 subjects). QFuzz compares favorably to three state-of-the-art detection techniques. QFuzz provides quantitative information about leaks beyond the capabilities of all three techniques. Crucially, we compare QFuzz to a state-of-the-art quantification tool and find that QFuzz significantly outperforms the tool in scalability while maintaining similar precision. Overall, we find that our approach scales well for real-world applications and provides useful information to evaluate resulting threats. Additionally, QFuzz identifies a zero-d...

</details>

<details>

<summary>2021-07-08 03:19:40 - Adaptive Stress Testing for Adversarial Learning in a Financial Environment</summary>

- *Khalid El-Awady*

- `2107.03577v1` - [abs](http://arxiv.org/abs/2107.03577v1) - [pdf](http://arxiv.org/pdf/2107.03577v1)

> We demonstrate the use of Adaptive Stress Testing to detect and address potential vulnerabilities in a financial environment. We develop a simplified model for credit card fraud detection that utilizes a linear regression classifier based on historical payment transaction data coupled with business rules. We then apply the reinforcement learning model known as Adaptive Stress Testing to train an agent, that can be thought of as a potential fraudster, to find the most likely path to system failure -- successfully defrauding the system. We show the connection between this most likely failure path and the limits of the classifier and discuss how the fraud detection system's business rules can be further augmented to mitigate these failure modes.

</details>

<details>

<summary>2021-07-08 06:44:04 - Mitigating Memorization in Sample Selection for Learning with Noisy Labels</summary>

- *Kyeongbo Kong, Junggi Lee, Youngchul Kwak, Young-Rae Cho, Seong-Eun Kim, Woo-Jin Song*

- `2107.07041v1` - [abs](http://arxiv.org/abs/2107.07041v1) - [pdf](http://arxiv.org/pdf/2107.07041v1)

> Because deep learning is vulnerable to noisy labels, sample selection techniques, which train networks with only clean labeled data, have attracted a great attention. However, if the labels are dominantly corrupted by few classes, these noisy samples are called dominant-noisy-labeled samples, the network also learns dominant-noisy-labeled samples rapidly via content-aware optimization. In this study, we propose a compelling criteria to penalize dominant-noisy-labeled samples intensively through class-wise penalty labels. By averaging prediction confidences for the each observed label, we obtain suitable penalty labels that have high values if the labels are largely corrupted by some classes. Experiments were performed using benchmarks (CIFAR-10, CIFAR-100, Tiny-ImageNet) and real-world datasets (ANIMAL-10N, Clothing1M) to evaluate the proposed criteria in various scenarios with different noise rates. Using the proposed sample selection, the learning process of the network becomes significantly robust to noisy labels compared to existing methods in several noise types.

</details>

<details>

<summary>2021-07-08 07:47:35 - Duplicate-sensitivity Guided Transformation Synthesis for DBMS Correctness Bug Detection</summary>

- *Yushan Zhang, Peisen Yao, Rongxin Wu, Charles Zhang*

- `2107.03660v1` - [abs](http://arxiv.org/abs/2107.03660v1) - [pdf](http://arxiv.org/pdf/2107.03660v1)

> Database Management System (DBMS) plays a core role in modern software from mobile apps to online banking. It is critical that DBMS should provide correct data to all applications. When the DBMS returns incorrect data, a correctness bug is triggered. Current production-level DBMSs still suffer from insufficient testing due to the limited hand-written test cases. Recently several works proposed to automatically generate many test cases with query transformation, a process of generating an equivalent query pair and testing a DBMS by checking whether the system returns the same result set for both queries. However, all of them still heavily rely on manual work to provide a transformation which largely confines their exploration of the valid input query space.   This paper introduces duplicate-sensitivity guided transformation synthesis which automatically finds new transformations by first synthesizing many candidates then filtering the nonequivalent ones. Our automated synthesis is achieved by mutating a query while keeping its duplicate sensitivity, which is a necessary condition for query equivalence. After candidate synthesis, we keep the mutant query which is equivalent to the given one by using a query equivalent checker. Furthermore, we have implemented our idea in a tool Eqsql and used it to test the production-level DBMSs. In two months, we detected in total 30 newly confirmed and unique bugs in MySQL, TiDB and CynosDB.

</details>

<details>

<summary>2021-07-09 01:06:13 - Towards Robust Active Feature Acquisition</summary>

- *Yang Li, Siyuan Shan, Qin Liu, Junier B. Oliva*

- `2107.04163v1` - [abs](http://arxiv.org/abs/2107.04163v1) - [pdf](http://arxiv.org/pdf/2107.04163v1)

> Truly intelligent systems are expected to make critical decisions with incomplete and uncertain data. Active feature acquisition (AFA), where features are sequentially acquired to improve the prediction, is a step towards this goal. However, current AFA models all deal with a small set of candidate features and have difficulty scaling to a large feature space. Moreover, they are ignorant about the valid domains where they can predict confidently, thus they can be vulnerable to out-of-distribution (OOD) inputs. In order to remedy these deficiencies and bring AFA models closer to practical use, we propose several techniques to advance the current AFA approaches. Our framework can easily handle a large number of features using a hierarchical acquisition policy and is more robust to OOD inputs with the help of an OOD detector for partially observed data. Extensive experiments demonstrate the efficacy of our framework over strong baselines.

</details>

<details>

<summary>2021-07-09 06:15:58 - Sirius: Static Program Repair with Dependence Graph-Based Systematic Edit Patterns</summary>

- *Kunihiro Noda, Haruki Yokoyama, Shinji Kikuchi*

- `2107.04234v1` - [abs](http://arxiv.org/abs/2107.04234v1) - [pdf](http://arxiv.org/pdf/2107.04234v1)

> Software development often involves systematic edits, similar but nonidentical changes to many code locations, that are error-prone and laborious for developers. Mining and learning such systematic edit patterns (SEPs) from past code changes enable us to detect and repair overlooked buggy code that requires systematic edits.   A recent study presented a promising SEP mining technique that is based on program dependence graphs (PDGs), while traditional approaches leverage syntax-based representations. PDG-based SEPs are highly expressive and can capture more meaningful changes than syntax-based ones. The next challenge to tackle is to apply the same code changes as in PDG-based SEPs to other code locations; detection and repair of overlooked locations that require systematic edits. Existing program transformation techniques cannot well address this challenge because (1) they expect many structural code similarities that are not guaranteed in PDG-based SEPs or (2) they work on the basis of PDGs but are limited to specific domains (e.g., API migrations).   We present in this paper a general-purpose program transformation algorithm for applying PDG-based SEPs. Our algorithm identifies a small transplantable structural subtree for each PDG node, thereby adapting code changes from PDG-based SEPs to other locations. We construct a program repair pipeline Sirius that incorporates the algorithm and automates the processes of mining SEPs, detecting overlooked code locations (bugs) that require systematic edits, and repairing them by applying SEPs.   We evaluated the repair performance of Sirius with a corpus of open source software consisting of over 80 repositories. Sirius achieved a precision of 0.710, recall of 0.565, and F1-score of 0.630, while those of the state-of-the-art technique were 0.470, 0.141, and 0.216, respectively.

</details>

<details>

<summary>2021-07-09 12:12:36 - GGT: Graph-Guided Testing for Adversarial Sample Detection of Deep Neural Network</summary>

- *Zuohui Chen, Renxuan Wang, Jingyang Xiang, Yue Yu, Xin Xia, Shouling Ji, Qi Xuan, Xiaoniu Yang*

- `2107.07043v1` - [abs](http://arxiv.org/abs/2107.07043v1) - [pdf](http://arxiv.org/pdf/2107.07043v1)

> Deep Neural Networks (DNN) are known to be vulnerable to adversarial samples, the detection of which is crucial for the wide application of these DNN models. Recently, a number of deep testing methods in software engineering were proposed to find the vulnerability of DNN systems, and one of them, i.e., Model Mutation Testing (MMT), was used to successfully detect various adversarial samples generated by different kinds of adversarial attacks. However, the mutated models in MMT are always huge in number (e.g., over 100 models) and lack diversity (e.g., can be easily circumvented by high-confidence adversarial samples), which makes it less efficient in real applications and less effective in detecting high-confidence adversarial samples. In this study, we propose Graph-Guided Testing (GGT) for adversarial sample detection to overcome these aforementioned challenges. GGT generates pruned models with the guide of graph characteristics, each of them has only about 5% parameters of the mutated model in MMT, and graph guided models have higher diversity. The experiments on CIFAR10 and SVHN validate that GGT performs much better than MMT with respect to both effectiveness and efficiency.

</details>

<details>

<summary>2021-07-09 16:13:41 - Towards Quantum-Secure Authentication and Key Agreement via Abstract Multi-Agent Interaction</summary>

- *Ibrahim H. Ahmed, Josiah P. Hanna, Elliot Fosong, Stefano V. Albrecht*

- `2007.09327v2` - [abs](http://arxiv.org/abs/2007.09327v2) - [pdf](http://arxiv.org/pdf/2007.09327v2)

> Current methods for authentication and key agreement based on public-key cryptography are vulnerable to quantum computing. We propose a novel approach based on artificial intelligence research in which communicating parties are viewed as autonomous agents which interact repeatedly using their private decision models. Authentication and key agreement are decided based on the agents' observed behaviors during the interaction. The security of this approach rests upon the difficulty of modeling the decisions of interacting agents from limited observations, a problem which we conjecture is also hard for quantum computing. We release PyAMI, a prototype authentication and key agreement system based on the proposed method. We empirically validate our method for authenticating legitimate users while detecting different types of adversarial attacks. Finally, we show how reinforcement learning techniques can be used to train server models which effectively probe a client's decisions to achieve more sample-efficient authentication.

</details>

<details>

<summary>2021-07-10 17:04:22 - Intermittent Jamming against Telemetry and Telecommand of Satellite Systems and A Learning-driven Detection Strategy</summary>

- *Selen Gecgel, Gunes Karabulut Kurt*

- `2107.06181v1` - [abs](http://arxiv.org/abs/2107.06181v1) - [pdf](http://arxiv.org/pdf/2107.06181v1)

> Towards sixth-generation networks (6G), satellite communication systems, especially based on Low Earth Orbit (LEO) networks, become promising due to their unique and comprehensive capabilities. These advantages are accompanied by a variety of challenges such as security vulnerabilities, management of hybrid systems, and high mobility. In this paper, firstly, a security deficiency in the physical layer is addressed with a conceptual framework, considering the cyber-physical nature of the satellite systems, highlighting the potential attacks. Secondly, a learning-driven detection scheme is proposed, and the lightweight convolutional neural network (CNN) is designed. The performance of the designed CNN architecture is compared with a prevalent machine learning algorithm, support vector machine (SVM). The results show that deficiency attacks against the satellite systems can be detected by employing the proposed scheme.

</details>

<details>

<summary>2021-07-10 20:54:29 - Cyber-Security Challenges in Aviation Industry: A Review of Current and Future Trends</summary>

- *Elochukwu Ukwandu, Mohamed Amine Ben Farah, Hanan Hindy, Miroslav Bures, Robert Atkinson, Christos Tachtatzis, Xavier Bellekens*

- `2107.04910v1` - [abs](http://arxiv.org/abs/2107.04910v1) - [pdf](http://arxiv.org/pdf/2107.04910v1)

> The integration of Information and Communication Technology (ICT) tools into mechanical devices found in aviation industry has raised security concerns. The more integrated the system, the more vulnerable due to the inherent vulnerabilities found in ICT tools and software that drives the system. The security concerns have become more heightened as the concept of electronic-enabled aircraft and smart airports get refined and implemented underway. In line with the above, this paper undertakes a review of cyber-security incidence in the aviation sector over the last 20 years. The essence is to understand the common threat actors, their motivations, the type of attacks, aviation infrastructure that is commonly attacked and then match these so as to provide insight on the current state of the cyber-security in the aviation sector. The review showed that the industry's threats come mainly from Advance Persistent Threat (APT) groups that work in collaboration with some state actors to steal intellectual property and intelligence, in order to advance their domestic aerospace capabilities as well as possibly monitor, infiltrate and subvert other nations' capabilities. The segment of the aviation industry commonly attacked is the Information Technology infrastructure, and the prominent type of attacks is malicious hacking activities that aim at gaining unauthorised access using known malicious password cracking techniques such as Brute force attacks, Dictionary attacks and so on. The review further analysed the different attack surfaces that exist in aviation industry, threat dynamics, and use these dynamics to predict future trends of cyberattacks in the industry. The aim is to provide information for the cybersecurity professionals and aviation stakeholders for proactive actions in protecting these critical infrastructures against cyberincidence for an optimal customer service oriented industry.

</details>

<details>

<summary>2021-07-11 02:09:52 - You Really Shouldn't Roll Your Own Crypto: An Empirical Study of Vulnerabilities in Cryptographic Libraries</summary>

- *Jenny Blessing, Michael A. Specter, Daniel J. Weitzner*

- `2107.04940v1` - [abs](http://arxiv.org/abs/2107.04940v1) - [pdf](http://arxiv.org/pdf/2107.04940v1)

> The security of the Internet rests on a small number of open-source cryptographic libraries: a vulnerability in any one of them threatens to compromise a significant percentage of web traffic. Despite this potential for security impact, the characteristics and causes of vulnerabilities in cryptographic software are not well understood. In this work, we conduct the first comprehensive analysis of cryptographic libraries and the vulnerabilities affecting them. We collect data from the National Vulnerability Database, individual project repositories and mailing lists, and other relevant sources for eight widely used cryptographic libraries.   Among our most interesting findings is that only 27.2% of vulnerabilities in cryptographic libraries are cryptographic issues while 37.2% of vulnerabilities are memory safety issues, indicating that systems-level bugs are a greater security concern than the actual cryptographic procedures. In our investigation of the causes of these vulnerabilities, we find evidence of a strong correlation between the complexity of these libraries and their (in)security, empirically demonstrating the potential risks of bloated cryptographic codebases. We further compare our findings with non-cryptographic systems, observing that these systems are, indeed, more complex than similar counterparts, and that this excess complexity appears to produce significantly more vulnerabilities in cryptographic libraries than in non-cryptographic software.

</details>

<details>

<summary>2021-07-11 02:13:36 - Selfish Mining in Ethereum</summary>

- *Jianyu Niu, Chen Feng*

- `1901.04620v4` - [abs](http://arxiv.org/abs/1901.04620v4) - [pdf](http://arxiv.org/pdf/1901.04620v4)

> As the second largest cryptocurrency by market capitalization and today's biggest decentralized platform that runs smart contracts, Ethereum has received much attention from both industry and academia. Nevertheless, there exist very few studies about the security of its mining strategies, especially from the selfish mining perspective. In this paper, we aim to fill this research gap by analyzing selfish mining in Ethereum and understanding its potential threat. First, we introduce a 2-dimensional Markov process to model the behavior of a selfish mining strategy inspired by a Bitcoin mining strategy proposed by Eyal and Sirer. Second, we derive the stationary distribution of our Markov model and compute long-term average mining rewards. This allows us to determine the threshold of computational power that makes selfish mining profitable in Ethereum. We find that this threshold is lower than that in Bitcoin mining (which is 25% as discovered by Eyal and Sirer), suggesting that Ethereum is more vulnerable to selfish mining than Bitcoin.

</details>

<details>

<summary>2021-07-12 03:06:49 - Deep Transfer Learning Based Intrusion Detection System for Electric Vehicular Networks</summary>

- *Sk. Tanzir Mehedi, Adnan Anwar, Ziaur Rahman, Kawsar Ahmed*

- `2107.05172v1` - [abs](http://arxiv.org/abs/2107.05172v1) - [pdf](http://arxiv.org/pdf/2107.05172v1)

> The Controller Area Network (CAN) bus works as an important protocol in the real-time In-Vehicle Network (IVN) systems for its simple, suitable, and robust architecture. The risk of IVN devices has still been insecure and vulnerable due to the complex data-intensive architectures which greatly increase the accessibility to unauthorized networks and the possibility of various types of cyberattacks. Therefore, the detection of cyberattacks in IVN devices has become a growing interest. With the rapid development of IVNs and evolving threat types, the traditional machine learning-based IDS has to update to cope with the security requirements of the current environment. Nowadays, the progression of deep learning, deep transfer learning, and its impactful outcome in several areas has guided as an effective solution for network intrusion detection. This manuscript proposes a deep transfer learning-based IDS model for IVN along with improved performance in comparison to several other existing models. The unique contributions include effective attribute selection which is best suited to identify malicious CAN messages and accurately detect the normal and abnormal activities, designing a deep transfer learning-based LeNet model, and evaluating considering real-world data. To this end, an extensive experimental performance evaluation has been conducted. The architecture along with empirical analyses shows that the proposed IDS greatly improves the detection accuracy over the mainstream machine learning, deep learning, and benchmark deep transfer learning models and has demonstrated better performance for real-time IVN security.

</details>

<details>

<summary>2021-07-12 06:37:10 - Towards Secure Wireless Mesh Networks for UAV Swarm Connectivity: Current Threats, Research, and Opportunities</summary>

- *Martin Andreoni Lopez, Michael Baddeley, Willian T. Lunardi, Anshul Pandey, Jean-Pierre Giacalone*

- `2108.13154v1` - [abs](http://arxiv.org/abs/2108.13154v1) - [pdf](http://arxiv.org/pdf/2108.13154v1)

> UAVs are increasingly appearing in swarms or formations to leverage cooperative behavior, forming flying ad hoc networks. These UAV-enabled networks can meet several complex mission requirements and are seen as a potential enabler for many of the emerging use-cases in future communication networks. Such networks, however, are characterized by a highly dynamic and mobile environment with no guarantee of a central network infrastructure which can cause both connectivity and security issues. While wireless mesh networks are envisioned as a solution for such scenarios, these networks come with their own challenges and security vulnerabilities. In this paper, we analyze the key security and resilience issues resulting from the application of wireless mesh networks within UAV swarms. Specifically, we highlight the main challenges of applying current mesh technologies within the domain of UAV swarms and expose existing vulnerabilities across the communication stack. Based on this analysis, we present a security-focused architecture for UAV mesh communications. Finally, from the identification of these vulnerabilities, we discuss research opportunities posed by the unique challenges of UAV swarm connectivity.

</details>

<details>

<summary>2021-07-12 08:07:09 - Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning</summary>

- *Jun Wang, Chang Xu, Francisco Guzman, Ahmed El-Kishky, Yuqing Tang, Benjamin I. P. Rubinstein, Trevor Cohn*

- `2107.05243v1` - [abs](http://arxiv.org/abs/2107.05243v1) - [pdf](http://arxiv.org/pdf/2107.05243v1)

> Neural machine translation systems are known to be vulnerable to adversarial test inputs, however, as we show in this paper, these systems are also vulnerable to training attacks. Specifically, we propose a poisoning attack in which a malicious adversary inserts a small poisoned sample of monolingual text into the training set of a system trained using back-translation. This sample is designed to induce a specific, targeted translation behaviour, such as peddling misinformation. We present two methods for crafting poisoned examples, and show that only a tiny handful of instances, amounting to only 0.02% of the training set, is sufficient to enact a successful attack. We outline a defence method against said attacks, which partly ameliorates the problem. However, we stress that this is a blind-spot in modern NMT, demanding immediate attention.

</details>

<details>

<summary>2021-07-12 16:32:47 - LATTE: LSTM Self-Attention based Anomaly Detection in Embedded Automotive Platforms</summary>

- *Vipin K. Kukkala, Sooryaa V. Thiruloga, Sudeep Pasricha*

- `2107.05561v1` - [abs](http://arxiv.org/abs/2107.05561v1) - [pdf](http://arxiv.org/pdf/2107.05561v1)

> Modern vehicles can be thought of as complex distributed embedded systems that run a variety of automotive applications with real-time constraints. Recent advances in the automotive industry towards greater autonomy are driving vehicles to be increasingly connected with various external systems (e.g., roadside beacons, other vehicles), which makes emerging vehicles highly vulnerable to cyber-attacks. Additionally, the increased complexity of automotive applications and the in-vehicle networks results in poor attack visibility, which makes detecting such attacks particularly challenging in automotive systems. In this work, we present a novel anomaly detection framework called LATTE to detect cyber-attacks in Controller Area Network (CAN) based networks within automotive platforms. Our proposed LATTE framework uses a stacked Long Short Term Memory (LSTM) predictor network with novel attention mechanisms to learn the normal operating behavior at design time. Subsequently, a novel detection scheme (also trained at design time) is used to detect various cyber-attacks (as anomalies) at runtime. We evaluate our proposed LATTE framework under different automotive attack scenarios and present a detailed comparison with the best-known prior works in this area, to demonstrate the potential of our approach.

</details>

<details>

<summary>2021-07-13 11:59:05 - Wasserstein GAN: Deep Generation applied on Bitcoins financial time series</summary>

- *Rikli Samuel, Bigler Daniel Nico, Pfenninger Moritz, Osterrieder Joerg*

- `2107.06008v1` - [abs](http://arxiv.org/abs/2107.06008v1) - [pdf](http://arxiv.org/pdf/2107.06008v1)

> Modeling financial time series is challenging due to their high volatility and unexpected happenings on the market. Most financial models and algorithms trying to fill the lack of historical financial time series struggle to perform and are highly vulnerable to overfitting. As an alternative, we introduce in this paper a deep neural network called the WGAN-GP, a data-driven model that focuses on sample generation. The WGAN-GP consists of a generator and discriminator function which utilize an LSTM architecture. The WGAN-GP is supposed to learn the underlying structure of the input data, which in our case, is the Bitcoin. Bitcoin is unique in its behavior; the prices fluctuate what makes guessing the price trend hardly impossible. Through adversarial training, the WGAN-GP should learn the underlying structure of the bitcoin and generate very similar samples of the bitcoin distribution. The generated synthetic time series are visually indistinguishable from the real data. But the numerical results show that the generated data were close to the real data distribution but distinguishable. The model mainly shows a stable learning behavior. However, the model has space for optimization, which could be achieved by adjusting the hyperparameters.

</details>

<details>

<summary>2021-07-13 13:46:33 - PakeMail: authentication and key management in decentralized secure email and messaging via PAKE</summary>

- *Itzel Vazquez Sandoval, Arash Atashpendar, Gabriele Lenzini, Peter Y. A. Ryan*

- `2107.06090v1` - [abs](http://arxiv.org/abs/2107.06090v1) - [pdf](http://arxiv.org/pdf/2107.06090v1)

> We propose the use of PAKE for achieving and enhancing entity authentication (EA) and key management (KM) in the context of decentralized end-to-end encrypted email and secure messaging, i.e., where neither a public key infrastructure nor trusted third parties are used. This approach not only simplifies the EA process by requiring users to share only a low-entropy secret, e.g., a memorable word, but it also allows us to establish a high-entropy secret key; this key enables a series of cryptographic enhancements and security properties, which are hard to achieve using out-of-band (OOB) authentication. We first study a few vulnerabilities in voice-based OOB authentication, in particular a combinatorial attack against lazy users, which we analyze in the context of a secure email solution. We then propose tackling public key authentication by solving the problem of "secure equality test" using PAKE, and discuss various protocols and their properties. This method enables the automation of important KM tasks (e.g. key renewal and future key pair authentications), reduces the impact of human errors, and lends itself to the asynchronous nature of email and modern messaging. It also provides cryptographic enhancements including multi-device synchronization and secure secret storage/retrieval, and paves the path for forward secrecy, deniability and post-quantum security. We also discuss the use of auditable PAKEs for mitigating a class of online guess and abort attacks in authentication protocols. To demonstrate the feasibility of our proposal, we present PakeMail, an implementation of the core idea, and discuss some of its cryptographic details, implemented features and efficiency aspects. We conclude with some design and security considerations, followed by future lines of work.

</details>

<details>

<summary>2021-07-13 14:33:37 - Defence against adversarial attacks using classical and quantum-enhanced Boltzmann machines</summary>

- *Aidan Kehoe, Peter Wittek, Yanbo Xue, Alejandro Pozas-Kerstjens*

- `2012.11619v2` - [abs](http://arxiv.org/abs/2012.11619v2) - [pdf](http://arxiv.org/pdf/2012.11619v2)

> We provide a robust defence to adversarial attacks on discriminative algorithms. Neural networks are naturally vulnerable to small, tailored perturbations in the input data that lead to wrong predictions. On the contrary, generative models attempt to learn the distribution underlying a dataset, making them inherently more robust to small perturbations. We use Boltzmann machines for discrimination purposes as attack-resistant classifiers, and compare them against standard state-of-the-art adversarial defences. We find improvements ranging from 5% to 72% against attacks with Boltzmann machines on the MNIST dataset. We furthermore complement the training with quantum-enhanced sampling from the D-Wave 2000Q annealer, finding results comparable with classical techniques and with marginal improvements in some cases. These results underline the relevance of probabilistic methods in constructing neural networks and highlight a novel scenario of practical relevance where quantum computers, even with limited hardware capabilites, could provide advantages over classical computers. This work is dedicated to the memory of Peter Wittek.

</details>

<details>

<summary>2021-07-13 15:13:39 - Correlation Analysis between the Robustness of Sparse Neural Networks and their Random Hidden Structural Priors</summary>

- *M. Ben Amor, J. Stier, M. Granitzer*

- `2107.06158v1` - [abs](http://arxiv.org/abs/2107.06158v1) - [pdf](http://arxiv.org/pdf/2107.06158v1)

> Deep learning models have been shown to be vulnerable to adversarial attacks. This perception led to analyzing deep learning models not only from the perspective of their performance measures but also their robustness to certain types of adversarial attacks. We take another step forward in relating the architectural structure of neural networks from a graph theoretic perspective to their robustness. We aim to investigate any existing correlations between graph theoretic properties and the robustness of Sparse Neural Networks. Our hypothesis is, that graph theoretic properties as a prior of neural network structures are related to their robustness. To answer to this hypothesis, we designed an empirical study with neural network models obtained through random graphs used as sparse structural priors for the networks. We additionally investigated the evaluation of a randomly pruned fully connected network as a point of reference.   We found that robustness measures are independent of initialization methods but show weak correlations with graph properties: higher graph densities correlate with lower robustness, but higher average path lengths and average node eccentricities show negative correlations with robustness measures. We hope to motivate further empirical and analytical research to tightening an answer to our hypothesis.

</details>

<details>

<summary>2021-07-14 07:20:48 - Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations</summary>

- *Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, Cho-Jui Hsieh*

- `2003.08938v7` - [abs](http://arxiv.org/abs/2003.08938v7) - [pdf](http://arxiv.org/pdf/2003.08938v7)

> A deep reinforcement learning (DRL) agent observes its states through observations, which may contain natural measurement errors or adversarial noises. Since the observations deviate from the true states, they can mislead the agent into making suboptimal actions. Several works have shown this vulnerability via adversarial attacks, but existing approaches on improving the robustness of DRL under this setting have limited success and lack for theoretical principles. We show that naively applying existing techniques on improving robustness for classification tasks, like adversarial training, is ineffective for many RL tasks. We propose the state-adversarial Markov decision process (SA-MDP) to study the fundamental properties of this problem, and develop a theoretically principled policy regularization which can be applied to a large family of DRL algorithms, including proximal policy optimization (PPO), deep deterministic policy gradient (DDPG) and deep Q networks (DQN), for both discrete and continuous action control problems. We significantly improve the robustness of PPO, DDPG and DQN agents under a suite of strong white box adversarial attacks, including new attacks of our own. Additionally, we find that a robust policy noticeably improves DRL performance even without an adversary in a number of environments. Our code is available at https://github.com/chenhongge/StateAdvDRL.

</details>

<details>

<summary>2021-07-14 08:34:39 - FAPR: Fast and Accurate Program Repair for Introductory Programming Courses</summary>

- *Yunlong Lu, Na Meng, Wenxin Li*

- `2107.06550v1` - [abs](http://arxiv.org/abs/2107.06550v1) - [pdf](http://arxiv.org/pdf/2107.06550v1)

> In introductory programming courses, it is challenging for instructors to provide debugging feedback on students' incorrect programs. Some recent tools automatically offer program repair feedback by identifying any differences between incorrect and correct programs, but suffer from issues related to scalability, accuracy, and cross-language portability. This paper presents FAPR -- our novel approach that suggests repairs based on program differences in a fast and accurate manner. FAPR is different from current tools in three aspects. First, it encodes syntactic information into token sequences to enable high-speed comparison between incorrect and correct programs. Second, to accurately extract program differences, FAPR adopts a novel matching algorithm that maximizes token-level matches and minimizes statement-level differences. Third, FAPR relies on testing instead of static/dynamic analysis to validate and refine candidate repairs, so it eliminates the language dependency or high runtime overhead incurred by complex program analysis. We implemented FAPR to suggest repairs for both C and C++ programs; our experience shows the great cross-language portability of FAPR. More importantly, we empirically compared FAPR with a state-of-the-art tool Clara. FAPR suggested repairs for over 95.5% of incorrect solutions. We sampled 250 repairs among FAPR's suggestions, and found 89.6% of the samples to be minimal and correct. FAPR outperformed Clara by suggesting repairs for more cases, creating smaller repairs, producing higher-quality fixes, and causing lower runtime overheads. Our results imply that FAPR can potentially help instructors or TAs to effectively locate bugs in incorrect code, and to provide debugging hints/guidelines based on those generated repairs.

</details>

<details>

<summary>2021-07-14 11:59:12 - Preventing Spoliation of Evidence with Blockchain: A Perspective from South Asia</summary>

- *Ali Shahaab, Chaminda Hewage, Imtiaz Khan*

- `2107.14050v1` - [abs](http://arxiv.org/abs/2107.14050v1) - [pdf](http://arxiv.org/pdf/2107.14050v1)

> Evidence destruction and tempering is a time-tested tactic to protect the powerful perpetrators, criminals, and corrupt officials. Countries where law enforcing institutions and judicial system can be comprised, and evidence destroyed or tampered, ordinary citizens feel disengaged with the investigation or prosecution process, and in some instances, intimidated due to the vulnerability to exposure and retribution. Using Distributed Ledger Technologies (DLT), such as blockchain, as the underpinning technology, here we propose a conceptual model - 'EvidenceChain', through which citizens can anonymously upload digital evidence, having assurance that the integrity of the evidence will be preserved in an immutable and indestructible manner. Person uploading the evidence can anonymously share it with investigating authorities or openly with public, if coerced by the perpetrators or authorities. Transferring the ownership of evidence from authority to ordinary citizen, and custodianship of evidence from susceptible centralized repository to an immutable and indestructible distributed repository, can cause a paradigm shift of power that not only can minimize spoliation of evidence but human rights abuse too. Here the conceptual model was theoretically tested against some high-profile spoliation of evidence cases from four South Asian developing countries that often rank high in global corruption index and low in human rights index.

</details>

<details>

<summary>2021-07-14 12:45:48 - DeepMutants: Training neural bug detectors with contextual mutations</summary>

- *Cedric Richter, Heike Wehrheim*

- `2107.06657v1` - [abs](http://arxiv.org/abs/2107.06657v1) - [pdf](http://arxiv.org/pdf/2107.06657v1)

> Learning-based bug detectors promise to find bugs in large code bases by exploiting natural hints such as names of variables and functions or comments. Still, existing techniques tend to underperform when presented with realistic bugs. We believe bug detector learning to currently suffer from a lack of realistic defective training examples. In fact, real world bugs are scarce which has driven existing methods to train on artificially created and mostly unrealistic mutants. In this work, we propose a novel contextual mutation operator which incorporates knowledge about the mutation context to dynamically inject natural and more realistic faults into code. Our approach employs a masked language model to produce a context-dependent distribution over feasible token replacements. The evaluation shows that sampling from a language model does not only produce mutants which more accurately represent real bugs but also lead to better performing bug detectors, both on artificial benchmarks and on real world source code.

</details>

<details>

<summary>2021-07-15 12:50:49 - Robust Backdoor Attacks against Deep Neural Networks in Real Physical World</summary>

- *Mingfu Xue, Can He, Shichang Sun, Jian Wang, Weiqiang Liu*

- `2104.07395v2` - [abs](http://arxiv.org/abs/2104.07395v2) - [pdf](http://arxiv.org/pdf/2104.07395v2)

> Deep neural networks (DNN) have been widely deployed in various applications. However, many researches indicated that DNN is vulnerable to backdoor attacks. The attacker can create a hidden backdoor in target DNN model, and trigger the malicious behaviors by submitting specific backdoor instance. However, almost all the existing backdoor works focused on the digital domain, while few studies investigate the backdoor attacks in real physical world. Restricted to a variety of physical constraints, the performance of backdoor attacks in the real physical world will be severely degraded. In this paper, we propose a robust physical backdoor attack method, PTB (physical transformations for backdoors), to implement the backdoor attacks against deep learning models in the real physical world. Specifically, in the training phase, we perform a series of physical transformations on these injected backdoor instances at each round of model training, so as to simulate various transformations that a backdoor may experience in real world, thus improves its physical robustness. Experimental results on the state-of-the-art face recognition model show that, compared with the backdoor methods that without PTB, the proposed attack method can significantly improve the performance of backdoor attacks in real physical world. Under various complex physical conditions, by injecting only a very small ratio (0.5%) of backdoor instances, the attack success rate of physical backdoor attacks with the PTB method on VGGFace is 82%, while the attack success rate of backdoor attacks without the proposed PTB method is lower than 11%. Meanwhile, the normal performance of the target DNN model has not been affected.

</details>

<details>

<summary>2021-07-15 16:55:49 - GI-NNet \& RGI-NNet: Development of Robotic Grasp Pose Models, Trainable with Large as well as Limited Labelled Training Datasets, under supervised and semi supervised paradigms</summary>

- *Priya Shukla, Nilotpal Pramanik, Deepesh Mehta, G. C. Nandi*

- `2107.07452v1` - [abs](http://arxiv.org/abs/2107.07452v1) - [pdf](http://arxiv.org/pdf/2107.07452v1)

> Our way of grasping objects is challenging for efficient, intelligent and optimal grasp by COBOTs. To streamline the process, here we use deep learning techniques to help robots learn to generate and execute appropriate grasps quickly. We developed a Generative Inception Neural Network (GI-NNet) model, capable of generating antipodal robotic grasps on seen as well as unseen objects. It is trained on Cornell Grasping Dataset (CGD) and attained 98.87% grasp pose accuracy for detecting both regular and irregular shaped objects from RGB-Depth (RGB-D) images while requiring only one third of the network trainable parameters as compared to the existing approaches. However, to attain this level of performance the model requires the entire 90% of the available labelled data of CGD keeping only 10% labelled data for testing which makes it vulnerable to poor generalization. Furthermore, getting sufficient and quality labelled dataset is becoming increasingly difficult keeping in pace with the requirement of gigantic networks. To address these issues, we attach our model as a decoder with a semi-supervised learning based architecture known as Vector Quantized Variational Auto Encoder (VQVAE), which works efficiently when trained both with the available labelled and unlabelled data. The proposed model, which we name as Representation based GI-NNet (RGI-NNet), has been trained with various splits of label data on CGD with as minimum as 10% labelled dataset together with latent embedding generated from VQVAE up to 50% labelled data with latent embedding obtained from VQVAE. The performance level, in terms of grasp pose accuracy of RGI-NNet, varies between 92.13% to 95.6% which is far better than several existing models trained with only labelled dataset. For the performance verification of both GI-NNet and RGI-NNet models, we use Anukul (Baxter) hardware cobot.

</details>

<details>

<summary>2021-07-15 17:59:10 - VAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording</summary>

- *Hirofumi Inaguma, Tatsuya Kawahara*

- `2107.07509v1` - [abs](http://arxiv.org/abs/2107.07509v1) - [pdf](http://arxiv.org/pdf/2107.07509v1)

> In this work, we propose novel decoding algorithms to enable streaming automatic speech recognition (ASR) on unsegmented long-form recordings without voice activity detection (VAD), based on monotonic chunkwise attention (MoChA) with an auxiliary connectionist temporal classification (CTC) objective. We propose a block-synchronous beam search decoding to take advantage of efficient batched output-synchronous and low-latency input-synchronous searches. We also propose a VAD-free inference algorithm that leverages CTC probabilities to determine a suitable timing to reset the model states to tackle the vulnerability to long-form data. Experimental evaluations demonstrate that the block-synchronous decoding achieves comparable accuracy to the label-synchronous one. Moreover, the VAD-free inference can recognize long-form speech robustly for up to a few hours.

</details>

<details>

<summary>2021-07-15 22:39:34 - GLIB: Towards Automated Test Oracle for Graphically-Rich Applications</summary>

- *Ke Chen, Yufei Li, Yingfeng Chen, Changjie Fan, Zhipeng Hu, Wei Yang*

- `2106.10507v3` - [abs](http://arxiv.org/abs/2106.10507v3) - [pdf](http://arxiv.org/pdf/2106.10507v3)

> Graphically-rich applications such as games are ubiquitous with attractive visual effects of Graphical User Interface (GUI) that offers a bridge between software applications and end-users. However, various types of graphical glitches may arise from such GUI complexity and have become one of the main component of software compatibility issues. Our study on bug reports from game development teams in NetEase Inc. indicates that graphical glitches frequently occur during the GUI rendering and severely degrade the quality of graphically-rich applications such as video games. Existing automated testing techniques for such applications focus mainly on generating various GUI test sequences and check whether the test sequences can cause crashes. These techniques require constant human attention to captures non-crashing bugs such as bugs causing graphical glitches. In this paper, we present the first step in automating the test oracle for detecting non-crashing bugs in graphically-rich applications. Specifically, we propose \texttt{GLIB} based on a code-based data augmentation technique to detect game GUI glitches. We perform an evaluation of \texttt{GLIB} on 20 real-world game apps (with bug reports available) and the result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall in detecting non-crashing bugs such as game GUI glitches. Practical application of \texttt{GLIB} on another 14 real-world games (without bug reports) further demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48 of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.

</details>

<details>

<summary>2021-07-16 03:31:29 - An Empirical Study of Rule-Based and Learning-Based Approaches for Static Application Security Testing</summary>

- *Roland Croft, Dominic Newlands, Ziyu Chen, M. Ali Babar*

- `2107.01921v2` - [abs](http://arxiv.org/abs/2107.01921v2) - [pdf](http://arxiv.org/pdf/2107.01921v2)

> Background: Static Application Security Testing (SAST) tools purport to assist developers in detecting security issues in source code. These tools typically use rule-based approaches to scan source code for security vulnerabilities. However, due to the significant shortcomings of these tools (i.e., high false positive rates), learning-based approaches for Software Vulnerability Prediction (SVP) are becoming a popular approach. Aims: Despite the similar objectives of these two approaches, their comparative value is unexplored. We provide an empirical analysis of SAST tools and SVP models, to identify their relative capabilities for source code security analysis. Method: We evaluate the detection and assessment performance of several common SAST tools and SVP models on a variety of vulnerability datasets. We further assess the viability and potential benefits of combining the two approaches. Results: SAST tools and SVP models provide similar detection capabilities, but SVP models exhibit better overall performance for both detection and assessment. Unification of the two approaches is difficult due to lacking synergies. Conclusions: Our study generates 12 main findings which provide insights into the capabilities and synergy of these two approaches. Through these observations we provide recommendations for use and improvement.

</details>

<details>

<summary>2021-07-16 15:36:03 - How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks?</summary>

- *Camille Koenders, Johannes Filla, Nicolai Schneider, Vinicius Woloszyn*

- `2107.07970v1` - [abs](http://arxiv.org/abs/2107.07970v1) - [pdf](http://arxiv.org/pdf/2107.07970v1)

> As the spread of false information on the internet has increased dramatically in recent years, more and more attention is being paid to automated fake news detection. Some fake news detection methods are already quite successful. Nevertheless, there are still many vulnerabilities in the detection algorithms. The reason for this is that fake news publishers can structure and formulate their texts in such a way that a detection algorithm does not expose this text as fake news. This paper shows that it is possible to automatically attack state-of-the-art models that have been trained to detect Fake News, making these vulnerable. For this purpose, corresponding models were first trained based on a dataset. Then, using Text-Attack, an attempt was made to manipulate the trained models in such a way that previously correctly identified fake news was classified as true news. The results show that it is possible to automatically bypass Fake News detection mechanisms, leading to implications concerning existing policy initiatives.

</details>

<details>

<summary>2021-07-16 17:58:08 - Towards a Benchmark Set for Program Repair Based on Partial Fixes</summary>

- *Dirk Beyer, Lars Grunske, Thomas Lemberger, Minxing Tang*

- `2107.08038v1` - [abs](http://arxiv.org/abs/2107.08038v1) - [pdf](http://arxiv.org/pdf/2107.08038v1)

> Software bugs significantly contribute to software cost and increase the risk of system malfunctioning. In recent years, many automated program-repair approaches have been proposed to automatically fix undesired program behavior. Despite of their great success, specific problems such as fixing bugs with partial fixes still remain unresolved. A partial fix to a known software issue is a programmer's failed attempt to fix the issue the first time. Even though it fails, this fix attempt still conveys important information such as the suspicious software region and the bug type. In this work we do not propose an approach for program repair with partial fixes, but instead answer a preliminary question: Do partial fixes occur often enough, in general, to be relevant for the research area of automated program repair? We crawled 1500 open-source C repositories on GitHub for partial fixes. The result is a benchmark set of 2204 benchmark tasks for automated program repair based on partial fixes. The benchmark set is available open source and open to further contributions and improvement.

</details>

<details>

<summary>2021-07-17 15:49:22 - Tea: Program Repair Using Neural Network Based on Program Information Attention Matrix</summary>

- *Wenshuo Wang, Chen Wu, Liang Cheng, Yang Zhang*

- `2107.08262v1` - [abs](http://arxiv.org/abs/2107.08262v1) - [pdf](http://arxiv.org/pdf/2107.08262v1)

> The advance in machine learning (ML)-driven natural language process (NLP) points a promising direction for automatic bug fixing for software programs, as fixing a buggy program can be transformed to a translation task. While software programs contain much richer information than one-dimensional natural language documents, pioneering work on using ML-driven NLP techniques for automatic program repair only considered a limited set of such information. We hypothesize that more comprehensive information of software programs, if appropriately utilized, can improve the effectiveness of ML-driven NLP approaches in repairing software programs. As the first step towards proving this hypothesis, we propose a unified representation to capture the syntax, data flow, and control flow aspects of software programs, and devise a method to use such a representation to guide the transformer model from NLP in better understanding and fixing buggy programs. Our preliminary experiment confirms that the more comprehensive information of software programs used, the better ML-driven NLP techniques can perform in fixing bugs in these programs.

</details>

<details>

<summary>2021-07-17 17:54:28 - Systematic Mutation-based Evaluation of the Soundness of Security-focused Android Static Analysis Techniques</summary>

- *Amit Seal Ami, Kaushal Kafle, Kevin Moran, Adwait Nadkarni, Denys Poshyvanyk*

- `2102.06829v2` - [abs](http://arxiv.org/abs/2102.06829v2) - [pdf](http://arxiv.org/pdf/2102.06829v2)

> Mobile application security has been a major area of focus for security research over the course of the last decade. Numerous application analysis tools have been proposed in response to malicious, curious, or vulnerable apps. However, existing tools, and specifically, static analysis tools, trade soundness of the analysis for precision and performance and are hence soundy. Unfortunately, the specific unsound choices or flaws in the design of these tools is often not known or well-documented, leading to misplaced confidence among researchers, developers, and users. This paper describes the Mutation-based Soundness Evaluation ($\mu$SE) framework, which systematically evaluates Android static analysis tools to discover, document, and fix flaws, by leveraging the well-founded practice of mutation analysis. We implemented $\mu$SE and applied it to a set of prominent Android static analysis tools that detect private data leaks in apps. In a study conducted previously, we used $\mu$SE to discover $13$ previously undocumented flaws in FlowDroid, one of the most prominent data leak detectors for Android apps. Moreover, we discovered that flaws also propagated to other tools that build upon the design or implementation of FlowDroid or its components. This paper substantially extends our $\mu$SE framework and offers an new in-depth analysis of two more major tools in our 2020 study, we find $12$ new, undocumented flaws and demonstrate that all $25$ flaws are found in more than one tool, regardless of any inheritance-relation among the tools. Our results motivate the need for systematic discovery and documentation of unsound choices in soundy tools and demonstrate the opportunities in leveraging mutation testing in achieving this goal.

</details>

<details>

<summary>2021-07-18 09:34:57 - RobustFed: A Truth Inference Approach for Robust Federated Learning</summary>

- *Farnaz Tahmasebian, Jian Lou, Li Xiong*

- `2107.08402v1` - [abs](http://arxiv.org/abs/2107.08402v1) - [pdf](http://arxiv.org/pdf/2107.08402v1)

> Federated learning is a prominent framework that enables clients (e.g., mobile devices or organizations) to train a collaboratively global model under a central server's orchestration while keeping local training datasets' privacy. However, the aggregation step in federated learning is vulnerable to adversarial attacks as the central server cannot manage clients' behavior. Therefore, the global model's performance and convergence of the training process will be affected under such attacks.To mitigate this vulnerability issue, we propose a novel robust aggregation algorithm inspired by the truth inference methods in crowdsourcing via incorporating the worker's reliability into aggregation. We evaluate our solution on three real-world datasets with a variety of machine learning models. Experimental results show that our solution ensures robust federated learning and is resilient to various types of attacks, including noisy data attacks, Byzantine attacks, and label flipping attacks.

</details>

<details>

<summary>2021-07-18 13:12:17 - Weighted Voting on the Blockchain: Improving Consensus in Proof of Stake Protocols</summary>

- *Stefanos Leonardos, Daniel Reijsbergen, Georgios Piliouras*

- `1903.04213v4` - [abs](http://arxiv.org/abs/1903.04213v4) - [pdf](http://arxiv.org/pdf/1903.04213v4)

> Proof of Stake (PoS) protocols rely on voting mechanisms to reach consensus on the current state. If an enhanced majority of staking nodes, also called validators, agree on a proposed block, then this block is appended to the blockchain. Yet, these protocols remain vulnerable to faults caused by validators who abstain either accidentally or maliciously. To protect against such faults while retaining the PoS selection and reward allocation schemes, we study weighted voting in validator committees. We formalize the block creation process and introduce validators' voting profiles which we update by a multiplicative weights algorithm relative to validators' voting behavior and aggregate blockchain rewards. Using this framework, we leverage weighted majority voting rules that optimize collective decision making to show, both numerically and analytically, that the consensus mechanism is more robust if validators' votes are appropriately scaled. We raise potential issues and limitations of weighted voting in trustless, decentralized networks and relate our results to the design of current PoS protocols.

</details>

<details>

<summary>2021-07-19 11:34:09 - CVEfixes: Automated Collection of Vulnerabilities and Their Fixes from Open-Source Software</summary>

- *Guru Prasad Bhandari, Amara Naseer, Leon Moonen*

- `2107.08760v1` - [abs](http://arxiv.org/abs/2107.08760v1) - [pdf](http://arxiv.org/pdf/2107.08760v1)

> Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the public National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes.   The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits.   CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.

</details>

<details>

<summary>2021-07-19 20:44:26 - SMS Goes Nuclear: Fortifying SMS-Based MFA in Online Account Ecosystem</summary>

- *Weizhao Jin, Xiaoyu Ji, Ruiwen He, Zhou Zhuang, Wenyuan Xu, Yuan Tian*

- `2104.08651v2` - [abs](http://arxiv.org/abs/2104.08651v2) - [pdf](http://arxiv.org/pdf/2104.08651v2)

> With the rapid growth of online services, the number of online accounts proliferates. The security of a single user account no longer depends merely on its own service provider but also the accounts on other service platforms(We refer to this online account environment as Online Account Ecosystem). In this paper, we first uncover the vulnerability of Online Account Ecosystem, which stems from the defective multi-factor authentication (MFA), specifically the ones with SMS-based verification, and dependencies among accounts on different platforms. We propose Chain Reaction Attack that exploits the weakest point in Online Account Ecosystem and can ultimately compromise the most secure platform. Furthermore, we design and implement ActFort, a systematic approach to detect the vulnerability of Online Account Ecosystem by analyzing the authentication credential factors and sensitive personal information as well as evaluating the dependency relationships among online accounts. We evaluate our system on hundreds of representative online services listed in Alexa in diversified fields. Based on the analysis from ActFort, we provide several pragmatic insights into the current Online Account Ecosystem and propose several feasible countermeasures including the online account exposed information protection mechanism and the built-in authentication to fortify the security of Online Account Ecosystem.

</details>

<details>

<summary>2021-07-20 09:30:37 - Secure Access Control for DAG-based Distributed Ledgers</summary>

- *Lianna Zhao, Luigi Vigneri, Andrew Cullen, William Sanders, Pietro Ferraro, Robert Shorten*

- `2107.10238v1` - [abs](http://arxiv.org/abs/2107.10238v1) - [pdf](http://arxiv.org/pdf/2107.10238v1)

> Access control is a fundamental component of the design of distributed ledgers, influencing many aspects of their design, such as fairness, efficiency, traditional notions of network security, and adversarial attacks such as Denial-of-Service (DoS) attacks. In this work, we consider the security of a recently proposed access control protocol for Directed Acyclic Graph-based distributed ledgers. We present a number of attack scenarios and potential vulnerabilities of the protocol and introduce a number of additional features which enhance its resilience. Specifically, a blacklisting algorithm, which is based on a reputation-weighted threshold, is introduced to handle both spamming and multi-rate malicious attackers. The introduction of a solidification request component is also introduced to ensure the fairness and consistency of network in the presence of attacks. Finally, a timestamp component is also introduced to maintain the consistency of the network in the presence of multi-rate attackers. Simulations to illustrate the efficacy and robustness of the revised protocol are also described.

</details>

<details>

<summary>2021-07-21 07:40:28 - "Won't We Fix this Issue?" Qualitative Characterization and Automated Identification of Wontfix Issues on GitHub</summary>

- *Andrea Di Sorbo, Gerardo Canfora, Sebastiano Panichella*

- `1904.02414v3` - [abs](http://arxiv.org/abs/1904.02414v3) - [pdf](http://arxiv.org/pdf/1904.02414v3)

> Context: Addressing user requests in the form of bug reports and Github issues represents a crucial task of any successful software project. However, user-submitted issue reports tend to widely differ in their quality, and developers spend a considerable amount of time handling them. Objective: By collecting a dataset of around 6,000 issues of 279 GitHub projects, we observe that developers take significant time (i.e., about five months, on average) before labeling an issue as a wontfix. For this reason, in this paper, we empirically investigate the nature of wontfix issues and methods to facilitate issue management process. Method: We first manually analyze a sample of 667 wontfix issues, extracted from heterogeneous projects, investigating the common reasons behind a "wontfix decision", the main characteristics of wontfix issues and the potential factors that could be connected with the time to close them. Furthermore, we experiment with approaches enabling the prediction of wontfix issues by analyzing the titles and descriptions of reported issues when submitted. Results and conclusion: Our investigation sheds some light on the wontfix issues' characteristics, as well as the potential factors that may affect the time required to make a "wontfix decision". Our results also demonstrate that it is possible to perform prediction of wontfix issues with high average values of precision, recall, and F-measure (90%-93%).

</details>

<details>

<summary>2021-07-21 08:14:48 - Predicting Issue Types on GitHub</summary>

- *Rafael Kallis, Andrea Di Sorbo, Gerardo Canfora, Sebastiano Panichella*

- `2107.09936v1` - [abs](http://arxiv.org/abs/2107.09936v1) - [pdf](http://arxiv.org/pdf/2107.09936v1)

> Software maintenance and evolution involves critical activities for the success of software projects. To support such activities and keep code up-to-date and error-free, software communities make use of issue trackers, i.e., tools for signaling, handling, and addressing the issues occurring in software systems. However, in popular projects, tens or hundreds of issue reports are daily submitted. In this context, identifying the type of each submitted report (e.g., bug report, feature request, etc.) would facilitate the management and the prioritization of the issues to address. To support issue handling activities, in this paper, we propose Ticket Tagger, a GitHub app analyzing the issue title and description through machine learning techniques to automatically recognize the types of reports submitted on GitHub and assign labels to each issue accordingly. We empirically evaluated the tool's prediction performance on about 30,000 GitHub issues. Our results show that the Ticket Tagger can identify the correct labels to assign to GitHub issues with reasonably high effectiveness. Considering these results and the fact that the tool is designed to be easily integrated in the GitHub issue management process, Ticket Tagger consists in a useful solution for developers.

</details>

<details>

<summary>2021-07-21 09:59:16 - The analysis approach of ThreatGet</summary>

- *Korbinian Christl, Thorsten Tarrach*

- `2107.09986v1` - [abs](http://arxiv.org/abs/2107.09986v1) - [pdf](http://arxiv.org/pdf/2107.09986v1)

> Nowadays, almost all electronic devices include a communication interface that allows to interact with them, exchange data, or operate their services remotely. The trend toward increased interconnectivity simultaneously increases the vulnerability of these systems. Due to the high costs associated with comprehensive security analysis, many manufacturers neglect the safety aspect of a product in order to avoid costs. However, the importance of secure IT systems is growing, as the security of a system can also influence safety-critical aspects. Standard security analysis approaches are nowadays still mainly based on time-intensive and error-prone manual activities. In this paper, we present the formal concepts of the automatic threat and vulnerability analysis tool ThreatGet. Therefore, we introduce the concept of the Extended Data-Flow Diagram that is used to represent the system under investigation in an abstracted form, and we highlight the formal analysis language of the tool. This domain-specific language is used to formulate so-called anti-patterns. These anti-patterns that can be interpreted by the tool for an automatic security analysis of the system. Besides the language declaration, we present the entire semantic evaluation of the language during the analysis. Parts of the definitions and elaborations of the diagram model and the analysis language were developed in the context of the master thesis of Korbinian Christl, in cooperation with the University of Vienna.

</details>

<details>

<summary>2021-07-21 12:30:14 - A Tandem Framework Balancing Privacy and Security for Voice User Interfaces</summary>

- *Ranya Aloufi, Hamed Haddadi, David Boyle*

- `2107.10045v1` - [abs](http://arxiv.org/abs/2107.10045v1) - [pdf](http://arxiv.org/pdf/2107.10045v1)

> Speech synthesis, voice cloning, and voice conversion techniques present severe privacy and security threats to users of voice user interfaces (VUIs). These techniques transform one or more elements of a speech signal, e.g., identity and emotion, while preserving linguistic information. Adversaries may use advanced transformation tools to trigger a spoofing attack using fraudulent biometrics for a legitimate speaker. Conversely, such techniques have been used to generate privacy-transformed speech by suppressing personally identifiable attributes in the voice signals, achieving anonymization. Prior works have studied the security and privacy vectors in parallel, and thus it raises alarm that if a benign user can achieve privacy by a transformation, it also means that a malicious user can break security by bypassing the anti-spoofing mechanism. In this paper, we take a step towards balancing two seemingly conflicting requirements: security and privacy. It remains unclear what the vulnerabilities in one domain imply for the other, and what dynamic interactions exist between them. A better understanding of these aspects is crucial for assessing and mitigating vulnerabilities inherent with VUIs and building effective defenses. In this paper,(i) we investigate the applicability of the current voice anonymization methods by deploying a tandem framework that jointly combines anti-spoofing and authentication models, and evaluate the performance of these methods;(ii) examining analytical and empirical evidence, we reveal a duality between the two mechanisms as they offer different ways to achieve the same objective, and we show that leveraging one vector significantly amplifies the effectiveness of the other;(iii) we demonstrate that to effectively defend from potential attacks against VUIs, it is necessary to investigate the attacks from multiple complementary perspectives(security and privacy).

</details>

<details>

<summary>2021-07-21 19:37:38 - The Curse of Correlations for Robust Fingerprinting of Relational Databases</summary>

- *Tianxi Ji, Emre Yilmaz, Erman Ayday, Pan Li*

- `2103.06438v2` - [abs](http://arxiv.org/abs/2103.06438v2) - [pdf](http://arxiv.org/pdf/2103.06438v2)

> Database fingerprinting have been widely adopted to prevent unauthorized sharing of data and identify the source of data leakages. Although existing schemes are robust against common attacks, like random bit flipping and subset attack, their robustness degrades significantly if attackers utilize the inherent correlations among database entries. In this paper, we first demonstrate the vulnerability of existing database fingerprinting schemes by identifying different correlation attacks: column-wise correlation attack, row-wise correlation attack, and the integration of them. To provide robust fingerprinting against the identified correlation attacks, we then develop mitigation techniques, which can work as post-processing steps for any off-the-shelf database fingerprinting schemes. The proposed mitigation techniques also preserve the utility of the fingerprinted database considering different utility metrics. We empirically investigate the impact of the identified correlation attacks and the performance of mitigation techniques using real-world relational databases. Our results show (i) high success rates of the identified correlation attacks against existing fingerprinting schemes (e.g., the integrated correlation attack can distort 64.8\% fingerprint bits by just modifying 14.2\% entries in a fingerprinted database), and (ii) high robustness of the proposed mitigation techniques (e.g., with the mitigation techniques, the integrated correlation attack can only distort $3\%$ fingerprint bits).

</details>

<details>

<summary>2021-07-22 00:40:33 - Theoretical foundations and limits of word embeddings: what types of meaning can they capture?</summary>

- *Alina Arseniev-Koehler*

- `2107.10413v1` - [abs](http://arxiv.org/abs/2107.10413v1) - [pdf](http://arxiv.org/pdf/2107.10413v1)

> Measuring meaning is a central problem in cultural sociology and word embeddings may offer powerful new tools to do so. But like any tool, they build on and exert theoretical assumptions. In this paper I theorize the ways in which word embeddings model three core premises of a structural linguistic theory of meaning: that meaning is relational, coherent, and may be analyzed as a static system. In certain ways, word embedding methods are vulnerable to the same, enduring critiques of these premises. In other ways, they offer novel solutions to these critiques. More broadly, formalizing the study of meaning with word embeddings offers theoretical opportunities to clarify core concepts and debates in cultural sociology, such as the coherence of meaning. Just as network analysis specified the once vague notion of social relations (Borgatti et al. 2009), formalizing meaning with embedding methods can push us to specify and reimagine meaning itself.

</details>

<details>

<summary>2021-07-22 06:54:18 - Unsupervised Detection of Adversarial Examples with Model Explanations</summary>

- *Gihyuk Ko, Gyumin Lim*

- `2107.10480v1` - [abs](http://arxiv.org/abs/2107.10480v1) - [pdf](http://arxiv.org/pdf/2107.10480v1)

> Deep Neural Networks (DNNs) have shown remarkable performance in a diverse range of machine learning applications. However, it is widely known that DNNs are vulnerable to simple adversarial perturbations, which causes the model to incorrectly classify inputs. In this paper, we propose a simple yet effective method to detect adversarial examples, using methods developed to explain the model's behavior. Our key observation is that adding small, humanly imperceptible perturbations can lead to drastic changes in the model explanations, resulting in unusual or irregular forms of explanations. From this insight, we propose an unsupervised detection of adversarial examples using reconstructor networks trained only on model explanations of benign examples. Our evaluations with MNIST handwritten dataset show that our method is capable of detecting adversarial examples generated by the state-of-the-art algorithms with high confidence. To the best of our knowledge, this work is the first in suggesting unsupervised defense method using model explanations.

</details>

<details>

<summary>2021-07-22 13:35:36 - Survey of Methods for Automated Code-Reuse Exploit Generation</summary>

- *Alexey Vishnyakov, Alexey Nurmukhametov*

- `2011.07862v2` - [abs](http://arxiv.org/abs/2011.07862v2) - [pdf](http://arxiv.org/pdf/2011.07862v2)

> This paper provides a survey of methods and tools for automated code-reuse exploit generation. Such exploits use code that is already contained in a vulnerable program. The code-reuse approach allows one to exploit vulnerabilities in the presence of operating system protection that prohibits data memory execution. This paper contains a description of various code-reuse methods: return-to-libc attack, return-oriented programming, jump-oriented programming, and others. We define fundamental terms: gadget, gadget frame, gadget catalog. Moreover, we show that, in fact, a gadget is an instruction, and a set of gadgets defines a virtual machine. We can reduce an exploit creation problem to code generation for this virtual machine. Each particular executable file defines a virtual machine instruction set. We provide a survey of methods for gadgets searching and determining their semantics (creating a gadget catalog). These methods allow one to get the virtual machine instruction set. If a set of gadgets is Turing-complete, then a compiler can use a gadget catalog as a target architecture. However, some instructions can be absent. Hence we discuss several approaches to replace missing instructions with multiple gadgets. An exploit generation tool can chain gadgets by pattern searching (regular expressions) or considering gadgets semantics. Furthermore, some chaining methods use genetic algorithms, while others use SMT-solvers. We compare existing open-source tools and propose a testing system rop-benchmark that can be used to verify whether a generated chain successfully opens a shell.

</details>

<details>

<summary>2021-07-22 16:19:50 - SMEs' Confidentiality Concerns for Security Information Sharing</summary>

- *Alireza Shojaifar, Samuel A. Fricker*

- `2007.06308v2` - [abs](http://arxiv.org/abs/2007.06308v2) - [pdf](http://arxiv.org/pdf/2007.06308v2)

> Small and medium-sized enterprises are considered an essential part of the EU economy, however, highly vulnerable to cyberattacks. SMEs have specific characteristics which separate them from large companies and influence their adoption of good cybersecurity practices. To mitigate the SMEs' cybersecurity adoption issues and raise their awareness of cyber threats, we have designed a self-paced security assessment and capability improvement method, CYSEC. CYSEC is a security awareness and training method that utilises self-reporting questionnaires to collect companies' information about cybersecurity awareness, practices, and vulnerabilities to generate automated recommendations for counselling. However, confidentiality concerns about cybersecurity information have an impact on companies' willingness to share their information. Security information sharing decreases the risk of incidents and increases users' self-efficacy in security awareness programs. This paper presents the results of semi-structured interviews with seven chief information security officers of SMEs to evaluate the impact of online consent communication on motivation for information sharing. The results were analysed in respect of the Self Determination Theory. The findings demonstrate that online consent with multiple options for indicating a suitable level of agreement improved motivation for information sharing. This allows many SMEs to participate in security information sharing activities and supports security experts to have a better overview of common vulnerabilities. The final publication is available at Springer via https://doi.org/10.1007/978-3-030-57404-8_22

</details>

<details>

<summary>2021-07-22 17:05:47 - Membership Inference Attack and Defense for Wireless Signal Classifiers with Deep Learning</summary>

- *Yi Shi, Yalin E. Sagduyu*

- `2107.12173v1` - [abs](http://arxiv.org/abs/2107.12173v1) - [pdf](http://arxiv.org/pdf/2107.12173v1)

> An over-the-air membership inference attack (MIA) is presented to leak private information from a wireless signal classifier. Machine learning (ML) provides powerful means to classify wireless signals, e.g., for PHY-layer authentication. As an adversarial machine learning attack, the MIA infers whether a signal of interest has been used in the training data of a target classifier. This private information incorporates waveform, channel, and device characteristics, and if leaked, can be exploited by an adversary to identify vulnerabilities of the underlying ML model (e.g., to infiltrate the PHY-layer authentication). One challenge for the over-the-air MIA is that the received signals and consequently the RF fingerprints at the adversary and the intended receiver differ due to the discrepancy in channel conditions. Therefore, the adversary first builds a surrogate classifier by observing the spectrum and then launches the black-box MIA on this classifier. The MIA results show that the adversary can reliably infer signals (and potentially the radio and channel information) used to build the target classifier. Therefore, a proactive defense is developed against the MIA by building a shadow MIA model and fooling the adversary. This defense can successfully reduce the MIA accuracy and prevent information leakage from the wireless signal classifier.

</details>

<details>

<summary>2021-07-23 13:00:13 - A Characterisation of Smart Grid DoS Attacks</summary>

- *Dilara Acarali, Muttukrishnan Rajarajan, Doron Chema, Mark Ginzburg*

- `2107.11202v1` - [abs](http://arxiv.org/abs/2107.11202v1) - [pdf](http://arxiv.org/pdf/2107.11202v1)

> Traditional power grids are evolving to keep pace with the demands of the modern age. Smart grids contain integrated IT systems for better management and efficiency, but in doing so, also inherit a plethora of cyber-security threats and vulnerabilities. Denial-of-Service (DoS) is one such threat. At the same time, the smart grid has particular characteristics (e.g. minimal delay tolerance), which can influence the nature of threats and so require special consideration. In this paper, we identify a set of possible smart grid-specific DoS scenarios based on current research, and analyse them in the context of the grid components they target. Based on this, we propose a novel target-based classification scheme and further characterise each scenario by qualitatively exploring it in the context of the underlying grid infrastructure. This culminates in a smart grid-centric analysis of the threat to reveal the nature of DoS in this environment.

</details>

<details>

<summary>2021-07-23 22:56:32 - Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics</summary>

- *Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov*

- `2104.13346v2` - [abs](http://arxiv.org/abs/2104.13346v2) - [pdf](http://arxiv.org/pdf/2104.13346v2)

> Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights into the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations, we identify the proportion of different categories of factual errors in various summarization models and benchmark factuality metrics, showing their correlation with human judgment as well as their specific strengths and weaknesses.

</details>

<details>

<summary>2021-07-24 00:15:56 - BIoTA Control-Aware Attack Analytics for Building Internet of Things</summary>

- *Nur Imtiazul Haque, Mohammad Ashiqur Rahman, Dong Chen, Hisham Kholidy*

- `2107.14136v1` - [abs](http://arxiv.org/abs/2107.14136v1) - [pdf](http://arxiv.org/pdf/2107.14136v1)

> Modern building control systems adopt demand control heating, ventilation, and cooling (HVAC) for increased energy efficiency. The integration of the Internet of Things (IoT) in the building control system can determine real-time demand, which has made the buildings smarter, reliable, and efficient. As occupants in a building are the main source of continuous heat and $CO_2$ generation, estimating the accurate number of people in real-time using building IoT (BIoT) system facilities is essential for optimal energy consumption and occupants' comfort. However, the incorporation of less secured IoT sensor nodes and open communication network in the building control system eventually increases the number of vulnerable points to be compromised. Exploiting these vulnerabilities, attackers can manipulate the controller with false sensor measurements and disrupt the system's consistency. The attackers with the knowledge of overall system topology and control logics can launch attacks without alarming the system. This paper proposes a building internet of things analyzer (BIoTA) framework\footnote{https://github.com/imtiazulhaque/research-implementations/tree/main/biota} that assesses the smart building HVAC control system's security using formal attack modeling. We evaluate the proposed attack analyzer's effectiveness on the commercial occupancy dataset (COD) and the KTH live-in lab dataset. To the best of our knowledge, this is the first research attempt to formally model a BIoT-based HVAC control system and perform an attack analysis.

</details>

<details>

<summary>2021-07-24 13:16:30 - Combining Graph Neural Networks with Expert Knowledge for Smart Contract Vulnerability Detection</summary>

- *Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin Qiu, Xun Wang*

- `2107.11598v1` - [abs](http://arxiv.org/abs/2107.11598v1) - [pdf](http://arxiv.org/pdf/2107.11598v1)

> Smart contract vulnerability detection draws extensive attention in recent years due to the substantial losses caused by hacker attacks. Existing efforts for contract security analysis heavily rely on rigid rules defined by experts, which are labor-intensive and non-scalable. More importantly, expert-defined rules tend to be error-prone and suffer the inherent risk of being cheated by crafty attackers. Recent researches focus on the symbolic execution and formal analysis of smart contracts for vulnerability detection, yet to achieve a precise and scalable solution. Although several methods have been proposed to detect vulnerabilities in smart contracts, there is still a lack of effort that considers combining expert-defined security patterns with deep neural networks. In this paper, we explore using graph neural networks and expert knowledge for smart contract vulnerability detection. Specifically, we cast the rich control- and data- flow semantics of the source code into a contract graph. To highlight the critical nodes in the graph, we further design a node elimination phase to normalize the graph. Then, we propose a novel temporal message propagation network to extract the graph feature from the normalized graph, and combine the graph feature with designed expert patterns to yield a final detection system. Extensive experiments are conducted on all the smart contracts that have source code in Ethereum and VNT Chain platforms. Empirical results show significant accuracy improvements over the state-of-the-art methods on three types of vulnerabilities, where the detection accuracy of our method reaches 89.15%, 89.02%, and 83.21% for reentrancy, timestamp dependence, and infinite loop vulnerabilities, respectively.

</details>

<details>

<summary>2021-07-25 14:19:29 - Reinforced Imitation Learning by Free Energy Principle</summary>

- *Ryoya Ogishima, Izumi Karino, Yasuo Kuniyoshi*

- `2107.11811v1` - [abs](http://arxiv.org/abs/2107.11811v1) - [pdf](http://arxiv.org/pdf/2107.11811v1)

> Reinforcement Learning (RL) requires a large amount of exploration especially in sparse-reward settings. Imitation Learning (IL) can learn from expert demonstrations without exploration, but it never exceeds the expert's performance and is also vulnerable to distributional shift between demonstration and execution. In this paper, we radically unify RL and IL based on Free Energy Principle (FEP). FEP is a unified Bayesian theory of the brain that explains perception, action and model learning by a common fundamental principle. We present a theoretical extension of FEP and derive an algorithm in which an agent learns the world model that internalizes expert demonstrations and at the same time uses the model to infer the current and future states and actions that maximize rewards. The algorithm thus reduces exploration costs by partially imitating experts as well as maximizing its return in a seamless way, resulting in a higher performance than the suboptimal expert. Our experimental results show that this approach is promising in visual control tasks especially in sparse-reward environments.

</details>

<details>

<summary>2021-07-26 00:58:45 - BASAR:Black-box Attack on Skeletal Action Recognition</summary>

- *Yunfeng Diao, Tianjia Shao, Yong-Liang Yang, Kun Zhou, He Wang*

- `2103.05266v6` - [abs](http://arxiv.org/abs/2103.05266v6) - [pdf](http://arxiv.org/pdf/2103.05266v6)

> Skeletal motion plays a vital role in human activity recognition as either an independent data source or a complement. The robustness of skeleton-based activity recognizers has been questioned recently, which shows that they are vulnerable to adversarial attacks when the full-knowledge of the recognizer is accessible to the attacker. However, this white-box requirement is overly restrictive in most scenarios and the attack is not truly threatening. In this paper, we show that such threats do exist under black-box settings too. To this end, we propose the first black-box adversarial attack method BASAR. Through BASAR, we show that adversarial attack is not only truly a threat but also can be extremely deceitful, because on-manifold adversarial samples are rather common in skeletal motions, in contrast to the common belief that adversarial samples only exist off-manifold. Through exhaustive evaluation and comparison, we show that BASAR can deliver successful attacks across models, data, and attack modes. Through harsh perceptual studies, we show that it achieves effective yet imperceptible attacks. By analyzing the attack on different activity recognizers, BASAR helps identify the potential causes of their vulnerability and provides insights on what classifiers are likely to be more robust against attack. Code is available at https://github.com/realcrane/BASAR-Black-box-Attack-on-Skeletal-Action-Recognition.

</details>

<details>

<summary>2021-07-26 20:58:57 - Adversarial Attacks with Time-Scale Representations</summary>

- *Alberto Santamaria-Pang, Jianwei Qiu, Aritra Chowdhury, James Kubricht, Peter Tu, Iyer Naresh, Nurali Virani*

- `2107.12473v1` - [abs](http://arxiv.org/abs/2107.12473v1) - [pdf](http://arxiv.org/pdf/2107.12473v1)

> We propose a novel framework for real-time black-box universal attacks which disrupts activations of early convolutional layers in deep learning models. Our hypothesis is that perturbations produced in the wavelet space disrupt early convolutional layers more effectively than perturbations performed in the time domain. The main challenge in adversarial attacks is to preserve low frequency image content while minimally changing the most meaningful high frequency content. To address this, we formulate an optimization problem using time-scale (wavelet) representations as a dual space in three steps. First, we project original images into orthonormal sub-spaces for low and high scales via wavelet coefficients. Second, we perturb wavelet coefficients for high scale projection using a generator network. Third, we generate new adversarial images by projecting back the original coefficients from the low scale and the perturbed coefficients from the high scale sub-space. We provide a theoretical framework that guarantees a dual mapping from time and time-scale domain representations. We compare our results with state-of-the-art black-box attacks from generative-based and gradient-based models. We also verify efficacy against multiple defense methods such as JPEG compression, Guided Denoiser and Comdefend. Our results show that wavelet-based perturbations consistently outperform time-based attacks thus providing new insights into vulnerabilities of deep learning models and could potentially lead to robust architectures or new defense and attack mechanisms by leveraging time-scale representations.

</details>

<details>

<summary>2021-07-27 04:38:46 - Detection of cybersecurity attacks through analysis of web browsing activities using principal component analysis</summary>

- *Insha Ullah, Kerrie Mengersen, Rob J Hyndman, James McGree*

- `2107.12592v1` - [abs](http://arxiv.org/abs/2107.12592v1) - [pdf](http://arxiv.org/pdf/2107.12592v1)

> Organizations such as government departments and financial institutions provide online service facilities accessible via an increasing number of internet connected devices which make their operational environment vulnerable to cyber attacks. Consequently, there is a need to have mechanisms in place to detect cyber security attacks in a timely manner. A variety of Network Intrusion Detection Systems (NIDS) have been proposed and can be categorized into signature-based NIDS and anomaly-based NIDS. The signature-based NIDS, which identify the misuse through scanning the activity signature against the list of known attack activities, are criticized for their inability to identify new attacks (never-before-seen attacks). Among anomaly-based NIDS, which declare a connection anomalous if it expresses deviation from a trained model, the unsupervised learning algorithms circumvent this issue since they have the ability to identify new attacks. In this study, we use an unsupervised learning algorithm based on principal component analysis to detect cyber attacks. In the training phase, our approach has the advantage of also identifying outliers in the training dataset. In the monitoring phase, our approach first identifies the affected dimensions and then calculates an anomaly score by aggregating across only those components that are affected by the anomalies. We explore the performance of the algorithm via simulations and through two applications, namely to the UNSW-NB15 dataset recently released by the Australian Centre for Cyber Security and to the well-known KDD'99 dataset. The algorithm is scalable to large datasets in both training and monitoring phases, and the results from both the simulated and real datasets show that the method has promise in detecting suspicious network activities.

</details>

<details>

<summary>2021-07-27 12:18:54 - Automatic Firmware Emulation through Invalidity-guided Knowledge Inference (Extended Version)</summary>

- *Wei Zhou, Le Guan, Peng Liu, Yuqing Zhang*

- `2107.07759v2` - [abs](http://arxiv.org/abs/2107.07759v2) - [pdf](http://arxiv.org/pdf/2107.07759v2)

> Emulating firmware for microcontrollers is challenging due to the tight coupling between the hardware and firmware. This has greatly impeded the application of dynamic analysis tools to firmware analysis. The state-of-the-art work automatically models unknown peripherals by observing their access patterns, and then leverages heuristics to calculate the appropriate responses when unknown peripheral registers are accessed. However, we empirically found that this approach and the corresponding heuristics are frequently insufficient to emulate firmware. In this work, we propose a new approach called uEmu to emulate firmware with unknown peripherals. Unlike existing work that attempts to build a general model for each peripheral, our approach learns how to correctly emulate firmware execution at individual peripheral access points. It takes the image as input and symbolically executes it by representing unknown peripheral registers as symbols. During symbolic execution, it infers the rules to respond to unknown peripheral accesses. These rules are stored in a knowledge base, which is referred to during the dynamic firmware analysis. uEmu achieved a passing rate of 95% in a set of unit tests for peripheral drivers without any manual assistance. We also evaluated uEmu with real-world firmware samples and new bugs were discovered.

</details>

<details>

<summary>2021-07-27 13:10:49 - Cryptanalysis of Semidirect Product Key Exchange Using Matrices Over Non-Commutative Rings</summary>

- *Christopher Battarbee, Delaram Kahrobaei, Siamak F. Shahandashti*

- `2105.07692v2` - [abs](http://arxiv.org/abs/2105.07692v2) - [pdf](http://arxiv.org/pdf/2105.07692v2)

> It was recently demonstrated that the Matrix Action Key Exchange (MAKE) algorithm, a new type of key exchange protocol using the semidirect product of matrix groups, is vulnerable to a linear algebraic attack if the matrices are over a commutative ring. In this note, we establish conditions under which protocols using matrices over a non-commutative ring are also vulnerable to this attack. We then demonstrate that group rings $R[G]$ are examples of non-commutative rings that satisfy these conditions.

</details>

<details>

<summary>2021-07-27 13:26:23 - Improving the Authentication with Built-in Camera Protocol Using Built-in Motion Sensors: A Deep Learning Solution</summary>

- *Cezara Benegui, Radu Tudor Ionescu*

- `2107.10536v3` - [abs](http://arxiv.org/abs/2107.10536v3) - [pdf](http://arxiv.org/pdf/2107.10536v3)

> We propose an enhanced version of the Authentication with Built-in Camera (ABC) protocol by employing a deep learning solution based on built-in motion sensors. The standard ABC protocol identifies mobile devices based on the photo-response non-uniformity (PRNU) of the camera sensor, while also considering QR-code-based meta-information. During authentication, the user is required to take two photos that contain two QR codes presented on a screen. The presented QR code images also contain a unique probe signal, similar to a camera fingerprint, generated by the protocol. During verification, the server computes the fingerprint of the received photos and authenticates the user if (i) the probe signal is present, (ii) the metadata embedded in the QR codes is correct and (iii) the camera fingerprint is identified correctly. However, the protocol is vulnerable to forgery attacks when the attacker can compute the camera fingerprint from external photos, as shown in our preliminary work. In this context, we propose an enhancement for the ABC protocol based on motion sensor data, as an additional and passive authentication layer. Smartphones can be identified through their motion sensor data, which, unlike photos, is never posted by users on social media platforms, thus being more secure than using photographs alone. To this end, we transform motion signals into embedding vectors produced by deep neural networks, applying Support Vector Machines for the smartphone identification task. Our change to the ABC protocol results in a multi-modal protocol that lowers the false acceptance rate for the attack proposed in our previous work to a percentage as low as 0.07%.

</details>

<details>

<summary>2021-07-27 14:03:55 - Development of a NIC driver in C#</summary>

- *Samuel Chassot*

- `2107.12833v1` - [abs](http://arxiv.org/abs/2107.12833v1) - [pdf](http://arxiv.org/pdf/2107.12833v1)

> Drivers have a special status among the developer community that sees them as mysterious and inaccessible. We think their extensive communication with the hardware and their need of high performance are the cause of this bad reputation. According to a widely held view, these two requirements cannot be met using high level languages. However high level languages' compilers and runtimes made great progress these past years to enhance the performance of programs. The use of these languages can also reduce by a significant amount the number of bugs and security issues introduced by the programmers by taking care of some error-prone parts like memory allocation and accesses. We also think that using high level languages can help to demystify the drivers' development. With this project, we try to develop a driver for a network card, the Intel 82599, in C\#. Our goal is to find out the feasibility of such a development and the performance of such a driver. We will also be able to tell what could be missing today in C\# to write a driver. We base our driver on the model proposed by Pirelli (2020) and its implementation in C.

</details>

<details>

<summary>2021-07-27 15:15:20 - PDF-Malware: An Overview on Threats, Detection and Evasion Attacks</summary>

- *Nicolas Fleury, Theo Dubrunquez, Ihsen Alouani*

- `2107.12873v1` - [abs](http://arxiv.org/abs/2107.12873v1) - [pdf](http://arxiv.org/pdf/2107.12873v1)

> In the recent years, Portable Document Format, commonly known as PDF, has become a democratized standard for document exchange and dissemination. This trend has been due to its characteristics such as its flexibility and portability across platforms. The widespread use of PDF has installed a false impression of inherent safety among benign users. However, the characteristics of PDF motivated hackers to exploit various types of vulnerabilities, overcome security safeguards, thereby making the PDF format one of the most efficient malicious code attack vectors. Therefore, efficiently detecting malicious PDF files is crucial for information security. Several analysis techniques has been proposed in the literature, be it static or dynamic, to extract the main features that allow the discrimination of malware files from benign ones. Since classical analysis techniques may be limited in case of zero-days, machine-learning based techniques have emerged recently as an automatic PDF-malware detection method that is able to generalize from a set of training samples. These techniques are themselves facing the challenge of evasion attacks where a malicious PDF is transformed to look benign. In this work, we give an overview on the PDF-malware detection problem. We give a perspective on the new challenges and emerging solutions.

</details>

<details>

<summary>2021-07-28 00:52:04 - T-RECS: A Simulation Tool to Study the Societal Impact of Recommender Systems</summary>

- *Eli Lucherini, Matthew Sun, Amy Winecoff, Arvind Narayanan*

- `2107.08959v2` - [abs](http://arxiv.org/abs/2107.08959v2) - [pdf](http://arxiv.org/pdf/2107.08959v2)

> Simulation has emerged as a popular method to study the long-term societal consequences of recommender systems. This approach allows researchers to specify their theoretical model explicitly and observe the evolution of system-level outcomes over time. However, performing simulation-based studies often requires researchers to build their own simulation environments from the ground up, which creates a high barrier to entry, introduces room for implementation error, and makes it difficult to disentangle whether observed outcomes are due to the model or the implementation.   We introduce T-RECS, an open-sourced Python package designed for researchers to simulate recommendation systems and other types of sociotechnical systems in which an algorithm mediates the interactions between multiple stakeholders, such as users and content creators. To demonstrate the flexibility of T-RECS, we perform a replication of two prior simulation-based research on sociotechnical systems. We additionally show how T-RECS can be used to generate novel insights with minimal overhead. Our tool promotes reproducibility in this area of research, provides a unified language for simulating sociotechnical systems, and removes the friction of implementing simulations from scratch.

</details>

<details>

<summary>2021-07-28 06:43:36 - TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing</summary>

- *Aoting Hu, Renjie Xie, Zhigang Lu, Aiqun Hu, Minhui Xue*

- `2107.13190v1` - [abs](http://arxiv.org/abs/2107.13190v1) - [pdf](http://arxiv.org/pdf/2107.13190v1)

> Generative Adversarial Networks (GAN)-synthesized table publishing lets people privately learn insights without access to the private table. However, existing studies on Membership Inference (MI) Attacks show promising results on disclosing membership of training datasets of GAN-synthesized tables. Different from those works focusing on discovering membership of a given data point, in this paper, we propose a novel Membership Collision Attack against GANs (TableGAN-MCA), which allows an adversary given only synthetic entries randomly sampled from a black-box generator to recover partial GAN training data. Namely, a GAN-synthesized table immune to state-of-the-art MI attacks is vulnerable to the TableGAN-MCA. The success of TableGAN-MCA is boosted by an observation that GAN-synthesized tables potentially collide with the training data of the generator.   Our experimental evaluations on TableGAN-MCA have five main findings. First, TableGAN-MCA has a satisfying training data recovery rate on three commonly used real-world datasets against four generative models. Second, factors, including the size of GAN training data, GAN training epochs and the number of synthetic samples available to the adversary, are positively correlated to the success of TableGAN-MCA. Third, highly frequent data points have high risks of being recovered by TableGAN-MCA. Fourth, some unique data are exposed to unexpected high recovery risks in TableGAN-MCA, which may attribute to GAN's generalization. Fifth, as expected, differential privacy, without the consideration of the correlations between features, does not show commendable mitigation effect against the TableGAN-MCA. Finally, we propose two mitigation methods and show promising privacy and utility trade-offs when protecting against TableGAN-MCA.

</details>

<details>

<summary>2021-07-28 09:54:46 - Structack: Structure-based Adversarial Attacks on Graph Neural Networks</summary>

- *Hussain Hussain, Tomislav Duricic, Elisabeth Lex, Denis Helic, Markus Strohmaier, Roman Kern*

- `2107.11327v2` - [abs](http://arxiv.org/abs/2107.11327v2) - [pdf](http://arxiv.org/pdf/2107.11327v2)

> Recent work has shown that graph neural networks (GNNs) are vulnerable to adversarial attacks on graph data. Common attack approaches are typically informed, i.e. they have access to information about node attributes such as labels and feature vectors. In this work, we study adversarial attacks that are uninformed, where an attacker only has access to the graph structure, but no information about node attributes. Here the attacker aims to exploit structural knowledge and assumptions, which GNN models make about graph data. In particular, literature has shown that structural node centrality and similarity have a strong influence on learning with GNNs. Therefore, we study the impact of centrality and similarity on adversarial attacks on GNNs. We demonstrate that attackers can exploit this information to decrease the performance of GNNs by focusing on injecting links between nodes of low similarity and, surprisingly, low centrality. We show that structure-based uninformed attacks can approach the performance of informed attacks, while being computationally more efficient. With our paper, we present a new attack strategy on GNNs that we refer to as Structack. Structack can successfully manipulate the performance of GNNs with very limited information while operating under tight computational constraints. Our work contributes towards building more robust machine learning approaches on graphs.

</details>

<details>

<summary>2021-07-28 19:44:27 - Clones in Deep Learning Code: What, Where, and Why?</summary>

- *Hadhemi Jebnoun, Md Saidur Rahman, Foutse Khomh, Biruk Asmare Muse*

- `2107.13614v1` - [abs](http://arxiv.org/abs/2107.13614v1) - [pdf](http://arxiv.org/pdf/2107.13614v1)

> Deep Learning applications are becoming increasingly popular. Developers of deep learning systems strive to write more efficient code. Deep learning systems are constantly evolving, imposing tighter development timelines and increasing complexity, which may lead to bad design decisions. A copy-paste approach is widely used among deep learning developers because they rely on common frameworks and duplicate similar tasks. Developers often fail to properly propagate changes to all clones fragments during a maintenance activity. To our knowledge, no study has examined code cloning practices in deep learning development. Given the negative impacts of clones on software quality reported in the studies on traditional systems, it is very important to understand the characteristics and potential impacts of code clones on deep learning systems. To this end, we use the NiCad tool to detect clones from 59 Python, 14 C# and 6 Java-based deep learning systems and an equal number of traditional software systems. We then analyze the frequency and distribution of code clones in deep learning and traditional systems. We do further analysis of the distribution of code clones using location-based taxonomy. We also study the correlation between bugs and code clones to assess the impacts of clones on the quality of the studied systems. Finally, we introduce a code clone taxonomy related to deep learning programs and identify the deep learning system development phases in which cloning has the highest risk of faults. Our results show that code cloning is a frequent practice in deep learning systems and that deep learning developers often clone code from files in distant repositories in the system. In addition, we found that code cloning occurs more frequently during DL model construction. And that hyperparameters setting is the phase during which cloning is the riskiest, since it often leads to faults.

</details>

<details>

<summary>2021-07-29 08:52:10 - Defensive Approximation: Securing CNNs using Approximate Computing</summary>

- *Amira Guesmi, Ihsen Alouani, Khaled Khasawneh, Mouna Baklouti, Tarek Frikha, Mohamed Abid, Nael Abu-Ghazaleh*

- `2006.07700v3` - [abs](http://arxiv.org/abs/2006.07700v3) - [pdf](http://arxiv.org/pdf/2006.07700v3)

> In the past few years, an increasing number of machine-learning and deep learning structures, such as Convolutional Neural Networks (CNNs), have been applied to solving a wide range of real-life problems. However, these architectures are vulnerable to adversarial attacks. In this paper, we propose for the first time to use hardware-supported approximate computing to improve the robustness of machine learning classifiers. We show that our approximate computing implementation achieves robustness across a wide range of attack scenarios. Specifically, for black-box and grey-box attack scenarios, we show that successful adversarial attacks against the exact classifier have poor transferability to the approximate implementation. Surprisingly, the robustness advantages also apply to white-box attacks where the attacker has access to the internal implementation of the approximate classifier. We explain some of the possible reasons for this robustness through analysis of the internal operation of the approximate implementation. Furthermore, our approximate computing model maintains the same level in terms of classification accuracy, does not require retraining, and reduces resource utilization and energy consumption of the CNN. We conducted extensive experiments on a set of strong adversarial attacks; We empirically show that the proposed implementation increases the robustness of a LeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong grey-box adversarial attacks along with up to 67% saving in energy consumption due to the simpler nature of the approximate logic. We also show that a white-box attack requires a remarkably higher noise budget to fool the approximate classifier, causing an average of 4db degradation of the PSNR of the input image relative to the images that succeed in fooling the exact classifier

</details>

<details>

<summary>2021-07-29 10:22:20 - Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality</summary>

- *Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Felice Antonio Merra*

- `2107.13876v1` - [abs](http://arxiv.org/abs/2107.13876v1) - [pdf](http://arxiv.org/pdf/2107.13876v1)

> Recommender systems (RSs) employ user-item feedback, e.g., ratings, to match customers to personalized lists of products. Approaches to top-k recommendation mainly rely on Learning-To-Rank algorithms and, among them, the most widely adopted is Bayesian Personalized Ranking (BPR), which bases on a pair-wise optimization approach. Recently, BPR has been found vulnerable against adversarial perturbations of its model parameters. Adversarial Personalized Ranking (APR) mitigates this issue by robustifying BPR via an adversarial training procedure. The empirical improvements of APR's accuracy performance on BPR have led to its wide use in several recommender models. However, a key overlooked aspect has been the beyond-accuracy performance of APR, i.e., novelty, coverage, and amplification of popularity bias, considering that recent results suggest that BPR, the building block of APR, is sensitive to the intensification of biases and reduction of recommendation novelty. In this work, we model the learning characteristics of the BPR and APR optimization frameworks to give mathematical evidence that, when the feedback data have a tailed distribution, APR amplifies the popularity bias more than BPR due to an unbalanced number of received positive updates from short-head items. Using matrix factorization (MF), we empirically validate the theoretical results by performing preliminary experiments on two public datasets to compare BPR-MF and APR-MF performance on accuracy and beyond-accuracy metrics. The experimental results consistently show the degradation of novelty and coverage measures and a worrying amplification of bias.

</details>

<details>

<summary>2021-07-29 11:26:08 - Developers perception on the severity of test smells: an empirical study</summary>

- *Denivan Campos, Larissa Rocha, Ivan Machado*

- `2107.13902v1` - [abs](http://arxiv.org/abs/2107.13902v1) - [pdf](http://arxiv.org/pdf/2107.13902v1)

> Unit testing is an essential component of the software development life-cycle. A developer could easily and quickly catch and fix software faults introduced in the source code by creating and running unit tests. Despite their importance, unit tests are subject to bad design or implementation decisions, the so-called test smells. These might decrease software systems quality from various aspects, making it harder to understand, more complex to maintain, and more prone to errors and bugs. Many studies discuss the likely effects of test smells on test code. However, there is a lack of studies that capture developers perceptions of such issues. This study empirically analyzes how developers perceive the severity of test smells in the test code they develop. Severity refers to the degree to how a test smell may negatively impact the test code. We selected six open-source software projects from GitHub and interviewed their developers to understand whether and how the test smells affected the test code. Although most of the interviewed developers considered the test smells as having a low severity to their code, they indicated that test smells might negatively impact the project, particularly in test code maintainability and evolution. Also, detecting and removing test smells from the test code may be positive for the project.

</details>

<details>

<summary>2021-07-29 12:46:51 - Firmware Re-hosting Through Static Binary-level Porting</summary>

- *Mingfeng Xin, Hui Wen, Liting Deng, Hong Li, Qiang Li, Limin Sun*

- `2107.09856v2` - [abs](http://arxiv.org/abs/2107.09856v2) - [pdf](http://arxiv.org/pdf/2107.09856v2)

> The rapid growth of the Industrial Internet of Things (IIoT) has brought embedded systems into focus as major targets for both security analysts and malicious adversaries. Due to the non-standard hardware and diverse software, embedded devices present unique challenges to security analysts for the accurate analysis of firmware binaries. The diversity in hardware components and tight coupling between firmware and hardware makes it hard to perform dynamic analysis, which must have the ability to execute firmware code in virtualized environments. However, emulating the large expanse of hardware peripherals makes analysts have to frequently modify the emulator for executing various firmware code in different virtualized environments, greatly limiting the ability of security analysis.   In this work, we explore the problem of firmware re-hosting related to the real-time operating system (RTOS). Specifically, developers create a Board Support Package (BSP) and develop device drivers to make that RTOS run on their platform. By providing high-level replacements for BSP routines and device drivers, we can make the minimal modification of the firmware that is to be migrated from its original hardware environment into a virtualized one. We show that an approach capable of offering the ability to execute firmware at scale through patching firmware in an automated manner without modifying the existing emulators. Our approach, called static binary-level porting, first identifies the BSP and device drivers in target firmware, then patches the firmware with pre-built BSP routines and drivers that can be adapted to the existing emulators. Finally, we demonstrate the practicality of the proposed method on multiple hardware platforms and firmware samples for security analysis. The result shows that the approach is flexible enough to emulate firmware for vulnerability assessment and exploits development.

</details>

<details>

<summary>2021-07-30 09:58:19 - Blockchain-Enabled End-to-End Encryption for Instant Messaging Applications</summary>

- *Raman Singh, Ark Nandan Singh Chauhan, Hitesh Tewari*

- `2104.08494v2` - [abs](http://arxiv.org/abs/2104.08494v2) - [pdf](http://arxiv.org/pdf/2104.08494v2)

> In the era of social media and messaging applications, people are becoming increasingly aware of data privacy issues associated with such apps. Major messaging applications are moving towards end-to-end encryption (E2EE) to give their users the privacy they are demanding. However the current security mechanisms employed by different service providers are not unfeigned E2EE implementations, and are blended with many vulnerabilities. In the present scenario, the major part of the E2EE mechanism is controlled by the service provider's servers, and the decryption keys are stored by them in case of backup restoration. These shortcomings diminish the user's confidence in the privacy of their data while using these apps. A public Key infrastructure (PKI) mechanism can be used to circumvent some of these issues, but it comes with high monetary costs, which makes it impossible to roll out for millions of users. The paper proposes a blockchain-based E2EE framework that can mitigate the contemporary vulnerabilities in messaging applications. The user's device generates the public/private key pair during application installation, and asks its mobile network operator (MNO) to issue a digital certificate and store it on the blockchain. A user can fetch a certificate for another user from the chat server and communicate securely with them using a ratchet forward encryption mechanism.

</details>

<details>

<summary>2021-07-30 12:54:46 - Who's Afraid of Thomas Bayes?</summary>

- *Erick Galinkin*

- `2107.14601v1` - [abs](http://arxiv.org/abs/2107.14601v1) - [pdf](http://arxiv.org/pdf/2107.14601v1)

> In many cases, neural networks perform well on test data, but tend to overestimate their confidence on out-of-distribution data. This has led to adoption of Bayesian neural networks, which better capture uncertainty and therefore more accurately reflect the model's confidence. For machine learning security researchers, this raises the natural question of how making a model Bayesian affects the security of the model. In this work, we explore the interplay between Bayesianism and two measures of security: model privacy and adversarial robustness. We demonstrate that Bayesian neural networks are more vulnerable to membership inference attacks in general, but are at least as robust as their non-Bayesian counterparts to adversarial examples.

</details>

<details>

<summary>2021-07-30 14:07:49 - Practical Attacks on Voice Spoofing Countermeasures</summary>

- *Andre Kassis, Urs Hengartner*

- `2107.14642v1` - [abs](http://arxiv.org/abs/2107.14642v1) - [pdf](http://arxiv.org/pdf/2107.14642v1)

> Voice authentication has become an integral part in security-critical operations, such as bank transactions and call center conversations. The vulnerability of automatic speaker verification systems (ASVs) to spoofing attacks instigated the development of countermeasures (CMs), whose task is to tell apart bonafide and spoofed speech. Together, ASVs and CMs form today's voice authentication platforms, advertised as an impregnable access control mechanism. We develop the first practical attack on CMs, and show how a malicious actor may efficiently craft audio samples to bypass voice authentication in its strictest form. Previous works have primarily focused on non-proactive attacks or adversarial strategies against ASVs that do not produce speech in the victim's voice. The repercussions of our attacks are far more severe, as the samples we generate sound like the victim, eliminating any chance of plausible deniability. Moreover, the few existing adversarial attacks against CMs mistakenly optimize spoofed speech in the feature space and do not take into account the existence of ASVs, resulting in inferior synthetic audio that fails in realistic settings. We eliminate these obstacles through our key technical contribution: a novel joint loss function that enables mounting advanced adversarial attacks against combined ASV/CM deployments directly in the time domain. Our adversarials achieve concerning black-box success rates against state-of-the-art authentication platforms (up to 93.57\%). Finally, we perform the first targeted, over-telephony-network attack on CMs, bypassing several challenges and enabling various potential threats, given the increased use of voice biometrics in call centers. Our results call into question the security of modern voice authentication systems in light of the real threat of attackers bypassing these measures to gain access to users' most valuable resources.

</details>

<details>

<summary>2021-07-30 15:17:00 - Set It and Forget It! Turnkey ECC for Instant Integration</summary>

- *Dmitry Belyavsky, Billy Bob Brumley, Jesús-Javier Chi-Domínguez, Luis Rivera-Zamarripa, Igor Ustinov*

- `2007.11481v2` - [abs](http://arxiv.org/abs/2007.11481v2) - [pdf](http://arxiv.org/pdf/2007.11481v2)

> Historically, Elliptic Curve Cryptography (ECC) is an active field of applied cryptography where recent focus is on high speed, constant time, and formally verified implementations. While there are a handful of outliers where all these concepts join and land in real-world deployments, these are generally on a case-by-case basis: e.g. a library may feature such X25519 or P-256 code, but not for all curves. In this work, we propose and implement a methodology that fully automates the implementation, testing, and integration of ECC stacks with the above properties. We demonstrate the flexibility and applicability of our methodology by seamlessly integrating into three real-world projects: OpenSSL, Mozilla's NSS, and the GOST OpenSSL Engine, achieving roughly 9.5x, 4.5x, 13.3x, and 3.7x speedup on any given curve for key generation, key agreement, signing, and verifying, respectively. Furthermore, we showcase the efficacy of our testing methodology by uncovering flaws and vulnerabilities in OpenSSL, and a specification-level vulnerability in a Russian standard. Our work bridges the gap between significant applied cryptography research results and deployed software, fully automating the process.

</details>

<details>

<summary>2021-07-30 17:19:37 - Debiased Explainable Pairwise Ranking from Implicit Feedback</summary>

- *Khalil Damak, Sami Khenissi, Olfa Nasraoui*

- `2107.14768v1` - [abs](http://arxiv.org/abs/2107.14768v1) - [pdf](http://arxiv.org/pdf/2107.14768v1)

> Recent work in recommender systems has emphasized the importance of fairness, with a particular interest in bias and transparency, in addition to predictive accuracy. In this paper, we focus on the state of the art pairwise ranking model, Bayesian Personalized Ranking (BPR), which has previously been found to outperform pointwise models in predictive accuracy, while also being able to handle implicit feedback. Specifically, we address two limitations of BPR: (1) BPR is a black box model that does not explain its outputs, thus limiting the user's trust in the recommendations, and the analyst's ability to scrutinize a model's outputs; and (2) BPR is vulnerable to exposure bias due to the data being Missing Not At Random (MNAR). This exposure bias usually translates into an unfairness against the least popular items because they risk being under-exposed by the recommender system. In this work, we first propose a novel explainable loss function and a corresponding Matrix Factorization-based model called Explainable Bayesian Personalized Ranking (EBPR) that generates recommendations along with item-based explanations. Then, we theoretically quantify additional exposure bias resulting from the explainability, and use it as a basis to propose an unbiased estimator for the ideal EBPR loss. The result is a ranking model that aptly captures both debiased and explainable user preferences. Finally, we perform an empirical study on three real-world datasets that demonstrate the advantages of our proposed models.

</details>

<details>

<summary>2021-07-31 14:18:20 - Privacy Enhancement via Dummy Points in the Shuffle Model</summary>

- *Xiaochen Li, Weiran Liu, Hanwen Feng, Kunzhe Huang, Jinfei Liu, Kui Ren, Zhan Qin*

- `2009.13738v2` - [abs](http://arxiv.org/abs/2009.13738v2) - [pdf](http://arxiv.org/pdf/2009.13738v2)

> The shuffle model is recently proposed to address the issue of severe utility loss in Local Differential Privacy (LDP) due to distributed data randomization.In the shuffle model, a shuffler is utilized to break the link between the user identity and the message uploaded to the data analyst. Since less noise needs to be introduced to achieve the same privacy guarantee, following this paradigm, the utility of privacy-preserving data collection is improved.   We propose DUMP (\underline{DUM}my-\underline{P}oint-based), a framework for privacy-preserving histogram estimation in the shuffle model. The core of DUMP is a new concept of \emph{dummy blanket}, which enables enhancing privacy by just introducing \textit{points }on the user side and further improving the utility of the shuffle model.We instantiate DUMP by proposing two protocols: pureDUMP and mixDUMP, and conduct a comprehensive experimental evaluation to compare them with existing protocols. The experimental results show that, under the same privacy guarantee, (1) the proposed protocols have significant improvements in communication efficiency over all existing multi-message protocols, by at least 3 orders of magnitude; (2) they achieve competitive utility, while the only known protocol (Ghazi \textit{et al.}, PMLR 2020) having better utility than ours employs hard-to-exactly-sample distributions which are vulnerable to floating-point attacks (CCS 2012).

</details>


## 2021-08

<details>

<summary>2021-08-01 15:03:23 - Remote quantum-safe authentication of entities with physical unclonable functions</summary>

- *Georgios M. Nikolopoulos*

- `2108.00468v1` - [abs](http://arxiv.org/abs/2108.00468v1) - [pdf](http://arxiv.org/pdf/2108.00468v1)

> Physical unclonable functions have been shown a useful resource of randomness for implementing various cryptographic tasks including entity authentication. All of the related entity authentication protocols that have been discussed in the literature so far, either they are vulnerable to an emulation attack, or they are limited to short distances. Hence, quantum-safe remote entity authentication over large distances remains an open question. In the first part of this work we discuss the requirements that an entity authentication protocol has to offer in order to be useful for remote entity authentication in practice. Subsequently, we propose a protocol, which can operate over large distances, and offers security against both classical and quantum adversaries. The proposed protocol relies on standard techniques, it is fully compatible with the infrastructure of existing and future photonic networks, and it can operate in parallel with other quantum protocols, including QKD protocols.

</details>

<details>

<summary>2021-08-02 06:51:13 - Transfer Learning for Mining Feature Requests and Bug Reports from Tweets and App Store Reviews</summary>

- *Pablo Restrepo Henao, Jannik Fischbach, Dominik Spies, Julian Frattini, Andreas Vogelsang*

- `2108.00663v1` - [abs](http://arxiv.org/abs/2108.00663v1) - [pdf](http://arxiv.org/pdf/2108.00663v1)

> Identifying feature requests and bug reports in user comments holds great potential for development teams. However, automated mining of RE-related information from social media and app stores is challenging since (1) about 70% of user comments contain noisy, irrelevant information, (2) the amount of user comments grows daily making manual analysis unfeasible, and (3) user comments are written in different languages. Existing approaches build on traditional machine learning (ML) and deep learning (DL), but fail to detect feature requests and bug reports with high Recall and acceptable Precision which is necessary for this task. In this paper, we investigate the potential of transfer learning (TL) for the classification of user comments. Specifically, we train both monolingual and multilingual BERT models and compare the performance with state-of-the-art methods. We found that monolingual BERT models outperform existing baseline methods in the classification of English App Reviews as well as English and Italian Tweets. However, we also observed that the application of heavyweight TL models does not necessarily lead to better performance. In fact, our multilingual BERT models perform worse than traditional ML methods.

</details>

<details>

<summary>2021-08-02 17:32:06 - GPU Accelerated Exhaustive Search for Optimal Ensemble of Black-Box Optimization Algorithms</summary>

- *Jiwei Liu, Bojan Tunguz, Gilberto Titericz*

- `2012.04201v3` - [abs](http://arxiv.org/abs/2012.04201v3) - [pdf](http://arxiv.org/pdf/2012.04201v3)

> Black-box optimization is essential for tuning complex machine learning algorithms which are easier to experiment with than to understand. In this paper, we show that a simple ensemble of black-box optimization algorithms can outperform any single one of them. However, searching for such an optimal ensemble requires a large number of experiments. We propose a Multi-GPU-optimized framework to accelerate a brute force search for the optimal ensemble of black-box optimization algorithms by running many experiments in parallel. The lightweight optimizations are performed by CPU while expensive model training and evaluations are assigned to GPUs. We evaluate 15 optimizers by training 2.7 million models and running 541,440 optimizations. On a DGX-1, the search time is reduced from more than 10 days on two 20-core CPUs to less than 24 hours on 8-GPUs. With the optimal ensemble found by GPU-accelerated exhaustive search, we won the 2nd place of NeurIPS 2020 black-box optimization challenge.

</details>

<details>

<summary>2021-08-02 18:50:12 - Efficacy of Statistical and Artificial Intelligence-based False Information Cyberattack Detection Models for Connected Vehicles</summary>

- *Sakib Mahmud Khan, Gurcan Comert, Mashrur Chowdhury*

- `2108.01124v1` - [abs](http://arxiv.org/abs/2108.01124v1) - [pdf](http://arxiv.org/pdf/2108.01124v1)

> Connected vehicles (CVs), because of the external connectivity with other CVs and connected infrastructure, are vulnerable to cyberattacks that can instantly compromise the safety of the vehicle itself and other connected vehicles and roadway infrastructure. One such cyberattack is the false information attack, where an external attacker injects inaccurate information into the connected vehicles and eventually can cause catastrophic consequences by compromising safety-critical applications like the forward collision warning. The occurrence and target of such attack events can be very dynamic, making real-time and near-real-time detection challenging. Change point models, can be used for real-time anomaly detection caused by the false information attack. In this paper, we have evaluated three change point-based statistical models; Expectation Maximization, Cumulative Summation, and Bayesian Online Change Point Algorithms for cyberattack detection in the CV data. Also, data-driven artificial intelligence (AI) models, which can be used to detect known and unknown underlying patterns in the dataset, have the potential of detecting a real-time anomaly in the CV data. We have used six AI models to detect false information attacks and compared the performance for detecting the attacks with our developed change point models. Our study shows that change points models performed better in real-time false information attack detection compared to the performance of the AI models. Change point models having the advantage of no training requirements can be a feasible and computationally efficient alternative to AI models for false information attack detection in connected vehicles.

</details>

<details>

<summary>2021-08-02 18:52:21 - Fast Intra-kernel Isolation and Security with IskiOS</summary>

- *Spyridoula Gravani, Mohammad Hedayati, John Criswell, Michael L. Scott*

- `1903.04654v5` - [abs](http://arxiv.org/abs/1903.04654v5) - [pdf](http://arxiv.org/pdf/1903.04654v5)

> The kernels of operating systems such as Windows, Linux, and MacOS are vulnerable to control-flow hijacking. Defenses exist, but many require efficient intra-address-space isolation. Execute-only memory, for example, requires read protection on code segments, and shadow stacks require protection from buffer overwrites. Intel's Protection Keys for Userspace (PKU) could, in principle, provide the intra-kernel isolation needed by such defenses, but, when used as designed, it applies only to user-mode application code. This paper presents an unconventional approach to memory protection, allowing PKU to be used within the operating system kernel on existing Intel hardware, replacing the traditional user/supervisor isolation mechanism and, simultaneously, enabling efficient intra-kernel isolation. We call the resulting mechanism Protection Keys for Kernelspace (PKK). To demonstrate its utility and efficiency, we present a system we call IskiOS: a Linux variant featuring execute-only memory (XOM) and the first-ever race-free shadow stacks for x86-64. Experiments with the LMBench kernel microbenchmarks display a geometric mean overhead of about 11% for PKK and no additional overhead for XOM. IskiOS's shadow stacks bring the total to 22%. For full applications, experiments with the system benchmarks of the Phoronix test suite display negligible overhead for PKK and XOM, and less than 5% geometric mean overhead for shadow stacks.

</details>

<details>

<summary>2021-08-03 16:21:08 - On the Exploitability of Audio Machine Learning Pipelines to Surreptitious Adversarial Examples</summary>

- *Adelin Travers, Lorna Licollari, Guanghan Wang, Varun Chandrasekaran, Adam Dziedzic, David Lie, Nicolas Papernot*

- `2108.02010v1` - [abs](http://arxiv.org/abs/2108.02010v1) - [pdf](http://arxiv.org/pdf/2108.02010v1)

> Machine learning (ML) models are known to be vulnerable to adversarial examples. Applications of ML to voice biometrics authentication are no exception. Yet, the implications of audio adversarial examples on these real-world systems remain poorly understood given that most research targets limited defenders who can only listen to the audio samples. Conflating detectability of an attack with human perceptibility, research has focused on methods that aim to produce imperceptible adversarial examples which humans cannot distinguish from the corresponding benign samples. We argue that this perspective is coarse for two reasons: 1. Imperceptibility is impossible to verify; it would require an experimental process that encompasses variations in listener training, equipment, volume, ear sensitivity, types of background noise etc, and 2. It disregards pipeline-based detection clues that realistic defenders leverage. This results in adversarial examples that are ineffective in the presence of knowledgeable defenders. Thus, an adversary only needs an audio sample to be plausible to a human. We thus introduce surreptitious adversarial examples, a new class of attacks that evades both human and pipeline controls. In the white-box setting, we instantiate this class with a joint, multi-stage optimization attack. Using an Amazon Mechanical Turk user study, we show that this attack produces audio samples that are more surreptitious than previous attacks that aim solely for imperceptibility. Lastly we show that surreptitious adversarial examples are challenging to develop in the black-box setting.

</details>

<details>

<summary>2021-08-03 18:29:28 - Linking Common Vulnerabilities and Exposures to the MITRE ATT&CK Framework: A Self-Distillation Approach</summary>

- *Benjamin Ampel, Sagar Samtani, Steven Ullman, Hsinchun Chen*

- `2108.01696v1` - [abs](http://arxiv.org/abs/2108.01696v1) - [pdf](http://arxiv.org/pdf/2108.01696v1)

> Due to the ever-increasing threat of cyber-attacks to critical cyber infrastructure, organizations are focusing on building their cybersecurity knowledge base. A salient list of cybersecurity knowledge is the Common Vulnerabilities and Exposures (CVE) list, which details vulnerabilities found in a wide range of software and hardware. However, these vulnerabilities often do not have a mitigation strategy to prevent an attacker from exploiting them. A well-known cybersecurity risk management framework, MITRE ATT&CK, offers mitigation techniques for many malicious tactics. Despite the tremendous benefits that both CVEs and the ATT&CK framework can provide for key cybersecurity stakeholders (e.g., analysts, educators, and managers), the two entities are currently separate. We propose a model, named the CVE Transformer (CVET), to label CVEs with one of ten MITRE ATT&CK tactics. The CVET model contains a fine-tuning and self-knowledge distillation design applied to the state-of-the-art pre-trained language model RoBERTa. Empirical results on a gold-standard dataset suggest that our proposed novelties can increase model performance in F1-score. The results of this research can allow cybersecurity stakeholders to add preliminary MITRE ATT&CK information to their collected CVEs.

</details>

<details>

<summary>2021-08-03 20:19:28 - Tutorials on Testing Neural Networks</summary>

- *Nicolas Berthier, Youcheng Sun, Wei Huang, Yanghao Zhang, Wenjie Ruan, Xiaowei Huang*

- `2108.01734v1` - [abs](http://arxiv.org/abs/2108.01734v1) - [pdf](http://arxiv.org/pdf/2108.01734v1)

> Deep learning achieves remarkable performance on pattern recognition, but can be vulnerable to defects of some important properties such as robustness and security. This tutorial is based on a stream of research conducted since the summer of 2018 at a few UK universities, including the University of Liverpool, University of Oxford, Queen's University Belfast, University of Lancaster, University of Loughborough, and University of Exeter.   The research aims to adapt software engineering methods, in particular software testing methods, to work with machine learning models. Software testing techniques have been successful in identifying software bugs, and helping software developers in validating the software they design and implement. It is for this reason that a few software testing techniques -- such as the MC/DC coverage metric -- have been mandated in industrial standards for safety critical systems, including the ISO26262 for automotive systems and the RTCA DO-178B/C for avionics systems. However, these techniques cannot be directly applied to machine learning models, because the latter are drastically different from traditional software, and their design follows a completely different development life-cycle.   As the outcome of this thread of research, the team has developed a series of methods that adapt the software testing techniques to work with a few classes of machine learning models. The latter notably include convolutional neural networks, recurrent neural networks, and random forest. The tools developed from this research are now collected, and publicly released, in a GitHub repository: \url{https://github.com/TrustAI/DeepConcolic}, with the BSD 3-Clause licence.   This tutorial is to go through the major functionalities of the tools with a few running examples, to exhibit how the developed techniques work, what the results are, and how to interpret them.

</details>

<details>

<summary>2021-08-04 05:42:11 - Towards Making Deep Learning-based Vulnerability Detectors Robust</summary>

- *Zhen Li, Jing Tang, Deqing Zou, Qian Chen, Shouhuai Xu, Chao Zhang, Yichen Li, Hai Jin*

- `2108.00669v2` - [abs](http://arxiv.org/abs/2108.00669v2) - [pdf](http://arxiv.org/pdf/2108.00669v2)

> Automatically detecting software vulnerabilities in source code is an important problem that has attracted much attention. In particular, deep learning-based vulnerability detectors, or DL-based detectors, are attractive because they do not need human experts to define features or patterns of vulnerabilities. However, such detectors' robustness is unclear. In this paper, we initiate the study in this aspect by demonstrating that DL-based detectors are not robust against simple code transformations, dubbed attacks in this paper, as these transformations may be leveraged for malicious purposes. As a first step towards making DL-based detectors robust against such attacks, we propose an innovative framework, dubbed ZigZag, which is centered at (i) decoupling feature learning and classifier learning and (ii) using a ZigZag-style strategy to iteratively refine them until they converge to robust features and robust classifiers. Experimental results show that the ZigZag framework can substantially improve the robustness of DL-based detectors.

</details>

<details>

<summary>2021-08-04 08:58:24 - Secure and Privacy-Preserving Federated Learning via Co-Utility</summary>

- *Josep Domingo-Ferrer, Alberto Blanco-Justicia, Jesús Manjón, David Sánchez*

- `2108.01913v1` - [abs](http://arxiv.org/abs/2108.01913v1) - [pdf](http://arxiv.org/pdf/2108.01913v1)

> The decentralized nature of federated learning, that often leverages the power of edge devices, makes it vulnerable to attacks against privacy and security. The privacy risk for a peer is that the model update she computes on her private data may, when sent to the model manager, leak information on those private data. Even more obvious are security attacks, whereby one or several malicious peers return wrong model updates in order to disrupt the learning process and lead to a wrong model being learned. In this paper we build a federated learning framework that offers privacy to the participating peers as well as security against Byzantine and poisoning attacks. Our framework consists of several protocols that provide strong privacy to the participating peers via unlinkable anonymity and that are rationally sustainable based on the co-utility property. In other words, no rational party is interested in deviating from the proposed protocols. We leverage the notion of co-utility to build a decentralized co-utile reputation management system that provides incentives for parties to adhere to the protocols. Unlike privacy protection via differential privacy, our approach preserves the values of model updates and hence the accuracy of plain federated learning; unlike privacy protection via update aggregation, our approach preserves the ability to detect bad model updates while substantially reducing the computational overhead compared to methods based on homomorphic encryption.

</details>

<details>

<summary>2021-08-04 11:03:30 - A Systematic Assessment on Android Third-party Library Detection Tools</summary>

- *Xian Zhan, Tianming Liu, Yepang Liu, Yang Liu, Li Li, Haoyu Wang, Xiapu Luo*

- `2108.01964v1` - [abs](http://arxiv.org/abs/2108.01964v1) - [pdf](http://arxiv.org/pdf/2108.01964v1)

> Third-party libraries (TPLs) have become a significant part of the Android ecosystem. Developers can employ various TPLs to facilitate their app development. Unfortunately, the popularity of TPLs also brings new security issues. For example, TPLs may carry malicious or vulnerable code, which can infect popular apps to pose threats to mobile users. Furthermore, TPL detection is essential for downstream tasks, such as vulnerabilities and malware detection. Thus, various tools have been developed to identify TPLs. However, no existing work has studied these TPL detection tools in detail, and different tools focus on different applications and techniques with performance differences. A comprehensive understanding of these tools will help us make better use of them. To this end, we conduct a comprehensive empirical study to fill the gap by evaluating and comparing all publicly available TPL detection tools based on six criteria: accuracy of TPL construction, effectiveness, efficiency, accuracy of version identification, resiliency to code obfuscation, and ease of use. Besides, we enhance these open-source tools by fixing their limitations, to improve their detection ability. Finally, we build an extensible framework that integrates all existing available TPL detection tools, providing an online service for the research community. We release the evaluation dataset and enhanced tools. According to our study, we also present the essential findings and discuss promising implications to the community. We believe our work provides a clear picture of existing TPL detection techniques and also gives a roadmap for future research.

</details>

<details>

<summary>2021-08-04 13:14:25 - A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python</summary>

- *Amirreza Bagheri, Péter Hegedűs*

- `2108.02044v1` - [abs](http://arxiv.org/abs/2108.02044v1) - [pdf](http://arxiv.org/pdf/2108.02044v1)

> In the age of big data and machine learning, at a time when the techniques and methods of software development are evolving rapidly, a problem has arisen: programmers can no longer detect all the security flaws and vulnerabilities in their code manually. To overcome this problem, developers can now rely on automatic techniques, like machine learning based prediction models, to detect such issues. An inherent property of such approaches is that they work with numeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to transform the source code into such feature vectors, often referred to as code embedding. A popular approach for code embedding is to adapt natural language processing techniques, like text representation, to automatically derive the necessary features from the source code. However, the suitability and comparison of different text representation techniques for solving Software Engineering (SE) problems is rarely studied systematically. In this paper, we present a comparative study on three popular text representation methods, word2vec, fastText, and BERT applied to the SE task of detecting vulnerabilities in Python code. Using a data mining approach, we collected a large volume of Python source code in both vulnerable and fixed forms that we embedded with word2vec, fastText, and BERT to vectors and used a Long Short-Term Memory network to train on them. Using the same LSTM architecture, we could compare the efficiency of the different embeddings in deriving meaningful feature vectors. Our findings show that all the text representation methods are suitable for code representation in this particular task, but the BERT model is the most promising as it is the least time consuming and the LSTM model based on it achieved the best overall accuracy(93.8%) in predicting Python source code vulnerabilities.

</details>

<details>

<summary>2021-08-04 13:43:42 - Fairness in Algorithmic Profiling: A German Case Study</summary>

- *Christoph Kern, Ruben L. Bach, Hannah Mautner, Frauke Kreuter*

- `2108.04134v1` - [abs](http://arxiv.org/abs/2108.04134v1) - [pdf](http://arxiv.org/pdf/2108.04134v1)

> Algorithmic profiling is increasingly used in the public sector as a means to allocate limited public resources effectively and objectively. One example is the prediction-based statistical profiling of job seekers to guide the allocation of support measures by public employment services. However, empirical evaluations of potential side-effects such as unintended discrimination and fairness concerns are rare. In this study, we compare and evaluate statistical models for predicting job seekers' risk of becoming long-term unemployed with respect to prediction performance, fairness metrics, and vulnerabilities to data analysis decisions. Focusing on Germany as a use case, we evaluate profiling models under realistic conditions by utilizing administrative data on job seekers' employment histories that are routinely collected by German public employment services. Besides showing that these data can be used to predict long-term unemployment with competitive levels of accuracy, we highlight that different classification policies have very different fairness implications. We therefore call for rigorous auditing processes before such models are put to practice.

</details>

<details>

<summary>2021-08-04 21:14:46 - A Survey of Honeypots and Honeynets for Internet of Things, Industrial Internet of Things, and Cyber-Physical Systems</summary>

- *Javier Franco, Ahmet Aris, Berk Canberk, A. Selcuk Uluagac*

- `2108.02287v1` - [abs](http://arxiv.org/abs/2108.02287v1) - [pdf](http://arxiv.org/pdf/2108.02287v1)

> The Internet of Things (IoT), the Industrial Internet of Things (IIoT), and Cyber-Physical Systems (CPS) have become essential for our daily lives in contexts such as our homes, buildings, cities, health, transportation, manufacturing, infrastructure, and agriculture. However, they have become popular targets of attacks, due to their inherent limitations which create vulnerabilities. Honeypots and honeynets can prove essential to understand and defend against attacks on IoT, IIoT, and CPS environments by attracting attackers and deceiving them into thinking that they have gained access to the real systems. Honeypots and honeynets can complement other security solutions (i.e., firewalls, Intrusion Detection Systems - IDS) to form a strong defense against malicious entities. This paper provides a comprehensive survey of the research that has been carried out on honeypots and honeynets for IoT, IIoT, and CPS. It provides a taxonomy and extensive analysis of the existing honeypots and honeynets, states key design factors for the state-of-the-art honeypot/honeynet research and outlines open issues for future honeypots and honeynets for IoT, IIoT, and CPS environments.

</details>

<details>

<summary>2021-08-05 02:30:13 - Robust Transfer Learning with Pretrained Language Models through Adapters</summary>

- *Wenjuan Han, Bo Pang, Yingnian Wu*

- `2108.02340v1` - [abs](http://arxiv.org/abs/2108.02340v1) - [pdf](http://arxiv.org/pdf/2108.02340v1)

> Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.

</details>

<details>

<summary>2021-08-05 09:48:49 - How are Project-Specific Forums Utilized? A Study of Participation, Content, and Sentiment in the Eclipse Ecosystem</summary>

- *Yusuf Sulistyo Nugroho, Syful Islam, Keitaro Nakasai, Ifraz Rehman, Hideaki Hata, Raula Gaikovina Kula, Meiyappan Nagappan, Kenichi Matsumoto*

- `2009.09130v3` - [abs](http://arxiv.org/abs/2009.09130v3) - [pdf](http://arxiv.org/pdf/2009.09130v3)

> Although many software development projects have moved their developer discussion forums to generic platforms such as Stack Overflow, Eclipse has been steadfast in hosting their self-supported community forums. While recent studies show forums share similarities to generic communication channels, it is unknown how project-specific forums are utilized. In this paper, we analyze 832,058 forum threads and their linkages to four systems with 2,170 connected contributors to understand the participation, content and sentiment. Results show that Seniors are the most active participants to respond bug and non-bug-related threads in the forums (i.e., 66.1% and 45.5%), and sentiment among developers are inconsistent while knowledge sharing within Eclipse. We recommend the users to identify appropriate topics and ask in a positive procedural way when joining forums. For developers, preparing project-specific forums could be an option to bridge the communication between members. Irrespective of the popularity of Stack Overflow, we argue the benefits of using project-specific forum initiatives, such as GitHub Discussions, are needed to cultivate a community and its ecosystem.

</details>

<details>

<summary>2021-08-05 13:52:31 - Using a Collated Cybersecurity Dataset for Machine Learning and Artificial Intelligence</summary>

- *Erik Hemberg, Una-May O'Reilly*

- `2108.02618v1` - [abs](http://arxiv.org/abs/2108.02618v1) - [pdf](http://arxiv.org/pdf/2108.02618v1)

> Artificial Intelligence (AI) and Machine Learning (ML) algorithms can support the span of indicator-level, e.g. anomaly detection, to behavioral level cyber security modeling and inference. This contribution is based on a dataset named BRON which is amalgamated from public threat and vulnerability behavioral sources. We demonstrate how BRON can support prediction of related threat techniques and attack patterns. We also discuss other AI and ML uses of BRON to exploit its behavioral knowledge.

</details>

<details>

<summary>2021-08-05 14:51:36 - Computing and Authentication Practices in Global Oil and Gas Fields</summary>

- *Mary Rose Martinez, Shriram Krishnamurthi*

- `2108.02660v1` - [abs](http://arxiv.org/abs/2108.02660v1) - [pdf](http://arxiv.org/pdf/2108.02660v1)

> Oil and gas fields are a critical part of our infrastructure, and vulnerable to attack by powerful adversaries. In addition, these are often difficult work environments, with constraints on space, clothing, and more. Yet there is little research on the technology practices and constraints of workers in these environments. We present what we believe is the first survey of oil- and gas-field workers located around the world. We establish the presence and status of a variety of computing devices and of the security practices that govern their use. We also determine the working conditions (such as personal protective equipment) under which these devices are used, which impacts usable security aspects like feasible forms of authentication. We extend these basic insights with additional information from a small number of in-depth interviews. Our preliminary work suggests many directions for improving security in this critical sector.

</details>

<details>

<summary>2021-08-05 19:49:46 - Hate Speech Detection in Roman Urdu</summary>

- *Moin Khan, Khurram Shahzad, Kamran Malik*

- `2108.02830v1` - [abs](http://arxiv.org/abs/2108.02830v1) - [pdf](http://arxiv.org/pdf/2108.02830v1)

> Hate speech is a specific type of controversial content that is widely legislated as a crime that must be identified and blocked. However, due to the sheer volume and velocity of the Twitter data stream, hate speech detection cannot be performed manually. To address this issue, several studies have been conducted for hate speech detection in European languages, whereas little attention has been paid to low-resource South Asian languages, making the social media vulnerable for millions of users. In particular, to the best of our knowledge, no study has been conducted for hate speech detection in Roman Urdu text, which is widely used in the sub-continent. In this study, we have scrapped more than 90,000 tweets and manually parsed them to identify 5,000 Roman Urdu tweets. Subsequently, we have employed an iterative approach to develop guidelines and used them for generating the Hate Speech Roman Urdu 2020 corpus. The tweets in the this corpus are classified at three levels: Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another contribution, we have used five supervised learning techniques, including a deep learning technique, to evaluate and compare their effectiveness for hate speech detection. The results show that Logistic Regression outperformed all other techniques, including deep learning techniques for the two levels of classification, by achieved an F1 score of 0.906 for distinguishing between Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate speech tweets.

</details>

<details>

<summary>2021-08-05 21:34:25 - Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control with Model-assisted Learning</summary>

- *Shichao Xu, Yangyang Fu, Yixuan Wang, Zheng O'Neill, Qi Zhu*

- `2106.14144v2` - [abs](http://arxiv.org/abs/2106.14144v2) - [pdf](http://arxiv.org/pdf/2106.14144v2)

> As people spend up to 87% of their time indoors, intelligent Heating, Ventilation, and Air Conditioning (HVAC) systems in buildings are essential for maintaining occupant comfort and reducing energy consumption. These HVAC systems in smart buildings rely on real-time sensor readings, which in practice often suffer from various faults and could also be vulnerable to malicious attacks. Such faulty sensor inputs may lead to the violation of indoor environment requirements (e.g., temperature, humidity, etc.) and the increase of energy consumption. While many model-based approaches have been proposed in the literature for building HVAC control, it is costly to develop accurate physical models for ensuring their performance and even more challenging to address the impact of sensor faults. In this work, we present a novel learning-based framework for sensor fault-tolerant HVAC control, which includes three deep learning based components for 1) generating temperature proposals with the consideration of possible sensor faults, 2) selecting one of the proposals based on the assessment of their accuracy, and 3) applying reinforcement learning with the selected temperature proposal. Moreover, to address the challenge of training data insufficiency in building-related tasks, we propose a model-assisted learning method leveraging an abstract model of building physical dynamics. Through extensive experiments, we demonstrate that the proposed fault-tolerant HVAC control framework can significantly reduce building temperature violations under a variety of sensor fault patterns while maintaining energy efficiency.

</details>

<details>

<summary>2021-08-06 04:52:09 - Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles</summary>

- *Jindi Zhang, Yang Lou, Jianping Wang, Kui Wu, Kejie Lu, Xiaohua Jia*

- `2108.02940v1` - [abs](http://arxiv.org/abs/2108.02940v1) - [pdf](http://arxiv.org/pdf/2108.02940v1)

> In recent years, many deep learning models have been adopted in autonomous driving. At the same time, these models introduce new vulnerabilities that may compromise the safety of autonomous vehicles. Specifically, recent studies have demonstrated that adversarial attacks can cause a significant decline in detection precision of deep learning-based 3D object detection models. Although driving safety is the ultimate concern for autonomous driving, there is no comprehensive study on the linkage between the performance of deep learning models and the driving safety of autonomous vehicles under adversarial attacks. In this paper, we investigate the impact of two primary types of adversarial attacks, perturbation attacks and patch attacks, on the driving safety of vision-based autonomous vehicles rather than the detection precision of deep learning models. In particular, we consider two state-of-the-art models in vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving safety, we propose an end-to-end evaluation framework with a set of driving safety performance metrics. By analyzing the results of our extensive evaluation experiments, we find that (1) the attack's impact on the driving safety of autonomous vehicles and the attack's impact on the precision of 3D object detectors are decoupled, and (2) the DSGN model demonstrates stronger robustness to adversarial attacks than the Stereo R-CNN model. In addition, we further investigate the causes behind the two findings with an ablation study. The findings of this paper provide a new perspective to evaluate adversarial attacks and guide the selection of deep learning models in autonomous driving.

</details>

<details>

<summary>2021-08-06 05:56:14 - HIPPODROME: Data Race Repair using Static Analysis Summaries</summary>

- *Andreea Costea, Abhishek Tiwari, Sigmund Chianasta, Kishore R, Abhik Roychoudhury, Ilya Sergey*

- `2108.02490v2` - [abs](http://arxiv.org/abs/2108.02490v2) - [pdf](http://arxiv.org/pdf/2108.02490v2)

> Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way.   In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair (APR) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting, into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to APR, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyse and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study conducted on popular open-source projects has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past.

</details>

<details>

<summary>2021-08-06 13:25:46 - Robust Federated Learning with Attack-Adaptive Aggregation</summary>

- *Ching Pui Wan, Qifeng Chen*

- `2102.05257v2` - [abs](http://arxiv.org/abs/2102.05257v2) - [pdf](http://arxiv.org/pdf/2102.05257v2)

> Federated learning is vulnerable to various attacks, such as model poisoning and backdoor attacks, even if some existing defense strategies are used. To address this challenge, we propose an attack-adaptive aggregation strategy to defend against various attacks for robust federated learning. The proposed approach is based on training a neural network with an attention mechanism that learns the vulnerability of federated learning models from a set of plausible attacks. To the best of our knowledge, our aggregation strategy is the first one that can be adapted to defend against various attacks in a data-driven fashion. Our approach has achieved competitive performance in defending model poisoning and backdoor attacks in federated learning tasks on image and text datasets.

</details>

<details>

<summary>2021-08-06 16:03:11 - HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks</summary>

- *Mohammad Hossein Samavatian, Saikat Majumdar, Kristin Barber, Radu Teodorescu*

- `2106.05825v3` - [abs](http://arxiv.org/abs/2106.05825v3) - [pdf](http://arxiv.org/pdf/2106.05825v3)

> Deep Neural Networks (DNNs) are employed in an increasing number of applications, some of which are safety critical. Unfortunately, DNNs are known to be vulnerable to so-called adversarial attacks that manipulate inputs to cause incorrect results that can be beneficial to an attacker or damaging to the victim. Multiple defenses have been proposed to increase the robustness of DNNs. In general, these defenses have high overhead, some require attack-specific re-training of the model or careful tuning to adapt to different attacks.   This paper presents HASI, a hardware-accelerated defense that uses a process we call stochastic inference to detect adversarial inputs. We show that by carefully injecting noise into the model at inference time, we can differentiate adversarial inputs from benign ones. HASI uses the output distribution characteristics of noisy inference compared to a non-noisy reference to detect adversarial inputs. We show an adversarial detection rate of 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds the detection rate of the state of the art approaches, with a much lower overhead. We demonstrate two software/hardware-accelerated co-designs, which reduces the performance impact of stochastic inference to 1.58X-2X relative to the unprotected baseline, compared to 15X-20X overhead for a software-only GPU implementation.

</details>

<details>

<summary>2021-08-06 18:18:32 - Using Undervolting as an On-Device Defense Against Adversarial Machine Learning Attacks</summary>

- *Saikat Majumdar, Mohammad Hossein Samavatian, Kristin Barber, Radu Teodorescu*

- `2107.09804v2` - [abs](http://arxiv.org/abs/2107.09804v2) - [pdf](http://arxiv.org/pdf/2107.09804v2)

> Deep neural network (DNN) classifiers are powerful tools that drive a broad spectrum of important applications, from image recognition to autonomous vehicles. Unfortunately, DNNs are known to be vulnerable to adversarial attacks that affect virtually all state-of-the-art models. These attacks make small imperceptible modifications to inputs that are sufficient to induce the DNNs to produce the wrong classification.   In this paper we propose a novel, lightweight adversarial correction and/or detection mechanism for image classifiers that relies on undervolting (running a chip at a voltage that is slightly below its safe margin). We propose using controlled undervolting of the chip running the inference process in order to introduce a limited number of compute errors. We show that these errors disrupt the adversarial input in a way that can be used either to correct the classification or detect the input as adversarial. We evaluate the proposed solution in an FPGA design and through software simulation. We evaluate 10 attacks and show average detection rates of 77% and 90% on two popular DNNs.

</details>

<details>

<summary>2021-08-07 19:22:47 - Membership Inference Attacks on Lottery Ticket Networks</summary>

- *Aadesh Bagmar, Shishira R Maiya, Shruti Bidwalka, Amol Deshpande*

- `2108.03506v1` - [abs](http://arxiv.org/abs/2108.03506v1) - [pdf](http://arxiv.org/pdf/2108.03506v1)

> The vulnerability of the Lottery Ticket Hypothesis has not been studied from the purview of Membership Inference Attacks. Through this work, we are the first to empirically show that the lottery ticket networks are equally vulnerable to membership inference attacks. A Membership Inference Attack (MIA) is the process of determining whether a data sample belongs to a training set of a trained model or not. Membership Inference Attacks could leak critical information about the training data that can be used for targeted attacks. Recent deep learning models often have very large memory footprints and a high computational cost associated with training and drawing inferences. Lottery Ticket Hypothesis is used to prune the networks to find smaller sub-networks that at least match the performance of the original model in terms of test accuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and ImageNet datasets to perform image classification tasks and observe that the attack accuracies are similar. We also see that the attack accuracy varies directly according to the number of classes in the dataset and the sparsity of the network. We demonstrate that these attacks are transferable across models with high accuracy.

</details>

<details>

<summary>2021-08-07 20:07:08 - Machine Learning Assisted Security Analysis of 5G-Network-Connected Systems</summary>

- *Tanujay Saha, Najwa Aaraj, Niraj K. Jha*

- `2108.03514v1` - [abs](http://arxiv.org/abs/2108.03514v1) - [pdf](http://arxiv.org/pdf/2108.03514v1)

> The core network architecture of telecommunication systems has undergone a paradigm shift in the fifth-generation (5G)networks. 5G networks have transitioned to software-defined infrastructures, thereby reducing their dependence on hardware-based network functions. New technologies, like network function virtualization and software-defined networking, have been incorporated in the 5G core network (5GCN) architecture to enable this transition. This has resulted in significant improvements in efficiency, performance, and robustness of the networks. However, this has also made the core network more vulnerable, as software systems are generally easier to compromise than hardware systems. In this article, we present a comprehensive security analysis framework for the 5GCN. The novelty of this approach lies in the creation and analysis of attack graphs of the software-defined and virtualized 5GCN through machine learning. This analysis points to 119 novel possible exploits in the 5GCN. We demonstrate that these possible exploits of 5GCN vulnerabilities generate five novel attacks on the 5G Authentication and Key Agreement protocol. We combine the attacks at the network, protocol, and the application layers to generate complex attack vectors. In a case study, we use these attack vectors to find four novel security loopholes in WhatsApp running on a 5G network.

</details>

<details>

<summary>2021-08-09 13:11:47 - A Weak Supervision Approach to Detecting Visual Anomalies for Automated Testing of Graphics Units</summary>

- *Adi Szeskin, Lev Faivishevsky, Ashwin K Muppalla, Amitai Armon, Tom Hope*

- `1912.04138v2` - [abs](http://arxiv.org/abs/1912.04138v2) - [pdf](http://arxiv.org/pdf/1912.04138v2)

> We present a deep learning system for testing graphics units by detecting novel visual corruptions in videos. Unlike previous work in which manual tagging was required to collect labeled training data, our weak supervision method is fully automatic and needs no human labelling. This is achieved by reproducing driver bugs that increase the probability of generating corruptions, and by making use of ideas and methods from the Multiple Instance Learning (MIL) setting. In our experiments, we significantly outperform unsupervised methods such as GAN-based models and discover novel corruptions undetected by baselines, while adhering to strict requirements on accuracy and efficiency of our real-time system.

</details>

<details>

<summary>2021-08-09 14:41:22 - A Machine learning approach for rapid disaster response based on multi-modal data. The case of housing & shelter needs</summary>

- *Karla Saldana Ochoa, Tina Comes*

- `2108.00887v2` - [abs](http://arxiv.org/abs/2108.00887v2) - [pdf](http://arxiv.org/pdf/2108.00887v2)

> Along with climate change, more frequent extreme events, such as flooding and tropical cyclones, threaten the livelihoods and wellbeing of poor and vulnerable populations. One of the most immediate needs of people affected by a disaster is finding shelter. While the proliferation of data on disasters is already helping to save lives, identifying damages in buildings, assessing shelter needs, and finding appropriate places to establish emergency shelters or settlements require a wide range of data to be combined rapidly. To address this gap and make a headway in comprehensive assessments, this paper proposes a machine learning workflow that aims to fuse and rapidly analyse multimodal data. This workflow is built around open and online data to ensure scalability and broad accessibility. Based on a database of 19 characteristics for more than 200 disasters worldwide, a fusion approach at the decision level was used. This technique allows the collected multimodal data to share a common semantic space that facilitates the prediction of individual variables. Each fused numerical vector was fed into an unsupervised clustering algorithm called Self-Organizing-Maps (SOM). The trained SOM serves as a predictor for future cases, allowing predicting consequences such as total deaths, total people affected, and total damage, and provides specific recommendations for assessments in the shelter and housing sector. To achieve such prediction, a satellite image from before the disaster and the geographic and demographic conditions are shown to the trained model, which achieved a prediction accuracy of 62 %

</details>

<details>

<summary>2021-08-09 15:54:56 - Practical Relative Order Attack in Deep Ranking</summary>

- *Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Yinghui Xu, Nanning Zheng, Gang Hua*

- `2103.05248v4` - [abs](http://arxiv.org/abs/2103.05248v4) - [pdf](http://arxiv.org/pdf/2103.05248v4)

> Recent studies unveil the vulnerabilities of deep ranking models, where an imperceptible perturbation can trigger dramatic changes in the ranking result. While previous attempts focus on manipulating absolute ranks of certain candidates, the possibility of adjusting their relative order remains under-explored. In this paper, we formulate a new adversarial attack against deep ranking systems, i.e., the Order Attack, which covertly alters the relative order among a selected set of candidates according to an attacker-specified permutation, with limited interference to other unrelated candidates. Specifically, it is formulated as a triplet-style loss imposing an inequality chain reflecting the specified permutation. However, direct optimization of such white-box objective is infeasible in a real-world attack scenario due to various black-box limitations. To cope with them, we propose a Short-range Ranking Correlation metric as a surrogate objective for black-box Order Attack to approximate the white-box method. The Order Attack is evaluated on the Fashion-MNIST and Stanford-Online-Products datasets under both white-box and black-box threat models. The black-box attack is also successfully implemented on a major e-commerce platform. Comprehensive experimental evaluations demonstrate the effectiveness of the proposed methods, revealing a new type of ranking model vulnerability.

</details>

<details>

<summary>2021-08-09 16:57:42 - Learning and Certification under Instance-targeted Poisoning</summary>

- *Ji Gao, Amin Karbasi, Mohammad Mahmoody*

- `2105.08709v2` - [abs](http://arxiv.org/abs/2105.08709v2) - [pdf](http://arxiv.org/pdf/2105.08709v2)

> In this paper, we study PAC learnability and certification of predictions under instance-targeted poisoning attacks, where the adversary who knows the test instance may change a fraction of the training set with the goal of fooling the learner at the test instance. Our first contribution is to formalize the problem in various settings and to explicitly model subtle aspects such as the proper or improper nature of the learning, learner's randomness, and whether (or not) adversary's attack can depend on it. Our main result shows that when the budget of the adversary scales sublinearly with the sample complexity, (improper) PAC learnability and certification are achievable; in contrast, when the adversary's budget grows linearly with the sample complexity, the adversary can potentially drive up the expected 0-1 loss to one. We also study distribution-specific PAC learning in the same attack model and show that proper learning with certification is possible for learning half spaces under natural distributions. Finally, we empirically study the robustness of K nearest neighbour, logistic regression, multi-layer perceptron, and convolutional neural network on real data sets against targeted-poisoning attacks. Our experimental results show that many models, especially state-of-the-art neural networks, are indeed vulnerable to these strong attacks. Interestingly, we observe that methods with high standard accuracy might be more vulnerable to instance-targeted poisoning attacks.

</details>

<details>

<summary>2021-08-10 02:27:00 - Graph Backdoor</summary>

- *Zhaohan Xi, Ren Pang, Shouling Ji, Ting Wang*

- `2006.11890v5` - [abs](http://arxiv.org/abs/2006.11890v5) - [pdf](http://arxiv.org/pdf/2006.11890v5)

> One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary; input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness; downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies; and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.

</details>

<details>

<summary>2021-08-10 02:54:38 - Issue Link Label Recovery and Prediction for Open Source Software</summary>

- *Alexander Nicholson, Jin L. C. Guo*

- `2108.04415v1` - [abs](http://arxiv.org/abs/2108.04415v1) - [pdf](http://arxiv.org/pdf/2108.04415v1)

> Modern open source software development heavily relies on the issue tracking systems to manage their feature requests, bug reports, tasks, and other similar artifacts. Together, those "issues" form a complex network with links to each other. The heterogeneous character of issues inherently results in varied link types and therefore poses a great challenge for users to create and maintain the label of the link manually. The goal of most existing automated issue link construction techniques ceases with only examining the existence of links between issues. In this work, we focus on the next important question of whether we can assess the type of issue link automatically through a data-driven method. We analyze the links between issues and their labels used the issue tracking system for 66 open source projects. Using three projects, we demonstrate promising results when using supervised machine learning classification for the task of link label recovery with careful model selection and tuning, achieving F1 scores of between 0.56-0.70 for the three studied projects. Further, the performance of our method for future link label prediction is convincing when there is sufficient historical data. Our work signifies the first step in systematically manage and maintain issue links faced in practice.

</details>

<details>

<summary>2021-08-10 03:42:03 - Provable Training Set Debugging for Linear Regression</summary>

- *Xiaomin Zhang, Xiaojin Zhu, Po-Ling Loh*

- `2006.09009v2` - [abs](http://arxiv.org/abs/2006.09009v2) - [pdf](http://arxiv.org/pdf/2006.09009v2)

> We investigate problems in penalized $M$-estimation, inspired by applications in machine learning debugging. Data are collected from two pools, one containing data with possibly contaminated labels, and the other which is known to contain only cleanly labeled points. We first formulate a general statistical algorithm for identifying buggy points and provide rigorous theoretical guarantees under the assumption that the data follow a linear model. We then present two case studies to illustrate the results of our general theory and the dependence of our estimator on clean versus buggy points. We further propose an algorithm for tuning parameter selection of our Lasso-based algorithm and provide corresponding theoretical guarantees. Finally, we consider a two-person "game" played between a bug generator and a debugger, where the debugger can augment the contaminated data set with cleanly labeled versions of points in the original data pool. We establish a theoretical result showing a sufficient condition under which the bug generator can always fool the debugger. Nonetheless, we provide empirical results showing that such a situation may not occur in practice, making it possible for natural augmentation strategies combined with our Lasso debugging algorithm to succeed.

</details>

<details>

<summary>2021-08-10 07:38:44 - Poisoning the Unlabeled Dataset of Semi-Supervised Learning</summary>

- *Nicholas Carlini*

- `2105.01622v2` - [abs](http://arxiv.org/abs/2105.01622v2) - [pdf](http://arxiv.org/pdf/2105.01622v2)

> Semi-supervised machine learning models learn from a (small) set of labeled training examples, and a (large) set of unlabeled training examples. State-of-the-art models can reach within a few percentage points of fully-supervised training, while requiring 100x less labeled data.   We study a new class of vulnerabilities: poisoning attacks that modify the unlabeled dataset. In order to be useful, unlabeled datasets are given strictly less review than labeled datasets, and adversaries can therefore poison them easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1% of the dataset size, we can manipulate a model trained on this poisoned dataset to misclassify arbitrary examples at test time (as any desired label). Our attacks are highly effective across datasets and semi-supervised learning methods.   We find that more accurate methods (thus more likely to be used) are significantly more vulnerable to poisoning attacks, and as such better training methods are unlikely to prevent this attack. To counter this we explore the space of defenses, and propose two methods that mitigate our attack.

</details>

<details>

<summary>2021-08-10 13:51:23 - Quantum collision finding for homomorphic hash functions</summary>

- *Juan Carlos Garcia-Escartin, Vicent Gimeno, Julio José Moyano-Fernández*

- `2108.00100v2` - [abs](http://arxiv.org/abs/2108.00100v2) - [pdf](http://arxiv.org/pdf/2108.00100v2)

> Hash functions are a basic cryptographic primitive. Certain hash functions try to prove security against collision and preimage attacks by reductions to known hard problems. These hash functions usually have some additional properties that allow for that reduction. Hash functions which are additive or multiplicative are vulnerable to a quantum attack using the hidden subgroup problem algorithm for quantum computers. Using a quantum oracle to the hash, we can reconstruct the kernel of the hash function, which is enough to find collisions and second preimages. When the hash functions are additive with respect to the group operation in an Abelian group, there is always an efficient implementation of this attack. We present concrete attack examples to provable hash functions, including a preimage attack to $\oplus$-linear hash functions and for certain multiplicative homomorphic hash schemes.

</details>

<details>

<summary>2021-08-11 00:59:34 - Simple black-box universal adversarial attacks on medical image classification based on deep neural networks</summary>

- *Kazuki Koga, Kazuhiro Takemoto*

- `2108.04979v1` - [abs](http://arxiv.org/abs/2108.04979v1) - [pdf](http://arxiv.org/pdf/2108.04979v1)

> Universal adversarial attacks, which hinder most deep neural network (DNN) tasks using only a small single perturbation called a universal adversarial perturbation (UAP), is a realistic security threat to the practical application of a DNN. In particular, such attacks cause serious problems in medical imaging. Given that computer-based systems are generally operated under a black-box condition in which only queries on inputs are allowed and outputs are accessible, the impact of UAPs seems to be limited because well-used algorithms for generating UAPs are limited to a white-box condition in which adversaries can access the model weights and loss gradients. Nevertheless, we demonstrate that UAPs are easily generatable using a relatively small dataset under black-box conditions. In particular, we propose a method for generating UAPs using a simple hill-climbing search based only on DNN outputs and demonstrate the validity of the proposed method using representative DNN-based medical image classifications. Black-box UAPs can be used to conduct both non-targeted and targeted attacks. Overall, the black-box UAPs showed high attack success rates (40% to 90%), although some of them had relatively low success rates because the method only utilizes limited information to generate UAPs. The vulnerability of black-box UAPs was observed in several model architectures. The results indicate that adversaries can also generate UAPs through a simple procedure under the black-box condition to foil or control DNN-based medical image diagnoses, and that UAPs are a more realistic security threat.

</details>

<details>

<summary>2021-08-11 09:36:34 - The Used, the Bloated, and the Vulnerable: Reducing the Attack Surface of an Industrial Application</summary>

- *Serena Elisa Ponta, Wolfram Fischer, Henrik Plate, Antonino Sabetta*

- `2108.05115v1` - [abs](http://arxiv.org/abs/2108.05115v1) - [pdf](http://arxiv.org/pdf/2108.05115v1)

> Software reuse may result in software bloat when significant portions of application dependencies are effectively unused. Several tools exist to remove unused (byte)code from an application or its dependencies, thus producing smaller artifacts and, potentially, reducing the overall attack surface. In this paper we evaluate the ability of three debloating tools to distinguish which dependency classes are necessary for an application to function correctly from those that could be safely removed. To do so, we conduct a case study on a real-world commercial Java application. Our study shows that the tools we used were able to correctly identify a considerable amount of redundant code, which could be removed without altering the results of the existing application tests. One of the redundant classes turned out to be (formerly) vulnerable, confirming that this technique has the potential to be applied for hardening purposes. However, by manually reviewing the results of our experiments, we observed that none of the tools can handle a widely used default mechanism for dynamic class loading.

</details>

<details>

<summary>2021-08-11 16:53:30 - Why are Some Bugs Non-Reproducible? An Empirical Investigation using Data Fusion</summary>

- *Mohammad Masudur Rahman, Foutse Khomh, Marco Castelluccio*

- `2108.05316v1` - [abs](http://arxiv.org/abs/2108.05316v1) - [pdf](http://arxiv.org/pdf/2108.05316v1)

> Software developers attempt to reproduce software bugs to understand their erroneous behaviours and to fix them. Unfortunately, they often fail to reproduce (or fix) them, which leads to faulty, unreliable software systems. However, to date, only a little research has been done to better understand what makes the software bugs non-reproducible. In this paper, we conduct a multimodal study to better understand the non-reproducibility of software bugs. First, we perform an empirical study using 576 non-reproducible bug reports from two popular software systems (Firefox, Eclipse) and identify 11 key factors that might lead a reported bug to non-reproducibility. Second, we conduct a user study involving 13 professional developers where we investigate how the developers cope with non-reproducible bugs. We found that they either close these bugs or solicit for further information, which involves long deliberations and counter-productive manual searches. Third, we offer several actionable insights on how to avoid non-reproducibility (e.g., false-positive bug report detector) and improve reproducibility of the reported bugs (e.g., sandbox for bug reproduction) by combining our analyses from multiple studies (e.g., empirical study, developer study).

</details>

<details>

<summary>2021-08-11 17:37:50 - The Forgotten Role of Search Queries in IR-based Bug Localization: An Empirical Study</summary>

- *Mohammad Masudur Rahman, Foutse Khomh, Shamima Yeasmin, Chanchal K. Roy*

- `2108.05341v1` - [abs](http://arxiv.org/abs/2108.05341v1) - [pdf](http://arxiv.org/pdf/2108.05341v1)

> Being light-weight and cost-effective, IR-based approaches for bug localization have shown promise in finding software bugs. However, the accuracy of these approaches heavily depends on their used bug reports. A significant number of bug reports contain only plain natural language texts. According to existing studies, IR-based approaches cannot perform well when they use these bug reports as search queries. On the other hand, there is a piece of recent evidence that suggests that even these natural language-only reports contain enough good keywords that could help localize the bugs successfully. On one hand, these findings suggest that natural language-only bug reports might be a sufficient source for good query keywords. On the other hand, they cast serious doubt on the query selection practices in the IR-based bug localization. In this article, we attempted to clear the sky on this aspect by conducting an in-depth empirical study that critically examines the state-of-the-art query selection practices in IR-based bug localization. In particular, we use a dataset of 2,320 bug reports, employ ten existing approaches from the literature, exploit the Genetic Algorithm-based approach to construct optimal, near-optimal search queries from these bug reports, and then answer three research questions. We confirmed that the state-of-the-art query construction approaches are indeed not sufficient for constructing appropriate queries (for bug localization) from certain natural language-only bug reports although they contain such queries. We also demonstrate that optimal queries and non-optimal queries chosen from bug report texts are significantly different in terms of several keyword characteristics, which has led us to actionable insights. Furthermore, we demonstrate 27%--34% improvement in the performance of non-optimal queries through the application of our actionable insights to them.

</details>

<details>

<summary>2021-08-12 01:41:21 - Attacks against Ranking Algorithms with Text Embeddings: a Case Study on Recruitment Algorithms</summary>

- *Anahita Samadi, Debapriya Banerjee, Shirin Nilizadeh*

- `2108.05490v1` - [abs](http://arxiv.org/abs/2108.05490v1) - [pdf](http://arxiv.org/pdf/2108.05490v1)

> Recently, some studies have shown that text classification tasks are vulnerable to poisoning and evasion attacks. However, little work has investigated attacks against decision making algorithms that use text embeddings, and their output is a ranking. In this paper, we focus on ranking algorithms for recruitment process, that employ text embeddings for ranking applicants resumes when compared to a job description. We demonstrate both white box and black box attacks that identify text items, that based on their location in embedding space, have significant contribution in increasing the similarity score between a resume and a job description. The adversary then uses these text items to improve the ranking of their resume among others. We tested recruitment algorithms that use the similarity scores obtained from Universal Sentence Encoder (USE) and Term Frequency Inverse Document Frequency (TF IDF) vectors. Our results show that in both adversarial settings, on average the attacker is successful. We also found that attacks against TF IDF is more successful compared to USE.

</details>

<details>

<summary>2021-08-12 08:55:17 - Automatically Locating ARM Instructions Deviation between Real Devices and CPU Emulators</summary>

- *Muhui Jiang, Tianyi Xu, Yajin Zhou, Yufeng Hu, Ming Zhong, Lei Wu, Xiapu Luo, Kui Ren*

- `2105.14273v2` - [abs](http://arxiv.org/abs/2105.14273v2) - [pdf](http://arxiv.org/pdf/2105.14273v2)

> Emulator is widely used to build dynamic analysis frameworks due to its fine-grained tracing capability, full system monitoring functionality, and scalability of running on different operating systemsand architectures. However, whether the emulator is consistent with real devices is unknown. To understand this problem, we aim to automatically locate inconsistent instructions, which behave differently between emulators and real devices.   We target ARM architecture, which provides machine readable specification. Based on the specification, we propose a test case generator by designing and implementing the first symbolic execution engine for ARM architecture specification language (ASL). We generate 2,774,649 representative instruction streams and conduct differential testing with these instruction streams between four ARM real devices in different architecture versions (i.e., ARMv5, ARMv6, ARMv7-a, and ARMv8-a) and the state-of-the-art emulators (i.e., QEMU). We locate 155,642 inconsistent instruction streams, which cover 30% of all instruction encodings and 47.8% of the instructions. We find undefined implementation in ARM manual and implementation bugs of QEMU are the major causes of inconsistencies. Furthermore, we discover four QEMU bugs, which are confirmed and patched by thedevelopers, covering 13 instruction encodings including the most commonly used ones (e.g.,STR,BLX). With the inconsistent instructions, we build three security applications and demonstrate thecapability of these instructions on detecting emulators, anti-emulation, and anti-fuzzing.

</details>

<details>

<summary>2021-08-12 16:51:56 - Automating the Removal of Obsolete TODO Comments</summary>

- *Zhipeng Gao, Xin Xia, David Lo, John Grundy, Thomas Zimmermann*

- `2108.05846v1` - [abs](http://arxiv.org/abs/2108.05846v1) - [pdf](http://arxiv.org/pdf/2108.05846v1)

> TODO comments are very widely used by software developers to describe their pending tasks during software development. However, after performing the task developers sometimes neglect or simply forget to remove the TODO comment, resulting in obsolete TODO comments. These obsolete TODO comments can confuse development teams and may cause the introduction of bugs in the future, decreasing the software's quality and maintainability. In this work, we propose a novel model, named TDCleaner (TODO comment Cleaner), to identify obsolete TODO comments in software projects. TDCleaner can assist developers in just-in-time checking of TODO comments status and avoid leaving obsolete TODO comments. Our approach has two main stages: offline learning and online prediction. During offline learning, we first automatically establish <code_change, todo_comment, commit_msg> training samples and leverage three neural encoders to capture the semantic features of TODO comment, code change and commit message respectively. TDCleaner then automatically learns the correlations and interactions between different encoders to estimate the final status of the TODO comment. For online prediction, we check a TODO comment's status by leveraging the offline trained model to judge the TODO comment's likelihood of being obsolete. We built our dataset by collecting TODO comments from the top-10,000 Python and Java Github repositories and evaluated TDCleaner on them. Extensive experimental results show the promising performance of our model over a set of benchmarks. We also performed an in-the-wild evaluation with real-world software projects, we reported 18 obsolete TODO comments identified by TDCleaner to Github developers and 9 of them have already been confirmed and removed by the developers, demonstrating the practical usage of our approach.

</details>

<details>

<summary>2021-08-13 06:01:05 - Adversarially Robust Low Dimensional Representations</summary>

- *Pranjal Awasthi, Vaggos Chatziafratis, Xue Chen, Aravindan Vijayaraghavan*

- `1911.13268v3` - [abs](http://arxiv.org/abs/1911.13268v3) - [pdf](http://arxiv.org/pdf/1911.13268v3)

> Many machine learning systems are vulnerable to small perturbations made to inputs either at test time or at training time. This has received much recent interest on the empirical front due to applications where reliability and security are critical. However, theoretical understanding of algorithms that are robust to adversarial perturbations is limited.   In this work we focus on Principal Component Analysis (PCA), a ubiquitous algorithmic primitive in machine learning. We formulate a natural robust variant of PCA where the goal is to find a low dimensional subspace to represent the given data with minimum projection error, that is in addition robust to small perturbations measured in $\ell_q$ norm (say $q=\infty$). Unlike PCA which is solvable in polynomial time, our formulation is computationally intractable to optimize as it captures a variant of the well-studied sparse PCA objective as a special case. We show the following results:   -Polynomial time algorithm that is constant factor competitive in the worst-case with respect to the best subspace, in terms of the projection error and the robustness criterion.   -We show that our algorithmic techniques can also be made robust to adversarial training-time perturbations, in addition to yielding representations that are robust to adversarial perturbations at test time. Specifically, we design algorithms for a strong notion of training-time perturbations, where every point is adversarially perturbed up to a specified amount.   -We illustrate the broad applicability of our algorithmic techniques in addressing robustness to adversarial perturbations, both at training time and test time. In particular, our adversarially robust PCA primitive leads to computationally efficient and robust algorithms for both unsupervised and supervised learning problems such as clustering and learning adversarially robust classifiers.

</details>

<details>

<summary>2021-08-13 06:29:21 - Asteria: Deep Learning-based AST-Encoding for Cross-platform Binary Code Similarity Detection</summary>

- *Shouguo Yang, Long Cheng, Yicheng Zeng, Zhe Lang, Hongsong Zhu, Zhiqiang Shi*

- `2108.06082v1` - [abs](http://arxiv.org/abs/2108.06082v1) - [pdf](http://arxiv.org/pdf/2108.06082v1)

> Binary code similarity detection is a fundamental technique for many security applications such as vulnerability search, patch analysis, and malware detection. There is an increasing need to detect similar code for vulnerability search across architectures with the increase of critical vulnerabilities in IoT devices. The variety of IoT hardware architectures and software platforms requires to capture semantic equivalence of code fragments in the similarity detection. However, existing approaches are insufficient in capturing the semantic similarity. We notice that the abstract syntax tree (AST) of a function contains rich semantic information. Inspired by successful applications of natural language processing technologies in sentence semantic understanding, we propose a deep learning-based AST-encoding method, named ASTERIA, to measure the semantic equivalence of functions in different platforms. Our method leverages the Tree-LSTM network to learn the semantic representation of a function from its AST. Then the similarity detection can be conducted efficiently and accurately by measuring the similarity between two representation vectors. We have implemented an open-source prototype of ASTERIA. The Tree-LSTM model is trained on a dataset with 1,022,616 function pairs and evaluated on a dataset with 95,078 function pairs. Evaluation results show that our method outperforms the AST-based tool Diaphora and the-state-of-art method Gemini by large margins with respect to the binary similarity detection. And our method is several orders of magnitude faster than Diaphora and Gemini for the similarity calculation. In the application of vulnerability search, our tool successfully identified 75 vulnerable functions in 5,979 IoT firmware images.

</details>

<details>

<summary>2021-08-13 07:52:52 - Enhancing the Transferability of Adversarial Attacks through Variance Tuning</summary>

- *Xiaosen Wang, Kun He*

- `2103.15571v3` - [abs](http://arxiv.org/abs/2103.15571v3) - [pdf](http://arxiv.org/pdf/2103.15571v3)

> Deep neural networks are vulnerable to adversarial examples that mislead the models with imperceptible perturbations. Though adversarial attacks have achieved incredible success rates in the white-box setting, most existing adversaries often exhibit weak transferability in the black-box setting, especially under the scenario of attacking models with defense mechanisms. In this work, we propose a new method called variance tuning to enhance the class of iterative gradient based attack methods and improve their attack transferability. Specifically, at each iteration for the gradient calculation, instead of directly using the current gradient for the momentum accumulation, we further consider the gradient variance of the previous iteration to tune the current gradient so as to stabilize the update direction and escape from poor local optima. Empirical results on the standard ImageNet dataset demonstrate that our method could significantly improve the transferability of gradient-based adversarial attacks. Besides, our method could be used to attack ensemble models or be integrated with various input transformations. Incorporating variance tuning with input transformations on iterative gradient-based attacks in the multi-model setting, the integrated method could achieve an average success rate of 90.1% against nine advanced defense methods, improving the current best attack performance significantly by 85.1% . Code is available at https://github.com/JHL-HUST/VT.

</details>

<details>

<summary>2021-08-13 15:19:55 - IncBL: Incremental Bug Localization</summary>

- *Zhou Yang, Jieke Shi, Shaowei Wang, David Lo*

- `2106.07413v2` - [abs](http://arxiv.org/abs/2106.07413v2) - [pdf](http://arxiv.org/pdf/2106.07413v2)

> Numerous efforts have been invested in improving the effectiveness of bug localization techniques, whereas little attention is paid to making these tools run more efficiently in continuously evolving software repositories. This paper first analyzes the information retrieval model behind a classic bug localization tool, BugLocator, and builds a mathematical foundation illustrating that the model can be updated incrementally when codebase or bug reports evolve. Then, we present IncBL, a tool for Incremental Bug Localization in evolving software repositories. IncBL is evaluated on the Bugzbook dataset, and the results show that IncBL can significantly reduce the running time by 77.79% on average compared with the re-computing the model, while maintaining the same level of accuracy. We also implement IncBL as a Github App that can be easily integrated into open-source projects on GitHub. Users can deploy and use IncBL locally as well. The demo video for IncBL can be viewed at https://youtu.be/G4gMuvlJSb0, and the source code can be found at https://github.com/soarsmu/IncBL.

</details>

<details>

<summary>2021-08-13 15:36:50 - Code Perfumes: Reporting Good Code to Encourage Learners</summary>

- *Florian Obermüller, Lena Bloch, Luisa Greifenstein, Ute Heuer, Gordon Fraser*

- `2108.06289v1` - [abs](http://arxiv.org/abs/2108.06289v1) - [pdf](http://arxiv.org/pdf/2108.06289v1)

> Block-based programming languages like Scratch enable children to be creative while learning to program. Even though the block-based approach simplifies the creation of programs, learning to program can nevertheless be challenging. Automated tools such as linters therefore support learners by providing feedback about potential bugs or code smells in their programs. Even when this feedback is elaborate and constructive, it still represents purely negative criticism and by construction ignores what learners have done correctly in their programs. In this paper we introduce an orthogonal approach to linting: We complement the criticism produced by a linter with positive feedback. We introduce the concept of code perfumes as the counterpart to code smells, indicating the correct application of programming practices considered to be good. By analysing not only what learners did wrong but also what they did right we hope to encourage learners, to provide teachers and students a better understanding of learners' progress, and to support the adoption of automated feedback tools. Using a catalogue of 25 code perfumes for Scratch, we empirically demonstrate that these represent frequent practices in Scratch, and we find that better programs indeed contain more code perfumes.

</details>

<details>

<summary>2021-08-13 20:34:50 - Quantum and semi-quantum sealed-bid auction: Vulnerabilities and advantages</summary>

- *Pramod Asagodu, Kishore Thapliyal, Anirban Pathak*

- `2108.06388v1` - [abs](http://arxiv.org/abs/2108.06388v1) - [pdf](http://arxiv.org/pdf/2108.06388v1)

> A family of existing protocols for quantum sealed-bid auction is critically analyzed, and it is shown that they are vulnerable under several attacks (e.g., the participant's and non-participant's attacks as well as the collusion attack of participants) and some of the claims made in these works are not correct. We obtained the bounds on the success probability of an eavesdropper in accessing the sealed-bids. Further, realizing the role of secure sealed-bid auction in the reduction of corruption, a new protocol for sealed-bid auction is proposed which is semi-quantum in nature, where the bidders do not have quantum resources but they can perform classical operations on the quantum states. The security of the proposed protocol is established against a set of attacks, and thus it is established that the proposed protocol is free from the vulnerabilities reported here in the context of the existing protocols.

</details>

<details>

<summary>2021-08-13 22:34:13 - Amata: An Annealing Mechanism for Adversarial Training Acceleration</summary>

- *Nanyang Ye, Qianxiao Li, Xiao-Yun Zhou, Zhanxing Zhu*

- `2012.08112v3` - [abs](http://arxiv.org/abs/2012.08112v3) - [pdf](http://arxiv.org/pdf/2012.08112v3)

> Despite the empirical success in various domains, it has been revealed that deep neural networks are vulnerable to maliciously perturbed input data that much degrade their performance. This is known as adversarial attacks. To counter adversarial attacks, adversarial training formulated as a form of robust optimization has been demonstrated to be effective. However, conducting adversarial training brings much computational overhead compared with standard training. In order to reduce the computational cost, we propose an annealing mechanism, Amata, to reduce the overhead associated with adversarial training. The proposed Amata is provably convergent, well-motivated from the lens of optimal control theory and can be combined with existing acceleration methods to further enhance performance. It is demonstrated that on standard datasets, Amata can achieve similar or better robustness with around 1/3 to 1/2 the computational time compared with traditional methods. In addition, Amata can be incorporated into other adversarial training acceleration algorithms (e.g. YOPO, Free, Fast, and ATTA), which leads to further reduction in computational time on large-scale problems.

</details>

<details>

<summary>2021-08-14 07:27:49 - Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems</summary>

- *Shangyu Xie, Han Wang, Yu Kong, Yuan Hong*

- `2107.04284v2` - [abs](http://arxiv.org/abs/2107.04284v2) - [pdf](http://arxiv.org/pdf/2107.04284v2)

> Widely deployed deep neural network (DNN) models have been proven to be vulnerable to adversarial perturbations in many applications (e.g., image, audio and text classifications). To date, there are only a few adversarial perturbations proposed to deviate the DNN models in video recognition systems by simply injecting 2D perturbations into video frames. However, such attacks may overly perturb the videos without learning the spatio-temporal features (across temporal frames), which are commonly extracted by DNN models for video recognition. To our best knowledge, we propose the first black-box attack framework that generates universal 3-dimensional (U3D) perturbations to subvert a variety of video recognition systems. U3D has many advantages, such as (1) as the transfer-based attack, U3D can universally attack multiple DNN models for video recognition without accessing to the target DNN model; (2) the high transferability of U3D makes such universal black-box attack easy-to-launch, which can be further enhanced by integrating queries over the target model when necessary; (3) U3D ensures human-imperceptibility; (4) U3D can bypass the existing state-of-the-art defense schemes; (5) U3D can be efficiently generated with a few pre-learned parameters, and then immediately injected to attack real-time DNN-based video recognition systems. We have conducted extensive experiments to evaluate U3D on multiple DNN models and three large-scale video datasets. The experimental results demonstrate its superiority and practicality.

</details>

<details>

<summary>2021-08-14 17:08:03 - Few-Sample Named Entity Recognition for Security Vulnerability Reports by Fine-Tuning Pre-Trained Language Models</summary>

- *Guanqun Yang, Shay Dineen, Zhipeng Lin, Xueqing Liu*

- `2108.06590v1` - [abs](http://arxiv.org/abs/2108.06590v1) - [pdf](http://arxiv.org/pdf/2108.06590v1)

> Public security vulnerability reports (e.g., CVE reports) play an important role in the maintenance of computer and network systems. Security companies and administrators rely on information from these reports to prioritize tasks on developing and deploying patches to their customers. Since these reports are unstructured texts, automatic information extraction (IE) can help scale up the processing by converting the unstructured reports to structured forms, e.g., software names and versions and vulnerability types. Existing works on automated IE for security vulnerability reports often rely on a large number of labeled training samples. However, creating massive labeled training set is both expensive and time consuming. In this work, for the first time, we propose to investigate this problem where only a small number of labeled training samples are available. In particular, we investigate the performance of fine-tuning several state-of-the-art pre-trained language models on our small training dataset. The results show that with pre-trained language models and carefully tuned hyperparameters, we have reached or slightly outperformed the state-of-the-art system on this task. Consistent with previous two-step process of first fine-tuning on main category and then transfer learning to others as in [7], if otherwise following our proposed approach, the number of required labeled samples substantially decrease in both stages: 90% reduction in fine-tuning from 5758 to 576,and 88.8% reduction in transfer learning with 64 labeled samples per category. Our experiments thus demonstrate the effectiveness of few-sample learning on NER for security vulnerability report. This result opens up multiple research opportunities for few-sample learning for security vulnerability reports, which is discussed in the paper. Code: https://github.com/guanqun-yang/FewVulnerability.

</details>

<details>

<summary>2021-08-15 19:37:38 - Evaluating Graph Vulnerability and Robustness using TIGER</summary>

- *Scott Freitas, Diyi Yang, Srijan Kumar, Hanghang Tong, Duen Horng Chau*

- `2006.05648v2` - [abs](http://arxiv.org/abs/2006.05648v2) - [pdf](http://arxiv.org/pdf/2006.05648v2)

> Network robustness plays a crucial role in our understanding of complex interconnected systems such as transportation, communication, and computer networks. While significant research has been conducted in the area of network robustness, no comprehensive open-source toolbox currently exists to assist researchers and practitioners in this important topic. This lack of available tools hinders reproducibility and examination of existing work, development of new research, and dissemination of new ideas. We contribute TIGER, an open-sourced Python toolbox to address these challenges. TIGER contains 22 graph robustness measures with both original and fast approximate versions; 17 failure and attack strategies; 15 heuristic and optimization-based defense techniques; and 4 simulation tools. By democratizing the tools required to study network robustness, our goal is to assist researchers and practitioners in analyzing their own networks; and facilitate the development of new research in the field. TIGER has been integrated into the Nvidia Data Science Teaching Kit available to educators across the world; and Georgia Tech's Data and Visual Analytics class with over 1,000 students. TIGER is open sourced at: https://github.com/safreita1/TIGER

</details>

<details>

<summary>2021-08-16 03:58:00 - Neural Architecture Dilation for Adversarial Robustness</summary>

- *Yanxi Li, Zhaohui Yang, Yunhe Wang, Chang Xu*

- `2108.06885v1` - [abs](http://arxiv.org/abs/2108.06885v1) - [pdf](http://arxiv.org/pdf/2108.06885v1)

> With the tremendous advances in the architecture and scale of convolutional neural networks (CNNs) over the past few decades, they can easily reach or even exceed the performance of humans in certain tasks. However, a recently discovered shortcoming of CNNs is that they are vulnerable to adversarial attacks. Although the adversarial robustness of CNNs can be improved by adversarial training, there is a trade-off between standard accuracy and adversarial robustness. From the neural architecture perspective, this paper aims to improve the adversarial robustness of the backbone CNNs that have a satisfactory accuracy. Under a minimal computational overhead, the introduction of a dilation architecture is expected to be friendly with the standard performance of the backbone CNN while pursuing adversarial robustness. Theoretical analyses on the standard and adversarial error bounds naturally motivate the proposed neural architecture dilation algorithm. Experimental results on real-world datasets and benchmark neural networks demonstrate the effectiveness of the proposed algorithm to balance the accuracy and adversarial robustness.

</details>

<details>

<summary>2021-08-16 05:57:01 - A Novel Attribute Reconstruction Attack in Federated Learning</summary>

- *Lingjuan Lyu, Chen Chen*

- `2108.06910v1` - [abs](http://arxiv.org/abs/2108.06910v1) - [pdf](http://arxiv.org/pdf/2108.06910v1)

> Federated learning (FL) emerged as a promising learning paradigm to enable a multitude of participants to construct a joint ML model without exposing their private training data. Existing FL designs have been shown to exhibit vulnerabilities which can be exploited by adversaries both within and outside of the system to compromise data privacy. However, most current works conduct attacks by leveraging gradients on a small batch of data, which is less practical in FL. In this work, we consider a more practical and interesting scenario in which participants share their epoch-averaged gradients (share gradients after at least 1 epoch of local training) rather than per-example or small batch-averaged gradients as in previous works. We perform the first systematic evaluation of attribute reconstruction attack (ARA) launched by the malicious server in the FL system, and empirically demonstrate that the shared epoch-averaged local model gradients can reveal sensitive attributes of local training data of any victim participant. To achieve this goal, we develop a more effective and efficient gradient matching based method called cos-matching to reconstruct the training data attributes. We evaluate our attacks on a variety of real-world datasets, scenarios, assumptions. Our experiments show that our proposed method achieves better attribute attack performance than most existing baselines.

</details>

<details>

<summary>2021-08-16 12:43:38 - Effects of Hints on Debugging Scratch Programs: An Empirical Study with Primary School Teachers in Training</summary>

- *Luisa Greifenstein, Florian Obermüller, Ewald Wasmeier, Ute Heuer, Gordon Fraser*

- `2108.07052v1` - [abs](http://arxiv.org/abs/2108.07052v1) - [pdf](http://arxiv.org/pdf/2108.07052v1)

> Bugs in learners' programs are often the result of fundamental misconceptions. Teachers frequently face the challenge of first having to understand such bugs, and then suggest ways to fix them. In order to enable teachers to do so effectively and efficiently, it is desirable to support them in recognising and fixing bugs. Misconceptions often lead to recurring patterns of similar bugs, enabling automated tools to provide this support in terms of hints on occurrences of common bug patterns. In this paper, we investigate to what extent the hints improve the effectiveness and efficiency of teachers in debugging learners' programs using a cohort of 163 primary school teachers in training, tasked to correct buggy Scratch programs, with and without hints on bug patterns. Our experiment suggests that automatically generated hints can reduce the effort of finding and fixing bugs from 8.66 to 5.24 minutes, while increasing the effectiveness by 34% more correct solutions. While this improvement is convincing, arguably teachers in training might first need to learn debugging "the hard way" to not miss the opportunity to learn by relying on tools. We therefore investigate whether the use of hints during training affects their ability to recognise and fix bugs without hints. Our experiment provides no significant evidence that either learning to debug with hints or learning to debug "the hard way" leads to better learning effects. Overall, this suggests that bug patterns might be a useful concept to include in the curriculum for teachers in training, while tool-support to recognise these patterns is desirable for teachers in practice.

</details>

<details>

<summary>2021-08-16 12:54:36 - Detecting and interpreting faults in vulnerable power grids with machine learning</summary>

- *Odin Foldvik Eikeland, Inga Setså Holmstrand, Sigurd Bakkejord, Matteo Chiesa, Filippo Maria Bianchi*

- `2108.07060v1` - [abs](http://arxiv.org/abs/2108.07060v1) - [pdf](http://arxiv.org/pdf/2108.07060v1)

> Unscheduled power disturbances cause severe consequences both for customers and grid operators. To defend against such events, it is necessary to identify the causes of interruptions in the power distribution network. In this work, we focus on the power grid of a Norwegian community in the Arctic that experiences several faults whose sources are unknown. First, we construct a data set consisting of relevant meteorological data and information about the current power quality logged by power-quality meters. Then, we adopt machine-learning techniques to predict the occurrence of faults. Experimental results show that both linear and non-linear classifiers achieve good classification performance. This indicates that the considered power-quality and weather variables explain well the power disturbances. Interpreting the decision process of the classifiers provides valuable insights to understand the main causes of disturbances. Traditional features selection methods can only indicate which are the variables that, on average, mostly explain the fault occurrences in the dataset. Besides providing such a global interpretation, it is also important to identify the specific set of variables that explain each individual fault. To address this challenge, we adopt a recent technique to interpret the decision process of a deep learning model, called Integrated Gradients. The proposed approach allows to gain detailed insights on the occurrence of a specific fault, which are valuable for the distribution system operators to implement strategies to prevent and mitigate power disturbances.

</details>

<details>

<summary>2021-08-16 13:19:32 - My Fuzzer Beats Them All! Developing a Framework for Fair Evaluation and Comparison of Fuzzers</summary>

- *David Paaßen, Sebastian Surminski, Michael Rodler, Lucas Davi*

- `2108.07076v1` - [abs](http://arxiv.org/abs/2108.07076v1) - [pdf](http://arxiv.org/pdf/2108.07076v1)

> Fuzzing has become one of the most popular techniques to identify bugs in software. To improve the fuzzing process, a plethora of techniques have recently appeared in academic literature. However, evaluating and comparing these techniques is challenging as fuzzers depend on randomness when generating test inputs. Commonly, existing evaluations only partially follow best practices for fuzzing evaluations. We argue that the reason for this are twofold. First, it is unclear if the proposed guidelines are necessary due to the lack of comprehensive empirical data in the case of fuzz testing. Second, there does not yet exist a framework that integrates statistical evaluation techniques to enable fair comparison of fuzzers. To address these limitations, we introduce a novel fuzzing evaluation framework called SENF (Statistical EvaluatioN of Fuzzers). We demonstrate the practical applicability of our framework by utilizing the most wide-spread fuzzer AFL as our baseline fuzzer and exploring the impact of different evaluation parameters (e.g., the number of repetitions or run-time), compilers, seeds, and fuzzing strategies. Using our evaluation framework, we show that supposedly small changes of the parameters can have a major influence on the measured performance of a fuzzer.

</details>

<details>

<summary>2021-08-16 13:40:01 - Identifying and Exploiting Structures for Reliable Deep Learning</summary>

- *Amartya Sanyal*

- `2108.07083v1` - [abs](http://arxiv.org/abs/2108.07083v1) - [pdf](http://arxiv.org/pdf/2108.07083v1)

> Deep learning research has recently witnessed an impressively fast-paced progress in a wide range of tasks including computer vision, natural language processing, and reinforcement learning. The extraordinary performance of these systems often gives the impression that they can be used to revolutionise our lives for the better. However, as recent works point out, these systems suffer from several issues that make them unreliable for use in the real world, including vulnerability to adversarial attacks (Szegedy et al. [248]), tendency to memorise noise (Zhang et al. [292]), being over-confident on incorrect predictions (miscalibration) (Guo et al. [99]), and unsuitability for handling private data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of these issues in detail, investigate their causes, and propose computationally cheap algorithms for mitigating them in practice. To do this, we identify structures in deep neural networks that can be exploited to mitigate the above causes of unreliability of deep learning algorithms.

</details>

<details>

<summary>2021-08-16 18:49:06 - A Content-Based Deep Intrusion Detection System</summary>

- *Mahdi Soltani, Mahdi Jafari Siavoshani, Amir Hossein Jahangir*

- `2001.05009v2` - [abs](http://arxiv.org/abs/2001.05009v2) - [pdf](http://arxiv.org/pdf/2001.05009v2)

> The growing number of Internet users and the prevalence of web applications make it necessary to deal with very complex software and applications in the network. This results in an increasing number of new vulnerabilities in the systems, and leading to an increase in cyber threats and, in particular, zero-day attacks. The cost of generating appropriate signatures for these attacks is a potential motive for using machine learning-based methodologies. Although there are many studies on using learning-based methods for attack detection, they generally use extracted features and overlook raw contents. This approach can lessen the performance of detection systems against content-based attacks like SQL injection, Cross-site Scripting (XSS), and various viruses.   In this work, we propose a framework, called deep intrusion detection (DID) system, that uses the pure content of traffic flows in addition to traffic metadata in the learning and detection phases of a passive DNN IDS. To this end, we deploy and evaluate an offline IDS following the framework using LSTM as a deep learning technique. Due to the inherent nature of deep learning, it can process high dimensional data content and, accordingly, discover the sophisticated relations between the auto extracted features of the traffic. To evaluate the proposed DID system, we use the CIC-IDS2017 and CSE-CIC-IDS2018 datasets. The evaluation metrics, such as precision and recall, reach $0.992$ and $0.998$ on CIC-IDS2017, and $0.933$ and $0.923$ on CSE-CIC-IDS2018 respectively, which show the high performance of the proposed DID method.

</details>

<details>

<summary>2021-08-17 05:56:58 - Group-wise Inhibition based Feature Regularization for Robust Classification</summary>

- *Haozhe Liu, Haoqian Wu, Weicheng Xie, Feng Liu, Linlin Shen*

- `2103.02152v3` - [abs](http://arxiv.org/abs/2103.02152v3) - [pdf](http://arxiv.org/pdf/2103.02152v3)

> The convolutional neural network (CNN) is vulnerable to degraded images with even very small variations (e.g. corrupted and adversarial samples). One of the possible reasons is that CNN pays more attention to the most discriminative regions, but ignores the auxiliary features when learning, leading to the lack of feature diversity for final judgment. In our method, we propose to dynamically suppress significant activation values of CNN by group-wise inhibition, but not fixedly or randomly handle them when training. The feature maps with different activation distribution are then processed separately to take the feature independence into account. CNN is finally guided to learn richer discriminative features hierarchically for robust classification according to the proposed regularization. Our method is comprehensively evaluated under multiple settings, including classification against corruptions, adversarial attacks and low data regime. Extensive experimental results show that the proposed method can achieve significant improvements in terms of both robustness and generalization performances, when compared with the state-of-the-art methods. Code is available at https://github.com/LinusWu/TENET_Training.

</details>

<details>

<summary>2021-08-17 10:48:04 - Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack</summary>

- *Jaehui Hwang, Jun-Hyuk Kim, Jun-Ho Choi, Jong-Seok Lee*

- `2011.14585v2` - [abs](http://arxiv.org/abs/2011.14585v2) - [pdf](http://arxiv.org/pdf/2011.14585v2)

> The video-based action recognition task has been extensively studied in recent years. In this paper, we study the structural vulnerability of deep learning-based action recognition models against the adversarial attack using the one frame attack that adds an inconspicuous perturbation to only a single frame of a given video clip. Our analysis shows that the models are highly vulnerable against the one frame attack due to their structural properties. Experiments demonstrate high fooling rates and inconspicuous characteristics of the attack. Furthermore, we show that strong universal one frame perturbations can be obtained under various scenarios. Our work raises the serious issue of adversarial vulnerability of the state-of-the-art action recognition models in various perspectives.

</details>

<details>

<summary>2021-08-18 03:42:05 - Verifying Low-dimensional Input Neural Networks via Input Quantization</summary>

- *Kai Jia, Martin Rinard*

- `2108.07961v1` - [abs](http://arxiv.org/abs/2108.07961v1) - [pdf](http://arxiv.org/pdf/2108.07961v1)

> Deep neural networks are an attractive tool for compressing the control policy lookup tables in systems such as the Airborne Collision Avoidance System (ACAS). It is vital to ensure the safety of such neural controllers via verification techniques. The problem of analyzing ACAS Xu networks has motivated many successful neural network verifiers. These verifiers typically analyze the internal computation of neural networks to decide whether a property regarding the input/output holds. The intrinsic complexity of neural network computation renders such verifiers slow to run and vulnerable to floating-point error.   This paper revisits the original problem of verifying ACAS Xu networks. The networks take low-dimensional sensory inputs with training data provided by a precomputed lookup table. We propose to prepend an input quantization layer to the network. Quantization allows efficient verification via input state enumeration, whose complexity is bounded by the size of the quantization space. Quantization is equivalent to nearest-neighbor interpolation at run time, which has been shown to provide acceptable accuracy for ACAS in simulation. Moreover, our technique can deliver exact verification results immune to floating-point error if we directly enumerate the network outputs on the target inference implementation or on an accurate simulation of the target implementation.

</details>

<details>

<summary>2021-08-18 06:29:57 - Admix: Enhancing the Transferability of Adversarial Attacks</summary>

- *Xiaosen Wang, Xuanran He, Jingdong Wang, Kun He*

- `2102.00436v3` - [abs](http://arxiv.org/abs/2102.00436v3) - [pdf](http://arxiv.org/pdf/2102.00436v3)

> Deep neural networks are known to be extremely vulnerable to adversarial examples under white-box setting. Moreover, the malicious adversaries crafted on the surrogate (source) model often exhibit black-box transferability on other models with the same learning task but having different architectures. Recently, various methods are proposed to boost the adversarial transferability, among which the input transformation is one of the most effective approaches. We investigate in this direction and observe that existing transformations are all applied on a single image, which might limit the adversarial transferability. To this end, we propose a new input transformation based attack method called Admix that considers the input image and a set of images randomly sampled from other categories. Instead of directly calculating the gradient on the original input, Admix calculates the gradient on the input image admixed with a small portion of each add-in image while using the original label of the input to craft more transferable adversaries. Empirical evaluations on standard ImageNet dataset demonstrate that Admix could achieve significantly better transferability than existing input transformation methods under both single model setting and ensemble-model setting. By incorporating with existing input transformations, our method could further improve the transferability and outperforms the state-of-the-art combination of input transformations by a clear margin when attacking nine advanced defense models under ensemble-model setting. Code is available at https://github.com/JHL-HUST/Admix.

</details>

<details>

<summary>2021-08-18 08:43:36 - DeepCVA: Automated Commit-level Vulnerability Assessment with Deep Multi-task Learning</summary>

- *Triet H. M. Le, David Hin, Roland Croft, M. Ali Babar*

- `2108.08041v1` - [abs](http://arxiv.org/abs/2108.08041v1) - [pdf](http://arxiv.org/pdf/2108.08041v1)

> It is increasingly suggested to identify Software Vulnerabilities (SVs) in code commits to give early warnings about potential security risks. However, there is a lack of effort to assess vulnerability-contributing commits right after they are detected to provide timely information about the exploitability, impact and severity of SVs. Such information is important to plan and prioritize the mitigation for the identified SVs. We propose a novel Deep multi-task learning model, DeepCVA, to automate seven Commit-level Vulnerability Assessment tasks simultaneously based on Common Vulnerability Scoring System (CVSS) metrics. We conduct large-scale experiments on 1,229 vulnerability-contributing commits containing 542 different SVs in 246 real-world software projects to evaluate the effectiveness and efficiency of our model. We show that DeepCVA is the best-performing model with 38% to 59.8% higher Matthews Correlation Coefficient than many supervised and unsupervised baseline models. DeepCVA also requires 6.3 times less training and validation time than seven cumulative assessment models, leading to significantly less model maintenance cost as well. Overall, DeepCVA presents the first effective and efficient solution to automatically assess SVs early in software systems.

</details>

<details>

<summary>2021-08-18 16:35:26 - Evaluation of individual attributes associated with shared HIV risk behaviors among two network-based studies of people who inject drugs</summary>

- *Valerie Ryan, TingFang Lee, Ashley L. Buchanan, Natallia V. Katenka, Samuel R. Friedman, Georgios Nikolopoulos*

- `2108.12287v1` - [abs](http://arxiv.org/abs/2108.12287v1) - [pdf](http://arxiv.org/pdf/2108.12287v1)

> Social context plays an important role in perpetuating or reducing HIV risk behaviors. This study analyzed the network and individual attributes that were associated with the likelihood that people who inject drugs (PWID) will engage in HIV risk behaviors with one another. We analyze data collected in the Social Risk Factors and HIV Risk Study (SFHR) and Transmission Reduction Intervention Project (TRIP) to perform the analysis. Exponential random graph models were used to determine which attributes were associated with the likelihood of people engaging in HIV risk behaviors, such as injection behaviors that are associated with one another, among PWID. Results across all models and across both data sets indicated that people were more likely to engage in risk behaviors with others who were similar to them in some way (e.g., were the same sex, race/ethnicity, living conditions). In both SFHR and TRIP, we explore the effects of missingness at individual and network levels on the likelihood of individuals to engage in HIV risk behaviors among PWID. In this study, we found that known individual-level risk factors, including housing instability and race/ethnicity, are also important factors in determining the structure of the observed network among PWID. Future development of interventions should consider not only individual risk factors, but communities and social influences leaving individuals vulnerable to HIV risk.

</details>

<details>

<summary>2021-08-18 17:56:18 - Membership Inference Attacks are Easier on Difficult Problems</summary>

- *Avital Shafran, Shmuel Peleg, Yedid Hoshen*

- `2102.07762v3` - [abs](http://arxiv.org/abs/2102.07762v3) - [pdf](http://arxiv.org/pdf/2102.07762v3)

> Membership inference attacks (MIA) try to detect if data samples were used to train a neural network model, e.g. to detect copyright abuses. We show that models with higher dimensional input and output are more vulnerable to MIA, and address in more detail models for image translation and semantic segmentation, including medical image segmentation. We show that reconstruction-errors can lead to very effective MIA attacks as they are indicative of memorization. Unfortunately, reconstruction error alone is less effective at discriminating between non-predictable images used in training and easy to predict images that were never seen before. To overcome this, we propose using a novel predictability error that can be computed for each sample, and its computation does not require a training set. Our membership error, obtained by subtracting the predictability error from the reconstruction error, is shown to achieve high MIA accuracy on an extensive number of benchmarks.

</details>

<details>

<summary>2021-08-18 20:06:49 - Dependency Smells in JavaScript Projects</summary>

- *Abbas Javan Jafari, Diego Elias Costa, Rabe Abdalkareem, Emad Shihab, Nikolaos Tsantalis*

- `2010.14573v2` - [abs](http://arxiv.org/abs/2010.14573v2) - [pdf](http://arxiv.org/pdf/2010.14573v2)

> Dependency management in modern software development poses many challenges for developers who wish to stay up to date with the latest features and fixes whilst ensuring backwards compatibility. Project maintainers have opted for varied, and sometimes conflicting, approaches for maintaining their dependencies. Opting for unsuitable approaches can introduce bugs and vulnerabilities into the project, introduce breaking changes, cause extraneous installations, and reduce dependency understandability, making it harder for others to contribute effectively. In this paper, we empirically examine evidence of recurring dependency management issues (dependency smells). We look at the commit data for a dataset of 1,146 active JavaScript repositories to catalog, quantify and understand dependency smells. Through a series of surveys with practitioners, we identify and quantify seven dependency smells with varying degrees of popularity and investigate why they are introduced throughout project history. Our findings indicate that dependency smells are prevalent in JavaScript projects with two or more distinct smells appearing in 80% of the projects, but they generally infect a minority of a project's dependencies. Our observations show that the number of dependency smells tend to increase over time. Practitioners agree that dependency smells bring about many problems including security threats, bugs, dependency breakage, runtime errors, and other maintenance issues. These smells are generally introduced as developers react to dependency misbehaviour and the shortcomings of the npm ecosystem.

</details>

<details>

<summary>2021-08-19 00:52:10 - Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes</summary>

- *Mingjun Yin, Shasha Li, Zikui Cai, Chengyu Song, M. Salman Asif, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy*

- `2108.08421v1` - [abs](http://arxiv.org/abs/2108.08421v1) - [pdf](http://arxiv.org/pdf/2108.08421v1)

> Vision systems that deploy Deep Neural Networks (DNNs) are known to be vulnerable to adversarial examples. Recent research has shown that checking the intrinsic consistencies in the input data is a promising way to detect adversarial attacks (e.g., by checking the object co-occurrence relationships in complex scenes). However, existing approaches are tied to specific models and do not offer generalizability. Motivated by the observation that language descriptions of natural scene images have already captured the object co-occurrence relationships that can be learned by a language model, we develop a novel approach to perform context consistency checks using such language models. The distinguishing aspect of our approach is that it is independent of the deployed object detector and yet offers very high accuracy in terms of detecting adversarial examples in practical scenes with multiple objects.

</details>

<details>

<summary>2021-08-19 02:59:36 - BackREST: A Model-Based Feedback-Driven Greybox Fuzzer for Web Applications</summary>

- *François Gauthier, Behnaz Hassanshahi, Benjamin Selwyn-Smith, Trong Nhan Mai, Max Schlüter, Micah Williams*

- `2108.08455v1` - [abs](http://arxiv.org/abs/2108.08455v1) - [pdf](http://arxiv.org/pdf/2108.08455v1)

> Following the advent of the American Fuzzy Lop (AFL), fuzzing had a surge in popularity, and modern day fuzzers range from simple blackbox random input generators to complex whitebox concolic frameworks that are capable of deep program introspection. Web application fuzzers, however, did not benefit from the tremendous advancements in fuzzing for binary programs and remain largely blackbox in nature. This paper introduces BackREST, a fully automated, model-based, coverage- and taint-driven fuzzer that uses its feedback loops to find more critical vulnerabilities, faster (speedups between 7.4x and 25.9x). To model the server-side of web applications, BackREST automatically infers REST specifications through directed state-aware crawling. Comparing BackREST against three other web fuzzers on five large (>500 KLOC) Node.js applications shows how it consistently achieves comparable coverage while reporting more vulnerabilities than state-of-the-art. Finally, using BackREST, we uncovered nine 0-days, out of which six were not reported by any other fuzzer. All the 0-days have been disclosed and most are now public, including two in the highly popular Sequelize and Mongodb libraries.

</details>

<details>

<summary>2021-08-19 09:06:16 - Pruning in the Face of Adversaries</summary>

- *Florian Merkle, Maximilian Samsinger, Pascal Schöttle*

- `2108.08560v1` - [abs](http://arxiv.org/abs/2108.08560v1) - [pdf](http://arxiv.org/pdf/2108.08560v1)

> The vulnerability of deep neural networks against adversarial examples - inputs with small imperceptible perturbations - has gained a lot of attention in the research community recently. Simultaneously, the number of parameters of state-of-the-art deep learning models has been growing massively, with implications on the memory and computational resources required to train and deploy such models. One approach to control the size of neural networks is retrospectively reducing the number of parameters, so-called neural network pruning. Available research on the impact of neural network pruning on the adversarial robustness is fragmentary and often does not adhere to established principles of robustness evaluation. We close this gap by evaluating the robustness of pruned models against L-0, L-2 and L-infinity attacks for a wide range of attack strengths, several architectures, data sets, pruning methods, and compression rates. Our results confirm that neural network pruning and adversarial robustness are not mutually exclusive. Instead, sweet spots can be found that are favorable in terms of model size and adversarial robustness. Furthermore, we extend our analysis to situations that incorporate additional assumptions on the adversarial scenario and show that depending on the situation, different strategies are optimal.

</details>

<details>

<summary>2021-08-19 11:22:05 - Machine Learning for Security in Vehicular Networks: A Comprehensive Survey</summary>

- *Anum Talpur, Mohan Gurusamy*

- `2105.15035v2` - [abs](http://arxiv.org/abs/2105.15035v2) - [pdf](http://arxiv.org/pdf/2105.15035v2)

> Machine Learning (ML) has emerged as an attractive and viable technique to provide effective solutions for a wide range of application domains. An important application domain is vehicular networks wherein ML-based approaches are found to be very useful to address various problems. The use of wireless communication between vehicular nodes and/or infrastructure makes it vulnerable to different types of attacks. In this regard, ML and its variants are gaining popularity to detect attacks and deal with different kinds of security issues in vehicular communication. In this paper, we present a comprehensive survey of ML-based techniques for different security issues in vehicular networks. We first briefly introduce the basics of vehicular networks and different types of communications. Apart from the traditional vehicular networks, we also consider modern vehicular network architectures. We propose a taxonomy of security attacks in vehicular networks and discuss various security challenges and requirements. We classify the ML techniques developed in the literature according to their use in vehicular network applications. We explain the solution approaches and working principles of these ML techniques in addressing various security challenges and provide insightful discussion. The limitations and challenges in using ML-based methods in vehicular networks are discussed. Finally, we present observations and lessons learned before we conclude our work.

</details>

<details>

<summary>2021-08-19 11:44:21 - An Innovative Attack Modelling and Attack Detection Approach for a Waiting Time-based Adaptive Traffic Signal Controller</summary>

- *Sagar Dasgupta, Courtland Hollis, Mizanur Rahman, Travis Atkison*

- `2108.08627v1` - [abs](http://arxiv.org/abs/2108.08627v1) - [pdf](http://arxiv.org/pdf/2108.08627v1)

> An adaptive traffic signal controller (ATSC) combined with a connected vehicle (CV) concept uses real-time vehicle trajectory data to regulate green time and has the ability to reduce intersection waiting time significantly and thereby improve travel time in a signalized corridor. However, the CV-based ATSC increases the size of the surface vulnerable to potential cyber-attack, allowing an attacker to generate disastrous traffic congestion in a roadway network. An attacker can congest a route by generating fake vehicles by maintaining traffic and car-following rules at a slow rate so that the signal timing and phase change without having any abrupt changes in number of vehicles. Because of the adaptive nature of ATSC, it is a challenge to model this kind of attack and also to develop a strategy for detection. This paper introduces an innovative "slow poisoning" cyberattack for a waiting time based ATSC algorithm and a corresponding detection strategy. Thus, the objectives of this paper are to: (i) develop a "slow poisoning" attack generation strategy for an ATSC, and (ii) develop a prediction-based "slow poisoning" attack detection strategy using a recurrent neural network -- i.e., long short-term memory model. We have generated a "slow poisoning" attack modeling strategy using a microscopic traffic simulator -- Simulation of Urban Mobility (SUMO) -- and used generated data from the simulation to develop both the attack model and detection model. Our analyses revealed that the attack strategy is effective in creating a congestion in an approach and detection strategy is able to flag the attack.

</details>

<details>

<summary>2021-08-19 13:45:55 - MESH: A Memory-Efficient Safe Heap for C/C++</summary>

- *Emanuel Q. Vintila, Philipp Zieris, Julian Horsch*

- `2108.08683v1` - [abs](http://arxiv.org/abs/2108.08683v1) - [pdf](http://arxiv.org/pdf/2108.08683v1)

> While memory corruption bugs stemming from the use of unsafe programming languages are an old and well-researched problem, the resulting vulnerabilities still dominate real-world exploitation today. Various mitigations have been proposed to alleviate the problem, mainly in the form of language dialects, static program analysis, and code or binary instrumentation. Solutions like AdressSanitizer (ASan) and Softbound/CETS have proven that the latter approach is very promising, being able to achieve memory safety without requiring manual source code adaptions, albeit suffering substantial performance and memory overheads. While performance overhead can be seen as a flexible constraint, extensive memory overheads can be prohibitive for the use of such solutions in memory-constrained environments. To address this problem, we propose MESH, a highly memory-efficient safe heap for C/C++. With its constant, very small memory overhead (configurable up to 2 MB on x86-64) and constant complexity for pointer access checking, MESH offers efficient, byte-precise spatial and temporal memory safety for memory-constrained scenarios. Without jeopardizing the security of safe heap objects, MESH is fully compatible with existing code and uninstrumented libraries, making it practical to use in heterogeneous environments. We show the feasibility of our approach with a full LLVM-based prototype supporting both major architectures, i.e., x86-64 and ARM64, in a Linux runtime environment. Our prototype evaluation shows that, compared to ASan and Softbound/CETS, MESH can achieve huge memory savings while preserving similar execution performance.

</details>

<details>

<summary>2021-08-19 20:20:34 - STRATA: Simple, Gradient-Free Attacks for Models of Code</summary>

- *Jacob M. Springer, Bryn Marie Reinstadler, Una-May O'Reilly*

- `2009.13562v2` - [abs](http://arxiv.org/abs/2009.13562v2) - [pdf](http://arxiv.org/pdf/2009.13562v2)

> Neural networks are well-known to be vulnerable to imperceptible perturbations in the input, called adversarial examples, that result in misclassification. Generating adversarial examples for source code poses an additional challenge compared to the domains of images and natural language, because source code perturbations must retain the functional meaning of the code. We identify a striking relationship between token frequency statistics and learned token embeddings: the L2 norm of learned token embeddings increases with the frequency of the token except for the highest-frequnecy tokens. We leverage this relationship to construct a simple and efficient gradient-free method for generating state-of-the-art adversarial examples on models of code. Our method empirically outperforms competing gradient-based methods with less information and less computational effort.

</details>

<details>

<summary>2021-08-20 02:33:40 - Software Security Patch Management -- A Systematic Literature Review of Challenges, Approaches, Tools and Practices</summary>

- *Nesara Dissanayake, Asangi Jayatilaka, Mansooreh Zahedi, M. Ali Babar*

- `2012.00544v3` - [abs](http://arxiv.org/abs/2012.00544v3) - [pdf](http://arxiv.org/pdf/2012.00544v3)

> Context: Software security patch management purports to support the process of patching known software security vulnerabilities. Given the increasing recognition of the importance of software security patch management, it is important and timely to systematically review and synthesise the relevant literature on this topic.   Objective: This paper aims at systematically reviewing the state of the art of software security patch management to identify the socio-technical challenges in this regard, reported solutions (i.e., approaches, tools, and practices), the rigour of the evaluation and the industrial relevance of the reported solutions, and to identify the gaps for future research.   Method: We conducted a systematic literature review of 72 studies published from 2002 to March 2020, with extended coverage until September 2020 through forward snowballing.   Results: We identify 14 socio-technical challenges, 18 solution approaches, tools and practices mapped onto the software security patch management process. We provide a mapping between the solutions and challenges to enable a reader to obtain a holistic overview of the gap areas. The findings also reveal that only 20.8% of the reported solutions have been rigorously evaluated in industrial settings.   Conclusion: Our results reveal that 50% of the common challenges have not been directly addressed in the solutions and that most of them (38.9%) address the challenges in one phase of the process, namely vulnerability scanning, assessment and prioritisation. Based on the results that highlight the important concerns in software security patch management and the lack of solutions, we recommend a list of future research directions. This study also provides useful insights about different opportunities for practitioners to adopt new solutions and understand the variations of their practical utility.

</details>

<details>

<summary>2021-08-20 06:14:25 - The Discrepancy Attack on Polyshard-ed Blockchains</summary>

- *Nastaran Abadi Khooshemehr, Mohammad Ali Maddah-Ali*

- `2102.02867v3` - [abs](http://arxiv.org/abs/2102.02867v3) - [pdf](http://arxiv.org/pdf/2102.02867v3)

> Sharding, i.e. splitting the miners or validators to form and run several subchains in parallel, is known as one of the main solutions to the scalability problem of blockchains. The drawback is that as the number of miners expanding each subchain becomes small, it becomes vulnerable to security attacks. To solve this problem, a framework, named as \textit{Polyshard}, has been proposed in which each validator verifies a coded combination of the blocks introduced by different subchains, thus helping to protect the security of all subchains. In this paper, we introduce an attack on Polyshard, called \textit{the discrepancy} attack, which is the result of malicious nodes controlling a few subchains and dispersing different blocks to different nodes. We show that this attack undermines the security of Polyshard and is undetectable in its current setting.

</details>

<details>

<summary>2021-08-20 09:27:28 - 5G System Security Analysis</summary>

- *Gerrit Holtrup, William Lacube, Dimitri Percia David, Alain Mermoud, Gérôme Bovet, Vincent Lenders*

- `2108.08700v2` - [abs](http://arxiv.org/abs/2108.08700v2) - [pdf](http://arxiv.org/pdf/2108.08700v2)

> Fifth generation mobile networks (5G) are currently being deployed by mobile operators around the globe. 5G acts as an enabler for various use cases and also improves the security and privacy over 4G and previous network generations. However, as recent security research has revealed, the standard still has security weaknesses that may be exploitable by attackers. In addition, the migration from 4G to 5G systems is taking place by first deploying 5G solutions in a non-standalone (NSA) manner where the first step of the 5G deployment is restricted to the new radio aspects of 5G, while the control of the user equipment is still based on 4G protocols, i.e. the core network is still the legacy 4G evolved packet core (EPC) network. As a result, many security vulnerabilities of 4G networks are still present in current 5G deployments. This paper presents a systematic risk analysis of standalone and non-standalone 5G networks. We first describe an overview of the 5G system specification and the new security features of 5G compared to 4G. Then, we define possible threats according to the STRIDE threat classification model and derive a risk matrix based on the likelihood and impact of 12 threat scenarios that affect the radio access and the network core. Finally, we discuss possible mitigations and security controls. Our analysis is generic and does not account for the specifics of particular 5G network vendors or operators. Further work is required to understand the security vulnerabilities and risks of specific 5G implementations and deployments.

</details>

<details>

<summary>2021-08-20 11:24:00 - TOUR: Dynamic Topic and Sentiment Analysis of User Reviews for Assisting App Release</summary>

- *Tianyi Yang, Cuiyun Gao, Jingya Zang, David Lo, Michael R. Lyu*

- `2103.15774v2` - [abs](http://arxiv.org/abs/2103.15774v2) - [pdf](http://arxiv.org/pdf/2103.15774v2)

> App reviews deliver user opinions and emerging issues (e.g., new bugs) about the app releases. Due to the dynamic nature of app reviews, topics and sentiment of the reviews would change along with app release versions. Although several studies have focused on summarizing user opinions by analyzing user sentiment towards app features, no practical tool is released. The large quantity of reviews and noise words also necessitates an automated tool for monitoring user reviews. In this paper, we introduce TOUR for dynamic TOpic and sentiment analysis of User Reviews. TOUR is able to (i) detect and summarize emerging app issues over app versions, (ii) identify user sentiment towards app features, and (iii) prioritize important user reviews for facilitating developers' examination. The core techniques of TOUR include the online topic modeling approach and sentiment prediction strategy. TOUR provides entries for developers to customize the hyper-parameters and the results are presented in an interactive way. We evaluate TOUR by conducting a developer survey that involves 15 developers, and all of them confirm the practical usefulness of the recommended feature changes by TOUR.

</details>

<details>

<summary>2021-08-20 14:36:44 - Defending Medical Image Diagnostics against Privacy Attacks using Generative Methods</summary>

- *William Paul, Yinzhi Cao, Miaomiao Zhang, Phil Burlina*

- `2103.03078v2` - [abs](http://arxiv.org/abs/2103.03078v2) - [pdf](http://arxiv.org/pdf/2103.03078v2)

> Machine learning (ML) models used in medical imaging diagnostics can be vulnerable to a variety of privacy attacks, including membership inference attacks, that lead to violations of regulations governing the use of medical data and threaten to compromise their effective deployment in the clinic. In contrast to most recent work in privacy-aware ML that has been focused on model alteration and post-processing steps, we propose here a novel and complementary scheme that enhances the security of medical data by controlling the data sharing process. We develop and evaluate a privacy defense protocol based on using a generative adversarial network (GAN) that allows a medical data sourcer (e.g. a hospital) to provide an external agent (a modeler) a proxy dataset synthesized from the original images, so that the resulting diagnostic systems made available to model consumers is rendered resilient to privacy attackers. We validate the proposed method on retinal diagnostics AI used for diabetic retinopathy that bears the risk of possibly leaking private information. To incorporate concerns of both privacy advocates and modelers, we introduce a metric to evaluate privacy and utility performance in combination, and demonstrate, using these novel and classical metrics, that our approach, by itself or in conjunction with other defenses, provides state of the art (SOTA) performance for defending against privacy attacks.

</details>

<details>

<summary>2021-08-20 16:30:31 - Mining Secure Behavior of Hardware Designs</summary>

- *Calvin Deutschbein*

- `2108.09249v1` - [abs](http://arxiv.org/abs/2108.09249v1) - [pdf](http://arxiv.org/pdf/2108.09249v1)

> Specification mining offers a solution by automating security specification for hardware. Specification miners use a form of machine learning to specify behaviors of a system by studying a system in execution. However, specification mining was first developed for use with software. Complex hardware designs offer unique challenges for this technique. Further, specification miners traditionally capture functional specifications without a notion of security, and may not use the specification logics necessary to describe some security requirements.   This work demonstrates specification mining for hardware security. On CISC architectures such as x86, I demonstrate that a miner partitioning the design state space along control signals discovers a specification that includes manually defined properties and, if followed, would secure CPU designs against Memory Sinkhole and SYSRET privilege escalation. For temporal properties, I demonstrate that a miner using security specific linear temporal logic (LTL) templates for specification detection may find properties that, if followed, would secure designs against historical documented security vulnerabilities and against potential future attacks targeting system initialization. For information--flow hyperproperties, I demonstrate that a miner may use Information Flow Tracking (IFT) to develop output properties containing designer specified information--flow security properties as well as properties that demonstrate a design does not contain certain Common Weakness Enumerations (CWEs).

</details>

<details>

<summary>2021-08-21 18:58:54 - Coverage Hole Detection for mmWave Networks: An Unsupervised Learning Approach</summary>

- *Chethan K. Anjinappa, Ismail Guvenc*

- `2108.07854v2` - [abs](http://arxiv.org/abs/2108.07854v2) - [pdf](http://arxiv.org/pdf/2108.07854v2)

> The utilization of millimeter-wave (mmWave) bands in 5G networks poses new challenges to network planning. Vulnerability to blockages at mmWave bands can cause coverage holes (CHs) in the radio environment, leading to radio link failure when a user enters these CHs. Detection of the CHs carries critical importance so that necessary remedies can be introduced to improve coverage. In this letter, we propose a novel approach to identify the CHs in an unsupervised fashion using a state-of-the-art manifold learning technique: uniform manifold approximation and projection. The key idea is to preserve the local-connectedness structure inherent in the collected unlabelled channel samples, such that the CHs from the service area are detectable. Our results on the DeepMIMO dataset scenario demonstrate that the proposed method can learn the structure within the data samples and provide visual holes in the low-dimensional embedding while preserving the CH boundaries. Once the CH boundary is determined in the low-dimensional embedding, channel-based localization techniques can be applied to these samples to obtain the geographical boundaries of the CHs.

</details>

<details>

<summary>2021-08-21 20:14:12 - A Survey on Common Threats in npm and PyPi Registries</summary>

- *Berkay Kaplan, Jingyu Qian*

- `2108.09576v1` - [abs](http://arxiv.org/abs/2108.09576v1) - [pdf](http://arxiv.org/pdf/2108.09576v1)

> Software engineers regularly use JavaScript and Python for both front-end and back-end automation tasks. On top of JavaScript and Python, there are several frameworks to facilitate automation tasks further. Some of these frameworks are Node Manager Package (npm) and Python Package Index (PyPi), which are open source (OS) package libraries. The public registries npm and PyPi use to host packages allow any user with a verified email to publish code. The lack of a comprehensive scanning tool when publishing to the registry creates security concerns. Users can report malicious code on the registry; however, attackers can still cause damage until they remove their tool from the platform. Furthermore, several packages depend on each other, making them more vulnerable to a bad package in the dependency tree. The heavy code reuse creates security artifacts developers have to consider, such as the package reach. This project will illustrate a high-level overview of common risks associated with OS registries and the package dependency structure. There are several attack types, such as typosquatting and combosquatting, in the OS package registries. Outdated packages pose a security risk, and we will examine the extent of technical lag present in the npm environment. In this paper, our main contribution consists of a survey of common threats in OS registries. Afterward, we will offer countermeasures to mitigate the risks presented. These remedies will heavily focus on the applications of Machine Learning (ML) to detect suspicious activities. To the best of our knowledge, the ML-focused countermeasures are the first proposed possible solutions to the security problems listed. In addition, this project is the first survey of threats in npm and PyPi, although several studies focus on a subset of threats.

</details>

<details>

<summary>2021-08-22 02:46:12 - Pick-Object-Attack: Type-Specific Adversarial Attack for Object Detection</summary>

- *Omid Mohamad Nezami, Akshay Chaturvedi, Mark Dras, Utpal Garain*

- `2006.03184v3` - [abs](http://arxiv.org/abs/2006.03184v3) - [pdf](http://arxiv.org/pdf/2006.03184v3)

> Many recent studies have shown that deep neural models are vulnerable to adversarial samples: images with imperceptible perturbations, for example, can fool image classifiers. In this paper, we present the first type-specific approach to generating adversarial examples for object detection, which entails detecting bounding boxes around multiple objects present in the image and classifying them at the same time, making it a harder task than against image classification. We specifically aim to attack the widely used Faster R-CNN by changing the predicted label for a particular object in an image: where prior work has targeted one specific object (a stop sign), we generalise to arbitrary objects, with the key challenge being the need to change the labels of all bounding boxes for all instances of that object type. To do so, we propose a novel method, named Pick-Object-Attack. Pick-Object-Attack successfully adds perturbations only to bounding boxes for the targeted object, preserving the labels of other detected objects in the image. In terms of perceptibility, the perturbations induced by the method are very small. Furthermore, for the first time, we examine the effect of adversarial attacks on object detection in terms of a downstream task, image captioning; we show that where a method that can modify all object types leads to very obvious changes in captions, the changes from our constrained attack are much less apparent.

</details>

<details>

<summary>2021-08-22 13:10:10 - Resilient ICT4D: Building and Sustaining our Community in Pandemic Times</summary>

- *Silvia Masiero, Petter Nielsen*

- `2108.09712v1` - [abs](http://arxiv.org/abs/2108.09712v1) - [pdf](http://arxiv.org/pdf/2108.09712v1)

> The impacts of the COVID-19 pandemic, disproportionally affecting vulnerable people and deepening pre-existing inequalities (Dreze, 2020; Qureshi, 2021), have interested the very same "development" processes that the IFIP Working Group 9.4 on the Implications of Information and Digital Technologies for Development has dealt with over time. A global development paradigm (Oldekop et al., 2020) has emerged in response to the global nature of the crisis, infusing new meaning in the spirit of "making a better world" with ICTs (Walsham, 2012) that always have characterised ICT4D research. Such a new meaning contextualises our research in the landscape of the first pandemic of the datafied society (Milan & Trere, 2020), coming to terms with the silencing of narratives from the margins within the pandemic (Milan et al., 2021) - in Qureshi's (2021) words, a "pandemics within the pandemic" producing new socio-economic inequities in a state of global emergency.

</details>

<details>

<summary>2021-08-22 13:56:56 - Building Resilient Information Systems for Child Nutrition in Post-conflict Sri Lanka during COVID-19 Pandemic</summary>

- *Pamod Amarakoon, Jørn Braa, Sundeep Sahay, Lakmini Magodarathna, Rajeev Moorthy*

- `2108.09726v1` - [abs](http://arxiv.org/abs/2108.09726v1) - [pdf](http://arxiv.org/pdf/2108.09726v1)

> Post-conflict, low-resource settings are menaced with challenges related to low-resources, economic and social instability. The objective of the study is to understand the socio-technical determinants of resilience of resilience of routine information systems a backdrop of an implementation of a mobile-based nutrition information system in a post-conflict district in Sri Lanka. The longitudinal events in the study spans across several years into the period of COVID-19 pandemic and tries to understand the process of developing resilience of in a vulnerable district. The qualitative study deploys interviews, observations and document analysis for collection of empirical data. The case study reveals the long-standing capacity building, leadership and local governance, multisector collaboration, platform resilience and empowering of field health staff contribute in building resilience in everyday context. The empirical insights include the mechanisms in building resilience in routine system in low resource settings while promoting data quality and data use at field level.

</details>

<details>

<summary>2021-08-22 14:53:10 - A Resilient ICT4D Approach to ECO Countries' Education Response during COVID-19 Pandemic</summary>

- *Azadeh Akbari*

- `2108.09742v1` - [abs](http://arxiv.org/abs/2108.09742v1) - [pdf](http://arxiv.org/pdf/2108.09742v1)

> According to the United Nations, schools' closures have impacted up to 99 per cent of the student population in low and lower-middle-income countries. This research-in-progress report introduces a project on Emergency Remote Teaching (ERT) measures in the ten member states of the Economic Cooperation Organization (ECO) with a focus on the application of Information and Communication Technologies (ICTs) in primary and secondary education levels. The project takes a comparative approach within a resilient ICT-for-Development (ICT4D) framework, where the coping, endurance, and return to pre-crisis functionalities in education systems are studied. The preliminary research demonstrates the impacts of the country's general COVID-19 strategy, the education system in place, and digital infrastructure's level of development on instigating distance-learning platforms. The paper further shows that in addition to access to stable internet connections and digital devices, other infrastructural factors such as access to food, electricity and health services play a significant role in education response planning and implementation. Human factors in the education system, such as teacher training for the usage of ICTs, digital literacy of students and parents, and already existing vulnerabilities in the education system pose challenges to crisis management in the education sector. Other socio-political factors such as attitudes towards girls' education, level of corruption, institutional capacity, and international sanctions or available funds also make the education system less resilient.

</details>

<details>

<summary>2021-08-22 15:22:44 - Custom-Tailored Clone Detection for IEC 61131-3 Programming Languages</summary>

- *Kamil Rosiak, Alexander Schlie, Lukas Linsbauer, Birgit Vogel-Heuser, Ina Schaefer*

- `2108.09753v1` - [abs](http://arxiv.org/abs/2108.09753v1) - [pdf](http://arxiv.org/pdf/2108.09753v1)

> Automated production systems (aPS) are highly customized systems that consist of hardware and software. Such aPS are controlled by a programmable logic controller (PLC), often in accordance with the IEC 61131-3 standard that divides system implementation into so-called program organization units (POUs) as the smallest software unit and is comprised of multiple textual and graphical programming languages that can be arbitrarily nested. A common practice during the development of such systems is reusing implementation artifacts by copying, pasting, and then modifying code. This approach is referred to as code cloning. It is used on a fine-granular level where a POU is cloned within a system variant. It is also applied on the coarse-granular system level, where the entire system is cloned and adapted to create a system variant, for example for another customer. This ad hoc practice for the development of variants is commonly referred to as clone-and-own. It allows the fast development of variants to meet varying customer requirements or altered regulatory guidelines. However, clone-and-own is a non-sustainable approach and does not scale with an increasing number of variants. It has a detrimental effect on the overall quality of a software system, such as the propagation of bugs to other variants, which harms maintenance. In order to support the effective development and maintenance of such systems, a detailed code clone analysis is required. On the one hand, an analysis of code clones within a variant (i.e., clone detection in the classical sense) supports experts in refactoring respective code into library components. On the other hand, an analysis of commonalities and differences between cloned variants (i.e., variability analysis) supports the maintenance and further reuse and facilitates the migration of variants into a software product line (SPL).

</details>

<details>

<summary>2021-08-22 15:50:55 - A Climate Change Vulnerability Assessment Framework: A Spatial Approach</summary>

- *Claudia Cáceres, Yan Li, Brian Hilton*

- `2108.09762v1` - [abs](http://arxiv.org/abs/2108.09762v1) - [pdf](http://arxiv.org/pdf/2108.09762v1)

> Climate change is affecting every known society, especially for small farmers in Low-Income Countries because they depend heavily on rain, seasonality patterns, and known temperature ranges. To build climate change resilient communities among rural farmers, the first step is to understand the impact of climate change on the population. This paper proposes a Climate Change Vulnerability Assessment Framework (CCVAF) to assess climate change vulnerabilities among rural farmers. The CCVAF framework uses information and communication technology (ICT) to assess climate change vulnerabilities among rural farmers by integrating both community level and individual household level indicators. The CCVAF was instantiated into a GIS-based web application named THRIVE for different decision-makers to better assess how climate change is affecting rural farmers in Western Honduras. Qualitative evaluation of the THRIVE showed that it is an innovative and useful tool. The CCVAF contributes to not only the knowledge base of the climate change vulnerability assessment but also the design science literature by providing guidelines to design a class of climate change vulnerability assessment solutions.

</details>

<details>

<summary>2021-08-22 16:30:27 - Disentangled Contrastive Learning for Learning Robust Textual Representations</summary>

- *Xiang Chen, Xin Xie, Zhen Bi, Hongbin Ye, Shumin Deng, Ningyu Zhang, Huajun Chen*

- `2104.04907v2` - [abs](http://arxiv.org/abs/2104.04907v2) - [pdf](http://arxiv.org/pdf/2104.04907v2)

> Although the self-supervised pre-training of transformer models has resulted in the revolutionizing of natural language processing (NLP) applications and the achievement of state-of-the-art results with regard to various benchmarks, this process is still vulnerable to small and imperceptible permutations originating from legitimate inputs. Intuitively, the representations should be similar in the feature space with subtle input permutations, while large variations occur with different meanings. This motivates us to investigate the learning of robust textual representation in a contrastive manner. However, it is non-trivial to obtain opposing semantic instances for textual samples. In this study, we propose a disentangled contrastive learning method that separately optimizes the uniformity and alignment of representations without negative sampling. Specifically, we introduce the concept of momentum representation consistency to align features and leverage power normalization while conforming the uniformity. Our experimental results for the NLP benchmarks demonstrate that our approach can obtain better results compared with the baselines, as well as achieve promising improvements with invariance tests and adversarial attacks. The code is available in https://github.com/zxlzr/DCL.

</details>

<details>

<summary>2021-08-22 16:56:31 - The Liberalities and Tyrannies of ICTs for Vulnerable Migrants: The Status Quo, Gaps and Directions</summary>

- *Yidnekachew Redda Haile*

- `2108.09782v1` - [abs](http://arxiv.org/abs/2108.09782v1) - [pdf](http://arxiv.org/pdf/2108.09782v1)

> Information and communication technologies (ICTs) have increasingly become vital for people on the move including the nearly 80 million displaced due to conflict, violence, and human right violations globally. However, existing research on ICTs and migrants, which almost entirely focused on migrants' ICT use 'en route' or within developed economies principally in the perspectives of researchers from these regions, is very fragmented posing a difficulty in understanding the key objects of research. Moreover, ICTs are often celebrated as liberating and exploitable at migrants' rational discretion even though they are 'double-edged swords' with significant risks, burdens, pressures and inequality challenges particularly for vulnerable migrants including those forcefully displaced and trafficked. Towards addressing these limitations and illuminating future directions, this paper, first, scrutinises the existing research vis-a-vis ICTs' liberating and authoritarian role particularly for vulnerable migrants whereby explicating key issues in the research domain. Second, it identifies key gaps and opportunities for future research. Using a tailored methodology, broad literature relating to ICTs and migration/development published in the period 1990-2020 was surveyed resulting in 157 selected publications which were critically appraised vis-a-vis the key themes, major technologies dealt with, and methodologies and theories/concepts adopted. Furthermore, key insights, trends, gaps, and future research opportunities pertaining to both the existing and missing objects of research in ICTs and migration/development are spotlighted.

</details>

<details>

<summary>2021-08-22 18:22:17 - Moments in the Production of Space: Developing a Generic Adolescent Girls and Young Women Health Information Systems in Zimbabwe</summary>

- *Rangarirai Matavire, Jørn Braa, Shorai Huwa, Lameck Munangaidzwa, Zeferino Saugene, Isaac Taramusi, Bob Jolliffe*

- `2108.09811v1` - [abs](http://arxiv.org/abs/2108.09811v1) - [pdf](http://arxiv.org/pdf/2108.09811v1)

> With global targets to end AIDS by 2030 and to eliminate new HIV infections, Adolescent Girls and Young Women (AGYW) are seen to be particularly vulnerable, especially in Sub Saharan Africa. Numerous nations have therefore rolled out interventions to provide services to remove the determinants of vulnerability, such as limited education, early marriage, poverty, domestic violence, and exposure by male partners. Within this context, subpopulations such as sex workers increase the vulnerability amongst AGYW and are also supported through prevention programming. This study follows a project to develop a generic health information systems solution to provide a means to monitor and evaluate the successes of the AGYW initiative in reducing new infections. It borrows theoretical ideas from Henri Lefebvre's theory of moments to describe the process in which the space for the development of the solution is produced.

</details>

<details>

<summary>2021-08-23 03:06:44 - PointBA: Towards Backdoor Attacks in 3D Point Cloud</summary>

- *Xinke Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, Joey Tianyi Zhou*

- `2103.16074v3` - [abs](http://arxiv.org/abs/2103.16074v3) - [pdf](http://arxiv.org/pdf/2103.16074v3)

> 3D deep learning has been increasingly more popular for a variety of tasks including many safety-critical applications. However, recently several works raise the security issues of 3D deep models. Although most of them consider adversarial attacks, we identify that backdoor attack is indeed a more serious threat to 3D deep learning systems but remains unexplored. We present the backdoor attacks in 3D point cloud with a unified framework that exploits the unique properties of 3D data and networks. In particular, we design two attack approaches on point cloud: the poison-label backdoor attack (PointPBA) and the clean-label backdoor attack (PointCBA). The first one is straightforward and effective in practice, while the latter is more sophisticated assuming there are certain data inspections. The attack algorithms are mainly motivated and developed by 1) the recent discovery of 3D adversarial samples suggesting the vulnerability of deep models under spatial transformation; 2) the proposed feature disentanglement technique that manipulates the feature of the data through optimization methods and its potential to embed a new task. Extensive experiments show the efficacy of the PointPBA with over 95% success rate across various 3D datasets and models, and the more stealthy PointCBA with around 50% success rate. Our proposed backdoor attack in 3D point cloud is expected to perform as a baseline for improving the robustness of 3D deep models.

</details>

<details>

<summary>2021-08-23 05:17:47 - The Commodification of Open Educational Resources for Teaching and Learning by Academics in an Open Distance e-Learning Institution</summary>

- *Lancelord Siphamandla Mncube, Maureen Tanner, Wallace Chigona*

- `2108.09938v1` - [abs](http://arxiv.org/abs/2108.09938v1) - [pdf](http://arxiv.org/pdf/2108.09938v1)

> The use of open educational resources (OER) is gaining momentum in higher education institutions. This study sought to establish academics' perceptions and knowledge of OER for teaching and learning in an open distance e-learning (ODeL) university. The study also sought to establish how perceptions are formed. The inductive approach followed the lens of commodification to answer the research questions. The commodification phase allowed for a better understanding of the academics' prior knowledge, informers, academics behaviour about OER, and how they perceived OER to be useful for teaching and learning. The study employed a qualitative method, with semi-structured interviews to collect data. The study found that academics with prior experience and knowledge of OER are more successful in the use of these resources for teaching, learning, and research. OER is also perceived as a useful tool to promote African knowledge, showcase the contributions of African academics, improve academic research capabilities, improve student's success rate, particularly for financially vulnerable students. Based on the acquired perceptions, the study able to propose a new guideline to formulate user perceptions. However, this can only be achieved through a solid OER policy with the support of government and tertiary institution top management. The findings may inform higher education institutions when they consider the development of OER strategies and policies, especially in response to the Covid-19 emergency online learning transition.

</details>

<details>

<summary>2021-08-23 07:48:15 - Q&A MAESTRO: Q&A Post Recommendation for Fixing Java Runtime Exceptions</summary>

- *Yusuke Kimura, Takumi Akazaki, Shinji Kikuchi, Sonal Mahajan, Mukul R. Prasad*

- `2108.09991v1` - [abs](http://arxiv.org/abs/2108.09991v1) - [pdf](http://arxiv.org/pdf/2108.09991v1)

> Programmers often use Q&A sites (e.g., Stack Overflow) to understand a root cause of program bugs. Runtime exceptions is one of such important class of bugs that is actively discussed on Stack Overflow. However, it may be difficult for beginner programmers to come up with appropriate keywords for search. Moreover, they need to switch their attentions between IDE and browser, and it is time-consuming. To overcome these difficulties, we proposed a method, ``Q&A MAESTRO'', to find suitable Q&A posts automatically for Java runtime exception by utilizing structure information of codes described in programming Q&A website. In this paper, we describe a usage scenario of IDE-plugin, the architecture and user interface of the implementation, and results of user studies. A video is available at https://youtu.be/4X24jJrMUVw. A demo software is available at https://github.com/FujitsuLaboratories/Q-A-MAESTRO.

</details>

<details>

<summary>2021-08-23 11:54:59 - The Ethical Implications of Digital Contact Tracing for LGBTQIA+ Communities</summary>

- *Izak van Zyl, Nyx McLean*

- `2108.10096v1` - [abs](http://arxiv.org/abs/2108.10096v1) - [pdf](http://arxiv.org/pdf/2108.10096v1)

> The onset of COVID-19 has led to the introduction of far-reaching digital interventions in the interest of public health. Among these, digital contact tracing has been proposed as a viable means of targeted control in countries across the globe, including on the African continent. This, in turn, creates significant ethical challenges for vulnerable communities, including LGBTQIA+ persons. In this research paper, we explore some of the ethical implications of digital contact tracing for the LGBTQIA+ community. We refer specifically to the digital infringement of freedoms, and ground our discussion in the discourse of data colonisation and Big Tech. We propose a critical intersectional feminism towards developing inclusive technology that is decentralised and user controlled. This approach is informed by a feminist ethics of care that emphasises multiple lived experiences.

</details>

<details>

<summary>2021-08-23 14:52:44 - Deep Bayesian Image Set Classification: A Defence Approach against Adversarial Attacks</summary>

- *Nima Mirnateghi, Syed Afaq Ali Shah, Mohammed Bennamoun*

- `2108.10217v1` - [abs](http://arxiv.org/abs/2108.10217v1) - [pdf](http://arxiv.org/pdf/2108.10217v1)

> Deep learning has become an integral part of various computer vision systems in recent years due to its outstanding achievements for object recognition, facial recognition, and scene understanding. However, deep neural networks (DNNs) are susceptible to be fooled with nearly high confidence by an adversary. In practice, the vulnerability of deep learning systems against carefully perturbed images, known as adversarial examples, poses a dire security threat in the physical world applications. To address this phenomenon, we present, what to our knowledge, is the first ever image set based adversarial defence approach. Image set classification has shown an exceptional performance for object and face recognition, owing to its intrinsic property of handling appearance variability. We propose a robust deep Bayesian image set classification as a defence framework against a broad range of adversarial attacks. We extensively experiment the performance of the proposed technique with several voting strategies. We further analyse the effects of image size, perturbation magnitude, along with the ratio of perturbed images in each image set. We also evaluate our technique with the recent state-of-the-art defence methods, and single-shot recognition task. The empirical results demonstrate superior performance on CIFAR-10, MNIST, ETH-80, and Tiny ImageNet datasets.

</details>

<details>

<summary>2021-08-23 16:09:18 - Exploring Biases and Prejudice of Facial Synthesis via Semantic Latent Space</summary>

- *Xuyang Shen, Jo Plested, Sabrina Caldwell, Tom Gedeon*

- `2108.10265v1` - [abs](http://arxiv.org/abs/2108.10265v1) - [pdf](http://arxiv.org/pdf/2108.10265v1)

> Deep learning (DL) models are widely used to provide a more convenient and smarter life. However, biased algorithms will negatively influence us. For instance, groups targeted by biased algorithms will feel unfairly treated and even fearful of negative consequences of these biases. This work targets biased generative models' behaviors, identifying the cause of the biases and eliminating them. We can (as expected) conclude that biased data causes biased predictions of face frontalization models. Varying the proportions of male and female faces in the training data can have a substantial effect on behavior on the test data: we found that the seemingly obvious choice of 50:50 proportions was not the best for this dataset to reduce biased behavior on female faces, which was 71% unbiased as compared to our top unbiased rate of 84%. Failure in generation and generating incorrect gender faces are two behaviors of these models. In addition, only some layers in face frontalization models are vulnerable to biased datasets. Optimizing the skip-connections of the generator in face frontalization models can make models less biased. We conclude that it is likely to be impossible to eliminate all training bias without an unlimited size dataset, and our experiments show that the bias can be reduced and quantified. We believe the next best to a perfect unbiased predictor is one that has minimized the remaining known bias.

</details>

<details>

<summary>2021-08-24 00:08:33 - Adversarial Robustness of Deep Learning: Theory, Algorithms, and Applications</summary>

- *Wenjie Ruan, Xinping Yi, Xiaowei Huang*

- `2108.10451v1` - [abs](http://arxiv.org/abs/2108.10451v1) - [pdf](http://arxiv.org/pdf/2108.10451v1)

> This tutorial aims to introduce the fundamentals of adversarial robustness of deep learning, presenting a well-structured review of up-to-date techniques to assess the vulnerability of various types of deep learning models to adversarial examples. This tutorial will particularly highlight state-of-the-art techniques in adversarial attacks and robustness verification of deep neural networks (DNNs). We will also introduce some effective countermeasures to improve the robustness of deep learning models, with a particular focus on adversarial training. We aim to provide a comprehensive overall picture about this emerging direction and enable the community to be aware of the urgency and importance of designing robust deep learning models in safety-critical data analytical applications, ultimately enabling the end-users to trust deep learning classifiers. We will also summarize potential research directions concerning the adversarial robustness of deep learning, and its potential benefits to enable accountable and trustworthy deep learning-based data analytical systems and applications.

</details>

<details>

<summary>2021-08-24 14:32:20 - Transient Execution of Non-Canonical Accesses</summary>

- *Saidgani Musaev, Christof Fetzer*

- `2108.10771v1` - [abs](http://arxiv.org/abs/2108.10771v1) - [pdf](http://arxiv.org/pdf/2108.10771v1)

> Recent years have brought microarchitectural security intothe spotlight, proving that modern CPUs are vulnerable toseveral classes of microarchitectural attacks. These attacksbypass the basic isolation primitives provided by the CPUs:process isolation, memory permissions, access checks, andso on. Nevertheless, most of the research was focused on In-tel CPUs, with only a few exceptions. As a result, few vulner-abilities have been found in other CPUs, leading to specula-tions about their immunity to certain types of microarchi-tectural attacks. In this paper, we provide a black-box anal-ysis of one of these under-explored areas. Namely, we inves-tigate the flaw of AMD CPUs which may lead to a transientexecution hijacking attack. Contrary to nominal immunity,we discover that AMD Zen family CPUs exhibit transient ex-ecution patterns similar for Meltdown/MDS. Our analysisof exploitation possibilities shows that AMDs design deci-sions indeed limit the exploitability scope comparing to In-tel CPUs, yet it may be possible to use them to amplify othermicroarchitectural attacks.

</details>

<details>

<summary>2021-08-24 16:16:36 - Resilience-by-design in Adaptive Multi-Agent Traffic Control Systems</summary>

- *Ranwa Al Mallah, Talal Halabi, Bilal Farooq*

- `2012.02675v4` - [abs](http://arxiv.org/abs/2012.02675v4) - [pdf](http://arxiv.org/pdf/2012.02675v4)

> Connected and Autonomous Vehicles (CAVs) with their evolving data gathering capabilities will play a significant role in road safety and efficiency applications supported by Intelligent Transport Systems (ITS), such as Traffic Signal Control (TSC) for urban traffic congestion management. However, their involvement will expand the space of security vulnerabilities and create larger threat vectors. In this paper, we perform the first detailed security analysis and implementation of a new cyber-physical attack category carried out by the network of CAVs against Adaptive Multi-Agent Traffic Signal Control (AMATSC), namely, coordinated Sybil attacks, where vehicles with forged or fake identities try to alter the data collected by the AMATSC algorithms to sabotage their decisions. Consequently, a novel, game-theoretic mitigation approach at the application layer is proposed to minimize the impact of such sophisticated data corruption attacks. The devised minimax game model enables the AMATSC algorithm to generate optimal decisions under a suspected attack, improving its resilience. Extensive experimentation is performed on a traffic dataset provided by the City of Montreal under real-world intersection settings to evaluate the attack impact. Our results improved time loss on attacked intersections by approximately 48.9%. Substantial benefits can be gained from the mitigation, yielding more robust adaptive control of traffic across networked intersections.

</details>

<details>

<summary>2021-08-24 18:18:07 - GGNB: Graph-Based Gaussian Naive Bayes Intrusion Detection System for CAN Bus</summary>

- *Riadul Islam, Maloy K. Devnath, Manar D. Samad, Syed Md Jaffrey Al Kadry*

- `2108.10908v1` - [abs](http://arxiv.org/abs/2108.10908v1) - [pdf](http://arxiv.org/pdf/2108.10908v1)

> The national highway traffic safety administration (NHTSA) identified cybersecurity of the automobile systems are more critical than the security of other information systems. Researchers already demonstrated remote attacks on critical vehicular electronic control units (ECUs) using controller area network (CAN). Besides, existing intrusion detection systems (IDSs) often propose to tackle a specific type of attack, which may leave a system vulnerable to numerous other types of attacks. A generalizable IDS that can identify a wide range of attacks within the shortest possible time has more practical value than attack-specific IDSs, which is not a trivial task to accomplish. In this paper we propose a novel {\textbf g}raph-based {\textbf G}aussian {\textbf n}aive {\textbf B}ayes (GGNB) intrusion detection algorithm by leveraging graph properties and PageRank-related features. The GGNB on the real rawCAN data set~\cite{Lee:2017} yields 99.61\%, 99.83\%, 96.79\%, and 96.20\% detection accuracy for denial of service (DoS), fuzzy, spoofing, replay, mixed attacks, respectively. Also, using OpelAstra data set~\cite{Guillaume:2019}, the proposed methodology has 100\%, 99.85\%, 99.92\%, 100\%, 99.92\%, 97.75\% and 99.57\% detection accuracy considering DoS, diagnostic, fuzzing CAN ID, fuzzing payload, replay, suspension, and mixed attacks, respectively. The GGNB-based methodology requires about $239\times$ and $135\times$ lower training and tests times, respectively, compared to the SVM classifier used in the same application. Using Xilinx Zybo Z7 field-programmable gate array (FPGA) board, the proposed GGNB requires $5.7 \times$, $5.9 \times$, $5.1 \times$, and $3.6 \times$ fewer slices, LUTs, flip-flops, and DSP units, respectively, than conventional NN architecture.

</details>

<details>

<summary>2021-08-25 08:39:40 - Adversarial Immunization for Certifiable Robustness on Graphs</summary>

- *Shuchang Tao, Huawei Shen, Qi Cao, Liang Hou, Xueqi Cheng*

- `2007.09647v5` - [abs](http://arxiv.org/abs/2007.09647v5) - [pdf](http://arxiv.org/pdf/2007.09647v5)

> Despite achieving strong performance in semi-supervised node classification task, graph neural networks (GNNs) are vulnerable to adversarial attacks, similar to other deep learning models. Existing researches focus on developing either robust GNN models or attack detection methods against adversarial attacks on graphs. However, little research attention is paid to the potential and practice of immunization to adversarial attacks on graphs. In this paper, we propose and formulate the graph adversarial immunization problem, i.e., vaccinating an affordable fraction of node pairs, connected or unconnected, to improve the certifiable robustness of graph against any admissible adversarial attack. We further propose an effective algorithm, called AdvImmune, which optimizes with meta-gradient in a discrete way to circumvent the computationally expensive combinatorial optimization when solving the adversarial immunization problem. Experiments are conducted on two citation networks and one social network. Experimental results demonstrate that the proposed AdvImmune method remarkably improves the ratio of robust nodes by 12%, 42%, 65%, with an affordable immune budget of only 5% edges.

</details>

<details>

<summary>2021-08-25 09:42:30 - Robustness Threats of Differential Privacy</summary>

- *Nurislam Tursynbek, Aleksandr Petiushko, Ivan Oseledets*

- `2012.07828v3` - [abs](http://arxiv.org/abs/2012.07828v3) - [pdf](http://arxiv.org/pdf/2012.07828v3)

> Differential privacy (DP) is a gold-standard concept of measuring and guaranteeing privacy in data analysis. It is well-known that the cost of adding DP to deep learning model is its accuracy. However, it remains unclear how it affects robustness of the model. Standard neural networks are not robust to different input perturbations: either adversarial attacks or common corruptions. In this paper, we empirically observe an interesting trade-off between privacy and robustness of neural networks. We experimentally demonstrate that networks, trained with DP, in some settings might be even more vulnerable in comparison to non-private versions. To explore this, we extensively study different robustness measurements, including FGSM and PGD adversaries, distance to linear decision boundaries, curvature profile, and performance on a corrupted dataset. Finally, we study how the main ingredients of differentially private neural networks training, such as gradient clipping and noise addition, affect (decrease and increase) the robustness of the model.

</details>

<details>

<summary>2021-08-25 12:31:44 - RefactorInsight: Enhancing IDE Representation of Changes in Git with Refactorings Information</summary>

- *Zarina Kurbatova, Vladimir Kovalenko, Ioana Savu, Bob Brockbernd, Dan Andreescu, Matei Anton, Roman Venediktov, Elena Tikhomirova, Timofey Bryksin*

- `2108.11202v1` - [abs](http://arxiv.org/abs/2108.11202v1) - [pdf](http://arxiv.org/pdf/2108.11202v1)

> Inspection of code changes is a time-consuming task that constitutes a big part of everyday work of software engineers. Existing IDEs provide little information about the semantics of code changes within the file editor view. Therefore developers have to track changes across multiple files, which is a hard task with large codebases.   In this paper, we present RefactorInsight, a plugin for IntelliJ IDEA that introduces a smart diff for code changes in Java and Kotlin where refactorings are auto-folded and provided with their description, thus allowing users to focus on changes that modify the code behavior like bug fixes and new features. RefactorInsight supports three usage scenarios: viewing smart diffs with auto-folded refactorings and hints, inspecting refactorings in pull requests and in any specific commit in the project change history, and exploring the refactoring history of methods and classes. The evaluation shows that commit processing time is acceptable: on median it is less than 0.2 seconds, which delay does not disrupt developers' IDE workflows.   RefactorInsight is available at https://github.com/JetBrains-Research/RefactorInsight. The demonstration video is available at https://youtu.be/-6L2AKQ66nA.

</details>

<details>

<summary>2021-08-26 03:34:26 - Communication-Efficient LDPC Code Design for Data Availability Oracle in Side Blockchains</summary>

- *Debarnab Mitra, Lev Tauz, Lara Dolecek*

- `2105.06004v2` - [abs](http://arxiv.org/abs/2105.06004v2) - [pdf](http://arxiv.org/pdf/2105.06004v2)

> A popular method of improving the throughput of blockchain systems is by running smaller side blockchains that push the hashes of their blocks onto a trusted blockchain. Side blockchains are vulnerable to stalling attacks where a side blockchain node pushes the hash of a block to the trusted blockchain but makes the block unavailable to other side blockchain nodes. Recently, Sheng et al. proposed a data availability oracle based on LDPC codes and a data dispersal protocol as a solution to the above problem. While showing improvements, the codes and dispersal protocol were designed disjointly which may not be optimal in terms of the communication cost associated with the oracle. In this paper, we provide a tailored dispersal protocol and specialized LDPC code construction based on the Progressive Edge Growth (PEG) algorithm, called the dispersal-efficient PEG (DE-PEG) algorithm, aimed to reduce the communication cost associated with the new dispersal protocol. Our new code construction reduces the communication cost and, additionally, is less restrictive in terms of system design.

</details>

<details>

<summary>2021-08-26 10:42:36 - Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-based Traffic Congestion Control Systems</summary>

- *Yue Wang, Esha Sarkar, Wenqing Li, Michail Maniatakos, Saif Eddin Jabari*

- `2003.07859v4` - [abs](http://arxiv.org/abs/2003.07859v4) - [pdf](http://arxiv.org/pdf/2003.07859v4)

> Recent work has shown that the introduction of autonomous vehicles (AVs) in traffic could help reduce traffic jams. Deep reinforcement learning methods demonstrate good performance in complex control problems, including autonomous vehicle control, and have been used in state-of-the-art AV controllers. However, deep neural networks (DNNs) render automated driving vulnerable to machine learning-based attacks. In this work, we explore the backdooring/trojanning of DRL-based AV controllers. We develop a trigger design methodology that is based on well-established principles of traffic physics. The malicious actions include vehicle deceleration and acceleration to cause stop-and-go traffic waves to emerge (congestion attacks) or AV acceleration resulting in the AV crashing into the vehicle in front (insurance attack). We test our attack on single-lane and two-lane circuits. Our experimental results show that the backdoored model does not compromise normal operation performance, with the maximum decrease in cumulative rewards being 1%. Still, it can be maliciously activated to cause a crash or congestion when the corresponding triggers appear.

</details>

<details>

<summary>2021-08-26 14:34:13 - Understanding Money Trails of Suspicious Activities in a cryptocurrency-based Blockchain</summary>

- *Banwari Lal, Rachit Agarwal, Sandeep Kumar Shukla*

- `2108.11818v1` - [abs](http://arxiv.org/abs/2108.11818v1) - [pdf](http://arxiv.org/pdf/2108.11818v1)

> The decentralization, redundancy, and pseudo-anonymity features have made permission-less public blockchain platforms attractive for adoption as technology platforms for cryptocurrencies. However, such adoption has enabled cybercriminals to exploit vulnerabilities in blockchain platforms and target the users through social engineering to carry out malicious activities. Most of the state-of-the-art techniques for detecting malicious actors depend on the transactional behavior of individual wallet addresses but do not analyze the money trails. We propose a heuristics-based approach that adds new features associated with money trails to analyze and find suspicious activities in cryptocurrency blockchains. Here, we focus only on the cyclic behavior and identify hidden patterns present in the temporal transactions graphs in a blockchain. We demonstrate our methods on the transaction data of the Ethereum blockchain. We find that malicious activities (such as Gambling, Phishing, and Money Laundering) have different cyclic patterns in Ethereum. We also identify two suspicious temporal cyclic path-based transfers in Ethereum. Our techniques may apply to other cryptocurrency blockchains with appropriate modifications adapted to the nature of the crypto-currency under investigation.

</details>

<details>

<summary>2021-08-26 15:15:19 - A Smart and Defensive Human-Machine Approach to Code Analysis</summary>

- *Fitzroy D. Nembhard, Marco M. Carvalho*

- `2108.03294v3` - [abs](http://arxiv.org/abs/2108.03294v3) - [pdf](http://arxiv.org/pdf/2108.03294v3)

> Static analysis remains one of the most popular approaches for detecting and correcting poor or vulnerable program code. It involves the examination of code listings, test results, or other documentation to identify errors, violations of development standards, or other problems, with the ultimate goal of fixing these errors so that systems and software are as secure as possible. There exists a plethora of static analysis tools, which makes it challenging for businesses and programmers to select a tool to analyze their program code. It is imperative to find ways to improve code analysis so that it can be employed by cyber defenders to mitigate security risks. In this research, we propose a method that employs the use of virtual assistants to work with programmers to ensure that software are as safe as possible in order to protect safety-critical systems from data breaches and other attacks. The proposed method employs a recommender system that uses various metrics to help programmers select the most appropriate code analysis tool for their project and guides them through the analysis process. The system further tracks the user's behavior regarding the adoption of the recommended practices.

</details>

<details>

<summary>2021-08-27 03:35:54 - On Procedural Adversarial Noise Attack And Defense</summary>

- *Jun Yan, Xiaoyang Deng, Huilin Yin, Wancheng Ge*

- `2108.04409v2` - [abs](http://arxiv.org/abs/2108.04409v2) - [pdf](http://arxiv.org/pdf/2108.04409v2)

> Deep Neural Networks (DNNs) are vulnerable to adversarial examples which would inveigle neural networks to make prediction errors with small perturbations on the input images. Researchers have been devoted to promoting the research on the universal adversarial perturbations (UAPs) which are gradient-free and have little prior knowledge on data distributions. Procedural adversarial noise attack is a data-free universal perturbation generation method. In this paper, we propose two universal adversarial perturbation (UAP) generation methods based on procedural noise functions: Simplex noise and Worley noise. In our framework, the shading which disturbs visual classification is generated with rendering technology. Without changing the semantic representations, the adversarial examples generated via our methods show superior performance on the attack.

</details>

<details>

<summary>2021-08-27 12:47:19 - Deep learning models are not robust against noise in clinical text</summary>

- *Milad Moradi, Kathrin Blagec, Matthias Samwald*

- `2108.12242v1` - [abs](http://arxiv.org/abs/2108.12242v1) - [pdf](http://arxiv.org/pdf/2108.12242v1)

> Artificial Intelligence (AI) systems are attracting increasing interest in the medical domain due to their ability to learn complicated tasks that require human intelligence and expert knowledge. AI systems that utilize high-performance Natural Language Processing (NLP) models have achieved state-of-the-art results on a wide variety of clinical text processing benchmarks. They have even outperformed human accuracy on some tasks. However, performance evaluation of such AI systems have been limited to accuracy measures on curated and clean benchmark datasets that may not properly reflect how robustly these systems can operate in real-world situations. In order to address this challenge, we introduce and implement a wide variety of perturbation methods that simulate different types of noise and variability in clinical text data. While noisy samples produced by these perturbation methods can often be understood by humans, they may cause AI systems to make erroneous decisions. Conducting extensive experiments on several clinical text processing tasks, we evaluated the robustness of high-performance NLP models against various types of character-level and word-level noise. The results revealed that the NLP models performance degrades when the input contains small amounts of noise. This study is a significant step towards exposing vulnerabilities of AI models utilized in clinical text processing systems. The proposed perturbation methods can be used in performance evaluation tests to assess how robustly clinical NLP models can operate on noisy data, in real-world settings.

</details>

<details>

<summary>2021-08-27 21:43:27 - An Experimental Analysis of Graph-Distance Algorithms for Comparing API Usages</summary>

- *Sebastian Nielebock, Paul Blockhaus, Jacob Krüger, Frank Ortmeier*

- `2108.12511v1` - [abs](http://arxiv.org/abs/2108.12511v1) - [pdf](http://arxiv.org/pdf/2108.12511v1)

> Modern software development heavily relies on the reuse of functionalities through Application Programming Interfaces (APIs). However, client developers can have issues identifying the correct usage of a certain API, causing misuses accompanied by software crashes or usability bugs. Therefore, researchers have aimed at identifying API misuses automatically by comparing client code usages to correct API usages. Some techniques rely on certain API-specific graph-based data structures to improve the abstract representation of API usages. Such techniques need to compare graphs, for instance, by computing distance metrics based on the minimal graph edit distance or the largest common subgraphs, whose computations are known to be NP-hard problems. Fortunately, there exist many abstractions for simplifying graph distance computation. However, their applicability for comparing graph representations of API usages has not been analyzed. In this paper, we provide a comparison of different distance algorithms of API-usage graphs regarding correctness and runtime. Particularly, correctness relates to the algorithms' ability to identify similar correct API usages, but also to discriminate similar correct and false usages as well as non-similar usages. For this purpose, we systematically identified a set of eight graph-based distance algorithms and applied them on two datasets of real-world API usages and misuses. Interestingly, our results suggest that existing distance algorithms are not reliable for comparing API usage graphs. To improve on this situation, we identified and discuss the algorithms' issues, based on which we formulate hypotheses to initiate research on overcoming them.

</details>

<details>

<summary>2021-08-28 05:25:03 - Power-Based Attacks on Spatial DNN Accelerators</summary>

- *Ge Li, Mohit Tiwari, Michael Orshansky*

- `2108.12579v1` - [abs](http://arxiv.org/abs/2108.12579v1) - [pdf](http://arxiv.org/pdf/2108.12579v1)

> With proliferation of DNN-based applications, the confidentiality of DNN model is an important commercial goal. Spatial accelerators, that parallelize matrix/vector operations, are utilized for enhancing energy efficiency of DNN computation. Recently, model extraction attacks on simple accelerators, either with a single processing element or running a binarized network, were demonstrated using the methodology derived from differential power analysis (DPA) attack on cryptographic devices. This paper investigates the vulnerability of realistic spatial accelerators using general, 8-bit, number representation.   We investigate two systolic array architectures with weight-stationary dataflow: (1) a 3 $\times$ 1 array for a dot-product operation, and (2) a 3 $\times$ 3 array for matrix-vector multiplication. Both are implemented on the SAKURA-G FPGA board. We show that both architectures are ultimately vulnerable. A conventional DPA succeeds fully on the 1D array, requiring 20K power measurements. However, the 2D array exhibits higher security even with 460K traces. We show that this is because the 2D array intrinsically entails multiple MACs simultaneously dependent on the same input. However, we find that a novel template-based DPA with multiple profiling phases is able to fully break the 2D array with only 40K traces. Corresponding countermeasures need to be investigated for spatial DNN accelerators.

</details>

<details>

<summary>2021-08-28 21:00:33 - CHAINGE: A Blockchain Solution to Automate Payment Detail Updates to Subscription Services</summary>

- *David Buckley, Gueltoum Bendiab, Stavros Shiaeles, Nick Savage, Nicholas Kolokotronis*

- `2108.12705v1` - [abs](http://arxiv.org/abs/2108.12705v1) - [pdf](http://arxiv.org/pdf/2108.12705v1)

> The rise of the subscription-based business model has led to a corresponding increase in the number of subscriptions where a customer needs to manage their payments. This management of payments for multiple subscriptions has become a very complicated and insecure task for customers, especially when it comes to renewing payment details when the card is lost, stolen, or expires. In addition, this, mostly manual, process is vulnerable to human error, digital frauds, and data breaches, according to security reports. Thus, in this paper, we propose a novel approach to automate, manage and simplify the Financial Supply Chain involved in the process of updating and managing payments to user subscriptions. This is done by utilising the Hyperledger Sawtooth blockchain framework, that allows a consumer to enter their payment card details in a central digital wallet and link their subscriptions to their cards. The card being updated triggers an event on the blockchain, which allow for the payment details to be updated on subscription systems automatically. The verification tests performed on the prototype of the proposed system shows that its current implementation has been securely achieved.

</details>

<details>

<summary>2021-08-29 19:55:54 - BoostNSift: A Query Boosting and Code Sifting Technique for Method Level Bug Localization</summary>

- *Abdul Razzaq, Jim Buckley, James Vincent Patten, Muslim Chochlov, Ashish Rajendra Sai*

- `2108.12901v1` - [abs](http://arxiv.org/abs/2108.12901v1) - [pdf](http://arxiv.org/pdf/2108.12901v1)

> Locating bugs is an important, but effort-intensive and time-consuming task, when dealing with large-scale systems. To address this, Information Retrieval (IR) techniques are increasingly being used to suggest potential buggy source code locations, for given bug reports. While IR techniques are very scalable, in practice their effectiveness in accurately localizing bugs in a software system remains low. Results of empirical studies suggest that the effectiveness of bug localization techniques can be augmented by the configuration of queries used to locate buggy code. However, in most IR-based bug localization techniques, presented by researchers, the impact of the queries' configurations is not fully considered. In a similar vein, techniques consider all code elements as equally suspicious of being buggy while localizing bugs, but this is not always the case either.In this paper, we present a new method-level, information-retrieval-based bug localization technique called ``BoostNSift''. BoostNSift exploits the important information in queries by `boost'ing that information, and then `sift's the identified code elements, based on a novel technique that emphasizes the code elements' specific relatedness to a bug report over its generic relatedness to all bug reports. To evaluate the performance of BoostNSift, we employed a state-of-the-art empirical design that has been commonly used for evaluating file level IR-based bug localization techniques: 6851 bugs are selected from commonly used Eclipse, AspectJ, SWT, and ZXing benchmarks and made openly available for method-level analyses.

</details>

<details>

<summary>2021-08-30 03:59:50 - Improving Transferability of Adversarial Patches on Face Recognition with Generative Models</summary>

- *Zihao Xiao, Xianfeng Gao, Chilin Fu, Yinpeng Dong, Wei Gao, Xiaolu Zhang, Jun Zhou, Jun Zhu*

- `2106.15058v2` - [abs](http://arxiv.org/abs/2106.15058v2) - [pdf](http://arxiv.org/pdf/2106.15058v2)

> Face recognition is greatly improved by deep convolutional neural networks (CNNs). Recently, these face recognition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically realizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the attacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we observe that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indicating the overfitting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face images. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transferability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well.

</details>

<details>

<summary>2021-08-30 10:04:50 - Investigating Vulnerabilities of Deep Neural Policies</summary>

- *Ezgi Korkmaz*

- `2108.13093v1` - [abs](http://arxiv.org/abs/2108.13093v1) - [pdf](http://arxiv.org/pdf/2108.13093v1)

> Reinforcement learning policies based on deep neural networks are vulnerable to imperceptible adversarial perturbations to their inputs, in much the same way as neural network image classifiers. Recent work has proposed several methods to improve the robustness of deep reinforcement learning agents to adversarial perturbations based on training in the presence of these imperceptible perturbations (i.e. adversarial training). In this paper, we study the effects of adversarial training on the neural policy learned by the agent. In particular, we follow two distinct parallel approaches to investigate the outcomes of adversarial training on deep neural policies based on worst-case distributional shift and feature sensitivity. For the first approach, we compare the Fourier spectrum of minimal perturbations computed for both adversarially trained and vanilla trained neural policies. Via experiments in the OpenAI Atari environments we show that minimal perturbations computed for adversarially trained policies are more focused on lower frequencies in the Fourier domain, indicating a higher sensitivity of these policies to low frequency perturbations. For the second approach, we propose a novel method to measure the feature sensitivities of deep neural policies and we compare these feature sensitivity differences in state-of-the-art adversarially trained deep neural policies and vanilla trained deep neural policies. We believe our results can be an initial step towards understanding the relationship between adversarial training and different notions of robustness for neural policies.

</details>

<details>

<summary>2021-08-30 14:19:00 - Thermal Management in Large Data Centers: Security Threats and Mitigation</summary>

- *Betty Saridou, Gueltoum Bendiab, Stavros N. Shiaeles, Basil K. Papadopoulos*

- `2108.13261v1` - [abs](http://arxiv.org/abs/2108.13261v1) - [pdf](http://arxiv.org/pdf/2108.13261v1)

> Data centres are experiencing significant growth in their scale, especially, with the ever-increasing demand for cloud and IoT services. However, this rapid growth has raised numerous security issues and vulnerabilities; new types of strategic cyber-attacks are aimed at specific physical components of data centres that keep them operating. Attacks against temperature monitoring and cooling systems of data centres, also known as thermal attacks, can cause a complete meltdown and are generally considered difficult to address. In this paper, we focus on this issue by analysing the potential security threats to these systems and their impact on the overall data center safety and performance. We also present current thermal anomaly detection methods and their limitations. Finally, we propose a hybrid method that uses multi-variant anomaly detection to prevent thermal attacks, as well as a fuzzy-based health factor to enhance data center thermal awareness and security

</details>

<details>

<summary>2021-08-30 16:36:52 - Coding with Purpose: Learning AI in Rural California</summary>

- *Stephanie Tena-Meza, Miroslav Suzara, AJ Alvero*

- `2108.13363v1` - [abs](http://arxiv.org/abs/2108.13363v1) - [pdf](http://arxiv.org/pdf/2108.13363v1)

> We use an autoethnographic case study of a Latinx high school student from an agricultural community in California to highlight how AI is learned outside classrooms and how her personal background influenced her social-justice oriented applications of AI technologies. Applying the concept of learning pathways from the learning sciences, we argue that redesigning AI education to be more inclusive with respect to socioeconomic status, ethnoracial identity, and gender is important in the development of computational projects that address social-injustice. We also learn about the role of institutions, power structures, and community as they relate to her journey of learning and applying AI. The future of AI, its potential to address issues of social injustice and limiting the negative consequences of its use, will depend on the participation and voice of students from the most vulnerable communities.

</details>

<details>

<summary>2021-08-31 14:45:18 - Arms Race in Adversarial Malware Detection: A Survey</summary>

- *Deqiang Li, Qianmu Li, Yanfang Ye, Shouhuai Xu*

- `2005.11671v3` - [abs](http://arxiv.org/abs/2005.11671v3) - [pdf](http://arxiv.org/pdf/2005.11671v3)

> Malicious software (malware) is a major cyber threat that has to be tackled with Machine Learning (ML) techniques because millions of new malware examples are injected into cyberspace on a daily basis. However, ML is vulnerable to attacks known as adversarial examples. In this paper, we survey and systematize the field of Adversarial Malware Detection (AMD) through the lens of a unified conceptual framework of assumptions, attacks, defenses, and security properties. This not only leads us to map attacks and defenses to partial order structures, but also allows us to clearly describe the attack-defense arms race in the AMD context. We draw a number of insights, including: knowing the defender's feature set is critical to the success of transfer attacks; the effectiveness of practical evasion attacks largely depends on the attacker's freedom in conducting manipulations in the problem space; knowing the attacker's manipulation set is critical to the defender's success; the effectiveness of adversarial training depends on the defender's capability in identifying the most powerful attack. We also discuss a number of future research directions.

</details>

<details>

<summary>2021-08-31 14:47:37 - Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning</summary>

- *Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu*

- `2108.13888v1` - [abs](http://arxiv.org/abs/2108.13888v1) - [pdf](http://arxiv.org/pdf/2108.13888v1)

> \textbf{P}re-\textbf{T}rained \textbf{M}odel\textbf{s} have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat. These backdoors generated by the poisoning methods can be erased by changing hyper-parameters during fine-tuning or detected by finding the triggers. In this paper, we propose a stronger weight-poisoning attack method that introduces a layerwise weight poisoning strategy to plant deeper backdoors; we also introduce a combinatorial trigger that cannot be easily detected. The experiments on text classification tasks show that previous defense methods cannot resist our weight-poisoning method, which indicates that our method can be widely applied and may provide hints for future model robustness studies.

</details>

<details>

<summary>2021-08-31 17:22:34 - DeepTaskAPT: Insider APT detection using Task-tree based Deep Learning</summary>

- *Mohammad Mamun, Kevin Shi*

- `2108.13989v1` - [abs](http://arxiv.org/abs/2108.13989v1) - [pdf](http://arxiv.org/pdf/2108.13989v1)

> APT, known as Advanced Persistent Threat, is a difficult challenge for cyber defence. These threats make many traditional defences ineffective as the vulnerabilities exploited by these threats are insiders who have access to and are within the network. This paper proposes DeepTaskAPT, a heterogeneous task-tree based deep learning method to construct a baseline model based on sequences of tasks using a Long Short-Term Memory (LSTM) neural network that can be applied across different users to identify anomalous behaviour. Rather than applying the model to sequential log entries directly, as most current approaches do, DeepTaskAPT applies a process tree based task generation method to generate sequential log entries for the deep learning model. To assess the performance of DeepTaskAPT, we use a recently released synthetic dataset, DARPA Operationally Transparent Computing (OpTC) dataset and a real-world dataset, Los Alamos National Laboratory (LANL) dataset. Both of them are composed of host-based data collected from sensors. Our results show that DeepTaskAPT outperforms similar approaches e.g. DeepLog and the DeepTaskAPT baseline model demonstrate its capability to detect malicious traces in various attack scenarios while having high accuracy and low false-positive rates. To the best of knowledge this is the very first attempt of using recently introduced OpTC dataset for cyber threat detection.

</details>

<details>

<summary>2021-08-31 17:51:29 - Securing RPL using Network Coding: The Chained Secure Mode (CSM)</summary>

- *Ahmed Raoof, Chung-Horng Lung, Ashraf Matrawy*

- `2102.06254v2` - [abs](http://arxiv.org/abs/2102.06254v2) - [pdf](http://arxiv.org/pdf/2102.06254v2)

> As the de facto routing protocol for many Internet of Things (IoT) networks nowadays, and to assure the confidentiality and integrity of its control messages, the Routing Protocol for Low Power and Lossy Networks (RPL) incorporates three modes of security: the Unsecured Mode (UM), Preinstalled Secure Mode (PSM), and the Authenticated Secure Mode (ASM). While the PSM and ASM are intended to protect against external routing attacks and some replay attacks (through an optional replay protection mechanism), recent research showed that RPL in PSM is still vulnerable to many routing attacks, both internal and external. In this paper, we propose a novel secure mode for RPL, the Chained Secure Mode (CSM), based on the concept of intraflow Network Coding (NC). The CSM is designed to enhance RPL resilience and mitigation capability against replay attacks while allowing the integration with external security measures such as Intrusion Detection Systems (IDSs). The security and performance of the proposed CSM were evaluated and compared against RPL in UM and PSM (with and without the optional replay protection) under several routing attacks: the Neighbor attack (NA), Wormhole (WH), and CloneID attack (CA), using average packet delivery rate (PDR), End-to-End (E2E) latency, and power consumption as metrics. It showed that CSM has better performance and more enhanced security than both the UM and PSM with the replay protection, while mitigating both the NA and WH attacks and significantly reducing the effect of the CA in the investigated scenarios.

</details>

<details>

<summary>2021-08-31 17:55:25 - EthClipper: A Clipboard Meddling Attack on Hardware Wallets with Address Verification Evasion</summary>

- *Nikolay Ivanov, Qiben Yan*

- `2108.14004v1` - [abs](http://arxiv.org/abs/2108.14004v1) - [pdf](http://arxiv.org/pdf/2108.14004v1)

> Hardware wallets are designed to withstand malware attacks by isolating their private keys from the cyberspace, but they are vulnerable to the attacks that fake an address stored in a clipboard. To prevent such attacks, a hardware wallet asks the user to verify the recipient address shown on the wallet display. Since crypto addresses are long sequences of random symbols, their manual verification becomes a difficult task. Consequently, many users of hardware wallets elect to verify only a few symbols in the address, and this can be exploited by an attacker. In this work, we introduce EthClipper, an attack that targets owners of hardware wallets on the Ethereum platform. EthClipper malware queries a distributed database of pre-mined accounts in order to select the address with maximum visual similarity to the original one. We design and implement a EthClipper malware, which we test on Trezor, Ledger, and KeepKey wallets. To deliver computation and storage resources for the attack, we implement a distributed service, ClipperCloud, and test it on different deployment environments. Our evaluation shows that with off-the-shelf PCs and NAS storage, an attacker would be able to mine a database capable of matching 25% of the digits in an address to achieve a 50% chance of finding a fitting fake address. For responsible disclosure, we have contacted the manufactures of the hardware wallets used in the attack evaluation, and they all confirm the danger of EthClipper.

</details>

<details>

<summary>2021-08-31 20:32:49 - Students' Information Privacy Concerns in Learning Analytics: Towards a Model Development</summary>

- *Chantal Mutimukwe, Jean Damascene Twizeyimana, Olga Viberg*

- `2109.00068v1` - [abs](http://arxiv.org/abs/2109.00068v1) - [pdf](http://arxiv.org/pdf/2109.00068v1)

> The widespread interest in learning analytics (LA) is associated with increased availability of and access to student data where students' actions are monitored, collected, stored and analysed. The availability and analysis of such data is argued to be crucial for improved learning and teaching. Yet, these data can be exposed to misuse, for example to be used for commercial purposes, consequently, resulting in information privacy concerns (IPC) of students who are the key stakeholders and data subjects in the LA context. The main objective of this study is to propose a theoretical model to understand the IPC of students in relation to LA. We explore the IPC as a central construct between its two antecedents: perceived privacy vulnerability and perceived privacy control, and its consequences, trusting beliefs and self-disclosure behavior. Although these relationships have been investigated in other contexts, this study aims to offer mainly theoretical insights on how these relationships may be shaped in the context of LA in higher education. Understanding students' IPC, the related root causes and consequences in LA is the key step to a more comprehensive understanding of privacy issues and the development of effective privacy practices that would protect students' privacy in the evolving setting of data-driven higher education.

</details>


## 2021-09

<details>

<summary>2021-09-01 00:54:49 - Let Your Camera See for You: A Novel Two-Factor Authentication Method against Real-Time Phishing Attacks</summary>

- *Yuanyi Sun, Sencun Zhu, Yao Zhao, Pengfei Sun*

- `2109.00132v1` - [abs](http://arxiv.org/abs/2109.00132v1) - [pdf](http://arxiv.org/pdf/2109.00132v1)

> Today, two-factor authentication (2FA) is a widely implemented mechanism to counter phishing attacks. Although much effort has been investigated in 2FA, most 2FA systems are still vulnerable to carefully designed phishing attacks, and some even request special hardware, which limits their wide deployment. Recently, real-time phishing (RTP) has made the situation even worse because an adversary can effortlessly establish a phishing website replicating a target website without any background of the web page design technique. Traditional 2FA can be easily bypassed by such RTP attacks. In this work, we propose a novel 2FA system to counter RTP attacks. The main idea is to request a user to take a photo of the web browser with the domain name in the address bar as the 2nd authentication factor. The web server side extracts the domain name information based on Optical Character Recognition (OCR), and then determines if the user is visiting this website or a fake one, thus defeating the RTP attacks where an adversary must set up a fake website with a different domain. We prototyped our system and evaluated its performance in various environments. The results showed that PhotoAuth is an effective technique with good scalability. We also showed that compared to other 2FA systems, PhotoAuth has several advantages, especially no special hardware or software support is needed on the client side except a phone, making it readily deployable.

</details>

<details>

<summary>2021-09-01 02:13:24 - Security and privacy for 6G: A survey on prospective technologies and challenges</summary>

- *Van-Linh Nguyen, Po-Ching Lin, Bo-Chao Cheng, Ren-Hung Hwang, Ying-Dar Lin*

- `2108.11861v2` - [abs](http://arxiv.org/abs/2108.11861v2) - [pdf](http://arxiv.org/pdf/2108.11861v2)

> Sixth-generation (6G) mobile networks will have to cope with diverse threats on a space-air-ground integrated network environment, novel technologies, and an accessible user information explosion. However, for now, security and privacy issues for 6G remain largely in concept. This survey provides a systematic overview of security and privacy issues based on prospective technologies for 6G in the physical, connection, and service layers, as well as through lessons learned from the failures of existing security architectures and state-of-the-art defenses. Two key lessons learned are as follows. First, other than inheriting vulnerabilities from the previous generations, 6G has new threat vectors from new radio technologies, such as the exposed location of radio stripes in ultra-massive MIMO systems at Terahertz bands and attacks against pervasive intelligence. Second, physical layer protection, deep network slicing, quantum-safe communications, artificial intelligence (AI) security, platform-agnostic security, real-time adaptive security, and novel data protection mechanisms such as distributed ledgers and differential privacy are the top promising techniques to mitigate the attack magnitude and personal data breaches substantially.

</details>

<details>

<summary>2021-09-01 02:38:56 - Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction</summary>

- *Zhenrui Yue, Zhankui He, Huimin Zeng, Julian McAuley*

- `2109.01165v1` - [abs](http://arxiv.org/abs/2109.01165v1) - [pdf](http://arxiv.org/pdf/2109.01165v1)

> We investigate whether model extraction can be used to "steal" the weights of sequential recommender systems, and the potential threats posed to victims of such attacks. This type of risk has attracted attention in image and text classification, but to our knowledge not in recommender systems. We argue that sequential recommender systems are subject to unique vulnerabilities due to the specific autoregressive regimes used to train them. Unlike many existing recommender attackers, which assume the dataset used to train the victim model is exposed to attackers, we consider a data-free setting, where training data are not accessible. Under this setting, we propose an API-based model extraction method via limited-budget synthetic data generation and knowledge distillation. We investigate state-of-the-art models for sequential recommendation and show their vulnerability under model extraction and downstream attacks. We perform attacks in two stages. (1) Model extraction: given different types of synthetic data and their labels retrieved from a black-box recommender, we extract the black-box model to a white-box model via distillation. (2) Downstream attacks: we attack the black-box model with adversarial samples generated by the white-box recommender. Experiments show the effectiveness of our data-free model extraction and downstream attacks on sequential recommenders in both profile pollution and data poisoning settings.

</details>

<details>

<summary>2021-09-01 06:04:08 - Federated Learning: Issues in Medical Application</summary>

- *Joo Hun Yoo, Hyejun Jeong, Jaehyeok Lee, Tai-Myoung Chung*

- `2109.00202v1` - [abs](http://arxiv.org/abs/2109.00202v1) - [pdf](http://arxiv.org/pdf/2109.00202v1)

> Since the federated learning, which makes AI learning possible without moving local data around, was introduced by google in 2017 it has been actively studied particularly in the field of medicine. In fact, the idea of machine learning in AI without collecting data from local clients is very attractive because data remain in local sites. However, federated learning techniques still have various open issues due to its own characteristics such as non identical distribution, client participation management, and vulnerable environments. In this presentation, the current issues to make federated learning flawlessly useful in the real world will be briefly overviewed. They are related to data/system heterogeneity, client management, traceability, and security. Also, we introduce the modularized federated learning framework, we currently develop, to experiment various techniques and protocols to find solutions for aforementioned issues. The framework will be open to public after development completes.

</details>

<details>

<summary>2021-09-01 16:58:06 - A Comparative Study of Vulnerability Reporting by Software Composition Analysis Tools</summary>

- *Nasif Imtiaz, Seaver Thorne, Laurie Williams*

- `2108.12078v2` - [abs](http://arxiv.org/abs/2108.12078v2) - [pdf](http://arxiv.org/pdf/2108.12078v2)

> Background: Modern software uses many third-party libraries and frameworks as dependencies. Known vulnerabilities in these dependencies are a potential security risk. Software composition analysis (SCA) tools, therefore, are being increasingly adopted by practitioners to keep track of vulnerable dependencies. Aim: The goal of this study is to understand the difference in vulnerability reporting by various SCA tools. Understanding if and how existing SCA tools differ in their analysis may help security practitioners to choose the right tooling and identify future research needs. Method: We present an in-depth case study by comparing the analysis reports of 9 industry-leading SCA tools on a large web application, OpenMRS, composed of Maven (Java) and npm (JavaScript) projects. Results: We find that the tools vary in their vulnerability reporting. The count of reported vulnerable dependencies ranges from 17 to 332 for Maven and from 32 to 239 for npm projects across the studied tools. Similarly, the count of unique known vulnerabilities reported by the tools ranges from 36 to 313 for Maven and from 45 to 234 for npm projects. Our manual analysis of the tools' results suggest that accuracy of the vulnerability database is a key differentiator for SCA tools. Conclusion: We recommend that practitioners should not rely on any single tool at the present, as that can result in missing known vulnerabilities. We point out two research directions in the SCA space: i) establishing frameworks and metrics to identify false positives for dependency vulnerabilities; and ii) building automation technologies for continuous monitoring of vulnerability data from open source package ecosystems.

</details>

<details>

<summary>2021-09-02 01:49:11 - Testing DBMS Performance with Mutations</summary>

- *Xinyu Liu, Qi Zhou, Joy Arulraj, Alessandro Orso*

- `2105.10016v2` - [abs](http://arxiv.org/abs/2105.10016v2) - [pdf](http://arxiv.org/pdf/2105.10016v2)

> Because database systems are the critical component of modern data-intensive applications, it is important to ensure that they operate correctly. To this end, developers extensively test these systems to eliminate bugs that negatively affect functionality. In addition to functional bugs, however, there is another important class of bugs: performance bugs. These bugs negatively affect the response time of a database system and can therefore affect the overall performance of the system. Despite their impact on end-user experience, performance bugs have received considerably less attention than functional bugs.   In this paper, we present AMOEBA, a system for automatically detecting performance bugs in database systems. The core idea behind AMOEBA is to construct query pairs that are semantically equivalent to each other and then compare their response time on the same database system. If the queries exhibit a significant difference in their runtime performance, then the root cause is likely a performance bug in the system. We propose a novel set of structure and predicate mutation rules for constructing query pairs that are likely to uncover performance bugs. We introduce feedback mechanisms for improving the efficacy and computational efficiency of the tool. We evaluate AMOEBA on two widely-used DBMSs, namely PostgreSQL and CockroachDB. AMOEBA has discovered 20 previously-unknown performance bugs, among which developers have already confirmed 14 and fixed 4.

</details>

<details>

<summary>2021-09-02 04:12:13 - Active Learning Under Malicious Mislabeling and Poisoning Attacks</summary>

- *Jing Lin, Ryan Luley, Kaiqi Xiong*

- `2101.00157v4` - [abs](http://arxiv.org/abs/2101.00157v4) - [pdf](http://arxiv.org/pdf/2101.00157v4)

> Deep neural networks usually require large labeled datasets for training to achieve state-of-the-art performance in many tasks, such as image classification and natural language processing. Although a lot of data is created each day by active Internet users, most of these data are unlabeled and are vulnerable to data poisoning attacks. In this paper, we develop an efficient active learning method that requires fewer labeled instances and incorporates the technique of adversarial retraining in which additional labeled artificial data are generated without increasing the budget of the labeling. The generated adversarial examples also provide a way to measure the vulnerability of the model. To check the performance of the proposed method under an adversarial setting, i.e., malicious mislabeling and data poisoning attacks, we perform an extensive evaluation on the reduced CIFAR-10 dataset, which contains only two classes: airplane and frog. Our experimental results demonstrate that the proposed active learning method is efficient for defending against malicious mislabeling and data poisoning attacks. Specifically, whereas the baseline active learning method based on the random sampling strategy performs poorly (about 50%) under a malicious mislabeling attack, the proposed active learning method can achieve the desired accuracy of 89% using only one-third of the dataset on average.

</details>

<details>

<summary>2021-09-02 06:49:00 - EtherClue: Digital investigation of attacks on Ethereum smart contracts</summary>

- *Simon Joseph Aquilina, Fran Casino, Mark Vella, Joshua Ellul, Constantinos Patsakis*

- `2104.05293v2` - [abs](http://arxiv.org/abs/2104.05293v2) - [pdf](http://arxiv.org/pdf/2104.05293v2)

> Programming errors in Ethereum smart contracts can result in catastrophic financial losses from stolen cryptocurrency. While vulnerability detectors can prevent vulnerable contracts from being deployed, this does not mean that such contracts will not be deployed. Once a vulnerable contract is instantiated on the blockchain and becomes the target of attacks, the identification of exploit transactions becomes indispensable in assessing whether it has been actually exploited and identifying which malicious or subverted accounts were involved.   In this work, we study the problem of post-factum investigation of Ethereum attacks using Indicators of Compromise (IoCs) specially crafted for use in the blockchain. IoC definitions need to capture the side-effects of successful exploitation in the context of the Ethereum blockchain. Therefore, we define a model for smart contract execution, comprising multiple abstraction levels that mirror the multiple views of code execution on a blockchain. Subsequently, we compare IoCs defined across the different levels in terms of their effectiveness and practicality through EtherClue, a prototype tool for investigating Ethereum security incidents. Our results illustrate that coarse-grained IoCs defined over blocks of transactions can detect exploit transactions with less computation; however, they are contract-specific and suffer from false negatives. On the other hand, fine-grained IoCs defined over virtual machine instructions can avoid these pitfalls at the expense of increased computation which are nevertheless applicable for practical use.

</details>

<details>

<summary>2021-09-02 07:54:52 - Stop Bugging Me! Evading Modern-Day Wiretapping Using Adversarial Perturbations</summary>

- *Yael Mathov, Tal Ben Senior, Asaf Shabtai, Yuval Elovici*

- `2010.12809v2` - [abs](http://arxiv.org/abs/2010.12809v2) - [pdf](http://arxiv.org/pdf/2010.12809v2)

> Mass surveillance systems for voice over IP (VoIP) conversations pose a great risk to privacy. These automated systems use learning models to analyze conversations, and calls that involve specific topics are routed to a human agent for further examination. In this study, we present an adversarial-learning-based framework for privacy protection for VoIP conversations. We present a novel method that finds a universal adversarial perturbation (UAP), which, when added to the audio stream, prevents an eavesdropper from automatically detecting the conversation's topic. As shown in our experiments, the UAP is agnostic to the speaker or audio length, and its volume can be changed in real time, as needed. Our real-world solution uses a Teensy microcontroller that acts as an external microphone and adds the UAP to the audio in real time. We examine different speakers, VoIP applications (Skype, Zoom, Slack, and Google Meet), and audio lengths. Our results in the real world suggest that our approach is a feasible solution for privacy protection.

</details>

<details>

<summary>2021-09-02 09:14:33 - Advances in adversarial attacks and defenses in computer vision: A survey</summary>

- *Naveed Akhtar, Ajmal Mian, Navid Kardan, Mubarak Shah*

- `2108.00401v2` - [abs](http://arxiv.org/abs/2108.00401v2) - [pdf](http://arxiv.org/pdf/2108.00401v2)

> Deep Learning (DL) is the most widely used tool in the contemporary field of computer vision. Its ability to accurately solve complex problems is employed in vision research to learn deep neural models for a variety of tasks, including security critical applications. However, it is now known that DL is vulnerable to adversarial attacks that can manipulate its predictions by introducing visually imperceptible perturbations in images and videos. Since the discovery of this phenomenon in 2013~[1], it has attracted significant attention of researchers from multiple sub-fields of machine intelligence. In [2], we reviewed the contributions made by the computer vision community in adversarial attacks on deep learning (and their defenses) until the advent of year 2018. Many of those contributions have inspired new directions in this area, which has matured significantly since witnessing the first generation methods. Hence, as a legacy sequel of [2], this literature review focuses on the advances in this area since 2018. To ensure authenticity, we mainly consider peer-reviewed contributions published in the prestigious sources of computer vision and machine learning research. Besides a comprehensive literature review, the article also provides concise definitions of technical terminologies for non-experts in this domain. Finally, this article discusses challenges and future outlook of this direction based on the literature reviewed herein and [2].

</details>

<details>

<summary>2021-09-02 10:50:53 - Survey about social engineering and the Varni na internetu awareness campaign, 2020</summary>

- *Simon Vrhovec*

- `2109.00837v1` - [abs](http://arxiv.org/abs/2109.00837v1) - [pdf](http://arxiv.org/pdf/2109.00837v1)

> This paper reports on a study aiming to explore factors associated with behavioral intention to follow a social engineering awareness campaign. The objectives of this study were to determine how perceived severity, perceived vulnerability, perceived threat, fear, subjective norm, attitude towards behavior, perceived behavioral control, self-efficacy, response efficacy, trust in authorities, perceived regulation, authorities performance, information sensitivity and privacy concern are associated with individuals' behavioral intention to follow a social engineering awareness campaign. The study employed a cross-sectional research design. A survey was conducted among individuals in Slovenia between January and June 2020. A total of 553 respondents completed the survey providing for N=542 useful responses after excluding poorly completed responses (27.9 percent response rate). The survey questionnaire was developed in English. A Slovenian translation of the survey questionnaire is available.

</details>

<details>

<summary>2021-09-02 11:55:26 - Security-Hardening Software Libraries with Ada and SPARK -- A TCP Stack Use Case</summary>

- *Kyriakos Georgiou, Guillaume Cluzel, Paul Butcher, Yannick Moy*

- `2109.10347v1` - [abs](http://arxiv.org/abs/2109.10347v1) - [pdf](http://arxiv.org/pdf/2109.10347v1)

> This white paper demonstrates how the assurance, reliability, and security of an existing professional-grade, open-source embedded TCP/IP stack implementation written in the C programming language is significantly enhanced by adopting the SPARK technology. A multifaceted approach achieves this. Firstly, the TCP layer's C code is being replaced with formally verified SPARK, a subset of the Ada programming language supported by formal verification tools. Then the lower layers, still written in C and on which the TCP layer depends, are modeled using SPARK contracts and validated using symbolic execution with KLEE. Finally, formal contracts for the upper layers are defined to call the TCP layer. The work allowed the detection and correction of two bugs in the TCP layer. In an increasingly connected world, where Cyber Security is of paramount importance, the powerful approach detailed in this work can be applied to any existing critical C library to harden their reliability and security significantly.

</details>

<details>

<summary>2021-09-02 13:55:34 - CURE: Code-Aware Neural Machine Translation for Automatic Program Repair</summary>

- *Nan Jiang, Thibaud Lutellier, Lin Tan*

- `2103.00073v4` - [abs](http://arxiv.org/abs/2103.00073v4) - [pdf](http://arxiv.org/pdf/2103.00073v4)

> Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.   We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.   Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.

</details>

<details>

<summary>2021-09-02 17:19:30 - Imperceptible Adversarial Examples by Spatial Chroma-Shift</summary>

- *Ayberk Aydin, Deniz Sen, Berat Tuna Karli, Oguz Hanoglu, Alptekin Temizel*

- `2108.02502v2` - [abs](http://arxiv.org/abs/2108.02502v2) - [pdf](http://arxiv.org/pdf/2108.02502v2)

> Deep Neural Networks have been shown to be vulnerable to various kinds of adversarial perturbations. In addition to widely studied additive noise based perturbations, adversarial examples can also be created by applying a per pixel spatial drift on input images. While spatial transformation based adversarial examples look more natural to human observers due to absence of additive noise, they still possess visible distortions caused by spatial transformations. Since the human vision is more sensitive to the distortions in the luminance compared to those in chrominance channels, which is one of the main ideas behind the lossy visual multimedia compression standards, we propose a spatial transformation based perturbation method to create adversarial examples by only modifying the color components of an input image. While having competitive fooling rates on CIFAR-10 and NIPS2017 Adversarial Learning Challenge datasets, examples created with the proposed method have better scores with regards to various perceptual quality metrics. Human visual perception studies validate that the examples are more natural looking and often indistinguishable from their original counterparts.

</details>

<details>

<summary>2021-09-03 02:18:57 - A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples</summary>

- *Guanxiong Liu, Issa Khalil, Abdallah Khreishah, NhatHai Phan*

- `2109.01275v1` - [abs](http://arxiv.org/abs/2109.01275v1) - [pdf](http://arxiv.org/pdf/2109.01275v1)

> In this work, we show how to jointly exploit adversarial perturbation and model poisoning vulnerabilities to practically launch a new stealthy attack, dubbed AdvTrojan. AdvTrojan is stealthy because it can be activated only when: 1) a carefully crafted adversarial perturbation is injected into the input examples during inference, and 2) a Trojan backdoor is implanted during the training process of the model. We leverage adversarial noise in the input space to move Trojan-infected examples across the model decision boundary, making it difficult to detect. The stealthiness behavior of AdvTrojan fools the users into accidentally trust the infected model as a robust classifier against adversarial examples. AdvTrojan can be implemented by only poisoning the training data similar to conventional Trojan backdoor attacks. Our thorough analysis and extensive experiments on several benchmark datasets show that AdvTrojan can bypass existing defenses with a success rate close to 100% in most of our experimental scenarios and can be extended to attack federated learning tasks as well.

</details>

<details>

<summary>2021-09-03 11:42:52 - Security Enhancement of Drone Considering the Characteristics of Data Transmitted between Wireless Channels</summary>

- *Daegeon Kim, Hyeonho Lee*

- `2109.01458v1` - [abs](http://arxiv.org/abs/2109.01458v1) - [pdf](http://arxiv.org/pdf/2109.01458v1)

> With the development of the fourth industrial revolution technology, the type and scope of use of drones are rapidly increasing. As interest in drones increases, research on related security vulnerabilities is actively being conducted, and discussions on security measures have also been developed to prevent them. Military has established and applied various security measures for this, and in particular, drones equipped with KCMVP-certified cryptographic modules are required to be introduced as a way to protect data on the drone's wireless channels. However, despite being equipped with such a certified cryptographic module, the drone's operating environment and structural properties have the potential to expose it to several security vulnerabilities. In this paper, we present a theoretical model for these security vulnerabilities and measures to complement them.

</details>

<details>

<summary>2021-09-03 16:45:49 - Improving Metric Dimensionality Reduction with Distributed Topology</summary>

- *Alexander Wagner, Elchanan Solomon, Paul Bendich*

- `2106.07613v2` - [abs](http://arxiv.org/abs/2106.07613v2) - [pdf](http://arxiv.org/pdf/2106.07613v2)

> We propose a novel approach to dimensionality reduction combining techniques of metric geometry and distributed persistent homology, in the form of a gradient-descent based method called DIPOLE. DIPOLE is a dimensionality-reduction post-processing step that corrects an initial embedding by minimizing a loss functional with both a local, metric term and a global, topological term. By fixing an initial embedding method (we use Isomap), DIPOLE can also be viewed as a full dimensionality-reduction pipeline. This framework is based on the strong theoretical and computational properties of distributed persistent homology and comes with the guarantee of almost sure convergence. We observe that DIPOLE outperforms popular methods like UMAP, t-SNE, and Isomap on a number of popular datasets, both visually and in terms of precise quantitative metrics.

</details>

<details>

<summary>2021-09-03 17:52:14 - Privacy and Data Balkanization: Circumventing the Barriers</summary>

- *Bernardo A. Huberman, Tad Hogg*

- `2010.03672v2` - [abs](http://arxiv.org/abs/2010.03672v2) - [pdf](http://arxiv.org/pdf/2010.03672v2)

> The rapid growth in digital data forms the basis for a wide range of new services and research, e.g, large-scale medical studies. At the same time, increasingly restrictive privacy concerns and laws are leading to significant overhead in arranging for sharing or combining different data sets to obtain these benefits. For new applications, where the benefit of combined data is not yet clear, this overhead can inhibit organizations from even trying to determine whether they can mutually benefit from sharing their data. In this paper, we discuss techniques to overcome this difficulty by employing private information transfer to determine whether there is a benefit from sharing data, and whether there is room to negotiate acceptable prices. These techniques involve cryptographic protocols. While currently considered secure, these protocols are potentially vulnerable to the development of quantum technology, particularly for ensuring privacy over significant periods of time into the future. To mitigate this concern, we describe how developments in practical quantum technology can improve the security of these protocols.

</details>

<details>

<summary>2021-09-06 12:18:02 - Byzantine-Robust Federated Learning via Credibility Assessment on Non-IID Data</summary>

- *Kun Zhai, Qiang Ren, Junli Wang, Chungang Yan*

- `2109.02396v1` - [abs](http://arxiv.org/abs/2109.02396v1) - [pdf](http://arxiv.org/pdf/2109.02396v1)

> Federated learning is a novel framework that enables resource-constrained edge devices to jointly learn a model, which solves the problem of data protection and data islands. However, standard federated learning is vulnerable to Byzantine attacks, which will cause the global model to be manipulated by the attacker or fail to converge. On non-iid data, the current methods are not effective in defensing against Byzantine attacks. In this paper, we propose a Byzantine-robust framework for federated learning via credibility assessment on non-iid data (BRCA). Credibility assessment is designed to detect Byzantine attacks by combing adaptive anomaly detection model and data verification. Specially, an adaptive mechanism is incorporated into the anomaly detection model for the training and prediction of the model. Simultaneously, a unified update algorithm is given to guarantee that the global model has a consistent direction. On non-iid data, our experiments demonstrate that the BRCA is more robust to Byzantine attacks compared with conventional methods

</details>

<details>

<summary>2021-09-06 15:06:26 - VulSPG: Vulnerability detection based on slice property graph representation learning</summary>

- *Weining Zheng, Yuan Jiang, Xiaohong Su*

- `2109.02527v1` - [abs](http://arxiv.org/abs/2109.02527v1) - [pdf](http://arxiv.org/pdf/2109.02527v1)

> Vulnerability detection is an important issue in software security. Although various data-driven vulnerability detection methods have been proposed, the task remains challenging since the diversity and complexity of real-world vulnerable code in syntax and semantics make it difficult to extract vulnerable features with regular deep learning models, especially in analyzing a large program. Moreover, the fact that real-world vulnerable codes contain a lot of redundant information unrelated to vulnerabilities will further aggravate the above problem. To mitigate such challenges, we define a novel code representation named Slice Property Graph (SPG), and then propose VulSPG, a new vulnerability detection approach using the improved R-GCN model with triple attention mechanism to identify potential vulnerabilities in SPG. Our approach has at least two advantages over other methods. First, our proposed SPG can reflect the rich semantics and explicit structural information that may be relevance to vulnerabilities, while eliminating as much irrelevant information as possible to reduce the complexity of graph. Second, VulSPG incorporates triple attention mechanism in R-GCNs to achieve more effective learning of vulnerability patterns from SPG. We have extensively evaluated VulSPG on two large-scale datasets with programs from SARD and real-world projects. Experimental results prove the effectiveness and efficiency of VulSPG.

</details>

<details>

<summary>2021-09-06 15:30:10 - Intrusion Detection using Network Traffic Profiling and Machine Learning for IoT</summary>

- *Joseph Rose, Matthew Swann, Gueltoum Bendiab, Stavros Shiaeles, Nicholas Kolokotronis*

- `2109.02544v1` - [abs](http://arxiv.org/abs/2109.02544v1) - [pdf](http://arxiv.org/pdf/2109.02544v1)

> The rapid increase in the use of IoT devices brings many benefits to the digital society, ranging from improved efficiency to higher productivity. However, the limited resources and the open nature of these devices make them vulnerable to various cyber threats. A single compromised device can have an impact on the whole network and lead to major security and physical damages. This paper explores the potential of using network profiling and machine learning to secure IoT against cyber-attacks. The proposed anomaly-based intrusion detection solution dynamically and actively profiles and monitors all networked devices for the detection of IoT device tampering attempts as well as suspicious network transactions. Any deviation from the defined profile is considered to be an attack and is subject to further analysis. Raw traffic is also passed on to the machine learning classifier for examination and identification of potential attacks. Performance assessment of the proposed methodology is conducted on the Cyber-Trust testbed using normal and malicious network traffic. The experimental results show that the proposed anomaly detection system delivers promising results with an overall accuracy of 98.35% and 0.98% of false-positive alarms.

</details>

<details>

<summary>2021-09-06 22:34:04 - Robustness and Generalization via Generative Adversarial Training</summary>

- *Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie, SerNam Lim*

- `2109.02765v1` - [abs](http://arxiv.org/abs/2109.02765v1) - [pdf](http://arxiv.org/pdf/2109.02765v1)

> While deep neural networks have achieved remarkable success in various computer vision tasks, they often fail to generalize to new domains and subtle variations of input images. Several defenses have been proposed to improve the robustness against these variations. However, current defenses can only withstand the specific attack used in training, and the models often remain vulnerable to other input variations. Moreover, these methods often degrade performance of the model on clean images and do not generalize to out-of-domain samples. In this paper we present Generative Adversarial Training, an approach to simultaneously improve the model's generalization to the test set and out-of-domain samples as well as its robustness to unseen adversarial attacks. Instead of altering a low-level pre-defined aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. Adversarial training with these examples enable the model to withstand a wide range of attacks by observing a variety of input alterations during training. We show that our approach not only improves performance of the model on clean images and out-of-domain samples but also makes it robust against unforeseen attacks and outperforms prior work. We validate effectiveness of our method by demonstrating results on various tasks such as classification, segmentation and object detection.

</details>

<details>

<summary>2021-09-06 23:43:53 - Lightweight, Multi-Stage, Compiler-Assisted Application Specialization</summary>

- *Mohannad Alhanahnah, Rithik Jain, Vaibhav Rastogi, Somesh Jha, Thomas Reps*

- `2109.02775v1` - [abs](http://arxiv.org/abs/2109.02775v1) - [pdf](http://arxiv.org/pdf/2109.02775v1)

> Program debloating aims to enhance the performance and reduce the attack surface of bloated applications. Several techniques have been recently proposed to specialize programs. These approaches are either based on unsound strategies or demanding techniques, leading to unsafe results or a high overhead debloating process. In this paper, we address these limitations by applying partial-evaluation principles to generate specialized applications. Our approach relies on a simple observation that an application typically consists of configuration logic, followed by the main logic of the program. The configuration logic specifies what functionality in the main logic should be executed. LMCAS performs partial interpretation to capture a precise program state of the configuration logic based on the supplied inputs. LMCAS then applies partial-evaluation optimizations to generate a specialized program by propagating the constants in the captured partial state, eliminating unwanted code, and preserving the desired functionalities. Our evaluation of LMCAS on commonly used benchmarks and real-world applications shows that it successfully removes unwanted features while preserving the functionality and robustness of the deblated programs, runs faster than prior tools, and reduces the attack surface of specialized programs. LMCAS runs 1500x, 4.6x, and 1.2x faster than the state-of-the-art debloating tools CHISEL, RAZOR, and OCCAM, respectively; achieves 25% reduction in the binary size; reduces the attack surface of code-reuse attacks by removing 51.7% of the total gadgets and eliminating 83% of known CVE vulnerabilities

</details>

<details>

<summary>2021-09-07 01:34:24 - Statistical Model Checking of Common Attack Scenarios on Blockchain</summary>

- *Ivan Fedotov, Anton Khritankov*

- `2109.02803v1` - [abs](http://arxiv.org/abs/2109.02803v1) - [pdf](http://arxiv.org/pdf/2109.02803v1)

> Blockchain technology has developed significantly over the last decade. One of the reasons for this is its sustainability architecture, which does not allow modification of the history of committed transactions. That means that developers should consider blockchain vulnerabilities and eliminate them before the deployment of the system. In this paper, we demonstrate a statistical model checking approach for the verification of blockchain systems on three real-world attack scenarios. We build and verify models of DNS attack, double-spending with memory pool flooding, and consensus delay scenario. After that, we analyze experimental results and propose solutions to avoid these kinds of attacks.

</details>

<details>

<summary>2021-09-07 03:07:03 - Trojan Signatures in DNN Weights</summary>

- *Greg Fields, Mohammad Samragh, Mojan Javaheripi, Farinaz Koushanfar, Tara Javidi*

- `2109.02836v1` - [abs](http://arxiv.org/abs/2109.02836v1) - [pdf](http://arxiv.org/pdf/2109.02836v1)

> Deep neural networks have been shown to be vulnerable to backdoor, or trojan, attacks where an adversary has embedded a trigger in the network at training time such that the model correctly classifies all standard inputs, but generates a targeted, incorrect classification on any input which contains the trigger. In this paper, we present the first ultra light-weight and highly effective trojan detection method that does not require access to the training/test data, does not involve any expensive computations, and makes no assumptions on the nature of the trojan trigger. Our approach focuses on analysis of the weights of the final, linear layer of the network. We empirically demonstrate several characteristics of these weights that occur frequently in trojaned networks, but not in benign networks. In particular, we show that the distribution of the weights associated with the trojan target class is clearly distinguishable from the weights associated with other classes. Using this, we demonstrate the effectiveness of our proposed detection method against state-of-the-art attacks across a variety of architectures, datasets, and trigger types.

</details>

<details>

<summary>2021-09-07 07:12:08 - ÐArcher: Detecting On-Chain-Off-Chain Synchronization Bugs in Decentralized Applications</summary>

- *Wuqi Zhang, Lili Wei, Shuqing Li, Yepang Liu, Shing-Chi Cheung*

- `2106.09440v2` - [abs](http://arxiv.org/abs/2106.09440v2) - [pdf](http://arxiv.org/pdf/2106.09440v2)

> Since the emergence of Ethereum, blockchain-based decentralized applications (DApps) have become increasingly popular and important. To balance the security, performance, and costs, a DApp typically consists of two layers: an on-chain layer to execute transactions and store crucial data on the blockchain and an off-chain layer to interact with users. A DApp needs to synchronize its off-chain layer with the on-chain layer proactively. Otherwise, the inconsistent data in the off-chain layer could mislead users and cause undesirable consequences, e.g., loss of transaction fees. However, transactions sent to the blockchain are not guaranteed to be executed and could even be reversed after execution due to chain reorganization. Such non-determinism in the transaction execution is unique to blockchain. DApp developers may fail to perform the on-chain-off-chain synchronization accurately due to their lack of familiarity with the complex transaction lifecycle. In this work, we investigate the challenges of synchronizing on-chain and off-chain data in Ethereum-based DApps. We present two types of bugs that could result in inconsistencies between the on-chain and off-chain layers. To help detect such on-chain-off-chain synchronization bugs, we introduce a state transition model to guide the testing of DApps and propose two effective oracles to facilitate the automatic identification of bugs. We build the first testing framework, DArcher, to detect on-chain-off-chain synchronization bugs in DApps. We have evaluated DArcher on 11 popular real-world DApps. DArcher achieves high precision (99.3%), recall (87.6%), and accuracy (89.4%) in bug detection and significantly outperforms the baseline methods. It has found 15 real bugs in the 11 DApps. So far, six of the 15 bugs have been confirmed by the developers, and three have been fixed. These promising results demonstrate the usefulness of DArcher.

</details>

<details>

<summary>2021-09-07 13:09:55 - Sequential Diagnosis Prediction with Transformer and Ontological Representation</summary>

- *Xueping Peng, Guodong Long, Tao Shen, Sen Wang, Jing Jiang*

- `2109.03069v1` - [abs](http://arxiv.org/abs/2109.03069v1) - [pdf](http://arxiv.org/pdf/2109.03069v1)

> Sequential diagnosis prediction on the Electronic Health Record (EHR) has been proven crucial for predictive analytics in the medical domain. EHR data, sequential records of a patient's interactions with healthcare systems, has numerous inherent characteristics of temporality, irregularity and data insufficiency. Some recent works train healthcare predictive models by making use of sequential information in EHR data, but they are vulnerable to irregular, temporal EHR data with the states of admission/discharge from hospital, and insufficient data. To mitigate this, we propose an end-to-end robust transformer-based model called SETOR, which exploits neural ordinary differential equation to handle both irregular intervals between a patient's visits with admitted timestamps and length of stay in each visit, to alleviate the limitation of insufficient data by integrating medical ontology, and to capture the dependencies between the patient's visits by employing multi-layer transformer blocks. Experiments conducted on two real-world healthcare datasets show that, our sequential diagnoses prediction model SETOR not only achieves better predictive results than previous state-of-the-art approaches, irrespective of sufficient or insufficient training data, but also derives more interpretable embeddings of medical codes. The experimental codes are available at the GitHub repository (https://github.com/Xueping/SETOR).

</details>

<details>

<summary>2021-09-07 13:31:02 - Proceedings of KDD 2020 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning</summary>

- *Snehalkumar, S. Gaikwad, Shankar Iyer, Dalton Lunga, Yu-Ru Lin*

- `2109.00435v3` - [abs](http://arxiv.org/abs/2109.00435v3) - [pdf](http://arxiv.org/pdf/2109.00435v3)

> Humanitarian challenges, including natural disasters, food insecurity, climate change, racial and gender violence, environmental crises, the COVID-19 coronavirus pandemic, human rights violations, and forced displacements, disproportionately impact vulnerable communities worldwide. According to UN OCHA, 235 million people will require humanitarian assistance in 2021 . Despite these growing perils, there remains a notable paucity of data science research to scientifically inform equitable public policy decisions for improving the livelihood of at-risk populations. Scattered data science efforts exist to address these challenges, but they remain isolated from practice and prone to algorithmic harms concerning lack of privacy, fairness, interpretability, accountability, transparency, and ethics. Biases in data-driven methods carry the risk of amplifying inequalities in high-stakes policy decisions that impact the livelihood of millions of people. Consequently, proclaimed benefits of data-driven innovations remain inaccessible to policymakers, practitioners, and marginalized communities at the core of humanitarian actions and global development. To help fill this gap, we propose the Data-driven Humanitarian Mapping Research Program, which focuses on developing novel data science methodologies that harness human-machine intelligence for high-stakes public policy and resilience planning.   The proceedings of the 1st Data-driven Humanitarian Mapping workshop at the 26th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, August 24th, 2020.

</details>

<details>

<summary>2021-09-07 13:32:08 - Proceedings of KDD 2021 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning</summary>

- *Snehalkumar, S. Gaikwad, Shankar Iyer, Dalton Lunga, Elizabeth Bondi*

- `2109.00100v4` - [abs](http://arxiv.org/abs/2109.00100v4) - [pdf](http://arxiv.org/pdf/2109.00100v4)

> Humanitarian challenges, including natural disasters, food insecurity, climate change, racial and gender violence, environmental crises, the COVID-19 coronavirus pandemic, human rights violations, and forced displacements, disproportionately impact vulnerable communities worldwide. According to UN OCHA, 235 million people will require humanitarian assistance in 2021. Despite these growing perils, there remains a notable paucity of data science research to scientifically inform equitable public policy decisions for improving the livelihood of at-risk populations. Scattered data science efforts exist to address these challenges, but they remain isolated from practice and prone to algorithmic harms concerning lack of privacy, fairness, interpretability, accountability, transparency, and ethics. Biases in data-driven methods carry the risk of amplifying inequalities in high-stakes policy decisions that impact the livelihood of millions of people. Consequently, proclaimed benefits of data-driven innovations remain inaccessible to policymakers, practitioners, and marginalized communities at the core of humanitarian actions and global development. To help fill this gap, we propose the Data-driven Humanitarian Mapping Research Program, which focuses on developing novel data science methodologies that harness human-machine intelligence for high-stakes public policy and resilience planning.   The proceedings of the 2nd Data-driven Humanitarian Mapping workshop at the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. August 15th, 2021

</details>

<details>

<summary>2021-09-07 20:52:44 - CyGIL: A Cyber Gym for Training Autonomous Agents over Emulated Network Systems</summary>

- *Li Li, Raed Fayad, Adrian Taylor*

- `2109.03331v1` - [abs](http://arxiv.org/abs/2109.03331v1) - [pdf](http://arxiv.org/pdf/2109.03331v1)

> Given the success of reinforcement learning (RL) in various domains, it is promising to explore the application of its methods to the development of intelligent and autonomous cyber agents. Enabling this development requires a representative RL training environment. To that end, this work presents CyGIL: an experimental testbed of an emulated RL training environment for network cyber operations. CyGIL uses a stateless environment architecture and incorporates the MITRE ATT&CK framework to establish a high fidelity training environment, while presenting a sufficiently abstracted interface to enable RL training. Its comprehensive action space and flexible game design allow the agent training to focus on particular advanced persistent threat (APT) profiles, and to incorporate a broad range of potential threats and vulnerabilities. By striking a balance between fidelity and simplicity, it aims to leverage state of the art RL algorithms for application to real-world cyber defence.

</details>

<details>

<summary>2021-09-07 21:24:36 - Software Vulnerability Detection via Deep Learning over Disaggregated Code Graph Representation</summary>

- *Yufan Zhuang, Sahil Suneja, Veronika Thost, Giacomo Domeniconi, Alessandro Morari, Jim Laredo*

- `2109.03341v1` - [abs](http://arxiv.org/abs/2109.03341v1) - [pdf](http://arxiv.org/pdf/2109.03341v1)

> Identifying vulnerable code is a precautionary measure to counter software security breaches. Tedious expert effort has been spent to build static analyzers, yet insecure patterns are barely fully enumerated. This work explores a deep learning approach to automatically learn the insecure patterns from code corpora. Because code naturally admits graph structures with parsing, we develop a novel graph neural network (GNN) to exploit both the semantic context and structural regularity of a program, in order to improve prediction performance. Compared with a generic GNN, our enhancements include a synthesis of multiple representations learned from the several parsed graphs of a program, and a new training loss metric that leverages the fine granularity of labeling. Our model outperforms multiple text, image and graph-based approaches, across two real-world datasets.

</details>

<details>

<summary>2021-09-08 01:57:12 - Improving Adversarial Robustness via Attention and Adversarial Logit Pairing</summary>

- *Dou Goodman, Xingjian Li, Ji Liu, Dejing Dou, Tao Wei*

- `1908.11435v2` - [abs](http://arxiv.org/abs/1908.11435v2) - [pdf](http://arxiv.org/pdf/1908.11435v2)

> Though deep neural networks have achieved the state of the art performance in visual classification, recent studies have shown that they are all vulnerable to the attack of adversarial examples. In this paper, we develop improved techniques for defending against adversarial examples. First, we propose an enhanced defense technique denoted Attention and Adversarial Logit Pairing(AT+ALP), which encourages both attention map and logit for the pairs of examples to be similar. When being applied to clean examples and their adversarial counterparts, AT+ALP improves accuracy on adversarial examples over adversarial training. We show that AT+ALP can effectively increase the average activations of adversarial examples in the key area and demonstrate that it focuses on discriminate features to improve the robustness of the model. Finally, we conduct extensive experiments using a wide range of datasets and the experiment results show that our AT+ALP achieves the state of the art defense performance. For example, on 17 Flower Category Database, under strong 200-iteration PGD gray-box and black-box attacks where prior art has 34% and 39% accuracy, our method achieves 50% and 51%. Compared with previous work, our work is evaluated under highly challenging PGD attack: the maximum perturbation $\epsilon \in \{0.25,0.5\}$ i.e. $L_\infty \in \{0.25,0.5\}$ with 10 to 200 attack iterations. To the best of our knowledge, such a strong attack has not been previously explored on a wide range of datasets.

</details>

<details>

<summary>2021-09-08 07:46:42 - Challenges and Countermeasures for Adversarial Attacks on Deep Reinforcement Learning</summary>

- *Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala Al-Fuqaha, Dinh Thai Hoang, Dusit Niyato*

- `2001.09684v2` - [abs](http://arxiv.org/abs/2001.09684v2) - [pdf](http://arxiv.org/pdf/2001.09684v2)

> Deep Reinforcement Learning (DRL) has numerous applications in the real world thanks to its outstanding ability in quickly adapting to the surrounding environments. Despite its great advantages, DRL is susceptible to adversarial attacks, which precludes its use in real-life critical systems and applications (e.g., smart grids, traffic controls, and autonomous vehicles) unless its vulnerabilities are addressed and mitigated. Thus, this paper provides a comprehensive survey that discusses emerging attacks in DRL-based systems and the potential countermeasures to defend against these attacks. We first cover some fundamental backgrounds about DRL and present emerging adversarial attacks on machine learning techniques. We then investigate more details of the vulnerabilities that the adversary can exploit to attack DRL along with the state-of-the-art countermeasures to prevent such attacks. Finally, we highlight open issues and research challenges for developing solutions to deal with attacks for DRL-based intelligent systems.

</details>

<details>

<summary>2021-09-08 09:02:30 - Bionic Optical Physical Unclonable Functions for Authentication and Encryption</summary>

- *Yongbiao Wan, Pidong Wang, Feng Huang, Jun Yuan, Dong Li, Kun Chen, Jianbin Kang, Qian Li, Taiping Zhang, Song Sun, Zhiguang Qiu, Yao Yao*

- `2109.03505v1` - [abs](http://arxiv.org/abs/2109.03505v1) - [pdf](http://arxiv.org/pdf/2109.03505v1)

> Information security is of great importance for modern society with all things connected. Physical unclonable function (PUF) as a promising hardware primitive has been intensively studied for information security. However, the widely investigated silicon PUF with low entropy is vulnerable to various attacks. Herein, we introduce a concept of bionic optical PUFs inspired from unique biological architectures, and fabricate four types of bionic PUFs by molding the surface micro-nano structures of natural plant tissues with a simple, low-cost, green and environmentally friendly manufacturing process. The laser speckle responses of all bionic PUFs are statistically demonstrated to be random, unique, unpredictable and robust enough for cryptographic applications, indicating the broad applicability of bionic PUFs. On this ground, the feasibility of implementing bionic PUFs as cryptographic primitives in entity authentication and encrypted communication is experimentally validated, which shows its promising potential in the application of future information security.

</details>

<details>

<summary>2021-09-08 11:37:38 - BLESER: Bug Localization Based on Enhanced Semantic Retrieval</summary>

- *Weiqin Zou, Enming Li, Chunrong Fang*

- `2109.03555v1` - [abs](http://arxiv.org/abs/2109.03555v1) - [pdf](http://arxiv.org/pdf/2109.03555v1)

> Static bug localization techniques that locate bugs at method granularity have gained much attention from both researchers and practitioners. For a static method-level bug localization technique, a key but challenging step is to fully retrieve the semantics of methods and bug reports. Currently, existing studies mainly use the same bag-of-word space to represent the semantics of methods and bug reports without considering structure information of methods and textual contexts of bug reports, which largely and negatively affects bug localization performance.   To address this problem, we develop BLESER, a new bug localization technique based on enhanced semantic retrieval. Specifically, we use an AST-based code embedding model (capturing code structure better) to retrieve the semantics of methods, and word embedding models (capturing textual contexts better) to represent the semantics of bug reports. Then, a deep learning model is built on the enhanced semantic representations. During model building, we compare five typical word embedding models in representing bug reports and try to explore the usefulness of re-sampling strategies and cost-sensitive strategies in handling class imbalance problems. We evaluate our BLESER on five Java projects from the Defects4J dataset. We find that: (1) On the whole, the word embedding model ELMo outperformed the other four models (including word2vec, BERT, etc.) in facilitating bug localization techniques. (2) Among four strategies aiming at solving class imbalance problems, the strategy ROS (random over-sampling) performed much better than the other three strategies (including random under-sampling, Focal Loss, etc.). (3) By integrating ELMo and ROS into BLESER, at method-level bug localization, we could achieve MAP of 0.108-0.504, MRR of 0.134-0.510, and Accuracy@1 of 0.125-0.5 on five Defects4J projects.

</details>

<details>

<summary>2021-09-08 20:44:44 - Editing Factual Knowledge in Language Models</summary>

- *Nicola De Cao, Wilker Aziz, Ivan Titov*

- `2104.08164v2` - [abs](http://arxiv.org/abs/2104.08164v2) - [pdf](http://arxiv.org/pdf/2104.08164v2)

> The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix 'bugs' or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor's efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a 'probe' revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor

</details>

<details>

<summary>2021-09-09 01:36:39 - The challenge of reproducible ML: an empirical study on the impact of bugs</summary>

- *Emilio Rivera-Landos, Foutse Khomh, Amin Nikanjam*

- `2109.03991v1` - [abs](http://arxiv.org/abs/2109.03991v1) - [pdf](http://arxiv.org/pdf/2109.03991v1)

> Reproducibility is a crucial requirement in scientific research. When results of research studies and scientific papers have been found difficult or impossible to reproduce, we face a challenge which is called reproducibility crisis. Although the demand for reproducibility in Machine Learning (ML) is acknowledged in the literature, a main barrier is inherent non-determinism in ML training and inference. In this paper, we establish the fundamental factors that cause non-determinism in ML systems. A framework, ReproduceML, is then introduced for deterministic evaluation of ML experiments in a real, controlled environment. ReproduceML allows researchers to investigate software configuration effects on ML training and inference. Using ReproduceML, we run a case study: investigation of the impact of bugs inside ML libraries on performance of ML experiments. This study attempts to quantify the impact that the occurrence of bugs in a popular ML framework, PyTorch, has on the performance of trained models. To do so, a comprehensive methodology is proposed to collect buggy versions of ML libraries and run deterministic ML experiments using ReproduceML. Our initial finding is that there is no evidence based on our limited dataset to show that bugs which occurred in PyTorch do affect the performance of trained models. The proposed methodology as well as ReproduceML can be employed for further research on non-determinism and bugs.

</details>

<details>

<summary>2021-09-09 02:58:44 - Adversarial Attacks are Reversible with Natural Supervision</summary>

- *Chengzhi Mao, Mia Chiquier, Hao Wang, Junfeng Yang, Carl Vondrick*

- `2103.14222v3` - [abs](http://arxiv.org/abs/2103.14222v3) - [pdf](http://arxiv.org/pdf/2103.14222v3)

> We find that images contain intrinsic structure that enables the reversal of many adversarial attacks. Attack vectors cause not only image classifiers to fail, but also collaterally disrupt incidental structure in the image. We demonstrate that modifying the attacked image to restore the natural structure will reverse many types of attacks, providing a defense. Experiments demonstrate significantly improved robustness for several state-of-the-art models across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results show that our defense is still effective even if the attacker is aware of the defense mechanism. Since our defense is deployed during inference instead of training, it is compatible with pre-trained networks as well as most other defenses. Our results suggest deep networks are vulnerable to adversarial examples partly because their representations do not enforce the natural structure of images.

</details>

<details>

<summary>2021-09-09 04:42:24 - Automated Security Assessment for the Internet of Things</summary>

- *Xuanyu Duan, Mengmeng Ge, Triet H. M. Le, Faheem Ullah, Shang Gao, Xuequan Lu, M. Ali Babar*

- `2109.04029v1` - [abs](http://arxiv.org/abs/2109.04029v1) - [pdf](http://arxiv.org/pdf/2109.04029v1)

> Internet of Things (IoT) based applications face an increasing number of potential security risks, which need to be systematically assessed and addressed. Expert-based manual assessment of IoT security is a predominant approach, which is usually inefficient. To address this problem, we propose an automated security assessment framework for IoT networks. Our framework first leverages machine learning and natural language processing to analyze vulnerability descriptions for predicting vulnerability metrics. The predicted metrics are then input into a two-layered graphical security model, which consists of an attack graph at the upper layer to present the network connectivity and an attack tree for each node in the network at the bottom layer to depict the vulnerability information. This security model automatically assesses the security of the IoT network by capturing potential attack paths. We evaluate the viability of our approach using a proof-of-concept smart building system model which contains a variety of real-world IoT devices and potential vulnerabilities. Our evaluation of the proposed framework demonstrates its effectiveness in terms of automatically predicting the vulnerability metrics of new vulnerabilities with more than 90% accuracy, on average, and identifying the most vulnerable attack paths within an IoT network. The produced assessment results can serve as a guideline for cybersecurity professionals to take further actions and mitigate risks in a timely manner.

</details>

<details>

<summary>2021-09-09 09:42:01 - Survey about cyberattack protection motivation in higher education: Academics at Slovenian universities, 2017</summary>

- *Luka Jelovčan, Simon Vrhovec, Anže Mihelič*

- `2109.04132v1` - [abs](http://arxiv.org/abs/2109.04132v1) - [pdf](http://arxiv.org/pdf/2109.04132v1)

> This paper reports on a study aiming to explore factors associated with motivation of individuals in organizations to protect against cyberattacks. The objectives of this study were to determine how fear of cyberattacks, perceived severity, perceived vulnerability, perceived threats, measure efficacy, self-efficacy, measure costs, mandatoriness and psychological reactance are associated with protection motivation of individuals in organizations. The study employed a cross-sectional research design. A survey was conducted among academics at six Slovenian universities between June and September 2017. A total of 324 respondents completed the survey (7.6 percent response rate) providing for N=255 useful responses after excluding poorly completed responses. The survey questionnaire was developed in English. A Slovenian translation of the survey questionnaire is available.

</details>

<details>

<summary>2021-09-09 14:32:24 - Social Media Monitoring for IoT Cyber-Threats</summary>

- *Sofia Alevizopoulou, Paris Koloveas, Christos Tryfonopoulos, Paraskevi Raftopoulou*

- `2109.04306v1` - [abs](http://arxiv.org/abs/2109.04306v1) - [pdf](http://arxiv.org/pdf/2109.04306v1)

> The rapid development of IoT applications and their use in various fields of everyday life has resulted in an escalated number of different possible cyber-threats, and has consequently raised the need of securing IoT devices. Collecting Cyber-Threat Intelligence (e.g., zero-day vulnerabilities or trending exploits) from various online sources and utilizing it to proactively secure IoT systems or prepare mitigation scenarios has proven to be a promising direction. In this work, we focus on social media monitoring and investigate real-time Cyber-Threat Intelligence detection from the Twitter stream. Initially, we compare and extensively evaluate six different machine-learning based classification alternatives trained with vulnerability descriptions and tested with real-world data from the Twitter stream to identify the best-fitting solution. Subsequently, based on our findings, we propose a novel social media monitoring system tailored to the IoT domain; the system allows users to identify recent/trending vulnerabilities and exploits on IoT devices. Finally, to aid research on the field and support the reproducibility of our results we publicly release all annotated datasets created during this process.

</details>

<details>

<summary>2021-09-09 16:16:04 - Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification</summary>

- *Maximilian Mozes, Max Bartolo, Pontus Stenetorp, Bennett Kleinberg, Lewis D. Griffin*

- `2109.04385v1` - [abs](http://arxiv.org/abs/2109.04385v1) - [pdf](http://arxiv.org/pdf/2109.04385v1)

> Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human language ability. We report on crowdsourcing studies in which we task humans with iteratively modifying words in an input text, while receiving immediate model feedback, with the aim of causing a sentiment classification model to misclassify the example. Our findings suggest that humans are capable of generating a substantial amount of adversarial examples using semantics-preserving word substitutions. We analyze how human-generated adversarial examples compare to the recently proposed TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions naturalness, preservation of sentiment, grammaticality and substitution rate. Our findings suggest that human-generated adversarial examples are not more able than the best algorithms to generate natural-reading, sentiment-preserving examples, though they do so by being much more computationally efficient.

</details>

<details>

<summary>2021-09-10 01:31:53 - Spatially Focused Attack against Spatiotemporal Graph Neural Networks</summary>

- *Fuqiang Liu, Luis Miranda-Moreno, Lijun Sun*

- `2109.04608v1` - [abs](http://arxiv.org/abs/2109.04608v1) - [pdf](http://arxiv.org/pdf/2109.04608v1)

> Spatiotemporal forecasting plays an essential role in various applications in intelligent transportation systems (ITS), such as route planning, navigation, and traffic control and management. Deep Spatiotemporal graph neural networks (GNNs), which capture both spatial and temporal patterns, have achieved great success in traffic forecasting applications. Understanding how GNNs-based forecasting work and the vulnerability and robustness of these models becomes critical to real-world applications. For example, if spatiotemporal GNNs are vulnerable in real-world traffic prediction applications, a hacker can easily manipulate the results and cause serious traffic congestion and even a city-scale breakdown. However, despite that recent studies have demonstrated that deep neural networks (DNNs) are vulnerable to carefully designed perturbations in multiple domains like objection classification and graph representation, current adversarial works cannot be directly applied to spatiotemporal forecasting due to the causal nature and spatiotemporal mechanisms in forecasting models. To fill this gap, in this paper we design Spatially Focused Attack (SFA) to break spatiotemporal GNNs by attacking a single vertex. To achieve this, we first propose the inverse estimation to address the causality issue; then, we apply genetic algorithms with a universal attack method as the evaluation function to locate the weakest vertex; finally, perturbations are generated by solving an inverse estimation-based optimization problem. We conduct experiments on real-world traffic data and our results show that perturbations in one vertex designed by SA can be diffused into a large part of the graph.

</details>

<details>

<summary>2021-09-10 02:01:13 - Cybersecurity in Robotics: Challenges, Quantitative Modeling, and Practice</summary>

- *Quanyan Zhu, Stefan Rass, Bernhard Dieber, Victor Mayoral Vilches*

- `2103.05789v4` - [abs](http://arxiv.org/abs/2103.05789v4) - [pdf](http://arxiv.org/pdf/2103.05789v4)

> Robotics is becoming more and more ubiquitous, but the pressure to bring systems to market occasionally goes at the cost of neglecting security mechanisms during the development, deployment or while in production. As a result, contemporary robotic systems are vulnerable to diverse attack patterns, and an a posteriori hardening is at least challenging, if not impossible at all. This book aims to stipulate the inclusion of security in robotics from the earliest design phases onward and with a special focus on the cost-benefit tradeoff that can otherwise be an inhibitor for the fast development of affordable systems. We advocate quantitative methods of security management and design, covering vulnerability scoring systems tailored to robotic systems, and accounting for the highly distributed nature of robots as an interplay of potentially very many components. A powerful quantitative approach to model-based security is offered by game theory, providing a rich spectrum of techniques to optimize security against various kinds of attacks. Such a multi-perspective view on security is necessary to address the heterogeneity and complexity of robotic systems. This book is intended as an accessible starter for the theoretician and practitioner working in the field.

</details>

<details>

<summary>2021-09-10 04:43:51 - Sixteen Years of Phishing User Studies: What Have We Learned?</summary>

- *Shahryar Baki, Rakesh Verma*

- `2109.04661v1` - [abs](http://arxiv.org/abs/2109.04661v1) - [pdf](http://arxiv.org/pdf/2109.04661v1)

> Several previous studies have investigated user susceptibility to phishing attacks. A thorough meta-analysis or systematic review is required to gain a better understanding of these findings and to assess the strength of evidence for phishing susceptibility of a subpopulation, e.g., older users. We aim to determine whether an effect exists; another aim is to determine whether the effect is positive or negative and to obtain a single summary estimate of the effect.   OBJECTIVES: We systematically review the results of previous user studies on phishing susceptibility and conduct a meta-analysis.   METHOD: We searched four online databases for English studies on phishing. We included all user studies in phishing detection and prevention, whether they proposed new training techniques or analyzed users' vulnerability.   FINDINGS: A careful analysis reveals some discrepancies between the findings. More than half of the studies that analyzed the effect of age reported no statistically significant relationship between age and users' performance. Some studies reported older people performed better while some reported the opposite. A similar finding holds for the gender difference. The meta-analysis shows: 1) a significant relationship between participants' age and their susceptibility 2) females are more susceptible than males 3) users training significantly improves their detection ability

</details>

<details>

<summary>2021-09-10 06:20:17 - Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation</summary>

- *Shahar Levy, Koren Lazar, Gabriel Stanovsky*

- `2109.03858v2` - [abs](http://arxiv.org/abs/2109.03858v2) - [pdf](http://arxiv.org/pdf/2109.03858v2)

> Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains, resulting in a first large-scale gender bias dataset of 108K diverse real-world English sentences. We manually verify the quality of our corpus and use it to evaluate gender bias in various coreference resolution and machine translation models. We find that all tested models tend to over-rely on gender stereotypes when presented with natural inputs, which may be especially harmful when deployed in commercial systems. Finally, we show that our dataset lends itself to finetuning a coreference resolution model, finding it mitigates bias on a held out set. Our dataset and models are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will spur future research into gender bias evaluation mitigation techniques in realistic settings.

</details>

<details>

<summary>2021-09-10 14:34:32 - Vulnerabilities and Attacks Against Industrial Control Systems and Critical Infrastructures</summary>

- *Georgios Michail Makrakis, Constantinos Kolias, Georgios Kambourakis, Craig Rieger, Jacob Benjamin*

- `2109.03945v2` - [abs](http://arxiv.org/abs/2109.03945v2) - [pdf](http://arxiv.org/pdf/2109.03945v2)

> Critical infrastructures (CI) and industrial organizations aggressively move towards integrating elements of modern Information Technology (IT) into their monolithic Operational Technology (OT) architectures. Yet, as OT systems progressively become more and more interconnected, they silently have turned into alluring targets for diverse groups of adversaries. Meanwhile, the inherent complexity of these systems, along with their advanced-in-age nature, prevents defenders from fully applying contemporary security controls in a timely manner. Forsooth, the combination of these hindering factors has led to some of the most severe cybersecurity incidents of the past years. This work contributes a full-fledged and up-to-date survey of the most prominent threats against Industrial Control Systems (ICS) along with the communication protocols and devices adopted in these environments. Our study highlights that threats against CI follow an upward spiral due to the mushrooming of commodity tools and techniques that can facilitate either the early or late stages of attacks. Furthermore, our survey exposes that existing vulnerabilities in the design and implementation of several of the OT-specific network protocols may easily grant adversaries the ability to decisively impact physical processes. We provide a categorization of such threats and the corresponding vulnerabilities based on various criteria. As far as we are aware, this is the first time an exhaustive and detailed survey of this kind is attempted.

</details>

<details>

<summary>2021-09-10 17:43:47 - Revisiting Robust Neural Machine Translation: A Transformer Case Study</summary>

- *Peyman Passban, Puneeth S. M. Saladi, Qun Liu*

- `2012.15710v2` - [abs](http://arxiv.org/abs/2012.15710v2) - [pdf](http://arxiv.org/pdf/2012.15710v2)

> Transformers (Vaswani et al., 2017) have brought a remarkable improvement in the performance of neural machine translation (NMT) systems but they could be surprisingly vulnerable to noise. In this work, we try to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behavior of conventional models for the problem of noise but Transformers are relatively understudied in this context. Motivated by this, we introduce a novel data-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two other novel extensions to the original Transformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that modify the neural architecture as well as the training process to handle noise. One important characteristic of our techniques is that they only impact the training phase and do not impose any overhead at inference time. We evaluated our techniques to translate the English--German pair in both directions and observed that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to 10% of entire test words are infected by noise.

</details>

<details>

<summary>2021-09-11 02:45:52 - Understanding Structural Vulnerability in Graph Convolutional Networks</summary>

- *Liang Chen, Jintang Li, Qibiao Peng, Yang Liu, Zibin Zheng, Carl Yang*

- `2108.06280v2` - [abs](http://arxiv.org/abs/2108.06280v2) - [pdf](http://arxiv.org/pdf/2108.06280v2)

> Recent studies have shown that Graph Convolutional Networks (GCNs) are vulnerable to adversarial attacks on the graph structure. Although multiple works have been proposed to improve their robustness against such structural adversarial attacks, the reasons for the success of the attacks remain unclear. In this work, we theoretically and empirically demonstrate that structural adversarial examples can be attributed to the non-robust aggregation scheme (i.e., the weighted mean) of GCNs. Specifically, our analysis takes advantage of the breakdown point which can quantitatively measure the robustness of aggregation schemes. The key insight is that weighted mean, as the basic design of GCNs, has a low breakdown point and its output can be dramatically changed by injecting a single edge. We show that adopting the aggregation scheme with a high breakdown point (e.g., median or trimmed mean) could significantly enhance the robustness of GCNs against structural attacks. Extensive experiments on four real-world datasets demonstrate that such a simple but effective method achieves the best robustness performance compared to state-of-the-art models.

</details>

<details>

<summary>2021-09-12 16:45:30 - CoG: a Two-View Co-training Framework for Defending Adversarial Attacks on Graph</summary>

- *Xugang Wu, Huijun Wu, Xu Zhou, Kai Lu*

- `2109.05558v1` - [abs](http://arxiv.org/abs/2109.05558v1) - [pdf](http://arxiv.org/pdf/2109.05558v1)

> Graph neural networks exhibit remarkable performance in graph data analysis. However, the robustness of GNN models remains a challenge. As a result, they are not reliable enough to be deployed in critical applications. Recent studies demonstrate that GNNs could be easily fooled with adversarial perturbations, especially structural perturbations. Such vulnerability is attributed to the excessive dependence on the structure information to make predictions. To achieve better robustness, it is desirable to build the prediction of GNNs with more comprehensive features. Graph data, in most cases, has two views of information, namely structure information and feature information. In this paper, we propose CoG, a simple yet effective co-training framework to combine these two views for the purpose of robustness. CoG trains sub-models from the feature view and the structure view independently and allows them to distill knowledge from each other by adding their most confident unlabeled data into the training set. The orthogonality of these two views diversifies the sub-models, thus enhancing the robustness of their ensemble. We evaluate our framework on three popular datasets, and results show that CoG significantly improves the robustness of graph models against adversarial attacks without sacrificing their performance on clean data. We also show that CoG still achieves good robustness when both node features and graph structures are perturbed.

</details>

<details>

<summary>2021-09-12 18:56:20 - Machine Learning Interpretability Meets TLS Fingerprinting</summary>

- *Mahdi Jafari Siavoshani, Amir Hossein Khajepour, Amirmohammad Ziaei, Amir Ali Gatmiri, Ali Taheri*

- `2011.06304v2` - [abs](http://arxiv.org/abs/2011.06304v2) - [pdf](http://arxiv.org/pdf/2011.06304v2)

> Protecting users' privacy over the Internet is of great importance; however, it becomes harder and harder to maintain due to the increasing complexity of network protocols and components. Therefore, investigating and understanding how data is leaked from the information transmission platforms and protocols can lead us to a more secure environment.   In this paper, we propose a framework to systematically find the most vulnerable information fields in a network protocol. To this end, focusing on the transport layer security (TLS) protocol, we perform different machine-learning-based fingerprinting attacks on the collected data from more than 70 domains (websites) to understand how and where this information leakage occurs in the TLS protocol. Then, by employing the interpretation techniques developed in the machine learning community and applying our framework, we find the most vulnerable information fields in the TLS protocol. Our findings demonstrate that the TLS handshake (which is mainly unencrypted), the TLS record length appearing in the TLS application data header, and the initialization vector (IV) field are among the most critical leaker parts in this protocol, respectively.

</details>

<details>

<summary>2021-09-12 23:45:49 - Fixing Vulnerabilities Potentially Hinders Maintainability</summary>

- *Sofia Reis, Rui Abreu, Luis Cruz*

- `2106.03271v2` - [abs](http://arxiv.org/abs/2106.03271v2) - [pdf](http://arxiv.org/pdf/2106.03271v2)

> Security is a requirement of utmost importance to produce high-quality software. However, there is still a considerable amount of vulnerabilities being discovered and fixed almost weekly. We hypothesize that developers affect the maintainability of their codebases when patching vulnerabilities. This paper evaluates the impact of patches to improve security on the maintainability of open-source software. Maintainability is measured based on the Better Code Hub's model of 10 guidelines on a dataset, including 1300 security-related commits. Results show evidence of a trade-off between security and maintainability for 41.90% of the cases, i.e., developers may hinder software maintainability. Our analysis shows that 38.29% of patches increased software complexity and 37.87% of patches increased the percentage of LOCs per unit. The implications of our study are that changes to codebases while patching vulnerabilities need to be performed with extra care; tools for patch risk assessment should be integrated into the CI/CD pipeline; computer science curricula needs to be updated; and, more secure programming languages are necessary.

</details>

<details>

<summary>2021-09-13 01:09:48 - Source Inference Attacks in Federated Learning</summary>

- *Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Xuyun Zhang*

- `2109.05659v1` - [abs](http://arxiv.org/abs/2109.05659v1) - [pdf](http://arxiv.org/pdf/2109.05659v1)

> Federated learning (FL) has emerged as a promising privacy-aware paradigm that allows multiple clients to jointly train a model without sharing their private data. Recently, many studies have shown that FL is vulnerable to membership inference attacks (MIAs) that can distinguish the training members of the given model from the non-members. However, existing MIAs ignore the source of a training member, i.e., the information of which client owns the training member, while it is essential to explore source privacy in FL beyond membership privacy of examples from all clients. The leakage of source information can lead to severe privacy issues. For example, identification of the hospital contributing to the training of an FL model for COVID-19 pandemic can render the owner of a data record from this hospital more prone to discrimination if the hospital is in a high risk region. In this paper, we propose a new inference attack called source inference attack (SIA), which can derive an optimal estimation of the source of a training member. Specifically, we innovatively adopt the Bayesian perspective to demonstrate that an honest-but-curious server can launch an SIA to steal non-trivial source information of the training members without violating the FL protocol. The server leverages the prediction loss of local models on the training members to achieve the attack effectively and non-intrusively. We conduct extensive experiments on one synthetic and five real datasets to evaluate the key factors in an SIA, and the results show the efficacy of the proposed source inference attack.

</details>

<details>

<summary>2021-09-13 02:10:40 - Shape-Biased Domain Generalization via Shock Graph Embeddings</summary>

- *Maruthi Narayanan, Vickram Rajendran, Benjamin Kimia*

- `2109.05671v1` - [abs](http://arxiv.org/abs/2109.05671v1) - [pdf](http://arxiv.org/pdf/2109.05671v1)

> There is an emerging sense that the vulnerability of Image Convolutional Neural Networks (CNN), i.e., sensitivity to image corruptions, perturbations, and adversarial attacks, is connected with Texture Bias. This relative lack of Shape Bias is also responsible for poor performance in Domain Generalization (DG). The inclusion of a role of shape alleviates these vulnerabilities and some approaches have achieved this by training on negative images, images endowed with edge maps, or images with conflicting shape and texture information. This paper advocates an explicit and complete representation of shape using a classical computer vision approach, namely, representing the shape content of an image with the shock graph of its contour map. The resulting graph and its descriptor is a complete representation of contour content and is classified using recent Graph Neural Network (GNN) methods. The experimental results on three domain shift datasets, Colored MNIST, PACS, and VLCS demonstrate that even without using appearance the shape-based approach exceeds classical Image CNN based methods in domain generalization.

</details>

<details>

<summary>2021-09-13 02:27:41 - Automated Evolution of Feature Logging Statement Levels Using Git Histories and Degree of Interest</summary>

- *Yiming Tang, Allan Spektor, Raffi Khatchadourian, Mehdi Bagherzadeh*

- `2104.07736v4` - [abs](http://arxiv.org/abs/2104.07736v4) - [pdf](http://arxiv.org/pdf/2104.07736v4)

> Logging -- used for system events and security breaches to describe more informational yet essential aspects of software features -- is pervasive. Given the high transactionality of today's software, logging effectiveness can be reduced by information overload. Log levels help alleviate this problem by correlating a priority to logs that can be later filtered. As software evolves, however, levels of logs documenting surrounding feature implementations may also require modification as features once deemed important may have decreased in urgency and vice-versa. We present an automated approach that assists developers in evolving levels of such (feature) logs. The approach, based on mining Git histories and manipulating a degree of interest (DOI) model, transforms source code to revitalize feature log levels based on the "interestingness" of the surrounding code. Built upon JGit and Mylyn, the approach is implemented as an Eclipse IDE plug-in and evaluated on 18 Java projects with $\sim$3 million lines of code and $\sim$4K log statements. Our tool successfully analyzes 99.22% of logging statements, increases log level distributions by $\sim$20%, and increases the focus of logs in bug fix contexts $\sim$83% of the time. Moreover, pull (patch) requests were integrated into large and popular open-source projects. The results indicate that the approach is promising in assisting developers in evolving feature log levels.

</details>

<details>

<summary>2021-09-13 03:31:20 - TREATED:Towards Universal Defense against Textual Adversarial Attacks</summary>

- *Bin Zhu, Zhaoquan Gu, Le Wang, Zhihong Tian*

- `2109.06176v1` - [abs](http://arxiv.org/abs/2109.06176v1) - [pdf](http://arxiv.org/pdf/2109.06176v1)

> Recent work shows that deep neural networks are vulnerable to adversarial examples. Much work studies adversarial example generation, while very little work focuses on more critical adversarial defense. Existing adversarial detection methods usually make assumptions about the adversarial example and attack method (e.g., the word frequency of the adversarial example, the perturbation level of the attack method). However, this limits the applicability of the detection method. To this end, we propose TREATED, a universal adversarial detection method that can defend against attacks of various perturbation levels without making any assumptions. TREATED identifies adversarial examples through a set of well-designed reference models. Extensive experiments on three competitive neural networks and two widely used datasets show that our method achieves better detection performance than baselines. We finally conduct ablation studies to verify the effectiveness of our method.

</details>

<details>

<summary>2021-09-13 15:40:03 - A [in]Segurança dos Sistemas Governamentais Brasileiros: Um Estudo de Caso em Sistemas Web e Redes Abertas</summary>

- *Marcus Botacin, André Grégio*

- `2109.06068v1` - [abs](http://arxiv.org/abs/2109.06068v1) - [pdf](http://arxiv.org/pdf/2109.06068v1)

> Whereas the world relies on computer systems for providing public services, there is a lack of academic work that systematically assess the security of government systems. To partially fill this gap, we conducted a security evaluation of publicly available systems from public institutions. We revisited OWASP top-10 and identified multiple vulnerabilities in deployed services by scanning public government networks. Overall, the unprotected services found have inadequate security level, which must be properly discussed and addressed.

</details>

<details>

<summary>2021-09-13 23:38:42 - Sensor Adversarial Traits: Analyzing Robustness of 3D Object Detection Sensor Fusion Models</summary>

- *Won Park, Nan Liu, Qi Alfred Chen, Z. Morley Mao*

- `2109.06363v1` - [abs](http://arxiv.org/abs/2109.06363v1) - [pdf](http://arxiv.org/pdf/2109.06363v1)

> A critical aspect of autonomous vehicles (AVs) is the object detection stage, which is increasingly being performed with sensor fusion models: multimodal 3D object detection models which utilize both 2D RGB image data and 3D data from a LIDAR sensor as inputs. In this work, we perform the first study to analyze the robustness of a high-performance, open source sensor fusion model architecture towards adversarial attacks and challenge the popular belief that the use of additional sensors automatically mitigate the risk of adversarial attacks. We find that despite the use of a LIDAR sensor, the model is vulnerable to our purposefully crafted image-based adversarial attacks including disappearance, universal patch, and spoofing. After identifying the underlying reason, we explore some potential defenses and provide some recommendations for improved sensor fusion models.

</details>

<details>

<summary>2021-09-14 01:12:34 - Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages</summary>

- *Nay San, Martijn Bartelds, Mitchell Browne, Lily Clifford, Fiona Gibson, John Mansfield, David Nash, Jane Simpson, Myfany Turpin, Maria Vollmer, Sasha Wilmoth, Dan Jurafsky*

- `2103.14583v3` - [abs](http://arxiv.org/abs/2103.14583v3) - [pdf](http://arxiv.org/pdf/2103.14583v3)

> Pre-trained speech representations like wav2vec 2.0 are a powerful tool for automatic speech recognition (ASR). Yet many endangered languages lack sufficient data for pre-training such models, or are predominantly oral vernaculars without a standardised writing system, precluding fine-tuning. Query-by-example spoken term detection (QbE-STD) offers an alternative for iteratively indexing untranscribed speech corpora by locating spoken query terms. Using data from 7 Australian Aboriginal languages and a regional variety of Dutch, all of which are endangered or vulnerable, we show that QbE-STD can be improved by leveraging representations developed for ASR (wav2vec 2.0: the English monolingual model and XLSR53 multilingual model). Surprisingly, the English model outperformed the multilingual model on 4 Australian language datasets, raising questions around how to optimally leverage self-supervised speech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0 representations (either English or XLSR53) offer large improvements (56-86% relative) over state-of-the-art approaches on our endangered language datasets.

</details>

<details>

<summary>2021-09-14 09:49:39 - Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs</summary>

- *Mohammad Malekzadeh, Anastasia Borovykh, Deniz Gündüz*

- `2105.12049v3` - [abs](http://arxiv.org/abs/2105.12049v3) - [pdf](http://arxiv.org/pdf/2105.12049v3)

> It is known that deep neural networks, trained for the classification of non-sensitive target attributes, can reveal sensitive attributes of their input data through internal representations extracted by the classifier. We take a step forward and show that deep classifiers can be trained to secretly encode a sensitive attribute of their input data into the classifier's outputs for the target attribute, at inference time. Our proposed attack works even if users have a full white-box view of the classifier, can keep all internal representations hidden, and only release the classifier's estimations for the target attribute. We introduce an information-theoretical formulation for such attacks and present efficient empirical implementations for training honest-but-curious (HBC) classifiers: classifiers that can be accurate in predicting their target attribute, but can also exploit their outputs to secretly encode a sensitive attribute. Our work highlights a vulnerability that can be exploited by malicious machine learning service providers to attack their user's privacy in several seemingly safe scenarios; such as encrypted inferences, computations at the edge, or private knowledge distillation. Experimental results on several attributes in two face-image datasets show that a semi-trusted server can train classifiers that are not only perfectly honest but also accurately curious. We conclude by showing the difficulties in distinguishing between standard and HBC classifiers, discussing challenges in defending against this vulnerability of deep classifiers, and enumerating related open directions for future studies.

</details>

<details>

<summary>2021-09-14 10:59:11 - The pitfalls of using open data to develop deep learning solutions for COVID-19 detection in chest X-rays</summary>

- *Rachael Harkness, Geoff Hall, Alejandro F Frangi, Nishant Ravikumar, Kieran Zucker*

- `2109.08020v1` - [abs](http://arxiv.org/abs/2109.08020v1) - [pdf](http://arxiv.org/pdf/2109.08020v1)

> Since the emergence of COVID-19, deep learning models have been developed to identify COVID-19 from chest X-rays. With little to no direct access to hospital data, the AI community relies heavily on public data comprising numerous data sources. Model performance results have been exceptional when training and testing on open-source data, surpassing the reported capabilities of AI in pneumonia-detection prior to the COVID-19 outbreak. In this study impactful models are trained on a widely used open-source data and tested on an external test set and a hospital dataset, for the task of classifying chest X-rays into one of three classes: COVID-19, non-COVID pneumonia and no-pneumonia. Classification performance of the models investigated is evaluated through ROC curves, confusion matrices and standard classification metrics. Explainability modules are implemented to explore the image features most important to classification. Data analysis and model evaluations show that the popular open-source dataset COVIDx is not representative of the real clinical problem and that results from testing on this are inflated. Dependence on open-source data can leave models vulnerable to bias and confounding variables, requiring careful analysis to develop clinically useful/viable AI tools for COVID-19 detection in chest X-rays.

</details>

<details>

<summary>2021-09-14 12:36:16 - A Novel Data Encryption Method Inspired by Adversarial Attacks</summary>

- *Praveen Fernando, Jin Wei-Kocsis*

- `2109.06634v1` - [abs](http://arxiv.org/abs/2109.06634v1) - [pdf](http://arxiv.org/pdf/2109.06634v1)

> Due to the advances of sensing and storage technologies, a tremendous amount of data becomes available and, it supports the phenomenal growth of artificial intelligence (AI) techniques especially, deep learning (DL), in various application domains. While the data sources become valuable assets for enabling the success of autonomous decision-making, they also lead to critical vulnerabilities in privacy and security. For example, data leakage can be exploited via querying and eavesdropping in the exploratory phase for black-box attacks against DL-based autonomous decision-making systems. To address this issue, in this work, we propose a novel data encryption method, called AdvEncryption, by exploiting the principle of adversarial attacks. Different from existing encryption technologies, the AdvEncryption method is not developed to prevent attackers from exploiting the dataset. Instead, our proposed method aims to trap the attackers in a misleading feature distillation of the data. To achieve this goal, our AdvEncryption method consists of two essential components: 1) an adversarial attack-inspired encryption mechanism to encrypt the data with stealthy adversarial perturbation, and 2) a decryption mechanism that minimizes the impact of the perturbations on the effectiveness of autonomous decision making. In the performance evaluation section, we evaluate the performance of our proposed AdvEncryption method through case studies considering different scenarios.

</details>

<details>

<summary>2021-09-14 22:15:59 - Rigorous Guarantees for Tyler's M-estimator via quantum expansion</summary>

- *Cole Franks, Ankur Moitra*

- `2002.00071v5` - [abs](http://arxiv.org/abs/2002.00071v5) - [pdf](http://arxiv.org/pdf/2002.00071v5)

> Estimating the shape of an elliptical distribution is a fundamental problem in statistics. One estimator for the shape matrix, Tyler's M-estimator, has been shown to have many appealing asymptotic properties. It performs well in numerical experiments and can be quickly computed in practice by a simple iterative procedure. Despite the many years the estimator has been studied in the statistics community, there was neither a tight non-asymptotic bound on the rate of the estimator nor a proof that the iterative procedure converges in polynomially many steps.   Here we observe a surprising connection between Tyler's M-estimator and operator scaling, which has been intensively studied in recent years in part because of its connections to the Brascamp-Lieb inequality in analysis. We use this connection, together with novel results on quantum expanders, to show that Tyler's M-estimator has the optimal rate up to factors logarithmic in the dimension, and that in the generative model the iterative procedure has a linear convergence rate even without regularization.

</details>

<details>

<summary>2021-09-15 08:05:16 - Universal Adversarial Attack on Deep Learning Based Prognostics</summary>

- *Arghya Basak, Pradeep Rathore, Sri Harsha Nistala, Sagar Srinivas, Venkataramana Runkana*

- `2109.07142v1` - [abs](http://arxiv.org/abs/2109.07142v1) - [pdf](http://arxiv.org/pdf/2109.07142v1)

> Deep learning-based time series models are being extensively utilized in engineering and manufacturing industries for process control and optimization, asset monitoring, diagnostic and predictive maintenance. These models have shown great improvement in the prediction of the remaining useful life (RUL) of industrial equipment but suffer from inherent vulnerability to adversarial attacks. These attacks can be easily exploited and can lead to catastrophic failure of critical industrial equipment. In general, different adversarial perturbations are computed for each instance of the input data. This is, however, difficult for the attacker to achieve in real time due to higher computational requirement and lack of uninterrupted access to the input data. Hence, we present the concept of universal adversarial perturbation, a special imperceptible noise to fool regression based RUL prediction models. Attackers can easily utilize universal adversarial perturbations for real-time attack since continuous access to input data and repetitive computation of adversarial perturbations are not a prerequisite for the same. We evaluate the effect of universal adversarial attacks using NASA turbofan engine dataset. We show that addition of universal adversarial perturbation to any instance of the input data increases error in the output predicted by the model. To the best of our knowledge, we are the first to study the effect of the universal adversarial perturbation on time series regression models. We further demonstrate the effect of varying the strength of perturbations on RUL prediction models and found that model accuracy decreases with the increase in perturbation strength of the universal adversarial attack. We also showcase that universal adversarial perturbation can be transferred across different models.

</details>

<details>

<summary>2021-09-15 09:13:10 - Balancing detectability and performance of attacks on the control channel of Markov Decision Processes</summary>

- *Alessio Russo, Alexandre Proutiere*

- `2109.07171v1` - [abs](http://arxiv.org/abs/2109.07171v1) - [pdf](http://arxiv.org/pdf/2109.07171v1)

> We investigate the problem of designing optimal stealthy poisoning attacks on the control channel of Markov decision processes (MDPs). This research is motivated by the recent interest of the research community for adversarial and poisoning attacks applied to MDPs, and reinforcement learning (RL) methods. The policies resulting from these methods have been shown to be vulnerable to attacks perturbing the observations of the decision-maker. In such an attack, drawing inspiration from adversarial examples used in supervised learning, the amplitude of the adversarial perturbation is limited according to some norm, with the hope that this constraint will make the attack imperceptible. However, such constraints do not grant any level of undetectability and do not take into account the dynamic nature of the underlying Markov process. In this paper, we propose a new attack formulation, based on information-theoretical quantities, that considers the objective of minimizing the detectability of the attack as well as the performance of the controlled process. We analyze the trade-off between the efficiency of the attack and its detectability. We conclude with examples and numerical simulations illustrating this trade-off.

</details>

<details>

<summary>2021-09-15 16:00:05 - Can one hear the shape of a neural network?: Snooping the GPU via Magnetic Side Channel</summary>

- *Henrique Teles Maia, Chang Xiao, Dingzeyu Li, Eitan Grinspun, Changxi Zheng*

- `2109.07395v1` - [abs](http://arxiv.org/abs/2109.07395v1) - [pdf](http://arxiv.org/pdf/2109.07395v1)

> Neural network applications have become popular in both enterprise and personal settings. Network solutions are tuned meticulously for each task, and designs that can robustly resolve queries end up in high demand. As the commercial value of accurate and performant machine learning models increases, so too does the demand to protect neural architectures as confidential investments. We explore the vulnerability of neural networks deployed as black boxes across accelerated hardware through electromagnetic side channels. We examine the magnetic flux emanating from a graphics processing unit's power cable, as acquired by a cheap $3 induction sensor, and find that this signal betrays the detailed topology and hyperparameters of a black-box neural network model. The attack acquires the magnetic signal for one query with unknown input values, but known input dimensions. The network reconstruction is possible due to the modular layer sequence in which deep neural networks are evaluated. We find that each layer component's evaluation produces an identifiable magnetic signal signature, from which layer topology, width, function type, and sequence order can be inferred using a suitably trained classifier and a joint consistency optimization based on integer programming. We study the extent to which network specifications can be recovered, and consider metrics for comparing network similarity. We demonstrate the potential accuracy of this side channel attack in recovering the details for a broad range of network architectures, including random designs. We consider applications that may exploit this novel side channel exposure, such as adversarial transfer attacks. In response, we discuss countermeasures to protect against our method and other similar snooping techniques.

</details>

<details>

<summary>2021-09-16 16:34:30 - Improving Linux-Kernel Tests for LockDoc with Feedback-driven Fuzzing</summary>

- *Alexander Lochmann, Robin Thunig, Horst Schirmeier*

- `2009.08768v2` - [abs](http://arxiv.org/abs/2009.08768v2) - [pdf](http://arxiv.org/pdf/2009.08768v2)

> LockDoc is an approach to extract locking rules for kernel data structures from a dynamic execution trace recorded while the system is under a benchmark load. These locking rules can e.g. be used to locate synchronization bugs. For high rule precision and thorough bug finding, the approach heavily depends on the choice of benchmarks: They must trigger the execution of as much code as possible in the kernel subsystem relevant for the targeted data structures. However, existing test suites such as those provided by the Linux Test Project (LTP) only achieve -- in the case of LTP -- about 35 percent basic-block coverage for the VFS subsystem, which is the relevant subsystem when extracting locking rules for filesystem-related data structures.   In this article, we discuss how to complement the LTP suites to improve the code coverage for our LockDoc scenario. We repurpose syzkaller -- a coverage-guided fuzzer with the goal to validate the robustness of kernel APIs -- to 1) not aim for kernel crashes, and to 2) maximize code coverage for a specific kernel subsystem. Thereby, we generate new benchmark programs that can be run in addition to the LTP, and increase VFS basic-block coverage by 26.1 percent.

</details>

<details>

<summary>2021-09-16 17:42:34 - Verifying Quantized Neural Networks using SMT-Based Model Checking</summary>

- *Luiz Sena, Xidan Song, Erickson Alves, Iury Bessa, Edoardo Manino, Lucas Cordeiro, Eddie de Lima Filho*

- `2106.05997v2` - [abs](http://arxiv.org/abs/2106.05997v2) - [pdf](http://arxiv.org/pdf/2106.05997v2)

> Artificial Neural Networks (ANNs) are being deployed for an increasing number of safety-critical applications, including autonomous cars and medical diagnosis. However, concerns about their reliability have been raised due to their black-box nature and apparent fragility to adversarial attacks. These concerns are amplified when ANNs are deployed on restricted system, which limit the precision of mathematical operations and thus introduce additional quantization errors. Here, we develop and evaluate a novel symbolic verification framework using software model checking (SMC) and satisfiability modulo theories (SMT) to check for vulnerabilities in ANNs. More specifically, we propose several ANN-related optimizations for SMC, including invariant inference via interval analysis, slicing, expression simplifications, and discretization of non-linear activation functions. With this verification framework, we can provide formal guarantees on the safe behavior of ANNs implemented both in floating- and fixed-point arithmetic. In this regard, our verification approach was able to verify and produce adversarial examples for $52$ test cases spanning image classification and general machine learning applications. Furthermore, for small- to medium-sized ANN, our approach completes most of its verification runs in minutes. Moreover, in contrast to most state-of-the-art methods, our approach is not restricted to specific choices regarding activation functions and non-quantized representations. Our experiments show that our approach can analyze larger ANN implementations and substantially reduce the verification time compared to state-of-the-art techniques that use SMT solving.

</details>

<details>

<summary>2021-09-16 19:21:39 - Disparate Vulnerability to Membership Inference Attacks</summary>

- *Bogdan Kulynych, Mohammad Yaghini, Giovanni Cherubin, Michael Veale, Carmela Troncoso*

- `1906.00389v4` - [abs](http://arxiv.org/abs/1906.00389v4) - [pdf](http://arxiv.org/pdf/1906.00389v4)

> A membership inference attack (MIA) against a machine-learning model enables an attacker to determine whether a given data record was part of the model's training data or not. In this paper, we provide an in-depth study of the phenomenon of disparate vulnerability against MIAs: unequal success rate of MIAs against different population subgroups. We first establish necessary and sufficient conditions for MIAs to be prevented, both on average and for population subgroups, using a notion of distributional generalization. Second, we derive connections of disparate vulnerability to algorithmic fairness and to differential privacy. We show that fairness can only prevent disparate vulnerability against limited classes of adversaries. Differential privacy bounds disparate vulnerability but can significantly reduce the accuracy of the model. We show that estimating disparate vulnerability to MIAs by na\"ively applying existing attacks can lead to overestimation. We then establish which attacks are suitable for estimating disparate vulnerability, and provide a statistical framework for doing so reliably. We conduct experiments on synthetic and real-world data finding statistically significant evidence of disparate vulnerability in realistic settings. The code is available at https://github.com/spring-epfl/disparate-vulnerability

</details>

<details>

<summary>2021-09-16 19:44:28 - Spinner: Automated Dynamic Command Subsystem Perturbation</summary>

- *Meng Wang, Chijung Jung, Ali Ahad, Yonghwi Kwon*

- `2105.00391v2` - [abs](http://arxiv.org/abs/2105.00391v2) - [pdf](http://arxiv.org/pdf/2105.00391v2)

> Injection attacks have been a major threat to web applications. Despite the significant effort in thwarting injection attacks, protection against injection attacks remains challenging due to the sophisticated attacks that exploit the existing protection techniques' design and implementation flaws. In this paper, we develop Spinner, a system that provides general protection against input injection attacks, including OS/shell command, SQL, and XXE injection. Instead of focusing on detecting malicious inputs, Spinner constantly randomizes underlying subsystems so that injected inputs (e.g., commands or SQL queries) that are not properly randomized will not be executed, hence prevented. We revisit the design and implementation choices of previous randomization-based techniques and develop a more robust and practical protection against various sophisticated input injection attacks. To handle complex real-world applications, we develop a bidirectional analysis that combines forward and backward static analysis techniques to identify intended commands or SQL queries to ensure the correct execution of the randomized target program. We implement Spinner for the shell command processor and two different database engines (MySQL and SQLite) and in diverse programming languages including C/C++, PHP, JavaScript and Lua. Our evaluation results on 42 real-world applications including 27 vulnerable ones show that it effectively prevents a variety of input injection attacks with low runtime overhead (around 5%).

</details>

<details>

<summary>2021-09-17 16:36:40 - Facilitating Parallel Fuzzing with mutually-exclusive Task Distribution</summary>

- *Yifan Wang, Yuchen Zhang, Chengbin Pang, Peng Li, Nikolaos Triandopoulos, Jun Xu*

- `2109.08635v1` - [abs](http://arxiv.org/abs/2109.08635v1) - [pdf](http://arxiv.org/pdf/2109.08635v1)

> Fuzz testing, or fuzzing, has become one of the de facto standard techniques for bug finding in the software industry. In general, fuzzing provides various inputs to the target program to discover unhandled exceptions and crashes. In business sectors where the time budget is limited, software vendors often launch many fuzzing instances in parallel as common means of increasing code coverage. However, most of the popular fuzzing tools in their parallel mode-naively run multiple instances concurrently, without elaborate distribution of workload. This can lead different instances to explore overlapped code regions, eventually reducing the benefits of concurrency. In this paper, we propose a general model to describe parallel fuzzing. This model distributes mutually-exclusive but similarly-weighted tasks to different instances, facilitating concurrency and also fairness across instances. Following this model, we develop a solution, called AFL-EDGE, to improve the parallel mode of AFL, considering a round of mutations to a unique seed as a task and adopting edge coverage to define the uniqueness of a seed. We have implemented AFL-EDGE on top of AFL and evaluated the implementation with AFL on 9 widely used benchmark programs. It shows that AFL-EDGE can benefit the edge coverage of AFL. In a 24-hour test, the increase of edge coverage brought by AFL-EDGE to AFL ranges from 9.49% to 10.20%, depending on the number of instances. As a side benefit, we discovered 14 previously unknown bugs.

</details>

<details>

<summary>2021-09-17 17:40:37 - Membership Leakage in Label-Only Exposures</summary>

- *Zheng Li, Yang Zhang*

- `2007.15528v3` - [abs](http://arxiv.org/abs/2007.15528v3) - [pdf](http://arxiv.org/pdf/2007.15528v3)

> Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision.   In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack, and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.

</details>

<details>

<summary>2021-09-18 12:37:13 - Adversarial examples attack based on random warm restart mechanism and improved Nesterov momentum</summary>

- *Tiangang Li*

- `2105.05029v2` - [abs](http://arxiv.org/abs/2105.05029v2) - [pdf](http://arxiv.org/pdf/2105.05029v2)

> The deep learning algorithm has achieved great success in the field of computer vision, but some studies have pointed out that the deep learning model is vulnerable to attacks adversarial examples and makes false decisions. This challenges the further development of deep learning, and urges researchers to pay more attention to the relationship between adversarial examples attacks and deep learning security. This work focuses on adversarial examples, optimizes the generation of adversarial examples from the view of adversarial robustness, takes the perturbations added in adversarial examples as the optimization parameter. We propose RWR-NM-PGD attack algorithm based on random warm restart mechanism and improved Nesterov momentum from the view of gradient optimization. The algorithm introduces improved Nesterov momentum, using its characteristics of accelerating convergence and improving gradient update direction in optimization algorithm to accelerate the generation of adversarial examples. In addition, the random warm restart mechanism is used for optimization, and the projected gradient descent algorithm is used to limit the range of the generated perturbations in each warm restart, which can obtain better attack effect. Experiments on two public datasets show that the algorithm proposed in this work can improve the success rate of attacking deep learning models without extra time cost. Compared with the benchmark attack method, the algorithm proposed in this work can achieve better attack success rate for both normal training model and defense model. Our method has average attack success rate of 46.3077%, which is 27.19% higher than I-FGSM and 9.27% higher than PGD. The attack results in 13 defense models show that the attack algorithm proposed in this work is superior to the benchmark algorithm in attack universality and transferability.

</details>

<details>

<summary>2021-09-18 13:17:44 - SōjiTantei: Function-Call Reachability Detection of Vulnerable Code for npm Packages</summary>

- *Bodin Chinthanet, Raula Gaikovina Kula, Rodrigo Eliza Zapata, Takashi Ishio, Kenichi Matsumoto, Akinori Ihara*

- `2109.08931v1` - [abs](http://arxiv.org/abs/2109.08931v1) - [pdf](http://arxiv.org/pdf/2109.08931v1)

> It has become common practice for software projects to adopt third-party dependencies. Developers are encouraged to update any outdated dependency to remain safe from potential threats of vulnerabilities. In this study, we present an approach to aid developers show whether or not a vulnerable code is reachable for JavaScript projects. Our prototype, S\=ojiTantei, is evaluated in two ways (i) the accuracy when compared to a manual approach and (ii) a larger-scale analysis of 780 clients from 78 security vulnerability cases. The first evaluation shows that S\=ojiTantei has a high accuracy of 83.3%, with a speed of less than a second analysis per client. The second evaluation reveals that 68 out of the studied 78 vulnerabilities reported having at least one clean client. The study proves that automation is promising with the potential for further improvement.

</details>

<details>

<summary>2021-09-19 17:24:22 - Architecture and Its Vulnerabilities in Smart-Lighting Systems</summary>

- *Florian Hofer, Barbara Russo*

- `2109.09171v1` - [abs](http://arxiv.org/abs/2109.09171v1) - [pdf](http://arxiv.org/pdf/2109.09171v1)

> Industry 4.0 embodies one of the significant technological changes of this decade. Cyber-physical systems and the Internet Of Things are two central technologies in this change that embed or connect with sensors and actuators and interact with the physical environment. However, such systems-of-systems undergo additional restrictions in an endeavor to maintain reliability and security when building and interconnecting components to a heterogeneous, multi-domain \textit{Smart-*} systems architecture. This paper presents an application-specific, layer-based approach to an offline security analysis inspired by design science that merges preceding expertise from relevant domains. With the example of a Smart-lighting system, we create a dedicated unified taxonomy for the use case and analyze its distributed Smart-* architecture by multiple layer-based models. We derive potential attacks from the system specifications in an iterative and incremental process and discuss resulting threats and vulnerabilities. Finally, we suggest immediate countermeasures for the latter potential multiple-domain security concerns.

</details>

<details>

<summary>2021-09-20 01:11:43 - A Deep Learning-based Penetration Testing Framework for Vulnerability Identification in Internet of Things Environments</summary>

- *Nickolaos Koroniotis, Nour Moustafa, Benjamin Turnbull, Francesco Schiliro, Praveen Gauravaram, Helge Janicke*

- `2109.09259v1` - [abs](http://arxiv.org/abs/2109.09259v1) - [pdf](http://arxiv.org/pdf/2109.09259v1)

> The Internet of Things (IoT) paradigm has displayed tremendous growth in recent years, resulting in innovations like Industry 4.0 and smart environments that provide improvements to efficiency, management of assets and facilitate intelligent decision making. However, these benefits are offset by considerable cybersecurity concerns that arise due to inherent vulnerabilities, which hinder IoT-based systems' Confidentiality, Integrity, and Availability. Security vulnerabilities can be detected through the application of penetration testing, and specifically, a subset of the information-gathering stage, known as vulnerability identification. Yet, existing penetration testing solutions can not discover zero-day vulnerabilities from IoT environments, due to the diversity of generated data, hardware constraints, and environmental complexity. Thus, it is imperative to develop effective penetration testing solutions for the detection of vulnerabilities in smart IoT environments. In this paper, we propose a deep learning-based penetration testing framework, namely Long Short-Term Memory Recurrent Neural Network-Enabled Vulnerability Identification (LSTM-EVI). We utilize this framework through a novel cybersecurity-oriented testbed, which is a smart airport-based testbed comprised of both physical and virtual elements. The framework was evaluated using this testbed and on real-time data sources. Our results revealed that the proposed framework achieves about 99% detection accuracy for scanning attacks, outperforming other four peer techniques.

</details>

<details>

<summary>2021-09-20 13:20:51 - IoT Vulnerability Data Crawling and Analysis</summary>

- *Stavros Shiaeles, Nicholas Kolokotronis, Emanuele Bellini*

- `2109.09526v1` - [abs](http://arxiv.org/abs/2109.09526v1) - [pdf](http://arxiv.org/pdf/2109.09526v1)

> Internet of Things (IoT) is a whole new ecosystem comprised of heterogeneous connected devices -i.e. computers, laptops, smart-phones and tablets as well as embedded devices and sensors-that communicate to deliver capabilities making our living, cities, transport, energy, and many other areas more intelligent. The main concerns raised from the IoT ecosystem are the devices poor support for patching/updating and the poor on-board computational power. A number of issues stem from this: inherent vulnerabilities and the inability to detect and defend against external attacks. Also, due to the nature of their operation, the devices tend to be rather open to communication, which makes attacks easy to spread once reaching a network. The aim of this research is to investigate if it is possible to extract useful results regarding attacks' trends and be able to predict them, before it is too late, by crawling Deep/Dark and Surface web. The results of this work show that is possible to find the trend and be able to act proactively in order to protect the IoT ecosystem.

</details>

<details>

<summary>2021-09-20 13:30:11 - A Novel Online Incremental Learning Intrusion Prevention System</summary>

- *Christos Constantinides, Stavros Shiaeles, Bogdan Ghita, Nicholas Kolokotronis*

- `2109.09530v1` - [abs](http://arxiv.org/abs/2109.09530v1) - [pdf](http://arxiv.org/pdf/2109.09530v1)

> Attack vectors are continuously evolving in order to evade Intrusion Detection systems. Internet of Things (IoT) environments, while beneficial for the IT ecosystem, suffer from inherent hardware limitations, which restrict their ability to implement comprehensive security measures and increase their exposure to vulnerability attacks. This paper proposes a novel Network Intrusion Prevention System that utilises a SelfOrganizing Incremental Neural Network along with a Support Vector Machine. Due to its structure, the proposed system provides a security solution that does not rely on signatures or rules and is capable to mitigate known and unknown attacks in real-time with high accuracy. Based on our experimental results with the NSL KDD dataset, the proposed framework can achieve on-line updated incremental learning, making it suitable for efficient and scalable industrial applications.

</details>

<details>

<summary>2021-09-20 13:31:15 - Characterizing User Susceptibility to COVID-19 Misinformation on Twitter</summary>

- *Xian Teng, Yu-Ru Lin, Wen-Ting Chung, Ang Li, Adriana Kovashka*

- `2109.09532v1` - [abs](http://arxiv.org/abs/2109.09532v1) - [pdf](http://arxiv.org/pdf/2109.09532v1)

> Though significant efforts such as removing false claims and promoting reliable sources have been increased to combat COVID-19 "misinfodemic", it remains an unsolved societal challenge if lacking a proper understanding of susceptible online users, i.e., those who are likely to be attracted by, believe and spread misinformation. This study attempts to answer {\it who} constitutes the population vulnerable to the online misinformation in the pandemic, and what are the robust features and short-term behavior signals that distinguish susceptible users from others. Using a 6-month longitudinal user panel on Twitter collected from a geopolitically diverse network-stratified samples in the US, we distinguish different types of users, ranging from social bots to humans with various level of engagement with COVID-related misinformation. We then identify users' online features and situational predictors that correlate with their susceptibility to COVID-19 misinformation. This work brings unique contributions: First, contrary to the prior studies on bot influence, our analysis shows that social bots' contribution to misinformation sharing was surprisingly low, and human-like users' misinformation behaviors exhibit heterogeneity and temporal variability. While the sharing of misinformation was highly concentrated, the risk of occasionally sharing misinformation for average users remained alarmingly high. Second, our findings highlight the political sensitivity activeness and responsiveness to emotionally-charged content among susceptible users. Third, we demonstrate a feasible solution to efficiently predict users' transient susceptibility solely based on their short-term news consumption and exposure from their networks. Our work has an implication in designing effective intervention mechanism to mitigate the misinformation dissipation.

</details>

<details>

<summary>2021-09-20 18:39:31 - Blockchain Security by Design Framework for Trust and Adoption in IoT Environment</summary>

- *Gohar Sargsyan, Nicolas Castellon, Raymond Binnendijk, Peter Cozijnsen*

- `2109.09789v1` - [abs](http://arxiv.org/abs/2109.09789v1) - [pdf](http://arxiv.org/pdf/2109.09789v1)

> With the recent advances of IoT (Internet of Things) new and more robust security frameworks are needed to detect and mitigate new forms of cyber-attacks, which exploit complex and heterogeneity IoT networks, as well as, the existence of many vulnerabilities in IoT devices. With the rise of blockchain technologies service providers pay considerable attention to better understand and adopt blockchain technologies in order to have better secure and trusted systems for own organisations and their customers. The present paper introduces a high level guide for the senior officials and decision makers in the organisations and technology managers for blockchain security framework by design principle for trust and adoption in IoT environments. The paper discusses Cyber-Trust project blockchain technology development as a representative case study for offered security framework. Security and privacy by design approach is introduced as an important consideration in setting up the framework.

</details>

<details>

<summary>2021-09-20 19:06:07 - Bugs4Q: A Benchmark of Real Bugs for Quantum Programs</summary>

- *Pengzhan Zhao, Jianjun Zhao, Zhongtao Miao, Shuhan Lan*

- `2108.09744v2` - [abs](http://arxiv.org/abs/2108.09744v2) - [pdf](http://arxiv.org/pdf/2108.09744v2)

> Realistic benchmarks of reproducible bugs and fixes are vital to good experimental evaluation of debugging and testing approaches. However, there is no suitable benchmark suite that can systematically evaluate the debugging and testing methods of quantum programs until now. This paper proposes Bugs4Q, a benchmark of thirty-six real, manually validated Qiskit bugs from four popular Qiskit elements (Terra, Aer, Ignis, and Aqua), supplemented with the test cases for reproducing buggy behaviors. Bugs4Q also provides interfaces for accessing the buggy and fixed versions of the Qiskit programs and executing the corresponding test cases, facilitating the reproducible empirical studies and comparisons of Qiskit program debugging and testing tools. Bugs4Q is publicly available at https://github.com/Z-928/Bugs4Q.

</details>

<details>

<summary>2021-09-21 01:31:58 - Comparative Analysis of Cryptographic Key Management Systems</summary>

- *Levgeniia Kuzminykh, Bogdan Ghita, Stavros Shiaeles*

- `2109.09905v1` - [abs](http://arxiv.org/abs/2109.09905v1) - [pdf](http://arxiv.org/pdf/2109.09905v1)

> Managing cryptographic keys can be a complex task for an enterprise and particularly difficult to scale when an increasing number of users and applications need to be managed. In order to address scalability issues, typical IT infrastructures employ key management systems that are able to handle a large number of encryption keys and associate them with the authorized requests. Given their necessity, recent years have witnessed a variety of key management systems, aligned with the features, quality, price and security needs of specific organisations. While the spectrum of such solutions is welcome and demonstrates the expanding nature of the market, it also makes it time consuming for IT managers to identify the appropriate system for their respective company needs. This paper provides a list of key management tools which include a minimum set of features, such as availability of secure database for managing keys, an authentication, authorization, and access control model for restricting and managing access to keys, effective logging of actions with keys, and the presence of an API for accessing functions directly from the application code. Five systems were comprehensively compared by evaluating the attributes related to complexity of the implementation, its popularity, linked vulnerabilities and technical performance in terms of response time and network usage. These were Pinterest Knox, Hashicorp Vault, Square Keywhiz, OpenStack Barbican, and Cyberark Conjur. Out of these five, Hachicorp Vault was determined to be the most suitable system for small businesses.

</details>

<details>

<summary>2021-09-21 04:34:22 - DeSMP: Differential Privacy-exploited Stealthy Model Poisoning Attacks in Federated Learning</summary>

- *Md Tamjid Hossain, Shafkat Islam, Shahriar Badsha, Haoting Shen*

- `2109.09955v1` - [abs](http://arxiv.org/abs/2109.09955v1) - [pdf](http://arxiv.org/pdf/2109.09955v1)

> Federated learning (FL) has become an emerging machine learning technique lately due to its efficacy in safeguarding the client's confidential information. Nevertheless, despite the inherent and additional privacy-preserving mechanisms (e.g., differential privacy, secure multi-party computation, etc.), the FL models are still vulnerable to various privacy-violating and security-compromising attacks (e.g., data or model poisoning) due to their numerous attack vectors which in turn, make the models either ineffective or sub-optimal. Existing adversarial models focusing on untargeted model poisoning attacks are not enough stealthy and persistent at the same time because of their conflicting nature (large scale attacks are easier to detect and vice versa) and thus, remain an unsolved research problem in this adversarial learning paradigm. Considering this, in this paper, we analyze this adversarial learning process in an FL setting and show that a stealthy and persistent model poisoning attack can be conducted exploiting the differential noise. More specifically, we develop an unprecedented DP-exploited stealthy model poisoning (DeSMP) attack for FL models. Our empirical analysis on both the classification and regression tasks using two popular datasets reflects the effectiveness of the proposed DeSMP attack. Moreover, we develop a novel reinforcement learning (RL)-based defense strategy against such model poisoning attacks which can intelligently and dynamically select the privacy level of the FL models to minimize the DeSMP attack surface and facilitate the attack detection.

</details>

<details>

<summary>2021-09-21 04:46:08 - FakeWake: Understanding and Mitigating Fake Wake-up Words of Voice Assistants</summary>

- *Yanjiao Chen, Yijie Bai, Richard Mitev, Kaibo Wang, Ahmad-Reza Sadeghi, Wenyuan Xu*

- `2109.09958v1` - [abs](http://arxiv.org/abs/2109.09958v1) - [pdf](http://arxiv.org/pdf/2109.09958v1)

> In the area of Internet of Things (IoT) voice assistants have become an important interface to operate smart speakers, smartphones, and even automobiles. To save power and protect user privacy, voice assistants send commands to the cloud only if a small set of pre-registered wake-up words are detected. However, voice assistants are shown to be vulnerable to the FakeWake phenomena, whereby they are inadvertently triggered by innocent-sounding fuzzy words. In this paper, we present a systematic investigation of the FakeWake phenomena from three aspects. To start with, we design the first fuzzy word generator to automatically and efficiently produce fuzzy words instead of searching through a swarm of audio materials. We manage to generate 965 fuzzy words covering 8 most popular English and Chinese smart speakers. To explain the causes underlying the FakeWake phenomena, we construct an interpretable tree-based decision model, which reveals phonetic features that contribute to false acceptance of fuzzy words by wake-up word detectors. Finally, we propose remedies to mitigate the effect of FakeWake. The results show that the strengthened models are not only resilient to fuzzy words but also achieve better overall performance on original training datasets.

</details>

<details>

<summary>2021-09-21 05:10:32 - Power up! Robust Graph Convolutional Network via Graph Powering</summary>

- *Ming Jin, Heng Chang, Wenwu Zhu, Somayeh Sojoudi*

- `1905.10029v2` - [abs](http://arxiv.org/abs/1905.10029v2) - [pdf](http://arxiv.org/pdf/1905.10029v2)

> Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations.

</details>

<details>

<summary>2021-09-21 12:47:34 - Home Energy Management Systems: Operation and Resilience of Heuristics against Cyberattacks</summary>

- *Hafiz Majid Hussain, Arun Narayanan, Subham Sahoo, Yongheng Yang, Pedro H. J. Nardelli, Frede Blaabjerg*

- `2109.11627v1` - [abs](http://arxiv.org/abs/2109.11627v1) - [pdf](http://arxiv.org/pdf/2109.11627v1)

> Internet of Things (IoT) and advanced communication technologies have demonstrated great potential to manage residential energy resources by enabling demand-side management (DSM). Home energy management systems (HEMSs) can automatically control electricity production and usage inside homes using DSM techniques. These HEMSs will wirelessly collect information from hardware installed in the power system and in homes with the objective to intelligently and efficiently optimize electricity usage and minimize costs. However, HEMSs can be vulnerable to cyberattacks that target the electricity pricing model. The cyberattacker manipulates the pricing information collected by a customer's HEMS to misguide its algorithms toward non-optimal solutions. The customer's electricity bill increases, and additional peaks are created without being detected by the system operator. This article introduces demand-response (DR)-based DSM in HEMSs and discusses DR optimization using heuristic algorithms. Moreover, it discusses the possibilities and impacts of cyberattacks, their effectiveness, and the degree of resilience of heuristic algorithms against cyberattacks. This article also opens research questions and shows prospective directions.

</details>

<details>

<summary>2021-09-21 13:08:42 - A Variability Fault Localization Approach for Software Product Lines</summary>

- *Thu-Trang Nguyen, Kien-Tuan Ngo, Son Nguyen, Hieu Dinh Vo*

- `2109.10156v1` - [abs](http://arxiv.org/abs/2109.10156v1) - [pdf](http://arxiv.org/pdf/2109.10156v1)

> Software fault localization is one of the most expensive, tedious, and time-consuming activities in program debugging. This activity becomes even much more challenging in Software Product Line (SPL) systems due to variability of failures. These unexpected behaviors are induced by variability faults which can only be exposed under some combinations of system features. The interaction among these features causes the failures of the system. Although localizing bugs in single-system engineering has been studied in-depth, variability fault localization in SPL systems still remains mostly unexplored. In this article, we present VarCop, a novel and effective variability fault localization approach. For an SPL system failed by variability bugs, VarCop isolates suspicious code statements by analyzing the overall test results of the sampled products and their source code. The isolated suspicious statements are the statements related to the interaction among the features which are necessary for the visibility of the bugs in the system. The suspiciousness of each isolated statement is assessed based on both the overall test results of the products containing the statement as well as the detailed results of the test cases executed by the statement in these products. On a large dataset of buggy SPL systems, empirical evaluation shows that VarCop significantly improves two state-of-the-art fault localization techniques by 33% and 50% in ranking the incorrect statements in the systems containing a single bug each. In about two-thirds of the cases, VarCop ranks the buggy statements at the top-3 positions in the resulting lists. For multiple-bug cases, VarCop outperforms the state-of-the-art approaches 2 times and 10 times in the proportion of bugs localized at the top-1 positions. In 22% and 65% of the buggy versions, VarCop correctly ranks at least one bug in a system at the top-1 and top-5 positions.

</details>

<details>

<summary>2021-09-21 13:16:45 - Variability Fault Localization: A Benchmark</summary>

- *Kien-Tuan Ngo, Thu-Trang Nguyen, Son Nguyen, Hieu Dinh Vo*

- `2107.04741v2` - [abs](http://arxiv.org/abs/2107.04741v2) - [pdf](http://arxiv.org/pdf/2107.04741v2)

> Software fault localization is one of the most expensive, tedious, and time-consuming activities in program debugging. This activity becomes even much more challenging in Software Product Line (SPL) systems due to the variability of failures in SPL systems. These unexpected behaviors are caused by variability faults which can only be exposed under some combinations of system features. Although localizing bugs in non-configurable code has been investigated in-depth, variability fault localization in SPL systems still remains mostly unexplored. To approach this challenge, we propose a benchmark for variability fault localization with a large set of 1,570 buggy versions of six SPL systems and baseline variability fault localization performance results. Our hope is to engage the community to propose new and better approaches to the problem of variability fault localization in SPL systems.

</details>

<details>

<summary>2021-09-21 13:53:48 - Quantifying and Mitigating Privacy Risks of Contrastive Learning</summary>

- *Xinlei He, Yang Zhang*

- `2102.04140v2` - [abs](http://arxiv.org/abs/2102.04140v2) - [pdf](http://arxiv.org/pdf/2102.04140v2)

> Data is the key factor to drive the development of machine learning (ML) during the past decade. However, high-quality data, in particular labeled data, is often hard and expensive to collect. To leverage large-scale unlabeled data, self-supervised learning, represented by contrastive learning, is introduced. The objective of contrastive learning is to map different views derived from a training sample (e.g., through data augmentation) closer in their representation space, while different views derived from different samples more distant. In this way, a contrastive model learns to generate informative representations for data samples, which are then used to perform downstream ML tasks. Recent research has shown that machine learning models are vulnerable to various privacy attacks. However, most of the current efforts concentrate on models trained with supervised learning. Meanwhile, data samples' informative representations learned with contrastive learning may cause severe privacy risks as well.   In this paper, we perform the first privacy analysis of contrastive learning through the lens of membership inference and attribute inference. Our experimental results show that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks compared to supervised models. The former is due to the fact that contrastive models are less prone to overfitting, while the latter is caused by contrastive models' capability of representing data samples expressively. To remedy this situation, we propose the first privacy-preserving contrastive learning mechanism, Talos, relying on adversarial training. Empirical results show that Talos can successfully mitigate attribute inference risks for contrastive models while maintaining their membership privacy and model utility.

</details>

<details>

<summary>2021-09-21 14:09:46 - The Highs and Lows of Simple Lexical Domain Adaptation Approaches for Neural Machine Translation</summary>

- *Nikolay Bogoychev, Pinzhen Chen*

- `2101.00421v3` - [abs](http://arxiv.org/abs/2101.00421v3) - [pdf](http://arxiv.org/pdf/2101.00421v3)

> Machine translation systems are vulnerable to domain mismatch, especially in a low-resource scenario. Out-of-domain translations are often of poor quality and prone to hallucinations, due to exposure bias and the decoder acting as a language model. We adopt two approaches to alleviate this problem: lexical shortlisting restricted by IBM statistical alignments, and hypothesis re-ranking based on similarity. The methods are computationally cheap, widely known, but not extensively experimented on domain adaptation. We demonstrate success on low-resource out-of-domain test sets, however, the methods are ineffective when there is sufficient data or too great domain mismatch. This is due to both the IBM model losing its advantage over the implicitly learned neural alignment, and issues with subword segmentation of out-of-domain words.

</details>

<details>

<summary>2021-09-21 17:37:33 - HyperDegrade: From GHz to MHz Effective CPU Frequencies</summary>

- *Alejandro Cabrera Aldaya, Billy Bob Brumley*

- `2101.01077v2` - [abs](http://arxiv.org/abs/2101.01077v2) - [pdf](http://arxiv.org/pdf/2101.01077v2)

> Performance degradation techniques are an important complement to side-channel attacks. In this work, we propose HyperDegrade -- a combination of previous approaches and the use of simultaneous multithreading (SMT) architectures. In addition to the new technique, we investigate the root causes of performance degradation using cache eviction, discovering a previously unknown slowdown origin. The slowdown produced is significantly higher than previous approaches, which translates into an increased time granularity for Flush+Reload attacks. We evaluate HyperDegrade on different Intel microarchitectures, yielding significant slowdowns that achieve, in select microbenchmark cases, three orders of magnitude improvement over state-of-the-art. To evaluate the efficacy of performance degradation in side-channel amplification, we propose and evaluate leakage assessment metrics. The results evidence that HyperDegrade increases time granularity without a meaningful impact on trace quality. Additionally, we designed a fair experiment that compares three performance degradation strategies when coupled with Flush+Reload from an attacker perspective. We developed an attack on an unexploited vulnerability in OpenSSL in which HyperDegrade excels -- reducing by three times the number of required Flush+Reload traces to succeed. Regarding cryptography contributions, we revisit the recently proposed Raccoon attack on TLS-DH key exchanges, demonstrating its application to other protocols. Using HyperDegrade, we developed an end-to-end attack that shows how a Raccoon-like attack can succeed with real data, filling a missing gap from previous research.

</details>

<details>

<summary>2021-09-21 20:11:58 - Adversarial Attacks on Camera-LiDAR Models for 3D Car Detection</summary>

- *Mazen Abdelfattah, Kaiwen Yuan, Z. Jane Wang, Rabab Ward*

- `2103.09448v2` - [abs](http://arxiv.org/abs/2103.09448v2) - [pdf](http://arxiv.org/pdf/2103.09448v2)

> Most autonomous vehicles (AVs) rely on LiDAR and RGB camera sensors for perception. Using these point cloud and image data, perception models based on deep neural nets (DNNs) have achieved state-of-the-art performance in 3D detection. The vulnerability of DNNs to adversarial attacks has been heavily investigated in the RGB image domain and more recently in the point cloud domain, but rarely in both domains simultaneously. Multi-modal perception systems used in AVs can be divided into two broad types: cascaded models which use each modality independently, and fusion models which learn from different modalities simultaneously. We propose a universal and physically realizable adversarial attack for each type, and study and contrast their respective vulnerabilities to attacks. We place a single adversarial object with specific shape and texture on top of a car with the objective of making this car evade detection. Evaluating on the popular KITTI benchmark, our adversarial object made the host vehicle escape detection by each model type more than 50% of the time. The dense RGB input contributed more to the success of the adversarial attacks on both cascaded and fusion models.

</details>

<details>

<summary>2021-09-22 02:35:08 - On the Transferability of Adversarial Attacksagainst Neural Text Classifier</summary>

- *Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh, Kai-wei Chang*

- `2011.08558v3` - [abs](http://arxiv.org/abs/2011.08558v3) - [pdf](http://arxiv.org/pdf/2011.08558v3)

> Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one model can fool another model. In this paper, we present the first study to systematically investigate the transferability of adversarial examples for text classification models and explore how various factors, including network architecture, tokenization scheme, word embedding, and model capacity, affect the transferability of adversarial examples. Based on these studies, we propose a genetic algorithm to find an ensemble of models that can be used to induce adversarial examples to fool almost all existing models. Such adversarial examples reflect the defects of the learning process and the data bias in the training set. Finally, we derive word replacement rules that can be used for model diagnostics from these adversarial examples.

</details>

<details>

<summary>2021-09-22 04:19:59 - Backdoor Attacks on Federated Learning with Lottery Ticket Hypothesis</summary>

- *Zeyuan Yin, Ye Yuan, Panfeng Guo, Pan Zhou*

- `2109.10512v1` - [abs](http://arxiv.org/abs/2109.10512v1) - [pdf](http://arxiv.org/pdf/2109.10512v1)

> Edge devices in federated learning usually have much more limited computation and communication resources compared to servers in a data center. Recently, advanced model compression methods, like the Lottery Ticket Hypothesis, have already been implemented on federated learning to reduce the model size and communication cost. However, Backdoor Attack can compromise its implementation in the federated learning scenario. The malicious edge device trains the client model with poisoned private data and uploads parameters to the center, embedding a backdoor to the global shared model after unwitting aggregative optimization. During the inference phase, the model with backdoors classifies samples with a certain trigger as one target category, while shows a slight decrease in inference accuracy to clean samples. In this work, we empirically demonstrate that Lottery Ticket models are equally vulnerable to backdoor attacks as the original dense models, and backdoor attacks can influence the structure of extracted tickets. Based on tickets' similarities between each other, we provide a feasible defense for federated learning against backdoor attacks on various datasets.

</details>

<details>

<summary>2021-09-22 05:05:56 - Privacy-Preserving Machine Learning: Methods, Challenges and Directions</summary>

- *Runhua Xu, Nathalie Baracaldo, James Joshi*

- `2108.04417v2` - [abs](http://arxiv.org/abs/2108.04417v2) - [pdf](http://arxiv.org/pdf/2108.04417v2)

> Machine learning (ML) is increasingly being adopted in a wide variety of application domains. Usually, a well-performing ML model relies on a large volume of training data and high-powered computational resources. Such a need for and the use of huge volumes of data raise serious privacy concerns because of the potential risks of leakage of highly privacy-sensitive information; further, the evolving regulatory environments that increasingly restrict access to and use of privacy-sensitive data add significant challenges to fully benefiting from the power of ML for data-driven applications. A trained ML model may also be vulnerable to adversarial attacks such as membership, attribute, or property inference attacks and model inversion attacks. Hence, well-designed privacy-preserving ML (PPML) solutions are critically needed for many emerging applications. Increasingly, significant research efforts from both academia and industry can be seen in PPML areas that aim toward integrating privacy-preserving techniques into ML pipeline or specific algorithms, or designing various PPML architectures. In particular, existing PPML research cross-cut ML, systems and applications design, as well as security and privacy areas; hence, there is a critical need to understand state-of-the-art research, related challenges and a research roadmap for future research in PPML area. In this paper, we systematically review and summarize existing privacy-preserving approaches and propose a Phase, Guarantee, and Utility (PGU) triad based model to understand and guide the evaluation of various PPML solutions by decomposing their privacy-preserving functionalities. We discuss the unique characteristics and challenges of PPML and outline possible research directions that leverage as well as benefit multiple research communities such as ML, distributed systems, security and privacy.

</details>

<details>

<summary>2021-09-22 11:17:39 - Gotta catch 'em all: a Multistage Framework for honeypot fingerprinting</summary>

- *Shreyas Srinivasa, Jens Myrup Pedersen, Emmanouil Vasilomanolakis*

- `2109.10652v1` - [abs](http://arxiv.org/abs/2109.10652v1) - [pdf](http://arxiv.org/pdf/2109.10652v1)

> Honeypots are decoy systems that lure attackers by presenting them with a seemingly vulnerable system. They provide an early detection mechanism as well as a method for learning how adversaries work and think. However, over the last years, a number of researchers have shown methods for fingerprinting honeypots. This significantly decreases the value of a honeypot; if an attacker is able to recognize the existence of such a system, they can evade it. In this article, we revisit the honeypot identification field, by providing a holistic framework that includes state of the art and novel fingerprinting components. We decrease the probability of false positives by proposing a rigid multi-step approach for labeling a system as a honeypot. We perform extensive scans covering 2.9 billion addresses of the IPv4 space and identify a total of 21,855 honeypot instances. Moreover, we present a number of interesting side-findings such as the identification of more than 354,431 non-honeypot systems that represent potentially vulnerable servers (e.g. SSH servers with default password configurations and vulnerable versions). Lastly, we discuss countermeasures against honeypot fingerprinting techniques.

</details>

<details>

<summary>2021-09-22 11:32:00 - VIA: Analyzing Device Interfaces of Protected Virtual Machines</summary>

- *Felicitas Hetzelt, Martin Radev, Robert Buhren, Mathias Morbitzer, Jean-Pierre Seifert*

- `2109.10660v1` - [abs](http://arxiv.org/abs/2109.10660v1) - [pdf](http://arxiv.org/pdf/2109.10660v1)

> Both AMD and Intel have presented technologies for confidential computing in cloud environments. The proposed solutions - AMD SEV (-ES, -SNP) and Intel TDX - protect Virtual Machines (VMs) against attacks from higher privileged layers through memory encryption and integrity protection. This model of computation draws a new trust boundary between virtual devices and the VM, which in so far lacks thorough examination. In this paper, we therefore present an analysis of the virtual device interface and discuss several attack vectors against a protected VM. Further, we develop and evaluate VIA, an automated analysis tool to detect cases of improper sanitization of input recieved via the virtual device interface. VIA improves upon existing approaches for the automated analysis of device interfaces in the following aspects: (i) support for virtualization relevant buses, (ii) efficient Direct Memory Access (DMA) support and (iii) performance. VIA builds upon the Linux Kernel Library and clang's libfuzzer to fuzz the communication between the driver and the device via MMIO, PIO, and DMA. An evaluation of VIA shows that it performs 570 executions per second on average and improves performance compared to existing approaches by an average factor of 2706. Using VIA, we analyzed 22 drivers in Linux 5.10.0-rc6, thereby uncovering 50 bugs and initiating multiple patches to the virtual device driver interface of Linux. To prove our findings criticality under the threat model of AMD SEV and Intel TDX, we showcase three exemplary attacks based on the bugs found. The attacks enable a malicious hypervisor to corrupt the memory and gain code execution in protected VMs with SEV-ES and are theoretically applicable to SEV-SNP and TDX.

</details>

<details>

<summary>2021-09-22 14:41:01 - A Deep Learning Perspective on Connected Automated Vehicle (CAV) Cybersecurity and Threat Intelligence</summary>

- *Manoj Basnet, Mohd. Hasan Ali*

- `2109.10763v1` - [abs](http://arxiv.org/abs/2109.10763v1) - [pdf](http://arxiv.org/pdf/2109.10763v1)

> The automation and connectivity of CAV inherit most of the cyber-physical vulnerabilities of incumbent technologies such as evolving network architectures, wireless communications, and AI-based automation. This book chapter entails the cyber-physical vulnerabilities and risks that originated in IT, OT, and the physical domains of the CAV ecosystem, eclectic threat landscapes, and threat intelligence. To deal with the security threats in high-speed, high dimensional, multimodal data and assets from eccentric stakeholders of the CAV ecosystem, this chapter presents and analyzes some of the state of art deep learning-based threat intelligence for attack detection. The frontiers in deep learning, namely Meta-Learning and Federated Learning, along with their challenges have been included in the chapter. We have proposed, trained, and tested the deep CNN-LSTM architecture for CAV threat intelligence; assessed and compared the performance of the proposed model against other deep learning algorithms such as DNN, CNN, LSTM. Our results indicate the superiority of the proposed model although DNN and 1d-CNN also achieved more than 99% of accuracy, precision, recall, f1-score, and AUC on the CAV-KDD dataset. The good performance of deep CNN-LSTM comes with the increased model complexity and cumbersome hyperparameters tuning. Still, there are open challenges on deep learning adoption in the CAV cybersecurity paradigm due to lack of properly developed protocols and policies, poorly defined privileges between stakeholders, costlier training, adversarial threats to the model, and poor generalizability of the model under out of data distributions.

</details>

<details>

<summary>2021-09-22 17:28:21 - BFClass: A Backdoor-free Text Classification Framework</summary>

- *Zichao Li, Dheeraj Mekala, Chengyu Dong, Jingbo Shang*

- `2109.10855v1` - [abs](http://arxiv.org/abs/2109.10855v1) - [pdf](http://arxiv.org/pdf/2109.10855v1)

> Backdoor attack introduces artificial vulnerabilities into the model by poisoning a subset of the training data via injecting triggers and modifying labels. Various trigger design strategies have been explored to attack text classifiers, however, defending such attacks remains an open problem. In this work, we propose BFClass, a novel efficient backdoor-free training framework for text classification. The backbone of BFClass is a pre-trained discriminator that predicts whether each token in the corrupted input was replaced by a masked language model. To identify triggers, we utilize this discriminator to locate the most suspicious token from each training sample and then distill a concise set by considering their association strengths with particular labels. To recognize the poisoned subset, we examine the training samples with these identified triggers as the most suspicious token, and check if removing the trigger will change the poisoned model's prediction. Extensive experiments demonstrate that BFClass can identify all the triggers, remove 95% poisoned training samples with very limited false alarms, and achieve almost the same performance as the models trained on the benign training data.

</details>

<details>

<summary>2021-09-22 17:59:05 - A Robust Asymmetric Kernel Function for Bayesian Optimization, with Application to Image Defect Detection in Manufacturing Systems</summary>

- *Areej AlBahar, Inyoung Kim, Xiaowei Yue*

- `2109.10898v1` - [abs](http://arxiv.org/abs/2109.10898v1) - [pdf](http://arxiv.org/pdf/2109.10898v1)

> Some response surface functions in complex engineering systems are usually highly nonlinear, unformed, and expensive-to-evaluate. To tackle this challenge, Bayesian optimization, which conducts sequential design via a posterior distribution over the objective function, is a critical method used to find the global optimum of black-box functions. Kernel functions play an important role in shaping the posterior distribution of the estimated function. The widely used kernel function, e.g., radial basis function (RBF), is very vulnerable and susceptible to outliers; the existence of outliers is causing its Gaussian process surrogate model to be sporadic. In this paper, we propose a robust kernel function, Asymmetric Elastic Net Radial Basis Function (AEN-RBF). Its validity as a kernel function and computational complexity are evaluated. When compared to the baseline RBF kernel, we prove theoretically that AEN-RBF can realize smaller mean squared prediction error under mild conditions. The proposed AEN-RBF kernel function can also realize faster convergence to the global optimum. We also show that the AEN-RBF kernel function is less sensitive to outliers, and hence improves the robustness of the corresponding Bayesian optimization with Gaussian processes. Through extensive evaluations carried out on synthetic and real-world optimization problems, we show that AEN-RBF outperforms existing benchmark kernel functions.

</details>

<details>

<summary>2021-09-22 21:04:20 - Security Analysis of Capsule Network Inference using Horizontal Collaboration</summary>

- *Adewale Adeyemo, Faiq Khalid, Tolulope A. Odetola, Syed Rafay Hasan*

- `2109.11041v1` - [abs](http://arxiv.org/abs/2109.11041v1) - [pdf](http://arxiv.org/pdf/2109.11041v1)

> The traditional convolution neural networks (CNN) have several drawbacks like the Picasso effect and the loss of information by the pooling layer. The Capsule network (CapsNet) was proposed to address these challenges because its architecture can encode and preserve the spatial orientation of input images. Similar to traditional CNNs, CapsNet is also vulnerable to several malicious attacks, as studied by several researchers in the literature. However, most of these studies focus on single-device-based inference, but horizontally collaborative inference in state-of-the-art systems, like intelligent edge services in self-driving cars, voice controllable systems, and drones, nullify most of these analyses. Horizontal collaboration implies partitioning the trained CNN models or CNN tasks to multiple end devices or edge nodes. Therefore, it is imperative to examine the robustness of the CapsNet against malicious attacks when deployed in horizontally collaborative environments. Towards this, we examine the robustness of the CapsNet when subjected to noise-based inference attacks in a horizontal collaborative environment. In this analysis, we perturbed the feature maps of the different layers of four DNN models, i.e., CapsNet, Mini-VGG, LeNet, and an in-house designed CNN (ConvNet) with the same number of parameters as CapsNet, using two types of noised-based attacks, i.e., Gaussian Noise Attack and FGSM noise attack. The experimental results show that similar to the traditional CNNs, depending upon the access of the attacker to the DNN layer, the classification accuracy of the CapsNet drops significantly. For example, when Gaussian Noise Attack classification is performed at the DigitCap layer of the CapsNet, the maximum classification accuracy drop is approximately 97%.

</details>

<details>

<summary>2021-09-23 01:38:43 - Towards Adversarially Robust and Domain Generalizable Stereo Matching by Rethinking DNN Feature Backbones</summary>

- *Kelvin Cheng, Christopher Healey, Tianfu Wu*

- `2108.00335v2` - [abs](http://arxiv.org/abs/2108.00335v2) - [pdf](http://arxiv.org/pdf/2108.00335v2)

> Stereo matching has recently witnessed remarkable progress using Deep Neural Networks (DNNs). But, how robust are they? Although it has been well-known that DNNs often suffer from adversarial vulnerability with a catastrophic drop in performance, the situation is even worse in stereo matching. This paper first shows that a type of weak white-box attacks can overwhelm state-of-the-art methods. The attack is learned by a proposed stereo-constrained projected gradient descent (PGD) method in stereo matching. This observation raises serious concerns for the deployment of DNN-based stereo matching. Parallel to the adversarial vulnerability, DNN-based stereo matching is typically trained under the so-called simulation to reality pipeline, and thus domain generalizability is an important problem. This paper proposes to rethink the learnable DNN-based feature backbone towards adversarially-robust and domain generalizable stereo matching by completely removing it for matching. In experiments, the proposed method is tested in the SceneFlow dataset and the KITTI2015 benchmark, with promising results. We compute the matching cost volume using the classic multi-scale census transform (i.e., local binary pattern) of the raw input stereo images, followed by a stacked Hourglass head sub-network solving the matching problem. It significantly improves the adversarial robustness, while retaining accuracy performance comparable to state-of-the-art methods. It also shows better generalizability from simulation (SceneFlow) to real (KITTI) datasets when no fine-tuning is used.

</details>

<details>

<summary>2021-09-23 10:50:47 - Enhancing Navigational Safety in Crowded Environments using Semantic-Deep-Reinforcement-Learning-based Navigation</summary>

- *Linh Kästner, Junhui Li, Zhengcheng Shen, Jens Lambrecht*

- `2109.11288v1` - [abs](http://arxiv.org/abs/2109.11288v1) - [pdf](http://arxiv.org/pdf/2109.11288v1)

> Intelligent navigation among social crowds is an essential aspect of mobile robotics for applications such as delivery, health care, or assistance. Deep Reinforcement Learning emerged as an alternative planning method to conservative approaches and promises more efficient and flexible navigation. However, in highly dynamic environments employing different kinds of obstacle classes, safe navigation still presents a grand challenge. In this paper, we propose a semantic Deep-reinforcement-learning-based navigation approach that teaches object-specific safety rules by considering high-level obstacle information. In particular, the agent learns object-specific behavior by contemplating the specific danger zones to enhance safety around vulnerable object classes. We tested the approach against a benchmark obstacle avoidance approach and found an increase in safety. Furthermore, we demonstrate that the agent could learn to navigate more safely by keeping an individual safety distance dependent on the semantic information.

</details>

<details>

<summary>2021-09-23 12:42:36 - On The Vulnerability of Anti-Malware Solutions to DNS Attacks</summary>

- *Asaf Nadler, Ron Bitton, Oleg Brodt, Asaf Shabtai*

- `2109.11342v1` - [abs](http://arxiv.org/abs/2109.11342v1) - [pdf](http://arxiv.org/pdf/2109.11342v1)

> Anti-malware agents typically communicate with their remote services to share information about suspicious files. These remote services use their up-to-date information and global context (view) to help classify the files and instruct their agents to take a predetermined action (e.g., delete or quarantine). In this study, we provide a security analysis of a specific form of communication between anti-malware agents and their services, which takes place entirely over the insecure DNS protocol. These services, which we denote DNS anti-malware list (DNSAML) services, affect the classification of files scanned by anti-malware agents, therefore potentially putting their consumers at risk due to known integrity and confidentiality flaws of the DNS protocol. By analyzing a large-scale DNS traffic dataset made available to the authors by a well-known CDN provider, we identify anti-malware solutions that seem to make use of DNSAML services. We found that these solutions, deployed on almost three million machines worldwide, exchange hundreds of millions of DNS requests daily. These requests are carrying sensitive file scan information, oftentimes - as we demonstrate - without any additional safeguards to compensate for the insecurities of the DNS protocol. As a result, these anti-malware solutions that use DNSAML are made vulnerable to DNS attacks. For instance, an attacker capable of tampering with DNS queries, gains the ability to alter the classification of scanned files, without presence on the scanning machine. We showcase three attacks applicable to at least three anti-malware solutions that could result in the disclosure of sensitive information and improper behavior of the anti-malware agent, such as ignoring detected threats. Finally, we propose and review a set of countermeasures for anti-malware solution providers to prevent the attacks stemming from the use of DNSAML services.

</details>

<details>

<summary>2021-09-23 18:45:29 - A Tree-based Construction for Verifiable Diplomas with Issuer Transparency</summary>

- *Rodrigo Q. Saramago, Leander Jehl, Hein Meling, Vero Estrada-Galiñanes*

- `2109.11590v1` - [abs](http://arxiv.org/abs/2109.11590v1) - [pdf](http://arxiv.org/pdf/2109.11590v1)

> Still to this day, academic credentials are primarily paper-based, and the process to verify the authenticity of such documents is costly, time-consuming, and prone to human error and fraud. Digitally signed documents facilitate a cost-effective verification process. However, vulnerability to fraud remains due to reliance on centralized authorities that lack full transparency. In this paper, we present the mechanisms we designed to create secure and machine-verifiable academic credentials. Our protocol models a diploma as an evolving set of immutable credentials. The credentials are built as a tree-based data structure with linked time-stamping, where portions of credentials are distributed over a set of smart contracts. Our design prevents fraud of diplomas and eases the detection of degree mills, while increasing the transparency and trust in the issuer's procedures. Our evaluation shows that our solution offers a certification system with strong cryptographic security and imposes a high level of transparency of the certification process. We achieve these benefits with acceptable costs compared to existing solutions that lack such transparency.

</details>

<details>

<summary>2021-09-23 18:56:05 - MISA: Online Defense of Trojaned Models using Misattributions</summary>

- *Panagiota Kiourti, Wenchao Li, Anirban Roy, Karan Sikka, Susmit Jha*

- `2103.15918v2` - [abs](http://arxiv.org/abs/2103.15918v2) - [pdf](http://arxiv.org/pdf/2103.15918v2)

> Recent studies have shown that neural networks are vulnerable to Trojan attacks, where a network is trained to respond to specially crafted trigger patterns in the inputs in specific and potentially malicious ways. This paper proposes MISA, a new online approach to detect Trojan triggers for neural networks at inference time. Our approach is based on a novel notion called misattributions, which captures the anomalous manifestation of a Trojan activation in the feature space. Given an input image and the corresponding output prediction, our algorithm first computes the model's attribution on different features. It then statistically analyzes these attributions to ascertain the presence of a Trojan trigger. Across a set of benchmarks, we show that our method can effectively detect Trojan triggers for a wide variety of trigger patterns, including several recent ones for which there are no known defenses. Our method achieves 96% AUC for detecting images that include a Trojan trigger without any assumptions on the trigger pattern.

</details>

<details>

<summary>2021-09-23 22:41:08 - Security Review of Ethereum Beacon Clients</summary>

- *Jean-Philippe Aumasson, Denis Kolegov, Evangelia Stathopoulou*

- `2109.11677v1` - [abs](http://arxiv.org/abs/2109.11677v1) - [pdf](http://arxiv.org/pdf/2109.11677v1)

> The beacon chain is the backbone of the Ethereum's evolution towards a proof-of-stake-based scalable network. Beacon clients are the applications implementing the services required to operate the beacon chain, namely validators, beacon nodes, and slashers. Security defects in beacon clients could lead to loss of funds, consensus rules violation, network congestion, and other inconveniences. We reported more than 35 issues to the beacon client developers, including various security improvements, specification inconsistencies, missing security checks, exposure to known vulnerabilities. None of our findings appears to be high-severity. We covered the four main beacon clients, namely Lighthouse (Rust), Nimbus (Nim), Prysm (Go), and Teku (Java). We looked for bugs in the logic and implementation of the new security-critical components (BLS signatures, slashing, networking protocols, and API) over a 3-month project that followed a preliminary analysis of BLS signatures code. We focused on Lighthouse and Prysm, the most popular clients, and thus the highest-value targets. Furthermore, we identify protocol-level issues, including replay attacks and incomplete forward secrecy. In addition, we reviewed the network fingerprints of beacon clients, discussing the information obtainable from passive and active searches, and we analyzed the supply chain risk related to third-party dependencies, providing indicators and recommendations to reduce the risk of backdoors and unpatchable vulnerabilities. Our results suggest that despite intense scrutiny by security auditors and independent researchers, the complexity and constant evolution of a platform like Ethereum requires regular expert review and thorough SSDLC practices.

</details>

<details>

<summary>2021-09-24 08:29:50 - Local Intrinsic Dimensionality Signals Adversarial Perturbations</summary>

- *Sandamal Weerasinghe, Tansu Alpcan, Sarah M. Erfani, Christopher Leckie, Benjamin I. P. Rubinstein*

- `2109.11803v1` - [abs](http://arxiv.org/abs/2109.11803v1) - [pdf](http://arxiv.org/pdf/2109.11803v1)

> The vulnerability of machine learning models to adversarial perturbations has motivated a significant amount of research under the broad umbrella of adversarial machine learning. Sophisticated attacks may cause learning algorithms to learn decision functions or make decisions with poor predictive performance. In this context, there is a growing body of literature that uses local intrinsic dimensionality (LID), a local metric that describes the minimum number of latent variables required to describe each data point, for detecting adversarial samples and subsequently mitigating their effects. The research to date has tended to focus on using LID as a practical defence method often without fully explaining why LID can detect adversarial samples. In this paper, we derive a lower-bound and an upper-bound for the LID value of a perturbed data point and demonstrate that the bounds, in particular the lower-bound, has a positive correlation with the magnitude of the perturbation. Hence, we demonstrate that data points that are perturbed by a large amount would have large LID values compared to unperturbed samples, thus justifying its use in the prior literature. Furthermore, our empirical validation demonstrates the validity of the bounds on benchmark datasets.

</details>

<details>

<summary>2021-09-24 21:46:07 - Finding Taint-Style Vulnerabilities in Linux-based Embedded Firmware with SSE-based Alias Analysis</summary>

- *Kai Cheng, Tao Liu, Le Guan, Peng Liu, Hong Li, Hongsong Zhu, Limin Sun*

- `2109.12209v1` - [abs](http://arxiv.org/abs/2109.12209v1) - [pdf](http://arxiv.org/pdf/2109.12209v1)

> Although the importance of using static analysis to detect taint-style vulnerabilities in Linux-based embedded firmware is widely recognized, existing approaches are plagued by three major limitations. (a) Approaches based on symbolic execution may miss alias information and therefore suffer from a high false-negative rate. (b) Approaches based on VSA (value set analysis) often provide an over-approximate pointer range. As a result, many false positives could be produced. (c) Existing work for detecting taint-style vulnerability does not consider indirect call resolution, whereas indirect calls are frequently used in Internet-facing embedded devices. As a result, many false negatives could be produced. In this work, we propose a precise demand-driven flow-, context- and field-sensitive alias analysis approach. Based on this new approach, we also design a novel indirect call resolution scheme. Combined with sanitization rule checking, our solution discovers taint-style vulnerabilities by static taint analysis. We implemented our idea with a prototype called EmTaint and evaluated it against 35 real-world embedded firmware samples from six popular vendors. EmTaint discovered at least 192 bugs, including 41 n-day bugs and 151 0-day bugs. At least 115 CVE/PSV numbers have been allocated from a subset of the reported vulnerabilities at the time of writing. Compared to state-of-the-art tools such as KARONTE and SaTC, EmTaint found significantly more bugs on the same dataset in less time.

</details>

<details>

<summary>2021-09-25 17:24:48 - MINIMAL: Mining Models for Data Free Universal Adversarial Triggers</summary>

- *Swapnil Parekh, Yaman Singla Kumar, Somesh Singh, Changyou Chen, Balaji Krishnamurthy, Rajiv Ratn Shah*

- `2109.12406v1` - [abs](http://arxiv.org/abs/2109.12406v1) - [pdf](http://arxiv.org/pdf/2109.12406v1)

> It is well known that natural language models are vulnerable to adversarial attacks, which are mostly input-specific in nature. Recently, it has been shown that there also exist input-agnostic attacks in NLP models, called universal adversarial triggers. However, existing methods to craft universal triggers are data intensive. They require large amounts of data samples to generate adversarial triggers, which are typically inaccessible by attackers. For instance, previous works take 3000 data samples per class for the SNLI dataset to generate adversarial triggers. In this paper, we present a novel data-free approach, MINIMAL, to mine input-agnostic adversarial triggers from models. Using the triggers produced with our data-free algorithm, we reduce the accuracy of Stanford Sentiment Treebank's positive class from 93.6% to 9.6%. Similarly, for the Stanford Natural Language Inference (SNLI), our single-word trigger reduces the accuracy of the entailment class from 90.95% to less than 0.6\%. Despite being completely data-free, we get equivalent accuracy drops as data-dependent methods.

</details>

<details>

<summary>2021-09-25 18:07:09 - ConE: A Concurrent Edit Detection Tool for Large Scale Software Development</summary>

- *Chandra Maddila, Nachiappan Nagappan, Christian Bird, Georgios Gousios, Arie van Deursen*

- `2101.06542v3` - [abs](http://arxiv.org/abs/2101.06542v3) - [pdf](http://arxiv.org/pdf/2101.06542v3)

> Modern, complex software systems are being continuously extended and adjusted. The developers responsible for this may come from different teams or organizations, and may be distributed over the world. This may make it difficult to keep track of what other developers are doing, which may result in multiple developers concurrently editing the same code areas. This, in turn, may lead to hard-to-merge changes or even merge conflicts, logical bugs that are difficult to detect, duplication of work, and wasted developer productivity. To address this, we explore the extent of this problem in the pull request based software development model. We study half a year of changes made to six large repositories in Microsoft in which at least 1,000 pull requests are created each month. We find that files concurrently edited in different pull requests are more likely to introduce bugs. Motivated by these findings, we design, implement, and deploy a service named ConE (Concurrent Edit Detector) that proactively detects pull requests containing concurrent edits, to help mitigate the problems caused by them. ConE has been designed to scale, and to minimize false alarms while still flagging relevant concurrently edited files. Key concepts of ConE include the detection of the Extent of Overlap between pull requests, and the identification of Rarely Concurrently Edited Files. To evaluate ConE, we report on its operational deployment on 234 repositories inside Microsoft. ConE assessed 26,000 pull requests and made 775 recommendations about conflicting changes, which were rated as useful in over 70% (554) of the cases. From interviews with 48 users we learned that they believed ConE would save time in conflict resolution and avoiding duplicate work, and that over 90% intend to keep using the service on a daily basis.

</details>

<details>

<summary>2021-09-25 23:02:47 - Contributions to Large Scale Bayesian Inference and Adversarial Machine Learning</summary>

- *Víctor Gallego*

- `2109.13232v1` - [abs](http://arxiv.org/abs/2109.13232v1) - [pdf](http://arxiv.org/pdf/2109.13232v1)

> The rampant adoption of ML methodologies has revealed that models are usually adopted to make decisions without taking into account the uncertainties in their predictions. More critically, they can be vulnerable to adversarial examples. Thus, we believe that developing ML systems that take into account predictive uncertainties and are robust against adversarial examples is a must for critical, real-world tasks. We start with a case study in retailing. We propose a robust implementation of the Nerlove-Arrow model using a Bayesian structural time series model. Its Bayesian nature facilitates incorporating prior information reflecting the manager's views, which can be updated with relevant data. However, this case adopted classical Bayesian techniques, such as the Gibbs sampler. Nowadays, the ML landscape is pervaded with neural networks and this chapter also surveys current developments in this sub-field. Then, we tackle the problem of scaling Bayesian inference to complex models and large data regimes. In the first part, we propose a unifying view of two different Bayesian inference algorithms, Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) and Stein Variational Gradient Descent (SVGD), leading to improved and efficient novel sampling schemes. In the second part, we develop a framework to boost the efficiency of Bayesian inference in probabilistic models by embedding a Markov chain sampler within a variational posterior approximation. After that, we present an alternative perspective on adversarial classification based on adversarial risk analysis, and leveraging the scalable Bayesian approaches from chapter 2. In chapter 4 we turn to reinforcement learning, introducing Threatened Markov Decision Processes, showing the benefits of accounting for adversaries in RL while the agent learns.

</details>

<details>

<summary>2021-09-25 23:45:16 - VirTex: Learning Visual Representations from Textual Annotations</summary>

- *Karan Desai, Justin Johnson*

- `2006.06666v3` - [abs](http://arxiv.org/abs/2006.06666v3) - [pdf](http://arxiv.org/pdf/2006.06666v3)

> The de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.

</details>

<details>

<summary>2021-09-26 10:01:52 - MixNN: Protection of Federated Learning Against Inference Attacks by Mixing Neural Network Layers</summary>

- *Antoine Boutet, Thomas Lebrun, Jan Aalmoes, Adrien Baud*

- `2109.12550v1` - [abs](http://arxiv.org/abs/2109.12550v1) - [pdf](http://arxiv.org/pdf/2109.12550v1)

> Machine Learning (ML) has emerged as a core technology to provide learning models to perform complex tasks. Boosted by Machine Learning as a Service (MLaaS), the number of applications relying on ML capabilities is ever increasing. However, ML models are the source of different privacy violations through passive or active attacks from different entities. In this paper, we present MixNN a proxy-based privacy-preserving system for federated learning to protect the privacy of participants against a curious or malicious aggregation server trying to infer sensitive attributes. MixNN receives the model updates from participants and mixes layers between participants before sending the mixed updates to the aggregation server. This mixing strategy drastically reduces privacy without any trade-off with utility. Indeed, mixing the updates of the model has no impact on the result of the aggregation of the updates computed by the server. We experimentally evaluate MixNN and design a new attribute inference attack, Sim, exploiting the privacy vulnerability of SGD algorithm to quantify privacy leakage in different settings (i.e., the aggregation server can conduct a passive or an active attack). We show that MixNN significantly limits the attribute inference compared to a baseline using noisy gradient (well known to damage the utility) while keeping the same level of utility as classic federated learning.

</details>

<details>

<summary>2021-09-26 16:25:49 - Defect Prediction Guided Search-Based Software Testing</summary>

- *Anjana Perera, Aldeida Aleti, Marcel Böhme, Burak Turhan*

- `2109.12645v1` - [abs](http://arxiv.org/abs/2109.12645v1) - [pdf](http://arxiv.org/pdf/2109.12645v1)

> Today, most automated test generators, such as search-based software testing (SBST) techniques focus on achieving high code coverage. However, high code coverage is not sufficient to maximise the number of bugs found, especially when given a limited testing budget. In this paper, we propose an automated test generation technique that is also guided by the estimated degree of defectiveness of the source code. Parts of the code that are likely to be more defective receive more testing budget than the less defective parts. To measure the degree of defectiveness, we leverage Schwa, a notable defect prediction technique.   We implement our approach into EvoSuite, a state of the art SBST tool for Java. Our experiments on the Defects4J benchmark demonstrate the improved efficiency of defect prediction guided test generation and confirm our hypothesis that spending more time budget on likely defective parts increases the number of bugs found in the same time budget.

</details>

<details>

<summary>2021-09-26 23:12:10 - Strategic Remote Attestation: Testbed for Internet-of-Things Devices and Stackelberg Security Game for Optimal Strategies</summary>

- *Shanto Roy, Salah Uddin Kadir, Yevgeniy Vorobeychik, Aron Laszka*

- `2109.07724v2` - [abs](http://arxiv.org/abs/2109.07724v2) - [pdf](http://arxiv.org/pdf/2109.07724v2)

> Internet of Things (IoT) devices and applications can have significant vulnerabilities, which may be exploited by adversaries to cause considerable harm. An important approach for mitigating this threat is remote attestation, which enables the defender to remotely verify the integrity of devices and their software. There are a number of approaches for remote attestation, and each has its unique advantages and disadvantages in terms of detection accuracy and computational cost. Further, an attestation method may be applied in multiple ways, such as various levels of software coverage. Therefore, to minimize both security risks and computational overhead, defenders need to decide strategically which attestation methods to apply and how to apply them, depending on the characteristic of the devices and the potential losses.   To answer these questions, we first develop a testbed for remote attestation of IoT devices, which enables us to measure the detection accuracy and performance overhead of various attestation methods. Our testbed integrates two example IoT applications, memory-checksum based attestation, and a variety of software vulnerabilities that allow adversaries to inject arbitrary code into running applications. Second, we model the problem of finding an optimal strategy for applying remote attestation as a Stackelberg security game between a defender and an adversary. We characterize the defender's optimal attestation strategy in a variety of special cases. Finally, building on experimental results from our testbed, we evaluate our model and show that optimal strategic attestation can lead to significantly lower losses than naive baseline strategies.

</details>

<details>

<summary>2021-09-27 03:16:49 - Review of the Security of Backward-Compatible Automotive Inter-ECU Communication</summary>

- *Chandra Sharma, Samuel Moylan, George Amariucai, Eugene Y. Vasserman*

- `1910.04150v3` - [abs](http://arxiv.org/abs/1910.04150v3) - [pdf](http://arxiv.org/pdf/1910.04150v3)

> Advanced electronic units inside modern vehicles have enhanced the driving experience, but also introduced a myriad of security problems due to the inherent limitations of the internal communication protocol. In the last two decades, a number of security threats have been identified and accordingly, security measures have been proposed. While a large body of research on the vehicular security domain is focused on exposing vulnerabilities and proposing counter measures, there is an apparent paucity of research aimed at reviewing existing works on automotive security and at extracting insights. This paper provides a systematic review of security threats and countermeasures for the ubiquitous CAN bus communication protocol. It further exposes the limitations of the existing security measures, and discusses a seemingly-overlooked, simple, cost-effective and incrementally deployable solution which can provide a reasonable defense against a major class of packet injection attacks and many denial of service attacks.

</details>

<details>

<summary>2021-09-27 03:56:29 - A Hard Label Black-box Adversarial Attack Against Graph Neural Networks</summary>

- *Jiaming Mu, Binghui Wang, Qi Li, Kun Sun, Mingwei Xu, Zhuotao Liu*

- `2108.09513v2` - [abs](http://arxiv.org/abs/2108.09513v2) - [pdf](http://arxiv.org/pdf/2108.09513v2)

> Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph structure related tasks such as node classification and graph classification. However, GNNs are vulnerable to adversarial attacks. Existing works mainly focus on attacking GNNs for node classification; nevertheless, the attacks against GNNs for graph classification have not been well explored.   In this work, we conduct a systematic study on adversarial attacks against GNNs for graph classification via perturbing the graph structure. In particular, we focus on the most challenging attack, i.e., hard label black-box attack, where an attacker has no knowledge about the target GNN model and can only obtain predicted labels through querying the target model.To achieve this goal, we formulate our attack as an optimization problem, whose objective is to minimize the number of edges to be perturbed in a graph while maintaining the high attack success rate. The original optimization problem is intractable to solve, and we relax the optimization problem to be a tractable one, which is solved with theoretical convergence guarantee. We also design a coarse-grained searching algorithm and a query-efficient gradient computation algorithm to decrease the number of queries to the target GNN model. Our experimental results on three real-world datasets demonstrate that our attack can effectively attack representative GNNs for graph classification with less queries and perturbations. We also evaluate the effectiveness of our attack under two defenses: one is well-designed adversarial graph detector and the other is that the target GNN model itself is equipped with a defense to prevent adversarial graph generation. Our experimental results show that such defenses are not effective enough, which highlights more advanced defenses.

</details>

<details>

<summary>2021-09-27 07:15:01 - MUTEN: Boosting Gradient-Based Adversarial Attacks via Mutant-Based Ensembles</summary>

- *Yuejun Guo, Qiang Hu, Maxime Cordy, Michail Papadakis, Yves Le Traon*

- `2109.12838v1` - [abs](http://arxiv.org/abs/2109.12838v1) - [pdf](http://arxiv.org/pdf/2109.12838v1)

> Deep Neural Networks (DNNs) are vulnerable to adversarial examples, which causes serious threats to security-critical applications. This motivated much research on providing mechanisms to make models more robust against adversarial attacks. Unfortunately, most of these defenses, such as gradient masking, are easily overcome through different attack means. In this paper, we propose MUTEN, a low-cost method to improve the success rate of well-known attacks against gradient-masking models. Our idea is to apply the attacks on an ensemble model which is built by mutating the original model elements after training. As we found out that mutant diversity is a key factor in improving success rate, we design a greedy algorithm for generating diverse mutants efficiently. Experimental results on MNIST, SVHN, and CIFAR10 show that MUTEN can increase the success rate of four attacks by up to 0.45.

</details>

<details>

<summary>2021-09-27 09:09:44 - Dynamically Sampled Nonlocal Gradients for Stronger Adversarial Attacks</summary>

- *Leo Schwinn, An Nguyen, René Raab, Dario Zanca, Bjoern Eskofier, Daniel Tenbrinck, Martin Burger*

- `2011.02707v4` - [abs](http://arxiv.org/abs/2011.02707v4) - [pdf](http://arxiv.org/pdf/2011.02707v4)

> The vulnerability of deep neural networks to small and even imperceptible perturbations has become a central topic in deep learning research. Although several sophisticated defense mechanisms have been introduced, most were later shown to be ineffective. However, a reliable evaluation of model robustness is mandatory for deployment in safety-critical scenarios. To overcome this problem we propose a simple yet effective modification to the gradient calculation of state-of-the-art first-order adversarial attacks. Normally, the gradient update of an attack is directly calculated for the given data point. This approach is sensitive to noise and small local optima of the loss function. Inspired by gradient sampling techniques from non-convex optimization, we propose Dynamically Sampled Nonlocal Gradient Descent (DSNGD). DSNGD calculates the gradient direction of the adversarial attack as the weighted average over past gradients of the optimization history. Moreover, distribution hyperparameters that define the sampling operation are automatically learned during the optimization scheme. We empirically show that by incorporating this nonlocal gradient information, we are able to give a more accurate estimation of the global descent direction on noisy and non-convex loss surfaces. In addition, we show that DSNGD-based attacks are on average 35% faster while achieving 0.9% to 27.1% higher success rates compared to their gradient descent-based counterparts.

</details>

<details>

<summary>2021-09-27 12:48:40 - Federated Deep Learning with Bayesian Privacy</summary>

- *Hanlin Gu, Lixin Fan, Bowen Li, Yan Kang, Yuan Yao, Qiang Yang*

- `2109.13012v1` - [abs](http://arxiv.org/abs/2109.13012v1) - [pdf](http://arxiv.org/pdf/2109.13012v1)

> Federated learning (FL) aims to protect data privacy by cooperatively learning a model without sharing private data among users. For Federated Learning of Deep Neural Network with billions of model parameters, existing privacy-preserving solutions are unsatisfactory. Homomorphic encryption (HE) based methods provide secure privacy protections but suffer from extremely high computational and communication overheads rendering it almost useless in practice . Deep learning with Differential Privacy (DP) was implemented as a practical learning algorithm at a manageable cost in complexity. However, DP is vulnerable to aggressive Bayesian restoration attacks as disclosed in the literature and demonstrated in experimental results of this work. To address the aforementioned perplexity, we propose a novel Bayesian Privacy (BP) framework which enables Bayesian restoration attacks to be formulated as the probability of reconstructing private data from observed public information. Specifically, the proposed BP framework accurately quantifies privacy loss by Kullback-Leibler (KL) Divergence between the prior distribution about the privacy data and the posterior distribution of restoration private data conditioning on exposed information}. To our best knowledge, this Bayesian Privacy analysis is the first to provides theoretical justification of secure privacy-preserving capabilities against Bayesian restoration attacks. As a concrete use case, we demonstrate that a novel federated deep learning method using private passport layers is able to simultaneously achieve high model performance, privacy-preserving capability and low computational complexity. Theoretical analysis is in accordance with empirical measurements of information leakage extensively experimented with a variety of DNN networks on image classification MNIST, CIFAR10, and CIFAR100 datasets.

</details>

<details>

<summary>2021-09-27 14:50:45 - Casting exploit analysis as a Weird Machine reconstruction problem</summary>

- *Robert Abela, Mark Vella*

- `2109.13100v1` - [abs](http://arxiv.org/abs/2109.13100v1) - [pdf](http://arxiv.org/pdf/2109.13100v1)

> Exploits constitute malware in the form of application inputs. They take advantage of security vulnerabilities inside programs in order to yield execution control to attackers. The root cause of successful exploitation lies in emergent functionality introduced when programs are compiled and loaded in memory for execution, called `Weird Machines' (WMs). Essentially WMs are unexpected virtual machines that execute attackers' bytecode, complicating malware analysis whenever the bytecode set is unknown. We take the direction that WM bytecode is best understood at the level of the process memory layout attained by exploit execution. Each step building towards this memory layout comprises an exploit primitive, an exploit's basic building block. This work presents a WM reconstruction algorithm that works by identifying pre-defined exploit primitive-related behaviour during the dynamic analysis of target binaries, associating it with the responsible exploit segment - the WM bytecode. In this manner any analyst familiar with exploit programming will immediately recognise the reconstructed WM bytecode's semantics. This work is a first attempt at studying the feasibility of this method and focuses on web browsers when targeted by JavaScript exploits.

</details>

<details>

<summary>2021-09-27 17:35:42 - Classification and Adversarial examples in an Overparameterized Linear Model: A Signal Processing Perspective</summary>

- *Adhyyan Narang, Vidya Muthukumar, Anant Sahai*

- `2109.13215v1` - [abs](http://arxiv.org/abs/2109.13215v1) - [pdf](http://arxiv.org/pdf/2109.13215v1)

> State-of-the-art deep learning classifiers are heavily overparameterized with respect to the amount of training examples and observed to generalize well on "clean" data, but be highly susceptible to infinitesmal adversarial perturbations. In this paper, we identify an overparameterized linear ensemble, that uses the "lifted" Fourier feature map, that demonstrates both of these behaviors. The input is one-dimensional, and the adversary is only allowed to perturb these inputs and not the non-linear features directly. We find that the learned model is susceptible to adversaries in an intermediate regime where classification generalizes but regression does not. Notably, the susceptibility arises despite the absence of model mis-specification or label noise, which are commonly cited reasons for adversarial-susceptibility. These results are extended theoretically to a random-Fourier-sum setup that exhibits double-descent behavior. In both feature-setups, the adversarial vulnerability arises because of a phenomenon we term spatial localization: the predictions of the learned model are markedly more sensitive in the vicinity of training points than elsewhere. This sensitivity is a consequence of feature lifting and is reminiscent of Gibb's and Runge's phenomena from signal processing and functional analysis. Despite the adversarial susceptibility, we find that classification with these features can be easier than the more commonly studied "independent feature" models.

</details>

<details>

<summary>2021-09-27 18:36:20 - GANG-MAM: GAN based enGine for Modifying Android Malware</summary>

- *Renjith G, Sonia Laudanna, Aji S, Corrado Aaron Visaggio, Vinod P*

- `2109.13297v1` - [abs](http://arxiv.org/abs/2109.13297v1) - [pdf](http://arxiv.org/pdf/2109.13297v1)

> Malware detectors based on machine learning are vulnerable to adversarial attacks. Generative Adversarial Networks (GAN) are architectures based on Neural Networks that could produce successful adversarial samples. The interest towards this technology is quickly growing. In this paper, we propose a system that produces a feature vector for making an Android malware strongly evasive and then modify the malicious program accordingly. Such a system could have a twofold contribution: it could be used to generate datasets to validate systems for detecting GAN-based malware and to enlarge the training and testing dataset for making more robust malware classifiers.

</details>

<details>

<summary>2021-09-28 03:44:07 - A Toolkit for Generating Code Knowledge Graphs</summary>

- *Ibrahim Abdelaziz, Julian Dolby, Jamie McCusker, Kavitha Srinivas*

- `2002.09440v3` - [abs](http://arxiv.org/abs/2002.09440v3) - [pdf](http://arxiv.org/pdf/2002.09440v3)

> Knowledge graphs have been proven extremely useful in powering diverse applications in semantic search and natural language understanding. In this paper, we present GraphGen4Code, a toolkit to build code knowledge graphs that can similarly power various applications such as program search, code understanding, bug detection, and code automation. GraphGen4Code uses generic techniques to capture code semantics with the key nodes in the graph representing classes, functions, and methods. Edges indicate function usage (e.g., how data flows through function calls, as derived from program analysis of real code), and documentation about functions (e.g., code documentation, usage documentation, or forum discussions such as StackOverflow). Our toolkit uses named graphs in RDF to model graphs per program, or can output graphs as JSON. We show the scalability of the toolkit by applying it to 1.3 million Python files drawn from GitHub, 2,300 Python modules, and 47 million forum posts. This results in an integrated code graph with over 2 billion triples. We make the toolkit to build such graphs as well as the sample extraction of the 2 billion triples graph publicly available to the community for use.

</details>

<details>

<summary>2021-09-28 08:58:39 - Something Old, Something New: Grammar-based CCG Parsing with Transformer Models</summary>

- *Stephen Clark*

- `2109.10044v2` - [abs](http://arxiv.org/abs/2109.10044v2) - [pdf](http://arxiv.org/pdf/2109.10044v2)

> This report describes the parsing problem for Combinatory Categorial Grammar (CCG), showing how a combination of Transformer-based neural models and a symbolic CCG grammar can lead to substantial gains over existing approaches. The report also documents a 20-year research program, showing how NLP methods have evolved over this time. The staggering accuracy improvements provided by neural models for CCG parsing can be seen as a reflection of the improvements seen in NLP more generally. The report provides a minimal introduction to CCG and CCG parsing, with many pointers to the relevant literature. It then describes the CCG supertagging problem, and some recent work from Tian et al. (2020) which applies Transformer-based models to supertagging with great effect. I use this existing model to develop a CCG multitagger, which can serve as a front-end to an existing CCG parser. Simply using this new multitagger provides substantial gains in parsing accuracy. I then show how a Transformer-based model from the parsing literature can be combined with the grammar-based CCG parser, setting a new state-of-the-art for the CCGbank parsing task of almost 93% F-score for labelled dependencies, with complete sentence accuracies of over 50%.

</details>

<details>

<summary>2021-09-28 11:23:40 - Physical Unclonable Functions using speckle patterns of perfect optical vortices</summary>

- *Patnala Vanitha, Bhargavi Manupati, Inbarasan Muniraj, Satish Anamalamudi, Salla Gangi Reddy, R. P. Singh*

- `2109.13621v1` - [abs](http://arxiv.org/abs/2109.13621v1) - [pdf](http://arxiv.org/pdf/2109.13621v1)

> Encryption techniques demonstrate a great deal of security when implemented in an optical system (such as holography) due to the inherent physical properties of light and the precision it demands. However, such systems have shown to be vulnerable during digital implementations under various crypt-analysis attacks. One of the primary reasons for this is the predictable nature of the security keys (i.e., simulated random keys) used in the encryption process. To alleviate, in this work, we are presenting a Physically Unclonable Functions (PUFs) for producing a robust security key for digital encryption systems. To note, a correlation function of the scattered perfect optical vortex (POV) beams is utilized to generate the encryption keys. To the best of our knowledge, this is the first report on properly utilizing the scattered POV in optical encryption system. To validate the generated key, one of the standard optical encryption systems i.e., Double Random Phase Encoding, is opted. Experimental and simulation results validate that the proposed key generation method is an effective alternative to the digital keys.

</details>

<details>

<summary>2021-09-28 11:48:04 - A Formally Verified Configuration for Hardware Security Modules in the Cloud</summary>

- *Riccardo Focardi, Flaminia L. Luccio*

- `2109.13631v1` - [abs](http://arxiv.org/abs/2109.13631v1) - [pdf](http://arxiv.org/pdf/2109.13631v1)

> Hardware Security Modules (HSMs) are trusted machines that perform sensitive operations in critical ecosystems. They are usually required by law in financial and government digital services. The most important feature of an HSM is its ability to store sensitive credentials and cryptographic keys inside a tamper-resistant hardware, so that every operation is done internally through a suitable API, and such sensitive data are never exposed outside the device. HSMs are now conveniently provided in the cloud, meaning that the physical machines are remotely hosted by some provider and customers can access them through a standard API. The property of keeping sensitive data inside the device is even more important in this setting as a vulnerable application might expose the full API to an attacker. Unfortunately, in the last 20+ years a multitude of practical API-level attacks have been found and proved feasible in real devices. The latest version of PKCS#11, the most popular standard API for HSMs, does not address these issues leaving all the flaws possible. In this paper, we propose the first secure HSM configuration that does not require any restriction or modification of the PKCS#11 API and is suitable to cloud HSM solutions, where compliance to the standard API is of paramount importance. The configuration relies on a careful separation of roles among the different HSM users so that known API flaws are not exploitable by any attacker taking control of the application. We prove the correctness of the configuration by providing a formal model in the state-of-the-art Tamarin prover and we show how to implement the configuration in a real cloud HSM solution.

</details>

<details>

<summary>2021-09-28 11:49:21 - Adopting Automated Bug Assignment in Practice -- A Registered Report of an Industrial Case Study</summary>

- *Markus Borg, Leif Jonsson, Emelie Engström, Béla Bartalos, Attila Szabo*

- `2109.13635v1` - [abs](http://arxiv.org/abs/2109.13635v1) - [pdf](http://arxiv.org/pdf/2109.13635v1)

> [Background/Context] The continuous inflow of bug reports is a considerable challenge in large development projects. Inspired by contemporary work on mining software repositories, we designed a prototype bug assignment solution based on machine learning in 2011-2016. The prototype evolved into an internal Ericsson product, TRR, in 2017-2018. TRR's first bug assignment without human intervention happened in 2019. [Objective/Aim] Our exploratory study will evaluate the adoption of TRR within its industrial context at Ericsson. We seek to understand 1) how TRR performs in the field, 2) what value TRR provides to Ericsson, and 3) how TRR has influenced the ways of working. Secondly, we will provide lessons learned related to productization of a research prototype within a company. [Method] We design an industrial case study combining interviews with TRR developers and users with analysis of data extracted from the bug tracking system at Ericsson. Furthermore, we will analyze sprint planning meetings recorded during the productization. Our data analysis will include thematic analysis, descriptive statistics, and Bayesian causal analysis.

</details>

<details>

<summary>2021-09-28 13:06:54 - Hidden Backdoors in Human-Centric Language Models</summary>

- *Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu*

- `2105.00164v3` - [abs](http://arxiv.org/abs/2105.00164v3) - [pdf](http://arxiv.org/pdf/2105.00164v3)

> Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors. In this paper, we create covert and natural triggers for textual backdoor attacks, \textit{hidden backdoors}, where triggers can fool both modern language models and human inspection. We deploy our hidden backdoors through two state-of-the-art trigger embedding methods. The first approach via homograph replacement, embeds the trigger into deep neural networks through the visual spoofing of lookalike character replacement. The second approach uses subtle differences between text generated by language models and real natural text to produce trigger sentences with correct grammar and high fluency. We demonstrate that the proposed hidden backdoors can be effective across three downstream security-critical NLP tasks, representative of modern human-centric NLP systems, including toxic comment detection, neural machine translation (NMT), and question answering (QA). Our two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at least $97\%$ with an injection rate of only $3\%$ in toxic comment detection, $95.1\%$ ASR in NMT with less than $0.5\%$ injected data, and finally $91.12\%$ ASR against QA updated with only 27 poisoning data samples on a model previously trained with 92,024 samples (0.029\%). We are able to demonstrate the adversary's high success rate of attacks, while maintaining functionality for regular users, with triggers inconspicuous by the human administrators.

</details>

<details>

<summary>2021-09-28 15:55:10 - What to Prioritize? Natural Language Processing for the Development of a Modern Bug Tracking Solution in Hardware Development</summary>

- *Thi Thu Hang Do, Markus Dobler, Niklas Kühl*

- `2109.13825v1` - [abs](http://arxiv.org/abs/2109.13825v1) - [pdf](http://arxiv.org/pdf/2109.13825v1)

> Managing large numbers of incoming bug reports and finding the most critical issues in hardware development is time consuming, but crucial in order to reduce development costs. In this paper, we present an approach to predict the time to fix, the risk and the complexity of debugging and resolution of a bug report using different supervised machine learning algorithms, namely Random Forest, Naive Bayes, SVM, MLP and XGBoost. Further, we investigate the effect of the application of active learning and we evaluate the impact of different text representation techniques, namely TF-IDF, Word2Vec, Universal Sentence Encoder and XLNet on the model's performance. The evaluation shows that a combination of text embeddings generated through the Universal Sentence Encoder and MLP as classifier outperforms all other methods, and is well suited to predict the risk and complexity of bug tickets.

</details>

<details>

<summary>2021-09-28 17:55:44 - The Role of Communication Technology Across the Life Course: A Field Guide to Social Support in East York</summary>

- *Anabel Quan-Haase, Molly-Gloria Harper, Barry Wellmnan*

- `2109.13907v1` - [abs](http://arxiv.org/abs/2109.13907v1) - [pdf](http://arxiv.org/pdf/2109.13907v1)

> We examine how Canadians living in the East York section of Toronto exchange social support. Just as we have had to deconstruct social support to understand its component parts, we now deconstruct how different types of communication technologies play socially supportive roles. We draw on 101 in-depth interviews conducted in 2013-2014 to shed light on the support networks of a sample of East York residents and discern the role of communication technologies in the exchange of different types of social support across age groups. Our findings show that not much has changed since the 1960s in terms of the social ties that our sample of East Yorkers have, and the types of support mobilized via social networks: companionship, small and large services, emotional aid, and financial support. What has changed is how communication technologies interweave in complex ways with different types of social ties (partners, siblings, friends, etc.) to mobilize social support. We found that with siblings and extended kin communication technologies could boost the frequency of interaction and help exchange support at a distance. With friendship ties, communication technologies provide a continuous, constant flow of interaction. We draw implications for theories of social support and for social policy linked to interventions aimed at helping vulnerable groups during the COVID-19 pandemic.

</details>

<details>

<summary>2021-09-28 20:16:06 - Itsy Bitsy SpiderNet: Fully Connected Residual Network for Fraud Detection</summary>

- *Sergey Afanasiev, Anastasiya Smirnova, Diana Kotereva*

- `2105.08120v2` - [abs](http://arxiv.org/abs/2105.08120v2) - [pdf](http://arxiv.org/pdf/2105.08120v2)

> With the development of high technology, the scope of fraud is increasing, resulting in annual losses of billions of dollars worldwide. The preventive protection measures become obsolete and vulnerable over time, so effective detective tools are needed. In this paper, we propose a convolutional neural network architecture SpiderNet designed to solve fraud detection problems. We noticed that the principles of pooling and convolutional layers in neural networks are very similar to the way antifraud analysts work when conducting investigations. Moreover, the skip-connections used in neural networks make the usage of features of various power in antifraud models possible. Our experiments have shown that SpiderNet provides better quality compared to Random Forest and adapted for antifraud modeling problems 1D-CNN, 1D-DenseNet, F-DenseNet neural networks. We also propose new approaches for fraud feature engineering called B-tests and W-tests, which generalize the concepts of Benford's Law for fraud anomalies detection. Our results showed that B-tests and W-tests give a significant increase to the quality of our anti-fraud models. The SpiderNet code is available at https://github.com/aasmirnova24/SpiderNet

</details>

<details>

<summary>2021-09-29 11:08:37 - Smart-home anomaly detection using combination of in-home situation and user behavior</summary>

- *Masaaki Yamauchi, Masahiro Tanaka, Yuichi Ohsita, Masayuki Murata, Kensuke Ueda, Yoshiaki Kato*

- `2109.14348v1` - [abs](http://arxiv.org/abs/2109.14348v1) - [pdf](http://arxiv.org/pdf/2109.14348v1)

> Internet-of-things (IoT) devices are vulnerable to malicious operations by attackers, which can cause physical and economic harm to users; therefore, we previously proposed a sequence-based method that modeled user behavior as sequences of in-home events and a base home state to detect anomalous operations. However, that method modeled users' home states based on the time of day; hence, attackers could exploit the system to maximize attack opportunities. Therefore, we then proposed an estimation-based detection method that estimated the home state using not only the time of day but also the observable values of home IoT sensors and devices. However, it ignored short-term operational behaviors. Consequently, in the present work, we propose a behavior-modeling method that combines home state estimation and event sequences of IoT devices within the home to enable a detailed understanding of long- and short-term user behavior. We compared the proposed model to our previous methods using data collected from real homes. Compared with the estimation-based method, the proposed method achieved a 15.4% higher detection ratio with fewer than 10% misdetections. Compared with the sequence-based method, the proposed method achieved a 46.0% higher detection ratio with fewer than 10% misdetections.

</details>

<details>

<summary>2021-09-29 16:14:31 - Improving DRAM Performance, Security, and Reliability by Understanding and Exploiting DRAM Timing Parameter Margins</summary>

- *Jeremie S. Kim*

- `2109.14520v1` - [abs](http://arxiv.org/abs/2109.14520v1) - [pdf](http://arxiv.org/pdf/2109.14520v1)

> This dissertation rigorously characterizes many modern commodity DRAM devices and shows that by exploiting DRAM access timing margins within manufacturer-recommended DRAM timing specifications, we can significantly improve system performance, reduce power consumption, and improve device reliability and security. First, we characterize DRAM timing parameter margins and find that certain regions of DRAM can be accessed faster than other regions due to DRAM cell process manufacturing variation. We exploit this by enabling variable access times depending on the DRAM cells being accessed, which not only improves overall system performance, but also decreases power consumption. Second, we find that we can uniquely identify DRAM devices by the locations of failures that result when we access DRAM with timing parameters reduced below specification values. Because we induce these failures with DRAM accesses, we can generate these unique identifiers significantly more quickly than prior work. Third, we propose a random number generator that is based on our observation that timing failures in certain DRAM cells are randomly induced and can thus be repeatedly polled to very quickly generate true random values. Finally, we characterize the RowHammer security vulnerability on a wide range of modern DRAM chips while violating the DRAM refresh requirement in order to directly characterize the underlying DRAM technology without the interference of refresh commands. We demonstrate with our characterization of real chips, that existing RowHammer mitigation mechanisms either are not scalable or suffer from prohibitively large performance overheads in projected future devices and it is critical to research more effective solutions to RowHammer. Overall, our studies build a new understanding of modern DRAM devices to improve computing system performance, reliability and security all at the same time.

</details>

<details>

<summary>2021-09-29 19:29:10 - Mitigation of Adversarial Policy Imitation via Constrained Randomization of Policy (CRoP)</summary>

- *Nancirose Piazza, Vahid Behzadan*

- `2109.14678v1` - [abs](http://arxiv.org/abs/2109.14678v1) - [pdf](http://arxiv.org/pdf/2109.14678v1)

> Deep reinforcement learning (DRL) policies are vulnerable to unauthorized replication attacks, where an adversary exploits imitation learning to reproduce target policies from observed behavior. In this paper, we propose Constrained Randomization of Policy (CRoP) as a mitigation technique against such attacks. CRoP induces the execution of sub-optimal actions at random under performance loss constraints. We present a parametric analysis of CRoP, address the optimality of CRoP, and establish theoretical bounds on the adversarial budget and the expectation of loss. Furthermore, we report the experimental evaluation of CRoP in Atari environments under adversarial imitation, which demonstrate the efficacy and feasibility of our proposed method against policy replication attacks.

</details>

<details>

<summary>2021-09-30 15:00:20 - Using a Cyber Digital Twin for Continuous Automotive Security Requirements Verification</summary>

- *Ana Cristina Franco da Silva, Stefan Wagner, Eddie Lazebnik, Eyal Traitel*

- `2102.00790v2` - [abs](http://arxiv.org/abs/2102.00790v2) - [pdf](http://arxiv.org/pdf/2102.00790v2)

> A Digital Twin (DT) is a digital representation of a physical object used to simulate it before it is built or to predict failures after the object is deployed. In this article, we introduce our approach, which applies the concept of a Cyber Digital Twin (CDT) to automotive software for the purpose of security analysis. In our approach, automotive firmware is transformed into a CDT, which contains automatically extracted, security-relevant information from the firmware. Based on the CDT, we evaluate security requirements through automated analysis and requirements verification using policy enforcement checks and vulnerabilities detection. The evaluation of a CDT is conducted continuously integrating new checks derived from new security requirements and from newly disclosed vulnerabilities. We applied our approach to about 100 automotive firmwares. In average, about 600 publicly disclosed vulnerabilities and 80 unknown weaknesses were detected per firmware in the pre-production phase. Therefore, the use of a CDT enables efficient continuous verification of security requirements.

</details>

<details>

<summary>2021-09-30 15:34:28 - A formal model for ledger management systems based on contracts and temporal logic</summary>

- *Paolo Bottoni, Anna Labella, Remo Pareschi*

- `2109.15212v1` - [abs](http://arxiv.org/abs/2109.15212v1) - [pdf](http://arxiv.org/pdf/2109.15212v1)

> A key component of blockchain technology is the ledger, viz., a database that, unlike standard databases, keeps in memory the complete history of past transactions as in a notarial archive for the benefit of any future test. In second-generation blockchains such as Ethereum the ledger is coupled with smart contracts, which enable the automation of transactions associated with agreements between the parties of a financial or commercial nature. The coupling of smart contracts and ledgers provides the technological background for very innovative application areas, such as Decentralized Autonomous Organizations (DAOs), Initial Coin Offerings (ICOs) and Decentralized Finance (DeFi), which propelled blockchains beyond cryptocurrencies that were the only focus of first generation blockchains such as the Bitcoin. However, the currently used implementation of smart contracts as arbitrary programming constructs has made them susceptible to dangerous bugs that can be exploited maliciously and has moved their semantics away from that of legal contracts. We propose here to recompose the split and recover the reliability of databases by formalizing a notion of contract modelled as a finite-state automaton with well-defined computational characteristics derived from an encoding in terms of allocations of resources to actors, as an alternative to the approach based on programming. To complete the work, we use temporal logic as the basis for an abstract query language that is effectively suited to the historical nature of the information kept in the ledger.

</details>

<details>

<summary>2021-09-30 21:30:16 - RFID Exploitation and Countermeasures</summary>

- *Luciano Gavoni*

- `2110.00094v1` - [abs](http://arxiv.org/abs/2110.00094v1) - [pdf](http://arxiv.org/pdf/2110.00094v1)

> Radio Frequency Identification (RFID) systems are among the most widespread computing technologies with technical potential and profitable opportunities in numerous applications worldwide. Further, RFID is the core technology behind the Internet of Things (IoT), which can accomplish the real-time transmission of information between objects without manual operation. However, RFID security has been taken for granted for several years, causing multiple vulnerabilities that can even damage human functionalities. The latest ISO/IEC 18000-63:2015 standard concerning RFID dates to 2015, and much freedom has been given to manufacturers responsible for making their devices secure. The lack of a substantial standard for devices that implement RFID technology creates many vulnerabilities that expose end-users to elevated risk. Hence, this paper gives the reader a clear overview of the technology, and it analyzes 23 well-known RFID attacks such as Reverse Engineering, Buffer Overflow, Eavesdropping, and Malware. Moreover, given the exceptional capabilities and utilities of RFID devices, this paper has focused on security measures and defenses for protecting them, such as Active Jamming, Shielding tag, and Authentication.

</details>


## 2021-10

<details>

<summary>2021-10-01 03:02:58 - Predicting COVID-19 Patient Shielding: A Comprehensive Study</summary>

- *Vithya Yogarajan, Jacob Montiel, Tony Smith, Bernhard Pfahringer*

- `2110.00183v1` - [abs](http://arxiv.org/abs/2110.00183v1) - [pdf](http://arxiv.org/pdf/2110.00183v1)

> There are many ways machine learning and big data analytics are used in the fight against the COVID-19 pandemic, including predictions, risk management, diagnostics, and prevention. This study focuses on predicting COVID-19 patient shielding -- identifying and protecting patients who are clinically extremely vulnerable from coronavirus. This study focuses on techniques used for the multi-label classification of medical text. Using the information published by the United Kingdom NHS and the World Health Organisation, we present a novel approach to predicting COVID-19 patient shielding as a multi-label classification problem. We use publicly available, de-identified ICU medical text data for our experiments. The labels are derived from the published COVID-19 patient shielding data. We present an extensive comparison across 12 multi-label classifiers from the simple binary relevance to neural networks and the most recent transformers. To the best of our knowledge this is the first comprehensive study, where such a range of multi-label classifiers for medical text are considered. We highlight the benefits of various approaches, and argue that, for the task at hand, both predictive accuracy and processing time are essential.

</details>

<details>

<summary>2021-10-01 12:40:27 - A Modular End-to-End Framework for Secure Firmware Updates on Embedded Systems</summary>

- *Solon Falas, Charalambos Konstantinou, Maria K. Michael*

- `2007.09071v4` - [abs](http://arxiv.org/abs/2007.09071v4) - [pdf](http://arxiv.org/pdf/2007.09071v4)

> Firmware refers to device read-only resident code which includes microcode and macro-instruction -level routines. For Internet-of-Things (IoT) devices without an operating system, firmware includes all the necessary instructions on how such embedded systems operate and communicate. Thus, firmware updates are an essential part of device functionality. They provide the ability to patch vulnerabilities, address operational issues, and improve device reliability and performance during the lifetime of the system. This process, however, is often exploited by attackers in order to inject malicious firmware code into the embedded device. In this paper, we present a framework for secure firmware updates on embedded systems. The approach is based on hardware primitives and cryptographic modules, and it can be deployed in environments where communication channels might be insecure. The implementation of the framework is flexible as it can be adapted in regards to the IoT device's available hardware resources and constraints. Our security analysis shows that our framework is resilient to a variety of attack vectors. The experimental setup demonstrates the feasibility of the approach. By implementing a variety of test cases on FPGA, we demonstrate the adaptability and performance of the framework. Experiments indicate that the update procedure for a 1183kB firmware image could be achieved, in a secure manner, under 1.73 seconds.

</details>

<details>

<summary>2021-10-01 15:03:03 - New Evolutionary Computation Models and their Applications to Machine Learning</summary>

- *Mihai Oltean*

- `2110.00468v1` - [abs](http://arxiv.org/abs/2110.00468v1) - [pdf](http://arxiv.org/pdf/2110.00468v1)

> Automatic Programming is one of the most important areas of computer science research today. Hardware speed and capability have increased exponentially, but the software is years behind. The demand for software has also increased significantly, but it is still written in old fashion: by using humans.   There are multiple problems when the work is done by humans: cost, time, quality. It is costly to pay humans, it is hard to keep them satisfied for a long time, it takes a lot of time to teach and train them and the quality of their output is in most cases low (in software, mostly due to bugs).   The real advances in human civilization appeared during the industrial revolutions. Before the first revolution, most people worked in agriculture. Today, very few percent of people work in this field.   A similar revolution must appear in the computer programming field. Otherwise, we will have so many people working in this field as we had in the past working in agriculture.   How do people know how to write computer programs? Very simple: by learning. Can we do the same for software? Can we put the software to learn how to write software?   It seems that is possible (to some degree) and the term is called Machine Learning. It was first coined in 1959 by the first person who made a computer perform a serious learning task, namely, Arthur Samuel.   However, things are not so easy as in humans (well, truth to be said - for some humans it is impossible to learn how to write software). So far we do not have software that can learn perfectly to write software. We have some particular cases where some programs do better than humans, but the examples are sporadic at best. Learning from experience is difficult for computer programs. Instead of trying to simulate how humans teach humans how to write computer programs, we can simulate nature.

</details>

<details>

<summary>2021-10-02 02:11:22 - Universal Adversarial Spoofing Attacks against Face Recognition</summary>

- *Takuma Amada, Seng Pei Liew, Kazuya Kakizaki, Toshinori Araki*

- `2110.00708v1` - [abs](http://arxiv.org/abs/2110.00708v1) - [pdf](http://arxiv.org/pdf/2110.00708v1)

> We assess the vulnerabilities of deep face recognition systems for images that falsify/spoof multiple identities simultaneously. We demonstrate that, by manipulating the deep feature representation extracted from a face image via imperceptibly small perturbations added at the pixel level using our proposed Universal Adversarial Spoofing Examples (UAXs), one can fool a face verification system into recognizing that the face image belongs to multiple different identities with a high success rate. One characteristic of the UAXs crafted with our method is that they are universal (identity-agnostic); they are successful even against identities not known in advance. For a certain deep neural network, we show that we are able to spoof almost all tested identities (99\%), including those not known beforehand (not included in training). Our results indicate that a multiple-identity attack is a real threat and should be taken into account when deploying face recognition systems.

</details>

<details>

<summary>2021-10-02 11:10:50 - Recommending Code Understandability Improvements based on Code Reviews</summary>

- *Delano Oliveira*

- `2110.00782v1` - [abs](http://arxiv.org/abs/2110.00782v1) - [pdf](http://arxiv.org/pdf/2110.00782v1)

> Developers spend 70% of their time understanding code. Code that is easy to read can save time, while hard-to-read code can lead to the introduction of bugs. However, it is difficult to establish what makes code more understandable. Although there are guides and directives on improving code understandability, in some contexts, these practices can have a detrimental effect. Practical software development projects often employ code review to improve code quality, including understandability. Reviewers are often senior developers who have contributed extensively to projects and have an in-depth understanding of the impacts of different solutions on code understandability. This paper is an early research proposal to recommend code understandability improvements based on code reviewer knowledge. The core of the proposal comprises a dataset of code understandability improvements extracted from code reviews. This dataset will serve as a basis to train machine learning systems to recommend understandability improvements.

</details>

<details>

<summary>2021-10-03 00:15:50 - Evaluating Deep Learning Models and Adversarial Attacks on Accelerometer-Based Gesture Authentication</summary>

- *Elliu Huang, Fabio Di Troia, Mark Stamp*

- `2110.14597v1` - [abs](http://arxiv.org/abs/2110.14597v1) - [pdf](http://arxiv.org/pdf/2110.14597v1)

> Gesture-based authentication has emerged as a non-intrusive, effective means of authenticating users on mobile devices. Typically, such authentication techniques have relied on classical machine learning techniques, but recently, deep learning techniques have been applied this problem. Although prior research has shown that deep learning models are vulnerable to adversarial attacks, relatively little research has been done in the adversarial domain for behavioral biometrics. In this research, we collect tri-axial accelerometer gesture data (TAGD) from 46 users and perform classification experiments with both classical machine learning and deep learning models. Specifically, we train and test support vector machines (SVM) and convolutional neural networks (CNN). We then consider a realistic adversarial attack, where we assume the attacker has access to real users' TAGD data, but not the authentication model. We use a deep convolutional generative adversarial network (DC-GAN) to create adversarial samples, and we show that our deep learning model is surprisingly robust to such an attack scenario.

</details>

<details>

<summary>2021-10-03 12:01:28 - Adversarial Attacks on Time-Series Intrusion Detection for Industrial Control Systems</summary>

- *Giulio Zizzo, Chris Hankin, Sergio Maffeis, Kevin Jones*

- `1911.04278v2` - [abs](http://arxiv.org/abs/1911.04278v2) - [pdf](http://arxiv.org/pdf/1911.04278v2)

> Neural networks are increasingly used for intrusion detection on industrial control systems (ICS). With neural networks being vulnerable to adversarial examples, attackers who wish to cause damage to an ICS can attempt to hide their attacks from detection by using adversarial example techniques. In this work we address the domain specific challenges of constructing such attacks against autoregressive based intrusion detection systems (IDS) in an ICS setting.   We model an attacker that can compromise a subset of sensors in a ICS which has a LSTM based IDS. The attacker manipulates the data sent to the IDS, and seeks to hide the presence of real cyber-physical attacks occurring in the ICS.   We evaluate our adversarial attack methodology on the Secure Water Treatment system when examining solely continuous data, and on data containing a mixture of discrete and continuous variables. In the continuous data domain our attack successfully hides the cyber-physical attacks requiring 2.87 out of 12 monitored sensors to be compromised on average. With both discrete and continuous data our attack required, on average, 3.74 out of 26 monitored sensors to be compromised.

</details>

<details>

<summary>2021-10-04 05:29:34 - Empirical Understanding of Deletion Privacy: Experiences, Expectations, and Measures</summary>

- *Mohsen Minaei, Mainack Mondal, Aniket Kate*

- `2008.11317v2` - [abs](http://arxiv.org/abs/2008.11317v2) - [pdf](http://arxiv.org/pdf/2008.11317v2)

> Social platforms are heavily used by individuals to share their thoughts and personal information. However, due to regret over time about posting inappropriate social content, embarrassment, or even life or relationship changes, some past posts might also pose serious privacy concerns for them. To cope with these privacy concerns, social platforms offer deletion mechanisms that allow users to remove their contents. Quite naturally, these deletion mechanisms are really useful for removing past posts as and when needed. However, these same mechanisms also leave the users potentially vulnerable to attacks by adversaries who specifically seek the users' damaging content and exploit the act of deletion as a strong signal for identifying such content. Unfortunately, today user experiences and contextual expectations regarding such attacks on deletion privacy and deletion privacy in general are not well understood.   To that end, in this paper, we conduct a user survey-based exploration involving 191 participants to unpack their prior deletion experiences, their expectations of deletion privacy, and how effective they find the current deletion mechanisms. We find that more than 80% of the users have deleted at least a social media post, and users self-reported that, on average, around 35% of their deletions happened after a week of posting. While the participants identified the irrelevancy (due to time passing) as the main reason for content removal, most of them believed that deletions indicate that the deleted content includes some damaging information to the owner. Importantly, the participants are significantly more concerned about their deletions being noticed by large-scale data collectors (e.g., the government) than individuals from their social circle. Finally, the participants felt that popular deletion mechanisms are not very effective in protecting the privacy of those deletions.

</details>

<details>

<summary>2021-10-04 11:33:51 - Identifying non-natural language artifacts in bug reports</summary>

- *Thomas Hirsch, Birgit Hofer*

- `2110.01336v1` - [abs](http://arxiv.org/abs/2110.01336v1) - [pdf](http://arxiv.org/pdf/2110.01336v1)

> Bug reports are a popular target for natural language processing (NLP). However, bug reports often contain artifacts such as code snippets, log outputs and stack traces. These artifacts not only inflate the bug reports with noise, but often constitute a real problem for the NLP approach at hand and have to be removed. In this paper, we present a machine learning based approach to classify content into natural language and artifacts at line level implemented in Python. We show how data from GitHub issue trackers can be used for automated training set generation, and present a custom preprocessing approach for bug reports. Our model scores at 0.95 ROC-AUC and 0.93 F1 against our manually annotated validation set, and classifies 10k lines in 0.72 seconds. We cross evaluated our model against a foreign dataset and a foreign R model for the same task. The Python implementation of our model and our datasets are made publicly available under an open source license.

</details>

<details>

<summary>2021-10-04 18:59:32 - BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements</summary>

- *Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, Yang Zhang*

- `2006.01043v2` - [abs](http://arxiv.org/abs/2006.01043v2) - [pdf](http://arxiv.org/pdf/2006.01043v2)

> Deep neural networks (DNNs) have progressed rapidly during the past decade and have been deployed in various real-world applications. Meanwhile, DNN models have been shown to be vulnerable to security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. Specifically, the adversary poisons the target model's training set to mislead any input with an added secret trigger to a target class.   Previous backdoor attacks predominantly focus on computer vision (CV) applications, such as image classification. In this paper, we perform a systematic investigation of backdoor attack on NLP models, and propose BadNL, a general NLP backdoor attack framework including novel attack methods. Specifically, we propose three methods to construct triggers, namely BadChar, BadWord, and BadSentence, including basic and semantic-preserving variants. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model's utility. For instance, using the BadChar, our backdoor attack achieves a 98.9% attack success rate with yielding a utility improvement of 1.5% on the SST-5 dataset when only poisoning 3% of the original set. Moreover, we conduct a user study to prove that our triggers can well preserve the semantics from humans perspective.

</details>

<details>

<summary>2021-10-05 05:44:21 - Fully Automated Functional Fuzzing of Android Apps for Detecting Non-crashing Logic Bugs</summary>

- *Ting Su, Yichen Yan, Jue Wang, Jingling Sun, Yiheng Xiong, Geguang Pu, Ke Wang, Zhendong Su*

- `2008.03585v2` - [abs](http://arxiv.org/abs/2008.03585v2) - [pdf](http://arxiv.org/pdf/2008.03585v2)

> Android apps are GUI-based event-driven software and have become ubiquitous in recent years. Obviously, functional correctness is critical for an app's success. However, in addition to crash bugs, non-crashing functional bugs (in short as "non-crashing bugs" in this work) like inadvertent function failures, silent user data lost and incorrect display information are prevalent, even in popular, well-tested apps. These non-crashing functional bugs are usually caused by program logic errors and manifest themselves on the graphic user interfaces (GUIs). In practice, such bugs pose significant challenges in effectively detecting them because (1) current practices heavily rely on expensive, small-scale manual validation (the lack of automation); and (2) modern fully automated testing has been limited to crash bugs (the lack of test oracles). This paper fills this gap by introducing independent view fuzzing, a novel, fully automated approach for detecting non-crashing functional bugs in Android apps. Inspired by metamorphic testing, our key insight is to leverage the commonly-held independent view property of Android apps to manufacture property-preserving mutant tests from a set of seed tests that validate certain app properties. The mutated tests help exercise the tested apps under additional, adverse conditions. Any property violations indicate likely functional bugs for further manual confirmation. We have realized our approach as an automated, end-to-end functional fuzzing tool, Genie. Given an app, (1) Genie automatically detects non-crashing bugs without requiring human-provided tests and oracles (thus fully automated); and (2) the detected non-crashing bugs are diverse (thus general and not limited to specific functional properties), which set Genie apart from prior work.

</details>

<details>

<summary>2021-10-05 05:46:37 - Securing Federated Learning: A Covert Communication-based Approach</summary>

- *Yuan-Ai Xie, Jiawen Kang, Dusit Niyato, Nguyen Thi Thanh Van, Nguyen Cong Luong, Zhixin Liu, Han Yu*

- `2110.02221v1` - [abs](http://arxiv.org/abs/2110.02221v1) - [pdf](http://arxiv.org/pdf/2110.02221v1)

> Federated Learning Networks (FLNs) have been envisaged as a promising paradigm to collaboratively train models among mobile devices without exposing their local privacy data. Due to the need for frequent model updates and communications, FLNs are vulnerable to various attacks (e.g., eavesdropping attacks, inference attacks, poisoning attacks, and backdoor attacks). Balancing privacy protection with efficient distributed model training is a key challenge for FLNs. Existing countermeasures incur high computation costs and are only designed for specific attacks on FLNs. In this paper, we bridge this gap by proposing the Covert Communication-based Federated Learning (CCFL) approach. Based on the emerging communication security technique of covert communication which hides the existence of wireless communication activities, CCFL can degrade attackers' capability of extracting useful information from the FLN training protocol, which is a fundamental step for most existing attacks, and thereby holistically enhances the privacy of FLNs. We experimentally evaluate CCFL extensively under real-world settings in which the FL latency is optimized under given security requirements. Numerical results demonstrate the significant effectiveness of the proposed approach in terms of both training efficiency and communication security.

</details>

<details>

<summary>2021-10-05 08:20:37 - Recent advances for quantum classifiers</summary>

- *Weikang Li, Dong-Ling Deng*

- `2108.13421v2` - [abs](http://arxiv.org/abs/2108.13421v2) - [pdf](http://arxiv.org/pdf/2108.13421v2)

> Machine learning has achieved dramatic success in a broad spectrum of applications. Its interplay with quantum physics may lead to unprecedented perspectives for both fundamental research and commercial applications, giving rise to an emergent research frontier of quantum machine learning. Along this line, quantum classifiers, which are quantum devices that aim to solve classification problems in machine learning, have attracted tremendous attention recently. In this review, we give a relatively comprehensive overview for the studies of quantum classifiers, with a focus on recent advances. First, we will review a number of quantum classification algorithms, including quantum support vector machines, quantum kernel methods, quantum decision tree classifiers, quantum nearest neighbor algorithms, and quantum annealing based classifiers. Then, we move on to introduce the variational quantum classifiers, which are essentially variational quantum circuits for classifications. We will review different architectures for constructing variational quantum classifiers and introduce the barren plateau problem, where the training of quantum classifiers might be hindered by the exponentially vanishing gradient. In addition, the vulnerability aspect of quantum classifiers in the setting of adversarial learning and the recent experimental progress on different quantum classifiers will also be discussed.

</details>

<details>

<summary>2021-10-05 09:45:30 - Poisoned classifiers are not only backdoored, they are fundamentally broken</summary>

- *Mingjie Sun, Siddhant Agarwal, J. Zico Kolter*

- `2010.09080v2` - [abs](http://arxiv.org/abs/2010.09080v2) - [pdf](http://arxiv.org/pdf/2010.09080v2)

> Under a commonly-studied backdoor poisoning attack against classification models, an attacker adds a small trigger to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is incorrect. We describe a new threat model for poisoned classifier, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, we propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of smoothed adversarial images with human interaction. We demonstrate the effectiveness of our attack through extensive experiments on high-resolution datasets: ImageNet and TrojAI. We also compare our approach to previous work on modeling trigger distributions and find that our method are more scalable and efficient in generating effective triggers. Last, we include a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Thus, we argue that there is no such thing as a secret backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier.

</details>

<details>

<summary>2021-10-05 11:36:25 - Distribution Mismatch Correction for Improved Robustness in Deep Neural Networks</summary>

- *Alexander Fuchs, Christian Knoll, Franz Pernkopf*

- `2110.01955v1` - [abs](http://arxiv.org/abs/2110.01955v1) - [pdf](http://arxiv.org/pdf/2110.01955v1)

> Deep neural networks rely heavily on normalization methods to improve their performance and learning behavior. Although normalization methods spurred the development of increasingly deep and efficient architectures, they also increase the vulnerability with respect to noise and input corruptions. In most applications, however, noise is ubiquitous and diverse; this can often lead to complete failure of machine learning systems as they fail to cope with mismatches between the input distribution during training- and test-time. The most common normalization method, batch normalization, reduces the distribution shift during training but is agnostic to changes in the input distribution during test time. This makes batch normalization prone to performance degradation whenever noise is present during test-time. Sample-based normalization methods can correct linear transformations of the activation distribution but cannot mitigate changes in the distribution shape; this makes the network vulnerable to distribution changes that cannot be reflected in the normalization parameters. We propose an unsupervised non-parametric distribution correction method that adapts the activation distribution of each layer. This reduces the mismatch between the training and test-time distribution by minimizing the 1-D Wasserstein distance. In our experiments, we empirically show that the proposed method effectively reduces the impact of intense image corruptions and thus improves the classification performance without the need for retraining or fine-tuning the model.

</details>

<details>

<summary>2021-10-05 13:42:45 - An Approach of Replicating Multi-Staged Cyber-Attacks and Countermeasures in a Smart Grid Co-Simulation Environment</summary>

- *Ömer Sen, Dennis van der Velde, Sebastian N. Peters, Martin Henze*

- `2110.02040v1` - [abs](http://arxiv.org/abs/2110.02040v1) - [pdf](http://arxiv.org/pdf/2110.02040v1)

> While the digitization of power distribution grids brings many benefits, it also introduces new vulnerabilities for cyber-attacks. To maintain secure operations in the emerging threat landscape, detecting and implementing countermeasures against cyber-attacks are paramount. However, due to the lack of publicly available attack data against Smart Grids (SGs) for countermeasure development, simulation-based data generation approaches offer the potential to provide the needed data foundation. Therefore, our proposed approach provides flexible and scalable replication of multi-staged cyber-attacks in an SG Co-Simulation Environment (COSE). The COSE consists of an energy grid simulator, simulators for Operation Technology (OT) devices, and a network emulator for realistic IT process networks. Focusing on defensive and offensive use cases in COSE, our simulated attacker can perform network scans, find vulnerabilities, exploit them, gain administrative privileges, and execute malicious commands on OT devices. As an exemplary countermeasure, we present a built-in Intrusion Detection System (IDS) that analyzes generated network traffic using anomaly detection with Machine Learning (ML) approaches. In this work, we provide an overview of the SG COSE, present a multi-stage attack model with the potential to disrupt grid operations, and show exemplary performance evaluations of the IDS in specific scenarios.

</details>

<details>

<summary>2021-10-05 15:49:18 - A Survey on Security and Privacy Issues of UAVs</summary>

- *Yassine Mekdad, Ahmet Aris, Leonardo Babun, Abdeslam EL Fergougui, Mauro Conti, Riccardo Lazzeretti, A. Selcuk Uluagac*

- `2109.14442v2` - [abs](http://arxiv.org/abs/2109.14442v2) - [pdf](http://arxiv.org/pdf/2109.14442v2)

> In the 21st century, the industry of drones, also known as Unmanned Aerial Vehicles (UAVs), has witnessed a rapid increase with its large number of airspace users. The tremendous benefits of this technology in civilian applications such as hostage rescue and parcel delivery will integrate smart cities in the future. Nowadays, the affordability of commercial drones expands its usage at a large scale. However, the development of drone technology is associated with vulnerabilities and threats due to the lack of efficient security implementations. Moreover, the complexity of UAVs in software and hardware triggers potential security and privacy issues. Thus, posing significant challenges for the industry, academia, and governments. In this paper, we extensively survey the security and privacy issues of UAVs by providing a systematic classification at four levels: Hardware-level, Software-level, Communication-level, and Sensor-level. In particular, for each level, we thoroughly investigate (1) common vulnerabilities affecting UAVs for potential attacks from malicious actors, (2) existing threats that are jeopardizing the civilian application of UAVs, (3) active and passive attacks performed by the adversaries to compromise the security and privacy of UAVs, (4) possible countermeasures and mitigation techniques to protect UAVs from such malicious activities. In addition, we summarize the takeaways that highlight lessons learned about UAVs' security and privacy issues. Finally, we conclude our survey by presenting the critical pitfalls and suggesting promising future research directions for security and privacy of UAVs.

</details>

<details>

<summary>2021-10-05 18:08:55 - Multi-Modal Attack Detection for Cyber-Physical Additive Manufacturing</summary>

- *Shih-Yuan Yu, Arnav Vaibhav Malawade, Mohammad Abdullah Al Faruque*

- `2110.02259v1` - [abs](http://arxiv.org/abs/2110.02259v1) - [pdf](http://arxiv.org/pdf/2110.02259v1)

> Cyber-Physical Additive Manufacturing (AM) constructs a physical 3D object layer-by-layer according to its digital representation and has been vastly applied to fast prototyping and the manufacturing of functional end-products across fields. The computerization of traditional production processes propels these technological advancements; however, this also introduces new vulnerabilities, necessitating the study of cyberattacks on these systems. The AM Sabotage Attack is one kind of kinetic cyberattack that originates from the cyber domain and can eventually lead to physical damage, injury, or even death. By introducing inconspicuous yet damaging alterations in any specific process of the AM digital process chain, the attackers can compromise the structural integrity of a manufactured component in a manner that is invisible to a human observer. If the manufactured objects are critical for their system, those attacks can even compromise the whole system's structural integrity and pose a severe safety risk to its users. For example, an inconspicuous void (less than 1 mm in dimension) placed in the 3D design of a tensile test specimen can reduce its yield load by 14%. However, security studies primarily focus on securing digital assets, overlooking the fact that AM systems are CPSs.

</details>

<details>

<summary>2021-10-05 23:43:45 - Online Advertising Security: Issues, Taxonomy, and Future Directions</summary>

- *Zahra Pooranian, Mauro Conti, Hamed Haddadi, Rahim Tafazolli*

- `2006.03986v4` - [abs](http://arxiv.org/abs/2006.03986v4) - [pdf](http://arxiv.org/pdf/2006.03986v4)

> Online advertising has become the backbone of the Internet economy by revolutionizing business marketing. It provides a simple and efficient way for advertisers to display their advertisements to specific individual users, and over the last couple of years has contributed to an explosion in the income stream for several web-based businesses. For example, Google's income from advertising grew 51.6% between 2016 and 2018, to $136.8 billion. This exponential growth in advertising revenue has motivated fraudsters to exploit the weaknesses of the online advertising model to make money, and researchers to discover new security vulnerabilities in the model, to propose countermeasures and to forecast future trends in research. Motivated by these considerations, this paper presents a comprehensive review of the security threats to online advertising systems. We begin by introducing the motivation for online advertising system, explain how it differs from traditional advertising networks, introduce terminology, and define the current online advertising architecture. We then devise a comprehensive taxonomy of attacks on online advertising to raise awareness among researchers about the vulnerabilities of online advertising ecosystem. We discuss the limitations and effectiveness of the countermeasures that have been developed to secure entities in the advertising ecosystem against these attacks. To complete our work, we identify some open issues and outline some possible directions for future research towards improving security methods for online advertising systems.

</details>

<details>

<summary>2021-10-06 02:48:58 - BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models</summary>

- *Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan*

- `2110.02467v1` - [abs](http://arxiv.org/abs/2110.02467v1) - [pdf](http://arxiv.org/pdf/2110.02467v1)

> Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.

</details>

<details>

<summary>2021-10-06 06:18:28 - Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution</summary>

- *Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, Kai-Wei Chang, Cho-Jui Hsieh*

- `2108.12777v2` - [abs](http://arxiv.org/abs/2108.12777v2) - [pdf](http://arxiv.org/pdf/2108.12777v2)

> Recent studies have shown that deep neural networks are vulnerable to intentionally crafted adversarial examples, and various methods have been proposed to defend against adversarial word-substitution attacks for neural NLP models. However, there is a lack of systematic study on comparing different defense approaches under the same attacking setting. In this paper, we seek to fill the gap of systematic studies through comprehensive researches on understanding the behavior of neural text classifiers trained by various defense methods under representative adversarial attacks. In addition, we propose an effective method to further improve the robustness of neural text classifiers against such attacks and achieved the highest accuracy on both clean and adversarial examples on AGNEWS and IMDB datasets by a significant margin.

</details>

<details>

<summary>2021-10-06 06:43:35 - Adversarial Visual Robustness by Causal Intervention</summary>

- *Kaihua Tang, Mingyuan Tao, Hanwang Zhang*

- `2106.09534v2` - [abs](http://arxiv.org/abs/2106.09534v2) - [pdf](http://arxiv.org/pdf/2106.09534v2)

> Adversarial training is the de facto most promising defense against adversarial examples. Yet, its passive nature inevitably prevents it from being immune to unknown attackers. To achieve a proactive defense, we need a more fundamental understanding of adversarial examples, beyond the popular bounded threat model. In this paper, we provide a causal viewpoint of adversarial vulnerability: the cause is the spurious correlation ubiquitously existing in learning, i.e., the confounding effect, where attackers are precisely exploiting these effects. Therefore, a fundamental solution for adversarial robustness is by causal intervention. As these visual confounders are imperceptible in general, we propose to use the instrumental variable that achieves causal intervention without the need for confounder observation. We term our robust training method as Causal intervention by instrumental Variable (CiiV). It's a causal regularization that 1) augments the image with multiple retinotopic centers and 2) encourages the model to learn causal features, rather than local confounding patterns, by favoring features linearly responding to spatial interpolations. Extensive experiments on a wide spectrum of attackers and settings applied in CIFAR-10, CIFAR-100, and mini-ImageNet demonstrate that CiiV is robust to adaptive attacks, including the recent AutoAttack. Besides, as a general causal regularization, it can be easily plugged into other methods to further boost the robustness.

</details>

<details>

<summary>2021-10-06 07:09:40 - Impact of GPU uncertainty on the training of predictive deep neural networks</summary>

- *Maciej Pietrowski, Andrzej Gajda, Takuto Yamamoto, Taisuke Kobayashi, Lana Sinapayen, Eiji Watanabe*

- `2109.01451v4` - [abs](http://arxiv.org/abs/2109.01451v4) - [pdf](http://arxiv.org/pdf/2109.01451v4)

> [retracted] We found out that the difference was dependent on the Chainer library, and does not replicate with another library (pytorch) which indicates that the results are probably due to a bug in Chainer, rather than being hardware-dependent. -- old abstract Deep neural networks often present uncertainties such as hardware- and software-derived noise and randomness. We studied the effects of such uncertainty on learning outcomes, with a particular focus on the function of graphics processing units (GPUs), and found that GPU-induced uncertainty increased learning accuracy of a certain deep neural network. When training a predictive deep neural network using only the CPU without the GPU, the learning error is higher than when training the same number of epochs using the GPU, suggesting that the GPU plays a different role in the learning process than just increasing the computational speed. Because this effect cannot be observed in learning by a simple autoencoder, it could be a phenomenon specific to certain types of neural networks. GPU-specific computational processing is more indeterminate than that by CPUs, and hardware-derived uncertainties, which are often considered obstacles that need to be eliminated, might, in some cases, be successfully incorporated into the training of deep neural networks. Moreover, such uncertainties might be interesting phenomena to consider in brain-related computational processing, which comprises a large mass of uncertain signals.

</details>

<details>

<summary>2021-10-06 12:13:58 - How good does a Defect Predictor need to be to guide Search-Based Software Testing?</summary>

- *Anjana Perera, Burak Turhan, Aldeida Aleti, Marcel Böhme*

- `2110.02682v1` - [abs](http://arxiv.org/abs/2110.02682v1) - [pdf](http://arxiv.org/pdf/2110.02682v1)

> Defect predictors, static bug detectors and humans inspecting the code can locate the parts of the program that are buggy before they are discovered through testing. Automated test generators such as search-based software testing (SBST) techniques can use this information to direct their search for test cases to likely buggy code, thus speeding up the process of detecting existing bugs. However, often the predictions given by these tools or humans are imprecise, which can misguide the SBST technique and may deteriorate its performance. In this paper, we study the impact of imprecision in defect prediction on the bug detection effectiveness of SBST.   Our study finds that the recall of the defect predictor, i.e., the probability of correctly identifying buggy code, has a significant impact on bug detection effectiveness of SBST with a large effect size. On the other hand, the effect of precision, a measure for false alarms, is not of meaningful practical significance as indicated by a very small effect size. In particular, the SBST technique finds 7.5 less bugs on average (out of 420 bugs) for every 5% decrements of the recall.   In the context of combining defect prediction and SBST, our recommendation for practice is to increase the recall of defect predictors at the expense of precision, while maintaining a precision of at least 75%. To account for the imprecision of defect predictors, in particular low recall values, SBST techniques should be designed to search for test cases that also cover the predicted non-buggy parts of the program, while prioritising the parts that have been predicted as buggy.

</details>

<details>

<summary>2021-10-07 02:20:13 - Deep Adversarially-Enhanced k-Nearest Neighbors</summary>

- *Ren Wang, Tianqi Chen, Alfred Hero*

- `2108.06797v2` - [abs](http://arxiv.org/abs/2108.06797v2) - [pdf](http://arxiv.org/pdf/2108.06797v2)

> Recent works have theoretically and empirically shown that deep neural networks (DNNs) have an inherent vulnerability to small perturbations. Applying the Deep k-Nearest Neighbors (DkNN) classifier, we observe a dramatically increasing robustness-accuracy trade-off as the layer goes deeper. In this work, we propose a Deep Adversarially-Enhanced k-Nearest Neighbors (DAEkNN) method which achieves higher robustness than DkNN and mitigates the robustness-accuracy trade-off in deep layers through two key elements. First, DAEkNN is based on an adversarially trained model. Second, DAEkNN makes predictions by leveraging a weighted combination of benign and adversarial training data. Empirically, we find that DAEkNN improves both the robustness and the robustness-accuracy trade-off on MNIST and CIFAR-10 datasets.

</details>

<details>

<summary>2021-10-07 03:48:23 - Attacks on Onion Discovery and Remedies via Self-Authenticating Traditional Addresses</summary>

- *Paul Syverson, Matthew Finkel, Saba Eskandarian, Dan Boneh*

- `2110.03168v1` - [abs](http://arxiv.org/abs/2110.03168v1) - [pdf](http://arxiv.org/pdf/2110.03168v1)

> Onion addresses encode their own public key. They are thus self-authenticating, one of the security and privacy advantages of onion services, which are typically accessed via Tor Browser. Because of the mostly random-looking appearance of onion addresses, a number of onion discovery mechanisms have been created to permit routing to an onion address associated with a more meaningful URL, such as a registered domain name.   We describe novel vulnerabilities engendered by onion discovery mechanisms recently introduced by Tor Browser that facilitate hijack and tracking of user connections. We also recall previously known hijack and tracking vulnerabilities engendered by use of alternative services that are facilitated and rendered harder to detect if the alternative service is at an onion address.   Self-authenticating traditional addresses (SATAs) are valid DNS addresses or URLs that also contain a commitment to an onion public key. We describe how the use of SATAs in onion discovery counters these vulnerabilities. SATAs also expand the value of onion discovery by facilitating self-authenticated access from browsers that do not connect to services via the Tor network.

</details>

<details>

<summary>2021-10-07 04:04:01 - Fingerprinting Multi-exit Deep Neural Network Models via Inference Time</summary>

- *Tian Dong, Han Qiu, Tianwei Zhang, Jiwei Li, Hewu Li, Jialiang Lu*

- `2110.03175v1` - [abs](http://arxiv.org/abs/2110.03175v1) - [pdf](http://arxiv.org/pdf/2110.03175v1)

> Transforming large deep neural network (DNN) models into the multi-exit architectures can overcome the overthinking issue and distribute a large DNN model on resource-constrained scenarios (e.g. IoT frontend devices and backend servers) for inference and transmission efficiency. Nevertheless, intellectual property (IP) protection for the multi-exit models in the wild is still an unsolved challenge. Previous efforts to verify DNN model ownership mainly rely on querying the model with specific samples and checking the responses, e.g., DNN watermarking and fingerprinting. However, they are vulnerable to adversarial settings such as adversarial training and are not suitable for the IP verification for multi-exit DNN models. In this paper, we propose a novel approach to fingerprint multi-exit models via inference time rather than inference predictions. Specifically, we design an effective method to generate a set of fingerprint samples to craft the inference process with a unique and robust inference time cost as the evidence for model ownership. We conduct extensive experiments to prove the uniqueness and robustness of our method on three structures (ResNet-56, VGG-16, and MobileNet) and three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) under comprehensive adversarial settings.

</details>

<details>

<summary>2021-10-07 09:30:51 - Ranking Warnings of Static Analysis Tools Using Representation Learning</summary>

- *Kien-Tuan Ngo, Dinh-Truong Do, Thu-Trang Nguyen, Hieu Dinh Vo*

- `2110.03296v1` - [abs](http://arxiv.org/abs/2110.03296v1) - [pdf](http://arxiv.org/pdf/2110.03296v1)

> Static analysis tools are frequently used to detect potential vulnerabilities in software systems. However, an inevitable problem of these tools is their large number of warnings with a high false positive rate, which consumes time and effort for investigating. In this paper, we present DeFP, a novel method for ranking static analysis warnings. Based on the intuition that warnings which have similar contexts tend to have similar labels (true positive or false positive), DeFP is built with two BiLSTM models to capture the patterns associated with the contexts of labeled warnings. After that, for a set of new warnings, DeFP can calculate and rank them according to their likelihoods to be true positives (i.e., actual vulnerabilities). Our experimental results on a dataset of 10 real-world projects show that using DeFP, by investigating only 60% of the warnings, developers can find +90% of actual vulnerabilities. Moreover, DeFP improves the state-of-the-art approach 30% in both Precision and Recall.

</details>

<details>

<summary>2021-10-07 12:52:06 - Batch Normalization Increases Adversarial Vulnerability and Decreases Adversarial Transferability: A Non-Robust Feature Perspective</summary>

- *Philipp Benz, Chaoning Zhang, In So Kweon*

- `2010.03316v2` - [abs](http://arxiv.org/abs/2010.03316v2) - [pdf](http://arxiv.org/pdf/2010.03316v2)

> Batch normalization (BN) has been widely used in modern deep neural networks (DNNs) due to improved convergence. BN is observed to increase the model accuracy while at the cost of adversarial robustness. There is an increasing interest in the ML community to understand the impact of BN on DNNs, especially related to the model robustness. This work attempts to understand the impact of BN on DNNs from a non-robust feature perspective. Straightforwardly, the improved accuracy can be attributed to the better utilization of useful features. It remains unclear whether BN mainly favors learning robust features (RFs) or non-robust features (NRFs). Our work presents empirical evidence that supports that BN shifts a model towards being more dependent on NRFs. To facilitate the analysis of such a feature robustness shift, we propose a framework for disentangling robust usefulness into robustness and usefulness. Extensive analysis under the proposed framework yields valuable insight on the DNN behavior regarding robustness, e.g. DNNs first mainly learn RFs and then NRFs. The insight that RFs transfer better than NRFs, further inspires simple techniques to strengthen transfer-based black-box attacks.

</details>

<details>

<summary>2021-10-07 13:45:13 - Differential Anomaly Detection for Facial Images</summary>

- *Mathias Ibsen, Lázaro J. González-Soler, Christian Rathgeb, Pawel Drozdowski, Marta Gomez-Barrero, Christoph Busch*

- `2110.03464v1` - [abs](http://arxiv.org/abs/2110.03464v1) - [pdf](http://arxiv.org/pdf/2110.03464v1)

> Due to their convenience and high accuracy, face recognition systems are widely employed in governmental and personal security applications to automatically recognise individuals. Despite recent advances, face recognition systems have shown to be particularly vulnerable to identity attacks (i.e., digital manipulations and attack presentations). Identity attacks pose a big security threat as they can be used to gain unauthorised access and spread misinformation. In this context, most algorithms for detecting identity attacks generalise poorly to attack types that are unknown at training time. To tackle this problem, we introduce a differential anomaly detection framework in which deep face embeddings are first extracted from pairs of images (i.e., reference and probe) and then combined for identity attack detection. The experimental evaluation conducted over several databases shows a high generalisation capability of the proposed method for detecting unknown attacks in both the digital and physical domains.

</details>

<details>

<summary>2021-10-07 14:58:36 - Guidelines on Minimum Standards for Developer Verification of Software</summary>

- *Paul E. Black, Barbara Guttman, Vadim Okun*

- `2107.12850v2` - [abs](http://arxiv.org/abs/2107.12850v2) - [pdf](http://arxiv.org/pdf/2107.12850v2)

> Executive Order (EO) 14028, "Improving the Nation's Cybersecurity", 12 May 2021, directs the National Institute of Standards and Technology (NIST) to recommend minimum standards for software testing within 60 days. This document describes eleven recommendations for software verification techniques as well as providing supplemental information about the techniques and references for further information. It recommends the following techniques:   Threat modeling to look for design-level security issues   Automated testing for consistency and to minimize human effort   Static code scanning to look for top bugs   Heuristic tools to look for possible hardcoded secrets   Use of built-in checks and protections   "Black box" test cases   Code-based structural test cases   Historical test cases   Fuzzing   Web app scanners, if applicable   Address included code (libraries, packages, services)   The document does not address the totality of software verification, but instead, recommends techniques that are broadly applicable and form the minimum standards.   The document was developed by NIST in consultation with the National Security Agency (NSA). Additionally, we received input from numerous outside organizations through papers submitted to a NIST workshop on the Executive Order held in early June 2021, discussion at the workshop, as well as follow up with several of the submitters.

</details>

<details>

<summary>2021-10-08 03:08:35 - Dyn-Backdoor: Backdoor Attack on Dynamic Link Prediction</summary>

- *Jinyin Chen, Haiyang Xiong, Haibin Zheng, Jian Zhang, Guodong Jiang, Yi Liu*

- `2110.03875v1` - [abs](http://arxiv.org/abs/2110.03875v1) - [pdf](http://arxiv.org/pdf/2110.03875v1)

> Dynamic link prediction (DLP) makes graph prediction based on historical information. Since most DLP methods are highly dependent on the training data to achieve satisfying prediction performance, the quality of the training data is crucial. Backdoor attacks induce the DLP methods to make wrong prediction by the malicious training data, i.e., generating a subgraph sequence as the trigger and embedding it to the training data. However, the vulnerability of DLP toward backdoor attacks has not been studied yet. To address the issue, we propose a novel backdoor attack framework on DLP, denoted as Dyn-Backdoor. Specifically, Dyn-Backdoor generates diverse initial-triggers by a generative adversarial network (GAN). Then partial links of the initial-triggers are selected to form a trigger set, according to the gradient information of the attack discriminator in the GAN, so as to reduce the size of triggers and improve the concealment of the attack. Experimental results show that Dyn-Backdoor launches successful backdoor attacks on the state-of-the-art DLP models with success rate more than 90%. Additionally, we conduct a possible defense against Dyn-Backdoor to testify its resistance in defensive settings, highlighting the needs of defenses for backdoor attacks on DLP.

</details>

<details>

<summary>2021-10-08 09:34:58 - On the feasibility of automated prediction of bug and non-bug issues</summary>

- *Steffen Herbold, Alexander Trautsch, Fabian Trautsch*

- `2003.05357v3` - [abs](http://arxiv.org/abs/2003.05357v3) - [pdf](http://arxiv.org/pdf/2003.05357v3)

> Context: Issue tracking systems are used to track and describe tasks in the development process, e.g., requested feature improvements or reported bugs. However, past research has shown that the reported issue types often do not match the description of the issue.   Objective: We want to understand the overall maturity of the state of the art of issue type prediction with the goal to predict if issues are bugs and evaluate if we can improve existing models by incorporating manually specified knowledge about issues.   Method: We train different models for the title and description of the issue to account for the difference in structure between these fields, e.g., the length. Moreover, we manually detect issues whose description contains a null pointer exception, as these are strong indicators that issues are bugs.   Results: Our approach performs best overall, but not significantly different from an approach from the literature based on the fastText classifier from Facebook AI Research. The small improvements in prediction performance are due to structural information about the issues we used. We found that using information about the content of issues in form of null pointer exceptions is not useful. We demonstrate the usefulness of issue type prediction through the example of labelling bugfixing commits.   Conclusions: Issue type prediction can be a useful tool if the use case allows either for a certain amount of missed bug reports or the prediction of too many issues as bug is acceptable.

</details>

<details>

<summary>2021-10-08 13:01:58 - TFix+: Self-configuring Hybrid Timeout Bug Fixing for Cloud Systems</summary>

- *Jingzhu He, Ting Dai, Xiaohui Gu*

- `2110.04101v1` - [abs](http://arxiv.org/abs/2110.04101v1) - [pdf](http://arxiv.org/pdf/2110.04101v1)

> Timeout bugs can cause serious availability and performance issues which are often difficult to fix due to the lack of diagnostic information. Previous work proposed solutions for fixing specific type of timeout-related performance bugs. In this paper, we present TFix+, a self-configuring timeout bug fixing framework for automatically correcting two major kinds of timeout bugs (i.e., misused timeout bugs and missing timeout bugs) with dynamic timeout value predictions. TFix+ provides two new hybrid schemes for fixing misused and missing timeout bugs, respectively. TFix+ further provides prediction-driven timeout variable configuration based on runtime function tracing. We have implemented a prototype of TFix+ and conducted experiments on 16 real world timeout bugs. Our experimental results show that TFix+ can effectively fix 15 out of tested 16 timeout bugs.

</details>

<details>

<summary>2021-10-08 14:23:41 - Protecting Retail Investors from Order Book Spoofing using a GRU-based Detection Model</summary>

- *Jean-Noël Tuccella, Philip Nadler, Ovidiu Şerban*

- `2110.03687v1` - [abs](http://arxiv.org/abs/2110.03687v1) - [pdf](http://arxiv.org/pdf/2110.03687v1)

> Market manipulation is tackled through regulation in traditional markets because of its detrimental effect on market efficiency and many participating financial actors. The recent increase of private retail investors due to new low-fee platforms and new asset classes such as decentralised digital currencies has increased the number of vulnerable actors due to lack of institutional sophistication and strong regulation. This paper proposes a method to detect illicit activity and inform investors on spoofing attempts, a well-known market manipulation technique. Our framework is based on a highly extendable Gated Recurrent Unit (GRU) model and allows the inclusion of market variables that can explain spoofing and potentially other illicit activities. The model is tested on granular order book data, in one of the most unregulated markets prone to spoofing with a large number of non-institutional traders. The results show that the model is performing well in an early detection context, allowing the identification of spoofing attempts soon enough to allow investors to react. This is the first step to a fully comprehensive model that will protect investors in various unregulated trading environments and regulators to identify illicit activity.

</details>

<details>

<summary>2021-10-08 17:13:07 - A Wireless Intrusion Detection System for 802.11 WPA3 Networks</summary>

- *Neil Dalal, Nadeem Akhtar, Anubhav Gupta, Nikhil Karamchandani, Gaurav S. Kasbekar, Jatin Parekh*

- `2110.04259v1` - [abs](http://arxiv.org/abs/2110.04259v1) - [pdf](http://arxiv.org/pdf/2110.04259v1)

> Wi-Fi (802.11) networks have become an essential part of our daily lives; hence, their security is of utmost importance. However, Wi-Fi Protected Access 3 (WPA3), the latest security certification for 802.11 standards, has recently been shown to be vulnerable to several attacks. In this paper, we first describe the attacks on WPA3 networks that have been reported in prior work; additionally, we show that a deauthentication attack and a beacon flood attack, known to be possible on a WPA2 network, are still possible with WPA3. We launch and test all the above (a total of nine) attacks using a testbed that contains an enterprise Access Point (AP) and Intrusion Detection System (IDS). Our experimental results show that the AP is vulnerable to eight out of the nine attacks and the IDS is unable to detect any of them. We propose a design for a signature-based IDS, which incorporates techniques to detect all the above attacks. Also, we implement these techniques on our testbed and verify that our IDS is able to successfully detect all the above attacks. We provide schemes for mitigating the impact of the above attacks once they are detected. We make the code to perform the above attacks as well as that of our IDS publicly available, so that it can be used for future work by the research community at large.

</details>

<details>

<summary>2021-10-08 19:18:27 - Deep-Dup: An Adversarial Weight Duplication Attack Framework to Crush Deep Neural Network in Multi-Tenant FPGA</summary>

- *Adnan Siraj Rakin, Yukui Luo, Xiaolin Xu, Deliang Fan*

- `2011.03006v2` - [abs](http://arxiv.org/abs/2011.03006v2) - [pdf](http://arxiv.org/pdf/2011.03006v2)

> The wide deployment of Deep Neural Networks (DNN) in high-performance cloud computing platforms brought to light multi-tenant cloud field-programmable gate arrays (FPGA) as a popular choice of accelerator to boost performance due to its hardware reprogramming flexibility. Such a multi-tenant FPGA setup for DNN acceleration potentially exposes DNN interference tasks under severe threat from malicious users. This work, to the best of our knowledge, is the first to explore DNN model vulnerabilities in multi-tenant FPGAs. We propose a novel adversarial attack framework: Deep-Dup, in which the adversarial tenant can inject adversarial faults to the DNN model in the victim tenant of FPGA. Specifically, she can aggressively overload the shared power distribution system of FPGA with malicious power-plundering circuits, achieving adversarial weight duplication (AWD) hardware attack that duplicates certain DNN weight packages during data transmission between off-chip memory and on-chip buffer, to hijack the DNN function of the victim tenant. Further, to identify the most vulnerable DNN weight packages for a given malicious objective, we propose a generic vulnerable weight package searching algorithm, called Progressive Differential Evolution Search (P-DES), which is, for the first time, adaptive to both deep learning white-box and black-box attack models. The proposed Deep-Dup is experimentally validated in a developed multi-tenant FPGA prototype, for two popular deep learning applications, i.e., Object Detection and Image Classification. Successful attacks are demonstrated in six popular DNN architectures (e.g., YOLOv2, ResNet-50, MobileNet, etc.)

</details>

<details>

<summary>2021-10-08 21:58:00 - Classification of anomalous gait using Machine Learning techniques and embedded sensors</summary>

- *T. R. D. Sa, C. M. S. Figueiredo*

- `2110.06139v1` - [abs](http://arxiv.org/abs/2110.06139v1) - [pdf](http://arxiv.org/pdf/2110.06139v1)

> Human gait can be a predictive factor for detecting pathologies that affect human locomotion according to studies. In addition, it is known that a high investment is demanded in order to raise a traditional clinical infrastructure able to provide human gait examinations, making them unaffordable for economically vulnerable patients. In face of this scenario, this work proposes an accessible and modern solution composed of a wearable device, to acquire 3D-accelerometer and 3D-gyroscope measurements, and machine learning techniques to classify between distinct categories of induced gait disorders. In order to develop the proposed research, it was created a dataset with the target label being 4 distinct and balanced categories of anomalous gait. The machine learning techniques that achieved the best performances (in terms of accuracy) in this dataset were through the application of Principal Component Analysis algorithm following of a Support Vector Machines classifier (94 \%). Further, an architecture based on a Feedforward Neural Network yielded even better results (96 \%). Finally, it is also presented computational performance comparison between the models implemented.

</details>

<details>

<summary>2021-10-08 23:58:24 - Robustness Evaluation of Transformer-based Form Field Extractors via Form Attacks</summary>

- *Le Xue, Mingfei Gao, Zeyuan Chen, Caiming Xiong, Ran Xu*

- `2110.04413v1` - [abs](http://arxiv.org/abs/2110.04413v1) - [pdf](http://arxiv.org/pdf/2110.04413v1)

> We propose a novel framework to evaluate the robustness of transformer-based form field extraction methods via form attacks. We introduce 14 novel form transformations to evaluate the vulnerability of the state-of-the-art field extractors against form attacks from both OCR level and form level, including OCR location/order rearrangement, form background manipulation and form field-value augmentation. We conduct robustness evaluation using real invoices and receipts, and perform comprehensive research analysis. Experimental results suggest that the evaluated models are very susceptible to form perturbations such as the variation of field-values (~15% drop in F1 score), the disarrangement of input text order(~15% drop in F1 score) and the disruption of the neighboring words of field-values(~10% drop in F1 score). Guided by the analysis, we make recommendations to improve the design of field extractors and the process of data collection.

</details>

<details>

<summary>2021-10-09 13:27:12 - Cyberphysical Security Through Resiliency: A Systems-centric Approach</summary>

- *Cody Fleming, Carl Elks, Georgios Bakirtzis, Stephen C. Adams, Bryan Carter, Peter A. Beling, Barry Horowitz*

- `2011.14469v3` - [abs](http://arxiv.org/abs/2011.14469v3) - [pdf](http://arxiv.org/pdf/2011.14469v3)

> Cyber-physical systems (CPS) are often defended in the same manner as information technology (IT) systems -- by using perimeter security. Multiple factors make such defenses insufficient for CPS. Resiliency shows potential in overcoming these shortfalls. Techniques for achieving resilience exist; however, methods and theory for evaluating resilience in CPS are lacking. We argue that such methods and theory should assist stakeholders in deciding where and how to apply design patterns for resilience. Such a problem potentially involves tradeoffs between different objectives and criteria, and such decisions need to be driven by traceable, defensible, repeatable engineering evidence. Multi-criteria resiliency problems require a system-oriented approach that evaluates systems in the presence of threats as well as potential design solutions once vulnerabilities have been identified. We present a systems-oriented view of cyber-physical security, termed Mission Aware, that is based on a holistic understanding of mission goals, system dynamics, and risk.

</details>

<details>

<summary>2021-10-09 14:24:24 - Black-box Gradient Attack on Graph Neural Networks: Deeper Insights in Graph-based Attack and Defense</summary>

- *Haoxi Zhan, Xiaobing Pei*

- `2104.15061v2` - [abs](http://arxiv.org/abs/2104.15061v2) - [pdf](http://arxiv.org/pdf/2104.15061v2)

> Graph Neural Networks (GNNs) have received significant attention due to their state-of-the-art performance on various graph representation learning tasks. However, recent studies reveal that GNNs are vulnerable to adversarial attacks, i.e. an attacker is able to fool the GNNs by perturbing the graph structure or node features deliberately. While being able to successfully decrease the performance of GNNs, most existing attacking algorithms require access to either the model parameters or the training data, which is not practical in the real world.   In this paper, we develop deeper insights into the Mettack algorithm, which is a representative grey-box attacking method, and then we propose a gradient-based black-box attacking algorithm. Firstly, we show that the Mettack algorithm will perturb the edges unevenly, thus the attack will be highly dependent on a specific training set. As a result, a simple yet useful strategy to defense against Mettack is to train the GNN with the validation set. Secondly, to overcome the drawbacks, we propose the Black-Box Gradient Attack (BBGA) algorithm. Extensive experiments demonstrate that out proposed method is able to achieve stable attack performance without accessing the training sets of the GNNs. Further results shows that our proposed method is also applicable when attacking against various defense methods.

</details>

<details>

<summary>2021-10-10 10:12:42 - Dynamic Process Isolation</summary>

- *Martin Schwarzl, Pietro Borrello, Andreas Kogler, Kenton Varda, Thomas Schuster, Daniel Gruss, Michael Schwarz*

- `2110.04751v1` - [abs](http://arxiv.org/abs/2110.04751v1) - [pdf](http://arxiv.org/pdf/2110.04751v1)

> In the quest for efficiency and performance, edge-computing providers eliminate isolation boundaries between tenants, such as strict process isolation, and instead let them compute in a more lightweight multi-threaded single-process design. Edge-computing providers support a high number of tenants per machine to reduce the physical distance to customers without requiring a large number of machines. Isolation is provided by sandboxing mechanisms, e.g., tenants can only run sandboxed V8 JavaScript code. While this is as secure as a sandbox for software vulnerabilities, microarchitectural attacks can bypass these sandboxes.   In this paper, we show that it is possible to mount a Spectre attack on such a restricted environment, leaking secrets from co-located tenants. Cloudflare Workers is one of the top three edge-computing solutions and handles millions of HTTP requests per second worldwide across tens of thousands of web sites every day. We demonstrate a remote Spectre attack using amplification techniques in combination with a remote timing server, which is capable of leaking 120 bit/h. This motivates our main contribution, Dynamic Process Isolation, a process isolation mechanism that only isolates suspicious worker scripts following a detection mechanism. In the worst case of only false positives, Dynamic Process Isolation simply degrades to process isolation. Our proof-of-concept implementation augments a real-world cloud infrastructure framework, Cloudflare Workers, which is used in production at large scale. With a false-positive rate of only 0.61%, we demonstrate that our solution vastly outperforms strict process isolation in terms of performance. In our security evaluation, we show that Dynamic Process Isolation statistically provides the same security guarantees as strict process isolation, fully mitigating Spectre attacks between multiple tenants.

</details>

<details>

<summary>2021-10-10 13:29:21 - SOME/IP Intrusion Detection using Deep Learning-based Sequential Models in Automotive Ethernet Networks</summary>

- *Natasha Alkhatib, Hadi Ghauch, Jean-Luc Danger*

- `2108.08262v2` - [abs](http://arxiv.org/abs/2108.08262v2) - [pdf](http://arxiv.org/pdf/2108.08262v2)

> Intrusion Detection Systems are widely used to detect cyberattacks, especially on protocols vulnerable to hacking attacks such as SOME/IP. In this paper, we present a deep learning-based sequential model for offline intrusion detection on SOME/IP application layer protocol. To assess our intrusion detection system, we have generated and labeled a dataset with several classes representing realistic intrusions, and a normal class - a significant contribution due to the absence of such publicly available datasets. Furthermore, we also propose a recurrent neural network (RNN), as an instance of deep learning-based sequential model, that we apply to our generated dataset. The numerical results show that RNN excel at predicting in-vehicle intrusions, with F1 Scores and AUC values greater than 0.8 depending on each intrusion type.

</details>

<details>

<summary>2021-10-10 18:10:36 - Broccoli: Bug localization with the help of text search engines</summary>

- *Benjamin Ledel, Steffen Herbold*

- `2109.11902v2` - [abs](http://arxiv.org/abs/2109.11902v2) - [pdf](http://arxiv.org/pdf/2109.11902v2)

> Bug localization is a tedious activity in the bug fixing process in which a software developer tries to locate bugs in the source code described in a bug report. Since this process is time-consuming and requires additional knowledge about the software project, information retrieval techniques can aid the bug localization process. In this paper, we investigate if normal text search engines can improve existing bug localization approaches. In a case study, we evaluate the performance of our search engine approach Broccoli against seven state-of-the-art bug localization algorithms on 82 open source projects in two data sets. Our results show that including a search engine can increase the performance of the bug localization and that it is a useful extension to existing approaches. As part of our analysis we also exposed a flaw in a commonly used benchmark strategy, i.e., that files of a single release are considered. To increase the number of detectable files, we mitigate this flaw by considering the state of the software repository at the time of the bug report. Our results show that using single releases may lead to an underestimation of the the prediction performance.

</details>

<details>

<summary>2021-10-10 18:23:13 - Robustness May Be at Odds with Fairness: An Empirical Study on Class-wise Accuracy</summary>

- *Philipp Benz, Chaoning Zhang, Adil Karjauv, In So Kweon*

- `2010.13365v2` - [abs](http://arxiv.org/abs/2010.13365v2) - [pdf](http://arxiv.org/pdf/2010.13365v2)

> Convolutional neural networks (CNNs) have made significant advancement, however, they are widely known to be vulnerable to adversarial attacks. Adversarial training is the most widely used technique for improving adversarial robustness to strong white-box attacks. Prior works have been evaluating and improving the model average robustness without class-wise evaluation. The average evaluation alone might provide a false sense of robustness. For example, the attacker can focus on attacking the vulnerable class, which can be dangerous, especially, when the vulnerable class is a critical one, such as "human" in autonomous driving. We propose an empirical study on the class-wise accuracy and robustness of adversarially trained models. We find that there exists inter-class discrepancy for accuracy and robustness even when the training dataset has an equal number of samples for each class. For example, in CIFAR10, "cat" is much more vulnerable than other classes. Moreover, this inter-class discrepancy also exists for normally trained models, while adversarial training tends to further increase the discrepancy. Our work aims to investigate the following questions: (a) is the phenomenon of inter-class discrepancy universal regardless of datasets, model architectures and optimization hyper-parameters? (b) If so, what can be possible explanations for the inter-class discrepancy? (c) Can the techniques proposed in the long tail classification be readily extended to adversarial training for addressing the inter-class discrepancy?

</details>

<details>

<summary>2021-10-10 19:59:28 - Adversarial Attacks in a Multi-view Setting: An Empirical Study of the Adversarial Patches Inter-view Transferability</summary>

- *Bilel Tarchoun, Ihsen Alouani, Anouar Ben Khalifa, Mohamed Ali Mahjoub*

- `2110.04887v1` - [abs](http://arxiv.org/abs/2110.04887v1) - [pdf](http://arxiv.org/pdf/2110.04887v1)

> While machine learning applications are getting mainstream owing to a demonstrated efficiency in solving complex problems, they suffer from inherent vulnerability to adversarial attacks. Adversarial attacks consist of additive noise to an input which can fool a detector. Recently, successful real-world printable adversarial patches were proven efficient against state-of-the-art neural networks. In the transition from digital noise based attacks to real-world physical attacks, the myriad of factors affecting object detection will also affect adversarial patches. Among these factors, view angle is one of the most influential, yet under-explored. In this paper, we study the effect of view angle on the effectiveness of an adversarial patch. To this aim, we propose the first approach that considers a multi-view context by combining existing adversarial patches with a perspective geometric transformation in order to simulate the effect of view angle changes. Our approach has been evaluated on two datasets: the first dataset which contains most real world constraints of a multi-view context, and the second dataset which empirically isolates the effect of view angle. The experiments show that view angle significantly affects the performance of adversarial patches, where in some cases the patch loses most of its effectiveness. We believe that these results motivate taking into account the effect of view angles in future adversarial attacks, and open up new opportunities for adversarial defenses.

</details>

<details>

<summary>2021-10-11 01:07:42 - Bug Prediction Using Source Code Embedding Based on Doc2Vec</summary>

- *Tamás Aladics, Judit Jász, Rudolf Ferenc*

- `2110.04951v1` - [abs](http://arxiv.org/abs/2110.04951v1) - [pdf](http://arxiv.org/pdf/2110.04951v1)

> Bug prediction is a resource demanding task that is hard to automate using static source code analysis. In many fields of computer science, machine learning has proven to be extremely useful in tasks like this, however, for it to work we need a way to use source code as input. We propose a simple, but meaningful representation for source code based on its abstract syntax tree and the Doc2Vec embedding algorithm. This representation maps the source code to a fixed length vector which can be used for various upstream tasks -- one of which is bug prediction. We measured this approach's validity by itself and its effectiveness compared to bug prediction based solely on code metrics. We also experimented on numerous machine learning approaches to check the connection between different embedding parameters with different machine learning models. Our results show that this representation provides meaningful information as it improves the bug prediction accuracy in most cases, and is always at least as good as only using code metrics as features.

</details>

<details>

<summary>2021-10-11 10:51:56 - Human Values in Mobile App Development: An Empirical Study on Bangladeshi Agriculture Mobile Apps</summary>

- *Rifat Ara Shams, Mojtaba Shahin, Gillian Oliver, Jon Whittle, Waqar Hussain, Harsha Perera, Arif Nurwidyantoro*

- `2110.05150v1` - [abs](http://arxiv.org/abs/2110.05150v1) - [pdf](http://arxiv.org/pdf/2110.05150v1)

> Given the ubiquity of mobile applications (apps) in daily lives, understanding and reflecting end-users' human values (e.g., transparency, privacy, social recognition etc.) in apps has become increasingly important. Violations of end users' values by software applications have been reported in the media and have resulted in a wide range of difficulties for end users. Value violations may bring more and lasting problems for marginalized and vulnerable groups of end-users. This research aims to understand the extent to which the values of Bangladeshi female farmers, marginalized and vulnerable end-users, who are less studied by the software engineering community, are reflected in agriculture apps in Bangladesh. Further to this, we aim to identify possible strategies to embed their values in those apps. To this end, we conducted a mixed-methods empirical study consisting of 13 interviews with app practitioners and four focus groups with 20 Bangladeshi female farmers. The accumulated results from the interviews and focus groups identified 22 values of Bangladeshi female farmers, which the participants expect to be reflected in the agriculture apps. Among these 22 values, 15 values (e.g., accuracy, independence) are already reflected and 7 values (e.g., accessibility, pleasure) are ignored/violated in the existing agriculture apps. We also identified 14 strategies (e.g., "applying human-centered approaches to elicit values", "establishing a dedicated team/person for values concerns") to address Bangladeshi female farmers' values in agriculture apps.

</details>

<details>

<summary>2021-10-11 14:05:29 - Homogeneous Learning: Self-Attention Decentralized Deep Learning</summary>

- *Yuwei Sun, Hideya Ochiai*

- `2110.05290v1` - [abs](http://arxiv.org/abs/2110.05290v1) - [pdf](http://arxiv.org/pdf/2110.05290v1)

> Federated learning (FL) has been facilitating privacy-preserving deep learning in many walks of life such as medical image classification, network intrusion detection, and so forth. Whereas it necessitates a central parameter server for model aggregation, which brings about delayed model communication and vulnerability to adversarial attacks. A fully decentralized architecture like Swarm Learning allows peer-to-peer communication among distributed nodes, without the central server. One of the most challenging issues in decentralized deep learning is that data owned by each node are usually non-independent and identically distributed (non-IID), causing time-consuming convergence of model training. To this end, we propose a decentralized learning model called Homogeneous Learning (HL) for tackling non-IID data with a self-attention mechanism. In HL, training performs on each round's selected node, and the trained model of a node is sent to the next selected node at the end of each round. Notably, for the selection, the self-attention mechanism leverages reinforcement learning to observe a node's inner state and its surrounding environment's state, and find out which node should be selected to optimize the training. We evaluate our method with various scenarios for an image classification task. The result suggests that HL can produce a better performance compared with standalone learning and greatly reduce both the total training rounds by 50.8% and the communication cost by 74.6% compared with random policy-based decentralized learning for training on non-IID data.

</details>

<details>

<summary>2021-10-11 14:28:50 - Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs</summary>

- *Philipp Benz, Soomin Ham, Chaoning Zhang, Adil Karjauv, In So Kweon*

- `2110.02797v2` - [abs](http://arxiv.org/abs/2110.02797v2) - [pdf](http://arxiv.org/pdf/2110.02797v2)

> Convolutional Neural Networks (CNNs) have become the de facto gold standard in computer vision applications in the past years. Recently, however, new model architectures have been proposed challenging the status quo. The Vision Transformer (ViT) relies solely on attention modules, while the MLP-Mixer architecture substitutes the self-attention modules with Multi-Layer Perceptrons (MLPs). Despite their great success, CNNs have been widely known to be vulnerable to adversarial attacks, causing serious concerns for security-sensitive applications. Thus, it is critical for the community to know whether the newly proposed ViT and MLP-Mixer are also vulnerable to adversarial attacks. To this end, we empirically evaluate their adversarial robustness under several adversarial attack setups and benchmark them against the widely used CNNs. Overall, we find that the two architectures, especially ViT, are more robust than their CNN models. Using a toy example, we also provide empirical evidence that the lower adversarial robustness of CNNs can be partially attributed to their shift-invariant property. Our frequency analysis suggests that the most robust ViT architectures tend to rely more on low-frequency features compared with CNNs. Additionally, we have an intriguing finding that MLP-Mixer is extremely vulnerable to universal adversarial perturbations.

</details>

<details>

<summary>2021-10-11 15:59:43 - Classifying SMEs for Approaching Cybersecurity Competence and Awareness</summary>

- *Alireza Shojaifar, Heini Jarvinen*

- `2110.05370v1` - [abs](http://arxiv.org/abs/2110.05370v1) - [pdf](http://arxiv.org/pdf/2110.05370v1)

> Cybersecurity is increasingly a concern for small and medium-sized enterprises (SMEs), and there exist many awareness training programs and tools for them. The literature mainly studies SMEs as a unitary type of company and provides one-size-fits-all recommendations and solutions. However, SMEs are not homogeneous. They are diverse with different vulnerabilities, cybersecurity needs, and competencies. Few studies considered such differences in standards and certificates for security tools adoption and cybersecurity tailoring for these SMEs. This study proposes a classification framework with an outline of cybersecurity improvement needs for each class. The framework suggests five SME types based on their characteristics and specific security needs: cybersecurity abandoned SME, unskilled SME, expert-connected SME, capable SME, and cybersecurity provider SME. In addition to describing the five classes, the study explains the framework's usage in sampled SMEs. The framework proposes solutions for each class to approach cybersecurity awareness and competence more consistent with SME needs. The final publication is available at ACM Digital Library via this https URL https://doi.org/10.1145/3465481.3469200

</details>

<details>

<summary>2021-10-11 17:31:41 - User-driven Design and Evaluation of Liquid Types in Java</summary>

- *Catarina Gamboa, Paulo Alexandre Santos, Christopher S. Timperley, Alcides Fonseca*

- `2110.05444v1` - [abs](http://arxiv.org/abs/2110.05444v1) - [pdf](http://arxiv.org/pdf/2110.05444v1)

> Bugs that are detected earlier during the development lifecycle are easier and cheaper to fix, whereas bugs that are found during production are difficult and expensive to address, and may have dire consequences. Type systems are particularly effective at identifying and preventing bugs early in the development lifecycle by causing invalid programs to result in build failure. Liquid Types are more powerful than those found in mainstream programming languages, allowing the detection of more classes of bugs. However, while Liquid Types were proposed in 2008 with their integration in ML and subsequently introduced in C (2012), Javascript(2012) and Haskell(2014) through language extensions, they have yet to become widely adopted by mainstream developers. This paper investigates how Liquid Types can be integrated in a mainstream programming language, Java, by proposing a new design that aims to lower the barrier to entry and adapts to problems that Java developers commonly encounter at runtime. To promote accessibility, we conducted a series of developer surveys to design the syntax of LiquidJava, our prototype. To evaluate the prototype's usability, we conducted a user study of 30 Java developers, concluding that users intend to use LiquidJava and that it helped to find more bugs and debug faster.

</details>

<details>

<summary>2021-10-11 21:31:59 - Parameterizing Activation Functions for Adversarial Robustness</summary>

- *Sihui Dai, Saeed Mahloujifar, Prateek Mittal*

- `2110.05626v1` - [abs](http://arxiv.org/abs/2110.05626v1) - [pdf](http://arxiv.org/pdf/2110.05626v1)

> Deep neural networks are known to be vulnerable to adversarially perturbed inputs. A commonly used defense is adversarial training, whose performance is influenced by model capacity. While previous works have studied the impact of varying model width and depth on robustness, the impact of increasing capacity by using learnable parametric activation functions (PAFs) has not been studied. We study how using learnable PAFs can improve robustness in conjunction with adversarial training. We first ask the question: how should we incorporate parameters into activation functions to improve robustness? To address this, we analyze the direct impact of activation shape on robustness through PAFs and observe that activation shapes with positive outputs on negative inputs and with high finite curvature can increase robustness. We combine these properties to create a new PAF, which we call Parametric Shifted Sigmoidal Linear Unit (PSSiLU). We then combine PAFs (including PReLU, PSoftplus and PSSiLU) with adversarial training and analyze robust performance. We find that PAFs optimize towards activation shape properties found to directly affect robustness. Additionally, we find that while introducing only 1-2 learnable parameters into the network, smooth PAFs can significantly increase robustness over ReLU. For instance, when trained on CIFAR-10 with additional synthetic data, PSSiLU improves robust accuracy by 4.54% over ReLU on ResNet-18 and 2.69% over ReLU on WRN-28-10 in the $\ell_{\infty}$ threat model while adding only 2 additional parameters into the network architecture. The PSSiLU WRN-28-10 model achieves 61.96% AutoAttack accuracy, improving over the state-of-the-art robust accuracy on RobustBench (Croce et al., 2020).

</details>

<details>

<summary>2021-10-12 02:20:34 - Hiding Images into Images with Real-world Robustness</summary>

- *Qichao Ying, Hang Zhou, Xianhan Zeng, Haisheng Xu, Zhenxing Qian, Xinpeng Zhang*

- `2110.05689v1` - [abs](http://arxiv.org/abs/2110.05689v1) - [pdf](http://arxiv.org/pdf/2110.05689v1)

> The existing image embedding networks are basically vulnerable to malicious attacks such as JPEG compression and noise adding, not applicable for real-world copyright protection tasks. To solve this problem, we introduce a generative deep network based method for hiding images into images while assuring high-quality extraction from the destructive synthesized images. An embedding network is sequentially concatenated with an attack layer, a decoupling network and an image extraction network. The addition of decoupling network learns to extract the embedded watermark from the attacked image. We also pinpoint the weaknesses of the adversarial training for robustness in previous works and build our improved real-world attack simulator. Experimental results demonstrate the superiority of the proposed method against typical digital attacks by a large margin, as well as the performance boost of the recovered images with the aid of progressive recovery strategy. Besides, we are the first to robustly hide three secret images.

</details>

<details>

<summary>2021-10-12 11:06:49 - Hide and seek with quantum resources: New and modified protocols for quantum steganography</summary>

- *Rohan Joshi, Akhil Gupta, Kishore Thapliyal, R Srikanth, Anirban Pathak*

- `2110.05893v1` - [abs](http://arxiv.org/abs/2110.05893v1) - [pdf](http://arxiv.org/pdf/2110.05893v1)

> Steganography is the science of hiding and communicating a secret message by embedding it in an innocent looking text such that the eavesdropper is unaware of its existence. Previously, attempts were made to establish steganography using quantum key distribution (QKD). Recently, it has been shown that such protocols are vulnerable to a certain steganalysis attack that can detect the presence of the hidden message and suppress the entire communication. In this work, we elaborate on the vulnerabilities of the original protocol which make it insecure against this detection attack. Further, we propose a novel steganography protocol using discrete modulation continuous variable QKD that eliminates the threat of this detection-based attack. Deriving from the properties of our protocol, we also propose modifications in the original protocol to dispose of its vulnerabilities and make it insusceptible to steganalysis.

</details>

<details>

<summary>2021-10-12 14:04:15 - On the Security Risks of AutoML</summary>

- *Ren Pang, Zhaohan Xi, Shouling Ji, Xiapu Luo, Ting Wang*

- `2110.06018v1` - [abs](http://arxiv.org/abs/2110.06018v1) - [pdf](http://arxiv.org/pdf/2110.06018v1)

> Neural Architecture Search (NAS) represents an emerging machine learning (ML) paradigm that automatically searches for models tailored to given tasks, which greatly simplifies the development of ML systems and propels the trend of ML democratization. Yet, little is known about the potential security risks incurred by NAS, which is concerning given the increasing use of NAS-generated models in critical domains.   This work represents a solid initial step towards bridging the gap. Through an extensive empirical study of 10 popular NAS methods, we show that compared with their manually designed counterparts, NAS-generated models tend to suffer greater vulnerability to various malicious attacks (e.g., adversarial evasion, model poisoning, and functionality stealing). Further, with both empirical and analytical evidence, we provide possible explanations for such phenomena: given the prohibitive search space and training cost, most NAS methods favor models that converge fast at early training stages; this preference results in architectural properties associated with attack vulnerability (e.g., high loss smoothness and low gradient variance). Our findings not only reveal the relationships between model characteristics and attack vulnerability but also suggest the inherent connections underlying different attacks. Finally, we discuss potential remedies to mitigate such drawbacks, including increasing cell depth and suppressing skip connects, which lead to several promising research directions.

</details>

<details>

<summary>2021-10-12 22:12:26 - AutoNLU: Detecting, root-causing, and fixing NLU model errors</summary>

- *Pooja Sethi, Denis Savenkov, Forough Arabshahi, Jack Goetz, Micaela Tolliver, Nicolas Scheffer, Ilknur Kabul, Yue Liu, Ahmed Aly*

- `2110.06384v1` - [abs](http://arxiv.org/abs/2110.06384v1) - [pdf](http://arxiv.org/pdf/2110.06384v1)

> Improving the quality of Natural Language Understanding (NLU) models, and more specifically, task-oriented semantic parsing models, in production is a cumbersome task. In this work, we present a system called AutoNLU, which we designed to scale the NLU quality improvement process. It adds automation to three key steps: detection, attribution, and correction of model errors, i.e., bugs. We detected four times more failed tasks than with random sampling, finding that even a simple active learning sampling method on an uncalibrated model is surprisingly effective for this purpose. The AutoNLU tool empowered linguists to fix ten times more semantic parsing bugs than with prior manual processes, auto-correcting 65% of all identified bugs.

</details>

<details>

<summary>2021-10-13 04:26:10 - Robust Graph Data Learning via Latent Graph Convolutional Representation</summary>

- *Bo Jiang, Ziyan Zhang, Bin Luo*

- `1904.11883v2` - [abs](http://arxiv.org/abs/1904.11883v2) - [pdf](http://arxiv.org/pdf/1904.11883v2)

> Graph Convolutional Representation (GCR) has achieved impressive performance for graph data representation. However, existing GCR is generally defined on the input fixed graph which may restrict the representation capacity and also be vulnerable to the structural attacks and noises. To address this issue, we propose a novel Latent Graph Convolutional Representation (LatGCR) for robust graph data representation and learning. Our LatGCR is derived based on reformulating graph convolutional representation from the aspect of graph neighborhood reconstruction. Given an input graph $\textbf{A}$, LatGCR aims to generate a flexible latent graph $\widetilde{\textbf{A}}$ for graph convolutional representation which obviously enhances the representation capacity and also performs robustly w.r.t graph structural attacks and noises. Moreover, LatGCR is implemented in a self-supervised manner and thus provides a basic block for both supervised and unsupervised graph learning tasks. Experiments on several datasets demonstrate the effectiveness and robustness of LatGCR.

</details>

<details>

<summary>2021-10-13 06:39:57 - Defensive Cost-Benefit Analysis of Smart Grid Digital Functionalities</summary>

- *Jim Stright, Peter Cheetham, Charalambos Konstantinou*

- `2008.12843v2` - [abs](http://arxiv.org/abs/2008.12843v2) - [pdf](http://arxiv.org/pdf/2008.12843v2)

> Modern smart grids offer several types of digital control and monitoring of electric power transmission and distribution that enable greater efficiency and integrative functionality than traditional power grids. These benefits, however, introduce greater complexity and greatly disrupt and expand the threat landscape. The number of vulnerabilities is increasing as grid-connected devices proliferate. The potential costs to society of these vulnerabilities are difficult to determine, as are their likelihoods of successful exploitation. In this article, we present a method for comparing the net economic benefits and costs of the various cyber-functionalities associated with smart grids from the perspective of cyberattack vulnerabilities and defending against them. The economic considerations of cyber defense spending suggest the existence of optimal levels of expenditures, which might vary among digital functionalities. We illustrate hypothetical case studies on how digital functionalities can be assessed and compared with respect to the costs of defending them from cyberattacks.

</details>

<details>

<summary>2021-10-13 08:50:02 - CROW: Code Diversification for WebAssembly</summary>

- *Javier Cabrera Arteaga, Orestis Malivitsis, Oscar Vera Pérez, Benoit Baudry, Martin Monperrus*

- `2008.07185v4` - [abs](http://arxiv.org/abs/2008.07185v4) - [pdf](http://arxiv.org/pdf/2008.07185v4)

> The adoption of WebAssembly has rapidly increased in the last few years as it provides a fast and safe model for program execution. However, WebAssembly is not exempt from vulnerabilities that could be exploited by side channels attacks. This class of vulnerabilities that can be addressed by code diversification. In this paper, we present the first fully automated workflow for the diversification of WebAssembly binaries. We present CROW, an open-source tool implementing this workflow. We evaluate CROW's capabilities on 303 C programs and study its use on a real-life security-sensitive program: libsodium, a cryptographic library. Overall, CROWis able to generate diverse variants for 239 out of 303,(79%) small programs. Furthermore, our experiments show that our approach and tool is able to successfully diversify off-the-shelf cryptographic software (libsodium).

</details>

<details>

<summary>2021-10-13 11:46:30 - SmashEx: Smashing SGX Enclaves Using Exceptions</summary>

- *Jinhua Cui, Jason Zhijingcheng Yu, Shweta Shinde, Prateek Saxena, Zhiping Cai*

- `2110.06657v1` - [abs](http://arxiv.org/abs/2110.06657v1) - [pdf](http://arxiv.org/pdf/2110.06657v1)

> Exceptions are a commodity hardware functionality which is central to multi-tasking OSes as well as event-driven user applications. Normally, the OS assists the user application by lifting the semantics of exceptions received from hardware to program-friendly user signals and exception handling interfaces. However, can exception handlers work securely in user enclaves, such as those enabled by Intel SGX, where the OS is not trusted by the enclave code?   In this paper, we introduce a new attack called SmashEx which exploits the OS-enclave interface for asynchronous exceptions in SGX. It demonstrates the importance of a fundamental property of safe atomic execution that is required on this interface. In the absence of atomicity, we show that asynchronous exception handling in SGX enclaves is complicated and prone to re-entrancy vulnerabilities. Our attacks do not assume any memory errors in the enclave code, side channels, or application-specific logic flaws. We concretely demonstrate exploits that cause arbitrary disclosure of enclave private memory and code-reuse (ROP) attacks in the enclave. We show reliable exploits on two widely-used SGX runtimes, Intel SGX SDK and Microsoft Open Enclave, running OpenSSL and cURL libraries respectively. We tested a total of 14 frameworks, including Intel SGX SDK and Microsoft Open Enclave, 10 of which are vulnerable. We discuss how the vulnerability manifests on both SGX1-based and SGX2-based platforms. We present potential mitigation and long-term defenses for SmashEx.

</details>

<details>

<summary>2021-10-13 13:54:24 - Model-Agnostic Meta-Attack: Towards Reliable Evaluation of Adversarial Robustness</summary>

- *Xiao Yang, Yinpeng Dong, Wenzhao Xiang, Tianyu Pang, Hang Su, Jun Zhu*

- `2110.08256v1` - [abs](http://arxiv.org/abs/2110.08256v1) - [pdf](http://arxiv.org/pdf/2110.08256v1)

> The vulnerability of deep neural networks to adversarial examples has motivated an increasing number of defense strategies for promoting model robustness. However, the progress is usually hampered by insufficient robustness evaluations. As the de facto standard to evaluate adversarial robustness, adversarial attacks typically solve an optimization problem of crafting adversarial examples with an iterative process. In this work, we propose a Model-Agnostic Meta-Attack (MAMA) approach to discover stronger attack algorithms automatically. Our method learns the optimizer in adversarial attacks parameterized by a recurrent neural network, which is trained over a class of data samples and defenses to produce effective update directions during adversarial example generation. Furthermore, we develop a model-agnostic training algorithm to improve the generalization ability of the learned optimizer when attacking unseen defenses. Our approach can be flexibly incorporated with various attacks and consistently improves the performance with little extra computational cost. Extensive experiments demonstrate the effectiveness of the learned attacks by MAMA compared to the state-of-the-art attacks on different defenses, leading to a more reliable evaluation of adversarial robustness.

</details>

<details>

<summary>2021-10-13 13:55:40 - Diversity of Skills and Collective Intelligence in GitHub</summary>

- *Dorota Celińska-Kopczyńska*

- `2110.06725v1` - [abs](http://arxiv.org/abs/2110.06725v1) - [pdf](http://arxiv.org/pdf/2110.06725v1)

> A common assumption suggests that individuals tend to work with others who are similar to them. However, studies on team working and ability of the group to solve complex problems highlight that diversity plays a critical role during collaboration, allowing for the diffusion of information. In this paper, we investigate the patterns behind the connections among GitHub users in Open Source communities. To this end, we use Social Network Analysis and Self-Organizing Maps as the similarity measure. Analysis of textual artifacts reveals the roles of those connections. We find that diversity of skills plays an essential role in the creation of links among users who exchange information (e.g., in issues, comments, and following networks). The connections in networks related to actual coding are established among users with similar characteristics. Users who differ from the owner of the repository report bugs, problems and ask for help more often than the similar ones.

</details>

<details>

<summary>2021-10-13 14:27:43 - A Fine-grained Data Set and Analysis of Tangling in Bug Fixing Commits</summary>

- *Steffen Herbold, Alexander Trautsch, Benjamin Ledel, Alireza Aghamohammadi, Taher Ahmed Ghaleb, Kuljit Kaur Chahal, Tim Bossenmaier, Bhaveet Nagaria, Philip Makedonski, Matin Nili Ahmadabadi, Kristof Szabados, Helge Spieker, Matej Madeja, Nathaniel Hoy, Valentina Lenarduzzi, Shangwen Wang, Gema Rodríguez-Pérez, Ricardo Colomo-Palacios, Roberto Verdecchia, Paramvir Singh, Yihao Qin, Debasish Chakroborti, Willard Davis, Vijay Walunj, Hongjun Wu, Diego Marcilio, Omar Alam, Abdullah Aldaeej, Idan Amit, Burak Turhan, Simon Eismann, Anna-Katharina Wickert, Ivano Malavolta, Matus Sulir, Fatemeh Fard, Austin Z. Henley, Stratos Kourtzanidis, Eray Tuzun, Christoph Treude, Simin Maleki Shamasbi, Ivan Pashchenko, Marvin Wyrich, James Davis, Alexander Serebrenik, Ella Albrecht, Ethem Utku Aktas, Daniel Strüber, Johannes Erbel*

- `2011.06244v4` - [abs](http://arxiv.org/abs/2011.06244v4) - [pdf](http://arxiv.org/pdf/2011.06244v4)

> Context: Tangled commits are changes to software that address multiple concerns at once. For researchers interested in bugs, tangled commits mean that they actually study not only bugs, but also other concerns irrelevant for the study of bugs.   Objective: We want to improve our understanding of the prevalence of tangling and the types of changes that are tangled within bug fixing commits.   Methods: We use a crowd sourcing approach for manual labeling to validate which changes contribute to bug fixes for each line in bug fixing commits. Each line is labeled by four participants. If at least three participants agree on the same label, we have consensus.   Results: We estimate that between 17% and 32% of all changes in bug fixing commits modify the source code to fix the underlying problem. However, when we only consider changes to the production code files this ratio increases to 66% to 87%. We find that about 11% of lines are hard to label leading to active disagreements between participants. Due to confirmed tangling and the uncertainty in our data, we estimate that 3% to 47% of data is noisy without manual untangling, depending on the use case.   Conclusion: Tangled commits have a high prevalence in bug fixes and can lead to a large amount of noise in the data. Prior research indicates that this noise may alter results. As researchers, we should be skeptics and assume that unvalidated data is likely very noisy, until proven otherwise.

</details>

<details>

<summary>2021-10-13 16:13:16 - Privacy-Preserving Mutual Authentication and Key Agreement Scheme for Multi-Server Healthcare System</summary>

- *Trupil Limbasiya, Sanjay K. Sahay, Bharath Sridharan*

- `2110.09654v1` - [abs](http://arxiv.org/abs/2110.09654v1) - [pdf](http://arxiv.org/pdf/2110.09654v1)

> The usage of different technologies and smart devices helps people to get medical services remotely for multiple benefits. Thus, critical and sensitive data is exchanged between a user and a doctor. When health data is transmitted over a common channel, it becomes essential to preserve various privacy and security properties in the system. Further, the number of users for remote services is increasing day-by-day exponentially, and thus, it is not adequate to deal with all users using the one server due to the verification overhead, server failure, and scalability issues. Thus, researchers proposed various authentication protocols for multi-server architecture, but most of them are vulnerable to different security attacks and require high computational resources during the implementation. To Tackle privacy and security issues using less computational resources, we propose a privacy-preserving mutual authentication and key agreement protocol for a multi-server healthcare system. We discuss the proposed scheme's security analysis and performance results to understand its security strengths and the computational resource requirement, respectively. Further, we do the comparison of security and performance results with recent relevant authentication protocols.

</details>

<details>

<summary>2021-10-13 19:54:07 - Out-of-Distribution Robustness in Deep Learning Compression</summary>

- *Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti*

- `2110.07007v1` - [abs](http://arxiv.org/abs/2110.07007v1) - [pdf](http://arxiv.org/pdf/2110.07007v1)

> In recent years, deep neural network (DNN) compression systems have proved to be highly effective for designing source codes for many natural sources. However, like many other machine learning systems, these compressors suffer from vulnerabilities to distribution shifts as well as out-of-distribution (OOD) data, which reduces their real-world applications. In this paper, we initiate the study of OOD robust compression. Considering robustness to two types of ambiguity sets (Wasserstein balls and group shifts), we propose algorithmic and architectural frameworks built on two principled methods: one that trains DNN compressors using distributionally-robust optimization (DRO), and the other which uses a structured latent code. Our results demonstrate that both methods enforce robustness compared to a standard DNN compressor, and that using a structured code can be superior to the DRO compressor. We observe tradeoffs between robustness and distortion and corroborate these findings theoretically for a specific class of sources.

</details>

<details>

<summary>2021-10-14 03:54:16 - Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer</summary>

- *Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun*

- `2110.07139v1` - [abs](http://arxiv.org/abs/2110.07139v1) - [pdf](http://arxiv.org/pdf/2110.07139v1)

> Adversarial attacks and backdoor attacks are two common security threats that hang over deep learning. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a backdoor attack method, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transfer -- the attack success rates can exceed 90% without much effort. It reflects the limited ability of NLP models to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to baselines in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack.

</details>

<details>

<summary>2021-10-14 07:08:15 - Adversarial examples by perturbing high-level features in intermediate decoder layers</summary>

- *Vojtěch Čermák, Lukáš Adam*

- `2110.07182v1` - [abs](http://arxiv.org/abs/2110.07182v1) - [pdf](http://arxiv.org/pdf/2110.07182v1)

> We propose a novel method for creating adversarial examples. Instead of perturbing pixels, we use an encoder-decoder representation of the input image and perturb intermediate layers in the decoder. This changes the high-level features provided by the generative model. Therefore, our perturbation possesses semantic meaning, such as a longer beak or green tints. We formulate this task as an optimization problem by minimizing the Wasserstein distance between the adversarial and initial images under a misclassification constraint. We employ the projected gradient method with a simple inexact projection. Due to the projection, all iterations are feasible, and our method always generates adversarial images. We perform numerical experiments on the MNIST and ImageNet datasets in both targeted and untargeted settings. We demonstrate that our adversarial images are much less vulnerable to steganographic defence techniques than pixel-based attacks. Moreover, we show that our method modifies key features such as edges and that defence techniques based on adversarial training are vulnerable to our attacks.

</details>

<details>

<summary>2021-10-14 09:06:57 - SAGE: Intrusion Alert-driven Attack Graph Extractor</summary>

- *Azqa Nadeem, Sicco Verwer, Shanchieh Jay Yang*

- `2107.02783v2` - [abs](http://arxiv.org/abs/2107.02783v2) - [pdf](http://arxiv.org/pdf/2107.02783v2)

> Attack graphs (AG) are used to assess pathways availed by cyber adversaries to penetrate a network. State-of-the-art approaches for AG generation focus mostly on deriving dependencies between system vulnerabilities based on network scans and expert knowledge. In real-world operations however, it is costly and ineffective to rely on constant vulnerability scanning and expert-crafted AGs. We propose to automatically learn AGs based on actions observed through intrusion alerts, without prior expert knowledge. Specifically, we develop an unsupervised sequence learning system, SAGE, that leverages the temporal and probabilistic dependence between alerts in a suffix-based probabilistic deterministic finite automaton (S-PDFA) -- a model that accentuates infrequent severe alerts and summarizes paths leading to them. AGs are then derived from the S-PDFA on a per-objective, per-victim basis. Tested with intrusion alerts collected through Collegiate Penetration Testing Competition, SAGE compresses over 330k alerts into 93 AGs. These AGs reflect the strategies used by the participating teams. The AGs are succinct, interpretable, and capture behavioral dynamics, e.g., that attackers will often follow shorter paths to re-exploit objectives.

</details>

<details>

<summary>2021-10-14 15:18:49 - Bugs in our Pockets: The Risks of Client-Side Scanning</summary>

- *Hal Abelson, Ross Anderson, Steven M. Bellovin, Josh Benaloh, Matt Blaze, Jon Callas, Whitfield Diffie, Susan Landau, Peter G. Neumann, Ronald L. Rivest, Jeffrey I. Schiller, Bruce Schneier, Vanessa Teague, Carmela Troncoso*

- `2110.07450v1` - [abs](http://arxiv.org/abs/2110.07450v1) - [pdf](http://arxiv.org/pdf/2110.07450v1)

> Our increasing reliance on digital technology for personal, economic, and government affairs has made it essential to secure the communications and devices of private citizens, businesses, and governments. This has led to pervasive use of cryptography across society. Despite its evident advantages, law enforcement and national security agencies have argued that the spread of cryptography has hindered access to evidence and intelligence. Some in industry and government now advocate a new technology to access targeted data: client-side scanning (CSS). Instead of weakening encryption or providing law enforcement with backdoor keys to decrypt communications, CSS would enable on-device analysis of data in the clear. If targeted information were detected, its existence and, potentially, its source, would be revealed to the agencies; otherwise, little or no information would leave the client device. Its proponents claim that CSS is a solution to the encryption versus public safety debate: it offers privacy -- in the sense of unimpeded end-to-end encryption -- and the ability to successfully investigate serious crime. In this report, we argue that CSS neither guarantees efficacious crime prevention nor prevents surveillance. Indeed, the effect is the opposite. CSS by its nature creates serious security and privacy risks for all society while the assistance it can provide for law enforcement is at best problematic. There are multiple ways in which client-side scanning can fail, can be evaded, and can be abused.

</details>

<details>

<summary>2021-10-14 15:35:41 - On Adversarial Vulnerability of PHM algorithms: An Initial Study</summary>

- *Weizhong Yan, Zhaoyuan Yang, Jianwei Qiu*

- `2110.07462v1` - [abs](http://arxiv.org/abs/2110.07462v1) - [pdf](http://arxiv.org/pdf/2110.07462v1)

> With proliferation of deep learning (DL) applications in diverse domains, vulnerability of DL models to adversarial attacks has become an increasingly interesting research topic in the domains of Computer Vision (CV) and Natural Language Processing (NLP). DL has also been widely adopted to diverse PHM applications, where data are primarily time-series sensor measurements. While those advanced DL algorithms/models have resulted in an improved PHM algorithms' performance, the vulnerability of those PHM algorithms to adversarial attacks has not drawn much attention in the PHM community. In this paper we attempt to explore the vulnerability of PHM algorithms. More specifically, we investigate the strategies of attacking PHM algorithms by considering several unique characteristics associated with time-series sensor measurements data. We use two real-world PHM applications as examples to validate our attack strategies and to demonstrate that PHM algorithms indeed are vulnerable to adversarial attacks.

</details>

<details>

<summary>2021-10-14 18:52:39 - Interactive Analysis of CNN Robustness</summary>

- *Stefan Sietzen, Mathias Lechner, Judy Borowski, Ramin Hasani, Manuela Waldner*

- `2110.07667v1` - [abs](http://arxiv.org/abs/2110.07667v1) - [pdf](http://arxiv.org/pdf/2110.07667v1)

> While convolutional neural networks (CNNs) have found wide adoption as state-of-the-art models for image-related tasks, their predictions are often highly sensitive to small input perturbations, which the human vision is robust against. This paper presents Perturber, a web-based application that allows users to instantaneously explore how CNN activations and predictions evolve when a 3D input scene is interactively perturbed. Perturber offers a large variety of scene modifications, such as camera controls, lighting and shading effects, background modifications, object morphing, as well as adversarial attacks, to facilitate the discovery of potential vulnerabilities. Fine-tuned model versions can be directly compared for qualitative evaluation of their robustness. Case studies with machine learning experts have shown that Perturber helps users to quickly generate hypotheses about model vulnerabilities and to qualitatively compare model behavior. Using quantitative analyses, we could replicate users' insights with other CNN architectures and input images, yielding new insights about the vulnerability of adversarially trained models.

</details>

<details>

<summary>2021-10-14 23:36:00 - Assessing Risks and Modeling Threats in the Internet of Things</summary>

- *Paul Griffioen, Bruno Sinopoli*

- `2110.07771v1` - [abs](http://arxiv.org/abs/2110.07771v1) - [pdf](http://arxiv.org/pdf/2110.07771v1)

> Threat modeling and risk assessments are common ways to identify, estimate, and prioritize risk to national, organizational, and individual operations and assets. Several threat modeling and risk assessment approaches have been proposed prior to the advent of the Internet of Things (IoT) that focus on threats and risks in information technology (IT). Due to shortcomings in these approaches and the fact that there are significant differences between the IoT and IT, we synthesize and adapt these approaches to provide a threat modeling framework that focuses on threats and risks in the IoT. In doing so, we develop an IoT attack taxonomy that describes the adversarial assets, adversarial actions, exploitable vulnerabilities, and compromised properties that are components of any IoT attack. We use this IoT attack taxonomy as the foundation for designing a joint risk assessment and maturity assessment framework that is implemented as an interactive online tool. The assessment framework this tool encodes provides organizations with specific recommendations about where resources should be devoted to mitigate risk. The usefulness of this IoT framework is highlighted by case study implementations in the context of multiple industrial manufacturing companies, and the interactive implementation of this framework is available at http://iotrisk.andrew.cmu.edu.

</details>

<details>

<summary>2021-10-15 01:45:31 - Adversarial Purification through Representation Disentanglement</summary>

- *Tao Bai, Jun Zhao, Lanqing Guo, Bihan Wen*

- `2110.07801v1` - [abs](http://arxiv.org/abs/2110.07801v1) - [pdf](http://arxiv.org/pdf/2110.07801v1)

> Deep learning models are vulnerable to adversarial examples and make incomprehensible mistakes, which puts a threat on their real-world deployment. Combined with the idea of adversarial training, preprocessing-based defenses are popular and convenient to use because of their task independence and good generalizability. Current defense methods, especially purification, tend to remove ``noise" by learning and recovering the natural images. However, different from random noise, the adversarial patterns are much easier to be overfitted during model training due to their strong correlation to the images. In this work, we propose a novel adversarial purification scheme by presenting disentanglement of natural images and adversarial perturbations as a preprocessing defense. With extensive experiments, our defense is shown to be generalizable and make significant protection against unseen strong adversarial attacks. It reduces the success rates of state-of-the-art \textbf{ensemble} attacks from \textbf{61.7\%} to \textbf{14.9\%} on average, superior to a number of existing methods. Notably, our defense restores the perturbed images perfectly and does not hurt the clean accuracy of backbone models, which is highly desirable in practice.

</details>

<details>

<summary>2021-10-15 05:23:15 - A Shuffling Framework for Local Differential Privacy</summary>

- *Casey Meehan, Amrita Roy Chowdhury, Kamalika Chaudhuri, Somesh Jha*

- `2106.06603v2` - [abs](http://arxiv.org/abs/2106.06603v2) - [pdf](http://arxiv.org/pdf/2106.06603v2)

> ldp deployments are vulnerable to inference attacks as an adversary can link the noisy responses to their identity and subsequently, auxiliary information using the order of the data. An alternative model, shuffle DP, prevents this by shuffling the noisy responses uniformly at random. However, this limits the data learnability -- only symmetric functions (input order agnostic) can be learned. In this paper, we strike a balance and show that systematic shuffling of the noisy responses can thwart specific inference attacks while retaining some meaningful data learnability. To this end, we propose a novel privacy guarantee, d-sigma-privacy, that captures the privacy of the order of a data sequence. d-sigma-privacy allows tuning the granularity at which the ordinal information is maintained, which formalizes the degree the resistance to inference attacks trading it off with data learnability. Additionally, we propose a novel shuffling mechanism that can achieve \name-privacy and demonstrate the practicality of our mechanism via evaluation on real-world datasets.

</details>

<details>

<summary>2021-10-15 07:20:46 - Breaking Bad? Semantic Versioning and Impact of Breaking Changes in Maven Central</summary>

- *Lina Ochoa, Thomas Degueule, Jean-Rémy Falleri, Jurgen Vinju*

- `2110.07889v1` - [abs](http://arxiv.org/abs/2110.07889v1) - [pdf](http://arxiv.org/pdf/2110.07889v1)

> Just like any software, libraries evolve to incorporate new features, bug fixes, security patches, and refactorings. However, when a library evolves, it may break the contract previously established with its clients by introducing Breaking Changes (BCs) in its API. These changes might trigger compile-time, link-time, or run-time errors in client code. As a result, clients may hesitate to upgrade their dependencies, raising security concerns and making future upgrades even more difficult.Understanding how libraries evolve helps client developers to know which changes to expect and where to expect them, and library developers to understand how they might impact their clients. In the most extensive study to date, Raemaekers et al. investigate to what extent developers of Java libraries hosted on the Maven Central Repository (MCR) follow semantic versioning conventions to signal the introduction of BCs and how these changes impact client projects. Their results suggest that BCs are widespread without regard for semantic versioning, with a significant impact on clients.In this paper, we conduct an external and differentiated replication study of their work. We identify and address some limitations of the original protocol and expand the analysis to a new corpus spanning seven more years of the MCR. We also present a novel static analysis tool for Java bytecode, Maracas, which provides us with: (i) the set of all BCs between two versions of a library; and (ii) the set of locations in client code impacted by individual BCs. Our key findings, derived from the analysis of 119, 879 library upgrades and 293, 817 clients, contrast with the original study and show that 83.4% of these upgrades do comply with semantic versioning. Furthermore, we observe that the tendency to comply with semantic versioning has significantly increased over time. Finally, we find that most BCs affect code that is not used by any client, and that only 7.9% of all clients are affected by BCs. These findings should help (i) library developers to understand and anticipate the impact of their changes; (ii) library users to estimate library upgrading effort and to pick libraries that are less likely to break; and (iii) researchers to better understand the dynamics of library-client co-evolution in Java.

</details>

<details>

<summary>2021-10-15 12:12:41 - Adversarial Attacks on ML Defense Models Competition</summary>

- *Yinpeng Dong, Qi-An Fu, Xiao Yang, Wenzhao Xiang, Tianyu Pang, Hang Su, Jun Zhu, Jiayu Tang, Yuefeng Chen, XiaoFeng Mao, Yuan He, Hui Xue, Chao Li, Ye Liu, Qilong Zhang, Lianli Gao, Yunrui Yu, Xitong Gao, Zhe Zhao, Daquan Lin, Jiadong Lin, Chuanbiao Song, Zihao Wang, Zhennan Wu, Yang Guo, Jiequan Cui, Xiaogang Xu, Pengguang Chen*

- `2110.08042v1` - [abs](http://arxiv.org/abs/2110.08042v1) - [pdf](http://arxiv.org/pdf/2110.08042v1)

> Due to the vulnerability of deep neural networks (DNNs) to adversarial examples, a large number of defense techniques have been proposed to alleviate this problem in recent years. However, the progress of building more robust models is usually hampered by the incomplete or incorrect robustness evaluation. To accelerate the research on reliable evaluation of adversarial robustness of the current defense models in image classification, the TSAIL group at Tsinghua University and the Alibaba Security group organized this competition along with a CVPR 2021 workshop on adversarial machine learning (https://aisecure-workshop.github.io/amlcvpr2021/). The purpose of this competition is to motivate novel attack algorithms to evaluate adversarial robustness more effectively and reliably. The participants were encouraged to develop stronger white-box attack algorithms to find the worst-case robustness of different defenses. This competition was conducted on an adversarial robustness evaluation platform -- ARES (https://github.com/thu-ml/ares), and is held on the TianChi platform (https://tianchi.aliyun.com/competition/entrance/531847/introduction) as one of the series of AI Security Challengers Program. After the competition, we summarized the results and established a new adversarial robustness benchmark at https://ml.cs.tsinghua.edu.cn/ares-bench/, which allows users to upload adversarial attack algorithms and defense models for evaluation.

</details>

<details>

<summary>2021-10-15 14:24:45 - Generating Black-Box Adversarial Examples in Sparse Domain</summary>

- *Hadi Zanddizari, Behnam Zeinali, J. Morris Chang*

- `2101.09324v2` - [abs](http://arxiv.org/abs/2101.09324v2) - [pdf](http://arxiv.org/pdf/2101.09324v2)

> Applications of machine learning (ML) models and convolutional neural networks (CNNs) have been rapidly increased. Although state-of-the-art CNNs provide high accuracy in many applications, recent investigations show that such networks are highly vulnerable to adversarial attacks. The black-box adversarial attack is one type of attack that the attacker does not have any knowledge about the model or the training dataset, but it has some input data set and their labels. In this paper, we propose a novel approach to generate a black-box attack in sparse domain whereas the most important information of an image can be observed. Our investigation shows that large sparse (LaS) components play a critical role in the performance of image classifiers. Under this presumption, to generate adversarial example, we transfer an image into a sparse domain and put a threshold to choose only k LaS components. In contrast to the very recent works that randomly perturb k low frequency (LoF) components, we perturb k LaS components either randomly (query-based) or in the direction of the most correlated sparse signal from a different class. We show that LaS components contain some middle or higher frequency components information which leads fooling image classifiers with a fewer number of queries. We demonstrate the effectiveness of this approach by fooling six state-of-the-art image classifiers, the TensorFlow Lite (TFLite) model of Google Cloud Vision platform, and YOLOv5 model as an object detection algorithm. Mean squared error (MSE) and peak signal to noise ratio (PSNR) are used as quality metrics. We also present a theoretical proof to connect these metrics to the level of perturbation in the sparse domain.

</details>

<details>

<summary>2021-10-15 15:07:43 - Chunked-Cache: On-Demand and Scalable Cache Isolation for Security Architectures</summary>

- *Ghada Dessouky, Alexander Gruler, Pouya Mahmoody, Ahmad-Reza Sadeghi, Emmanuel Stapf*

- `2110.08139v1` - [abs](http://arxiv.org/abs/2110.08139v1) - [pdf](http://arxiv.org/pdf/2110.08139v1)

> Shared cache resources in multi-core processors are vulnerable to cache side-channel attacks. Recently proposed defenses have their own caveats: Randomization-based defenses are vulnerable to the evolving attack algorithms besides relying on weak cryptographic primitives, because they do not fundamentally address the root cause for cache side-channel attacks. Cache partitioning defenses, on the other hand, provide the strict resource partitioning and effectively block all side-channel threats. However, they usually rely on way-based partitioning which is not fine-grained and cannot scale to support a larger number of protection domains, e.g., in trusted execution environment (TEE) security architectures, besides degrading performance and often resulting in cache underutilization.   To overcome the shortcomings of both approaches, we present a novel and flexible set-associative cache partitioning design for TEE architectures, called Chunked-Cache. Chunked-Cache enables an execution context to "carve" out an exclusive configurable chunk of the cache if the execution requires side-channel resilience. If side-channel resilience is not required, mainstream cache resources are freely utilized. Hence, our solution addresses the security-performance trade-off practically by enabling selective and on-demand utilization of side-channel-resilient caches, while providing well-grounded future-proof security guarantees. We show that Chunked-Cache provides side-channel-resilient cache utilization for sensitive code execution, with small hardware overhead, while incurring no performance overhead on the OS. We also show that it outperforms conventional way-based cache partitioning by 43%, while scaling significantly better to support a larger number of protection domains.

</details>

<details>

<summary>2021-10-16 02:10:36 - TESDA: Transform Enabled Statistical Detection of Attacks in Deep Neural Networks</summary>

- *Chandramouli Amarnath, Aishwarya H. Balwani, Kwondo Ma, Abhijit Chatterjee*

- `2110.08447v1` - [abs](http://arxiv.org/abs/2110.08447v1) - [pdf](http://arxiv.org/pdf/2110.08447v1)

> Deep neural networks (DNNs) are now the de facto choice for computer vision tasks such as image classification. However, their complexity and "black box" nature often renders the systems they're deployed in vulnerable to a range of security threats. Successfully identifying such threats, especially in safety-critical real-world applications is thus of utmost importance, but still very much an open problem. We present TESDA, a low-overhead, flexible, and statistically grounded method for {online detection} of attacks by exploiting the discrepancies they cause in the distributions of intermediate layer features of DNNs. Unlike most prior work, we require neither dedicated hardware to run in real-time, nor the presence of a Trojan trigger to detect discrepancies in behavior. We empirically establish our method's usefulness and practicality across multiple architectures, datasets and diverse attacks, consistently achieving detection coverages of above 95% with operation count overheads as low as 1-2%.

</details>

<details>

<summary>2021-10-16 19:00:01 - Combating Informational Denial-of-Service (IDoS) Attacks: Modeling and Mitigation of Attentional Human Vulnerability</summary>

- *Linan Huang, Quanyan Zhu*

- `2108.08255v2` - [abs](http://arxiv.org/abs/2108.08255v2) - [pdf](http://arxiv.org/pdf/2108.08255v2)

> This work proposes a new class of proactive attacks called the Informational Denial-of-Service (IDoS) attacks that exploit the attentional human vulnerability. By generating a large volume of feints, IDoS attacks deplete the cognitive resources of human operators to prevent humans from identifying the real attacks hidden among feints. This work aims to formally define IDoS attacks, quantify their consequences, and develop human-assistive security technologies to mitigate the severity level and risks of IDoS attacks. To this end, we use the semi-Markov process to model the sequential arrivals of feints and real attacks with category labels attached in the associated alerts. The assistive technology strategically manages human attention by highlighting selective alerts periodically to prevent the distraction of other alerts. A data-driven approach is applied to evaluate human performance under different Attention Management (AM) strategies. Under a representative special case, we establish the computational equivalency between two dynamic programming representations to reduce the computation complexity and enable online learning with samples of reduced size and zero delays. A case study corroborates the effectiveness of the learning framework. The numerical results illustrate how AM strategies can alleviate the severity level and the risk of IDoS attacks. Furthermore, the results show that the minimum risk is achieved with a proper level of intentional inattention to alerts, which we refer to as the law of rational risk-reduction inattention.

</details>

<details>

<summary>2021-10-17 08:41:21 - Adapting Membership Inference Attacks to GNN for Graph Classification: Approaches and Implications</summary>

- *Bang Wu, Xiangwen Yang, Shirui Pan, Xingliang Yuan*

- `2110.08760v1` - [abs](http://arxiv.org/abs/2110.08760v1) - [pdf](http://arxiv.org/pdf/2110.08760v1)

> Graph Neural Networks (GNNs) are widely adopted to analyse non-Euclidean data, such as chemical networks, brain networks, and social networks, modelling complex relationships and interdependency between objects. Recently, Membership Inference Attack (MIA) against GNNs raises severe privacy concerns, where training data can be leaked from trained GNN models. However, prior studies focus on inferring the membership of only the components in a graph, e.g., an individual node or edge. How to infer the membership of an entire graph record is yet to be explored.   In this paper, we take the first step in MIA against GNNs for graph-level classification. Our objective is to infer whether a graph sample has been used for training a GNN model. We present and implement two types of attacks, i.e., training-based attacks and threshold-based attacks from different adversarial capabilities. We perform comprehensive experiments to evaluate our attacks in seven real-world datasets using five representative GNN models. Both our attacks are shown effective and can achieve high performance, i.e., reaching over 0.7 attack F1 scores in most cases. Furthermore, we analyse the implications behind the MIA against GNNs. Our findings confirm that GNNs can be even more vulnerable to MIA than the models with non-graph structures. And unlike the node-level classifier, MIAs on graph-level classification tasks are more co-related with the overfitting level of GNNs rather than the statistic property of their training graphs.

</details>

<details>

<summary>2021-10-17 11:39:38 - Adversarial Example Devastation and Detection on Speech Recognition System by Adding Random Noise</summary>

- *Mingyu Dong, Diqun Yan, Yongkang Gong, Rangding Wang*

- `2108.13562v3` - [abs](http://arxiv.org/abs/2108.13562v3) - [pdf](http://arxiv.org/pdf/2108.13562v3)

> An automatic speech recognition (ASR) system based on a deep neural network is vulnerable to attack by an adversarial example, especially if the command-dependent ASR fails. A defense method against adversarial examples is proposed to improve the robustness and security of the ASR system. We propose an algorithm of devastation and detection on adversarial examples that can attack current advanced ASR systems. We choose an advanced text- and command-dependent ASR system as our target, generating adversarial examples by an optimization-based attack on text-dependent ASR and the GA-based algorithm on command-dependent ASR. The method is based on input transformation of adversarial examples. Different random intensities and kinds of noise are added to adversarial examples to devastate the perturbation previously added to normal examples. Experimental results show that the method performs well. For the devastation of examples, the original speech similarity after adding noise can reach 99.68%, the similarity of adversarial examples can reach zero, and the detection rate of adversarial examples can reach 94%.

</details>

<details>

<summary>2021-10-17 20:00:34 - Studying Eventual Connectivity Issues in Android Apps</summary>

- *Camilo Escobar-Velásquez, Alejandro Mazuera-Rozo, Claudia Bedoya, Michael Osorio-Riaño, Mario Linares-Vásquez, Gabriele Bavota*

- `2110.08908v1` - [abs](http://arxiv.org/abs/2110.08908v1) - [pdf](http://arxiv.org/pdf/2110.08908v1)

> Mobile apps have become indispensable for daily life, not only for individuals but also for companies/organizations that offer their services digitally. Inherited by the mobility of devices, there are no limitations regarding the locations or conditions in which apps are being used. For example, apps can be used where no internet connection is available. Therefore, offline-first is a highly desired quality of mobile apps. Accordingly, inappropriate handling of connectivity issues and miss-implementation of good practices lead to bugs and crashes occurrences that reduce the confidence of users on the apps' quality. In this paper, we present the first study on Eventual Connectivity (ECn) issues exhibited by Android apps, by manually inspecting 971 scenarios related to 50 open-source apps. We found 304 instances of ECn issues (6 issues per app, on average) that we organized in a taxonomy of 10 categories. We found that the majority of ECn issues are related to the use of messages not providing correct information to the user about the connectivity status and to the improper use of external libraries/apps to which the check of the connectivity status is delegated. Based on our findings, we distill a list of lessons learned for both practitioners and researchers, indicating directions for future work.

</details>

<details>

<summary>2021-10-18 02:25:54 - An Empirical Study of Protocols in Smart Contracts</summary>

- *Timothy Mou, Michael Coblenz, Jonathan Aldrich*

- `2110.08983v1` - [abs](http://arxiv.org/abs/2110.08983v1) - [pdf](http://arxiv.org/pdf/2110.08983v1)

> Smart contracts are programs that are executed on a blockhain. They have been used for applications in voting, decentralized finance, and supply chain management. However, vulnerabilities in smart contracts have been abused by hackers, leading to financial losses. Understanding state machine protocols in smart contracts has been identified as important to catching common bugs, improving documentation, and optimizing smart contracts. We analyze Solidity smart contracts deployed on the Ethereum blockchain and study the prevalence of protocols and protocol-based bugs, as well as opportunities for gas optimizations.

</details>

<details>

<summary>2021-10-18 07:09:38 - Task Agnostic Continual Learning Using Online Variational Bayes with Fixed-Point Updates</summary>

- *Chen Zeno, Itay Golan, Elad Hoffer, Daniel Soudry*

- `2010.00373v2` - [abs](http://arxiv.org/abs/2010.00373v2) - [pdf](http://arxiv.org/pdf/2010.00373v2)

> Background: Catastrophic forgetting is the notorious vulnerability of neural networks to the changes in the data distribution during learning. This phenomenon has long been considered a major obstacle for using learning agents in realistic continual learning settings. A large body of continual learning research assumes that task boundaries are known during training. However, only a few works consider scenarios in which task boundaries are unknown or not well defined -- task agnostic scenarios. The optimal Bayesian solution for this requires an intractable online Bayes update to the weights posterior. Contributions: We aim to approximate the online Bayes update as accurately as possible. To do so, we derive novel fixed-point equations for the online variational Bayes optimization problem, for multivariate Gaussian parametric distributions. By iterating the posterior through these fixed-point equations, we obtain an algorithm (FOO-VB) for continual learning which can handle non-stationary data distribution using a fixed architecture and without using external memory (i.e. without access to previous data). We demonstrate that our method (FOO-VB) outperforms existing methods in task agnostic scenarios. FOO-VB Pytorch implementation will be available online.

</details>

<details>

<summary>2021-10-18 15:19:35 - Conditional De-Identification of 3D Magnetic Resonance Images</summary>

- *Lennart Alexander Van der Goten, Tobias Hepp, Zeynep Akata, Kevin Smith*

- `2110.09927v1` - [abs](http://arxiv.org/abs/2110.09927v1) - [pdf](http://arxiv.org/pdf/2110.09927v1)

> Privacy protection of medical image data is challenging. Even if metadata is removed, brain scans are vulnerable to attacks that match renderings of the face to facial image databases. Solutions have been developed to de-identify diagnostic scans by obfuscating or removing parts of the face. However, these solutions either fail to reliably hide the patient's identity or are so aggressive that they impair further analyses. We propose a new class of de-identification techniques that, instead of removing facial features, remodels them. Our solution relies on a conditional multi-scale GAN architecture. It takes a patient's MRI scan as input and generates a 3D volume conditioned on the patient's brain, which is preserved exactly, but where the face has been de-identified through remodeling. We demonstrate that our approach preserves privacy far better than existing techniques, without compromising downstream medical analyses. Analyses were run on the OASIS-3 and ADNI corpora.

</details>

<details>

<summary>2021-10-18 22:08:29 - A ground-truth dataset of real security patches</summary>

- *Sofia Reis, Rui Abreu*

- `2110.09635v1` - [abs](http://arxiv.org/abs/2110.09635v1) - [pdf](http://arxiv.org/pdf/2110.09635v1)

> Training machine learning approaches for vulnerability identification and producing reliable tools to assist developers in implementing quality software -- free of vulnerabilities -- is challenging due to the lack of large datasets and real data. Researchers have been looking at these issues and building datasets. However, these datasets usually miss natural language artifacts and programming language diversity. We scraped the entire CVE details database for GitHub references and augmented the data with 3 security-related datasets. We used the data to create a ground-truth dataset of natural language artifacts (such as commit messages, commits comments, and summaries), meta-data and code changes. Our dataset integrates a total of 8057 security-relevant commits -- the equivalent to 5942 security patches -- from 1339 different projects spanning 146 different types of vulnerabilities and 20 languages. A dataset of 110k non-security-related commits is also provided. Data and scripts are all available on GitHub. Data is stored in a .CSV file. Codebases can be downloaded using our scripts. Our dataset is a valuable asset to answer research questions on different topics such as the identification of security-relevant information using NLP models; software engineering and security best practices; and, vulnerability detection and patching; and, security program analysis.

</details>

<details>

<summary>2021-10-19 01:43:41 - Improving Robustness of Reinforcement Learning for Power System Control with Adversarial Training</summary>

- *Alexander Pan, Yongkyun Lee, Huan Zhang, Yize Chen, Yuanyuan Shi*

- `2110.08956v2` - [abs](http://arxiv.org/abs/2110.08956v2) - [pdf](http://arxiv.org/pdf/2110.08956v2)

> Due to the proliferation of renewable energy and its intrinsic intermittency and stochasticity, current power systems face severe operational challenges. Data-driven decision-making algorithms from reinforcement learning (RL) offer a solution towards efficiently operating a clean energy system. Although RL algorithms achieve promising performance compared to model-based control models, there has been limited investigation of RL robustness in safety-critical physical systems. In this work, we first show that several competition-winning, state-of-the-art RL agents proposed for power system control are vulnerable to adversarial attacks. Specifically, we use an adversary Markov Decision Process to learn an attack policy, and demonstrate the potency of our attack by successfully attacking multiple winning agents from the Learning To Run a Power Network (L2RPN) challenge, under both white-box and black-box attack settings. We then propose to use adversarial training to increase the robustness of RL agent against attacks and avoid infeasible operational decisions. To the best of our knowledge, our work is the first to highlight the fragility of grid control RL algorithms, and contribute an effective defense scheme towards improving their robustness and security.

</details>

<details>

<summary>2021-10-19 02:40:34 - Characterizing Improper Input Validation Vulnerabilities of Mobile Crowdsourcing Services</summary>

- *Sojhal Ismail Khan, Dominika Woszczyk, Chengzeng You, Soteris Demetriou, Muhammad Naveed*

- `2110.08517v2` - [abs](http://arxiv.org/abs/2110.08517v2) - [pdf](http://arxiv.org/pdf/2110.08517v2)

> Mobile crowdsourcing services (MCS), enable fast and economical data acquisition at scale and find applications in a variety of domains. Prior work has shown that Foursquare and Waze (a location-based and a navigation MCS) are vulnerable to different kinds of data poisoning attacks. Such attacks can be upsetting and even dangerous especially when they are used to inject improper inputs to mislead users. However, to date, there is no comprehensive study on the extent of improper input validation (IIV) vulnerabilities and the feasibility of their exploits in MCSs across domains. In this work, we leverage the fact that MCS interface with their participants through mobile apps to design tools and new methodologies embodied in an end-to-end feedback-driven analysis framework which we use to study 10 popular and previously unexplored services in five different domains. Using our framework we send tens of thousands of API requests with automatically generated input values to characterize their IIV attack surface. Alarmingly, we found that most of them (8/10) suffer from grave IIV vulnerabilities which allow an adversary to launch data poisoning attacks at scale: 7400 spoofed API requests were successful in faking online posts for robberies, gunshots, and other dangerous incidents, faking fitness activities with supernatural speeds and distances among many others. Lastly, we discuss easy to implement and deploy mitigation strategies which can greatly reduce the IIV attack surface and argue for their use as a necessary complementary measure working toward trustworthy mobile crowdsourcing services.

</details>

<details>

<summary>2021-10-19 10:55:33 - FedParking: A Federated Learning based Parking Space Estimation with Parked Vehicle assisted Edge Computing</summary>

- *Xumin Huang, Peichun Li, Rong Yu, Yuan Wu, Kan Xie, Shengli Xie*

- `2110.12876v1` - [abs](http://arxiv.org/abs/2110.12876v1) - [pdf](http://arxiv.org/pdf/2110.12876v1)

> As a distributed learning approach, federated learning trains a shared learning model over distributed datasets while preserving the training data privacy. We extend the application of federated learning to parking management and introduce FedParking in which Parking Lot Operators (PLOs) collaborate to train a long short-term memory model for parking space estimation without exchanging the raw data. Furthermore, we investigate the management of Parked Vehicle assisted Edge Computing (PVEC) by FedParking. In PVEC, different PLOs recruit PVs as edge computing nodes for offloading services through an incentive mechanism, which is designed according to the computation demand and parking capacity constraints derived from FedParking. We formulate the interactions among the PLOs and vehicles as a multi-lead multi-follower Stackelberg game. Considering the dynamic arrivals of the vehicles and time-varying parking capacity constraints, we present a multi-agent deep reinforcement learning approach to gradually reach the Stackelberg equilibrium in a distributed yet privacy-preserving manner. Finally, numerical results are provided to demonstrate the effectiveness and efficiency of our scheme.

</details>

<details>

<summary>2021-10-19 11:06:06 - Holistic Hardware Security Assessment Framework: A Microarchitectural Perspective</summary>

- *Tochukwu Idika, Ismail Akturk*

- `2110.09849v1` - [abs](http://arxiv.org/abs/2110.09849v1) - [pdf](http://arxiv.org/pdf/2110.09849v1)

> Our goal is to enable holistic hardware security evaluation from the microarchitectural point of view. To achieve this, we propose a framework that categorizes threat models based on the microarchitectural components being targeted, and provides a generic security metric that can be used to assess the vulnerability of components, as well as the system as a whole.

</details>

<details>

<summary>2021-10-19 15:05:35 - PETGEN: Personalized Text Generation Attack on Deep Sequence Embedding-based Classification Models</summary>

- *Bing He, Mustaque Ahamad, Srijan Kumar*

- `2109.06777v2` - [abs](http://arxiv.org/abs/2109.06777v2) - [pdf](http://arxiv.org/pdf/2109.06777v2)

> What should a malicious user write next to fool a detection model? Identifying malicious users is critical to ensure the safety and integrity of internet platforms. Several deep learning-based detection models have been created. However, malicious users can evade deep detection models by manipulating their behavior, rendering these models of little use. The vulnerability of such deep detection models against adversarial attacks is unknown. Here we create a novel adversarial attack model against deep user sequence embedding based classification models, which use the sequence of user posts to generate user embeddings and detect malicious users. In the attack, the adversary generates a new post to fool the classifier. We propose a novel end-to-end Personalized Text Generation Attack model, called PETGEN, that simultaneously reduces the efficacy of the detection model and generates posts that have several key desirable properties. Specifically, PETGEN generates posts that are personalized to the user's writing style, have knowledge about a given target context, are aware of the user's historical posts on the target context, and encapsulate the user's recent topical interests. We conduct extensive experiments on two real-world datasets (Yelp and Wikipedia, both with ground-truth of malicious users) to show that PETGEN significantly reduces the performance of popular deep user sequence embedding-based classification models. PETGEN outperforms five attack baselines in terms of text quality and attack efficacy in both white-box and black-box classifier settings. Overall, this work paves the path towards the next generation of adversary-aware sequence classification models.

</details>

<details>

<summary>2021-10-19 17:03:29 - TESSERACT: Gradient Flip Score to Secure Federated Learning Against Model Poisoning Attacks</summary>

- *Atul Sharma, Wei Chen, Joshua Zhao, Qiang Qiu, Somali Chaterji, Saurabh Bagchi*

- `2110.10108v1` - [abs](http://arxiv.org/abs/2110.10108v1) - [pdf](http://arxiv.org/pdf/2110.10108v1)

> Federated learning---multi-party, distributed learning in a decentralized environment---is vulnerable to model poisoning attacks, even more so than centralized learning approaches. This is because malicious clients can collude and send in carefully tailored model updates to make the global model inaccurate. This motivated the development of Byzantine-resilient federated learning algorithms, such as Krum, Bulyan, FABA, and FoolsGold. However, a recently developed untargeted model poisoning attack showed that all prior defenses can be bypassed. The attack uses the intuition that simply by changing the sign of the gradient updates that the optimizer is computing, for a set of malicious clients, a model can be diverted from the optima to increase the test error rate. In this work, we develop TESSERACT---a defense against this directed deviation attack, a state-of-the-art model poisoning attack. TESSERACT is based on a simple intuition that in a federated learning setting, certain patterns of gradient flips are indicative of an attack. This intuition is remarkably stable across different learning algorithms, models, and datasets. TESSERACT assigns reputation scores to the participating clients based on their behavior during the training phase and then takes a weighted contribution of the clients. We show that TESSERACT provides robustness against even a white-box version of the attack.

</details>

<details>

<summary>2021-10-19 22:14:19 - Multi-concept adversarial attacks</summary>

- *Vibha Belavadi, Yan Zhou, Murat Kantarcioglu, Bhavani M. Thuraisingham*

- `2110.10287v1` - [abs](http://arxiv.org/abs/2110.10287v1) - [pdf](http://arxiv.org/pdf/2110.10287v1)

> As machine learning (ML) techniques are being increasingly used in many applications, their vulnerability to adversarial attacks becomes well-known. Test time attacks, usually launched by adding adversarial noise to test instances, have been shown effective against the deployed ML models. In practice, one test input may be leveraged by different ML models. Test time attacks targeting a single ML model often neglect their impact on other ML models. In this work, we empirically demonstrate that naively attacking the classifier learning one concept may negatively impact classifiers trained to learn other concepts. For example, for the online image classification scenario, when the Gender classifier is under attack, the (wearing) Glasses classifier is simultaneously attacked with the accuracy dropped from 98.69 to 88.42. This raises an interesting question: is it possible to attack one set of classifiers without impacting the other set that uses the same test instance? Answers to the above research question have interesting implications for protecting privacy against ML model misuse. Attacking ML models that pose unnecessary risks of privacy invasion can be an important tool for protecting individuals from harmful privacy exploitation. In this paper, we address the above research question by developing novel attack techniques that can simultaneously attack one set of ML models while preserving the accuracy of the other. In the case of linear classifiers, we provide a theoretical framework for finding an optimal solution to generate such adversarial examples. Using this theoretical framework, we develop a multi-concept attack strategy in the context of deep learning. Our results demonstrate that our techniques can successfully attack the target classes while protecting the protected classes in many different settings, which is not possible with the existing test-time attack-single strategies.

</details>

<details>

<summary>2021-10-19 22:15:42 - A Deeper Look into RowHammer`s Sensitivities: Experimental Analysis of Real DRAM Chips and Implications on Future Attacks and Defenses</summary>

- *Lois Orosa, Abdullah Giray Yağlıkçı, Haocong Luo, Ataberk Olgun, Jisung Park, Hasan Hassan, Minesh Patel, Jeremie S. Kim, Onur Mutlu*

- `2110.10291v1` - [abs](http://arxiv.org/abs/2110.10291v1) - [pdf](http://arxiv.org/pdf/2110.10291v1)

> RowHammer is a circuit-level DRAM vulnerability where repeatedly accessing (i.e., hammering) a DRAM row can cause bit flips in physically nearby rows. The RowHammer vulnerability worsens as DRAM cell size and cell-to-cell spacing shrink. Recent studies demonstrate that modern DRAM chips, including chips previously marketed as RowHammer-safe, are even more vulnerable to RowHammer than older chips such that the required hammer count to cause a bit flip has reduced by more than 10X in the last decade. Therefore, it is essential to develop a better understanding and in-depth insights into the RowHammer vulnerability of modern DRAM chips to more effectively secure current and future systems.   Our goal in this paper is to provide insights into fundamental properties of the RowHammer vulnerability that are not yet rigorously studied by prior works, but can potentially be $i$) exploited to develop more effective RowHammer attacks or $ii$) leveraged to design more effective and efficient defense mechanisms. To this end, we present an experimental characterization using 248~DDR4 and 24~DDR3 modern DRAM chips from four major DRAM manufacturers demonstrating how the RowHammer effects vary with three fundamental properties: 1)~DRAM chip temperature, 2)~aggressor row active time, and 3)~victim DRAM cell's physical location. Among our 16 new observations, we highlight that a RowHammer bit flip 1)~is very likely to occur in a bounded range, specific to each DRAM cell (e.g., 5.4% of the vulnerable DRAM cells exhibit errors in the range 70C to 90C), 2)~is more likely to occur if the aggressor row is active for longer time (e.g., RowHammer vulnerability increases by 36% if we keep a DRAM row active for 15 column accesses), and 3)~is more likely to occur in certain physical regions of the DRAM module under attack (e.g., 5% of the rows are 2x more vulnerable than the remaining 95% of the rows).

</details>

<details>

<summary>2021-10-20 11:11:45 - On the Effectiveness of Clone Detection for Detecting IoT-related Vulnerable Clones</summary>

- *Kentaro Ohno, Norihiro Yoshida, Wenqing Zhu, Hiroaki Takada*

- `2110.10493v1` - [abs](http://arxiv.org/abs/2110.10493v1) - [pdf](http://arxiv.org/pdf/2110.10493v1)

> Since IoT systems provide services over the Internet, they must continue to operate safely even if malicious users attack them. Since the computational resources of edge devices connected to the IoT are limited, lightweight platforms and network protocols are often used. Lightweight platforms and network protocols are less resistant to attacks, increasing the risk that developers will embed vulnerabilities. The code clone research community has been developing approaches to fix buggy (e.g., vulnerable) clones simultaneously. However, there has been little research on IoT-related vulnerable clones. It is unclear whether existing code clone detection techniques can perform simultaneous fixes of the vulnerable clones. In this study, we first created two datasets of IoT-related vulnerable code. We then conducted a preliminary investigation to show whether existing code clone detection tools (e.g., NiCaD, CCFinderSW) are capable of detecting IoT-related vulnerable clones by applying them to the created datasets. The preliminary result shows that the existing tools can detect them partially.

</details>

<details>

<summary>2021-10-20 11:54:12 - Evaluation of Hyperparameter-Optimization Approaches in an Industrial Federated Learning System</summary>

- *Stephanie Holly, Thomas Hiessl, Safoura Rezapour Lakani, Daniel Schall, Clemens Heitzinger, Jana Kemnitz*

- `2110.08202v2` - [abs](http://arxiv.org/abs/2110.08202v2) - [pdf](http://arxiv.org/pdf/2110.08202v2)

> Federated Learning (FL) decouples model training from the need for direct access to the data and allows organizations to collaborate with industry partners to reach a satisfying level of performance without sharing vulnerable business information. The performance of a machine learning algorithm is highly sensitive to the choice of its hyperparameters. In an FL setting, hyperparameter optimization poses new challenges. In this work, we investigated the impact of different hyperparameter optimization approaches in an FL system. In an effort to reduce communication costs, a critical bottleneck in FL, we investigated a local hyperparameter optimization approach that -- in contrast to a global hyperparameter optimization approach -- allows every client to have its own hyperparameter configuration. We implemented these approaches based on grid search and Bayesian optimization and evaluated the algorithms on the MNIST data set using an i.i.d. partition and on an Internet of Things (IoT) sensor based industrial data set using a non-i.i.d. partition.

</details>

<details>

<summary>2021-10-20 12:21:04 - Detecting and Identifying Optical Signal Attacks on Autonomous Driving Systems</summary>

- *Jindi Zhang, Yifan Zhang, Kejie Lu, Jianping Wang, Kui Wu, Xiaohua Jia, Bin Liu*

- `2110.10523v1` - [abs](http://arxiv.org/abs/2110.10523v1) - [pdf](http://arxiv.org/pdf/2110.10523v1)

> For autonomous driving, an essential task is to detect surrounding objects accurately. To this end, most existing systems use optical devices, including cameras and light detection and ranging (LiDAR) sensors, to collect environment data in real time. In recent years, many researchers have developed advanced machine learning models to detect surrounding objects. Nevertheless, the aforementioned optical devices are vulnerable to optical signal attacks, which could compromise the accuracy of object detection. To address this critical issue, we propose a framework to detect and identify sensors that are under attack. Specifically, we first develop a new technique to detect attacks on a system that consists of three sensors. Our main idea is to: 1) use data from three sensors to obtain two versions of depth maps (i.e., disparity) and 2) detect attacks by analyzing the distribution of disparity errors. In our study, we use real data sets and the state-of-the-art machine learning model to evaluate our attack detection scheme and the results confirm the effectiveness of our detection method. Based on the detection scheme, we further develop an identification model that is capable of identifying up to n-2 attacked sensors in a system with one LiDAR and n cameras. We prove the correctness of our identification scheme and conduct experiments to show the accuracy of our identification method. Finally, we investigate the overall sensitivity of our framework.

</details>

<details>

<summary>2021-10-20 14:14:17 - Provably adaptive reinforcement learning in metric spaces</summary>

- *Tongyi Cao, Akshay Krishnamurthy*

- `2006.10875v2` - [abs](http://arxiv.org/abs/2006.10875v2) - [pdf](http://arxiv.org/pdf/2006.10875v2)

> We study reinforcement learning in continuous state and action spaces endowed with a metric. We provide a refined analysis of a variant of the algorithm of Sinclair, Banerjee, and Yu (2019) and show that its regret scales with the \emph{zooming dimension} of the instance. This parameter, which originates in the bandit literature, captures the size of the subsets of near optimal actions and is always smaller than the covering dimension used in previous analyses. As such, our results are the first provably adaptive guarantees for reinforcement learning in metric spaces.

</details>

<details>

<summary>2021-10-21 06:48:35 - PipAttack: Poisoning Federated Recommender Systems forManipulating Item Promotion</summary>

- *Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Quoc Viet Hung Nguyen, Lizhen Cui*

- `2110.10926v1` - [abs](http://arxiv.org/abs/2110.10926v1) - [pdf](http://arxiv.org/pdf/2110.10926v1)

> Due to the growing privacy concerns, decentralization emerges rapidly in personalized services, especially recommendation. Also, recent studies have shown that centralized models are vulnerable to poisoning attacks, compromising their integrity. In the context of recommender systems, a typical goal of such poisoning attacks is to promote the adversary's target items by interfering with the training dataset and/or process. Hence, a common practice is to subsume recommender systems under the decentralized federated learning paradigm, which enables all user devices to collaboratively learn a global recommender while retaining all the sensitive data locally. Without exposing the full knowledge of the recommender and entire dataset to end-users, such federated recommendation is widely regarded `safe' towards poisoning attacks. In this paper, we present a systematic approach to backdooring federated recommender systems for targeted item promotion. The core tactic is to take advantage of the inherent popularity bias that commonly exists in data-driven recommenders. As popular items are more likely to appear in the recommendation list, our innovatively designed attack model enables the target item to have the characteristics of popular items in the embedding space. Then, by uploading carefully crafted gradients via a small number of malicious users during the model update, we can effectively increase the exposure rate of a target (unpopular) item in the resulted federated recommender. Evaluations on two real-world datasets show that 1) our attack model significantly boosts the exposure rate of the target item in a stealthy way, without harming the accuracy of the poisoned recommender; and 2) existing defenses are not effective enough, highlighting the need for new defenses against our local model poisoning attacks to federated recommender systems.

</details>

<details>

<summary>2021-10-21 17:18:52 - Physical Side-Channel Attacks on Embedded Neural Networks: A Survey</summary>

- *Maria Méndez Real, Rubén Salvador*

- `2110.11290v1` - [abs](http://arxiv.org/abs/2110.11290v1) - [pdf](http://arxiv.org/pdf/2110.11290v1)

> During the last decade, Deep Neural Networks (DNN) have progressively been integrated on all types of platforms, from data centers to embedded systems including low-power processors and, recently, FPGAs. Neural Networks (NN) are expected to become ubiquitous in IoT systems by transforming all sorts of real-world applications, including applications in the safety-critical and security-sensitive domains. However, the underlying hardware security vulnerabilities of embedded NN implementations remain unaddressed. In particular, embedded DNN implementations are vulnerable to Side-Channel Analysis (SCA) attacks, which are especially important in the IoT and edge computing contexts where an attacker can usually gain physical access to the targeted device. A research field has therefore emerged and is rapidly growing in terms of the use of SCA including timing, electromagnetic attacks and power attacks to target NN embedded implementations. Since 2018, research papers have shown that SCA enables an attacker to recover inference models architectures and parameters, to expose industrial IP and endangers data confidentiality and privacy. Without a complete review of this emerging field in the literature so far, this paper surveys state-of-the-art physical SCA attacks relative to the implementation of embedded DNNs on micro-controllers and FPGAs in order to provide a thorough analysis on the current landscape. It provides a taxonomy and a detailed classification of current attacks. It first discusses mitigation techniques and then provides insights for future research leads.

</details>

<details>

<summary>2021-10-22 04:08:42 - PRECAD: Privacy-Preserving and Robust Federated Learning via Crypto-Aided Differential Privacy</summary>

- *Xiaolan Gu, Ming Li, Li Xiong*

- `2110.11578v1` - [abs](http://arxiv.org/abs/2110.11578v1) - [pdf](http://arxiv.org/pdf/2110.11578v1)

> Federated Learning (FL) allows multiple participating clients to train machine learning models collaboratively by keeping their datasets local and only exchanging model updates. Existing FL protocol designs have been shown to be vulnerable to attacks that aim to compromise data privacy and/or model robustness. Recently proposed defenses focused on ensuring either privacy or robustness, but not both. In this paper, we develop a framework called PRECAD, which simultaneously achieves differential privacy (DP) and enhances robustness against model poisoning attacks with the help of cryptography. Using secure multi-party computation (MPC) techniques (e.g., secret sharing), noise is added to the model updates by the honest-but-curious server(s) (instead of each client) without revealing clients' inputs, which achieves the benefit of centralized DP in terms of providing a better privacy-utility tradeoff than local DP based solutions. Meanwhile, a crypto-aided secure validation protocol is designed to verify that the contribution of model update from each client is bounded without leaking privacy. We show analytically that the noise added to ensure DP also provides enhanced robustness against malicious model submissions. We experimentally demonstrate that our PRECAD framework achieves higher privacy-utility tradeoff and enhances robustness for the trained models.

</details>

<details>

<summary>2021-10-22 06:43:09 - Understanding and Achieving Efficient Robustness with Adversarial Supervised Contrastive Learning</summary>

- *Anh Bui, Trung Le, He Zhao, Paul Montague, Seyit Camtepe, Dinh Phung*

- `2101.10027v3` - [abs](http://arxiv.org/abs/2101.10027v3) - [pdf](http://arxiv.org/pdf/2101.10027v3)

> Contrastive learning (CL) has recently emerged as an effective approach to learning representation in a range of downstream tasks. Central to this approach is the selection of positive (similar) and negative (dissimilar) sets to provide the model the opportunity to `contrast' between data and class representation in the latent space. In this paper, we investigate CL for improving model robustness using adversarial samples. We first designed and performed a comprehensive study to understand how adversarial vulnerability behaves in the latent space. Based on this empirical evidence, we propose an effective and efficient supervised contrastive learning to achieve model robustness against adversarial attacks. Moreover, we propose a new sample selection strategy that optimizes the positive/negative sets by removing redundancy and improving correlation with the anchor. Extensive experiments show that our Adversarial Supervised Contrastive Learning (ASCL) approach achieves comparable performance with the state-of-the-art defenses while significantly outperforms other CL-based defense methods by using only $42.8\%$ positives and $6.3\%$ negatives.

</details>

<details>

<summary>2021-10-22 08:41:07 - RULF: Rust Library Fuzzing via API Dependency Graph Traversal</summary>

- *Jianfeng Jiang, Hui Xu, Yangfan Zhou*

- `2104.12064v3` - [abs](http://arxiv.org/abs/2104.12064v3) - [pdf](http://arxiv.org/pdf/2104.12064v3)

> Robustness is a key concern for Rust library development because Rust promises no risks of undefined behaviors if developers use safe APIs only. Fuzzing is a practical approach for examining the robustness of programs. However, existing fuzzing tools are not directly applicable to library APIs due to the absence of fuzz targets. It mainly relies on human efforts to design fuzz targets case by case which is labor-intensive. To address this problem, this paper proposes a novel automated fuzz target generation approach for fuzzing Rust libraries via API dependency graph traversal. We identify several essential requirements for library fuzzing, including validity and effectiveness of fuzz targets, high API coverage, and efficiency. To meet these requirements, we first employ breadth-first search with pruning to find API sequences under a length threshold, then we backward search longer sequences for uncovered APIs, and finally we optimize the sequence set as a set covering problem. We implement our fuzz target generator and conduct fuzzing experiments with AFL++ on several real-world popular Rust projects. Our tool finally generates 7 to 118 fuzz targets for each library with API coverage up to 0.92. We exercise each target with a threshold of 24 hours and find 30 previously-unknown bugs from seven libraries.

</details>

<details>

<summary>2021-10-22 12:26:25 - Semantic Detection of Potential Wind-borne Debris in Construction Jobsites: Digital Twining for Hurricane Preparedness and Jobsite Safety</summary>

- *Mirsalar Kamari, Youngjib Ham*

- `2110.12968v1` - [abs](http://arxiv.org/abs/2110.12968v1) - [pdf](http://arxiv.org/pdf/2110.12968v1)

> In the United States, hurricanes are the most devastating natural disasters causing billions of dollars worth of damage every year. More importantly, construction jobsites are classified among the most vulnerable environments to severe wind events. During hurricanes, unsecured and incomplete elements of construction sites, such as scaffoldings, plywoods, and metal rods, will become the potential wind-borne debris, causing cascading damages to the construction projects and the neighboring communities. Thus, it is no wonder that construction firms implement jobsite emergency plans to enforce preparedness responses before extreme weather events. However, relying on checklist-based emergency action plans to carry out a thorough hurricane preparedness is challenging in large-scale and complex site environments. For enabling systematic responses for hurricane preparedness, we have proposed a vision-based technique to identify and analyze the potential wind-borne debris in construction jobsites. Building on this, this paper demonstrates the fidelity of a new machine vision-based method to support construction site hurricane preparedness and further discuss its implications. The outcomes indicate that the convenience of visual data collection and the advantages of the machine vision-based frameworks enable rapid scene understanding and thus, provide critical heads up for practitioners to recognize and localize the potential wind-borne derbies in construction jobsites and effectively implement hurricane preparedness.

</details>

<details>

<summary>2021-10-22 18:37:18 - SoK: Securing Email -- A Stakeholder-Based Analysis (Extended Version)</summary>

- *Jeremy Clark, P. C. van Oorschot, Scott Ruoti, Kent Seamons, Daniel Zappala*

- `1804.07706v3` - [abs](http://arxiv.org/abs/1804.07706v3) - [pdf](http://arxiv.org/pdf/1804.07706v3)

> While email is the most ubiquitous and interoperable form of online communication today, it was not conceived with strong security guarantees, and the ensuing security enhancements are, by contrast, lacking in both ubiquity and interoperability. This situation motivates our research. We begin by identifying a variety of stakeholders who have an interest in the current email system and in efforts to provide secure solutions. We then use the tussle among stakeholders to explain the evolution of fragmented secure email solutions undertaken by industry, academia, and independent developers. We also evaluate the building blocks of secure email -- cryptographic primitives, key management schemes, and system designs -- to identify their support for stakeholder properties. From our analysis, we conclude that a one-size-fits-all solution is unlikely. Furthermore, we highlight that vulnerable users are not well served by current solutions, account for the failure of PGP, and argue that secure messaging, while complementary, is not a fully substitutable technology.

</details>

<details>

<summary>2021-10-22 19:10:27 - Fairness Degrading Adversarial Attacks Against Clustering Algorithms</summary>

- *Anshuman Chhabra, Adish Singla, Prasant Mohapatra*

- `2110.12020v1` - [abs](http://arxiv.org/abs/2110.12020v1) - [pdf](http://arxiv.org/pdf/2110.12020v1)

> Clustering algorithms are ubiquitous in modern data science pipelines, and are utilized in numerous fields ranging from biology to facility location. Due to their widespread use, especially in societal resource allocation problems, recent research has aimed at making clustering algorithms fair, with great success. Furthermore, it has also been shown that clustering algorithms, much like other machine learning algorithms, are susceptible to adversarial attacks where a malicious entity seeks to subvert the performance of the learning algorithm. However, despite these known vulnerabilities, there has been no research undertaken that investigates fairness degrading adversarial attacks for clustering. We seek to bridge this gap by formulating a generalized attack optimization problem aimed at worsening the group-level fairness of centroid-based clustering algorithms. As a first step, we propose a fairness degrading attack algorithm for k-median clustering that operates under a whitebox threat model -- where the clustering algorithm, fairness notion, and the input dataset are known to the adversary. We provide empirical results as well as theoretical analysis for our simple attack algorithm, and find that the addition of the generated adversarial samples can lead to significantly lower fairness values. In this manner, we aim to motivate fairness degrading adversarial attacks as a direction for future research in fair clustering.

</details>

<details>

<summary>2021-10-23 22:11:30 - A Layer-wise Adversarial-aware Quantization Optimization for Improving Robustness</summary>

- *Chang Song, Riya Ranjan, Hai Li*

- `2110.12308v1` - [abs](http://arxiv.org/abs/2110.12308v1) - [pdf](http://arxiv.org/pdf/2110.12308v1)

> Neural networks are getting better accuracy with higher energy and computational cost. After quantization, the cost can be greatly saved, and the quantized models are more hardware friendly with acceptable accuracy loss. On the other hand, recent research has found that neural networks are vulnerable to adversarial attacks, and the robustness of a neural network model can only be improved with defense methods, such as adversarial training. In this work, we find that adversarially-trained neural networks are more vulnerable to quantization loss than plain models. To minimize both the adversarial and the quantization losses simultaneously and to make the quantized model robust, we propose a layer-wise adversarial-aware quantization method, using the Lipschitz constant to choose the best quantization parameter settings for a neural network. We theoretically derive the losses and prove the consistency of our metric selection. The experiment results show that our method can effectively and efficiently improve the robustness of quantized adversarially-trained neural networks.

</details>

<details>

<summary>2021-10-24 00:25:09 - ADC: Adversarial attacks against object Detection that evade Context consistency checks</summary>

- *Mingjun Yin, Shasha Li, Chengyu Song, M. Salman Asif, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy*

- `2110.12321v1` - [abs](http://arxiv.org/abs/2110.12321v1) - [pdf](http://arxiv.org/pdf/2110.12321v1)

> Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial examples, which are slightly perturbed input images which lead DNNs to make wrong predictions. To protect from such examples, various defense strategies have been proposed. A very recent defense strategy for detecting adversarial examples, that has been shown to be robust to current attacks, is to check for intrinsic context consistencies in the input data, where context refers to various relationships (e.g., object-to-object co-occurrence relationships) in images. In this paper, we show that even context consistency checks can be brittle to properly crafted adversarial examples and to the best of our knowledge, we are the first to do so. Specifically, we propose an adaptive framework to generate examples that subvert such defenses, namely, Adversarial attacks against object Detection that evade Context consistency checks (ADC). In ADC, we formulate a joint optimization problem which has two attack goals, viz., (i) fooling the object detector and (ii) evading the context consistency check system, at the same time. Experiments on both PASCAL VOC and MS COCO datasets show that examples generated with ADC fool the object detector with a success rate of over 85% in most cases, and at the same time evade the recently proposed context consistency checks, with a bypassing rate of over 80% in most cases. Our results suggest that how to robustly model context and check its consistency, is still an open problem.

</details>

<details>

<summary>2021-10-25 02:54:14 - Rallying Adversarial Techniques against Deep Learning for Network Security</summary>

- *Joseph Clements, Yuzhe Yang, Ankur Sharma, Hongxin Hu, Yingjie Lao*

- `1903.11688v2` - [abs](http://arxiv.org/abs/1903.11688v2) - [pdf](http://arxiv.org/pdf/1903.11688v2)

> Recent advances in artificial intelligence and the increasing need for powerful defensive measures in the domain of network security, have led to the adoption of deep learning approaches for use in network intrusion detection systems. These methods have achieved superior performance against conventional network attacks, which enable the deployment of practical security systems to unique and dynamic sectors. Adversarial machine learning, unfortunately, has recently shown that deep learning models are inherently vulnerable to adversarial modifications on their input data. Because of this susceptibility, the deep learning models deployed to power a network defense could in fact be the weakest entry point for compromising a network system. In this paper, we show that by modifying on average as little as 1.38 of the input features, an adversary can generate malicious inputs which effectively fool a deep learning based NIDS. Therefore, when designing such systems, it is crucial to consider the performance from not only the conventional network security perspective but also the adversarial machine learning domain.

</details>

<details>

<summary>2021-10-25 13:37:23 - Generating Watermarked Adversarial Texts</summary>

- *Mingjie Li, Hanzhou Wu, Xinpeng Zhang*

- `2110.12948v1` - [abs](http://arxiv.org/abs/2110.12948v1) - [pdf](http://arxiv.org/pdf/2110.12948v1)

> Adversarial example generation has been a hot spot in recent years because it can cause deep neural networks (DNNs) to misclassify the generated adversarial examples, which reveals the vulnerability of DNNs, motivating us to find good solutions to improve the robustness of DNN models. Due to the extensiveness and high liquidity of natural language over the social networks, various natural language based adversarial attack algorithms have been proposed in the literature. These algorithms generate adversarial text examples with high semantic quality. However, the generated adversarial text examples may be maliciously or illegally used. In order to tackle with this problem, we present a general framework for generating watermarked adversarial text examples. For each word in a given text, a set of candidate words are determined to ensure that all the words in the set can be used to either carry secret bits or facilitate the construction of adversarial example. By applying a word-level adversarial text generation algorithm, the watermarked adversarial text example can be finally generated. Experiments show that the adversarial text examples generated by the proposed method not only successfully fool advanced DNN models, but also carry a watermark that can effectively verify the ownership and trace the source of the adversarial examples. Moreover, the watermark can still survive after attacked with adversarial example generation algorithms, which has shown the applicability and superiority.

</details>

<details>

<summary>2021-10-25 14:09:45 - Stable Neural ODE with Lyapunov-Stable Equilibrium Points for Defending Against Adversarial Attacks</summary>

- *Qiyu Kang, Yang Song, Qinxu Ding, Wee Peng Tay*

- `2110.12976v1` - [abs](http://arxiv.org/abs/2110.12976v1) - [pdf](http://arxiv.org/pdf/2110.12976v1)

> Deep neural networks (DNNs) are well-known to be vulnerable to adversarial attacks, where malicious human-imperceptible perturbations are included in the input to the deep network to fool it into making a wrong classification. Recent studies have demonstrated that neural Ordinary Differential Equations (ODEs) are intrinsically more robust against adversarial attacks compared to vanilla DNNs. In this work, we propose a stable neural ODE with Lyapunov-stable equilibrium points for defending against adversarial attacks (SODEF). By ensuring that the equilibrium points of the ODE solution used as part of SODEF is Lyapunov-stable, the ODE solution for an input with a small perturbation converges to the same solution as the unperturbed input. We provide theoretical results that give insights into the stability of SODEF as well as the choice of regularizers to ensure its stability. Our analysis suggests that our proposed regularizers force the extracted feature points to be within a neighborhood of the Lyapunov-stable equilibrium points of the ODE. SODEF is compatible with many defense methods and can be applied to any neural network's final regressor layer to enhance its stability against adversarial attacks.

</details>

<details>

<summary>2021-10-25 14:30:31 - RoBin: Facilitating the Reproduction of Configuration-Related Vulnerability</summary>

- *Ligeng Chen, Jian Guo, Zhongling He, Dongliang Mu, Bing Mao*

- `2110.12989v1` - [abs](http://arxiv.org/abs/2110.12989v1) - [pdf](http://arxiv.org/pdf/2110.12989v1)

> Vulnerability reproduction paves a way in debugging software failures, which need intensive manual efforts. However, some key factors (e.g., software configuration, trigger method) are often missing, so we can not directly reproduce the failure without extra attempts. Even worse, highly customized configuration options of programs create a barrier for reproducing the vulnerabilities that only appear under some specific combinations of configurations. In this paper, we address the problem mentioned above -- reproducing the configuration-related vulnerability. We try to solve it by proposing a binary similarity-based method to infer the specific building configurations via the binary from crash report. The main challenges are as follows: precise compilation option inference, program configuration inference, and source-code-to-binary matching. To achieve the goal, we implement RoBin, a binary similarity-based building configuration inference tool. To demonstrate the effectiveness, we test RoBin on 21 vulnerable cases upon 4 well-known open-source programs. It shows a strong ability in pinpointing the building configurations causing the vulnerability. The result can help developers reproduce and diagnose the vulnerability, and finally, patch the programs.

</details>

<details>

<summary>2021-10-25 20:30:51 - Beyond $L_p$ clipping: Equalization-based Psychoacoustic Attacks against ASRs</summary>

- *Hadi Abdullah, Muhammad Sajidur Rahman, Christian Peeters, Cassidy Gibson, Washington Garcia, Vincent Bindschaedler, Thomas Shrimpton, Patrick Traynor*

- `2110.13250v1` - [abs](http://arxiv.org/abs/2110.13250v1) - [pdf](http://arxiv.org/pdf/2110.13250v1)

> Automatic Speech Recognition (ASR) systems convert speech into text and can be placed into two broad categories: traditional and fully end-to-end. Both types have been shown to be vulnerable to adversarial audio examples that sound benign to the human ear but force the ASR to produce malicious transcriptions. Of these attacks, only the "psychoacoustic" attacks can create examples with relatively imperceptible perturbations, as they leverage the knowledge of the human auditory system. Unfortunately, existing psychoacoustic attacks can only be applied against traditional models, and are obsolete against the newer, fully end-to-end ASRs. In this paper, we propose an equalization-based psychoacoustic attack that can exploit both traditional and fully end-to-end ASRs. We successfully demonstrate our attack against real-world ASRs that include DeepSpeech and Wav2Letter. Moreover, we employ a user study to verify that our method creates low audible distortion. Specifically, 80 of the 100 participants voted in favor of all our attack audio samples as less noisier than the existing state-of-the-art attack. Through this, we demonstrate both types of existing ASR pipelines can be exploited with minimum degradation to attack audio quality.

</details>

<details>

<summary>2021-10-25 20:58:10 - Memory visualization tool for training neural network</summary>

- *Mahendran N*

- `2110.13264v1` - [abs](http://arxiv.org/abs/2110.13264v1) - [pdf](http://arxiv.org/pdf/2110.13264v1)

> Software developed helps world a better place ranging from system software, open source, application software and so on. Software engineering does have neural network models applied to code suggestion, bug report summarizing and so on to demonstrate their effectiveness at a real SE task. Software and machine learning algorithms combine to make software give better solutions and understanding of environment. In software, there are both generalized applications which helps solve problems for entire world and also some specific applications which helps one particular community. To address the computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. Machine learning algorithms have a greater impact in the world but there is a considerable amount of memory utilization during the process. We propose a new tool for analysis of memory utilized for developing and training deep learning models. Our tool results in visual utilization of memory concurrently. Various parameters affecting the memory utilization are analysed while training. This tool helps in knowing better idea of processes or models which consumes more memory.

</details>

<details>

<summary>2021-10-25 21:52:05 - Secure Sum Outperforms Homomorphic Encryption in (Current) Collaborative Deep Learning</summary>

- *Derian Boer, Stefan Kramer*

- `2006.02894v2` - [abs](http://arxiv.org/abs/2006.02894v2) - [pdf](http://arxiv.org/pdf/2006.02894v2)

> Deep learning (DL) approaches are achieving extraordinary results in a wide range of domains, but often require a massive collection of private data. Hence, methods for training neural networks on the joint data of different data owners, that keep each party's input confidential, are called for. We address a specific setting in federated learning, namely that of deep learning from horizontally distributed data with a limited number of parties, where their vulnerable intermediate results have to be processed in a privacy-preserving manner. This setting can be found in medical and healthcare as well as industrial applications. The predominant scheme for this is based on homomorphic encryption (HE), and it is widely considered to be without alternative. In contrast to this, we demonstrate that a carefully chosen, less complex and computationally less expensive secure sum protocol in conjunction with default secure channels exhibits superior properties in terms of both collusion-resistance and runtime. Finally, we discuss several open research questions in the context of collaborative DL, especially regarding privacy risks caused by joint intermediate results.

</details>

<details>

<summary>2021-10-25 22:33:46 - Reconciling Risk Allocation and Prevalence Estimation in Public Health Using Batched Bandits</summary>

- *Ben Chugg, Daniel E. Ho*

- `2110.13306v1` - [abs](http://arxiv.org/abs/2110.13306v1) - [pdf](http://arxiv.org/pdf/2110.13306v1)

> In many public health settings, there is a perceived tension between allocating resources to known vulnerable areas and learning about the overall prevalence of the problem. Inspired by a door-to-door Covid-19 testing program we helped design, we combine multi-armed bandit strategies and insights from sampling theory to demonstrate how to recover accurate prevalence estimates while continuing to allocate resources to at-risk areas. We use the outbreak of an infectious disease as our running example. The public health setting has several characteristics distinguishing it from typical bandit settings, such as distribution shift (the true disease prevalence is changing with time) and batched sampling (multiple decisions must be made simultaneously). Nevertheless, we demonstrate that several bandit algorithms are capable out-performing greedy resource allocation strategies, which often perform worse than random allocation as they fail to notice outbreaks in new areas.

</details>

<details>

<summary>2021-10-26 03:55:20 - Ensemble Federated Adversarial Training with Non-IID data</summary>

- *Shuang Luo, Didi Zhu, Zexi Li, Chao Wu*

- `2110.14814v1` - [abs](http://arxiv.org/abs/2110.14814v1) - [pdf](http://arxiv.org/pdf/2110.14814v1)

> Despite federated learning endows distributed clients with a cooperative training mode under the premise of protecting data privacy and security, the clients are still vulnerable when encountering adversarial samples due to the lack of robustness. The adversarial samples can confuse and cheat the client models to achieve malicious purposes via injecting elaborate noise into normal input. In this paper, we introduce a novel Ensemble Federated Adversarial Training Method, termed as EFAT, that enables an efficacious and robust coupled training mechanism. Our core idea is to enhance the diversity of adversarial examples through expanding training data with different disturbances generated from other participated clients, which helps adversarial training perform well in Non-IID settings. Experimental results on different Non-IID situations, including feature distribution skew and label distribution skew, show that our proposed method achieves promising results compared with solely combining federated learning with adversarial approaches.

</details>

<details>

<summary>2021-10-26 06:06:31 - PettingZoo: Gym for Multi-Agent Reinforcement Learning</summary>

- *J. K. Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan, Luis Santos, Rodrigo Perez, Caroline Horsch, Clemens Dieffendahl, Niall L. Williams, Yashas Lokesh, Praveen Ravi*

- `2009.14471v7` - [abs](http://arxiv.org/abs/2009.14471v7) - [pdf](http://arxiv.org/pdf/2009.14471v7)

> This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning ("MARL"), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.

</details>

<details>

<summary>2021-10-26 11:29:03 - Adversarial Robustness in Multi-Task Learning: Promises and Illusions</summary>

- *Salah Ghamizi, Maxime Cordy, Mike Papadakis, Yves Le Traon*

- `2110.15053v1` - [abs](http://arxiv.org/abs/2110.15053v1) - [pdf](http://arxiv.org/pdf/2110.15053v1)

> Vulnerability to adversarial attacks is a well-known weakness of Deep Neural networks. While most of the studies focus on single-task neural networks with computer vision datasets, very little research has considered complex multi-task models that are common in real applications. In this paper, we evaluate the design choices that impact the robustness of multi-task deep learning networks. We provide evidence that blindly adding auxiliary tasks, or weighing the tasks provides a false sense of robustness. Thereby, we tone down the claim made by previous research and study the different factors which may affect robustness. In particular, we show that the choice of the task to incorporate in the loss function are important factors that can be leveraged to yield more robust models.

</details>

<details>

<summary>2021-10-26 17:13:35 - FL-WBC: Enhancing Robustness against Model Poisoning Attacks in Federated Learning from a Client Perspective</summary>

- *Jingwei Sun, Ang Li, Louis DiValentin, Amin Hassanzadeh, Yiran Chen, Hai Li*

- `2110.13864v1` - [abs](http://arxiv.org/abs/2110.13864v1) - [pdf](http://arxiv.org/pdf/2110.13864v1)

> Federated learning (FL) is a popular distributed learning framework that trains a global model through iterative communications between a central server and edge devices. Recent works have demonstrated that FL is vulnerable to model poisoning attacks. Several server-based defense approaches (e.g. robust aggregation), have been proposed to mitigate such attacks. However, we empirically show that under extremely strong attacks, these defensive methods fail to guarantee the robustness of FL. More importantly, we observe that as long as the global model is polluted, the impact of attacks on the global model will remain in subsequent rounds even if there are no subsequent attacks. In this work, we propose a client-based defense, named White Blood Cell for Federated Learning (FL-WBC), which can mitigate model poisoning attacks that have already polluted the global model. The key idea of FL-WBC is to identify the parameter space where long-lasting attack effect on parameters resides and perturb that space during local training. Furthermore, we derive a certified robustness guarantee against model poisoning attacks and a convergence guarantee to FedAvg after applying our FL-WBC. We conduct experiments on FasionMNIST and CIFAR10 to evaluate the defense against state-of-the-art model poisoning attacks. The results demonstrate that our method can effectively mitigate model poisoning attack impact on the global model within 5 communication rounds with nearly no accuracy drop under both IID and Non-IID settings. Our defense is also complementary to existing server-based robust aggregation approaches and can further improve the robustness of FL under extremely strong attacks.

</details>

<details>

<summary>2021-10-26 19:33:40 - Adversarial Attacks and Defenses for Social Network Text Processing Applications: Techniques, Challenges and Future Research Directions</summary>

- *Izzat Alsmadi, Kashif Ahmad, Mahmoud Nazzal, Firoj Alam, Ala Al-Fuqaha, Abdallah Khreishah, Abdulelah Algosaibi*

- `2110.13980v1` - [abs](http://arxiv.org/abs/2110.13980v1) - [pdf](http://arxiv.org/pdf/2110.13980v1)

> The growing use of social media has led to the development of several Machine Learning (ML) and Natural Language Processing(NLP) tools to process the unprecedented amount of social media content to make actionable decisions. However, these MLand NLP algorithms have been widely shown to be vulnerable to adversarial attacks. These vulnerabilities allow adversaries to launch a diversified set of adversarial attacks on these algorithms in different applications of social media text processing. In this paper, we provide a comprehensive review of the main approaches for adversarial attacks and defenses in the context of social media applications with a particular focus on key challenges and future research directions. In detail, we cover literature on six key applications, namely (i) rumors detection, (ii) satires detection, (iii) clickbait & spams identification, (iv) hate speech detection, (v)misinformation detection, and (vi) sentiment analysis. We then highlight the concurrent and anticipated future research questions and provide recommendations and directions for future work.

</details>

<details>

<summary>2021-10-26 23:22:45 - DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks</summary>

- *Chong Xiang, Prateek Mittal*

- `2102.02956v3` - [abs](http://arxiv.org/abs/2102.02956v3) - [pdf](http://arxiv.org/pdf/2102.02956v3)

> State-of-the-art object detectors are vulnerable to localized patch hiding attacks, where an adversary introduces a small adversarial patch to make detectors miss the detection of salient objects. The patch attacker can carry out a physical-world attack by printing and attaching an adversarial patch to the victim object. In this paper, we propose DetectorGuard as the first general framework for building provably robust object detectors against localized patch hiding attacks. DetectorGuard is inspired by recent advancements in robust image classification research; we ask: can we adapt robust image classifiers for robust object detection? Unfortunately, due to their task difference, an object detector naively adapted from a robust image classifier 1) may not necessarily be robust in the adversarial setting or 2) even maintain decent performance in the clean setting. To build a high-performance robust object detector, we propose an objectness explaining strategy: we adapt a robust image classifier to predict objectness for every image location and then explain each objectness using the bounding boxes predicted by a conventional object detector. If all objectness is well explained, we output the predictions made by the conventional object detector; otherwise, we issue an attack alert. Notably, 1) in the adversarial setting, we formally prove the end-to-end robustness of DetectorGuard on certified objects, i.e., it either detects the object or triggers an alert, against any patch hiding attacker within our threat model; 2) in the clean setting, we have almost the same performance as state-of-the-art object detectors. Our evaluation on the PASCAL VOC, MS COCO, and KITTI datasets further demonstrates that DetectorGuard achieves the first provable robustness against localized patch hiding attacks at a negligible cost (<1%) of clean performance.

</details>

<details>

<summary>2021-10-27 04:40:11 - Stubbifier: Debloating Dynamic Server-Side JavaScript Applications</summary>

- *Alexi Turcotte, Ellen Arteca, Ashish Mishra, Saba Alimadadi, Frank Tip*

- `2110.14162v1` - [abs](http://arxiv.org/abs/2110.14162v1) - [pdf](http://arxiv.org/pdf/2110.14162v1)

> JavaScript is an increasingly popular language for server-side development, thanks in part to the Node.js runtime environment and its vast ecosystem of modules. With the Node.js package manager npm, users are able to easily include external modules as dependencies in their projects. However, npm installs modules with all of their functionality, even if only a fraction is needed, which causes an undue increase in code size. Eliminating this unused functionality from distributions is desirable, but the sound analysis required to find unused code is difficult due to JavaScript's extreme dynamicity.   We present a fully automatic technique that identifies unused code by constructing static or dynamic call graphs from the application's tests, and replacing code deemed unreachable with either file- or function-level stubs. If a stub is called, it will fetch and execute the original code on-demand, thus relaxing the requirement that the call graph be sound. The technique also provides an optional guarded execution mode to guard application against injection vulnerabilities in untested code that resulted from stub expansion.   This technique is implemented in an open source tool called Stubbifier, which supports the ECMAScript 2019 standard. In an empirical evaluation on 15 Node.js applications and 75 clients of these applications, Stubbifier reduced application size by 56% on average while incurring only minor performance overhead. The evaluation also shows that Stubbifier's guarded execution mode is capable of preventing several known injection vulnerabilities that are manifested in stubbed-out code. Finally, Stubbifier can work alongside bundlers, popular JavaScript tools for bundling an application with its dependencies. For the considered subject applications, we measured an average size reduction of 37% in bundled distributions.

</details>

<details>

<summary>2021-10-27 15:21:44 - Validating Gaussian Process Models with Simulation-Based Calibration</summary>

- *John Mcleod, Fergus Simpson*

- `2110.15049v1` - [abs](http://arxiv.org/abs/2110.15049v1) - [pdf](http://arxiv.org/pdf/2110.15049v1)

> Gaussian process priors are a popular choice for Bayesian analysis of regression problems. However, the implementation of these models can be complex, and ensuring that the implementation is correct can be challenging. In this paper we introduce Gaussian process simulation-based calibration, a procedure for validating the implementation of Gaussian process models and demonstrate the efficacy of this procedure in identifying a bug in existing code. We also present a novel application of this procedure to identify when marginalisation of the model hyperparameters is necessary.

</details>

<details>

<summary>2021-10-27 16:43:13 - Investigating the Relationship Between World Development Indicators and the Occurrence of Disease Outbreaks in the 21st Century: A Case Study</summary>

- *Aboli Marathe, Harsh Sakhrani, Saloni Parekh*

- `2109.09314v2` - [abs](http://arxiv.org/abs/2109.09314v2) - [pdf](http://arxiv.org/pdf/2109.09314v2)

> The timely identification of socio-economic sectors vulnerable to a disease outbreak presents an important challenge to the civic authorities and healthcare workers interested in outbreak mitigation measures. This problem was traditionally solved by studying the aberrances in small-scale healthcare data. In this paper, we leverage data driven models to determine the relationship between the trends of World Development Indicators and occurrence of disease outbreaks using worldwide historical data from 2000-2019, and treat it as a classic supervised classification problem. CART based feature selection was employed in an unorthodox fashion to determine the covariates getting affected by the disease outbreak, thus giving the most vulnerable sectors. The result involves a comprehensive analysis of different classification algorithms and is indicative of the relationship between the disease outbreak occurrence and the magnitudes of various development indicators.

</details>

<details>

<summary>2021-10-28 09:04:32 - Gradient Inversion with Generative Image Prior</summary>

- *Jinwoo Jeon, Jaechang Kim, Kangwook Lee, Sewoong Oh, Jungseul Ok*

- `2110.14962v1` - [abs](http://arxiv.org/abs/2110.14962v1) - [pdf](http://arxiv.org/pdf/2110.14962v1)

> Federated Learning (FL) is a distributed learning framework, in which the local data never leaves clients devices to preserve privacy, and the server trains models on the data via accessing only the gradients of those local data. Without further privacy mechanisms such as differential privacy, this leaves the system vulnerable against an attacker who inverts those gradients to reveal clients sensitive data. However, a gradient is often insufficient to reconstruct the user data without any prior knowledge. By exploiting a generative model pretrained on the data distribution, we demonstrate that data privacy can be easily breached. Further, when such prior knowledge is unavailable, we investigate the possibility of learning the prior from a sequence of gradients seen in the process of FL training. We experimentally show that the prior in a form of generative model is learnable from iterative interactions in FL. Our findings strongly suggest that additional mechanisms are necessary to prevent privacy leakage in FL.

</details>

<details>

<summary>2021-10-28 20:48:25 - Fuzzm: Finding Memory Bugs through Binary-Only Instrumentation and Fuzzing of WebAssembly</summary>

- *Daniel Lehmann, Martin Toldam Torp, Michael Pradel*

- `2110.15433v1` - [abs](http://arxiv.org/abs/2110.15433v1) - [pdf](http://arxiv.org/pdf/2110.15433v1)

> WebAssembly binaries are often compiled from memory-unsafe languages, such as C and C++. Because of WebAssembly's linear memory and missing protection features, e.g., stack canaries, source-level memory vulnerabilities are exploitable in compiled WebAssembly binaries, sometimes even more easily than in native code. This paper addresses the problem of detecting such vulnerabilities through the first binary-only fuzzer for WebAssembly. Our approach, called Fuzzm, combines canary instrumentation to detect overflows and underflows on the stack and the heap, an efficient coverage instrumentation, a WebAssembly VM, and the input generation algorithm of the popular AFL fuzzer. Besides as an oracle for fuzzing, our canaries also serve as a stand-alone binary hardening technique to prevent the exploitation of vulnerable binaries in production. We evaluate Fuzzm with 28 real-world WebAssembly binaries, some compiled from source and some found in the wild without source code. The fuzzer explores thousands of execution paths, triggers dozens of crashes, and performs hundreds of program executions per second. When used for binary hardening, the approach prevents previously published exploits against vulnerable WebAssembly binaries while imposing low runtime overhead.

</details>

<details>

<summary>2021-10-29 04:03:27 - Topological Relational Learning on Graphs</summary>

- *Yuzhou Chen, Baris Coskunuzer, Yulia R. Gel*

- `2110.15529v1` - [abs](http://arxiv.org/abs/2110.15529v1) - [pdf](http://arxiv.org/pdf/2110.15529v1)

> Graph neural networks (GNNs) have emerged as a powerful tool for graph classification and representation learning. However, GNNs tend to suffer from over-smoothing problems and are vulnerable to graph perturbations. To address these challenges, we propose a novel topological neural framework of topological relational inference (TRI) which allows for integrating higher-order graph information to GNNs and for systematically learning a local graph structure. The key idea is to rewire the original graph by using the persistent homology of the small neighborhoods of nodes and then to incorporate the extracted topological summaries as the side information into the local algorithm. As a result, the new framework enables us to harness both the conventional information on the graph structure and information on the graph higher order topological properties. We derive theoretical stability guarantees for the new local topological representation and discuss their implications on the graph algebraic connectivity. The experimental results on node classification tasks demonstrate that the new TRI-GNN outperforms all 14 state-of-the-art baselines on 6 out 7 graphs and exhibit higher robustness to perturbations, yielding up to 10\% better performance under noisy scenarios.

</details>

<details>

<summary>2021-10-29 07:15:39 - Smoke Testing for Machine Learning: Simple Tests to Discover Severe Defects</summary>

- *Steffen Herbold, Tobias Haar*

- `2009.01521v2` - [abs](http://arxiv.org/abs/2009.01521v2) - [pdf](http://arxiv.org/pdf/2009.01521v2)

> Machine learning is nowadays a standard technique for data analysis within software applications. Software engineers need quality assurance techniques that are suitable for these new kinds of systems. Within this article, we discuss the question whether standard software testing techniques that have been part of textbooks since decades are also useful for the testing of machine learning software. Concretely, we try to determine generic and simple smoke tests that can be used to assert that basic functions can be executed without crashing. We found that we can derive such tests using techniques similar to equivalence classes and boundary value analysis. Moreover, we found that these concepts can also be applied to hyperparameters, to further improve the quality of the smoke tests. Even though our approach is almost trivial, we were able to find bugs in all three machine learning libraries that we tested and severe bugs in two of the three libraries. This demonstrates that common software testing techniques are still valid in the age of machine learning and that considerations how they can be adapted to this new context can help to find and prevent severe bugs, even in mature machine learning libraries.

</details>

<details>

<summary>2021-10-29 22:14:37 - Leaking Secrets through Modern Branch Predictor in the Speculative World</summary>

- *Md Hafizul Islam Chowdhuryy, Fan Yao*

- `2107.09833v2` - [abs](http://arxiv.org/abs/2107.09833v2) - [pdf](http://arxiv.org/pdf/2107.09833v2)

> Transient execution attacks that exploit speculation have raised significant concerns in computer systems. Typically, branch predictors are leveraged to trigger mis-speculation in transient execution attacks. In this work, we demonstrate a new class of speculation-based attack that targets branch prediction unit (BPU). We find that speculative resolution of conditional branches (i.e., in nested speculation) alter the states of pattern history table (PHT) in modern processors, which are not restored after the corresponding branches are later squashed. Such characteristic allows attackers to exploit BPU as the secret transmitting medium in transient execution attacks. To evaluate the discovered vulnerability, we build a novel attack framework, BranchSpectre, that enables exfiltration of unintended secrets through observing speculative PHT updates (in the form of covert and side channels). We further investigate PHT collision mechanism in the history-based predictor as well as the branch prediction mode transitions in Intel processors. Built upon such knowledge, we implement an ultra high-speed covert channel (BranchSpectre-cc) as well as two side channels (i.e., BranchSpectre-v1 and BranchSpectre-v2) that merely rely on BPU for mis-speculation trigger and secret inference in the speculative domain. Notably, BranchSpectre side channels can take advantage of much simpler code patterns than the ones used in Spectre attacks. We present an extensive BranchSpectre code gadget analysis on a set of popular real-world application code bases followed by a demonstration of real-world side channel attack on OpenSSL. The evaluation results show substantial wider existence and higher exploitability of BranchSpectre code patterns in real-world software. Finally, we discuss several secure branch prediction mechanisms that can mitigate transient execution attacks exploiting modern branch predictors.

</details>

<details>

<summary>2021-10-30 02:19:08 - DeceFL: A Principled Decentralized Federated Learning Framework</summary>

- *Ye Yuan, Jun Liu, Dou Jin, Zuogong Yue, Ruijuan Chen, Maolin Wang, Chuan Sun, Lei Xu, Feng Hua, Xin He, Xinlei Yi, Tao Yang, Hai-Tao Zhang, Shaochun Sui, Han Ding*

- `2107.07171v2` - [abs](http://arxiv.org/abs/2107.07171v2) - [pdf](http://arxiv.org/pdf/2107.07171v2)

> Traditional machine learning relies on a centralized data pipeline, i.e., data are provided to a central server for model training. In many applications, however, data are inherently fragmented. Such a decentralized nature of these databases presents the biggest challenge for collaboration: sending all decentralized datasets to a central server raises serious privacy concerns. Although there has been a joint effort in tackling such a critical issue by proposing privacy-preserving machine learning frameworks, such as federated learning, most state-of-the-art frameworks are built still in a centralized way, in which a central client is needed for collecting and distributing model information (instead of data itself) from every other client, leading to high communication pressure and high vulnerability when there exists a failure at or attack on the central client. Here we propose a principled decentralized federated learning algorithm (DeceFL), which does not require a central client and relies only on local information transmission between clients and their neighbors, representing a fully decentralized learning framework. It has been further proven that every client reaches the global minimum with zero performance gap and achieves the same convergence rate $O(1/T)$ (where $T$ is the number of iterations in gradient descent) as centralized federated learning when the loss function is smooth and strongly convex. Finally, the proposed algorithm has been applied to a number of applications to illustrate its effectiveness for both convex and nonconvex loss functions, demonstrating its applicability to a wide range of real-world medical and industrial applications.

</details>

<details>

<summary>2021-10-30 05:00:36 - On Quantitative Evaluations of Counterfactuals</summary>

- *Frederik Hvilshøj, Alexandros Iosifidis, Ira Assent*

- `2111.00177v1` - [abs](http://arxiv.org/abs/2111.00177v1) - [pdf](http://arxiv.org/pdf/2111.00177v1)

> As counterfactual examples become increasingly popular for explaining decisions of deep learning models, it is essential to understand what properties quantitative evaluation metrics do capture and equally important what they do not capture. Currently, such understanding is lacking, potentially slowing down scientific progress. In this paper, we consolidate the work on evaluating visual counterfactual examples through an analysis and experiments. We find that while most metrics behave as intended for sufficiently simple datasets, some fail to tell the difference between good and bad counterfactuals when the complexity increases. We observe experimentally that metrics give good scores to tiny adversarial-like changes, wrongly identifying such changes as superior counterfactual examples. To mitigate this issue, we propose two new metrics, the Label Variation Score and the Oracle score, which are both less vulnerable to such tiny changes. We conclude that a proper quantitative evaluation of visual counterfactual examples should combine metrics to ensure that all aspects of good counterfactuals are quantified.

</details>

<details>

<summary>2021-10-30 21:34:12 - Reconstructing Test Labels from Noisy Loss Functions</summary>

- *Abhinav Aggarwal, Shiva Prasad Kasiviswanathan, Zekun Xu, Oluwaseyi Feyisetan, Nathanael Teissier*

- `2107.03022v2` - [abs](http://arxiv.org/abs/2107.03022v2) - [pdf](http://arxiv.org/pdf/2107.03022v2)

> Machine learning classifiers rely on loss functions for performance evaluation, often on a private (hidden) dataset. In a recent line of research, label inference was introduced as the problem of reconstructing the ground truth labels of this private dataset from just the (possibly perturbed) cross-entropy loss function values evaluated at chosen prediction vectors (without any other access to the hidden dataset). In this paper, we formally study the necessary and sufficient conditions under which label inference is possible from \emph{any} (noisy) loss function value. Using tools from analytical number theory, we show that a broad class of commonly used loss functions, including general Bregman divergence-based losses and multiclass cross-entropy with common activation functions like sigmoid and softmax, it is possible to design label inference attacks that succeed even for arbitrary noise levels and using only a single query from the adversary. We formally study the computational complexity of label inference and show that while in general, designing adversarial prediction vectors for these attacks is co-NP-hard, once we have these vectors, the attacks can also be carried out through a lightweight augmentation to any neural network model, making them look benign and hard to detect. The observations in this paper provide a deeper understanding of the vulnerabilities inherent in modern machine learning and could be used for designing future trustworthy ML.

</details>

<details>

<summary>2021-10-31 10:39:04 - The Impact of Knowledge of the Issue of Identification and Authentication on the Information Security of Adolescents in the Virtual Space</summary>

- *Ljerka Luic, Drazenka Svelec-Juricic, Petar Misevic*

- `2111.00460v1` - [abs](http://arxiv.org/abs/2111.00460v1) - [pdf](http://arxiv.org/pdf/2111.00460v1)

> Information security in the context of digital literacy is a digital skill that enables safe and purposeful movement through virtual space. The age limit and frequency of use of the Internet by young generations has been moved back a year due to the COVID-19 pandemic, and the concern for information security of young people is increasingly emphasized. If, and to what extent, knowledge of the issue of identification and authentication affects the information security of high school students aged 16 to 19 in the virtual space, the research question addressed by the authors of this paper was to determine which student behaviors pose a potential danger compromising their information security by establishing a correlation between the variables that determine student behavior and the variables used to examine their level of security in a virtual environment. The research was conducted using a questionnaire on a sample of high school students in the Republic of Croatia, the results of which showed that some students practice behaviors that are potentially dangerous, make them vulnerable and easy targets of cyber predators and attackers, which is why there is cause for concern and a need for a additional education of children of primary and secondary school age in the field of information security in the form of the introduction of the subject Digital Literacy. Based on the results, a model for assessing the level of digital literacy of adolescents that affect information literacy can be designed, but also further related research in the field of information literacy of children and youth can be conducted.

</details>


## 2021-11

<details>

<summary>2021-11-01 01:13:45 - Evaluation of Cache Attacks on Arm Processors and Secure Caches</summary>

- *Shuwen Deng, Nikolay Matyunin, Wenjie Xiong, Stefan Katzenbeisser, Jakub Szefer*

- `2106.14054v2` - [abs](http://arxiv.org/abs/2106.14054v2) - [pdf](http://arxiv.org/pdf/2106.14054v2)

> Timing-based side and covert channels in processor caches continue to be a threat to modern computers. This work shows for the first time a systematic, large-scale analysis of Arm devices and the detailed results of attacks the processors are vulnerable to. Compared to x86, Arm uses different architectures, microarchitectural implementations, cache replacement policies, etc., which affects how attacks can be launched, and how security testing for the vulnerabilities should be done. To evaluate security, this paper presents security benchmarks specifically developed for testing Arm processors and their caches. The benchmarks are themselves evaluated with sensitivity tests, which examine how sensitive the benchmarks are to having a correct configuration in the testing phase. Further, to evaluate a large number of devices, this work leverages a novel approach of using a cloud-based Arm device testbed for architectural and security research on timing channels and runs the benchmarks on 34 different physical devices. In parallel, there has been much interest in secure caches to defend the various attacks. Consequently, this paper also investigates secure cache architectures using the proposed benchmarks. Especially, this paper implements and evaluates the secure PL and RF caches, showing the security of PL and RF caches, but also uncovers new weaknesses.

</details>

<details>

<summary>2021-11-01 04:59:19 - B-DAC: A Decentralized Access Control Framework on Northbound Interface for Securing SDN Using Blockchain</summary>

- *Phan The Duy, Hien Do Hoang, Do Thi Thu Hien, Anh Gia-Tuan Nguyen, Van-Hau Pham*

- `2111.00707v1` - [abs](http://arxiv.org/abs/2111.00707v1) - [pdf](http://arxiv.org/pdf/2111.00707v1)

> Software-Defined Network (SDN) is a new arising terminology of network architecture with outstanding features of orchestration by decoupling the control plane and the data plane in each network element. Even though it brings several benefits, SDN is vulnerable to a diversity of attacks. Abusing the single point of failure in the SDN controller component, hackers can shut down all network operations. More specifics, a malicious OpenFlow application can access to SDN controller to carry out harmful actions without any limitation owing to the lack of the access control mechanism as a standard in the Northbound. The sensitive information about the whole network such as network topology, flow information, and statistics can be gathered and leaked out. Even worse, the entire network can be taken over by the compromised controller. Hence, it is vital to build a scheme of access control for SDN's Northbound. Furthermore, it must also protect the data integrity and availability during data exchange between application and controller. To address such limitations, we introduce B-DAC, a blockchain-based framework for decentralized authentication and fine-grained access control for the Northbound interface to assist administrators in managing and protecting critical resources. With strict policy enforcement, B-DAC can perform decentralized access control for each request to keep network applications under surveillance for preventing over-privileged activities or security policy conflicts. To demonstrate the feasibility of our approach, we also implement a prototype of this framework to evaluate the security impact, effectiveness, and performance through typical use cases.

</details>

<details>

<summary>2021-11-02 08:30:37 - The Security Risk of Lacking Compiler Protection in WebAssembly</summary>

- *Quentin Stiévenart, Coen De Roover, Mohammad Ghafari*

- `2111.01421v1` - [abs](http://arxiv.org/abs/2111.01421v1) - [pdf](http://arxiv.org/pdf/2111.01421v1)

> WebAssembly is increasingly used as the compilation target for cross-platform applications. In this paper, we investigate whether one can rely on the security measures enforced by existing C compilers when compiling C programs to WebAssembly. We compiled 4,469 C programs with known buffer overflow vulnerabilities to x86 code and to WebAssembly, and observed the outcome of the execution of the generated code to differ for 1,088 programs. Through manual inspection, we identified that the root cause for these is the lack of security measures such as stack canaries in the generated WebAssembly: while x86 code crashes upon a stack-based buffer overflow, the corresponding WebAssembly continues to be executed. We conclude that compiling an existing C program to WebAssembly without additional precautions may hamper its security, and we encourage more research in this direction.

</details>

<details>

<summary>2021-11-02 14:08:10 - Further Investigation of the Survivability of Code Technical Debt Items</summary>

- *Ehsan Zabardast, Kwabena Ebo Bennin, Javier Gonzalez-Huerta*

- `2010.05178v2` - [abs](http://arxiv.org/abs/2010.05178v2) - [pdf](http://arxiv.org/pdf/2010.05178v2)

> Context: Technical Debt (TD) discusses the negative impact of sub-optimal decisions to cope with the need-for-speed in software development. Code Technical Debt Items (TDI) are atomic elements of TD that can be observed in code artefacts. Empirical results on open-source systems demonstrated how code-smells, which are just one type of TDIs, are introduced and "survive" during release cycles. However, little is known about whether the results on the survivability of code-smells hold for other types of code TDIs (i.e., bugs and vulnerabilities) and in industrial settings. Goal: Understanding the survivability of code TDIs by conducting an empirical study analysing two industrial cases and 31 open-source systems from Apache Foundation. Method: We analysed 133,670 code TDIs (35,703 from the industrial systems) detected by SonarQube (in 193,196 commits) to assess their survivability using survivability models. Results: In general, code TDIs tend to remain and linger for long periods in open-source systems, whereas they are removed faster in industrial systems. Code TDIs that survive over a certain threshold tend to remain much longer, which confirms previous results. Our results also suggest that bugs tend to be removed faster, while code smells and vulnerabilities tend to survive longer.

</details>

<details>

<summary>2021-11-02 22:47:24 - HASHTAG: Hash Signatures for Online Detection of Fault-Injection Attacks on Deep Neural Networks</summary>

- *Mojan Javaheripi, Farinaz Koushanfar*

- `2111.01932v1` - [abs](http://arxiv.org/abs/2111.01932v1) - [pdf](http://arxiv.org/pdf/2111.01932v1)

> We propose HASHTAG, the first framework that enables high-accuracy detection of fault-injection attacks on Deep Neural Networks (DNNs) with provable bounds on detection performance. Recent literature in fault-injection attacks shows the severe DNN accuracy degradation caused by bit flips. In this scenario, the attacker changes a few weight bits during DNN execution by tampering with the program's DRAM memory. To detect runtime bit flips, HASHTAG extracts a unique signature from the benign DNN prior to deployment. The signature is later used to validate the integrity of the DNN and verify the inference output on the fly. We propose a novel sensitivity analysis scheme that accurately identifies the most vulnerable DNN layers to the fault-injection attack. The DNN signature is then constructed by encoding the underlying weights in the vulnerable layers using a low-collision hash function. When the DNN is deployed, new hashes are extracted from the target layers during inference and compared against the ground-truth signatures. HASHTAG incorporates a lightweight methodology that ensures a low-overhead and real-time fault detection on embedded platforms. Extensive evaluations with the state-of-the-art bit-flip attack on various DNNs demonstrate the competitive advantage of HASHTAG in terms of both attack detection and execution overhead.

</details>

<details>

<summary>2021-11-03 02:47:05 - SO{U}RCERER: Developer-Driven Security Testing Framework for Android Apps</summary>

- *Muhammad Sajidur Rahman, Blas Kojusner, Ryon Kennedy, Prerit Pathak, Lin Qi, Byron Williams*

- `2111.01631v2` - [abs](http://arxiv.org/abs/2111.01631v2) - [pdf](http://arxiv.org/pdf/2111.01631v2)

> Frequently advised secure development recommendations often fall short in practice for app developers. Tool-driven (e.g., using static analysis tools) approaches lack context and domain-specific requirements of an app being tested. App developers struggle to find an actionable and prioritized list of vulnerabilities from a laundry list of security warnings reported by static analysis tools. Process-driven (e.g., applying threat modeling methods) approaches require substantial resources (e.g., security testing team, budget) and security expertise, which small to medium-scale app dev teams could barely afford. To help app developers securing their apps, we propose SO{U}RCERER, a guiding framework for Android app developers for security testing. SO{U}RCERER guides developers to identify domain-specific assets of an app, detect and prioritize vulnerabilities, and mitigate those vulnerabilities based on secure development guidelines. We evaluated SO{U}RCERER with a case study on analyzing and testing 36 Android mobile money apps. We found that by following activities guided by SO{U}RCERER, an app developer could get a concise and actionable list of vulnerabilities (24-61% fewer security warnings produced by SO{U}RCERER than a standalone static analyzer), directly affecting a mobile money app's critical assets, and devise a mitigation plan. Our findings from this preliminary study indicate a viable approach to Android app security testing without being overwhelmingly complex for app developers.

</details>

<details>

<summary>2021-11-03 17:31:56 - HoneyCar: A Framework to Configure Honeypot Vulnerabilities on the Internet of Vehicles</summary>

- *Sakshyam Panda, Stefan Rass, Sotiris Moschoyiannis, Kaitai Liang, George Loukas, Emmanouil Panaousis*

- `2111.02364v1` - [abs](http://arxiv.org/abs/2111.02364v1) - [pdf](http://arxiv.org/pdf/2111.02364v1)

> The Internet of Vehicles (IoV), whereby interconnected vehicles communicate with each other and with road infrastructure on a common network, has promising socio-economic benefits but also poses new cyber-physical threats. Data on vehicular attackers can be realistically gathered through cyber threat intelligence using systems like honeypots. Admittedly, configuring honeypots introduces a trade-off between the level of honeypot-attacker interactions and any incurred overheads and costs for implementing and monitoring these honeypots. We argue that effective deception can be achieved through strategically configuring the honeypots to represent components of the IoV and engage attackers to collect cyber threat intelligence. In this paper, we present HoneyCar, a novel decision support framework for honeypot deception in IoV. HoneyCar builds upon a repository of known vulnerabilities of the autonomous and connected vehicles found in the Common Vulnerabilities and Exposure (CVE) data within the National Vulnerability Database (NVD) to compute optimal honeypot configuration strategies. By taking a game-theoretic approach, we model the adversarial interaction as a repeated imperfect-information zero-sum game in which the IoV network administrator chooses a set of vulnerabilities to offer in a honeypot and a strategic attacker chooses a vulnerability of the IoV to exploit under uncertainty. Our investigation is substantiated by examining two different versions of the game, with and without the re-configuration cost to empower the network administrator to determine optimal honeypot configurations. We evaluate HoneyCar in a realistic use case to support decision makers with determining optimal honeypot configuration strategies for strategic deployment in IoV.

</details>

<details>

<summary>2021-11-03 17:55:51 - A Survey of Machine Learning Algorithms for Detecting Malware in IoT Firmware</summary>

- *Erik Larsen, Korey MacVittie, John Lilly*

- `2111.02388v1` - [abs](http://arxiv.org/abs/2111.02388v1) - [pdf](http://arxiv.org/pdf/2111.02388v1)

> This work explores the use of machine learning techniques on an Internet-of-Things firmware dataset to detect malicious attempts to infect edge devices or subsequently corrupt an entire network. Firmware updates are uncommon in IoT devices; hence, they abound with vulnerabilities. Attacks against such devices can go unnoticed, and users can become a weak point in security. Malware can cause DDoS attacks and even spy on sensitive areas like peoples' homes. To help mitigate this threat, this paper employs a number of machine learning algorithms to classify IoT firmware and the best performing models are reported. In a general comparison, the top three algorithms are Gradient Boosting, Logistic Regression, and Random Forest classifiers. Deep learning approaches including Convolutional and Fully Connected Neural Networks with both experimental and proven successful architectures are also explored.

</details>

<details>

<summary>2021-11-03 20:08:24 - Counterfactual Explanations Can Be Manipulated</summary>

- *Dylan Slack, Sophie Hilgard, Himabindu Lakkaraju, Sameer Singh*

- `2106.02666v2` - [abs](http://arxiv.org/abs/2106.02666v2) - [pdf](http://arxiv.org/pdf/2106.02666v2)

> Counterfactual explanations are emerging as an attractive option for providing recourse to individuals adversely impacted by algorithmic decisions. As they are deployed in critical applications (e.g. law enforcement, financial lending), it becomes important to ensure that we clearly understand the vulnerabilities of these methods and find ways to address them. However, there is little understanding of the vulnerabilities and shortcomings of counterfactual explanations. In this work, we introduce the first framework that describes the vulnerabilities of counterfactual explanations and shows how they can be manipulated. More specifically, we show counterfactual explanations may converge to drastically different counterfactuals under a small perturbation indicating they are not robust. Leveraging this insight, we introduce a novel objective to train seemingly fair models where counterfactual explanations find much lower cost recourse under a slight perturbation. We describe how these models can unfairly provide low-cost recourse for specific subgroups in the data while appearing fair to auditors. We perform experiments on loan and violent crime prediction data sets where certain subgroups achieve up to 20x lower cost recourse under the perturbation. These results raise concerns regarding the dependability of current counterfactual explanation techniques, which we hope will inspire investigations in robust counterfactual explanations.

</details>

<details>

<summary>2021-11-03 20:20:26 - Excess Capacity and Backdoor Poisoning</summary>

- *Naren Sarayu Manoj, Avrim Blum*

- `2109.00685v3` - [abs](http://arxiv.org/abs/2109.00685v3) - [pdf](http://arxiv.org/pdf/2109.00685v3)

> A backdoor data poisoning attack is an adversarial attack wherein the attacker injects several watermarked, mislabeled training examples into a training set. The watermark does not impact the test-time performance of the model on typical data; however, the model reliably errs on watermarked examples.   To gain a better foundational understanding of backdoor data poisoning attacks, we present a formal theoretical framework within which one can discuss backdoor data poisoning attacks for classification problems. We then use this to analyze important statistical and computational issues surrounding these attacks.   On the statistical front, we identify a parameter we call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack. This allows us to argue about the robustness of several natural learning problems to backdoor attacks. Our results favoring the attacker involve presenting explicit constructions of backdoor attacks, and our robustness results show that some natural problem settings cannot yield successful backdoor attacks.   From a computational standpoint, we show that under certain assumptions, adversarial training can detect the presence of backdoors in a training set. We then show that under similar assumptions, two closely related problems we call backdoor filtering and robust generalization are nearly equivalent. This implies that it is both asymptotically necessary and sufficient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors.

</details>

<details>

<summary>2021-11-04 12:59:25 - Unleashing the Tiger: Inference Attacks on Split Learning</summary>

- *Dario Pasquini, Giuseppe Ateniese, Massimo Bernaschi*

- `2012.02670v5` - [abs](http://arxiv.org/abs/2012.02670v5) - [pdf](http://arxiv.org/pdf/2012.02670v5)

> We investigate the security of Split Learning -- a novel collaborative machine learning framework that enables peak performance by requiring minimal resources consumption. In the present paper, we expose vulnerabilities of the protocol and demonstrate its inherent insecurity by introducing general attack strategies targeting the reconstruction of clients' private training sets. More prominently, we show that a malicious server can actively hijack the learning process of the distributed model and bring it into an insecure state that enables inference attacks on clients' data. We implement different adaptations of the attack and test them on various datasets as well as within realistic threat scenarios. We demonstrate that our attack is able to overcome recently proposed defensive techniques aimed at enhancing the security of the split learning protocol. Finally, we also illustrate the protocol's insecurity against malicious clients by extending previously devised attacks for Federated Learning. To make our results reproducible, we made our code available at https://github.com/pasquini-dario/SplitNN_FSHA.

</details>

<details>

<summary>2021-11-04 13:01:20 - Adversarial Attacks on Graph Classification via Bayesian Optimisation</summary>

- *Xingchen Wan, Henry Kenlay, Binxin Ru, Arno Blaas, Michael A. Osborne, Xiaowen Dong*

- `2111.02842v1` - [abs](http://arxiv.org/abs/2111.02842v1) - [pdf](http://arxiv.org/pdf/2111.02842v1)

> Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models.

</details>

<details>

<summary>2021-11-04 13:10:33 - Attacking Deep Reinforcement Learning-Based Traffic Signal Control Systems with Colluding Vehicles</summary>

- *Ao Qu, Yihong Tang, Wei Ma*

- `2111.02845v1` - [abs](http://arxiv.org/abs/2111.02845v1) - [pdf](http://arxiv.org/pdf/2111.02845v1)

> The rapid advancements of Internet of Things (IoT) and artificial intelligence (AI) have catalyzed the development of adaptive traffic signal control systems (ATCS) for smart cities. In particular, deep reinforcement learning (DRL) methods produce the state-of-the-art performance and have great potentials for practical applications. In the existing DRL-based ATCS, the controlled signals collect traffic state information from nearby vehicles, and then optimal actions (e.g., switching phases) can be determined based on the collected information. The DRL models fully "trust" that vehicles are sending the true information to the signals, making the ATCS vulnerable to adversarial attacks with falsified information. In view of this, this paper first time formulates a novel task in which a group of vehicles can cooperatively send falsified information to "cheat" DRL-based ATCS in order to save their total travel time. To solve the proposed task, we develop CollusionVeh, a generic and effective vehicle-colluding framework composed of a road situation encoder, a vehicle interpreter, and a communication mechanism. We employ our method to attack established DRL-based ATCS and demonstrate that the total travel time for the colluding vehicles can be significantly reduced with a reasonable number of learning episodes, and the colluding effect will decrease if the number of colluding vehicles increases. Additionally, insights and suggestions for the real-world deployment of DRL-based ATCS are provided. The research outcomes could help improve the reliability and robustness of the ATCS and better protect the smart mobility systems.

</details>

<details>

<summary>2021-11-04 17:16:50 - Nyx-Net: Network Fuzzing with Incremental Snapshots</summary>

- *Sergej Schumilo, Cornelius Aschermann, Andrea Jemmett, Ali Abbasi, Thorsten Holz*

- `2111.03013v1` - [abs](http://arxiv.org/abs/2111.03013v1) - [pdf](http://arxiv.org/pdf/2111.03013v1)

> Coverage-guided fuzz testing ("fuzzing") has become mainstream and we have observed lots of progress in this research area recently. However, it is still challenging to efficiently test network services with existing coverage-guided fuzzing methods. In this paper, we introduce the design and implementation of Nyx-Net, a novel snapshot-based fuzzing approach that can successfully fuzz a wide range of targets spanning servers, clients, games, and even Firefox's Inter-Process Communication (IPC) interface. Compared to state-of-the-art methods, Nyx-Net improves test throughput by up to 300x and coverage found by up to 70%. Additionally, Nyx-Net is able to find crashes in two of ProFuzzBench's targets that no other fuzzer found previously. When using Nyx-Net to play the game Super Mario, Nyx-Net shows speedups of 10-30x compared to existing work. Under some circumstances, Nyx-Net is even able play "faster than light": solving the level takes less wall-clock time than playing the level perfectly even once. Nyx-Net is able to find previously unknown bugs in servers such as Lighttpd, clients such as MySQL client, and even Firefox's IPC mechanism - demonstrating the strength and versatility of the proposed approach. Lastly, our prototype implementation was awarded a $20.000 bug bounty for enabling fuzzing on previously unfuzzable code in Firefox and solving a long-standing problem at Mozilla.

</details>

<details>

<summary>2021-11-04 19:38:48 - Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods</summary>

- *Peru Bhardwaj, John Kelleher, Luca Costabello, Declan O'Sullivan*

- `2111.03120v1` - [abs](http://arxiv.org/abs/2111.03120v1) - [pdf](http://arxiv.org/pdf/2111.03120v1)

> Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the security vulnerabilities that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model's predictions on test instances. We use these influential triples as adversarial deletions. We further propose a heuristic method to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62% over the baselines.

</details>

<details>

<summary>2021-11-04 23:49:35 - Vulnerability Characterization and Privacy Quantification for Cyber-Physical Systems</summary>

- *Arpan Bhattacharjee, Shahriar Badsha, Md Tamjid Hossain, Charalambos Konstantinou, Xueping Liang*

- `2110.15417v2` - [abs](http://arxiv.org/abs/2110.15417v2) - [pdf](http://arxiv.org/pdf/2110.15417v2)

> Cyber-physical systems (CPS) data privacy protection during sharing, aggregating, and publishing is a challenging problem. Several privacy protection mechanisms have been developed in the literature to protect sensitive data from adversarial analysis and eliminate the risk of re-identifying the original properties of shared data. However, most of the existing solutions have drawbacks, such as (i) lack of a proper vulnerability characterization model to accurately identify where privacy is needed, (ii) ignoring data providers privacy preference, (iii) using uniform privacy protection which may create inadequate privacy for some provider while overprotecting others, and (iv) lack of a comprehensive privacy quantification model assuring data privacy-preservation. To address these issues, we propose a personalized privacy preference framework by characterizing and quantifying the CPS vulnerabilities as well as ensuring privacy. First, we introduce a Standard Vulnerability Profiling Library (SVPL) by arranging the nodes of an energy-CPS from maximum to minimum vulnerable based on their privacy loss. Based on this model, we present our personalized privacy framework (PDP) in which Laplace noise is added based on the individual node's selected privacy preferences. Finally, combining these two proposed methods, we demonstrate that our privacy characterization and quantification model can attain better privacy preservation by eliminating the trade-off between privacy, utility, and risk of losing information.

</details>

<details>

<summary>2021-11-05 15:14:34 - Interpreting Representation Quality of DNNs for 3D Point Cloud Processing</summary>

- *Wen Shen, Qihan Ren, Dongrui Liu, Quanshi Zhang*

- `2111.03549v1` - [abs](http://arxiv.org/abs/2111.03549v1) - [pdf](http://arxiv.org/pdf/2111.03549v1)

> In this paper, we evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. We propose a method to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, we also propose metrics to evaluate the spatial smoothness of encoding 3D structures, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training.

</details>

<details>

<summary>2021-11-05 17:24:15 - Generalization Error of GAN from the Discriminator's Perspective</summary>

- *Hongkang Yang, Weinan E*

- `2107.03633v2` - [abs](http://arxiv.org/abs/2107.03633v2) - [pdf](http://arxiv.org/pdf/2107.03633v2)

> The generative adversarial network (GAN) is a well-known model for learning high-dimensional distributions, but the mechanism for its generalization ability is not understood. In particular, GAN is vulnerable to the memorization phenomenon, the eventual convergence to the empirical distribution. We consider a simplified GAN model with the generator replaced by a density, and analyze how the discriminator contributes to generalization. We show that with early stopping, the generalization error measured by Wasserstein metric escapes from the curse of dimensionality, despite that in the long term, memorization is inevitable. In addition, we present a hardness of learning result for WGAN.

</details>

<details>

<summary>2021-11-06 08:25:49 - How Developers Engineer Test Cases: An Observational Study</summary>

- *Maurício Aniche, Christoph Treude, Andy Zaidman*

- `2103.01783v4` - [abs](http://arxiv.org/abs/2103.01783v4) - [pdf](http://arxiv.org/pdf/2103.01783v4)

> One of the main challenges that developers face when testing their systems lies in engineering test cases that are good enough to reveal bugs. And while our body of knowledge on software testing and automated test case generation is already quite significant, in practice, developers are still the ones responsible for engineering test cases manually. Therefore, understanding the developers' thought- and decision-making processes while engineering test cases is a fundamental step in making developers better at testing software. In this paper, we observe 13 developers thinking-aloud while testing different real-world open-source methods, and use these observations to explain how developers engineer test cases. We then challenge and augment our main findings by surveying 72 software developers on their testing practices. We discuss our results from three different angles. First, we propose a general framework that explains how developers reason about testing. Second, we propose and describe in detail the three different overarching strategies that developers apply when testing. Third, we compare and relate our observations with the existing body of knowledge and propose future studies that would advance our knowledge on the topic.

</details>

<details>

<summary>2021-11-06 11:43:51 - Cryptography Vulnerabilities on HackerOne</summary>

- *Mohammadreza Hazhirpasand, Mohammad Ghafari*

- `2111.03859v1` - [abs](http://arxiv.org/abs/2111.03859v1) - [pdf](http://arxiv.org/pdf/2111.03859v1)

> Previous studies have shown that cryptography is hard for developers to use and misusing cryptography leads to severe security vulnerabilities. We studied relevant vulnerability reports on the HackerOne bug bounty platform to understand what types of cryptography vulnerabilities exist in the wild. We extracted eight themes of vulnerabilities from the vulnerability reports and discussed their real-world implications and mitigation strategies. We hope that our findings alert developers, familiarize them with the dire consequences of cryptography misuses, and support them in avoiding such mistakes.

</details>

<details>

<summary>2021-11-06 17:11:53 - Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs</summary>

- *Julian Aron Prenner, Romain Robbes*

- `2111.03922v1` - [abs](http://arxiv.org/abs/2111.03922v1) - [pdf](http://arxiv.org/pdf/2111.03922v1)

> OpenAI's Codex, a GPT-3 like model trained on a large code corpus, has made headlines in and outside of academia. Given a short user-provided description, it is capable of synthesizing code snippets that are syntactically and semantically valid in most cases. In this work, we want to investigate whether Codex is able to localize and fix bugs, a task of central interest in the field of automated program repair. Our initial evaluation uses the multi-language QuixBugs benchmark (40 bugs in both Python and Java). We find that, despite not being trained for APR, Codex is surprisingly effective, and competitive with recent state of the art techniques. Our results also show that Codex is slightly more successful at repairing Python than Java.

</details>

<details>

<summary>2021-11-06 17:24:14 - Generalized Depthwise-Separable Convolutions for Adversarially Robust and Efficient Neural Networks</summary>

- *Hassan Dbouk, Naresh R. Shanbhag*

- `2110.14871v2` - [abs](http://arxiv.org/abs/2110.14871v2) - [pdf](http://arxiv.org/pdf/2110.14871v2)

> Despite their tremendous successes, convolutional neural networks (CNNs) incur high computational/storage costs and are vulnerable to adversarial perturbations. Recent works on robust model compression address these challenges by combining model compression techniques with adversarial training. But these methods are unable to improve throughput (frames-per-second) on real-life hardware while simultaneously preserving robustness to adversarial perturbations. To overcome this problem, we propose the method of Generalized Depthwise-Separable (GDWS) convolution -- an efficient, universal, post-training approximation of a standard 2D convolution. GDWS dramatically improves the throughput of a standard pre-trained network on real-life hardware while preserving its robustness. Lastly, GDWS is scalable to large problem sizes since it operates on pre-trained models and doesn't require any additional training. We establish the optimality of GDWS as a 2D convolution approximator and present exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. We demonstrate the effectiveness of GDWS via extensive experiments on CIFAR-10, SVHN, and ImageNet datasets. Our code can be found at https://github.com/hsndbk4/GDWS.

</details>

<details>

<summary>2021-11-07 05:07:10 - Sdft: A PDG-based Summarization for Efficient Dynamic Data Flow Tracking</summary>

- *Xiao Kan, Cong Sun, Shen Liu, Yongzhe Huang, Gang Tan, Siqi Ma, Yumei Zhang*

- `2111.04005v1` - [abs](http://arxiv.org/abs/2111.04005v1) - [pdf](http://arxiv.org/pdf/2111.04005v1)

> Dynamic taint analysis (DTA) has been widely used in various security-relevant scenarios that need to track the runtime information flow of programs. Dynamic binary instrumentation (DBI) is a prevalent technique in achieving effective dynamic taint tracking on commodity hardware and systems. However, the significant performance overhead incurred by dynamic taint analysis restricts its usage in production systems. Previous efforts on mitigating the performance penalty fall into two categories, parallelizing taint tracking from program execution and abstracting the tainting logic to a higher granularity. Both approaches have only met with limited success.   In this work, we propose Sdft, an efficient approach that combines the precision of DBI-based instruction-level taint tracking and the efficiency of function-level abstract taint propagation. First, we build the library function summaries automatically with reachability analysis on the program dependency graph (PDG) to specify the control- and data dependencies between the input parameters, output parameters, and global variables of the target library. Then we derive the taint rules for the target library functions and develop taint tracking for library function that is tightly integrated into the state-of-the-art DTA framework Libdft. By applying our approach to the core C library functions of glibc, we report an average of 1.58x speed up of the tracking performance compared with Libdft64. We also validate the effectiveness of the hybrid taint tracking and the ability on detecting real-world vulnerabilities.

</details>

<details>

<summary>2021-11-07 13:57:36 - On the Robustness of Domain Constraints</summary>

- *Ryan Sheatsley, Blaine Hoak, Eric Pauley, Yohan Beugin, Michael J. Weisman, Patrick McDaniel*

- `2105.08619v2` - [abs](http://arxiv.org/abs/2105.08619v2) - [pdf](http://arxiv.org/pdf/2105.08619v2)

> Machine learning is vulnerable to adversarial examples-inputs designed to cause models to perform poorly. However, it is unclear if adversarial examples represent realistic inputs in the modeled domains. Diverse domains such as networks and phishing have domain constraints-complex relationships between features that an adversary must satisfy for an attack to be realized (in addition to any adversary-specific goals). In this paper, we explore how domain constraints limit adversarial capabilities and how adversaries can adapt their strategies to create realistic (constraint-compliant) examples. In this, we develop techniques to learn domain constraints from data, and show how the learned constraints can be integrated into the adversarial crafting process. We evaluate the efficacy of our approach in network intrusion and phishing datasets and find: (1) up to 82% of adversarial examples produced by state-of-the-art crafting algorithms violate domain constraints, (2) domain constraints are robust to adversarial examples; enforcing constraints yields an increase in model accuracy by up to 34%. We observe not only that adversaries must alter inputs to satisfy domain constraints, but that these constraints make the generation of valid adversarial examples far more challenging.

</details>

<details>

<summary>2021-11-07 17:36:50 - Mapping Access to Water and Sanitation in Colombia using Publicly Accessible Satellite Imagery, Crowd-sourced Geospatial Information and RandomForests</summary>

- *Niccolo Dejito, Ren Avell Flores, Rodolfo de Guzman, Isabelle Tingzon, Liliana Carvajal, Alberto Aroca, Carlos Delgado*

- `2111.04134v1` - [abs](http://arxiv.org/abs/2111.04134v1) - [pdf](http://arxiv.org/pdf/2111.04134v1)

> Up-to-date, granular, and reliable quality of life data is crucial for humanitarian organizations to develop targeted interventions for vulnerable communities, especially in times of crisis. One such quality of life data is access to water, sanitation and hygeine (WASH). Traditionally, data collection is done through door-to-door surveys sampled over large areas. Unfortunately, the huge costs associated with collecting these data deter more frequent and large-coverage surveys. To address this challenge, we present a scalable and inexpensive end-to-end WASH estimation workflow using a combination of machine learning and census data, publicly available satellite images, and crowd-sourced geospatial information. We generate a map of WASH estimates at a granularity of 250m x 250m across the entire country of Colombia. The model was able to explain up to 65% of the variation in predicting access to water supply, sewage, and toilets. The code is made available with MIT License at https://github.com/thinkingmachines/geoai-immap-wash.

</details>

<details>

<summary>2021-11-08 03:02:25 - Trust-aware Control for Intelligent Transportation Systems</summary>

- *Mingxi Cheng, Junyao Zhang, Shahin Nazarian, Jyotirmoy Deshmukh, Paul Bogdan*

- `2111.04248v1` - [abs](http://arxiv.org/abs/2111.04248v1) - [pdf](http://arxiv.org/pdf/2111.04248v1)

> Many intelligent transportation systems are multi-agent systems, i.e., both the traffic participants and the subsystems within the transportation infrastructure can be modeled as interacting agents. The use of AI-based methods to achieve coordination among the different agents systems can provide greater safety over transportation systems containing only human-operated vehicles, and also improve the system efficiency in terms of traffic throughput, sensing range, and enabling collaborative tasks. However, increased autonomy makes the transportation infrastructure vulnerable to compromised vehicular agents or infrastructure. This paper proposes a new framework by embedding the trust authority into transportation infrastructure to systematically quantify the trustworthiness of agents using an epistemic logic known as subjective logic. In this paper, we make the following novel contributions: (i) We propose a framework for using the quantified trustworthiness of agents to enable trust-aware coordination and control. (ii) We demonstrate how to synthesize trust-aware controllers using an approach based on reinforcement learning. (iii) We comprehensively analyze an autonomous intersection management (AIM) case study and develop a trust-aware version called AIM-Trust that leads to lower accident rates in scenarios consisting of a mixture of trusted and untrusted agents.

</details>

<details>

<summary>2021-11-08 03:52:47 - OpenMP aware MHP Analysis for Improved Static Data-Race Detection</summary>

- *Utpal Bora, Shraiysh Vaishay, Saurabh Joshi, Ramakrishna Upadrasta*

- `2111.04259v1` - [abs](http://arxiv.org/abs/2111.04259v1) - [pdf](http://arxiv.org/pdf/2111.04259v1)

> Data races, a major source of bugs in concurrent programs, can result in loss of manpower and time as well as data loss due to system failures. OpenMP, the de facto shared memory parallelism framework used in the HPC community, also suffers from data races. To detect race conditions in OpenMP programs and improve turnaround time and/or developer productivity, we present a data flow analysis based, fast, static data race checker in the LLVM compiler framework. Our tool can detect races in the presence or absence of explicit barriers, with implicit or explicit synchronization. In addition, our tool effectively works for the OpenMP target offloading constructs and also supports the frequently used OpenMP constructs. We formalize and provide a data flow analysis framework to perform Phase Interval Analysis (PIA) of OpenMP programs. Phase intervals are then used to compute the MHP (and its complement NHP) sets for the programs, which, in turn, are used to detect data races statically. We evaluate our work using multiple OpenMP race detection benchmarks and real world applications. Our experiments show that the checker is comparable to the state-of-the-art in various performance metrics with around 90% accuracy, almost perfect recall, and significantly lower runtime and memory footprint.

</details>

<details>

<summary>2021-11-08 07:16:57 - ATVHunter: Reliable Version Detection of Third-Party Libraries for Vulnerability Identification in Android Applications</summary>

- *Xian Zhan, Lingling Fan, Sen Chen, Feng Wu, Tianming Liu, Xiapu Luo, Yang Liu*

- `2102.08172v2` - [abs](http://arxiv.org/abs/2102.08172v2) - [pdf](http://arxiv.org/pdf/2102.08172v2)

> We propose a system, named ATVHunter, which can pinpoint the precise vulnerable in-app TPL versions and provide detailed information about the vulnerabilities and TPLs. We propose a two-phase detection approach to identify specific TPL versions. Specifically, we extract the Control Flow Graphs as the coarse-grained feature to match potential TPLs in the pre-defined TPL database, and then extract opcode in each basic block of CFG as the fine-grained feature to identify the exact TPL versions. We build a comprehensive TPL database (189,545 unique TPLs with 3,006,676 versions) as the reference database. Meanwhile, to identify the vulnerable in-app TPL versions, we also construct a comprehensive and known vulnerable TPL database containing 1,180 CVEs and 224 security bugs. Experimental results show ATVHunter outperforms state-of-the-art TPL detection tools, achieving 90.55% precision and 88.79% recall with high efficiency, and is also resilient to widely-used obfuscation techniques and scalable for large-scale TPL detection. Furthermore, to investigate the ecosystem of the vulnerable TPLs used by apps, we exploit ATVHunter to conduct a large-scale analysis on 104,446 apps and find that 9,050 apps include vulnerable TPL versions with 53,337 vulnerabilities and 7,480 security bugs, most of which are with high risks and are not recognized by app developers.

</details>

<details>

<summary>2021-11-08 07:18:34 - Defense Against Explanation Manipulation</summary>

- *Ruixiang Tang, Ninghao Liu, Fan Yang, Na Zou, Xia Hu*

- `2111.04303v1` - [abs](http://arxiv.org/abs/2111.04303v1) - [pdf](http://arxiv.org/pdf/2111.04303v1)

> Explainable machine learning attracts increasing attention as it improves transparency of models, which is helpful for machine learning to be trusted in real applications. However, explanation methods have recently been demonstrated to be vulnerable to manipulation, where we can easily change a model's explanation while keeping its prediction constant. To tackle this problem, some efforts have been paid to use more stable explanation methods or to change model configurations. In this work, we tackle the problem from the training perspective, and propose a new training scheme called Adversarial Training on EXplanations (ATEX) to improve the internal explanation stability of a model regardless of the specific explanation method being applied. Instead of directly specifying explanation values over data instances, ATEX only puts requirement on model predictions which avoids involving second-order derivatives in optimization. As a further discussion, we also find that explanation stability is closely related to another property of the model, i.e., the risk of being exposed to adversarial attack. Through experiments, besides showing that ATEX improves model robustness against manipulation targeting explanation, it also brings additional benefits including smoothing explanations and improving the efficacy of adversarial training if applied to the model.

</details>

<details>

<summary>2021-11-08 07:30:25 - On the Effectiveness of Small Input Noise for Defending Against Query-based Black-Box Attacks</summary>

- *Junyoung Byun, Hyojun Go, Changick Kim*

- `2101.04829v2` - [abs](http://arxiv.org/abs/2101.04829v2) - [pdf](http://arxiv.org/pdf/2101.04829v2)

> While deep neural networks show unprecedented performance in various tasks, the vulnerability to adversarial examples hinders their deployment in safety-critical systems. Many studies have shown that attacks are also possible even in a black-box setting where an adversary cannot access the target model's internal information. Most black-box attacks are based on queries, each of which obtains the target model's output for an input, and many recent studies focus on reducing the number of required queries. In this paper, we pay attention to an implicit assumption of query-based black-box adversarial attacks that the target model's output exactly corresponds to the query input. If some randomness is introduced into the model, it can break the assumption, and thus, query-based attacks may have tremendous difficulty in both gradient estimation and local search, which are the core of their attack process. From this motivation, we observe even a small additive input noise can neutralize most query-based attacks and name this simple yet effective approach Small Noise Defense (SND). We analyze how SND can defend against query-based black-box attacks and demonstrate its effectiveness against eight state-of-the-art attacks with CIFAR-10 and ImageNet datasets. Even with strong defense ability, SND almost maintains the original classification accuracy and computational speed. SND is readily applicable to pre-trained models by adding only one line of code at the inference.

</details>

<details>

<summary>2021-11-08 10:26:28 - Geometrically Adaptive Dictionary Attack on Face Recognition</summary>

- *Junyoung Byun, Hyojun Go, Changick Kim*

- `2111.04371v1` - [abs](http://arxiv.org/abs/2111.04371v1) - [pdf](http://arxiv.org/pdf/2111.04371v1)

> CNN-based face recognition models have brought remarkable performance improvement, but they are vulnerable to adversarial perturbations. Recent studies have shown that adversaries can fool the models even if they can only access the models' hard-label output. However, since many queries are needed to find imperceptible adversarial noise, reducing the number of queries is crucial for these attacks. In this paper, we point out two limitations of existing decision-based black-box attacks. We observe that they waste queries for background noise optimization, and they do not take advantage of adversarial perturbations generated for other images. We exploit 3D face alignment to overcome these limitations and propose a general strategy for query-efficient black-box attacks on face recognition named Geometrically Adaptive Dictionary Attack (GADA). Our core idea is to create an adversarial perturbation in the UV texture map and project it onto the face in the image. It greatly improves query efficiency by limiting the perturbation search space to the facial area and effectively recycling previous perturbations. We apply the GADA strategy to two existing attack methods and show overwhelming performance improvement in the experiments on the LFW and CPLFW datasets. Furthermore, we also present a novel attack strategy that can circumvent query similarity-based stateful detection that identifies the process of query-based black-box attacks.

</details>

<details>

<summary>2021-11-08 19:29:44 - ML-EXray: Visibility into ML Deployment on the Edge</summary>

- *Hang Qiu, Ioanna Vavelidou, Jian Li, Evgenya Pergament, Pete Warden, Sandeep Chinchali, Zain Asgar, Sachin Katti*

- `2111.04779v1` - [abs](http://arxiv.org/abs/2111.04779v1) - [pdf](http://arxiv.org/pdf/2111.04779v1)

> Benefiting from expanding cloud infrastructure, deep neural networks (DNNs) today have increasingly high performance when trained in the cloud. Researchers spend months of effort competing for an extra few percentage points of model accuracy. However, when these models are actually deployed on edge devices in practice, very often, the performance can abruptly drop over 10% without obvious reasons. The key challenge is that there is not much visibility into ML inference execution on edge devices, and very little awareness of potential issues during the edge deployment process. We present ML-EXray, an end-to-end framework, which provides visibility into layer-level details of the ML execution, and helps developers analyze and debug cloud-to-edge deployment issues. More often than not, the reason for sub-optimal edge performance does not only lie in the model itself, but every operation throughout the data flow and the deployment process. Evaluations show that ML-EXray can effectively catch deployment issues, such as pre-processing bugs, quantization issues, suboptimal kernels, etc. Using ML-EXray, users need to write less than 15 lines of code to fully examine the edge deployment pipeline. Eradicating these issues, ML-EXray can correct model performance by up to 30%, pinpoint error-prone layers, and guide users to optimize kernel execution latency by two orders of magnitude. Code and APIs will be released as an open-source multi-lingual instrumentation library and a Python deployment validation library.

</details>

<details>

<summary>2021-11-08 23:13:55 - Security Analysis of Vendor Implementations of the OPC UA Protocol for Industrial Control Systems</summary>

- *Alessandro Erba, Anne Müller, Nils Ole Tippenhauer*

- `2104.06051v2` - [abs](http://arxiv.org/abs/2104.06051v2) - [pdf](http://arxiv.org/pdf/2104.06051v2)

> The OPC UA protocol is an upcoming de-facto standard for building Industry 4.0 processes in Europe, and one of the few industrial protocols that promises security features to prevent attackers from manipulating and damaging critical infrastructures. Despite the importance of the protocol, challenges in the adoption of OPC UA's security features by product vendors, libraries implementing the standard, and end-users were not investigated so far.   In this work, we systematically investigate 48 publicly available artifacts consisting of products and libraries for OPC UA and show that 38 out of the 48 artifacts have one (or more) security issues. In particular, we show that 7 OPC UA artifacts do not support the security features of the protocol at all. In addition, 31 artifacts that partially feature OPC UA security rely on incomplete libraries and come with misleading instructions. Consequently, relying on those products and libraries will result in vulnerable implementations of OPC UA security features. To verify our analysis, we design, implement, and demonstrate attacks in which the attacker can steal credentials exchanged between victims, eavesdrop on process information, manipulate the physical process through sensor values and actuator commands, and prevent the detection of anomalies.

</details>

<details>

<summary>2021-11-09 05:34:21 - Improved Worst-Case Regret Bounds for Randomized Least-Squares Value Iteration</summary>

- *Priyank Agrawal, Jinglin Chen, Nan Jiang*

- `2010.12163v4` - [abs](http://arxiv.org/abs/2010.12163v4) - [pdf](http://arxiv.org/pdf/2010.12163v4)

> This paper studies regret minimization with randomized value functions in reinforcement learning. In tabular finite-horizon Markov Decision Processes, we introduce a clipping variant of one classical Thompson Sampling (TS)-like algorithm, randomized least-squares value iteration (RLSVI). Our $\tilde{\mathrm{O}}(H^2S\sqrt{AT})$ high-probability worst-case regret bound improves the previous sharpest worst-case regret bounds for RLSVI and matches the existing state-of-the-art worst-case TS-based regret bounds.

</details>

<details>

<summary>2021-11-09 15:28:21 - TDGIA:Effective Injection Attacks on Graph Neural Networks</summary>

- *Xu Zou, Qinkai Zheng, Yuxiao Dong, Xinyu Guan, Evgeny Kharlamov, Jialiang Lu, Jie Tang*

- `2106.06663v2` - [abs](http://arxiv.org/abs/2106.06663v2) - [pdf](http://arxiv.org/pdf/2106.06663v2)

> Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. However, recent studies have shown that GNNs are vulnerable to adversarial attacks. In this paper, we study a recently-introduced realistic attack scenario on graphs -- graph injection attack (GIA). In the GIA scenario, the adversary is not able to modify the existing link structure and node attributes of the input graph, instead the attack is performed by injecting adversarial nodes into it. We present an analysis on the topological vulnerability of GNNs under GIA setting, based on which we propose the Topological Defective Graph Injection Attack (TDGIA) for effective injection attacks. TDGIA first introduces the topological defective edge selection strategy to choose the original nodes for connecting with the injected ones. It then designs the smooth feature optimization objective to generate the features for the injected nodes. Extensive experiments on large-scale datasets show that TDGIA can consistently and significantly outperform various attack baselines in attacking dozens of defense GNN models. Notably, the performance drop on target GNNs resultant from TDGIA is more than double the damage brought by the best attack solution among hundreds of submissions on KDD-CUP 2020.

</details>

<details>

<summary>2021-11-09 15:58:44 - A research framework for writing differentiable PDE discretizations in JAX</summary>

- *Antonio Stanziola, Simon R. Arridge, Ben T. Cox, Bradley E. Treeby*

- `2111.05218v1` - [abs](http://arxiv.org/abs/2111.05218v1) - [pdf](http://arxiv.org/pdf/2111.05218v1)

> Differentiable simulators are an emerging concept with applications in several fields, from reinforcement learning to optimal control. Their distinguishing feature is the ability to calculate analytic gradients with respect to the input parameters. Like neural networks, which are constructed by composing several building blocks called layers, a simulation often requires computing the output of an operator that can itself be decomposed into elementary units chained together. While each layer of a neural network represents a specific discrete operation, the same operator can have multiple representations, depending on the discretization employed and the research question that needs to be addressed. Here, we propose a simple design pattern to construct a library of differentiable operators and discretizations, by representing operators as mappings between families of continuous functions, parametrized by finite vectors. We demonstrate the approach on an acoustic optimization problem, where the Helmholtz equation is discretized using Fourier spectral methods, and differentiability is demonstrated using gradient descent to optimize the speed of sound of an acoustic lens. The proposed framework is open-sourced and available at \url{https://github.com/ucl-bug/jaxdf}

</details>

<details>

<summary>2021-11-09 16:28:00 - Corrected CBOW Performs as well as Skip-gram</summary>

- *Ozan İrsoy, Adrian Benton, Karl Stratos*

- `2012.15332v2` - [abs](http://arxiv.org/abs/2012.15332v2) - [pdf](http://arxiv.org/pdf/2012.15332v2)

> Mikolov et al. (2013a) observed that continuous bag-of-words (CBOW) word embeddings tend to underperform Skip-gram (SG) embeddings, and this finding has been reported in subsequent works. We find that these observations are driven not by fundamental differences in their training objectives, but more likely on faulty negative sampling CBOW implementations in popular libraries such as the official implementation, word2vec.c, and Gensim. We show that after correcting a bug in the CBOW gradient update, one can learn CBOW word embeddings that are fully competitive with SG on various intrinsic and extrinsic tasks, while being many times faster to train.

</details>

<details>

<summary>2021-11-09 17:42:52 - Stateful Dynamic Partial Order Reduction for Model Checking Event-Driven Applications that Do Not Terminate</summary>

- *Rahmadi Trimananda, Weiyu Luo, Brian Demsky, Guoqing Harry Xu*

- `2111.05290v1` - [abs](http://arxiv.org/abs/2111.05290v1) - [pdf](http://arxiv.org/pdf/2111.05290v1)

> Event-driven architectures are broadly used for systems that must respond to events in the real world. Event-driven applications are prone to concurrency bugs that involve subtle errors in reasoning about the ordering of events. Unfortunately, there are several challenges in using existing model-checking techniques on these systems. Event-driven applications often loop indefinitely and thus pose a challenge for stateless model checking techniques. On the other hand, deploying purely stateful model checking can explore large sets of equivalent executions.   In this work, we explore a new technique that combines dynamic partial order reduction with stateful model checking to support non-terminating applications. Our work is (1) the first dynamic partial order reduction algorithm for stateful model checking that is sound for non-terminating applications and (2) the first dynamic partial reduction algorithm for stateful model checking of event-driven applications. We experimented with the IoTCheck dataset: a study of interactions in smart home app pairs. This dataset consists of app pairs originated from 198 real-world smart home apps. Overall, our DPOR algorithm successfully reduced the search space for the app pairs, enabling 69 pairs of apps that did not finish without DPOR to finish and providing a 7X average speedup.

</details>

<details>

<summary>2021-11-10 14:47:58 - Towards More Reliable Automated Program Repair by Integrating Static Analysis Techniques</summary>

- *Omar I. Al-Bataineh, Anastasiia Grishina, Leon Moonen*

- `2111.05713v1` - [abs](http://arxiv.org/abs/2111.05713v1) - [pdf](http://arxiv.org/pdf/2111.05713v1)

> A long-standing open challenge for automated program repair is the overfitting problem, which is caused by having insufficient or incomplete specifications to validate whether a generated patch is correct or not. Most available repair systems rely on weak specifications (i.e., specifications that are synthesized from test cases) which limits the quality of generated repairs. To strengthen specifications and improve the quality of repairs, we propose to closer integrate static bug detection techniques with automated program repair. The integration combines automated program repair with static analysis techniques in such a way that bug detection patterns can be synthesized into specifications that the repair system can use. We explore the feasibility of such integration using two types of bugs: arithmetic bugs, such as integer overflow, and logical bugs, such as termination bugs. As part of our analysis, we make several observations that help to improve patch generation for these classes of bugs. Moreover, these observations assist with narrowing down the candidate patch search space, and inferring an effective search order.

</details>

<details>

<summary>2021-11-10 23:18:58 - Robust Deep Reinforcement Learning through Adversarial Loss</summary>

- *Tuomas Oikarinen, Wang Zhang, Alexandre Megretski, Luca Daniel, Tsui-Wei Weng*

- `2008.01976v2` - [abs](http://arxiv.org/abs/2008.01976v2) - [pdf](http://arxiv.org/pdf/2008.01976v2)

> Recent studies have shown that deep reinforcement learning agents are vulnerable to small adversarial perturbations on the agent's inputs, which raises concerns about deploying such agents in the real world. To address this issue, we propose RADIAL-RL, a principled framework to train reinforcement learning agents with improved robustness against $l_p$-norm bounded adversarial attacks. Our framework is compatible with popular deep reinforcement learning algorithms and we demonstrate its performance with deep Q-learning, A3C and PPO. We experiment on three deep RL benchmarks (Atari, MuJoCo and ProcGen) to show the effectiveness of our robust training algorithm. Our RADIAL-RL agents consistently outperform prior methods when tested against attacks of varying strength and are more computationally efficient to train. In addition, we propose a new evaluation method called Greedy Worst-Case Reward (GWC) to measure attack agnostic robustness of deep RL agents. We show that GWC can be evaluated efficiently and is a good estimate of the reward under the worst possible sequence of adversarial attacks. All code used for our experiments is available at https://github.com/tuomaso/radial_rl_v2.

</details>

<details>

<summary>2021-11-11 00:34:00 - SyzScope: Revealing High-Risk Security Impacts of Fuzzer-Exposed Bugs in Linux kernel</summary>

- *Xiaochen Zou, Guoren Li, Weiteng Chen, Hang Zhang, Zhiyun Qian*

- `2111.06002v1` - [abs](http://arxiv.org/abs/2111.06002v1) - [pdf](http://arxiv.org/pdf/2111.06002v1)

> Fuzzing has become one of the most effective bug finding approach for software. In recent years, 24*7 continuous fuzzing platforms have emerged to test critical pieces of software, e.g., Linux kernel. Though capable of discovering many bugs and providing reproducers (e.g., proof-of-concepts), a major problem is that they neglect a critical function that should have been built-in, i.e., evaluation of a bug's security impact. It is well-known that the lack of understanding of security impact can lead to delayed bug fixes as well as patch propagation. In this paper, we develop SyzScope, a system that can automatically uncover new "high-risk" impacts given a bug with seemingly "low-risk" impacts. From analyzing over a thousand low-risk bugs on syzbot, SyzScope successfully determined that 183 low-risk bugs (more than 15%) in fact contain high-risk impacts, e.g., control flow hijack and arbitrary memory write, some of which still do not have patches available yet.

</details>

<details>

<summary>2021-11-11 08:58:23 - Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes</summary>

- *Sanghyun Hong, Michael-Andrei Panaitescu-Liess, Yiğitcan Kaya, Tudor Dumitraş*

- `2110.13541v2` - [abs](http://arxiv.org/abs/2110.13541v2) - [pdf](http://arxiv.org/pdf/2110.13541v2)

> Quantization is a popular technique that $transforms$ the parameter representation of a neural network from floating-point numbers into lower-precision ones ($e.g.$, 8-bit integers). It reduces the memory footprint and the computational cost at inference, facilitating the deployment of resource-hungry models. However, the parameter perturbations caused by this transformation result in $behavioral$ $disparities$ between the model before and after quantization. For example, a quantized model can misclassify some test-time samples that are otherwise classified correctly. It is not known whether such differences lead to a new security vulnerability. We hypothesize that an adversary may control this disparity to introduce specific behaviors that activate upon quantization. To study this hypothesis, we weaponize quantization-aware training and propose a new training framework to implement adversarial quantization outcomes. Following this framework, we present three attacks we carry out with quantization: (i) an indiscriminate attack for significant accuracy loss; (ii) a targeted attack against specific samples; and (iii) a backdoor attack for controlling the model with an input trigger. We further show that a single compromised model defeats multiple quantization schemes, including robust quantization techniques. Moreover, in a federated learning scenario, we demonstrate that a set of malicious participants who conspire can inject our quantization-activated backdoor. Lastly, we discuss potential counter-measures and show that only re-training consistently removes the attack artifacts. Our code is available at https://github.com/Secure-AI-Systems-Group/Qu-ANTI-zation

</details>

<details>

<summary>2021-11-11 11:07:05 - Problems with SZZ and Features: An empirical study of the state of practice of defect prediction data collection</summary>

- *Steffen Herbold, Alexander Trautsch, Fabian Trautsch, Benjamin Ledel*

- `1911.08938v3` - [abs](http://arxiv.org/abs/1911.08938v3) - [pdf](http://arxiv.org/pdf/1911.08938v3)

> Context: The SZZ algorithm is the de facto standard for labeling bug fixing commits and finding inducing changes for defect prediction data. Recent research uncovered potential problems in different parts of the SZZ algorithm. Most defect prediction data sets provide only static code metrics as features, while research indicates that other features are also important.   Objective: We provide an empirical analysis of the defect labels created with the SZZ algorithm and the impact of commonly used features on results.   Method: We used a combination of manual validation and adopted or improved heuristics for the collection of defect data. We conducted an empirical study on 398 releases of 38 Apache projects.   Results: We found that only half of the bug fixing commits determined by SZZ are actually bug fixing. If a six-month time frame is used in combination with SZZ to determine which bugs affect a release, one file is incorrectly labeled as defective for every file that is correctly labeled as defective. In addition, two defective files are missed. We also explored the impact of the relatively small set of features that are available in most defect prediction data sets, as there are multiple publications that indicate that, e.g., churn related features are important for defect prediction. We found that the difference of using more features is not significant.   Conclusion: Problems with inaccurate defect labels are a severe threat to the validity of the state of the art of defect prediction. Small feature sets seem to be a less severe threat.

</details>

<details>

<summary>2021-11-11 19:40:51 - Personalized multi-faceted trust modeling to determine trust links in social media and its potential for misinformation management</summary>

- *Alexandre Parmentier, Robin Cohen, Xueguang Ma, Gaurav Sahu, Queenie Chen*

- `2111.06440v1` - [abs](http://arxiv.org/abs/2111.06440v1) - [pdf](http://arxiv.org/pdf/2111.06440v1)

> In this paper, we present an approach for predicting trust links between peers in social media, one that is grounded in the artificial intelligence area of multiagent trust modeling. In particular, we propose a data-driven multi-faceted trust modeling which incorporates many distinct features for a comprehensive analysis. We focus on demonstrating how clustering of similar users enables a critical new functionality: supporting more personalized, and thus more accurate predictions for users. Illustrated in a trust-aware item recommendation task, we evaluate the proposed framework in the context of a large Yelp dataset. We then discuss how improving the detection of trusted relationships in social media can assist in supporting online users in their battle against the spread of misinformation and rumours, within a social networking environment which has recently exploded in popularity. We conclude with a reflection on a particularly vulnerable user base, older adults, in order to illustrate the value of reasoning about groups of users, looking to some future directions for integrating known preferences with insights gained through data analysis.

</details>

<details>

<summary>2021-11-11 22:38:24 - Mapping breakpoint types: an exploratory study</summary>

- *Eduardo Andreetta Fontana, Fabio Petrillo*

- `2109.00917v2` - [abs](http://arxiv.org/abs/2109.00917v2) - [pdf](http://arxiv.org/pdf/2109.00917v2)

> Debugging is a relevant task for finding bugs during software development, maintenance, and evolution. During debugging, developers use modern IDE debuggers to analyze variables, step execution, and set breakpoints. Observing IDE debuggers, we find several breakpoint types. However, what are the breakpoint types? The goal of our study is to map the breakpoint types among IDEs and academic literature. Thus, we mapped the gray literature on the documentation of the nine main IDEs used by developers according to the three public rankings. In addition, we performed a systematic mapping of academic literature over 68 articles describing breakpoint types. Finally, we analyzed the developers understanding of the main breakpoint types through a questionnaire. We present three main contributions: (1) the mapping of breakpoint types (IDEs and literature), (2) compiled definitions of breakpoint types, (3) a breakpoint type taxonomy. Our contributions provide the first step to organize breakpoint IDE taxonomy and lexicon, and support further debugging research.

</details>

<details>

<summary>2021-11-12 16:03:52 - Towards an open standard for assessing the severity of robot security vulnerabilities, the Robot Vulnerability Scoring System (RVSS)</summary>

- *Víctor Mayoral Vilches, Endika Gil-Uriarte, Irati Zamalloa Ugarte, Gorka Olalde Mendia, Rodrigo Izquierdo Pisón, Laura Alzola Kirschgens, Asier Bilbao Calvo, Alejandro Hernández Cordero, Lucas Apa, César Cerrudo*

- `1807.10357v4` - [abs](http://arxiv.org/abs/1807.10357v4) - [pdf](http://arxiv.org/pdf/1807.10357v4)

> Robots are typically not created with security as a main concern. Contrasting to typical IT systems, cyberphysical systems rely on security to handle safety aspects. In light of the former, classic scoring methods such as the Common Vulnerability Scoring System (CVSS) are not able to accurately capture the severity of robot vulnerabilities. The present research work focuses upon creating an open and free to access Robot Vulnerability Scoring System (RVSS) that considers major relevant issues in robotics including a) robot safety aspects, b) assessment of downstream implications of a given vulnerability, c) library and third-party scoring assessments and d) environmental variables, such as time since vulnerability disclosure or exposure on the web. Finally, an experimental evaluation of RVSS with contrast to CVSS is provided and discussed with focus on the robotics security landscape.

</details>

<details>

<summary>2021-11-12 16:26:10 - Robotics CTF (RCTF), a playground for robot hacking</summary>

- *Gorka Olalde Mendia, Lander Usategui San Juan, Xabier Perez Bascaran, Asier Bilbao Calvo, Alejandro Hernández Cordero, Irati Zamalloa Ugarte, Aday Muñiz Rosas, David Mayoral Vilches, Unai Ayucar Carbajo, Laura Alzola Kirschgens, Víctor Mayoral Vilches, Endika Gil-Uriarte*

- `1810.02690v4` - [abs](http://arxiv.org/abs/1810.02690v4) - [pdf](http://arxiv.org/pdf/1810.02690v4)

> Robots state of insecurity is onstage. There is an emerging concern about major robot vulnerabilities and their adverse consequences. However, there is still a considerable gap between robotics and cybersecurity domains. For the purpose of filling that gap, the present technical report presents the Robotics CTF (RCTF), an online playground to challenge robot security from any browser. We describe the architecture of the RCTF and provide 9 scenarios where hackers can challenge the security of different robotic setups. Our work empowers security researchers to a) reproduce virtual robotic scenarios locally and b) change the networking setup to mimic real robot targets. We advocate for hacker powered security in robotics and contribute by open sourcing our scenarios.

</details>

<details>

<summary>2021-11-12 18:50:00 - Introducing the Robot Vulnerability Database (RVD)</summary>

- *Víctor Mayoral Vilches, Lander Usategui San Juan, Bernhard Dieber, Unai Ayucar Carbajo, Endika Gil-Uriarte*

- `1912.11299v3` - [abs](http://arxiv.org/abs/1912.11299v3) - [pdf](http://arxiv.org/pdf/1912.11299v3)

> Cybersecurity in robotics is an emerging topic that has gained significant traction. Researchers have demonstrated some of the potentials and effects of cyber attacks on robots lately. This implies safety related adverse consequences causing human harm, death or lead to significant integrity loss clearly overcoming the privacy concerns in classical IT world. In cybersecurity research, the use of vulnerability databases is a very reliable tool to responsibly disclose vulnerabilities in software products and raise willingness of vendors to address these issues. In this paper we argue, that existing vulnerability databases are of insufficient information density and show some biased content with respect to vulnerabilities in robots. This paper presents the Robot Vulnerability Database (RVD), a directory for responsible disclosure of bugs, weaknesses and vulnerabilities in robots. This article aims to describe the design and process as well as the associated disclosure policy behind RVD. Furthermore the authors present preliminary selected vulnerabilities already contained in RVD and call to the robotics and security communities for contribution to the endeavour of eliminating zero-day vulnerabilities in robotics.

</details>

<details>

<summary>2021-11-12 22:59:45 - Neural Population Geometry Reveals the Role of Stochasticity in Robust Perception</summary>

- *Joel Dapello, Jenelle Feather, Hang Le, Tiago Marques, David D. Cox, Josh H. McDermott, James J. DiCarlo, SueYeon Chung*

- `2111.06979v1` - [abs](http://arxiv.org/abs/2111.06979v1) - [pdf](http://arxiv.org/pdf/2111.06979v1)

> Adversarial examples are often cited by neuroscientists and machine learning researchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired components to visual neural networks as a way to improve their adversarial robustness. One surprisingly effective component for reducing adversarial vulnerability is response stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations influence the internal representations of standard, adversarially trained, and biologically-inspired stochastic networks. We find distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrates that competing geometric effects of stochasticity mediate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneficial to machine and biological computation.

</details>

<details>

<summary>2021-11-13 00:53:12 - Understanding and Assessment of Mission-Centric Key Cyber Terrains for joint Military Operations</summary>

- *Álvaro Luis Martínez, Jorge Maestre Vidal, Victor A. Villagrá González*

- `2111.07005v1` - [abs](http://arxiv.org/abs/2111.07005v1) - [pdf](http://arxiv.org/pdf/2111.07005v1)

> Since the cyberspace consolidated as fifth warfare dimension, the different actors of the defense sector began an arms race toward achieving cyber superiority, on which research, academic and industrial stakeholders contribute from a dual vision, mostly linked to a large and heterogeneous heritage of developments and adoption of civilian cybersecurity capabilities. In this context, augmenting the conscious of the context and warfare environment, risks and impacts of cyber threats on kinetic actuations became a critical rule-changer that military decision-makers are considering. A major challenge on acquiring mission-centric Cyber Situational Awareness (CSA) is the dynamic inference and assessment of the vertical propagations from situations that occurred at the mission supportive Information and Communications Technologies (ICT), up to their relevance at military tactical, operational and strategical views. In order to contribute on acquiring CSA, this paper addresses a major gap in the cyber defence state-of-the-art: the dynamic identification of Key Cyber Terrains (KCT) on a mission-centric context. Accordingly, the proposed KCT identification approach explores the dependency degrees among tasks and assets defined by commanders as part of the assessment criteria. These are correlated with the discoveries on the operational network and the asset vulnerabilities identified thorough the supported mission development. The proposal is presented as a reference model that reveals key aspects for mission-centric KCT analysis and supports its enforcement and further enforcement by including an illustrative application case.

</details>

<details>

<summary>2021-11-13 07:28:52 - UNTANGLE: Unlocking Routing and Logic Obfuscation Using Graph Neural Networks-based Link Prediction</summary>

- *Lilas Alrahis, Satwik Patnaik, Muhammad Abdullah Hanif, Muhammad Shafique, Ozgur Sinanoglu*

- `2111.07062v1` - [abs](http://arxiv.org/abs/2111.07062v1) - [pdf](http://arxiv.org/pdf/2111.07062v1)

> Logic locking aims to prevent intellectual property (IP) piracy and unauthorized overproduction of integrated circuits (ICs). However, initial logic locking techniques were vulnerable to the Boolean satisfiability (SAT)-based attacks. In response, researchers proposed various SAT-resistant locking techniques such as point function-based locking and symmetric interconnection (SAT-hard) obfuscation. We focus on the latter since point function-based locking suffers from various structural vulnerabilities. The SAT-hard logic locking technique, InterLock [1], achieves a unified logic and routing obfuscation that thwarts state-of-the-art attacks on logic locking. In this work, we propose a novel link prediction-based attack, UNTANGLE, that successfully breaks InterLock in an oracle-less setting without having access to an activated IC (oracle). Since InterLock hides selected timing paths in key-controlled routing blocks, UNTANGLE reveals the gates and interconnections hidden in the routing blocks upon formulating this task as a link prediction problem. The intuition behind our approach is that ICs contain a large amount of repetition and reuse cores. Hence, UNTANGLE can infer the hidden timing paths by learning the composition of gates in the observed locked netlist or a circuit library leveraging graph neural networks. We show that circuits withstanding SAT-based and other attacks can be unlocked in seconds with 100% precision using UNTANGLE in an oracle-less setting. UNTANGLE is a generic attack platform (which we also open source [2]) that applies to multiplexer (MUX)-based obfuscation, as demonstrated through our experiments on ISCAS-85 and ITC-99 benchmarks locked using InterLock and random MUX-based locking.

</details>

<details>

<summary>2021-11-14 15:12:48 - Faster connectivity in low-rank hypergraphs via expander decomposition</summary>

- *Calvin Beideman, Karthekeyan Chandrasekaran, Sagnik Mukhopadhyay, Danupon Nanongkai*

- `2011.08097v4` - [abs](http://arxiv.org/abs/2011.08097v4) - [pdf](http://arxiv.org/pdf/2011.08097v4)

> We design an algorithm for computing connectivity in hypergraphs which runs in time $\hat O_r(p + \min\{\lambda^{\frac{r-3}{r-1}} n^2, n^r/\lambda^{r/(r-1)}\})$ (the $\hat O_r(\cdot)$ hides the terms subpolynomial in the main parameter and terms that depend only on $r$) where $p$ is the size, $n$ is the number of vertices, and $r$ is the rank of the hypergraph. Our algorithm is faster than existing algorithms when the the rank is constant and the connectivity $\lambda$ is $\omega(1)$. At the heart of our algorithm is a structural result regarding min-cuts in simple hypergraphs. We show a trade-off between the number of hyperedges taking part in all min-cuts and the size of the smaller side of the min-cut. This structural result can be viewed as a generalization of a well-known structural theorem for simple graphs [Kawarabayashi-Thorup, JACM 19]. We extend the framework of expander decomposition to simple hypergraphs in order to prove this structural result. We also make the proof of the structural result constructive to obtain our faster hypergraph connectivity algorithm.

</details>

<details>

<summary>2021-11-15 08:57:00 - Property Inference Attacks Against GANs</summary>

- *Junhao Zhou, Yufei Chen, Chao Shen, Yang Zhang*

- `2111.07608v1` - [abs](http://arxiv.org/abs/2111.07608v1) - [pdf](http://arxiv.org/pdf/2111.07608v1)

> While machine learning (ML) has made tremendous progress during the past decade, recent research has shown that ML models are vulnerable to various security and privacy attacks. So far, most of the attacks in this field focus on discriminative models, represented by classifiers. Meanwhile, little attention has been paid to the security and privacy risks of generative models, such as generative adversarial networks (GANs). In this paper, we propose the first set of training dataset property inference attacks against GANs. Concretely, the adversary aims to infer the macro-level training dataset property, i.e., the proportion of samples used to train a target GAN with respect to a certain attribute. A successful property inference attack can allow the adversary to gain extra knowledge of the target GAN's training dataset, thereby directly violating the intellectual property of the target model owner. Also, it can be used as a fairness auditor to check whether the target GAN is trained with a biased dataset. Besides, property inference can serve as a building block for other advanced attacks, such as membership inference. We propose a general attack pipeline that can be tailored to two attack scenarios, including the full black-box setting and partial black-box setting. For the latter, we introduce a novel optimization framework to increase the attack efficacy. Extensive experiments over four representative GAN models on five property inference tasks show that our attacks achieve strong performance. In addition, we show that our attacks can be used to enhance the performance of membership inference against GANs.

</details>

<details>

<summary>2021-11-15 13:40:03 - Beep: Fine-grained Fix Localization by Learning to Predict Buggy Code Elements</summary>

- *Shangwen Wang, Kui Liu, Bo Lin, Li Li, Jacques Klein, Xiaoguang Mao, Tegawendé F. Bissyandé*

- `2111.07739v1` - [abs](http://arxiv.org/abs/2111.07739v1) - [pdf](http://arxiv.org/pdf/2111.07739v1)

> Software Fault Localization refers to the activity of finding code elements (e.g., statements) that are related to a software failure. The state-of-the-art fault localization techniques, however, produce coarse-grained results that can deter manual debugging or mislead automated repair tools. In this work, we focus specifically on the fine-grained identification of code elements (i.e., tokens) that must be changed to fix a buggy program: we refer to it as fix localization. This paper introduces a neural network architecture (named Beep) that builds on AST paths to predict the buggy code element as well as the change action that must be applied to repair a program. Leveraging massive data of bugs and patches within the CoCoNut dataset, we trained a model that was (1) effective in localizing the buggy tokens with the Mean First Rank significantly higher than a statistics based baseline and a machine learning-based baseline, and (2) effective in predicting the repair operators (with the associated buggy code elements) with a Recall@1= 30-45% and the Mean First Rank=7-12 (evaluated by CoCoNut, ManySStuBs4J, and Defects4J datasets). To showcase how fine-grained fix localization can help program repair, we employ it in two repair pipelines where we use either a code completion engine to predict the correct token or a set of heuristics to search for the suitable donor code. A key strength of accurate fix localization for program repair is that it reduces the chance of patch overfitting, a challenge in generate-and-validate automated program repair: both two repair pipelines achieve a correctness ratio of 100%, i.e., all generated patches are found to be correct. Moreover, accurate fix localization helps enhance the efficiency of program repair.

</details>

<details>

<summary>2021-11-15 14:27:42 - Website fingerprinting on early QUIC traffic</summary>

- *Pengwei Zhan, Liming Wang, Yi Tang*

- `2101.11871v2` - [abs](http://arxiv.org/abs/2101.11871v2) - [pdf](http://arxiv.org/pdf/2101.11871v2)

> Cryptographic protocols have been widely used to protect the user's privacy and avoid exposing private information. QUIC (Quick UDP Internet Connections), including the version originally designed by Google (GQUIC) and the version standardized by IETF (IQUIC), as alternatives to the traditional HTTP, demonstrate their unique transmission characteristics: based on UDP for encrypted resource transmitting, accelerating web page rendering. However, existing encrypted transmission schemes based on TCP are vulnerable to website fingerprinting (WFP) attacks, allowing adversaries to infer the users' visited websites by eavesdropping on the transmission channel. Whether GQUIC and IQUIC can effectively resist such attacks is worth investigating. In this paper, we study the vulnerabilities of GQUIC, IQUIC, and HTTPS to WFP attacks from the perspective of traffic analysis. Extensive experiments show that, in the early traffic scenario, GQUIC is the most vulnerable to WFP attacks among GQUIC, IQUIC, and HTTPS, while IQUIC is more vulnerable than HTTPS, but the vulnerability of the three protocols is similar in the normal full traffic scenario. Features transferring analysis shows that most features are transferable between protocols when on normal full traffic scenario. However, combining with the qualitative analysis of latent feature representation, we find that the transferring is inefficient when on early traffic, as GQUIC, IQUIC, and HTTPS show the significantly different magnitude of variation in the traffic distribution on early traffic. By upgrading the one-time WFP attacks to multiple WFP Top-a attacks, we find that the attack accuracy on GQUIC and IQUIC reach 95.4% and 95.5%, respectively, with only 40 packets and just using simple features, whereas reach only 60.7% when on HTTPS. We also demonstrate that the vulnerability of IQUIC is only slightly dependent on the network environment.

</details>

<details>

<summary>2021-11-16 04:22:15 - NatiDroid: Cross-Language Android Permission Specification</summary>

- *Chaoran Li, Xiao Chen, Ruoxi Sun, Jason Xue, Sheng Wen, Muhammad Ejaz Ahmed, Seyit Camtepe, Yang Xiang*

- `2111.08217v1` - [abs](http://arxiv.org/abs/2111.08217v1) - [pdf](http://arxiv.org/pdf/2111.08217v1)

> The Android system manages access to sensitive APIs by permission enforcement. An application (app) must declare proper permissions before invoking specific Android APIs. However, there is no official documentation providing the complete list of permission-protected APIs and the corresponding permissions to date. Researchers have spent significant efforts extracting such API protection mapping from the Android API framework, which leverages static code analysis to determine if specific permissions are required before accessing an API. Nevertheless, none of them has attempted to analyze the protection mapping in the native library (i.e., code written in C and C++), an essential component of the Android framework that handles communication with the lower-level hardware, such as cameras and sensors. While the protection mapping can be utilized to detect various security vulnerabilities in Android apps, such as permission over-privilege and component hijacking, imprecise mapping will lead to false results in detecting such security vulnerabilities. To fill this gap, we develop a prototype system, named NatiDroid, to facilitate the cross-language static analysis to benchmark against two state-of-the-art tools, termed Axplorer and Arcade. We evaluate NatiDroid on more than 11,000 Android apps, including system apps from custom Android ROMs and third-party apps from the Google Play. Our NatiDroid can identify up to 464 new API-permission mappings, in contrast to the worst-case results derived from both Axplorer and Arcade, where approximately 71% apps have at least one false positive in permission over-privilege and up to 3.6% apps have at least one false negative in component hijacking. Additionally, we identify that 24 components with at least one Native-triggered component hijacking vulnerability are misidentified by two benchmarks.

</details>

<details>

<summary>2021-11-16 09:49:50 - Self-Supervised Bug Detection and Repair</summary>

- *Miltiadis Allamanis, Henry Jackson-Flux, Marc Brockschmidt*

- `2105.12787v3` - [abs](http://arxiv.org/abs/2105.12787v3) - [pdf](http://arxiv.org/pdf/2105.12787v3)

> Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair. BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BugLab improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software.

</details>

<details>

<summary>2021-11-16 14:31:09 - CVSS-BERT: Explainable Natural Language Processing to Determine the Severity of a Computer Security Vulnerability from its Description</summary>

- *Mustafizur Shahid, Hervé Debar*

- `2111.08510v1` - [abs](http://arxiv.org/abs/2111.08510v1) - [pdf](http://arxiv.org/pdf/2111.08510v1)

> When a new computer security vulnerability is publicly disclosed, only a textual description of it is available. Cybersecurity experts later provide an analysis of the severity of the vulnerability using the Common Vulnerability Scoring System (CVSS). Specifically, the different characteristics of the vulnerability are summarized into a vector (consisting of a set of metrics), from which a severity score is computed. However, because of the high number of vulnerabilities disclosed everyday this process requires lot of manpower, and several days may pass before a vulnerability is analyzed. We propose to leverage recent advances in the field of Natural Language Processing (NLP) to determine the CVSS vector and the associated severity score of a vulnerability from its textual description in an explainable manner. To this purpose, we trained multiple BERT classifiers, one for each metric composing the CVSS vector. Experimental results show that our trained classifiers are able to determine the value of the metrics of the CVSS vector with high accuracy. The severity score computed from the predicted CVSS vector is also very close to the real severity score attributed by a human expert. For explainability purpose, gradient-based input saliency method was used to determine the most relevant input words for a given prediction made by our classifiers. Often, the top relevant words include terms in agreement with the rationales of a human cybersecurity expert, making the explanation comprehensible for end-users.

</details>

<details>

<summary>2021-11-16 18:00:22 - Machine Learning and Ensemble Approach Onto Predicting Heart Disease</summary>

- *Aaditya Surya*

- `2111.08667v1` - [abs](http://arxiv.org/abs/2111.08667v1) - [pdf](http://arxiv.org/pdf/2111.08667v1)

> The four essential chambers of one's heart that lie in the thoracic cavity are crucial for one's survival, yet ironically prove to be the most vulnerable. Cardiovascular disease (CVD) also commonly referred to as heart disease has steadily grown to the leading cause of death amongst humans over the past few decades. Taking this concerning statistic into consideration, it is evident that patients suffering from CVDs need a quick and correct diagnosis in order to facilitate early treatment to lessen the chances of fatality. This paper attempts to utilize the data provided to train classification models such as Logistic Regression, K Nearest Neighbors, Support Vector Machine, Decision Tree, Gaussian Naive Bayes, Random Forest, and Multi-Layer Perceptron (Artificial Neural Network) and eventually using a soft voting ensemble technique in order to attain as many correct diagnoses as possible.

</details>

<details>

<summary>2021-11-16 21:42:23 - PROVENANCE: An Intermediary-Free Solution for Digital Content Verification</summary>

- *Bilal Yousuf, M. Atif Qureshi, Brendan Spillane, Gary Munnelly, Oisin Carroll, Matthew Runswick, Kirsty Park, Eileen Culloty, Owen Conlan, Jane Suiter*

- `2111.08791v1` - [abs](http://arxiv.org/abs/2111.08791v1) - [pdf](http://arxiv.org/pdf/2111.08791v1)

> The threat posed by misinformation and disinformation is one of the defining challenges of the 21st century. Provenance is designed to help combat this threat by warning users when the content they are looking at may be misinformation or disinformation. It is also designed to improve media literacy among its users and ultimately reduce susceptibility to the threat among vulnerable groups within society. The Provenance browser plugin checks the content that users see on the Internet and social media and provides warnings in their browser or social media feed. Unlike similar plugins, which require human experts to provide evaluations and can only provide simple binary warnings, Provenance's state of the art technology does not require human input and it analyses seven aspects of the content users see and provides warnings where necessary.

</details>

<details>

<summary>2021-11-17 13:53:25 - Address Behaviour Vulnerabilities in the Next Generation of Autonomous Robots</summary>

- *Michele Colledanchise*

- `2103.13268v2` - [abs](http://arxiv.org/abs/2103.13268v2) - [pdf](http://arxiv.org/pdf/2103.13268v2)

> Robots applications in our daily life increase at an unprecedented pace. As robots will soon operate "out in the wild", we must identify the safety and security vulnerabilities they will face. Robotics researchers and manufacturers focus their attention on new, cheaper, and more reliable applications. Still, they often disregard the operability in adversarial environments where a trusted or untrusted user can jeopardize or even alter the robot's task.   In this paper, we identify a new paradigm of security threats in the next generation of robots. These threats fall beyond the known hardware or network-based ones, and we must find new solutions to address them. These new threats include malicious use of the robot's privileged access, tampering with the robot sensors system, and tricking the robot's deliberation into harmful behaviors. We provide a taxonomy of attacks that exploit these vulnerabilities with realistic examples, and we outline effective countermeasures to prevent better, detect, and mitigate them.

</details>

<details>

<summary>2021-11-17 19:56:24 - A Crowdsourced Contact Tracing Model to Detect COVID-19 Patients using Smartphones</summary>

- *Linta Islam, Mafizur Rahman, Nabila Ahmad, Tasnia Sharmin, Jannatul Ferdous Sorna*

- `2112.01244v1` - [abs](http://arxiv.org/abs/2112.01244v1) - [pdf](http://arxiv.org/pdf/2112.01244v1)

> Millions of people have died all across the world because of the COVID-19 outbreak. Researchers worldwide are working together and facing many challenges to bring out the proper vaccines to prevent this infectious virus. Therefore, in this study, a system has been designed which will be adequate to stop the outbreak of COVID-19 by spreading awareness of the COVID-19 infected patient situated area. The model has been formulated for Location base COVID-19 patient identification using mobile crowdsourcing. In this system, the government will update the information about inflected COVID-19 patients. It will notify other users in the vulnerable area to stay at 6 feet or 1.8-meter distance to remain safe. We utilized the Haversine formula and circle formula to generate the unsafe area. Ten thousand valid information has been collected to support the results of this research. The algorithm is tested for 10 test cases every time, and the datasets are increased by 1000. The run time of that algorithm is growing linearly. Thus, we can say that the proposed algorithm can run in polynomial time. The algorithm's correctness is also being tested where it is found that the proposed algorithm is correct and efficient. We also implement the system, and the application is evaluated by taking feedback from users. Thus, people can use our system to keep themselves in a safe area and decrease COVID patients' rate.

</details>

<details>

<summary>2021-11-17 20:19:55 - Weapon Engagement Zone Maximum Launch Range Estimation Using a Deep Neural Network</summary>

- *Joao P. A. Dantas, Andre N. Costa, Diego Geraldo, Marcos R. O. A. Maximo, Takashi Yoneyama*

- `2111.04474v2` - [abs](http://arxiv.org/abs/2111.04474v2) - [pdf](http://arxiv.org/pdf/2111.04474v2)

> This work investigates the use of a Deep Neural Network (DNN) to perform an estimation of the Weapon Engagement Zone (WEZ) maximum launch range. The WEZ allows the pilot to identify an airspace in which the available missile has a more significant probability of successfully engaging a particular target, i.e., a hypothetical area surrounding an aircraft in which an adversary is vulnerable to a shot. We propose an approach to determine the WEZ of a given missile using 50,000 simulated launches in variate conditions. These simulations are used to train a DNN that can predict the WEZ when the aircraft finds itself on different firing conditions, with a coefficient of determination of 0.99. It provides another procedure concerning preceding research since it employs a non-discretized model, i.e., it considers all directions of the WEZ at once, which has not been done previously. Additionally, the proposed method uses an experimental design that allows for fewer simulation runs, providing faster model training.

</details>

<details>

<summary>2021-11-18 06:58:08 - ZeBRA: Precisely Destroying Neural Networks with Zero-Data Based Repeated Bit Flip Attack</summary>

- *Dahoon Park, Kon-Woo Kwon, Sunghoon Im, Jaeha Kung*

- `2111.01080v2` - [abs](http://arxiv.org/abs/2111.01080v2) - [pdf](http://arxiv.org/pdf/2111.01080v2)

> In this paper, we present Zero-data Based Repeated bit flip Attack (ZeBRA) that precisely destroys deep neural networks (DNNs) by synthesizing its own attack datasets. Many prior works on adversarial weight attack require not only the weight parameters, but also the training or test dataset in searching vulnerable bits to be attacked. We propose to synthesize the attack dataset, named distilled target data, by utilizing the statistics of batch normalization layers in the victim DNN model. Equipped with the distilled target data, our ZeBRA algorithm can search vulnerable bits in the model without accessing training or test dataset. Thus, our approach makes the adversarial weight attack more fatal to the security of DNNs. Our experimental results show that 2.0x (CIFAR-10) and 1.6x (ImageNet) less number of bit flips are required on average to destroy DNNs compared to the previous attack method. Our code is available at https://github. com/pdh930105/ZeBRA.

</details>

<details>

<summary>2021-11-18 11:10:04 - InspectJS: Leveraging Code Similarity and User-Feedback for Effective Taint Specification Inference for JavaScript</summary>

- *Saikat Dutta, Diego Garbervetsky, Shuvendu Lahiri, Max Schäfer*

- `2111.09625v1` - [abs](http://arxiv.org/abs/2111.09625v1) - [pdf](http://arxiv.org/pdf/2111.09625v1)

> Static analysis has established itself as a weapon of choice for detecting security vulnerabilities. Taint analysis in particular is a very general and powerful technique, where security policies are expressed in terms of forbidden flows, either from untrusted input sources to sensitive sinks (in integrity policies) or from sensitive sources to untrusted sinks (in confidentiality policies). The appeal of this approach is that the taint-tracking mechanism has to be implemented only once, and can then be parameterized with different taint specifications (that is, sets of sources and sinks, as well as any sanitizers that render otherwise problematic flows innocuous) to detect many different kinds of vulnerabilities.   But while techniques for implementing scalable inter-procedural static taint tracking are fairly well established, crafting taint specifications is still more of an art than a science, and in practice tends to involve a lot of manual effort.   Past work has focussed on automated techniques for inferring taint specifications for libraries either from their implementation or from the way they tend to be used in client code. Among the latter, machine learning-based approaches have shown great promise.   In this work we present our experience combining an existing machine-learning approach to mining sink specifications for JavaScript libraries with manual taint modelling in the context of GitHub's CodeQL analysis framework. We show that the machine-learning component can successfully infer many new taint sinks that either are not part of the manual modelling or are not detected due to analysis incompleteness. Moreover, we present techniques for organizing sink predictions using automated ranking and code-similarity metrics that allow an analysis engineer to efficiently sift through large numbers of predictions to identify true positives.

</details>

<details>

<summary>2021-11-18 11:10:28 - Enhancing the Insertion of NOP Instructions to Obfuscate Malware via Deep Reinforcement Learning</summary>

- *Daniel Gibert, Matt Fredrikson, Carles Mateu, Jordi Planes, Quan Le*

- `2111.09626v1` - [abs](http://arxiv.org/abs/2111.09626v1) - [pdf](http://arxiv.org/pdf/2111.09626v1)

> Current state-of-the-art research for tackling the problem of malware detection and classification is centered on the design, implementation and deployment of systems powered by machine learning because of its ability to generalize to never-before-seen malware families and polymorphic mutations. However, it has been shown that machine learning models, in particular deep neural networks, lack robustness against crafted inputs (adversarial examples). In this work, we have investigated the vulnerability of a state-of-the-art shallow convolutional neural network malware classifier against the dead code insertion technique. We propose a general framework powered by a Double Q-network to induce misclassification over malware families. The framework trains an agent through a convolutional neural network to select the optimal positions in a code sequence to insert dead code instructions so that the machine learning classifier mislabels the resulting executable. The experiments show that the proposed method significantly drops the classification accuracy of the classifier to 56.53% while having an evasion rate of 100% for the samples belonging to the Kelihos_ver3, Simda, and Kelihos_ver1 families. In addition, the average number of instructions needed to mislabel malware in comparison to a random agent decreased by 33%.

</details>

<details>

<summary>2021-11-18 13:12:01 - Are automated static analysis tools worth it? An investigation into relative warning density and external software quality</summary>

- *Alexander Trautsch, Steffen Herbold, Jens Grabowski*

- `2111.09188v2` - [abs](http://arxiv.org/abs/2111.09188v2) - [pdf](http://arxiv.org/pdf/2111.09188v2)

> Automated Static Analysis Tools (ASATs) are part of software development best practices. ASATs are able to warn developers about potential problems in the code. On the one hand, ASATs are based on best practices so there should be a noticeable effect on software quality. On the other hand, ASATs suffer from false positive warnings, which developers have to inspect and then ignore or mark as invalid. In this article, we ask the question if ASATs have a measurable impact on external software quality, using the example of PMD for Java. We investigate the relationship between ASAT warnings emitted by PMD on defects per change and per file. Our case study includes data for the history of each file as well as the differences between changed files and the project in which they are contained. We investigate whether files that induce a defect have more static analysis warnings than the rest of the project. Moreover, we investigate the impact of two different sets of ASAT rules. We find that, bug inducing files contain less static analysis warnings than other files of the project at that point in time. However, this can be explained by the overall decreasing warning density. When compared with all other changes, we find a statistically significant difference in one metric for all rules and two metrics for a subset of rules. However, the effect size is negligible in all cases, showing that the actual difference in warning density between bug inducing changes and other changes is small at best.

</details>

<details>

<summary>2021-11-18 16:22:13 - Differentiable Learning Under Triage</summary>

- *Nastaran Okati, Abir De, Manuel Gomez-Rodriguez*

- `2103.08902v4` - [abs](http://arxiv.org/abs/2103.08902v4) - [pdf](http://arxiv.org/pdf/2103.08902v4)

> Multiple lines of evidence suggest that predictive models may benefit from algorithmic triage. Under algorithmic triage, a predictive model does not predict all instances but instead defers some of them to human experts. However, the interplay between the prediction accuracy of the model and the human experts under algorithmic triage is not well understood. In this work, we start by formally characterizing under which circumstances a predictive model may benefit from algorithmic triage. In doing so, we also demonstrate that models trained for full automation may be suboptimal under triage. Then, given any model and desired level of triage, we show that the optimal triage policy is a deterministic threshold rule in which triage decisions are derived deterministically by thresholding the difference between the model and human errors on a per-instance level. Building upon these results, we introduce a practical gradient-based algorithm that is guaranteed to find a sequence of triage policies and predictive models of increasing performance. Experiments on a wide variety of supervised learning tasks using synthetic and real data from two important applications -- content moderation and scientific discovery -- illustrate our theoretical results and show that the models and triage policies provided by our gradient-based algorithm outperform those provided by several competitive baselines.

</details>

<details>

<summary>2021-11-18 20:13:07 - Constraint-based Diversification of JOP Gadgets</summary>

- *Rodothea Myrsini Tsoupidi, Roberto Castañeda Lozano, Benoit Baudry*

- `2111.09934v1` - [abs](http://arxiv.org/abs/2111.09934v1) - [pdf](http://arxiv.org/pdf/2111.09934v1)

> Modern software deployment process produces software that is uniform and hence vulnerable to large-scale code-reuse attacks, such as Jump-Oriented Programming (JOP) attacks. Compiler-based diversification improves the resilience of software systems by automatically generating different assembly code versions of a given program. Existing techniques are efficient but do not have a precise control over the quality of the generated variants. This paper introduces Diversity by Construction (DivCon), a constraint-based approach to software diversification. Unlike previous approaches, DivCon allows users to control and adjust the conflicting goals of diversity and code quality. A key enabler is the use of Large Neighborhood Search (LNS) to generate highly diverse code efficiently. For larger problems, we propose a combination of LNS with a structural decomposition of the problem. To further improve the diversification efficiency of DivCon against JOP attacks, we propose an application-specific distance measure tailored to the characteristics of JOP attacks. We evaluate DivCon with 20 functions from a popular benchmark suite for embedded systems. These experiments show that the combination of LNS and our application-specific distance measure generates binary programs that are highly resilient against JOP attacks. Our results confirm that there is a trade-off between the quality of each assembly code version and the diversity of the entire pool of versions. In particular, the experiments show that DivCon generates near-optimal binary programs that share a small number of gadgets. For constraint programming researchers and practitioners, this paper demonstrates that LNS is a valuable technique for finding diverse solutions. For security researchers and software engineers, DivCon extends the scope of compiler-based diversification to performance-critical and resource-constrained applications.

</details>

<details>

<summary>2021-11-18 22:13:43 - A Review of Adversarial Attack and Defense for Classification Methods</summary>

- *Yao Li, Minhao Cheng, Cho-Jui Hsieh, Thomas C. M. Lee*

- `2111.09961v1` - [abs](http://arxiv.org/abs/2111.09961v1) - [pdf](http://arxiv.org/pdf/2111.09961v1)

> Despite the efficiency and scalability of machine learning systems, recent studies have demonstrated that many classification methods, especially deep neural networks (DNNs), are vulnerable to adversarial examples; i.e., examples that are carefully crafted to fool a well-trained classification model while being indistinguishable from natural data to human. This makes it potentially unsafe to apply DNNs or related methods in security-critical areas. Since this issue was first identified by Biggio et al. (2013) and Szegedy et al.(2014), much work has been done in this field, including the development of attack methods to generate adversarial examples and the construction of defense techniques to guard against such examples. This paper aims to introduce this topic and its latest developments to the statistical community, primarily focusing on the generation and guarding of adversarial examples. Computing codes (in python and R) used in the numerical experiments are publicly available for readers to explore the surveyed methods. It is the hope of the authors that this paper will encourage more statisticians to work on this important and exciting field of generating and defending against adversarial examples.

</details>

<details>

<summary>2021-11-19 05:18:13 - Understanding and Testing Generalization of Deep Networks on Out-of-Distribution Data</summary>

- *Rui Hu, Jitao Sang, Jinqiang Wang, Rui Hu, Chaoquan Jiang*

- `2111.09190v2` - [abs](http://arxiv.org/abs/2111.09190v2) - [pdf](http://arxiv.org/pdf/2111.09190v2)

> Deep network models perform excellently on In-Distribution (ID) data, but can significantly fail on Out-Of-Distribution (OOD) data. While developing methods focus on improving OOD generalization, few attention has been paid to evaluating the capability of models to handle OOD data. This study is devoted to analyzing the problem of experimental ID test and designing OOD test paradigm to accurately evaluate the practical performance. Our analysis is based on an introduced categorization of three types of distribution shifts to generate OOD data. Main observations include: (1) ID test fails in neither reflecting the actual performance of a single model nor comparing between different models under OOD data. (2) The ID test failure can be ascribed to the learned marginal and conditional spurious correlations resulted from the corresponding distribution shifts. Based on this, we propose novel OOD test paradigms to evaluate the generalization capacity of models to unseen data, and discuss how to use OOD test results to find bugs of models to guide model debugging.

</details>

<details>

<summary>2021-11-19 05:54:14 - Towards Efficiently Evaluating the Robustness of Deep Neural Networks in IoT Systems: A GAN-based Method</summary>

- *Tao Bai, Jun Zhao, Jinlin Zhu, Shoudong Han, Jiefeng Chen, Bo Li, Alex Kot*

- `2111.10055v1` - [abs](http://arxiv.org/abs/2111.10055v1) - [pdf](http://arxiv.org/pdf/2111.10055v1)

> Intelligent Internet of Things (IoT) systems based on deep neural networks (DNNs) have been widely deployed in the real world. However, DNNs are found to be vulnerable to adversarial examples, which raises people's concerns about intelligent IoT systems' reliability and security. Testing and evaluating the robustness of IoT systems becomes necessary and essential. Recently various attacks and strategies have been proposed, but the efficiency problem remains unsolved properly. Existing methods are either computationally extensive or time-consuming, which is not applicable in practice. In this paper, we propose a novel framework called Attack-Inspired GAN (AI-GAN) to generate adversarial examples conditionally. Once trained, it can generate adversarial perturbations efficiently given input images and target classes. We apply AI-GAN on different datasets in white-box settings, black-box settings and targeted models protected by state-of-the-art defenses. Through extensive experiments, AI-GAN achieves high attack success rates, outperforming existing methods, and reduces generation time significantly. Moreover, for the first time, AI-GAN successfully scales to complex datasets e.g. CIFAR-100 and ImageNet, with about $90\%$ success rates among all classes.

</details>

<details>

<summary>2021-11-19 07:34:09 - Enhanced countering adversarial attacks via input denoising and feature restoring</summary>

- *Yanni Li, Wenhui Zhang, Jiawei Liu, Xiaoli Kou, Hui Li, Jiangtao Cui*

- `2111.10075v1` - [abs](http://arxiv.org/abs/2111.10075v1) - [pdf](http://arxiv.org/pdf/2111.10075v1)

> Despite the fact that deep neural networks (DNNs) have achieved prominent performance in various applications, it is well known that DNNs are vulnerable to adversarial examples/samples (AEs) with imperceptible perturbations in clean/original samples. To overcome the weakness of the existing defense methods against adversarial attacks, which damages the information on the original samples, leading to the decrease of the target classifier accuracy, this paper presents an enhanced countering adversarial attack method IDFR (via Input Denoising and Feature Restoring). The proposed IDFR is made up of an enhanced input denoiser (ID) and a hidden lossy feature restorer (FR) based on the convex hull optimization. Extensive experiments conducted on benchmark datasets show that the proposed IDFR outperforms the various state-of-the-art defense methods, and is highly effective for protecting target models against various adversarial black-box or white-box attacks. \footnote{Souce code is released at: \href{https://github.com/ID-FR/IDFR}{https://github.com/ID-FR/IDFR}}

</details>

<details>

<summary>2021-11-19 08:28:27 - Quantifying Cybersecurity Effectiveness of Software Diversity</summary>

- *Huashan Chen, Richard B. Garcia-Lebron, Zheyuan Sun, Jin-Hee Cho, Shouhuai Xu*

- `2111.10090v1` - [abs](http://arxiv.org/abs/2111.10090v1) - [pdf](http://arxiv.org/pdf/2111.10090v1)

> The deployment of monoculture software stacks can cause a devastating damage even by a single exploit against a single vulnerability. Inspired by the resilience benefit of biological diversity, the concept of software diversity has been proposed in the security domain. Although it is intuitive that software diversity may enhance security, its effectiveness has not been quantitatively investigated. Currently, no theoretical or empirical study has been explored to measure the security effectiveness of network diversity. In this paper, we take a first step towards ultimately tackling the problem. We propose a systematic framework that can model and quantify the security effectiveness of network diversity. We conduct simulations to demonstrate the usefulness of the framework. In contrast to the intuitive belief, we show that diversity does not necessarily improve security from a whole-network perspective. The root cause of this phenomenon is that the degree of vulnerability in diversified software implementations plays a critical role in determining the security effectiveness of software diversity.

</details>

<details>

<summary>2021-11-19 10:12:02 - Federated Learning for Malware Detection in IoT Devices</summary>

- *Valerian Rey, Pedro Miguel Sánchez Sánchez, Alberto Huertas Celdrán, Gérôme Bovet, Martin Jaggi*

- `2104.09994v3` - [abs](http://arxiv.org/abs/2104.09994v3) - [pdf](http://arxiv.org/pdf/2104.09994v3)

> This work investigates the possibilities enabled by federated learning concerning IoT malware detection and studies security issues inherent to this new learning paradigm. In this context, a framework that uses federated learning to detect malware affecting IoT devices is presented. N-BaIoT, a dataset modeling network traffic of several real IoT devices while affected by malware, has been used to evaluate the proposed framework. Both supervised and unsupervised federated models (multi-layer perceptron and autoencoder) able to detect malware affecting seen and unseen IoT devices of N-BaIoT have been trained and evaluated. Furthermore, their performance has been compared to two traditional approaches. The first one lets each participant locally train a model using only its own data, while the second consists of making the participants share their data with a central entity in charge of training a global model. This comparison has shown that the use of more diverse and large data, as done in the federated and centralized methods, has a considerable positive impact on the model performance. Besides, the federated models, while preserving the participant's privacy, show similar results as the centralized ones. As an additional contribution and to measure the robustness of the federated approach, an adversarial setup with several malicious participants poisoning the federated model has been considered. The baseline model aggregation averaging step used in most federated learning algorithms appears highly vulnerable to different attacks, even with a single adversary. The performance of other model aggregation functions acting as countermeasures is thus evaluated under the same attack scenarios. These functions provide a significant improvement against malicious participants, but more efforts are still needed to make federated approaches robust.

</details>

<details>

<summary>2021-11-20 07:42:18 - Quaternion-Based Graph Convolution Network for Recommendation</summary>

- *Yaxing Fang, Pengpeng Zhao, Guanfeng Liu, Yanchi Liu, Victor S. Sheng, Lei Zhao, Xiaofang Zhou*

- `2111.10536v1` - [abs](http://arxiv.org/abs/2111.10536v1) - [pdf](http://arxiv.org/pdf/2111.10536v1)

> Graph Convolution Network (GCN) has been widely applied in recommender systems for its representation learning capability on user and item embeddings. However, GCN is vulnerable to noisy and incomplete graphs, which are common in real world, due to its recursive message propagation mechanism. In the literature, some work propose to remove the feature transformation during message propagation, but making it unable to effectively capture the graph structural features. Moreover, they model users and items in the Euclidean space, which has been demonstrated to have high distortion when modeling complex graphs, further degrading the capability to capture the graph structural features and leading to sub-optimal performance. To this end, in this paper, we propose a simple yet effective Quaternion-based Graph Convolution Network (QGCN) recommendation model. In the proposed model, we utilize the hyper-complex Quaternion space to learn user and item representations and feature transformation to improve both performance and robustness. Specifically, we first embed all users and items into the Quaternion space. Then, we introduce the quaternion embedding propagation layers with quaternion feature transformation to perform message propagation. Finally, we combine the embeddings generated at each layer with the mean pooling strategy to obtain the final embeddings for recommendation. Extensive experiments on three public benchmark datasets demonstrate that our proposed QGCN model outperforms baseline methods by a large margin.

</details>

<details>

<summary>2021-11-20 17:56:32 - You Overtrust Your Printer</summary>

- *Giampaolo Bella, Pietro Biondi*

- `2111.10645v1` - [abs](http://arxiv.org/abs/2111.10645v1) - [pdf](http://arxiv.org/pdf/2111.10645v1)

> Printers are common devices whose networked use is vastly unsecured, perhaps due to an enrooted assumption that their services are somewhat negligible and, as such, unworthy of protection. This article develops structured arguments and conducts technical experiments in support of a qualitative risk assessment exercise that ultimately undermines that assumption. Three attacks that can be interpreted as post-exploitation activity are found and discussed, forming what we term the Printjack family of attacks to printers. Some printers may suffer vulnerabilities that would transform them into exploitable zombies. Moreover, a large number of printers, at least on an EU basis, are found to honour unauthenticated printing requests, thus raising the risk level of an attack that sees the crooks exhaust the printing facilities of an institution. There is also a remarkable risk of data breach following an attack consisting in the malicious interception of data while in transit towards printers. Therefore, the newborn IoT era demands printers to be as secure as other devices such as laptops should be, also to facilitate compliance with the General Data Protection Regulation (EU Regulation 2016/679) and reduce the odds of its administrative fines.

</details>

<details>

<summary>2021-11-20 18:09:46 - VoIP Can Still Be Exploited -- Badly</summary>

- *Pietro Biondi, Stefano Bognanni, Giampaolo Bella*

- `2111.15468v1` - [abs](http://arxiv.org/abs/2111.15468v1) - [pdf](http://arxiv.org/pdf/2111.15468v1)

> VoIP phones are early representatives as well as present enhancers of the IoT. This paper observes that they are still widely used in a traditional, unsecured configuration and demonstrates the Phonejack family of attacks: Phonejack 1 conjectures the exploitation of phone vulnerabilities; Phonejack 2 demonstrates how to mount a denial-of-service attack on a network of phones; Phonejack 3 sniffs calls. It is reassuring, however, that inexpensive devices such as a Raspberry Pi can be configured and programmed as effective countermeasures, thus supporting the approach of integrating both technologies. We demonstrate both attacks and defence measures in a video clip. The concluding evaluations argue that trusting the underlying network security measures may turn out overly optimistic; moreover, VoIP phones really ought to be protected as laptops routinely are today

</details>

<details>

<summary>2021-11-21 08:43:15 - Inconspicuous Adversarial Patches for Fooling Image Recognition Systems on Mobile Devices</summary>

- *Tao Bai, Jinqi Luo, Jun Zhao*

- `2106.15202v2` - [abs](http://arxiv.org/abs/2106.15202v2) - [pdf](http://arxiv.org/pdf/2106.15202v2)

> Deep learning based image recognition systems have been widely deployed on mobile devices in today's world. In recent studies, however, deep learning models are shown vulnerable to adversarial examples. One variant of adversarial examples, called adversarial patch, draws researchers' attention due to its strong attack abilities. Though adversarial patches achieve high attack success rates, they are easily being detected because of the visual inconsistency between the patches and the original images. Besides, it usually requires a large amount of data for adversarial patch generation in the literature, which is computationally expensive and time-consuming. To tackle these challenges, we propose an approach to generate inconspicuous adversarial patches with one single image. In our approach, we first decide the patch locations basing on the perceptual sensitivity of victim models, then produce adversarial patches in a coarse-to-fine way by utilizing multiple-scale generators and discriminators. The patches are encouraged to be consistent with the background images with adversarial training while preserving strong attack abilities. Our approach shows the strong attack abilities in white-box settings and the excellent transferability in black-box settings through extensive experiments on various models with different architectures and training methods. Compared to other adversarial patches, our adversarial patches hold the most negligible risks to be detected and can evade human observations, which is supported by the illustrations of saliency maps and results of user evaluations. Lastly, we show that our adversarial patches can be applied in the physical world.

</details>

<details>

<summary>2021-11-21 21:44:57 - Explainable Software Defect Prediction: Are We There Yet?</summary>

- *Jiho Shin, Reem Aleithan, Jaechang Nam, Junjie Wang, Song Wang*

- `2111.10901v1` - [abs](http://arxiv.org/abs/2111.10901v1) - [pdf](http://arxiv.org/pdf/2111.10901v1)

> Explaining the prediction results of software defect prediction models is a challenging while practical task, which can provide useful information for developers to understand and fix the predicted bugs. To address this issue, recently, Jiarpakdee et al. proposed to use {two state-of-the-art} model-agnostic techniques (i.e., LIME and BreakDown) to explain the prediction results of bug prediction models. Their experiments show these tools can generate promising results and the generated explanations can assist developers understand the prediction results. However, the fact that LIME and BreakDown were only examined on a single software defect prediction model setting calls into question about their consistency and reliability across software defect prediction models with various settings.   In this paper, we set out to investigate the consistency and reliability of model-agnostic technique based explanation generation approaches (i.e., LIME and BreakDown) on software defect prediction models with different settings , e.g., different data sampling techniques, different machine learning classifiers, and different prediction scenarios. Specifically, we use both LIME and BreakDown to generate explanations for the same instance under software defect prediction models with different settings and then check the consistency of the generated explanations for the instance. We reused the same defect data from Jiarpakdee et al. in our experiments. The results show that both LIME and BreakDown generate inconsistent explanations under different software defect prediction settings for the same test instances, which makes them unreliable for explanation generation. Overall, with this study, we call for more research in explainable software defect prediction towards achieving consistent and reliable explanation generation.

</details>

<details>

<summary>2021-11-22 08:13:51 - DBIA: Data-free Backdoor Injection Attack against Transformer Networks</summary>

- *Peizhuo Lv, Hualong Ma, Jiachen Zhou, Ruigang Liang, Kai Chen, Shengzhi Zhang, Yunfei Yang*

- `2111.11870v1` - [abs](http://arxiv.org/abs/2111.11870v1) - [pdf](http://arxiv.org/pdf/2111.11870v1)

> Recently, transformer architecture has demonstrated its significance in both Natural Language Processing (NLP) and Computer Vision (CV) tasks. Though other network models are known to be vulnerable to the backdoor attack, which embeds triggers in the model and controls the model behavior when the triggers are presented, little is known whether such an attack is still valid on the transformer models and if so, whether it can be done in a more cost-efficient manner. In this paper, we propose DBIA, a novel data-free backdoor attack against the CV-oriented transformer networks, leveraging the inherent attention mechanism of transformers to generate triggers and injecting the backdoor using the poisoned surrogate dataset. We conducted extensive experiments based on three benchmark transformers, i.e., ViT, DeiT and Swin Transformer, on two mainstream image classification tasks, i.e., CIFAR10 and ImageNet. The evaluation results demonstrate that, consuming fewer resources, our approach can embed backdoors with a high success rate and a low impact on the performance of the victim transformers. Our code is available at https://anonymous.4open.science/r/DBIA-825D.

</details>

<details>

<summary>2021-11-22 08:17:28 - Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks</summary>

- *Utku Ozbulak, Esla Timothy Anzaku, Wesley De Neve, Arnout Van Messem*

- `2106.07141v3` - [abs](http://arxiv.org/abs/2106.07141v3) - [pdf](http://arxiv.org/pdf/2106.07141v3)

> Although the adoption rate of deep neural networks (DNNs) has tremendously increased in recent years, a solution for their vulnerability against adversarial examples has not yet been found. As a result, substantial research efforts are dedicated to fix this weakness, with many studies typically using a subset of source images to generate adversarial examples, treating every image in this subset as equal. We demonstrate that, in fact, not every source image is equally suited for this kind of assessment. To do so, we devise a large-scale model-to-model transferability scenario for which we meticulously analyze the properties of adversarial examples, generated from every suitable source image in ImageNet by making use of three of the most frequently deployed attacks. In this transferability scenario, which involves seven distinct DNN models, including the recently proposed vision transformers, we reveal that it is possible to have a difference of up to $12.5\%$ in model-to-model transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$ ($8/225$) in average $L_{\infty}$ perturbation when $1,000$ source images are sampled randomly among all suitable candidates. We then take one of the first steps in evaluating the robustness of images used to create adversarial examples, proposing a number of simple but effective methods to identify unsuitable source images, thus making it possible to mitigate extreme cases in experimentation and support high-quality benchmarking.

</details>

<details>

<summary>2021-11-22 11:41:50 - Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack</summary>

- *Shengcai Liu, Ning Lu, Cheng Chen, Ke Tang*

- `2109.02229v3` - [abs](http://arxiv.org/abs/2109.02229v3) - [pdf](http://arxiv.org/pdf/2109.02229v3)

> Over the past few years, various word-level textual attack approaches have been proposed to reveal the vulnerability of deep neural networks used in natural language processing. Typically, these approaches involve an important optimization step to determine which substitute to be used for each word in the original input. However, current research on this step is still rather limited, from the perspectives of both problem-understanding and problem-solving. In this paper, we address these issues by uncovering the theoretical properties of the problem and proposing an efficient local search algorithm (LS) to solve it. We establish the first provable approximation guarantee on solving the problem in general cases.Extensive experiments involving 5 NLP tasks, 8 datasets and 26 NLP models show that LS can largely reduce the number of queries usually by an order of magnitude to achieve high attack success rates. Further experiments show that the adversarial examples crafted by LS usually have higher quality, exhibit better transferability, and can bring more robustness improvement to victim models by adversarial training.

</details>

<details>

<summary>2021-11-22 15:21:30 - SOMPS-Net : Attention based social graph framework for early detection of fake health news</summary>

- *Prasannakumaran D, Harish Srinivasan, Sowmiya Sree S, Sri Gayathri Devi I, Saikrishnan S, Vineeth Vijayaraghavan*

- `2111.11272v1` - [abs](http://arxiv.org/abs/2111.11272v1) - [pdf](http://arxiv.org/pdf/2111.11272v1)

> Fake news is fabricated information that is presented as genuine, with intention to deceive the reader. Recently, the magnitude of people relying on social media for news consumption has increased significantly. Owing to this rapid increase, the adverse effects of misinformation affect a wider audience. On account of the increased vulnerability of people to such deceptive fake news, a reliable technique to detect misinformation at its early stages is imperative. Hence, the authors propose a novel graph-based framework SOcial graph with Multi-head attention and Publisher information and news Statistics Network (SOMPS-Net) comprising of two components - Social Interaction Graph (SIG) and Publisher and News Statistics (PNS). The posited model is experimented on the HealthStory dataset and generalizes across diverse medical topics including Cancer, Alzheimer's, Obstetrics, and Nutrition. SOMPS-Net significantly outperformed other state-of-the-art graph-based models experimented on HealthStory by 17.1%. Further, experiments on early detection demonstrated that SOMPS-Net predicted fake news articles with 79% certainty within just 8 hours of its broadcast. Thus the contributions of this work lay down the foundation for capturing fake health news across multiple medical topics at its early stages.

</details>

<details>

<summary>2021-11-22 16:26:38 - SCOPE: Secure Compiling of PLCs in Cyber-Physical Systems</summary>

- *Eyasu Getahun Chekole, Martin Ochoa, Sudipta Chattopadhyay*

- `2012.12529v4` - [abs](http://arxiv.org/abs/2012.12529v4) - [pdf](http://arxiv.org/pdf/2012.12529v4)

> Cyber-Physical Systems (CPS) are being widely adopted in critical infrastructures, such as smart grids, nuclear plants, water systems, transportation systems, manufacturing and healthcare services, among others. However, the increasing prevalence of cyberattacks targeting them raises a growing security concern in the domain. In particular, memory-safety attacks, that exploit memory-safety vulnerabilities, constitute a major attack vector against real-time control devices in CPS. Traditional IT countermeasures against such attacks have limitations when applied to the CPS context: they typically incur in high runtime overheads; which conflicts with real-time constraints in CPS and they often abort the program when an attack is detected, thus harming availability of the system, which in turn can potentially result in damage to the physical world. In this work, we propose to enforce a full-stack memory-safety (covering user-space and kernel-space attack surfaces) based on secure compiling of PLCs to detect memory-safety attacks in CPS. Furthermore, to ensure availability, we enforce a resilient mitigation technique that bypasses illegal memory access instructions at runtime by dynamically instrumenting low-level code. We empirically measure the computational overhead caused by our approach on two experimental settings based on real CPS. The experimental results show that our approach effectively and efficiently detects and mitigates memory-safety attacks in realistic CPS.

</details>

<details>

<summary>2021-11-22 19:16:10 - Threat Modeling and Security Analysis of Containers: A Survey</summary>

- *Ann Yi Wong, Eyasu Getahun Chekole, Martin Ochoa, Jianying Zhou*

- `2111.11475v1` - [abs](http://arxiv.org/abs/2111.11475v1) - [pdf](http://arxiv.org/pdf/2111.11475v1)

> Traditionally, applications that are used in large and small enterprises were deployed on "bare metal" servers installed with operating systems. Recently, the use of multiple virtual machines (VMs) on the same physical server was adopted due to cost reduction and flexibility. Nowadays, containers have become popular for application deployment due to smaller footprints than the VMs, their ability to start and stop more quickly, and their capability to pack the application binaries and their dependencies in standalone units for seamless portability. A typical container ecosystem includes a code repository (e.g., GitHub) where the container images are built from the codes and libraries and then pushed to the image registry (e.g., Docker Hub) for subsequent deployment as application containers. However, the pervasive use of containers also leads to a wide-range of security breaches, such as stealing credentials and sensitive data from image registry and code repository, carrying out DoS attacks, and gaining root access to the underlying host. In this paper, we first perform threat modeling on the containers ecosystem using the popular threat modeling framework, called STRIDE. Using STRIDE, we identify the vulnerabilities in each system component, and investigate potential security threats and their consequences. Then, we conduct a comprehensive survey on the existing countermeasures designed against the identified threats and vulnerabilities. In particular, we assess the strengths and weaknesses of the existing mitigation strategies designed against such threats. We believe that this work will help researchers and practitioners to gain a deeper understanding of the threat landscape in containers and the state-of-the-art countermeasures. We also discuss open research problems and future research directions in containers security, which may ignite further research to be done in this area.

</details>

<details>

<summary>2021-11-23 02:27:08 - Developments in Connected Vehicles and the Requirement for Increased Cybersecurity</summary>

- *Phillip Garrad, Shane Gilroy*

- `2111.11612v1` - [abs](http://arxiv.org/abs/2111.11612v1) - [pdf](http://arxiv.org/pdf/2111.11612v1)

> The increase in popularity of connected features in intelligent transportation systems, has led to a greater risk of cyber-attacks and subsequently, requires a more robust validation of cybersecurity in vehicle design. This article explores three such cyber-attacks and the weaknesses in the connected networks. A review is carried out on current vulnerabilities and key considerations for future vehicle design and validation are highlighted. This article addresses the vehicle manufactures desire to add unnecessary remote connections without appropriate security analysis and assessment of the risks involved. The modern vehicle is All Connected and only as strong as its weakest link.

</details>

<details>

<summary>2021-11-23 09:08:25 - Towards an Integrated Penetration Testing Environment for the CAN Protocol</summary>

- *Giampaolo Bella, Pietro Biondi*

- `2111.11732v1` - [abs](http://arxiv.org/abs/2111.11732v1) - [pdf](http://arxiv.org/pdf/2111.11732v1)

> The Controller Area Network (CAN) is the most common protocol interconnecting the various control units of modern cars. Its vulnerabilities are somewhat known but we argue they are not yet fully explored -- although the protocol is obviously not secure by design, it remains to be thoroughly assessed how and to what extent it can be maliciously exploited. This manuscript describes the early steps towards a larger goal, that of integrating the various CAN pentesting activities together and carry them out holistically within an established pentesting environment such as the Metasploit Framework. In particular, we shall see how to build an exploit that upsets a simulated tachymeter running on a minimal Linux machine. While both portions are freely available from the authors' Github shares, the exploit is currently subject to a Metasploit pull request.

</details>

<details>

<summary>2021-11-23 13:41:29 - Is this IoT Device Likely to be Secure? Risk Score Prediction for IoT Devices Using Gradient Boosting Machines</summary>

- *Carlos A. Rivera Alvarez, Arash Shaghaghi, David D. Nguyen, Salil S. Kanhere*

- `2111.11874v1` - [abs](http://arxiv.org/abs/2111.11874v1) - [pdf](http://arxiv.org/pdf/2111.11874v1)

> Security risk assessment and prediction are critical for organisations deploying Internet of Things (IoT) devices. An absolute minimum requirement for enterprises is to verify the security risk of IoT devices for the reported vulnerabilities in the National Vulnerability Database (NVD). This paper proposes a novel risk prediction for IoT devices based on publicly available information about them. Our solution provides an easy and cost-efficient solution for enterprises of all sizes to predict the security risk of deploying new IoT devices. After an extensive analysis of the NVD records over the past eight years, we have created a unique, systematic, and balanced dataset for vulnerable IoT devices, including key technical features complemented with functional and descriptive features available from public resources. We then use machine learning classification models such as Gradient Boosting Decision Trees (GBDT) over this dataset and achieve 71% prediction accuracy in classifying the severity of device vulnerability score.

</details>

<details>

<summary>2021-11-24 06:35:38 - Poisoning Attacks to Local Differential Privacy Protocols for Key-Value Data</summary>

- *Yongji Wu, Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong*

- `2111.11534v2` - [abs](http://arxiv.org/abs/2111.11534v2) - [pdf](http://arxiv.org/pdf/2111.11534v2)

> Local Differential Privacy (LDP) protocols enable an untrusted server to perform privacy-preserving, federated data analytics. Various LDP protocols have been developed for different types of data such as categorical data, numerical data, and key-value data. Due to their distributed settings, LDP protocols are fundamentally vulnerable to poisoning attacks, in which fake users manipulate the server's analytics results via sending carefully crafted data to the server. However, existing poisoning attacks focused on LDP protocols for simple data types such as categorical and numerical data, leaving the security of LDP protocols for more advanced data types such as key-value data unexplored.   In this work, we aim to bridge the gap by introducing novel poisoning attacks to LDP protocols for key-value data. In such a LDP protocol, a server aims to simultaneously estimate the frequency and mean value of each key among some users, each of whom possesses a set of key-value pairs. Our poisoning attacks aim to simultaneously maximize the frequencies and mean values of some attacker-chosen target keys via sending carefully crafted data from some fake users to the sever. Specifically, since our attacks have two objectives, we formulate them as a two-objective optimization problem. Moreover, we propose a method to approximately solve the two-objective optimization problem, from which we obtain the optimal crafted data the fake users should send to the server. We demonstrate the effectiveness of our attacks to three LDP protocols for key-value data both theoretically and empirically. We also explore two defenses against our attacks, which are effective in some scenarios but have limited effectiveness in other scenarios. Our results highlight the needs for new defenses against our poisoning attacks.

</details>

<details>

<summary>2021-11-24 14:55:52 - Secure Random Sampling in Differential Privacy</summary>

- *Naoise Holohan, Stefano Braghin*

- `2107.10138v2` - [abs](http://arxiv.org/abs/2107.10138v2) - [pdf](http://arxiv.org/pdf/2107.10138v2)

> Differential privacy is among the most prominent techniques for preserving privacy of sensitive data, oweing to its robust mathematical guarantees and general applicability to a vast array of computations on data, including statistical analysis and machine learning. Previous work demonstrated that concrete implementations of differential privacy mechanisms are vulnerable to statistical attacks. This vulnerability is caused by the approximation of real values to floating point numbers. This paper presents a practical solution to the finite-precision floating point vulnerability, where the inverse transform sampling of the Laplace distribution can itself be inverted, thus enabling an attack where the original value can be retrieved with non-negligible advantage. The proposed solution has the advantages of being generalisable to any infinitely divisible probability distribution, and of simple implementation in modern architectures. Finally, the solution has been designed to make side channel attack infeasible, because of inherently exponential, in the size of the domain, brute force attacks.

</details>

<details>

<summary>2021-11-24 16:56:15 - WFDefProxy: Modularly Implementing and Empirically Evaluating Website Fingerprinting Defenses</summary>

- *Jiajun Gong, Wuqi Zhang, Charles Zhang, Tao Wang*

- `2111.12629v1` - [abs](http://arxiv.org/abs/2111.12629v1) - [pdf](http://arxiv.org/pdf/2111.12629v1)

> Tor, an onion-routing anonymity network, has been shown to be vulnerable to Website Fingerprinting (WF), which de-anonymizes web browsing by analyzing the unique characteristics of the encrypted network traffic. Although many defenses have been proposed, few have been implemented and tested in the real world; others were only simulated. Due to its synthetic nature, simulation may fail to capture the real performance of these defenses. To figure out how these defenses perform in the real world, we propose WFDefProxy, a general platform for WF defense implementation on Tor using pluggable transports. We create the first full implementation of three WF defenses: FRONT, Tamaraw and Random-WT. We evaluate each defense in both simulation and implementation to compare their results, and we find that simulation correctly captures the strength of each defense against attacks. In addition, we confirm that Random-WT is not effective in both simulation and implementation, reducing the strongest attacker's accuracy by only 7%.   We also found a minor difference in overhead between simulation and implementation. We analyze how this may be due to assumptions made in simulation regarding packet delays and queuing, or the soft stop condition we implemented in WFDefProxy to detect the end of a page load. The implementation of FRONT cost about 23% more data overhead than simulation, while the implementation of Tamaraw cost about 28% - 45% less data overhead. In addition, the implementation of Tamaraw incurred only 21% time overhead, compared to 51% - 242% estimated by simulation in previous work.

</details>

<details>

<summary>2021-11-24 18:52:56 - Robust and Differentially Private Mean Estimation</summary>

- *Xiyang Liu, Weihao Kong, Sham Kakade, Sewoong Oh*

- `2102.09159v2` - [abs](http://arxiv.org/abs/2102.09159v2) - [pdf](http://arxiv.org/pdf/2102.09159v2)

> In statistical learning and analysis from shared data, which is increasingly widely adopted in platforms such as federated learning and meta-learning, there are two major concerns: privacy and robustness. Each participating individual should be able to contribute without the fear of leaking one's sensitive information. At the same time, the system should be robust in the presence of malicious participants inserting corrupted data. Recent algorithmic advances in learning from shared data focus on either one of these threats, leaving the system vulnerable to the other. We bridge this gap for the canonical problem of estimating the mean from i.i.d. samples. We introduce PRIME, which is the first efficient algorithm that achieves both privacy and robustness for a wide range of distributions. We further complement this result with a novel exponential time algorithm that improves the sample complexity of PRIME, achieving a near-optimal guarantee and matching a known lower bound for (non-robust) private mean estimation. This proves that there is no extra statistical cost to simultaneously guaranteeing privacy and robustness.

</details>

<details>

<summary>2021-11-24 22:05:32 - Estimating g-Leakage via Machine Learning</summary>

- *Marco Romanelli, Konstantinos Chatzikokolakis, Catuscia Palamidessi, Pablo Piantanida*

- `2005.04399v3` - [abs](http://arxiv.org/abs/2005.04399v3) - [pdf](http://arxiv.org/pdf/2005.04399v3)

> This paper considers the problem of estimating the information leakage of a system in the black-box scenario. It is assumed that the system's internals are unknown to the learner, or anyway too complicated to analyze, and the only available information are pairs of input-output data samples, possibly obtained by submitting queries to the system or provided by a third party. Previous research has mainly focused on counting the frequencies to estimate the input-output conditional probabilities (referred to as frequentist approach), however this method is not accurate when the domain of possible outputs is large. To overcome this difficulty, the estimation of the Bayes error of the ideal classifier was recently investigated using Machine Learning (ML) models and it has been shown to be more accurate thanks to the ability of those models to learn the input-output correspondence. However, the Bayes vulnerability is only suitable to describe one-try attacks. A more general and flexible measure of leakage is the g-vulnerability, which encompasses several different types of adversaries, with different goals and capabilities. In this paper, we propose a novel approach to perform black-box estimation of the g-vulnerability using ML. A feature of our approach is that it does not require to estimate the conditional probabilities, and that it is suitable for a large class of ML algorithms. First, we formally show the learnability for all data distributions. Then, we evaluate the performance via various experiments using k-Nearest Neighbors and Neural Networks. Our results outperform the frequentist approach when the observables domain is large.

</details>

<details>

<summary>2021-11-25 09:04:41 - Normal vs. Adversarial: Salience-based Analysis of Adversarial Samples for Relation Extraction</summary>

- *Luoqiu Li, Xiang Chen, Zhen Bi, Xin Xie, Shumin Deng, Ningyu Zhang, Chuanqi Tan, Mosha Chen, Huajun Chen*

- `2104.00312v4` - [abs](http://arxiv.org/abs/2104.00312v4) - [pdf](http://arxiv.org/pdf/2104.00312v4)

> Recent neural-based relation extraction approaches, though achieving promising improvement on benchmark datasets, have reported their vulnerability towards adversarial attacks. Thus far, efforts mostly focused on generating adversarial samples or defending adversarial attacks, but little is known about the difference between normal and adversarial samples. In this work, we take the first step to leverage the salience-based method to analyze those adversarial samples. We observe that salience tokens have a direct correlation with adversarial perturbations. We further find the adversarial perturbations are either those tokens not existing in the training set or superficial cues associated with relation labels. To some extent, our approach unveils the characters against adversarial samples. We release an open-source testbed, "DiagnoseAdv" in https://github.com/zjunlp/DiagnoseAdv.

</details>

<details>

<summary>2021-11-25 17:12:14 - Simple Post-Training Robustness Using Test Time Augmentations and Random Forest</summary>

- *Gilad Cohen, Raja Giryes*

- `2109.08191v2` - [abs](http://arxiv.org/abs/2109.08191v2) - [pdf](http://arxiv.org/pdf/2109.08191v2)

> Although Deep Neural Networks (DNNs) achieve excellent performance on many real-world tasks, they are highly vulnerable to adversarial attacks. A leading defense against such attacks is adversarial training, a technique in which a DNN is trained to be robust to adversarial attacks by introducing adversarial noise to its input. This procedure is effective but must be done during the training phase. In this work, we propose Augmented Random Forest (ARF), a simple and easy-to-use strategy for robustifying an existing pretrained DNN without modifying its weights. For every image, we generate randomized test time augmentations by applying diverse color, blur, noise, and geometric transforms. Then we use the DNN's logits output to train a simple random forest to predict the real class label. Our method achieves state-of-the-art adversarial robustness on a diversity of white and black box attacks with minimal compromise on the natural images' classification. We test ARF also against numerous adaptive white-box attacks and it shows excellent results when combined with adversarial training. Code is available at https://github.com/giladcohen/ARF.

</details>

<details>

<summary>2021-11-25 20:32:15 - Going Grayscale: The Road to Understanding and Improving Unlearnable Examples</summary>

- *Zhuoran Liu, Zhengyu Zhao, Alex Kolmus, Tijn Berns, Twan van Laarhoven, Tom Heskes, Martha Larson*

- `2111.13244v1` - [abs](http://arxiv.org/abs/2111.13244v1) - [pdf](http://arxiv.org/pdf/2111.13244v1)

> Recent work has shown that imperceptible perturbations can be applied to craft unlearnable examples (ULEs), i.e. images whose content cannot be used to improve a classifier during training. In this paper, we reveal the road that researchers should follow for understanding ULEs and improving ULEs as they were originally formulated (ULEOs). The paper makes four contributions. First, we show that ULEOs exploit color and, consequently, their effects can be mitigated by simple grayscale pre-filtering, without resorting to adversarial training. Second, we propose an extension to ULEOs, which is called ULEO-GrayAugs, that forces the generated ULEs away from channel-wise color perturbations by making use of grayscale knowledge and data augmentations during optimization. Third, we show that ULEOs generated using Multi-Layer Perceptrons (MLPs) are effective in the case of complex Convolutional Neural Network (CNN) classifiers, suggesting that CNNs suffer specific vulnerability to ULEs. Fourth, we demonstrate that when a classifier is trained on ULEOs, adversarial training will prevent a drop in accuracy measured both on clean images and on adversarial images. Taken together, our contributions represent a substantial advance in the state of art of unlearnable examples, but also reveal important characteristics of their behavior that must be better understood in order to achieve further improvements.

</details>

<details>

<summary>2021-11-26 21:57:42 - ZLeaks: Passive Inference Attacks on Zigbee based Smart Homes</summary>

- *Narmeen Shafqat, Daniel J. Dubois, David Choffnes, Aaron Schulman, Dinesh Bharadia, Aanjhan Ranganathan*

- `2107.10830v2` - [abs](http://arxiv.org/abs/2107.10830v2) - [pdf](http://arxiv.org/pdf/2107.10830v2)

> Zigbee is an energy-efficient wireless IoT protocol that is increasingly being deployed in smart home settings. In this work, we analyze the privacy guarantees of Zigbee protocol. Specifically, we present ZLeaks, a tool that passively identifies in-home devices or events from the encrypted Zigbee traffic by 1) inferring a single application layer (APL) command in the event's traffic, and 2) exploiting the device's periodic reporting pattern and interval. This enables an attacker to infer user's habits or determine if the smart home is vulnerable to unauthorized entry. We evaluated ZLeaks' efficacy on 19 unique Zigbee devices across several categories and 5 popular smart hubs in three different scenarios; controlled RF shield, living smart-home IoT lab, and third-party Zigbee captures. We were able to i) identify unknown events and devices (without a-priori device signatures) using command inference approach with 83.6% accuracy, ii) automatically extract device's reporting signatures, iii) determine known devices using the reporting signatures with 99.8% accuracy, and iv) identify APL commands in a public capture with 91.2% accuracy. In short, we highlight the trade-off between designing a low-power, low-cost wireless network and achieving privacy guarantees. We have also released ZLeaks tool for the benefit of the research community.

</details>

<details>

<summary>2021-11-27 10:28:07 - The Global State of Security in Industrial Control Systems: An Empirical Analysis of Vulnerabilities around the World</summary>

- *Simon Daniel Duque Anton, Daniel Fraunholz, Daniel Krohmer, Daniel Reti, Daniel Schneider, Hans Dieter Schotten*

- `2111.13862v1` - [abs](http://arxiv.org/abs/2111.13862v1) - [pdf](http://arxiv.org/pdf/2111.13862v1)

> Operational Technology (OT)-networks and -devices, i.e. all components used in industrial environments, were not designed with security in mind. Efficiency and ease of use were the most important design characteristics. However, due to the digitisation of industry, an increasing number of devices and industrial networks is opened up to public networks. This is beneficial for administration and organisation of the industrial environments. However, it also increases the attack surface, providing possible points of entry for an attacker. Originally, breaking into production networks meant to break an Information Technology (IT)-perimeter first, such as a public website, and then to move laterally to Industrial Control Systems (ICSs) to influence the production environment. However, many OT-devices are connected directly to the Internet, which drastically increases the threat of compromise, especially since OT-devices contain several vulnerabilities. In this work, the presence of OT-devices in the Internet is analysed from an attacker's perspective. Publicly available tools, such as the search engine Shodan and vulnerability databases, are employed to find commonly used OT-devices and map vulnerabilities to them. These findings are grouped according to country of origin, manufacturer, and number as well as severity of vulnerability. More than 13000 devices were found, almost all contained at least one vulnerability. European and Northern American countries are by far the most affected ones.

</details>

<details>

<summary>2021-11-27 22:59:31 - Addressing Feature Suppression in Unsupervised Visual Representations</summary>

- *Tianhong Li, Lijie Fan, Yuan Yuan, Hao He, Yonglong Tian, Rogerio Feris, Piotr Indyk, Dina Katabi*

- `2012.09962v5` - [abs](http://arxiv.org/abs/2012.09962v5) - [pdf](http://arxiv.org/pdf/2012.09962v5)

> Contrastive learning is one of the fastest growing research areas in machine learning due to its ability to learn useful representations without labeled data. However, contrastive learning is susceptible to feature suppression, i.e., it may discard important information relevant to the task of interest, and learn irrelevant features. Past work has addressed this limitation via handcrafted data augmentations that eliminate irrelevant information. This approach however does not work across all datasets and tasks. Further, data augmentations fail in addressing feature suppression in multi-attribute classification when one attribute can suppress features relevant to other attributes. In this paper, we analyze the objective function of contrastive learning and formally prove that it is vulnerable to feature suppression. We then present predictive contrastive learning (PCL), a framework for learning unsupervised representations that are robust to feature suppression. The key idea is to force the learned representation to predict the input, and hence prevent it from discarding important information. Extensive experiments verify that PCL is robust to feature suppression and outperforms state-of-the-art contrastive learning methods on a variety of datasets and tasks.

</details>

<details>

<summary>2021-11-28 07:41:40 - A Comprehensive and Cross-Platform Test Suite for Memory Safety -- Towards an Open Framework for Testing Processor Hardware Supported Security Extensions</summary>

- *Wei Song, Jiameng Ying, Sihao Shen, Boya Li, Hao Ma, Peng Liu*

- `2111.14072v1` - [abs](http://arxiv.org/abs/2111.14072v1) - [pdf](http://arxiv.org/pdf/2111.14072v1)

> Memory safety remains a critical and widely violated property in reality. Numerous defense techniques have been proposed and developed but most of them are not applied or enabled by default in production-ready environment due to their substantial running cost. The situation might change in the near future because the hardware supported defenses against these attacks are finally beginning to be adopted by commercial processors, operating systems and compilers. We then face a question as there is currently no suitable test suite to measure the memory safety extensions supported on different processors. In fact, the issue is not constrained only for memory safety but all aspect of processor security. All of the existing test suites related to processor security lack some of the key properties, such as comprehensiveness, distinguishability and portability.   As an initial step, we propose an expandable test framework for measuring the processor security and open source a memory safety test suite utilizing this framework. The framework is deliberately designed to be flexible so it can be gradually extended to all types of hardware supported security extensions in processors. The initial test suite for memory safety currently contains 160 test cases covering spatial and temporal safety of memory, memory access control, pointer integrity and control-flow integrity. Each type of vulnerabilities and their related defenses have been individually evaluated by one or more test cases. The test suite has been ported to three different instruction set architectures (ISAs) and experimented on six different platforms. We have also utilized the test suite to explore the security benefits of applying different sets of compiler flags available on the latest GNU GCC and LLVM compilers.

</details>

<details>

<summary>2021-11-28 19:46:43 - Hand Me Your PIN! Inferring ATM PINs of Users Typing with a Covered Hand</summary>

- *Matteo Cardaioli, Stefano Cecconello, Mauro Conti, Simone Milani, Stjepan Picek, Eugen Saraci*

- `2110.08113v3` - [abs](http://arxiv.org/abs/2110.08113v3) - [pdf](http://arxiv.org/pdf/2110.08113v3)

> Automated Teller Machines (ATMs) represent the most used system for withdrawing cash. The European Central Bank reported more than 11 billion cash withdrawals and loading/unloading transactions on the European ATMs in 2019. Although ATMs have undergone various technological evolutions, Personal Identification Numbers (PINs) are still the most common authentication method for these devices. Unfortunately, the PIN mechanism is vulnerable to shoulder-surfing attacks performed via hidden cameras installed near the ATM to catch the PIN pad. To overcome this problem, people get used to covering the typing hand with the other hand. While such users probably believe this behavior is safe enough to protect against mentioned attacks, there is no clear assessment of this countermeasure in the scientific literature.   This paper proposes a novel attack to reconstruct PINs entered by victims covering the typing hand with the other hand. We consider the setting where the attacker can access an ATM PIN pad of the same brand/model as the target one. Afterward, the attacker uses that model to infer the digits pressed by the victim while entering the PIN. Our attack owes its success to a carefully selected deep learning architecture that can infer the PIN from the typing hand position and movements. We run a detailed experimental analysis including 58 users. With our approach, we can guess 30% of the 5-digit PINs within three attempts -- the ones usually allowed by ATM before blocking the card. We also conducted a survey with 78 users that managed to reach an accuracy of only 7.92% on average for the same setting. Finally, we evaluate a shielding countermeasure that proved to be rather inefficient unless the whole keypad is shielded.

</details>

<details>

<summary>2021-11-29 04:35:14 - BigFoot: Exploiting and Mitigating Leakage in Encrypted Write-Ahead Logs</summary>

- *Jialing Pei, Vitaly Shmatikov*

- `2111.09374v2` - [abs](http://arxiv.org/abs/2111.09374v2) - [pdf](http://arxiv.org/pdf/2111.09374v2)

> Modern databases and data-warehousing systems separate query processing and durable storage. Storage systems have idiosyncratic bugs and security vulnerabilities, thus attacks that compromise only storage are a realistic threat. In this paper, we show that encryption alone is not sufficient to protect databases from compromised storage. Using MongoDB WiredTiger as a concrete example, we demonstrate that sizes of encrypted writes to a durable write-ahead log can reveal sensitive information about the inputs and activities of MongoDB applications. We then design, implement, and evaluate BigFoot, a WAL modification that mitigates size leakage.

</details>

<details>

<summary>2021-11-29 06:20:44 - The effect of the COVID-19 pandemic on gendered research productivity and its correlates</summary>

- *Eunrang Kwon, Jinhyuk Yun, Jeong-han Kang*

- `2111.14342v1` - [abs](http://arxiv.org/abs/2111.14342v1) - [pdf](http://arxiv.org/pdf/2111.14342v1)

> Female researchers may have experienced more difficulties than their male counterparts since the COVID-19 outbreak because of gendered housework and childcare. Using Microsoft Academic Graph data from 2016 to 2020, this study examined how the proportion of female authors in academic journals on a global scale changed in 2020 (net of recent yearly trends). We observed a decrease in research productivity for female researchers in 2020, mostly as first authors, followed by last author position. Female researchers were not necessarily excluded from but were marginalised in research. We also identified various factors that amplified the gender gap by dividing the authors' backgrounds into individual, organisational and national characteristics. Female researchers were more vulnerable when they were in their mid-career, affiliated to the least influential organisations, and more importantly from less gender-equal countries with higher mortality and restricted mobility as a result of COVID-19.

</details>

<details>

<summary>2021-11-30 03:11:38 - Adversarial Robustness of Deep Code Comment Generation</summary>

- *Yu Zhou, Xiaoqing Zhang, Juanjuan Shen, Tingting Han, Taolue Chen, Harald Gall*

- `2108.00213v3` - [abs](http://arxiv.org/abs/2108.00213v3) - [pdf](http://arxiv.org/pdf/2108.00213v3)

> Deep neural networks (DNNs) have shown remarkable performance in a variety of domains such as computer vision, speech recognition, or natural language processing. Recently they also have been applied to various software engineering tasks, typically involving processing source code. DNNs are well-known to be vulnerable to adversarial examples, i.e., fabricated inputs that could lead to various misbehaviors of the DNN model while being perceived as benign by humans. In this paper, we focus on the code comment generation task in software engineering and study the robustness issue of the DNNs when they are applied to this task. We propose ACCENT, an identifier substitution approach to craft adversarial code snippets, which are syntactically correct and semantically close to the original code snippet, but may mislead the DNNs to produce completely irrelevant code comments. In order to improve the robustness, ACCENT also incorporates a novel training method, which can be applied to existing code comment generation models. We conduct comprehensive experiments to evaluate our approach by attacking the mainstream encoder-decoder architectures on two large-scale publicly available datasets. The results show that ACCENT efficiently produces stable attacks with functionality-preserving adversarial examples, and the generated examples have better transferability compared with baselines. We also confirm, via experiments, the effectiveness in improving model robustness with our training method.

</details>

<details>

<summary>2021-11-30 05:32:48 - Security Issues in Language-based Software Ecosystems</summary>

- *Ruturaj K. Vaidya, Lorenzo De Carli, Drew Davidson, Vaibhav Rastogi*

- `1903.02613v2` - [abs](http://arxiv.org/abs/1903.02613v2) - [pdf](http://arxiv.org/pdf/1903.02613v2)

> Language-based ecosystems (LBE), i.e., software ecosystems based on a single programming language, are very common. Examples include the npm ecosystem for JavaScript, and PyPI for Python. These environments encourage code reuse between packages, and incorporate utilities - package managers - for automatically resolving dependencies. However, the same aspects that make these systems popular - ease of publishing code and importing external code - also create novel security issues, which have so far seen little study.   We present an a systematic study of security issues that plague LBEs. These issues are inherent to the ways these ecosystems work and cannot be resolved by fixing software vulnerabilities in either the packages or the utilities, e.g., package manager tools, that build these ecosystems. We systematically characterize recent security attacks from various aspects, including attack strategies, vectors, and goals. Our characterization and in-depth analysis of npm and PyPI ecosystems, which represent the largest LBEs, covering nearly one million packages indicates that these ecosystems make an opportune environment for attackers to incorporate stealthy attacks.   Overall, we argue that (i) fully automated detection of malicious packages is likely to be unfeasible; however (ii) tools and metrics that help developers assess the risk of including external dependencies would go a long way toward preventing attacks.

</details>

<details>

<summary>2021-11-30 10:18:22 - Dos and Don'ts of Machine Learning in Computer Security</summary>

- *Daniel Arp, Erwin Quiring, Feargus Pendlebury, Alexander Warnecke, Fabio Pierazzi, Christian Wressnegger, Lorenzo Cavallaro, Konrad Rieck*

- `2010.09470v2` - [abs](http://arxiv.org/abs/2010.09470v2) - [pdf](http://arxiv.org/pdf/2010.09470v2)

> With the growing processing power of computing systems and the increasing availability of massive datasets, machine learning algorithms have led to major breakthroughs in many different areas. This development has influenced computer security, spawning a series of work on learning-based security systems, such as for malware detection, vulnerability discovery, and binary code analysis. Despite great potential, machine learning in security is prone to subtle pitfalls that undermine its performance and render learning-based systems potentially unsuitable for security tasks and practical deployment. In this paper, we look at this problem with critical eyes. First, we identify common pitfalls in the design, implementation, and evaluation of learning-based security systems. We conduct a study of 30 papers from top-tier security conferences within the past 10 years, confirming that these pitfalls are widespread in the current security literature. In an empirical analysis, we further demonstrate how individual pitfalls can lead to unrealistic performance and interpretations, obstructing the understanding of the security problem at hand. As a remedy, we propose actionable recommendations to support researchers in avoiding or mitigating the pitfalls where possible. Furthermore, we identify open problems when applying machine learning in security and provide directions for further research.

</details>


## 2021-12

<details>

<summary>2021-12-01 17:11:22 - Certified Adversarial Defenses Meet Out-of-Distribution Corruptions: Benchmarking Robustness and Simple Baselines</summary>

- *Jiachen Sun, Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, Dan Hendrycks, Jihun Hamm, Z. Morley Mao*

- `2112.00659v1` - [abs](http://arxiv.org/abs/2112.00659v1) - [pdf](http://arxiv.org/pdf/2112.00659v1)

> Certified robustness guarantee gauges a model's robustness to test-time attacks and can assess the model's readiness for deployment in the real world. In this work, we critically examine how the adversarial robustness guarantees from randomized smoothing-based certification methods change when state-of-the-art certifiably robust models encounter out-of-distribution (OOD) data. Our analysis demonstrates a previously unknown vulnerability of these models to low-frequency OOD data such as weather-related corruptions, rendering these models unfit for deployment in the wild. To alleviate this issue, we propose a novel data augmentation scheme, FourierMix, that produces augmentations to improve the spectral coverage of the training data. Furthermore, we propose a new regularizer that encourages consistent predictions on noise perturbations of the augmented data to improve the quality of the smoothed models. We find that FourierMix augmentations help eliminate the spectral bias of certifiably robust models enabling them to achieve significantly better robustness guarantees on a range of OOD benchmarks. Our evaluation also uncovers the inability of current OOD benchmarks at highlighting the spectral biases of the models. To this end, we propose a comprehensive benchmarking suite that contains corruptions from different regions in the spectral domain. Evaluation of models trained with popular augmentation methods on the proposed suite highlights their spectral biases and establishes the superiority of FourierMix trained models at achieving better-certified robustness guarantees under OOD shifts over the entire frequency spectrum.

</details>

<details>

<summary>2021-12-01 18:17:05 - They See Me Rollin': Inherent Vulnerability of the Rolling Shutter in CMOS Image Sensors</summary>

- *Sebastian Köhler, Giulio Lovisotto, Simon Birnbach, Richard Baker, Ivan Martinovic*

- `2101.10011v2` - [abs](http://arxiv.org/abs/2101.10011v2) - [pdf](http://arxiv.org/pdf/2101.10011v2)

> In this paper, we describe how the electronic rolling shutter in CMOS image sensors can be exploited using a bright, modulated light source (e.g., an inexpensive, off-the-shelf laser), to inject fine-grained image disruptions. We demonstrate the attack on seven different CMOS cameras, ranging from cheap IoT to semi-professional surveillance cameras, to highlight the wide applicability of the rolling shutter attack. We model the fundamental factors affecting a rolling shutter attack in an uncontrolled setting. We then perform an exhaustive evaluation of the attack's effect on the task of object detection, investigating the effect of attack parameters. We validate our model against empirical data collected on two separate cameras, showing that by simply using information from the camera's datasheet the adversary can accurately predict the injected distortion size and optimize their attack accordingly. We find that an adversary can hide up to 75% of objects perceived by state-of-the-art detectors by selecting appropriate attack parameters. We also investigate the stealthiness of the attack in comparison to a na\"{i}ve camera blinding attack, showing that common image distortion metrics can not detect the attack presence. Therefore, we present a new, accurate and lightweight enhancement to the backbone network of an object detector to recognize rolling shutter attacks. Overall, our results indicate that rolling shutter attacks can substantially reduce the performance and reliability of vision-based intelligent systems.

</details>

<details>

<summary>2021-12-01 22:18:00 - Common Bugs in Scratch Programs</summary>

- *Christoph Frädrich, Florian Obermüller, Nina Körber, Ute Heuer, Gordon Fraser*

- `2112.00858v1` - [abs](http://arxiv.org/abs/2112.00858v1) - [pdf](http://arxiv.org/pdf/2112.00858v1)

> Bugs in Scratch programs can spoil the fun and inhibit learning success. Many common bugs are the result of recurring patterns of bad code. In this paper we present a collection of common code patterns that typically hint at bugs in Scratch programs, and the LitterBox tool which can automatically detect them. We empirically evaluate how frequently these patterns occur, and how severe their consequences usually are. While fixing bugs inevitably is part of learning, the possibility to identify the bugs automatically provides the potential to support learners

</details>

<details>

<summary>2021-12-02 02:39:50 - Learning Task-aware Robust Deep Learning Systems</summary>

- *Keji Han, Yun Li, Xianzhong Long, Yao Ge*

- `2010.05125v2` - [abs](http://arxiv.org/abs/2010.05125v2) - [pdf](http://arxiv.org/pdf/2010.05125v2)

> Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep learning system. Our method can be viewed as improving the robustness of deep learning systems from both the learning task and deep model. Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracy.

</details>

<details>

<summary>2021-12-02 02:46:34 - Robust Sensor Fusion Algorithms Against Voice Command Attacks in Autonomous Vehicles</summary>

- *Jiwei Guan, Xi Zheng, Chen Wang, Yipeng Zhou, Alireza Jolfa*

- `2104.09872v3` - [abs](http://arxiv.org/abs/2104.09872v3) - [pdf](http://arxiv.org/pdf/2104.09872v3)

> With recent advances in autonomous driving, Voice Control Systems have become increasingly adopted as human-vehicle interaction methods. This technology enables drivers to use voice commands to control the vehicle and will be soon available in Advanced Driver Assistance Systems (ADAS). Prior work has shown that Siri, Alexa and Cortana, are highly vulnerable to inaudible command attacks. This could be extended to ADAS in real-world applications and such inaudible command threat is difficult to detect due to microphone nonlinearities. In this paper, we aim to develop a more practical solution by using camera views to defend against inaudible command attacks where ADAS are capable of detecting their environment via multi-sensors. To this end, we propose a novel multimodal deep learning classification system to defend against inaudible command attacks. Our experimental results confirm the feasibility of the proposed defense methods and the best classification accuracy reaches 89.2%. Code is available at https://github.com/ITSEG-MQ/Sensor-Fusion-Against-VoiceCommand-Attacks.

</details>

<details>

<summary>2021-12-02 09:54:05 - How BPE Affects Memorization in Transformers</summary>

- *Eugene Kharitonov, Marco Baroni, Dieuwke Hupkes*

- `2110.02782v2` - [abs](http://arxiv.org/abs/2110.02782v2) - [pdf](http://arxiv.org/pdf/2110.02782v2)

> Training data memorization in NLP can both be beneficial (e.g., closed-book QA) and undesirable (personal data extraction). In any case, successful model training requires a non-trivial amount of memorization to store word spellings, various linguistic idiosyncrasies and common knowledge. However, little is known about what affects the memorization behavior of NLP models, as the field tends to focus on the equally important question of generalization. In this work, we demonstrate that the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency of standard Transformer models to memorize training data, even when we control for the number of learned parameters. We find that with a large subword vocabulary size, Transformer models fit random mappings more easily and are more vulnerable to membership inference attacks. Similarly, given a prompt, Transformer-based language models with large subword vocabularies reproduce the training data more often. We conjecture this effect is caused by reduction in the sequences' length that happens as the BPE vocabulary grows. Our findings can allow a more informed choice of hyper-parameters, that is better tailored for a particular use-case.

</details>

<details>

<summary>2021-12-02 15:19:43 - Berserker: ASN.1-based Fuzzing of Radio Resource Control Protocol for 4G and 5G</summary>

- *Srinath Potnuru, Prajwol Kumar Nakarmi*

- `2107.01912v2` - [abs](http://arxiv.org/abs/2107.01912v2) - [pdf](http://arxiv.org/pdf/2107.01912v2)

> Telecom networks together with mobile phones must be rigorously tested for robustness against vulnerabilities in order to guarantee availability. RRC protocol is responsible for the management of radio resources and is among the most important telecom protocols whose extensive testing is warranted. To that end, we present a novel RRC fuzzer, called Berserker, for 4G and 5G. Berserker's novelty comes from being backward and forward compatible to any version of 4G and 5G RRC technical specifications. It is based on RRC message format definitions in ASN.1 and additionally covers fuzz testing of another protocol, called NAS, tunneled in RRC. Berserker uses concrete implementations of telecom protocol stack and is unaffected by lower layer protocol handlings like encryption and segmentation. It is also capable of evading size and type constraints in RRC message format definitions. Berserker discovered two previously unknown serious vulnerabilities in srsLTE -- one of which also affects openLTE -- confirming its applicability to telecom robustness.

</details>

<details>

<summary>2021-12-02 16:50:57 - FedRAD: Federated Robust Adaptive Distillation</summary>

- *Stefán Páll Sturluson, Samuel Trew, Luis Muñoz-González, Matei Grama, Jonathan Passerat-Palmbach, Daniel Rueckert, Amir Alansary*

- `2112.01405v1` - [abs](http://arxiv.org/abs/2112.01405v1) - [pdf](http://arxiv.org/pdf/2112.01405v1)

> The robustness of federated learning (FL) is vital for the distributed training of an accurate global model that is shared among large number of clients. The collaborative learning framework by typically aggregating model updates is vulnerable to model poisoning attacks from adversarial clients. Since the shared information between the global server and participants are only limited to model parameters, it is challenging to detect bad model updates. Moreover, real-world datasets are usually heterogeneous and not independent and identically distributed (Non-IID) among participants, which makes the design of such robust FL pipeline more difficult. In this work, we propose a novel robust aggregation method, Federated Robust Adaptive Distillation (FedRAD), to detect adversaries and robustly aggregate local models based on properties of the median statistic, and then performing an adapted version of ensemble Knowledge Distillation. We run extensive experiments to evaluate the proposed method against recently published works. The results show that FedRAD outperforms all other aggregators in the presence of adversaries, as well as in heterogeneous data distributions.

</details>

<details>

<summary>2021-12-02 19:46:13 - On the Documentation of Refactoring Types</summary>

- *Eman Abdullah AlOmar, Jiaqian Liu, Kenneth Addo, Mohamed Wiem Mkaouer, Christian Newman, Ali Ouni, Zhe Yu*

- `2112.01581v1` - [abs](http://arxiv.org/abs/2112.01581v1) - [pdf](http://arxiv.org/pdf/2112.01581v1)

> Commit messages are the atomic level of software documentation. They provide a natural language description of the code change and its purpose. Messages are critical for software maintenance and program comprehension. Unlike documenting feature updates and bug fixes, little is known about how developers document their refactoring activities. Developers can perform multiple refactoring operations, including moving methods, extracting classes, for various reasons. Yet, there is no systematic study that analyzes the extent to which the documentation of refactoring accurately describes the refactoring operations performed at the source code level. Therefore, this paper challenges the ability of refactoring documentation to adequately predict the refactoring types, performed at the commit level. Our analysis relies on the text mining of commit messages to extract the corresponding features that better represent each class. The extraction of text patterns, specific to each refactoring allows the design of a model that verifies the consistency of these patterns with their corresponding refactoring. Such verification process can be achieved via automatically predicting the method-level type of refactoring being applied, namely Extract Method, Inline Method, Move Method, Pull-up Method, Push-down Method, and Rename Method. We compared various classifiers, and a baseline keyword-based approach, in terms of their prediction performance, using a dataset of 5,004 commits. Our main findings show that the complexity of refactoring type prediction varies from one type to another. Rename method and Extract method were found to be the best documented refactoring activities, while Pull-up Method and Push-down Method were the hardest to be identified via textual descriptions. Such findings bring the attention of developers to the necessity of paying more attention to the documentation of these types.

</details>

<details>

<summary>2021-12-04 07:53:14 - Fast and Secure Key Generation with Channel Obfuscation in Slowly Varying Environments</summary>

- *Guyue Li, Haiyu Yang, Junqing Zhang, Hongbo Liu, Aiqun Hu*

- `2112.02273v1` - [abs](http://arxiv.org/abs/2112.02273v1) - [pdf](http://arxiv.org/pdf/2112.02273v1)

> The physical-layer secret key generation has emerged as a promising solution for establishing cryptographic keys by leveraging reciprocal and time-varying wireless channels. However, existing approaches suffer from low key generation rates and vulnerabilities under various attacks in slowly varying environments. We propose a new physical-layer secret key generation approach with channel obfuscation, which improves the dynamic property of channel parameters based on random filtering and random antenna scheduling. Our approach makes one party obfuscate the channel to allow the legitimate party to obtain similar dynamic channel parameters yet prevents a third party from inferring the obfuscation information. Our approach allows more random bits to be extracted from the obfuscated channel parameters by a joint design of the K-L transform and adaptive quantization. A testbed implementation shows that our approach, compared to the existing ones that we evaluate, performs the best in generating high entropy bits at a fast rate and a high-security level in slowly varying environments. Specifically, our approach can achieve a significantly faster secret bit generation rate at about $67$ bit/pkt, and the key sequences can pass the randomness tests of the NIST test suite.

</details>

<details>

<summary>2021-12-05 14:56:08 - Deep-Dive Analysis of Selfish and Stubborn Mining in Bitcoin and Ethereum</summary>

- *Runkai Yang, Xiaolin Chang, Jelena Mišić, Vojislav B. Mišić*

- `2112.02588v1` - [abs](http://arxiv.org/abs/2112.02588v1) - [pdf](http://arxiv.org/pdf/2112.02588v1)

> Bitcoin and Ethereum are the top two blockchain-based cryptocurrencies whether from cryptocurrency market cap or popularity. However, they are vulnerable to selfish mining and stubborn mining due to that both of them adopt Proof-of-Work consensus mechanism. In this paper, we develop a novel Markov model, which can study selfish mining and seven kinds of stubborn mining in both Bitcoin and Ethereum. The formulas are derived to calculate several key metrics, including relative revenue of miners, blockchain performance in terms of stale block ratio and transactions per second, and blockchain security in terms of resistance against double-spending attacks. Numerical analysis is conducted to investigate the quantitative relationship between the relative-revenue-optimal mining strategy for malicious miners and two miner features in Bitcoin and Ethereum, respectively. The quantitative analysis results can assist honest miners in detecting whether there is any malicious miner in the system and setting the threshold of mining node's hash power in order to prevent malicious miners from making profit through selfish and stubborn mining.

</details>

<details>

<summary>2021-12-05 18:40:32 - VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning</summary>

- *Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan Vasilescu, Claire Le Goues*

- `2112.02650v1` - [abs](http://arxiv.org/abs/2112.02650v1) - [pdf](http://arxiv.org/pdf/2112.02650v1)

> Variable names are critical for conveying intended program behavior. Machine learning-based program analysis methods use variable name representations for a wide range of tasks, such as suggesting new variable names and bug detection. Ideally, such methods could capture semantic relationships between names beyond syntactic similarity, e.g., the fact that the names average and mean are similar. Unfortunately, previous work has found that even the best of previous representation approaches primarily capture relatedness (whether two variables are linked at all), rather than similarity (whether they actually have the same meaning).   We propose VarCLR, a new approach for learning semantic representations of variable names that effectively captures variable similarity in this stricter sense. We observe that this problem is an excellent fit for contrastive learning, which aims to minimize the distance between explicitly similar inputs, while maximizing the distance between dissimilar inputs. This requires labeled training data, and thus we construct a novel, weakly-supervised variable renaming dataset mined from GitHub edits. We show that VarCLR enables the effective application of sophisticated, general-purpose language models like BERT, to variable name representation and thus also to related downstream tasks like variable name similarity search or spelling correction. VarCLR produces models that significantly outperform the state-of-the-art on IdBench, an existing benchmark that explicitly captures variable similarity (as distinct from relatedness). Finally, we contribute a release of all data, code, and pre-trained models, aiming to provide a drop-in replacement for variable representations used in either existing or future program analyses that rely on variable names.

</details>

<details>

<summary>2021-12-06 03:17:23 - Review of Data Integrity Attacks and Mitigation Methods in Edge computing</summary>

- *Poornima Mahadevappa, Raja Kumar Murugesan*

- `2112.02757v1` - [abs](http://arxiv.org/abs/2112.02757v1) - [pdf](http://arxiv.org/pdf/2112.02757v1)

> In recent years, edge computing has emerged as a promising technology due to its unique feature of real-time computing and parallel processing. They provide computing and storage capacity closer to the data source and bypass the distant links to the cloud. The edge data analytics process the ubiquitous data on the edge layer to offer real-time interactions for the application. However, this process can be prone to security threats like gaining malicious access or manipulating sensitive data. This can lead to the intruder's control, alter, or add erroneous data affecting the integrity and data analysis efficiency. Due to the lack of transparency of stakeholders processing edge data, it is challenging to identify the vulnerabilities. Many reviews are available on data security issues on the edge layer; however, they do not address integrity issues exclusively. Therefore, this paper concentrates only on data integrity threats that directly influence edge data analysis. Further shortcomings in existing work are identified with few research directions.

</details>

<details>

<summary>2021-12-06 04:05:48 - Staring Down the Digital Fulda Gap Path Dependency as a Cyber Defense Vulnerability</summary>

- *Jan Kallberg*

- `2112.02773v1` - [abs](http://arxiv.org/abs/2112.02773v1) - [pdf](http://arxiv.org/pdf/2112.02773v1)

> Academia, homeland security, defense, and media have accepted the perception that critical infrastructure in a future cyber war cyber conflict is the main gateway for a massive cyber assault on the U.S. The question is not if the assumption is correct or not, the question is instead of how did we arrive at that assumption. The cyber paradigm considers critical infrastructure the primary attack vector for future cyber conflicts. The national vulnerability embedded in critical infrastructure is given a position in the cyber discourse as close to an unquestionable truth as a natural law.   The American reaction to Sept. 11, and any attack on U.S. soil, hint to an adversary that attacking critical infrastructure to create hardship for the population could work contrary to the intended softening of the will to resist foreign influence. It is more likely that attacks that affect the general population instead strengthen the will to resist and fight, similar to the British reaction to the German bombing campaign Blitzen in 1940. We cannot rule out attacks that affect the general population, but there are not enough adversarial offensive capabilities to attack all 16 critical infrastructure sectors and gain strategic momentum. An adversary has limited cyberattack capabilities and needs to prioritize cyber targets that are aligned with the overall strategy. Logically, an adversary will focus their OCO on operations that has national security implications and support their military operations by denying, degrading, and confusing the U.S. information environment and U.S. cyber assets.

</details>

<details>

<summary>2021-12-06 10:55:03 - A Software-Repair Robot based on Continual Learning</summary>

- *Benoit Baudry, Zimin Chen, Khashayar Etemadi, Han Fu, Davide Ginelli, Steve Kommrusch, Matias Martinez, Martin Monperrus, Javier Ron, He Ye, Zhongxing Yu*

- `2012.06824v4` - [abs](http://arxiv.org/abs/2012.06824v4) - [pdf](http://arxiv.org/pdf/2012.06824v4)

> Software bugs are common and correcting them accounts for a significant part of costs in the software development and maintenance process. This calls for automatic techniques to deal with them. One promising direction towards this goal is gaining repair knowledge from historical bug fixing examples. Retrieving insights from software development history is particularly appealing with the constant progress of machine learning paradigms and skyrocketing `big' bug fixing data generated through Continuous Integration (CI). In this paper, we present R-Hero, a novel software repair bot that applies continual learning to acquire bug fixing strategies from continuous streams of source code changes, implemented for the single development platform Github/Travis CI. We describe R-Hero, our novel system for learning how to fix bugs based on continual training, and we uncover initial successes as well as novel research challenges for the community.

</details>

<details>

<summary>2021-12-06 12:16:41 - Hyperstyle: A Tool for Assessing the Code Quality of Solutions to Programming Assignments</summary>

- *Anastasiia Birillo, Ilya Vlasov, Artyom Burylov, Vitalii Selishchev, Artyom Goncharov, Elena Tikhomirova, Nikolay Vyahhi, Timofey Bryksin*

- `2112.02963v1` - [abs](http://arxiv.org/abs/2112.02963v1) - [pdf](http://arxiv.org/pdf/2112.02963v1)

> In software engineering, it is not enough to simply write code that only works as intended, even if it is free from vulnerabilities and bugs. Every programming language has a style guide and a set of best practices defined by its community, which help practitioners to build solutions that have a clear structure and therefore are easy to read and maintain. To introduce assessment of code quality into the educational process, we developed a tool called Hyperstyle. To make it reflect the needs of the programming community and at the same time be easily extendable, we built it upon several existing professional linters and code checkers. Hyperstyle supports four programming languages (Python, Java, Kotlin, and Javascript) and can be used as a standalone tool or integrated into a MOOC platform. We have integrated the tool into two educational platforms, Stepik and JetBrains Academy, and it has been used to process about one million submissions every week since May 2021.

</details>

<details>

<summary>2021-12-06 19:10:23 - Adversarial Machine Learning In Network Intrusion Detection Domain: A Systematic Review</summary>

- *Huda Ali Alatwi, Charles Morisset*

- `2112.03315v1` - [abs](http://arxiv.org/abs/2112.03315v1) - [pdf](http://arxiv.org/pdf/2112.03315v1)

> Due to their massive success in various domains, deep learning techniques are increasingly used to design network intrusion detection solutions that detect and mitigate unknown and known attacks with high accuracy detection rates and minimal feature engineering. However, it has been found that deep learning models are vulnerable to data instances that can mislead the model to make incorrect classification decisions so-called (adversarial examples). Such vulnerability allows attackers to target NIDSs by adding small crafty perturbations to the malicious traffic to evade detection and disrupt the system's critical functionalities. The problem of deep adversarial learning has been extensively studied in the computer vision domain; however, it is still an area of open research in network security applications. Therefore, this survey explores the researches that employ different aspects of adversarial machine learning in the area of network intrusion detection in order to provide directions for potential solutions. First, the surveyed studies are categorized based on their contribution to generating adversarial examples, evaluating the robustness of ML-based NIDs towards adversarial examples, and defending these models against such attacks. Second, we highlight the characteristics identified in the surveyed research. Furthermore, we discuss the applicability of the existing generic adversarial attacks for the NIDS domain, the feasibility of launching the proposed attacks in real-world scenarios, and the limitations of the existing mitigation solutions.

</details>

<details>

<summary>2021-12-06 20:12:15 - Shape Defense Against Adversarial Attacks</summary>

- *Ali Borji*

- `2008.13336v3` - [abs](http://arxiv.org/abs/2008.13336v3) - [pdf](http://arxiv.org/pdf/2008.13336v3)

> Humans rely heavily on shape information to recognize objects. Conversely, convolutional neural networks (CNNs) are biased more towards texture. This is perhaps the main reason why CNNs are vulnerable to adversarial examples. Here, we explore how shape bias can be incorporated into CNNs to improve their robustness. Two algorithms are proposed, based on the observation that edges are invariant to moderate imperceptible perturbations. In the first one, a classifier is adversarially trained on images with the edge map as an additional channel. At inference time, the edge map is recomputed and concatenated to the image. In the second algorithm, a conditional GAN is trained to translate the edge maps, from clean and/or perturbed images, into clean images. Inference is done over the generated image corresponding to the input's edge map. Extensive experiments over 10 datasets demonstrate the effectiveness of the proposed algorithms against FGSM and $\ell_\infty$ PGD-40 attacks. Further, we show that a) edge information can also benefit other adversarial training methods, and b) CNNs trained on edge-augmented inputs are more robust against natural image corruptions such as motion blur, impulse noise and JPEG compression, than CNNs trained solely on RGB images. From a broader perspective, our study suggests that CNNs do not adequately account for image structures that are crucial for robustness. Code is available at:~\url{https://github.com/aliborji/Shapedefense.git}.

</details>

<details>

<summary>2021-12-06 21:11:46 - Alice in Passphraseland: Assessing the Memorability of Familiar Vocabularies for System-Assigned Passphrases</summary>

- *Noopa Jagadeesh, Miguel Vargas Martin*

- `2112.03359v1` - [abs](http://arxiv.org/abs/2112.03359v1) - [pdf](http://arxiv.org/pdf/2112.03359v1)

> Text-based secrets are still the most commonly used authentication mechanism in information systems. IT managers must strike a balance between security and memorability while developing password policies. Initially introduced as more secure authentication keys that people could recall, passphrases are passwords consisting of multiple words. However, when left to the choice of users, they tend to choose predictable natural language patterns in passphrases, resulting in vulnerability to guessing attacks. System-assigned authentication keys can be guaranteed to be secure, but this comes at a cost to memorability. In this study we investigate the memorability of system-assigned passphrases from a familiar vocabulary to the user. The passphrases are generated with the Generative Pre-trained Transformer 2 (GPT-2) model trained on the familiar vocabulary and are readable, pronounceable, sentence like passphrases resembling natural English sentences. Through an online user study with 500 participants on Amazon Mechanical Turk, we test our hypothesis - following a spaced repetition schedule, passphrases as natural English sentences, based on familiar vocabulary are easier to recall than passphrases composed of random common words. As a proof-of-concept, we tested the idea with Amazon Mechanical Turk participants by assigning them GPT-2 generated passphrases based on stories they were familiar with. Contrary to expectations, following a spaced repetition schedule, passphrases as natural English sentences, based on familiar vocabulary performed similarly to system-assigned passphrases based on random common words.

</details>

<details>

<summary>2021-12-07 00:00:34 - A Survey of Verification, Validation and Testing Solutions for Smart Contracts</summary>

- *Chaïmaa Benabbou, Önder Gürcan*

- `2112.03426v1` - [abs](http://arxiv.org/abs/2112.03426v1) - [pdf](http://arxiv.org/pdf/2112.03426v1)

> Smart contracts are programs stored on a blockchain that run when predetermined conditions are met. However, designing and implementing a smart contract is not trivial since upon deployment on a blockchain, it is no longer possible to modify it (neither for improving nor for bug fixing). It is only possible by deploying a new version of the smart contract which is costly (deployment cost for the new contract and destruction cost for the old contract). To this end, there are many solutions for testing the smart contracts before their deployment. Since realizing bug-free smart contracts increase the reliability, as well as reduce the cost, testing is an essential activity. In this paper, we group the existing solutions that attempt to tackle smart contract testing into following categories: public test networks, security analysis tools, blockchain emulators and blockchain simulators. Then, we analyze these solutions, categorize them and show what their pros and cons are.

</details>

<details>

<summary>2021-12-07 05:46:41 - Control Parameters Considered Harmful: Detecting Range Specification Bugs in Drone Configuration Modules via Learning-Guided Search</summary>

- *Ruidong Han, Chao Yang, Siqi Ma, JiangFeng Ma, Cong Sun, Juanru Li, Elisa Bertino*

- `2112.03511v1` - [abs](http://arxiv.org/abs/2112.03511v1) - [pdf](http://arxiv.org/pdf/2112.03511v1)

> In order to support a variety of missions and deal with different flight environments, drone control programs typically provide configurable control parameters. However, such a flexibility introduces vulnerabilities. One such vulnerability, referred to as range specification bugs, has been recently identified. The vulnerability originates from the fact that even though each individual parameter receives a value in the recommended value range, certain combinations of parameter values may affect the drone physical stability. In this paper we develop a novel learning-guided search system to find such combinations, that we refer to as incorrect configurations. Our system applies metaheuristic search algorithms mutating configurations to detect the configuration parameters that have values driving the drone to unstable physical states. To guide the mutations, our system leverages a machine learning predictor as the fitness evaluator. Finally, by utilizing multi-objective optimization, our system returns the feasible ranges based on the mutation search results. Because in our system the mutations are guided by a predictor, evaluating the parameter configurations does not require realistic/simulation executions. Therefore, our system supports a comprehensive and yet efficient detection of incorrect configurations. We have carried out an experimental evaluation of our system. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over 85% lead to actual unstable physical states.

</details>

<details>

<summary>2021-12-07 10:18:43 - Saliency Diversified Deep Ensemble for Robustness to Adversaries</summary>

- *Alex Bogun, Dimche Kostadinov, Damian Borth*

- `2112.03615v1` - [abs](http://arxiv.org/abs/2112.03615v1) - [pdf](http://arxiv.org/pdf/2112.03615v1)

> Deep learning models have shown incredible performance on numerous image recognition, classification, and reconstruction tasks. Although very appealing and valuable due to their predictive capabilities, one common threat remains challenging to resolve. A specifically trained attacker can introduce malicious input perturbations to fool the network, thus causing potentially harmful mispredictions. Moreover, these attacks can succeed when the adversary has full access to the target model (white-box) and even when such access is limited (black-box setting). The ensemble of models can protect against such attacks but might be brittle under shared vulnerabilities in its members (attack transferability). To that end, this work proposes a novel diversity-promoting learning approach for the deep ensembles. The idea is to promote saliency map diversity (SMD) on ensemble members to prevent the attacker from targeting all ensemble members at once by introducing an additional term in our learning objective. During training, this helps us minimize the alignment between model saliencies to reduce shared member vulnerabilities and, thus, increase ensemble robustness to adversaries. We empirically show a reduced transferability between ensemble members and improved performance compared to the state-of-the-art ensemble defense against medium and high strength white-box attacks. In addition, we demonstrate that our approach combined with existing methods outperforms state-of-the-art ensemble algorithms for defense under white-box and black-box attacks.

</details>

<details>

<summary>2021-12-07 12:44:36 - Lightning: Striking the Secure Isolation on GPU Clouds with Transient Hardware Faults</summary>

- *Rihui Sun, Pefei Qiu, Yongqiang Lyu, Donsheng Wang, Jiang Dong, Gang Qu*

- `2112.03662v1` - [abs](http://arxiv.org/abs/2112.03662v1) - [pdf](http://arxiv.org/pdf/2112.03662v1)

> GPU clouds have become a popular computing platform because of the cost of owning and maintaining high-performance computing clusters. Many cloud architectures have also been proposed to ensure a secure execution environment for guest applications by enforcing strong security policies to isolate the untrusted hypervisor from the guest virtual machines (VMs). In this paper, we study the impact of GPU chip's hardware faults on the security of cloud "trusted" execution environment using Deep Neural Network (DNN) as the underlying application. We show that transient hardware faults of GPUs can be generated by exploiting the Dynamic Voltage and Frequency Scaling (DVFS) technology, and these faults may cause computation errors, but they have limited impact on the inference accuracy of DNN due to the robustness and fault-tolerant nature of well-developed DNN models. To take full advantage of these transient hardware faults, we propose the Lightning attack to locate the fault injection targets of DNNs and to control the fault injection precision in terms of timing and position. We conduct experiments on three commodity GPUs to attack four widely-used DNNs. Experimental results show that the proposed attack can reduce the inference accuracy of the models by as high as 78.3\% and 64.5\% on average. More importantly, 67.9\% of the targeted attacks have successfully misled the models to give our desired incorrect inference result. This demonstrates that the secure isolation on GPU clouds is vulnerable against transient hardware faults and the computation results may not be trusted.

</details>

<details>

<summary>2021-12-07 17:21:57 - Reinforcement Learning for Feedback-Enabled Cyber Resilience</summary>

- *Yunhan Huang, Linan Huang, Quanyan Zhu*

- `2107.00783v2` - [abs](http://arxiv.org/abs/2107.00783v2) - [pdf](http://arxiv.org/pdf/2107.00783v2)

> Digitization and remote connectivity have enlarged the attack surface and made cyber systems more vulnerable. As attackers become increasingly sophisticated and resourceful, mere reliance on traditional cyber protection, such as intrusion detection, firewalls, and encryption, is insufficient to secure the cyber systems. Cyber resilience provides a new security paradigm that complements inadequate protection with resilience mechanisms. A Cyber-Resilient Mechanism (CRM) adapts to the known or zero-day threats and uncertainties in real-time and strategically responds to them to maintain critical functions of the cyber systems in the event of successful attacks. Feedback architectures play a pivotal role in enabling the online sensing, reasoning, and actuation process of the CRM. Reinforcement Learning (RL) is an essential tool that epitomizes the feedback architectures for cyber resilience. It allows the CRM to provide sequential responses to attacks with limited or without prior knowledge of the environment and the attacker. In this work, we review the literature on RL for cyber resilience and discuss cyber resilience against three major types of vulnerabilities, i.e., posture-related, information-related, and human-related vulnerabilities. We introduce three application domains of CRMs: moving target defense, defensive cyber deception, and assistive human security technologies. The RL algorithms also have vulnerabilities themselves. We explain the three vulnerabilities of RL and present attack models where the attacker targets the information exchanged between the environment and the agent: the rewards, the state observations, and the action commands. We show that the attacker can trick the RL agent into learning a nefarious policy with minimum attacking effort. Lastly, we discuss the future challenges of RL for cyber security and resilience and emerging applications of RL-based CRMs.

</details>

<details>

<summary>2021-12-07 23:15:23 - DeepDiagnosis: Automatically Diagnosing Faults and Recommending Actionable Fixes in Deep Learning Programs</summary>

- *Mohammad Wardat, Breno Dantas Cruz, Wei Le, Hridesh Rajan*

- `2112.04036v1` - [abs](http://arxiv.org/abs/2112.04036v1) - [pdf](http://arxiv.org/pdf/2112.04036v1)

> Deep Neural Networks (DNNs) are used in a wide variety of applications. However, as in any software application, DNN-based apps are afflicted with bugs. Previous work observed that DNN bug fix patterns are different from traditional bug fix patterns. Furthermore, those buggy models are non-trivial to diagnose and fix due to inexplicit errors with several options to fix them. To support developers in locating and fixing bugs, we propose DeepDiagnosis, a novel debugging approach that localizes the faults, reports error symptoms and suggests fixes for DNN programs. In the first phase, our technique monitors a training model, periodically checking for eight types of error conditions. Then, in case of problems, it reports messages containing sufficient information to perform actionable repairs to the model. In the evaluation, we thoroughly examine 444 models -53 real-world from GitHub and Stack Overflow, and 391 curated by AUTOTRAINER. DeepDiagnosis provides superior accuracy when compared to UMLUAT and DeepLocalize. Our technique is faster than AUTOTRAINER for fault localization. The results show that our approach can support additional types of models, while state-of-the-art was only able to handle classification ones. Our technique was able to report bugs that do not manifest as numerical errors during training. Also, it can provide actionable insights for fix whereas DeepLocalize can only report faults that lead to numerical errors during training. DeepDiagnosis manifests the best capabilities of fault detection, bug localization, and symptoms identification when compared to other approaches.

</details>

<details>

<summary>2021-12-07 23:16:30 - Evaluation of Static Vulnerability Detection Tools with Java Cryptographic API Benchmarks</summary>

- *Sharmin Afrose, Ya Xiao, Sazzadur Rahaman, Barton P. Miller, Danfeng, Yao*

- `2112.04037v1` - [abs](http://arxiv.org/abs/2112.04037v1) - [pdf](http://arxiv.org/pdf/2112.04037v1)

> Several studies showed that misuses of cryptographic APIs are common in real-world code (e.g., Apache projects and Android apps). There exist several open-sourced and commercial security tools that automatically screen Java programs to detect misuses. To compare their accuracy and security guarantees, we develop two comprehensive benchmarks named CryptoAPI-Bench and ApacheCryptoAPI-Bench. CryptoAPI-Bench consists of 181 unit test cases that cover basic cases, as well as complex cases, including interprocedural, field sensitive, multiple class test cases, and path sensitive data flow of misuse cases. The benchmark also includes correct cases for testing false-positive rates. The ApacheCryptoAPI-Bench consists of 121 cryptographic cases from 10 Apache projects. We evaluate four tools, namely, SpotBugs, CryptoGuard, CrySL, and Coverity using both benchmarks. We present their performance and comparative analysis. The ApacheCryptoAPI-Bench also examines the scalability of the tools. Our benchmarks are useful for advancing state-of-the-art solutions in the space of misuse detection.

</details>

<details>

<summary>2021-12-08 07:54:03 - SNEAK: Synonymous Sentences-Aware Adversarial Attack on Natural Language Video Localization</summary>

- *Wenbo Gou, Wen Shi, Jian Lou, Lijie Huang, Pan Zhou, Ruixuan Li*

- `2112.04154v1` - [abs](http://arxiv.org/abs/2112.04154v1) - [pdf](http://arxiv.org/pdf/2112.04154v1)

> Natural language video localization (NLVL) is an important task in the vision-language understanding area, which calls for an in-depth understanding of not only computer vision and natural language side alone, but more importantly the interplay between both sides. Adversarial vulnerability has been well-recognized as a critical security issue of deep neural network models, which requires prudent investigation. Despite its extensive yet separated studies in video and language tasks, current understanding of the adversarial robustness in vision-language joint tasks like NLVL is less developed. This paper therefore aims to comprehensively investigate the adversarial robustness of NLVL models by examining three facets of vulnerabilities from both attack and defense aspects. To achieve the attack goal, we propose a new adversarial attack paradigm called synonymous sentences-aware adversarial attack on NLVL (SNEAK), which captures the cross-modality interplay between the vision and language sides.

</details>

<details>

<summary>2021-12-08 11:13:47 - Towards automation of threat modeling based on a semantic model of attack patterns and weaknesses</summary>

- *Andrei Brazhuk*

- `2112.04231v1` - [abs](http://arxiv.org/abs/2112.04231v1) - [pdf](http://arxiv.org/pdf/2112.04231v1)

> This works considers challenges of building and usage a formal knowledge base (model), which unites the ATT&CK, CAPEC, CWE, CVE security enumerations. The proposed model can be used to learn relations between attack techniques, attack pattern, weaknesses, and vulnerabilities in order to build various threat landscapes, in particular, for threat modeling. The model is created as an ontology with freely available datasets in the OWL and RDF formats. The use of ontologies is an alternative of structural and graph based approaches to integrate the security enumerations. In this work we consider an approach of threat modeling with the data components of ATT&CK based on the knowledge base and an ontology driven threat modeling framework. Also, some evaluations are made, how it can be possible to use the ontological approach of threat modeling and which challenges this can be faced.

</details>

<details>

<summary>2021-12-08 11:22:29 - A short review on quantum identity authentication protocols: How would Bob know that he is talking with Alice?</summary>

- *Arindam Dutta, Anirban Pathak*

- `2112.04234v1` - [abs](http://arxiv.org/abs/2112.04234v1) - [pdf](http://arxiv.org/pdf/2112.04234v1)

> Secure communication has achieved a new dimension with the advent of the schemes of quantum key distribution (QKD) as in contrast to classical cryptography, quantum cryptography can provide unconditional security. However, a successful implementation of a scheme of QKD requires identity authentication as a prerequisite. A security loophole in the identity authentication scheme may lead to the vulnerability of the entire secure communication scheme. Consequently, identity authentication is extremely important and in the last three decades several schemes for identity authentication, using quantum resources have been proposed. The chronological development of these protocols, which are now referred to as quantum identity authentication (QIA) protocols, are briefly reviewed here with specific attention to the causal connection involved in their development. The existing protocols are classified on the basis of the required quantum resources and their relative merits and demerits are analyzed. Further, in the process of the classification of the protocols for QIA, it's observed that the existing protocols can also be classified in a few groups based on the inherent computational tasks used to design the protocols. Realization of these symmetries has led to the possibility of designing a set of new protocols for quantum identity authentication, which are based on the existing schemes of the secure computational and communication tasks. The security of such protocols is also critically analyzed.

</details>

<details>

<summary>2021-12-08 11:30:53 - Application of Deep Reinforcement Learning to Payment Fraud</summary>

- *Siddharth Vimal, Kanishka Kayathwal, Hardik Wadhwa, Gaurav Dhama*

- `2112.04236v1` - [abs](http://arxiv.org/abs/2112.04236v1) - [pdf](http://arxiv.org/pdf/2112.04236v1)

> The large variety of digital payment choices available to consumers today has been a key driver of e-commerce transactions in the past decade. Unfortunately, this has also given rise to cybercriminals and fraudsters who are constantly looking for vulnerabilities in these systems by deploying increasingly sophisticated fraud attacks. A typical fraud detection system employs standard supervised learning methods where the focus is on maximizing the fraud recall rate. However, we argue that such a formulation can lead to sub-optimal solutions. The design requirements for these fraud models requires that they are robust to the high-class imbalance in the data, adaptive to changes in fraud patterns, maintain a balance between the fraud rate and the decline rate to maximize revenue, and be amenable to asynchronous feedback since usually there is a significant lag between the transaction and the fraud realization. To achieve this, we formulate fraud detection as a sequential decision-making problem by including the utility maximization within the model in the form of the reward function. The historical decline rate and fraud rate define the state of the system with a binary action space composed of approving or declining the transaction. In this study, we primarily focus on utility maximization and explore different reward functions to this end. The performance of the proposed Reinforcement Learning system has been evaluated for two publicly available fraud datasets using Deep Q-learning and compared with different classifiers. We aim to address the rest of the issues in future work.

</details>

<details>

<summary>2021-12-09 22:37:44 - The Dilemma Between Data Transformations and Adversarial Robustness for Time Series Application Systems</summary>

- *Sheila Alemany, Niki Pissinou*

- `2006.10885v2` - [abs](http://arxiv.org/abs/2006.10885v2) - [pdf](http://arxiv.org/pdf/2006.10885v2)

> Adversarial examples, or nearly indistinguishable inputs created by an attacker, significantly reduce machine learning accuracy. Theoretical evidence has shown that the high intrinsic dimensionality of datasets facilitates an adversary's ability to develop effective adversarial examples in classification models. Adjacently, the presentation of data to a learning model impacts its performance. For example, we have seen this through dimensionality reduction techniques used to aid with the generalization of features in machine learning applications. Thus, data transformation techniques go hand-in-hand with state-of-the-art learning models in decision-making applications such as intelligent medical or military systems. With this work, we explore how data transformations techniques such as feature selection, dimensionality reduction, or trend extraction techniques may impact an adversary's ability to create effective adversarial samples on a recurrent neural network. Specifically, we analyze it from the perspective of the data manifold and the presentation of its intrinsic features. Our evaluation empirically shows that feature selection and trend extraction techniques may increase the RNN's vulnerability. A data transformation technique reduces the vulnerability to adversarial examples only if it approximates the dataset's intrinsic dimension, minimizes codimension, and maintains higher manifold coverage.

</details>

<details>

<summary>2021-12-10 00:20:05 - Using Machine Learning to Predict Air Quality Index in New Delhi</summary>

- *Samayan Bhattacharya, Sk Shahnawaz*

- `2112.05753v1` - [abs](http://arxiv.org/abs/2112.05753v1) - [pdf](http://arxiv.org/pdf/2112.05753v1)

> Air quality has a significant impact on human health. Degradation in air quality leads to a wide range of health issues, especially in children. The ability to predict air quality enables the government and other concerned organizations to take necessary steps to shield the most vulnerable, from being exposed to the air with hazardous quality. Traditional approaches to this task have very limited success because of a lack of access of such methods to sufficient longitudinal data. In this paper, we use a Support Vector Regression (SVR) model to forecast the levels of various pollutants and the air quality index, using archive pollution data made publicly available by Central Pollution Control Board and the US Embassy in New Delhi. Among the tested methods, a Radial Basis Function (RBF) kernel produced the best results with SVR. According to our experiments, using the whole range of available variables produced better results than using features selected by principal component analysis. The model predicts levels of various pollutants, like, sulfur dioxide, carbon monoxide, nitrogen dioxide, particulate matter 2.5, and ground-level ozone, as well as the Air Quality Index (AQI), at an accuracy of 93.4 percent.

</details>

<details>

<summary>2021-12-10 05:03:02 - Automated Side Channel Analysis of Media Software with Manifold Learning</summary>

- *Yuanyuan Yuan, Qi Pang, Shuai Wang*

- `2112.04947v2` - [abs](http://arxiv.org/abs/2112.04947v2) - [pdf](http://arxiv.org/pdf/2112.04947v2)

> The prosperous development of cloud computing and machine learning as a service has led to the widespread use of media software to process confidential media data. This paper explores an adversary's ability to launch side channel analyses (SCA) against media software to reconstruct confidential media inputs. Recent advances in representation learning and perceptual learning inspired us to consider the reconstruction of media inputs from side channel traces as a cross-modality manifold learning task that can be addressed in a unified manner with an autoencoder framework trained to learn the mapping between media inputs and side channel observations. We further enhance the autoencoder with attention to localize the program points that make the primary contribution to SCA, thus automatically pinpointing information-leakage points in media software. We also propose a novel and highly effective defensive technique called perception blinding that can perturb media inputs with perception masks and mitigate manifold learning-based SCA.   Our evaluation exploits three popular media software to reconstruct inputs in image, audio, and text formats. We analyze three common side channels - cache bank, cache line, and page tables - and userspace-only cache set accesses logged by standard Prime+Probe. Our framework successfully reconstructs high-quality confidential inputs from the assessed media software and automatically pinpoint their vulnerable program points, many of which are unknown to the public. We further show that perception blinding can mitigate manifold learning-based SCA with negligible extra cost.

</details>

<details>

<summary>2021-12-10 16:06:03 - Preemptive Image Robustification for Protecting Users against Man-in-the-Middle Adversarial Attacks</summary>

- *Seungyong Moon, Gaon An, Hyun Oh Song*

- `2112.05634v1` - [abs](http://arxiv.org/abs/2112.05634v1) - [pdf](http://arxiv.org/pdf/2112.05634v1)

> Deep neural networks have become the driving force of modern image recognition systems. However, the vulnerability of neural networks against adversarial attacks poses a serious threat to the people affected by these systems. In this paper, we focus on a real-world threat model where a Man-in-the-Middle adversary maliciously intercepts and perturbs images web users upload online. This type of attack can raise severe ethical concerns on top of simple performance degradation. To prevent this attack, we devise a novel bi-level optimization algorithm that finds points in the vicinity of natural images that are robust to adversarial perturbations. Experiments on CIFAR-10 and ImageNet show our method can effectively robustify natural images within the given modification budget. We also show the proposed method can improve robustness when jointly used with randomized smoothing.

</details>

<details>

<summary>2021-12-10 18:14:47 - Malware Classification Using Static Disassembly and Machine Learning</summary>

- *Zhenshuo Chen, Eoin Brophy, Tomas Ward*

- `2201.07649v1` - [abs](http://arxiv.org/abs/2201.07649v1) - [pdf](http://arxiv.org/pdf/2201.07649v1)

> Network and system security are incredibly critical issues now. Due to the rapid proliferation of malware, traditional analysis methods struggle with enormous samples.   In this paper, we propose four easy-to-extract and small-scale features, including sizes and permissions of Windows PE sections, content complexity, and import libraries, to classify malware families, and use automatic machine learning to search for the best model and hyper-parameters for each feature and their combinations. Compared with detailed behavior-related features like API sequences, proposed features provide macroscopic information about malware. The analysis is based on static disassembly scripts and hexadecimal machine code. Unlike dynamic behavior analysis, static analysis is resource-efficient and offers complete code coverage, but is vulnerable to code obfuscation and encryption.   The results demonstrate that features which work well in dynamic analysis are not necessarily effective when applied to static analysis. For instance, API 4-grams only achieve 57.96% accuracy and involve a relatively high dimensional feature set (5000 dimensions). In contrast, the novel proposed features together with a classical machine learning algorithm (Random Forest) presents very good accuracy at 99.40% and the feature vector is of much smaller dimension (40 dimensions). We demonstrate the effectiveness of this approach through integration in IDA Pro, which also facilitates the collection of new training samples and subsequent model retraining.

</details>

<details>

<summary>2021-12-10 18:19:31 - Attacks on Wireless Coexistence: Exploiting Cross-Technology Performance Features for Inter-Chip Privilege Escalation</summary>

- *Jiska Classen, Francesco Gringoli, Michael Hermann, Matthias Hollick*

- `2112.05719v1` - [abs](http://arxiv.org/abs/2112.05719v1) - [pdf](http://arxiv.org/pdf/2112.05719v1)

> Modern mobile devices feature multiple wireless technologies, such as Bluetooth, Wi-Fi, and LTE. Each of them is implemented within a separate wireless chip, sometimes packaged as combo chips. However, these chips share components and resources, such as the same antenna or wireless spectrum. Wireless coexistence interfaces enable them to schedule packets without collisions despite shared resources, essential to maximizing networking performance. Today's hardwired coexistence interfaces hinder clear security boundaries and separation between chips and chip components. This paper shows practical coexistence attacks on Broadcom, Cypress, and Silicon Labs chips deployed in billions of devices. For example, we demonstrate that a Bluetooth chip can directly extract network passwords and manipulate traffic on a Wi-Fi chip. Coexistence attacks enable a novel type of lateral privilege escalation across chip boundaries. We responsibly disclosed the vulnerabilities to the vendors. Yet, only partial fixes were released for existing hardware since wireless chips would need to be redesigned from the ground up to prevent the presented attacks on coexistence.

</details>

<details>

<summary>2021-12-10 21:34:18 - Explanation-Based Human Debugging of NLP Models: A Survey</summary>

- *Piyawat Lertvittayakumjorn, Francesca Toni*

- `2104.15135v3` - [abs](http://arxiv.org/abs/2104.15135v3) - [pdf](http://arxiv.org/pdf/2104.15135v3)

> Debugging a machine learning model is hard since the bug usually involves the training data and the learning process. This becomes even harder for an opaque deep learning model if we have no clue about how the model actually works. In this survey, we review papers that exploit explanations to enable humans to give feedback and debug NLP models. We call this problem explanation-based human debugging (EBHD). In particular, we categorize and discuss existing work along three dimensions of EBHD (the bug context, the workflow, and the experimental setting), compile findings on how EBHD components affect the feedback providers, and highlight open problems that could be future research directions.

</details>

<details>

<summary>2021-12-11 00:27:12 - Bad Characters: Imperceptible NLP Attacks</summary>

- *Nicholas Boucher, Ilia Shumailov, Ross Anderson, Nicolas Papernot*

- `2106.09898v2` - [abs](http://arxiv.org/abs/2106.09898v2) - [pdf](http://arxiv.org/pdf/2106.09898v2)

> Several years of research have shown that machine-learning systems are vulnerable to adversarial examples, both in theory and in practice. Until now, such attacks have primarily targeted visual models, exploiting the gap between human and machine perception. Although text-based models have also been attacked with adversarial examples, such attacks struggled to preserve semantic meaning and indistinguishability. In this paper, we explore a large class of adversarial examples that can be used to attack text-based models in a black-box setting without making any human-perceptible visual modification to inputs. We use encoding-specific perturbations that are imperceptible to the human eye to manipulate the outputs of a wide range of Natural Language Processing (NLP) systems from neural machine-translation pipelines to web search engines. We find that with a single imperceptible encoding injection -- representing one invisible character, homoglyph, reordering, or deletion -- an attacker can significantly reduce the performance of vulnerable models, and with three injections most models can be functionally broken. Our attacks work against currently-deployed commercial systems, including those produced by Microsoft and Google, in addition to open source models published by Facebook, IBM, and HuggingFace. This novel series of attacks presents a significant threat to many language processing systems: an attacker can affect systems in a targeted manner without any assumptions about the underlying model. We conclude that text-based NLP systems require careful input sanitization, just like conventional applications, and that given such systems are now being deployed rapidly at scale, the urgent attention of architects and operators is required.

</details>

<details>

<summary>2021-12-11 08:30:52 - Cats vs. Spectre: An Axiomatic Approach to Modeling Speculative Execution Attacks</summary>

- *Hernán Ponce-de-León, Johannes Kinder*

- `2108.13818v2` - [abs](http://arxiv.org/abs/2108.13818v2) - [pdf](http://arxiv.org/pdf/2108.13818v2)

> The Spectre family of speculative execution attacks have required a rethinking of formal methods for security. Approaches based on operational speculative semantics have made initial inroads towards finding vulnerable code and validating defenses. However, with each new attack grows the amount of microarchitectural detail that has to be integrated into the underlying semantics. We propose an alternative, light-weight and axiomatic approach to specifying speculative semantics that relies on insights from memory models for concurrency. We use the CAT modeling language for memory consistency to specify execution models that capture speculative control flow, store-to-load forwarding, predictive store forwarding, and memory ordering machine clears. We present a bounded model checking framework parametrized by our speculative CAT models and evaluate its implementation against the state of the art. Due to the axiomatic approach, our models can be rapidly extended to allow our framework to detect new types of attacks and validate defenses against them.

</details>

<details>

<summary>2021-12-11 19:25:47 - ArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks</summary>

- *Hua Qi, Zhijie Wang, Qing Guo, Jianlang Chen, Felix Juefei-Xu, Lei Ma, Jianjun Zhao*

- `2111.13330v2` - [abs](http://arxiv.org/abs/2111.13330v2) - [pdf](http://arxiv.org/pdf/2111.13330v2)

> Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in the industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting, lacking robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block) level.   We first perform empirical studies to investigate the limitation of whole network-level and layer-level repairing, which motivates us to explore a novel repairing direction for DNN repair at the block level. To this end, we first propose adversarial-aware spectrum analysis for vulnerable block localization that considers the neurons' status and weights' gradients in blocks during the forward and backward processes, which enables more accurate candidate block localization for repairing even under a few examples. Then, we further propose the architecture-oriented search-based repairing that relaxes the targeted block to a continuous repairing search space at higher deep feature levels. By jointly optimizing the architecture and weights in that space, we can identify a much better block architecture. We implement our proposed repairing techniques as a tool, named ArchRepair, and conduct extensive experiments to validate the proposed method. The results show that our method can not only repair but also enhance accuracy & robustness, outperforming the state-of-the-art DNN repair techniques.

</details>

<details>

<summary>2021-12-11 20:01:37 - MedAttacker: Exploring Black-Box Adversarial Attacks on Risk Prediction Models in Healthcare</summary>

- *Muchao Ye, Junyu Luo, Guanjie Zheng, Cao Xiao, Ting Wang, Fenglong Ma*

- `2112.06063v1` - [abs](http://arxiv.org/abs/2112.06063v1) - [pdf](http://arxiv.org/pdf/2112.06063v1)

> Deep neural networks (DNNs) have been broadly adopted in health risk prediction to provide healthcare diagnoses and treatments. To evaluate their robustness, existing research conducts adversarial attacks in the white/gray-box setting where model parameters are accessible. However, a more realistic black-box adversarial attack is ignored even though most real-world models are trained with private data and released as black-box services on the cloud. To fill this gap, we propose the first black-box adversarial attack method against health risk prediction models named MedAttacker to investigate their vulnerability. MedAttacker addresses the challenges brought by EHR data via two steps: hierarchical position selection which selects the attacked positions in a reinforcement learning (RL) framework and substitute selection which identifies substitute with a score-based principle. Particularly, by considering the temporal context inside EHRs, it initializes its RL position selection policy by using the contribution score of each visit and the saliency score of each code, which can be well integrated with the deterministic substitute selection process decided by the score changes. In experiments, MedAttacker consistently achieves the highest average success rate and even outperforms a recent white-box EHR adversarial attack technique in certain cases when attacking three advanced health risk prediction models in the black-box setting across multiple real-world datasets. In addition, based on the experiment results we include a discussion on defending EHR adversarial attacks.

</details>

<details>

<summary>2021-12-11 21:01:29 - A Comparative Study on Robust Graph Neural Networks to Structural Noises</summary>

- *Zeyu Zhang, Yulong Pei*

- `2112.06070v1` - [abs](http://arxiv.org/abs/2112.06070v1) - [pdf](http://arxiv.org/pdf/2112.06070v1)

> Graph neural networks (GNNs) learn node representations by passing and aggregating messages between neighboring nodes. GNNs have been applied successfully in several application domains and achieved promising performance. However, GNNs could be vulnerable to structural noise because of the message passing mechanism where noise may be propagated through the entire graph. Although a series of robust GNNs have been proposed, they are evaluated with different structural noises, and it lacks a systematic comparison with consistent settings. In this work, we conduct a comprehensive and systematical comparative study on different types of robust GNNs under consistent structural noise settings. From the noise aspect, we design three different levels of structural noises, i.e., local, community, and global noises. From the model aspect, we select some representative models from sample-based, revision-based, and construction-based robust GNNs. Based on the empirical results, we provide some practical suggestions for robust GNNs selection.

</details>

<details>

<summary>2021-12-12 02:20:56 - Visualizing Environmental Justice Issues in Urban Areas with a Community-based Approach</summary>

- *Joel Flax-Hatch, Sanjana Srabanti, Fabio Miranda, Apostolis Sambanis, Michael D. Cailas*

- `2112.06119v1` - [abs](http://arxiv.org/abs/2112.06119v1) - [pdf](http://arxiv.org/pdf/2112.06119v1)

> According to environmental justice, environmental degradation and benefits should not be disproportionately shared between communities. Identifying disparities in the spatial distribution of environmental degradation is therefore a prerequisite for validating the state of environmental justice in a geographic region. Under ideal circumstances, environmental risk assessment is a preferred metric, but only when exposure levels have been quantified reliably after estimating the risk. In this study, we adopt a proximity burden metric caused by adjacent hazardous sources, allowing us to evaluate the environmental burden distribution and vulnerability to pollution sources. In close collaboration with a predominantly Latinx community in Chicago, we highlight the usefulness of our approach through a case study that shows how certain community areas in the city are likely to bear a disproportionate burden of environmental pollution caused by industrial roads.

</details>

<details>

<summary>2021-12-12 08:58:32 - Multi-Agent Vulnerability Discovery for Autonomous Driving with Hazard Arbitration Reward</summary>

- *Weilin Liu, Ye Mu, Chao Yu, Xuefei Ning, Zhong Cao, Yi Wu, Shuang Liang, Huazhong Yang, Yu Wang*

- `2112.06185v1` - [abs](http://arxiv.org/abs/2112.06185v1) - [pdf](http://arxiv.org/pdf/2112.06185v1)

> Discovering hazardous scenarios is crucial in testing and further improving driving policies. However, conducting efficient driving policy testing faces two key challenges. On the one hand, the probability of naturally encountering hazardous scenarios is low when testing a well-trained autonomous driving strategy. Thus, discovering these scenarios by purely real-world road testing is extremely costly. On the other hand, a proper determination of accident responsibility is necessary for this task. Collecting scenarios with wrong-attributed responsibilities will lead to an overly conservative autonomous driving strategy. To be more specific, we aim to discover hazardous scenarios that are autonomous-vehicle responsible (AV-responsible), i.e., the vulnerabilities of the under-test driving policy.   To this end, this work proposes a Safety Test framework by finding Av-Responsible Scenarios (STARS) based on multi-agent reinforcement learning. STARS guides other traffic participants to produce Av-Responsible Scenarios and make the under-test driving policy misbehave via introducing Hazard Arbitration Reward (HAR). HAR enables our framework to discover diverse, complex, and AV-responsible hazardous scenarios. Experimental results against four different driving policies in three environments demonstrate that STARS can effectively discover AV-responsible hazardous scenarios. These scenarios indeed correspond to the vulnerabilities of the under-test driving policies, thus are meaningful for their further improvements.

</details>

<details>

<summary>2021-12-12 14:31:50 - Boosting the Capability of Intelligent Vulnerability Detection by Training in a Human-Learning Manner</summary>

- *Shihan Dou, Yueming Wu, Wenxuan Li, Feng Cheng, Wei Yang, Yang Liu*

- `2112.06250v1` - [abs](http://arxiv.org/abs/2112.06250v1) - [pdf](http://arxiv.org/pdf/2112.06250v1)

> Due to its powerful automatic feature extraction, deep learning (DL) has been widely used in source code vulnerability detection. However, although it performs well on artificial datasets, its performance is not satisfactory when detecting real-world vulnerabilities due to the high complexity of real-world samples. In this paper, we propose to train DL-based vulnerability detection models in a human-learning manner, that is, start with the simplest samples and then gradually transition to difficult knowledge. Specifically, we design a novel framework (Humer) that can enhance the detection ability of DL-based vulnerability detectors. To validate the effectiveness of Humer, we select five state-of-the-art DL-based vulnerability detection models (TokenCNN, VulDeePecker, StatementGRU, ASTGRU, and Devign) to complete our evaluations. Through the results, we find that the use of Humer can increase the F1 of these models by an average of 10.5%. Moreover, Humer can make the model detect up to 16.7% more real-world vulnerabilities. Meanwhile, we also conduct a case study to uncover vulnerabilities from real-world open source products by using these enhanced DL-based vulnerability detectors. Through the results, we finally discover 281 unreported vulnerabilities in NVD, of which 98 have been silently patched by vendors in the latest version of corresponding products, but 159 still exist in the products.

</details>

<details>

<summary>2021-12-12 16:34:52 - SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with Sparsification</summary>

- *Ashwinee Panda, Saeed Mahloujifar, Arjun N. Bhagoji, Supriyo Chakraborty, Prateek Mittal*

- `2112.06274v1` - [abs](http://arxiv.org/abs/2112.06274v1) - [pdf](http://arxiv.org/pdf/2112.06274v1)

> Federated learning is inherently vulnerable to model poisoning attacks because its decentralized nature allows attackers to participate with compromised devices. In model poisoning attacks, the attacker reduces the model's performance on targeted sub-tasks (e.g. classifying planes as birds) by uploading "poisoned" updates. In this report we introduce \algoname{}, a novel defense that uses global top-k update sparsification and device-level gradient clipping to mitigate model poisoning attacks. We propose a theoretical framework for analyzing the robustness of defenses against poisoning attacks, and provide robustness and convergence analysis of our algorithm. To validate its empirical efficacy we conduct an open-source evaluation at scale across multiple benchmark datasets for computer vision and federated learning.

</details>

<details>

<summary>2021-12-13 04:23:57 - SAILFISH: Vetting Smart Contract State-Inconsistency Bugs in Seconds</summary>

- *Priyanka Bose, Dipanjan Das, Yanju Chen, Yu Feng, Christopher Kruegel, Giovanni Vigna*

- `2104.08638v2` - [abs](http://arxiv.org/abs/2104.08638v2) - [pdf](http://arxiv.org/pdf/2104.08638v2)

> This paper presents SAILFISH, a scalable system for automatically finding state-inconsistency bugs in smart contracts. To make the analysis tractable, we introduce a hybrid approach that includes (i) a light-weight exploration phase that dramatically reduces the number of instructions to analyze, and (ii) a precise refinement phase based on symbolic evaluation guided by our novel value-summary analysis, which generates extra constraints to over-approximate the side effects of whole-program execution, thereby ensuring the precision of the symbolic evaluation. We developed a prototype of SAILFISH and evaluated its ability to detect two state-inconsistency flaws, viz., reentrancy and transaction order dependence (TOD) in Ethereum smart contracts. Further, we present detection rules for other kinds of smart contract flaws that SAILFISH can be extended to detect.   Our experiments demonstrate the efficiency of our hybrid approach as well as the benefit of the value summary analysis. In particular, we show that S SAILFISH outperforms five state-of-the-art smart contract analyzers (SECURITY, MYTHRIL, OYENTE, SEREUM and VANDAL ) in terms of performance, and precision. In total, SAILFISH discovered 47 previously unknown vulnerable smart contracts out of 89,853 smart contracts from ETHERSCAN .

</details>

<details>

<summary>2021-12-13 06:57:34 - Detecting Audio Adversarial Examples with Logit Noising</summary>

- *Namgyu Park, Sangwoo Ji, Jong Kim*

- `2112.06443v1` - [abs](http://arxiv.org/abs/2112.06443v1) - [pdf](http://arxiv.org/pdf/2112.06443v1)

> Automatic speech recognition (ASR) systems are vulnerable to audio adversarial examples that attempt to deceive ASR systems by adding perturbations to benign speech signals. Although an adversarial example and the original benign wave are indistinguishable to humans, the former is transcribed as a malicious target sentence by ASR systems. Several methods have been proposed to generate audio adversarial examples and feed them directly into the ASR system (over-line). Furthermore, many researchers have demonstrated the feasibility of robust physical audio adversarial examples(over-air). To defend against the attacks, several studies have been proposed. However, deploying them in a real-world situation is difficult because of accuracy drop or time overhead. In this paper, we propose a novel method to detect audio adversarial examples by adding noise to the logits before feeding them into the decoder of the ASR. We show that carefully selected noise can significantly impact the transcription results of the audio adversarial examples, whereas it has minimal impact on the transcription results of benign audio waves. Based on this characteristic, we detect audio adversarial examples by comparing the transcription altered by logit noising with its original transcription. The proposed method can be easily applied to ASR systems without any structural changes or additional training. The experimental results show that the proposed method is robust to over-line audio adversarial examples as well as over-air audio adversarial examples compared with state-of-the-art detection methods.

</details>

<details>

<summary>2021-12-13 07:16:55 - A Novel Model for Vulnerability Analysis through Enhanced Directed Graphs and Quantitative Metrics</summary>

- *Ángel Longueira-Romero, Rosa Iglesias, Jose Luis Flores, Iñaki Garitano*

- `2112.06453v1` - [abs](http://arxiv.org/abs/2112.06453v1) - [pdf](http://arxiv.org/pdf/2112.06453v1)

> Industrial components are of high importance because they control critical infrastructures that form the lifeline of modern societies. However, the rapid evolution of industrial components, together with the new paradigm of Industry 4.0, and the new connectivity features that will be introduced by the 5G technology, all increase the likelihood of security incidents. These incidents are caused by the vulnerabilities present in these devices. In addition, although international standards define tasks to assess vulnerabilities, they do not specify any particular method. Having a secure design is important, but is also complex, costly, and an extra factor to manage during the lifespan of the device. This paper presents a model to analyze the known vulnerabilities of industrial components over time. The proposed model is based on two main elements: a directed graph representation of the internal structure of the component, and a set of quantitative metrics that are based on international security standards; such as, the Common Vulnerability Scoring System (CVSS). This model is applied throughout the entire lifespan of a device to track vulnerabilities, identify new requirements, root causes, and test cases. The proposed model also helps to prioritize patching activities. To test its potential, the proposed model is applied to the OpenPLC project. The results show that most of the root causes of these vulnerabilities are related to memory buffer operations and are concentrated in the \textit{libssl} library. Consequently, new requirements and test cases were generated from the obtained data.

</details>

<details>

<summary>2021-12-13 11:24:11 - Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning</summary>

- *Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, Daniel Ramage*

- `2108.10241v2` - [abs](http://arxiv.org/abs/2108.10241v2) - [pdf](http://arxiv.org/pdf/2108.10241v2)

> While recent works have indicated that federated learning (FL) may be vulnerable to poisoning attacks by compromised clients, their real impact on production FL systems is not fully understood. In this work, we aim to develop a comprehensive systemization for poisoning attacks on FL by enumerating all possible threat models, variations of poisoning, and adversary capabilities. We specifically put our focus on untargeted poisoning attacks, as we argue that they are significantly relevant to production FL deployments.   We present a critical analysis of untargeted poisoning attacks under practical, production FL environments by carefully characterizing the set of realistic threat models and adversarial capabilities. Our findings are rather surprising: contrary to the established belief, we show that FL is highly robust in practice even when using simple, low-cost defenses. We go even further and propose novel, state-of-the-art data and model poisoning attacks, and show via an extensive set of experiments across three benchmark datasets how (in)effective poisoning attacks are in the presence of simple defense mechanisms. We aim to correct previous misconceptions and offer concrete guidelines to conduct more accurate (and more realistic) research on this topic.

</details>

<details>

<summary>2021-12-13 17:05:39 - Open or Sneaky? Fast or Slow? Light or Heavy?: Investigating Security Releases of Open Source Packages</summary>

- *Nasif Imtiaz, Aniqa Khanom, Laurie Williams*

- `2112.06804v1` - [abs](http://arxiv.org/abs/2112.06804v1) - [pdf](http://arxiv.org/pdf/2112.06804v1)

> Vulnerabilities in open source packages can be a security risk for the client projects that use these packages as dependencies. When a new vulnerability is discovered in a package, the package should quickly release a fix in a new version, referred to as security release in this study. The security release should be well-documented and require minimal migration effort to facilitate fast adoption by the client projects. However, to what extent the open source packages follow these recommendations is not known. The goal of this study is to aid software practitioners and researchers in understanding the current practice of releasing security fixes by open source packages and identifying areas for improvement through an empirical study of security releases. Specifically, in this paper, we study (1) the time lag between fix and release; (2) how security fixes are documented in the release notes; (3) code change characteristics (size and semantic versioning) of the release; and (4) the time lag between the release and an advisory publication on Snyk or NVD (two popular vulnerability databases) for security releases over a dataset of 4,377 security advisories across seven package ecosystems. We find that the median security release is available in under 4 days of the corresponding fix and contains 134 lines of code (LOC) change. Further, we find that 61.5% of the security releases come with a release note that documents the corresponding security fix. However, Snyk and NVD may take a median of 25 days (from the release) to publish an advisory for these security releases, possibly resulting in delayed notification to the client projects. Based on our findings, we make four recommendations for the package maintainers and the ecosystem administrators, such as using private fork for security fixes and standardizing the practice for announcing security releases.

</details>

<details>

<summary>2021-12-13 17:32:06 - With False Friends Like These, Who Can Notice Mistakes?</summary>

- *Lue Tao, Lei Feng, Jinfeng Yi, Songcan Chen*

- `2012.14738v3` - [abs](http://arxiv.org/abs/2012.14738v3) - [pdf](http://arxiv.org/pdf/2012.14738v3)

> Adversarial examples crafted by an explicit adversary have attracted significant attention in machine learning. However, the security risk posed by a potential false friend has been largely overlooked. In this paper, we unveil the threat of hypocritical examples -- inputs that are originally misclassified yet perturbed by a false friend to force correct predictions. While such perturbed examples seem harmless, we point out for the first time that they could be maliciously used to conceal the mistakes of a substandard (i.e., not as good as required) model during an evaluation. Once a deployer trusts the hypocritical performance and applies the "well-performed" model in real-world applications, unexpected failures may happen even in benign environments. More seriously, this security risk seems to be pervasive: we find that many types of substandard models are vulnerable to hypocritical examples across multiple datasets. Furthermore, we provide the first attempt to characterize the threat with a metric called hypocritical risk and try to circumvent it via several countermeasures. Results demonstrate the effectiveness of the countermeasures, while the risk remains non-negligible even after adaptive robust training.

</details>

<details>

<summary>2021-12-13 18:04:58 - Burning graphs through farthest-first traversal</summary>

- *Jesús García Díaz, Julio César Pérez Sansalvador, Lil María Xibai Rodríguez Henríquez, José Alejandro Cornejo Acosta*

- `2011.15019v4` - [abs](http://arxiv.org/abs/2011.15019v4) - [pdf](http://arxiv.org/pdf/2011.15019v4)

> The graph burning problem is an NP-hard combinatorial optimization problem that helps quantify the vulnerability of a graph to contagion. This paper introduces a simple farthest-first traversal-based approximation algorithm for this problem over general graphs. We refer to this proposal as the Burning Farthest-First (BFF) algorithm. BFF runs in $O(n^3)$ steps and has an approximation factor of $3-2/b(G)$, where $b(G)$ is the size of an optimal solution. Despite its simplicity, BFF tends to generate near-optimal solutions when tested over some benchmark datasets; in fact, it returns similar solutions to those returned by much more elaborated heuristics from the literature.

</details>

<details>

<summary>2021-12-13 20:52:56 - Enhancing CryptoGuards Deployability for Continuous Software Security Scanning</summary>

- *Miles Frantz*

- `2201.07651v1` - [abs](http://arxiv.org/abs/2201.07651v1) - [pdf](http://arxiv.org/pdf/2201.07651v1)

> The increasing development speed via Agile may introduce overlooked security steps in the process, with an example being the Iowa Caucus application. Verifying the protection of confidential information such as social security numbers requires security at all levels, providing protection through any connected applications. CryptoGuard is a static code analyzer for Java. This program verifies that developers do not leave vulnerabilities in their applications. The program aids the developer by identifying cryptographic misuses such as hard-coded keys, weak program hashes, and using insecure protocols. In my Master's thesis work, I made several important contributions to improving the deployability, accessibility, and usability of CryptoGuard. I extended CryptoGuard to scan source and compiled code, created live documentation, and supported a dual cloud and local tool-suite. I also created build tool plugins and a program aid for CryptoGuard. In addition, I also analyzed several Java-related surveys encompassing more than 50,000 developers and reported interesting current practices of real-world software developers.

</details>

<details>

<summary>2021-12-14 03:44:44 - Better Pay Attention Whilst Fuzzing</summary>

- *Shunkai Zhu, Jingyi Wang, Jun Sun, Jie Yang, Xingwei Lin, Liyi Zhang, Peng Cheng*

- `2112.07143v1` - [abs](http://arxiv.org/abs/2112.07143v1) - [pdf](http://arxiv.org/pdf/2112.07143v1)

> Fuzzing is one of the prevailing methods for vulnerability detection. However, even state-of-the-art fuzzing methods become ineffective after some period of time, i.e., the coverage hardly improves as existing methods are ineffective to focus the attention of fuzzing on covering the hard-to-trigger program paths. In other words, they cannot generate inputs that can break the bottleneck due to the fundamental difficulty in capturing the complex relations between the test inputs and program coverage. In particular, existing fuzzers suffer from the following main limitations: 1) lacking an overall analysis of the program to identify the most "rewarding" seeds, and 2) lacking an effective mutation strategy which could continuously select and mutates the more relevant "bytes" of the seeds.   In this work, we propose an approach called ATTuzz to address these two issues systematically. First, we propose a lightweight dynamic analysis technique which estimates the "reward" of covering each basic block and selects the most rewarding seeds accordingly. Second, we mutate the selected seeds according to a neural network model which predicts whether a certain "rewarding" block will be covered given certain mutation on certain bytes of a seed. The model is a deep learning model equipped with attention mechanism which is learned and updated periodically whilst fuzzing. Our evaluation shows that ATTuzz significantly outperforms 5 state-of-the-art grey-box fuzzers on 13 popular real-world programs at achieving higher edge coverage and finding new bugs. In particular, ATTuzz achieved 2X edge coverage and 4X bugs detected than AFL over 24-hour runs. Moreover, ATTuzz persistently improves the edge coverage in the long run, i.e., achieving 50% more coverage than AFL in 5 days.

</details>

<details>

<summary>2021-12-14 05:35:43 - Subtle Data Crimes: Naively training machine learning algorithms could lead to overly-optimistic results</summary>

- *Efrat Shimron, Jonathan I. Tamir, Ke Wang, Michael Lustig*

- `2109.08237v3` - [abs](http://arxiv.org/abs/2109.08237v3) - [pdf](http://arxiv.org/pdf/2109.08237v3)

> While open databases are an important resource in the Deep Learning (DL) era, they are sometimes used "off-label": data published for one task are used for training algorithms for a different one. This work aims to highlight that in some cases, this common practice may lead to biased, overly-optimistic results. We demonstrate this phenomenon for inverse problem solvers and show how their biased performance stems from hidden data preprocessing pipelines. We describe two preprocessing pipelines typical of open-access databases and study their effects on three well-established algorithms developed for Magnetic Resonance Imaging (MRI) reconstruction: Compressed Sensing (CS), Dictionary Learning (DictL), and DL. In this large-scale study we performed extensive computations. Our results demonstrate that the CS, DictL and DL algorithms yield systematically biased results when naively trained on seemingly-appropriate data: the Normalized Root Mean Square Error (NRMSE) improves consistently with the preprocessing extent, showing an artificial increase of 25%-48% in some cases. Since this phenomenon is generally unknown, biased results are sometimes published as state-of-the-art; we refer to that as subtle data crimes. This work hence raises a red flag regarding naive off-label usage of Big Data and reveals the vulnerability of modern inverse problem solvers to the resulting bias.

</details>

<details>

<summary>2021-12-14 07:05:11 - Improving Calibration through the Relationship with Adversarial Robustness</summary>

- *Yao Qin, Xuezhi Wang, Alex Beutel, Ed H. Chi*

- `2006.16375v2` - [abs](http://arxiv.org/abs/2006.16375v2) - [pdf](http://arxiv.org/pdf/2006.16375v2)

> Neural networks lack adversarial robustness, i.e., they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated predictions, i.e., the predicted probability is not a good indicator of how much we should trust our model. In this paper, we study the connection between adversarial robustness and calibration and find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and calibration into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model calibration.

</details>

<details>

<summary>2021-12-14 07:34:13 - Play to Grade: Testing Coding Games as Classifying Markov Decision Process</summary>

- *Allen Nie, Emma Brunskill, Chris Piech*

- `2110.14615v2` - [abs](http://arxiv.org/abs/2110.14615v2) - [pdf](http://arxiv.org/pdf/2110.14615v2)

> Contemporary coding education often presents students with the task of developing programs that have user interaction and complex dynamic systems, such as mouse based games. While pedagogically compelling, there are no contemporary autonomous methods for providing feedback. Notably, interactive programs are impossible to grade by traditional unit tests. In this paper we formalize the challenge of providing feedback to interactive programs as a task of classifying Markov Decision Processes (MDPs). Each student's program fully specifies an MDP where the agent needs to operate and decide, under reasonable generalization, if the dynamics and reward model of the input MDP should be categorized as correct or broken. We demonstrate that by designing a cooperative objective between an agent and an autoregressive model, we can use the agent to sample differential trajectories from the input MDP that allows a classifier to determine membership: Play to Grade. Our method enables an automatic feedback system for interactive code assignments. We release a dataset of 711,274 anonymized student submissions to a single assignment with hand-coded bug labels to support future research.

</details>

<details>

<summary>2021-12-14 16:20:37 - Adversarial Examples for Extreme Multilabel Text Classification</summary>

- *Mohammadreza Qaraei, Rohit Babbar*

- `2112.07512v1` - [abs](http://arxiv.org/abs/2112.07512v1) - [pdf](http://arxiv.org/pdf/2112.07512v1)

> Extreme Multilabel Text Classification (XMTC) is a text classification problem in which, (i) the output space is extremely large, (ii) each data point may have multiple positive labels, and (iii) the data follows a strongly imbalanced distribution. With applications in recommendation systems and automatic tagging of web-scale documents, the research on XMTC has been focused on improving prediction accuracy and dealing with imbalanced data. However, the robustness of deep learning based XMTC models against adversarial examples has been largely underexplored.   In this paper, we investigate the behaviour of XMTC models under adversarial attacks. To this end, first, we define adversarial attacks in multilabel text classification problems. We categorize attacking multilabel text classifiers as (a) positive-targeted, where the target positive label should fall out of top-k predicted labels, and (b) negative-targeted, where the target negative label should be among the top-k predicted labels. Then, by experiments on APLC-XLNet and AttentionXML, we show that XMTC models are highly vulnerable to positive-targeted attacks but more robust to negative-targeted ones. Furthermore, our experiments show that the success rate of positive-targeted adversarial attacks has an imbalanced distribution. More precisely, tail classes are highly vulnerable to adversarial attacks for which an attacker can generate adversarial samples with high similarity to the actual data-points. To overcome this problem, we explore the effect of rebalanced loss functions in XMTC where not only do they increase accuracy on tail classes, but they also improve the robustness of these classes against adversarial attacks. The code for our experiments is available at https://github.com/xmc-aalto/adv-xmtc

</details>

<details>

<summary>2021-12-15 00:46:48 - 00</summary>

- *Nguyen Thoi Minh Quan*

- `2201.00815v1` - [abs](http://arxiv.org/abs/2201.00815v1) - [pdf](http://arxiv.org/pdf/2201.00815v1)

> What is the funniest number in cryptography (Episode 2)? 0 [1]. The reason is that $\forall x, x \cdot 0 = 0$, i.e., the equation is satisfied no matter what $x$ is. We'll use zero to attack zero-knowledge proof (ZKP). In particular, we'll discuss a critical issue in a cutting-edge ZKP PLONK [2] C++ implementation which allows an attacker to create a forged proof that all verifiers will accept. We'll show how theory guides the attack's direction. In practice, the attack works like a charm and we'll show how the attack falls through a chain of perfectly aligned software cracks. In the same codebase, there is an independent critical ECDSA bug where (r, s) = (0, 0) is a valid signature for arbitrary keys and messages, but we won't discuss it further because it's a known ECDSA attack vector in the Google Wycheproof cryptanalysis project [3] that I worked on a few years ago.   All bugs have been responsibly disclosed through the vendor's bug bounty program with total reward $\sim \$15,000$ (thank you).

</details>

<details>

<summary>2021-12-15 01:49:37 - Quantifying Cybersecurity Effectiveness of Dynamic Network Diversity</summary>

- *Huashan Chen, Hasan Cam, Shouhuai Xu*

- `2112.07826v1` - [abs](http://arxiv.org/abs/2112.07826v1) - [pdf](http://arxiv.org/pdf/2112.07826v1)

> The deployment of monoculture software stacks can have devastating consequences because a single attack can compromise all of the vulnerable computers in cyberspace. This one-vulnerability-affects-all phenomenon will continue until after software stacks are diversified, which is well recognized by the research community. However, existing studies mainly focused on investigating the effectiveness of software diversity at the building-block level (e.g., whether two independent implementations indeed exhibit independent vulnerabilities); the effectiveness of enforcing network-wide software diversity is little understood, despite its importance in possibly helping justify investment in software diversification. As a first step towards ultimately tackling this problem, we propose a systematic framework for modeling and quantifying the cybersecurity effectiveness of network diversity, including a suite of cybersecurity metrics. We also present an agent-based simulation to empirically demonstrate the usefulness of the framework. We draw a number of insights, including the surprising result that proactive diversity is effective under very special circumstances, but reactive-adaptive diversity is much more effective in most cases.

</details>

<details>

<summary>2021-12-15 12:27:27 - Meta Adversarial Perturbations</summary>

- *Chia-Hung Yuan, Pin-Yu Chen, Chia-Mu Yu*

- `2111.10291v2` - [abs](http://arxiv.org/abs/2111.10291v2) - [pdf](http://arxiv.org/pdf/2111.10291v2)

> A plethora of attack methods have been proposed to generate adversarial examples, among which the iterative methods have been demonstrated the ability to find a strong attack. However, the computation of an adversarial perturbation for a new data point requires solving a time-consuming optimization problem from scratch. To generate a stronger attack, it normally requires updating a data point with more iterations. In this paper, we show the existence of a meta adversarial perturbation (MAP), a better initialization that causes natural images to be misclassified with high probability after being updated through only a one-step gradient ascent update, and propose an algorithm for computing such perturbations. We conduct extensive experiments, and the empirical results demonstrate that state-of-the-art deep neural networks are vulnerable to meta perturbations. We further show that these perturbations are not only image-agnostic, but also model-agnostic, as a single perturbation generalizes well across unseen data points and different neural network architectures.

</details>

<details>

<summary>2021-12-15 15:25:08 - A Comprehensive Study of Code-removal Patches in Automated Program Repair</summary>

- *Davide Ginelli, Matias Martinez, Leonardo Mariani, Martin Monperrus*

- `2012.06264v3` - [abs](http://arxiv.org/abs/2012.06264v3) - [pdf](http://arxiv.org/pdf/2012.06264v3)

> Automatic Program Repair (APR) techniques can promisingly help reducing the cost of debugging. Many relevant APR techniques follow the generate-and-validate approach, that is, the faulty program is iteratively modified with different change operators and then validated with a test suite until a plausible patch is generated. In particular, Kali is a generate-and-validate technique developed to investigate the possibility of generating plausible patches by only removing code. Former studies show that indeed Kali successfully addressed several faults. This paper addresses the case of code-removal patches in automated program repair investigating the reasons and the scenarios that make their creation possible, and the relationship with patches implemented by developers. Our study reveals that code-removal patches are often insufficient to fix bugs, and proposes a comprehensive taxonomy of code-removal patches that provides evidence of the problems that may affect test suites, opening new opportunities for researchers in the field of automatic program repair.

</details>

<details>

<summary>2021-12-15 19:18:09 - Cybersecurity Revisited: Honeytokens meet Google Authenticator</summary>

- *Vasilis Papaspirou, Maria Papathanasaki, Leandros Maglaras, Ioanna Kantzavelou, Christos Douligeris, Mohamed Amine Ferrag, Helge Janicke*

- `2112.08431v1` - [abs](http://arxiv.org/abs/2112.08431v1) - [pdf](http://arxiv.org/pdf/2112.08431v1)

> Although sufficient authentication mechanisms were enhanced by the use of two or more factors that resulted in new multi factor authentication schemes, more sophisticated and targeted attacks have shown they are also vulnerable. This research work proposes a novel two factor authentication system that incorporates honeytokens into the two factor authentication process. The current implementation collaborates with Google authenticator. The novelty and simplicity of the presented approach aims at providing additional layers of security and protection into a system and thus making it more secure through a stronger and more efficient authentication mechanism.

</details>

<details>

<summary>2021-12-15 20:39:40 - IoT Security and Safety Testing Toolkits for Water Distribution Systems</summary>

- *Sean O'Toole, Cameron Sewell, Hoda Mehrpouyan*

- `2112.08473v1` - [abs](http://arxiv.org/abs/2112.08473v1) - [pdf](http://arxiv.org/pdf/2112.08473v1)

> Due to the critical importance of Industrial Control Systems (ICS) to the operations of cities and countries, research into the security of critical infrastructure has become increasingly relevant and necessary. As a component of both the research and application sides of smart city development, accurate and precise modeling, simulation, and verification are key parts of a robust design and development tools that provide critical assistance in the prevention, detection, and recovery from abnormal behavior in the sensors, controllers, and actuators which make up a modern ICS system. However, while these tools have potential, there is currently a need for helper-tools to assist with their setup and configuration, if they are to be utilized widely. Existing state-of-the-art tools are often technically complex and difficult to customize for any given IoT/ICS processes. This is a serious barrier to entry for most technicians, engineers, researchers, and smart city planners, while slowing down the critical aspects of safety and security verification. To remedy this issue, we take a case study of existing simulation toolkits within the field of water management and expand on existing tools and algorithms with simplistic automated retrieval functionality using a much more in-depth and usable customization interface to accelerate simulation scenario design and implementation, allowing for customization of the cyber-physical network infrastructure and cyber attack scenarios. We additionally provide a novel in-tool-assessment of network's resilience according to graph theory path diversity. Further, we lay out a roadmap for future development and application of the proposed tool, including expansions on resiliency and potential vulnerability model checking, and discuss applications of our work to other fields relevant to the design and operation of smart cities.

</details>

<details>

<summary>2021-12-16 01:32:21 - A Deep Learning Approach for Ontology Enrichment from Unstructured Text</summary>

- *Lalit Mohan Sanagavarapu, Vivek Iyer, Raghu Reddy*

- `2112.08554v1` - [abs](http://arxiv.org/abs/2112.08554v1) - [pdf](http://arxiv.org/pdf/2112.08554v1)

> Information Security in the cyber world is a major cause for concern, with a significant increase in the number of attack surfaces. Existing information on vulnerabilities, attacks, controls, and advisories available on the web provides an opportunity to represent knowledge and perform security analytics to mitigate some of the concerns. Representing security knowledge in the form of ontology facilitates anomaly detection, threat intelligence, reasoning and relevance attribution of attacks, and many more. This necessitates dynamic and automated enrichment of information security ontologies. However, existing ontology enrichment algorithms based on natural language processing and ML models have issues with contextual extraction of concepts in words, phrases, and sentences. This motivates the need for sequential Deep Learning architectures that traverse through dependency paths in text and extract embedded vulnerabilities, threats, controls, products, and other security-related concepts and instances from learned path representations. In the proposed approach, Bidirectional LSTMs trained on a large DBpedia dataset and Wikipedia corpus of 2.8 GB along with Universal Sentence Encoder is deployed to enrich ISO 27001-based information security ontology. The model is trained and tested on a high-performance computing (HPC) environment to handle Wiki text dimensionality. The approach yielded a test accuracy of over 80% when tested with knocked-out concepts from ontology and web page instances to validate the robustness.

</details>

<details>

<summary>2021-12-16 06:25:46 - Taming Repetition in Dialogue Generation</summary>

- *Yadong Xi, Jiashu Pu, Xiaoxi Mao*

- `2112.08657v1` - [abs](http://arxiv.org/abs/2112.08657v1) - [pdf](http://arxiv.org/pdf/2112.08657v1)

> The wave of pre-training language models has been continuously improving the quality of the machine-generated conversations, however, some of the generated responses still suffer from excessive repetition, sometimes repeating words from utterance, sometimes repeating words within self-generated responses, or both. Inappropriate repetition of words can significantly degrade the quality of the generated texts. Penalized sampling is one popular solution, reducing the sampling probability of existing words during inference, however, it is highly vulnerable to the inappropriate setting of the static weight. Setting it too high can yield strange and unrealistic sentences while setting it too low makes the task of suppressing repetition trivial. To remedy the shortcomings of the above methods, we design a context-aware classifier to explicitly decide when to allow repetition and when to employ penalized sampling. Such a classifier can be easily integrated with existing decoding methods, reducing repetitions where appropriate while preserving the diversity of the text. Experimental results demonstrate that our method can generate higher quality and more authentic dialogues.

</details>

<details>

<summary>2021-12-16 13:16:46 - Addressing Adversarial Machine Learning Attacks in Smart Healthcare Perspectives</summary>

- *Arawinkumaar Selvakkumar, Shantanu Pal, Zahra Jadidi*

- `2112.08862v1` - [abs](http://arxiv.org/abs/2112.08862v1) - [pdf](http://arxiv.org/pdf/2112.08862v1)

> Smart healthcare systems are gaining popularity with the rapid development of intelligent sensors, the Internet of Things (IoT) applications and services, and wireless communications. However, at the same time, several vulnerabilities and adversarial attacks make it challenging for a safe and secure smart healthcare system from a security point of view. Machine learning has been used widely to develop suitable models to predict and mitigate attacks. Still, the attacks could trick the machine learning models and misclassify outputs generated by the model. As a result, it leads to incorrect decisions, for example, false disease detection and wrong treatment plans for patients. In this paper, we address the type of adversarial attacks and their impact on smart healthcare systems. We propose a model to examine how adversarial attacks impact machine learning classifiers. To test the model, we use a medical image dataset. Our model can classify medical images with high accuracy. We then attacked the model with a Fast Gradient Sign Method attack (FGSM) to cause the model to predict the images and misclassify them inaccurately. Using transfer learning, we train a VGG-19 model with the medical dataset and later implement the FGSM to the Convolutional Neural Network (CNN) to examine the significant impact it causes on the performance and accuracy of the machine learning model. Our results demonstrate that the adversarial attack misclassifies the images, causing the model's accuracy rate to drop from 88% to 11%.

</details>

<details>

<summary>2021-12-16 15:12:22 - Slot-VPS: Object-centric Representation Learning for Video Panoptic Segmentation</summary>

- *Yi Zhou, Hui Zhang, Hana Lee, Shuyang Sun, Pingjun Li, Yangguang Zhu, ByungIn Yoo, Xiaojuan Qi, Jae-Joon Han*

- `2112.08949v1` - [abs](http://arxiv.org/abs/2112.08949v1) - [pdf](http://arxiv.org/pdf/2112.08949v1)

> Video Panoptic Segmentation (VPS) aims at assigning a class label to each pixel, uniquely segmenting and identifying all object instances consistently across all frames. Classic solutions usually decompose the VPS task into several sub-tasks and utilize multiple surrogates (e.g. boxes and masks, centres and offsets) to represent objects. However, this divide-and-conquer strategy requires complex post-processing in both spatial and temporal domains and is vulnerable to failures from surrogate tasks. In this paper, inspired by object-centric learning which learns compact and robust object representations, we present Slot-VPS, the first end-to-end framework for this task. We encode all panoptic entities in a video, including both foreground instances and background semantics, with a unified representation called panoptic slots. The coherent spatio-temporal object's information is retrieved and encoded into the panoptic slots by the proposed Video Panoptic Retriever, enabling it to localize, segment, differentiate, and associate objects in a unified manner. Finally, the output panoptic slots can be directly converted into the class, mask, and object ID of panoptic objects in the video. We conduct extensive ablation studies and demonstrate the effectiveness of our approach on two benchmark datasets, Cityscapes-VPS (\textit{val} and test sets) and VIPER (\textit{val} set), achieving new state-of-the-art performance of 63.7, 63.3 and 56.2 VPQ, respectively.

</details>

<details>

<summary>2021-12-16 15:54:11 - Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions</summary>

- *Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri*

- `2108.09293v3` - [abs](http://arxiv.org/abs/2108.09293v3) - [pdf](http://arxiv.org/pdf/2108.09293v3)

> There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described `AI pair programmer', GitHub Copilot, a language model trained over open-source GitHub code. However, code often contains bugs - and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot's code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk CWEs (e.g. those from MITRE's "Top 25" list). We explore Copilot's performance on three distinct code generation axes -- examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40% to be vulnerable.

</details>

<details>

<summary>2021-12-16 17:21:21 - Combating Adversaries with Anti-Adversaries</summary>

- *Motasem Alfarra, Juan C. Pérez, Ali Thabet, Adel Bibi, Philip H. S. Torr, Bernard Ghanem*

- `2103.14347v2` - [abs](http://arxiv.org/abs/2103.14347v2) - [pdf](http://arxiv.org/pdf/2103.14347v2)

> Deep neural networks are vulnerable to small input perturbations known as adversarial attacks. Inspired by the fact that these adversaries are constructed by iteratively minimizing the confidence of a network for the true class label, we propose the anti-adversary layer, aimed at countering this effect. In particular, our layer generates an input perturbation in the opposite direction of the adversarial one and feeds the classifier a perturbed version of the input. Our approach is training-free and theoretically supported. We verify the effectiveness of our approach by combining our layer with both nominally and robustly trained models and conduct large-scale experiments from black-box to adaptive attacks on CIFAR10, CIFAR100, and ImageNet. Our layer significantly enhances model robustness while coming at no cost on clean accuracy.

</details>

<details>

<summary>2021-12-16 19:30:57 - Timing Cache Accesses to Eliminate Side Channels in Shared Software</summary>

- *Divya Ojha, Sandhya Dwarkadas*

- `2009.14732v2` - [abs](http://arxiv.org/abs/2009.14732v2) - [pdf](http://arxiv.org/pdf/2009.14732v2)

> Timing side channels have been used to extract cryptographic keys and sensitive documents, even from trusted enclaves. In this paper, we focus on cache side channels created by access to shared code or data in the memory hierarchy. This vulnerability is exploited by several known attacks, e.g, evict+reload for recovering an RSA key and Spectre variants for data leaked due to speculative accesses. The key insight in this paper is the importance of the first access to the shared data after a victim brings the data into the cache. To eliminate the timing side channel, we ensure that the first access by a process to any cache line loaded by another process results in a miss. We accomplish this goal by using a combination of timestamps and a novel hardware design to allow efficient parallel comparisons of the timestamps. The solution works at all the cache levels and defends against an attacker process running on another core, same core, or another hyperthread. Our design retains the benefits of a shared cache: allowing processes to utilize the entire cache for their execution and retaining a single copy of shared code and data (data deduplication). Our implementation in the GEM5 simulator demonstrates that the system is able to defend against RSA key extraction. We evaluate performance using SPECCPU2006 and observe overhead due to first access delay to be 2.17%. The overhead due to the security context bookkeeping is of the order of 0.3%.

</details>

<details>

<summary>2021-12-16 19:50:26 - Defending Against Image Corruptions Through Adversarial Augmentations</summary>

- *Dan A. Calian, Florian Stimberg, Olivia Wiles, Sylvestre-Alvise Rebuffi, Andras Gyorgy, Timothy Mann, Sven Gowal*

- `2104.01086v3` - [abs](http://arxiv.org/abs/2104.01086v3) - [pdf](http://arxiv.org/pdf/2104.01086v3)

> Modern neural networks excel at image classification, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as AugMix and DeepAugment, introduce defenses that operate in expectation over a distribution of image corruptions. In contrast, the literature on $\ell_p$-norm bounded perturbations focuses on defenses against worst-case corruptions. In this work, we reconcile both approaches by proposing AdversarialAugment, a technique which optimizes the parameters of image-to-image models to generate adversarially corrupted augmented images. We theoretically motivate our method and give sufficient conditions for the consistency of its idealized version as well as that of DeepAugment. Our classifiers improve upon the state-of-the-art on common image corruption benchmarks conducted in expectation on CIFAR-10-C and improve worst-case performance against $\ell_p$-norm bounded perturbations on both CIFAR-10 and ImageNet.

</details>

<details>

<summary>2021-12-16 20:55:45 - Direction-Aggregated Attack for Transferable Adversarial Examples</summary>

- *Tianjin Huang, Vlado Menkovski, Yulong Pei, YuHao Wang, Mykola Pechenizkiy*

- `2104.09172v2` - [abs](http://arxiv.org/abs/2104.09172v2) - [pdf](http://arxiv.org/pdf/2104.09172v2)

> Deep neural networks are vulnerable to adversarial examples that are crafted by imposing imperceptible changes to the inputs. However, these adversarial examples are most successful in white-box settings where the model and its parameters are available. Finding adversarial examples that are transferable to other models or developed in a black-box setting is significantly more difficult. In this paper, we propose the Direction-Aggregated adversarial attacks that deliver transferable adversarial examples. Our method utilizes aggregated direction during the attack process for avoiding the generated adversarial examples overfitting to the white-box model. Extensive experiments on ImageNet show that our proposed method improves the transferability of adversarial examples significantly and outperforms state-of-the-art attacks, especially against adversarial robust models. The best averaged attack success rates of our proposed method reaches 94.6\% against three adversarial trained models and 94.8\% against five defense methods. It also reveals that current defense approaches do not prevent transferable adversarial attacks.

</details>

<details>

<summary>2021-12-17 07:02:10 - APTSHIELD: A Stable, Efficient and Real-time APT Detection System for Linux Hosts</summary>

- *Tiantian Zhu, Jinkai Yu, Tieming Chen, Jiayu Wang, Jie Ying, Ye Tian, Mingqi Lv, Yan Chen, Yuan Fan, Ting Wang*

- `2112.09008v2` - [abs](http://arxiv.org/abs/2112.09008v2) - [pdf](http://arxiv.org/pdf/2112.09008v2)

> Advanced Persistent Threat (APT) attack usually refers to the form of long-term, covert and sustained attack on specific targets, with an adversary using advanced attack techniques to destroy the key facilities of an organization. APT attacks have caused serious security threats and massive financial loss worldwide. Academics and industry thereby have proposed a series of solutions to detect APT attacks, such as dynamic/static code analysis, traffic detection, sandbox technology, endpoint detection and response (EDR), etc. However, existing defenses are failed to accurately and effectively defend against the current APT attacks that exhibit strong persistent, stealthy, diverse and dynamic characteristics due to the weak data source integrity, large data processing overhead and poor real-time performance in the process of real-world scenarios.   To overcome these difficulties, in this paper we propose APTSHIELD, a stable, efficient and real-time APT detection system for Linux hosts. In the aspect of data collection, audit is selected to stably collect kernel data of the operating system so as to carry out a complete portrait of the attack based on comprehensive analysis and comparison of existing logging tools; In the aspect of data processing, redundant semantics skipping and non-viable node pruning are adopted to reduce the amount of data, so as to reduce the overhead of the detection system; In the aspect of attack detection, an APT attack detection framework based on ATT\&CK model is designed to carry out real-time attack response and alarm through the transfer and aggregation of labels. Experimental results on both laboratory and Darpa Engagement show that our system can effectively detect web vulnerability attacks, file-less attacks and remote access trojan attacks, and has a low false positive rate, which adds far more value than the existing frontier work.

</details>

<details>

<summary>2021-12-17 14:07:23 - Symmetry-aware Neural Architecture for Embodied Visual Navigation</summary>

- *Shuang Liu, Takayuki Okatani*

- `2112.09515v1` - [abs](http://arxiv.org/abs/2112.09515v1) - [pdf](http://arxiv.org/pdf/2112.09515v1)

> Visual exploration is a task that seeks to visit all the navigable areas of an environment as quickly as possible. The existing methods employ deep reinforcement learning (RL) as the standard tool for the task. However, they tend to be vulnerable to statistical shifts between the training and test data, resulting in poor generalization over novel environments that are out-of-distribution (OOD) from the training data. In this paper, we attempt to improve the generalization ability by utilizing the inductive biases available for the task. Employing the active neural SLAM (ANS) that learns exploration policies with the advantage actor-critic (A2C) method as the base framework, we first point out that the mappings represented by the actor and the critic should satisfy specific symmetries. We then propose a network design for the actor and the critic to inherently attain these symmetries. Specifically, we use $G$-convolution instead of the standard convolution and insert the semi-global polar pooling (SGPP) layer, which we newly design in this study, in the last section of the critic network. Experimental results show that our method increases area coverage by $8.1 m^2$ when trained on the Gibson dataset and tested on the MP3D dataset, establishing the new state-of-the-art.

</details>

<details>

<summary>2021-12-17 18:03:14 - Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering</summary>

- *Jiayu Ding, Siyuan Wang, Qin Chen, Zhongyu Wei*

- `2112.09658v1` - [abs](http://arxiv.org/abs/2112.09658v1) - [pdf](http://arxiv.org/pdf/2112.09658v1)

> Recent years have witnessed impressive advances in challenging multi-hop QA tasks. However, these QA models may fail when faced with some disturbance in the input text and their interpretability for conducting multi-hop reasoning remains uncertain. Previous adversarial attack works usually edit the whole question sentence, which has limited effect on testing the entity-based multi-hop inference ability. In this paper, we propose a multi-hop reasoning chain based adversarial attack method. We formulate the multi-hop reasoning chains starting from the query entity to the answer entity in the constructed graph, which allows us to align the question to each reasoning hop and thus attack any hop. We categorize the questions into different reasoning types and adversarially modify part of the question corresponding to the selected reasoning hop to generate the distracting sentence. We test our adversarial scheme on three QA models on HotpotQA dataset. The results demonstrate significant performance reduction on both answer and supporting facts prediction, verifying the effectiveness of our reasoning chain based attack method for multi-hop reasoning models and the vulnerability of them. Our adversarial re-training further improves the performance and robustness of these models.

</details>

<details>

<summary>2021-12-19 04:48:23 - Anomaly-resistant Graph Neural Networks via Neural Architecture Search</summary>

- *M. Park*

- `2111.11406v3` - [abs](http://arxiv.org/abs/2111.11406v3) - [pdf](http://arxiv.org/pdf/2111.11406v3)

> In general, Graph Neural Networks(GNN) have been using a message passing method to aggregate and summarize information about neighbors to express their information. Nonetheless, previous studies have shown that the performance of graph neural networks becomes vulnerable when there are abnormal nodes in the neighborhood due to this message passing method. In this paper, inspired by the Neural Architecture Search method, we present an algorithm that recognizes abnormal nodes and automatically excludes them from information aggregation. Experiments on various real worlds datasets show that our proposed Neural Architecture Search-based Anomaly Resistance Graph Neural Network (NASAR-GNN) is actually effective.

</details>

<details>

<summary>2021-12-19 09:57:59 - An Architecture for Exploiting Native User-Land Checkpoint-Restart to Improve Fuzzing</summary>

- *Prashant Singh Chouhan, Gregory Price, Gene Cooperman*

- `2112.10100v1` - [abs](http://arxiv.org/abs/2112.10100v1) - [pdf](http://arxiv.org/pdf/2112.10100v1)

> Fuzzing is one of the most popular and widely used techniques to find vulnerabilities in any application. Fuzzers are fast enough, but they still spend a good portion of time to restart a crashed application and then fuzz it from the beginning. Fuzzing an application from a point deeper in the execution is also important. To do this, a user needs to take a snapshot of the program while fuzzing it on top of an emulator, virtual machine, or by utilizing a special kernel module to enable checkpointing. Even with this ability, it can be difficult to attach a fuzzer after restoring a checkpoint. As a result, most fuzzers leverage a form of fork-server design.   We propose a novel testing architecture that allows users to attach a fuzzer after the program has started running. We do this by natively checkpointing the target application at a point of interest, and attaching the fuzzer after restoring the checkpoint. A fork-server may even be engaged at the point of restoration. This not only improves the throughput of the fuzzing campaign by minimizing startup time, but opens up a new way to fuzz applications. With this architecture, a user can take a series of checkpoints at points of interest, and run parallel tests to reduce the overall state-complexity of an individual test. Checkpoints allow us to begin fuzzing from a deeper point in the execution path, omitting prior execution from the required coverage path. This and other checkpointing techniques are described in the paper to help improve fuzzing.

</details>

<details>

<summary>2021-12-19 10:02:47 - DeformRS: Certifying Input Deformations with Randomized Smoothing</summary>

- *Motasem Alfarra, Adel Bibi, Naeemullah Khan, Philip H. S. Torr, Bernard Ghanem*

- `2107.00996v2` - [abs](http://arxiv.org/abs/2107.00996v2) - [pdf](http://arxiv.org/pdf/2107.00996v2)

> Deep neural networks are vulnerable to input deformations in the form of vector fields of pixel displacements and to other parameterized geometric deformations e.g. translations, rotations, etc. Current input deformation certification methods either 1. do not scale to deep networks on large input datasets, or 2. can only certify a specific class of deformations, e.g. only rotations. We reformulate certification in randomized smoothing setting for both general vector field and parameterized deformations and propose DeformRS-VF and DeformRS-Par, respectively. Our new formulation scales to large networks on large input datasets. For instance, DeformRS-Par certifies rich deformations, covering translations, rotations, scaling, affine deformations, and other visually aligned deformations such as ones parameterized by Discrete-Cosine-Transform basis. Extensive experiments on MNIST, CIFAR10, and ImageNet show competitive performance of DeformRS-Par achieving a certified accuracy of $39\%$ against perturbed rotations in the set $[-10\degree,10\degree]$ on ImageNet.

</details>

<details>

<summary>2021-12-19 11:30:29 - Early Detection of Security-Relevant Bug Reports using Machine Learning: How Far Are We?</summary>

- *Arthur D. Sawadogo, Quentin Guimard, Tegawendé F. Bissyandé, Abdoul Kader Kaboré, Jacques Klein, Naouel Moha*

- `2112.10123v1` - [abs](http://arxiv.org/abs/2112.10123v1) - [pdf](http://arxiv.org/pdf/2112.10123v1)

> Bug reports are common artefacts in software development. They serve as the main channel for users to communicate to developers information about the issues that they encounter when using released versions of software programs. In the descriptions of issues, however, a user may, intentionally or not, expose a vulnerability. In a typical maintenance scenario, such security-relevant bug reports are prioritised by the development team when preparing corrective patches. Nevertheless, when security relevance is not immediately expressed (e.g., via a tag) or rapidly identified by triaging teams, the open security-relevant bug report can become a critical leak of sensitive information that attackers can leverage to perform zero-day attacks. To support practitioners in triaging bug reports, the research community has proposed a number of approaches for the detection of security-relevant bug reports. In recent years, approaches in this respect based on machine learning have been reported with promising performance. Our work focuses on such approaches, and revisits their building blocks to provide a comprehensive view on the current achievements. To that end, we built a large experimental dataset and performed extensive experiments with variations in feature sets and learning algorithms. Eventually, our study highlights different approach configurations that yield best performing classifiers.

</details>

<details>

<summary>2021-12-20 04:14:32 - Deriving Semantics-Aware Fuzzers from Web API Schemas</summary>

- *Zac Hatfield-Dodds, Dmitry Dygalo*

- `2112.10328v1` - [abs](http://arxiv.org/abs/2112.10328v1) - [pdf](http://arxiv.org/pdf/2112.10328v1)

> Fuzzing -- whether generating or mutating inputs -- has found many bugs and security vulnerabilities in a wide range of domains. Stateful and highly structured web APIs present significant challenges to traditional fuzzing techniques, as execution feedback is usually limited to a response code instead of code coverage and vulnerabilities of interest include silent information-disclosure in addition to explicit errors.   Our tool, Schemathesis, derives structure- and semantics-aware fuzzers from web API schemas in the OpenAPI or GraphQL formats, using property-based testing tools. Derived fuzzers can be incorporated into unit-test suites or run directly, with or without end-user customisation of data generation and semantic checks.   We construct the most comprehensive evaluation of web API fuzzers to date, running eight fuzzers against sixteen real-world open source web services. OpenAPI schemas found in the wild have a long tail of rare features and complex structures. Of the tools we evaluated, Schemathesis was the only one to handle more than two-thirds of our target services without a fatal internal error. Schemathesis finds 1.4 times to 4.5 times more unique defects than the respectively second-best fuzzer for each target, and is the only fuzzer to find defects in four targets.

</details>

<details>

<summary>2021-12-20 06:42:02 - How Do Developers Deal with Security Issue Reports on GitHub?</summary>

- *Noah Bühlmann, Mohammad Ghafari*

- `2112.10359v1` - [abs](http://arxiv.org/abs/2112.10359v1) - [pdf](http://arxiv.org/pdf/2112.10359v1)

> Security issue reports are the primary means of informing development teams of security risks in projects, but little is known about current practices. We aim to understand the characteristics of these reports in open-source projects and uncover opportunities to improve developer practices. We analysed 3,493 security issue reports in 182 different projects on GitHub and manually studied 333 reports, and their discussions and pull requests. We found that, the number of security issue reports has increased over time, they are resolved faster, and they are reported in earlier development stages compared to past years. Nevertheless, a tiny group of developers are involved frequently, security issues progress slowly, and a great number of them has been pending for a long time. We realized that only a small subset of security issue reports include reproducibility data, a potential fix is rarely suggested, and there is no hint regarding how a reporter spotted an issue. We noted that the resolution time of an issue is significantly shorter when the first reaction to a security report is fast and when a reference to a known vulnerability exists.

</details>

<details>

<summary>2021-12-20 09:18:03 - Differential Privacy for Eye Tracking with Temporal Correlations</summary>

- *Efe Bozkir, Onur Günlü, Wolfgang Fuhl, Rafael F. Schaefer, Enkelejda Kasneci*

- `2002.08972v3` - [abs](http://arxiv.org/abs/2002.08972v3) - [pdf](http://arxiv.org/pdf/2002.08972v3)

> New generation head-mounted displays, such as VR and AR glasses, are coming into the market with already integrated eye tracking and are expected to enable novel ways of human-computer interaction in numerous applications. However, since eye movement properties contain biometric information, privacy concerns have to be handled properly. Privacy-preservation techniques such as differential privacy mechanisms have recently been applied to eye movement data obtained from such displays. Standard differential privacy mechanisms; however, are vulnerable due to temporal correlations between the eye movement observations. In this work, we propose a novel transform-coding based differential privacy mechanism to further adapt it to the statistics of eye movement feature data and compare various low-complexity methods. We extend the Fourier perturbation algorithm, which is a differential privacy mechanism, and correct a scaling mistake in its proof. Furthermore, we illustrate significant reductions in sample correlations in addition to query sensitivities, which provide the best utility-privacy trade-off in the eye tracking literature. Our results provide significantly high privacy without any essential loss in classification accuracies while hiding personal identifiers.

</details>

<details>

<summary>2021-12-20 10:48:29 - Vulnerability Analysis of the Android Kernel</summary>

- *Joseph R. Barr, Peter Shaw, Tyler Thatcher*

- `2112.11214v1` - [abs](http://arxiv.org/abs/2112.11214v1) - [pdf](http://arxiv.org/pdf/2112.11214v1)

> We describe a workflow used to analyze the source code of the {\sc Android OS kernel} and rate for a particular kind of bugginess that exposes a program to hacking. The workflow represents a novel approach for components' vulnerability rating. The approach is inspired by recent work on embedding source code functions. The workflow combines deep learning with heuristics and machine learning. Deep learning is used to embed function/method labels into a Euclidean space. Because the corpus of Android kernel source code is rather limited (containing approximately 2 million C/C++ functions \& Java methods), a straightforward embedding is untenable. To overcome the challenge of the dearth of data, it's necessary to go through an intermediate step of the \textit{Byte-Pair Encoding}. Subsequently, we embed the tokens from which we assemble an embedding of function/method labels. Long short-term memory networks (LSTM) are used to embed tokens into vectors in $\mathbb{R}^d$ from which we form a \textit{cosine matrix} consisting of the cosine between every pair of vectors. The cosine matrix may be interpreted as a (combinatorial) `weighted' graph whose vertices represent functions/methods and `weighted' edges correspond to matrix entries. Features that include function vectors plus those defined heuristically are used to score for risk of bugginess.

</details>

<details>

<summary>2021-12-20 13:11:04 - Relational Models of Microarchitectures for Formal Security Analyses</summary>

- *Nicholas Mosier, Hanna Lachnitt, Hamed Nemati, Caroline Trippel*

- `2112.10511v1` - [abs](http://arxiv.org/abs/2112.10511v1) - [pdf](http://arxiv.org/pdf/2112.10511v1)

> There is a growing need for hardware-software contracts which precisely define the implications of microarchitecture on software security-i.e., security contracts. It is our view that such contracts should explicitly account for microarchitecture-level implementation details that underpin hardware leakage, thereby establishing a direct correspondence between a contract and the microarchitecture it represents. At the same time, these contracts should remain as abstract as possible so as to support efficient formal analyses. With these goals in mind, we propose leakage containment models (LCMs)-novel axiomatic security contracts which support formally reasoning about the security guarantees of programs when they run on particular microarchitectures. Our core contribution is an axiomatic vocabulary for formally defining LCMs, derived from the established axiomatic vocabulary used to formalize processor memory consistency models. Using this vocabulary, we formalize microarchitectural leakage-focusing on leakage through hardware memory systems-so that it can be automatically detected in programs. To illustrate the efficacy of LCMs, we present two case studies. First, we demonstrate that our leakage definition faithfully captures a sampling of (transient and non-transient) microarchitectural attacks from the literature. Second, we develop a static analysis tool based on LCMs which automatically identifies Spectre vulnerabilities in programs and scales to analyze realistic-sized codebases, like libsodium.

</details>

<details>

<summary>2021-12-20 15:41:57 - FuSeBMC v.4: Smart Seed Generation for Hybrid Fuzzing</summary>

- *Kaled M. Alshmrany, Mohannad Aldughaim, Ahmed Bhayat, Lucas C. Cordeiro*

- `2112.10627v1` - [abs](http://arxiv.org/abs/2112.10627v1) - [pdf](http://arxiv.org/pdf/2112.10627v1)

> FuSeBMC is a test generator for finding security vulnerabilities in C programs. In earlier work [4], we described a previous version that incrementally injected labels to guide Bounded Model Checking (BMC) and Evolutionary Fuzzing engines to produce test cases for code coverage and bug finding. This paper introduces a new version of FuSeBMC that utilizes both engines to produce smart seeds. First, the engines are run with a short time limit on a lightly instrumented version of the program to produce the seeds. The BMC engine is particularly useful in producing seeds that can pass through complex mathematical guards. Then, FuSeBMC runs its engines with more extended time limits using the smart seeds created in the previous round. FuSeBMC manages this process in two main ways using its Tracer subsystem. Firstly, it uses shared memory to record the labels covered by each test case. Secondly, it evaluates test cases, and those of high impact are turned into seeds for subsequent test fuzzing. As a result, we significantly increased our code coverage score from last year, outperforming all tools that participated in this year's competition in every single category.

</details>

<details>

<summary>2021-12-20 16:22:53 - An Evasion Attack against Stacked Capsule Autoencoder</summary>

- *Jiazhu Dai, Siwei Xiong*

- `2010.07230v5` - [abs](http://arxiv.org/abs/2010.07230v5) - [pdf](http://arxiv.org/pdf/2010.07230v5)

> Capsule network is a type of neural network that uses the spatial relationship between features to classify images. By capturing the poses and relative positions between features, its ability to recognize affine transformation is improved, and it surpasses traditional convolutional neural networks (CNNs) when handling translation, rotation and scaling. The Stacked Capsule Autoencoder (SCAE) is the state-of-the-art capsule network. The SCAE encodes an image as capsules, each of which contains poses of features and their correlations. The encoded contents are then input into the downstream classifier to predict the categories of the images. Existing research mainly focuses on the security of capsule networks with dynamic routing or EM routing, and little attention has been given to the security and robustness of the SCAE. In this paper, we propose an evasion attack against the SCAE. After a perturbation is generated based on the output of the object capsules in the model, it is added to an image to reduce the contribution of the object capsules related to the original category of the image so that the perturbed image will be misclassified. We evaluate the attack using an image classification experiment, and the experimental results indicate that the attack can achieve high success rates and stealthiness. It confirms that the SCAE has a security vulnerability whereby it is possible to craft adversarial samples without changing the original structure of the image to fool the classifiers. We hope that our work will make the community aware of the threat of this attack and raise the attention given to the SCAE's security.

</details>

<details>

<summary>2021-12-20 21:53:51 - Channel-Aware Adversarial Attacks Against Deep Learning-Based Wireless Signal Classifiers</summary>

- *Brian Kim, Yalin E. Sagduyu, Kemal Davaslioglu, Tugba Erpek, Sennur Ulukus*

- `2005.05321v3` - [abs](http://arxiv.org/abs/2005.05321v3) - [pdf](http://arxiv.org/pdf/2005.05321v3)

> This paper presents channel-aware adversarial attacks against deep learning-based wireless signal classifiers. There is a transmitter that transmits signals with different modulation types. A deep neural network is used at each receiver to classify its over-the-air received signals to modulation types. In the meantime, an adversary transmits an adversarial perturbation (subject to a power budget) to fool receivers into making errors in classifying signals that are received as superpositions of transmitted signals and adversarial perturbations. First, these evasion attacks are shown to fail when channels are not considered in designing adversarial perturbations. Then, realistic attacks are presented by considering channel effects from the adversary to each receiver. After showing that a channel-aware attack is selective (i.e., it affects only the receiver whose channel is considered in the perturbation design), a broadcast adversarial attack is presented by crafting a common adversarial perturbation to simultaneously fool classifiers at different receivers. The major vulnerability of modulation classifiers to over-the-air adversarial attacks is shown by accounting for different levels of information available about the channel, the transmitter input, and the classifier model. Finally, a certified defense based on randomized smoothing that augments training data with noise is introduced to make the modulation classifier robust to adversarial perturbations.

</details>

<details>

<summary>2021-12-21 00:14:57 - Elixir: Effective object-oriented program repair</summary>

- *Ripon K. Saha, Yingjun Lyu, Hiroaki Yoshida, Mukul R. Prasad*

- `2112.10915v1` - [abs](http://arxiv.org/abs/2112.10915v1) - [pdf](http://arxiv.org/pdf/2112.10915v1)

> This work is motivated by the pervasive use of method invocations in object-oriented (OO) programs, and indeed their prevalence in patches of OO-program bugs. We propose a generate-and-validate repair technique, called ELIXIR designed to be able to generate such patches. ELIXIR aggressively uses method calls, on par with local variables, fields, or constants, to construct more expressive repair-expressions, that go into synthesizing patches. The ensuing enlargement of the repair space, on account of the wider use of method calls, is effectively tackled by using a machine-learnt model to rank concrete repairs. The machine-learnt model relies on four features derived from the program context, i.e., the code surrounding the potential repair location, and the bug report. We implement ELIXIR and evaluate it on two datasets, the popular Defects4J dataset and a new dataset Bugs.jar created by us, and against 2 baseline versions of our technique, and 5 other techniques representing the state of the art in program repair. Our evaluation shows that ELIXIR is able to increase the number of correctly repaired bugs in Defects4J by 85% (from 14 to 26) and by 57% in Bugs.jar (from 14 to 22), while also significantly out-performing other state-of-the-art repair techniques including ACS, HD-Repair, NOPOL, PAR, and jGenProg.

</details>

<details>

<summary>2021-12-21 07:41:02 - Well Begun is Half Done: An Empirical Study of Exploitability & Impact of Base-Image Vulnerabilities</summary>

- *Mubin Ul Haque, M. Ali Babar*

- `2112.12597v1` - [abs](http://arxiv.org/abs/2112.12597v1) - [pdf](http://arxiv.org/pdf/2112.12597v1)

> Container technology, (e.g., Docker) is being widely adopted for deploying software infrastructures or applications in the form of container images. Security vulnerabilities in the container images are a primary concern for developing containerized software. Exploitation of the vulnerabilities could result in disastrous impact, such as loss of confidentiality, integrity, and availability of containerized software. Understanding the exploitability and impact characteristics of vulnerabilities can help in securing the configuration of containerized software. However, there is a lack of research aimed at empirically identifying and understanding the exploitability and impact of vulnerabilities in container images. We carried out an empirical study to investigate the exploitability and impact of security vulnerabilities in base-images and their prevalence in open-source containerized software. We considered base-images since container images are built from base-images that provide all the core functionalities to build and operate containerized software. We discovered and characterized the exploitability and impact of security vulnerabilities in 261 base-images, which are the origin of 4,681 actively maintained official container images in the largest container registry, i.e., Docker Hub. To characterize the prevalence of vulnerable base-images in real-world projects, we analysed 64,579 containerized software from GitHub. Our analysis of a set of $1,983$ unique base-image security vulnerabilities revealed 13 novel findings. These findings are expected to help developers to understand the potential security problems related to base-images and encourage them to investigate base-images from security perspective before developing their applications.

</details>

<details>

<summary>2021-12-21 18:12:10 - Towards quantifying information flows: relative entropy in deep neural networks and the renormalization group</summary>

- *Johanna Erdmenger, Kevin T. Grosvenor, Ro Jefferson*

- `2107.06898v2` - [abs](http://arxiv.org/abs/2107.06898v2) - [pdf](http://arxiv.org/pdf/2107.06898v2)

> We investigate the analogy between the renormalization group (RG) and deep neural networks, wherein subsequent layers of neurons are analogous to successive steps along the RG. In particular, we quantify the flow of information by explicitly computing the relative entropy or Kullback-Leibler divergence in both the one- and two-dimensional Ising models under decimation RG, as well as in a feedforward neural network as a function of depth. We observe qualitatively identical behavior characterized by the monotonic increase to a parameter-dependent asymptotic value. On the quantum field theory side, the monotonic increase confirms the connection between the relative entropy and the c-theorem. For the neural networks, the asymptotic behavior may have implications for various information maximization methods in machine learning, as well as for disentangling compactness and generalizability. Furthermore, while both the two-dimensional Ising model and the random neural networks we consider exhibit non-trivial critical points, the relative entropy appears insensitive to the phase structure of either system. In this sense, more refined probes are required in order to fully elucidate the flow of information in these models.

</details>

<details>

<summary>2021-12-22 05:04:41 - How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?</summary>

- *Xinhsuai Dong, Luu Anh Tuan, Min Lin, Shuicheng Yan, Hanwang Zhang*

- `2112.11668v1` - [abs](http://arxiv.org/abs/2112.11668v1) - [pdf](http://arxiv.org/pdf/2112.11668v1)

> The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.

</details>

<details>

<summary>2021-12-22 07:08:08 - Semantics-Recovering Decompilation through Neural Machine Translation</summary>

- *Ruigang Liang, Ying Cao, Peiwei Hu, Jinwen He, Kai Chen*

- `2112.15491v1` - [abs](http://arxiv.org/abs/2112.15491v1) - [pdf](http://arxiv.org/pdf/2112.15491v1)

> Decompilation transforms low-level program languages (PL) (e.g., binary code) into high-level PLs (e.g., C/C++). It has been widely used when analysts perform security analysis on software (systems) whose source code is unavailable, such as vulnerability search and malware analysis. However, current decompilation tools usually need lots of experts' efforts, even for years, to generate the rules for decompilation, which also requires long-term maintenance as the syntax of high-level PL or low-level PL changes. Also, an ideal decompiler should concisely generate high-level PL with similar functionality to the source low-level PL and semantic information (e.g., meaningful variable names), just like human-written code. Unfortunately, existing manually-defined rule-based decompilation techniques only functionally restore the low-level PL to a similar high-level PL and are still powerless to recover semantic information. In this paper, we propose a novel neural decompilation approach to translate low-level PL into accurate and user-friendly high-level PL, effectively improving its readability and understandability. Furthermore, we implement the proposed approaches called SEAM. Evaluations on four real-world applications show that SEAM has an average accuracy of 94.41%, which is much better than prior neural machine translation (NMT) models. Finally, we evaluate the effectiveness of semantic information recovery through a questionnaire survey, and the average accuracy is 92.64%, which is comparable or superior to the state-of-the-art compilers.

</details>

<details>

<summary>2021-12-22 07:43:50 - HAD-GAN: A Human-perception Auxiliary Defense GAN to Defend Adversarial Examples</summary>

- *Wanting Yu, Hongyi Yu, Lingyun Jiang, Mengli Zhang, Kai Qiao*

- `1909.07558v5` - [abs](http://arxiv.org/abs/1909.07558v5) - [pdf](http://arxiv.org/pdf/1909.07558v5)

> Adversarial examples reveal the vulnerability and unexplained nature of neural networks. Studying the defense of adversarial examples is of considerable practical importance. Most adversarial examples that misclassify networks are often undetectable by humans. In this paper, we propose a defense model to train the classifier into a human-perception classification model with shape preference. The proposed model comprising a texture transfer network (TTN) and an auxiliary defense generative adversarial networks (GAN) is called Human-perception Auxiliary Defense GAN (HAD-GAN). The TTN is used to extend the texture samples of a clean image and helps classifiers focus on its shape. GAN is utilized to form a training framework for the model and generate the necessary images. A series of experiments conducted on MNIST, Fashion-MNIST and CIFAR10 show that the proposed model outperforms the state-of-the-art defense methods for network robustness. The model also demonstrates a significant improvement on defense capability of adversarial examples.

</details>

<details>

<summary>2021-12-22 09:24:55 - Security Risks of Porting C Programs to WebAssembly</summary>

- *Quentin Stiévenart, Coen De Roover, Mohammad Ghafari*

- `2112.11745v1` - [abs](http://arxiv.org/abs/2112.11745v1) - [pdf](http://arxiv.org/pdf/2112.11745v1)

> WebAssembly is a compilation target for cross-platform applications that is increasingly being used. In this paper, we investigate whether one can transparently cross-compile C programs to WebAssembly, and if not, what impact porting can have on their security. We compile 17,802 programs that exhibit common vulnerabilities to 64-bit x86 and to WebAssembly binaries, and we observe that the execution of 4,911 binaries produces different results across these platforms. Through manual inspection, we identify three classes of root causes for such differences: the use of a different standard library implementation, the lack of security measures in WebAssembly, and the different semantics of the execution environments. We describe our observations and discuss the ones that are critical from a security point of view and need most attention from developers. We conclude that compiling an existing C program to WebAssembly for cross-platform distribution may require source code adaptations; otherwise, the security of the WebAssembly application may be at risk.

</details>

<details>

<summary>2021-12-22 11:34:09 - QFlow: Quantitative Information Flow for Security-Aware Hardware Design in Verilog</summary>

- *Lennart M. Reimann, Luca Hanel, Dominik Sisejkovic, Farhad Merchant, Rainer Leupers*

- `2109.02379v2` - [abs](http://arxiv.org/abs/2109.02379v2) - [pdf](http://arxiv.org/pdf/2109.02379v2)

> The enormous amount of code required to design modern hardware implementations often leads to critical vulnerabilities being overlooked. Especially vulnerabilities that compromise the confidentiality of sensitive data, such as cryptographic keys, have a major impact on the trustworthiness of an entire system. Information flow analysis can elaborate whether information from sensitive signals flows towards outputs or untrusted components of the system. But most of these analytical strategies rely on the non-interference property, stating that the untrusted targets must not be influenced by the source's data, which is shown to be too inflexible for many applications. To address this issue, there are approaches to quantify the information flow between components such that insignificant leakage can be neglected. Due to the high computational complexity of this quantification, approximations are needed, which introduce mispredictions. To tackle those limitations, we reformulate the approximations. Further, we propose a tool QFlow with a higher detection rate than previous tools. It can be used by non-experienced users to identify data leakages in hardware designs, thus facilitating a security-aware design process.

</details>

<details>

<summary>2021-12-22 12:27:19 - Exploring the Security Boundary of Data Reconstruction via Neuron Exclusivity Analysis</summary>

- *Xudong Pan, Mi Zhang, Yifan Yan, Jiaming Zhu, Min Yang*

- `2010.13356v2` - [abs](http://arxiv.org/abs/2010.13356v2) - [pdf](http://arxiv.org/pdf/2010.13356v2)

> Among existing privacy attacks on the gradient of neural networks, \emph{data reconstruction attack}, which reverse engineers the training batch from the gradient, poses a severe threat on the private training data. Despite its empirical success on large architectures and small training batches, unstable reconstruction accuracy is also observed when a smaller architecture or a larger batch is under attack. Due to the weak interpretability of existing learning-based attacks, there is little known on why, when and how data reconstruction attack is feasible.   In our work, we perform the first analytic study on the security boundary of data reconstruction from gradient via a microcosmic view on neural networks with rectified linear units (ReLUs), the most popular activation function in practice. For the first time, we characterize the insecure/secure boundary of data reconstruction attack in terms of the \emph{neuron exclusivity state} of a training batch, indexed by the number of \emph{\textbf{Ex}clusively \textbf{A}ctivated \textbf{N}eurons} (ExANs, i.e., a ReLU activated by only one sample in a batch). Intuitively, we show a training batch with more ExANs are more vulnerable to data reconstruction attack and vice versa. On the one hand, we construct a novel deterministic attack algorithm which substantially outperforms previous attacks for reconstructing training batches lying in the insecure boundary of a neural network. Meanwhile, for training batches lying in the secure boundary, we prove the impossibility of unique reconstruction, based on which an exclusivity reduction strategy is devised to enlarge the secure boundary for mitigation purposes.

</details>

<details>

<summary>2021-12-22 13:33:39 - CompilerGym: Robust, Performant Compiler Optimization Environments for AI Research</summary>

- *Chris Cummins, Bram Wasti, Jiadong Guo, Brandon Cui, Jason Ansel, Sahir Gomez, Somya Jain, Jia Liu, Olivier Teytaud, Benoit Steiner, Yuandong Tian, Hugh Leather*

- `2109.08267v2` - [abs](http://arxiv.org/abs/2109.08267v2) - [pdf](http://arxiv.org/pdf/2109.08267v2)

> Interest in applying Artificial Intelligence (AI) techniques to compiler optimizations is increasing rapidly, but compiler research has a high entry barrier. Unlike in other domains, compiler and AI researchers do not have access to the datasets and frameworks that enable fast iteration and development of ideas, and getting started requires a significant engineering investment. What is needed is an easy, reusable experimental infrastructure for real world compiler optimization tasks that can serve as a common benchmark for comparing techniques, and as a platform to accelerate progress in the field.   We introduce CompilerGym, a set of environments for real world compiler optimization tasks, and a toolkit for exposing new optimization tasks to compiler researchers. CompilerGym enables anyone to experiment on production compiler optimization problems through an easy-to-use package, regardless of their experience with compilers. We build upon the popular OpenAI Gym interface enabling researchers to interact with compilers using Python and a familiar API.   We describe the CompilerGym architecture and implementation, characterize the optimization spaces and computational efficiencies of three included compiler environments, and provide extensive empirical evaluations. Compared to prior works, CompilerGym offers larger datasets and optimization spaces, is 27x more computationally efficient, is fault-tolerant, and capable of detecting reproducibility bugs in the underlying compilers.   In making it easy for anyone to experiment with compilers - irrespective of their background - we aim to accelerate progress in the AI and compiler research domains.

</details>

<details>

<summary>2021-12-22 17:54:54 - Detect & Reject for Transferability of Black-box Adversarial Attacks Against Network Intrusion Detection Systems</summary>

- *Islam Debicha, Thibault Debatty, Jean-Michel Dricot, Wim Mees, Tayeb Kenaza*

- `2112.12095v1` - [abs](http://arxiv.org/abs/2112.12095v1) - [pdf](http://arxiv.org/pdf/2112.12095v1)

> In the last decade, the use of Machine Learning techniques in anomaly-based intrusion detection systems has seen much success. However, recent studies have shown that Machine learning in general and deep learning specifically are vulnerable to adversarial attacks where the attacker attempts to fool models by supplying deceptive input. Research in computer vision, where this vulnerability was first discovered, has shown that adversarial images designed to fool a specific model can deceive other machine learning models. In this paper, we investigate the transferability of adversarial network traffic against multiple machine learning-based intrusion detection systems. Furthermore, we analyze the robustness of the ensemble intrusion detection system, which is notorious for its better accuracy compared to a single model, against the transferability of adversarial attacks. Finally, we examine Detect & Reject as a defensive mechanism to limit the effect of the transferability property of adversarial network traffic against machine learning-based intrusion detection systems.

</details>

<details>

<summary>2021-12-22 19:49:49 - An Approach to Detecting Bugs in Pattern-Based Bug Detectors</summary>

- *Junjie Wang, Yuchao Huang, Song Wang, Qing Wang*

- `2109.02245v2` - [abs](http://arxiv.org/abs/2109.02245v2) - [pdf](http://arxiv.org/pdf/2109.02245v2)

> Static bug finders have been widely-adopted by developers to find bugs in real world software projects. They leverage predefined heuristic static analysis rules to scan source code or binary code of a software project, and report violations to these rules as warnings to be verified. However, the advantages of static bug finders are overshadowed by such issues as uncovered obvious bugs, false positives, etc. To improve these tools, many techniques have been proposed to filter out false positives reported or design new static analysis rules. Nevertheless, the under-performance of bug finders can also be caused by the incorrectness of current rules contained in the static bug finders, which is not explored yet. In this work, we propose a differential testing approach to detect bugs in the rules of four widely-used static bug finders, i.e., SonarQube, PMD, SpotBugs, and ErrorProne, and conduct a qualitative study about the bugs found. To retrieve paired rules across static bug finders for differential testing, we design a heuristic-based rule mapping method which combines the similarity in rules description and the overlap in warning information reported by the tools. The experiment on 2,728 open source projects reveals 46 bugs in the static bug finders, among which 24 are fixed or confirmed and the left are awaiting confirmation. We also summarize 13 bug patterns in the static analysis rules based on their context and root causes, which can serve as the checklist for designing and implementing other rules and or in other tools. This study indicates that the commonly-used static bug finders are not as reliable as they might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of the static bug finders.

</details>

<details>

<summary>2021-12-23 19:57:09 - A Rationale-Based Classification of MISRA C Guidelines</summary>

- *Roberto Bagnara, Abramo Bagnara, Patricia M. Hill*

- `2112.12823v1` - [abs](http://arxiv.org/abs/2112.12823v1) - [pdf](http://arxiv.org/pdf/2112.12823v1)

> MISRA C is the most authoritative language subset for the C programming language that is a de facto standard in several industry sectors where safety and security are of paramount importance. While MISRA C is currently encoded in 175 guidelines (coding rules and directives), it does not coincide with them: proper adoption of MISRA C requires embracing its preventive approach (as opposed to the "bug finding" approach) and a documented development process where justifiable non-compliances are authorized and recorded as deviations. MISRA C guidelines are classified along several axes in the official MISRA documents. In this paper, we add to these an orthogonal classification that associates guidelines with their main rationale. The advantages of this new classification are illustrated for different kinds of projects, including those not (yet) having MISRA compliance among their objectives.

</details>

<details>

<summary>2021-12-24 14:19:49 - A Triangular Fuzzy based Multicriteria Decision Making Approach for Assessing Security Risks in 5G Networks</summary>

- *Hisham A. Kholidy*

- `2112.13072v1` - [abs](http://arxiv.org/abs/2112.13072v1) - [pdf](http://arxiv.org/pdf/2112.13072v1)

> The emerging 5G network is a new global wireless standard after 1G, 2G, 3G, and 4G networks. In comparison to 4G, it has lower latency, larger capacity, and more bandwidth. These network upgrades will have a profound impact on how people throughout the world live and work. The current research investigates mechanisms to protect the 5G networks to meet resilience requirements and to minimize the damage from attacks that do occur. The main contribution of this paper includes: (1) Improving the current 5G security testbed by orchestrating the security services using the OSM and Open stack, Integrating the FlexRan with the testbed components to control and manage the eNodeB, and implementing some real-time security experiments to test and validate the testbed. (2) Develop an intelligent fuzzy method to improve the accuracy of the current Vulnerability Assessment Approach (VAA) using a new approach that integrates the Triangular Fuzzy Numbers (TFNs) and a multi-criteria decision-making technique to find the attack graph paths where the attack most probably will propagate in the 5G network.

</details>

<details>

<summary>2021-12-24 16:48:30 - An Investigation on Learning, Polluting, and Unlearning the Spam Emails for Lifelong Learning</summary>

- *Nishchal Parne, Kyathi Puppaala, Nithish Bhupathi, Ripon Patgiri*

- `2111.14609v2` - [abs](http://arxiv.org/abs/2111.14609v2) - [pdf](http://arxiv.org/pdf/2111.14609v2)

> Machine unlearning for security is studied in this context. Several spam email detection methods exist, each of which employs a different algorithm to detect undesired spam emails. But these models are vulnerable to attacks. Many attackers exploit the model by polluting the data, which are trained to the model in various ways. So to act deftly in such situations model needs to readily unlearn the polluted data without the need for retraining. Retraining is impractical in most cases as there is already a massive amount of data trained to the model in the past, which needs to be trained again just for removing a small amount of polluted data, which is often significantly less than 1%. This problem can be solved by developing unlearning frameworks for all spam detection models. In this research, unlearning module is integrated into spam detection models that are based on Naive Bayes, Decision trees, and Random Forests algorithms. To assess the benefits of unlearning over retraining, three spam detection models are polluted and exploited by taking attackers' positions and proving models' vulnerability. Reduction in accuracy and true positive rates are shown in each case showing the effect of pollution on models. Then unlearning modules are integrated into the models, and polluted data is unlearned; on testing the models after unlearning, restoration of performance is seen. Also, unlearning and retraining times are compared with different pollution data sizes on all models. On analyzing the findings, it can be concluded that unlearning is considerably superior to retraining. Results show that unlearning is fast, easy to implement, easy to use, and effective.

</details>

<details>

<summary>2021-12-25 00:32:52 - Stealthy Attack on Algorithmic-Protected DNNs via Smart Bit Flipping</summary>

- *Behnam Ghavami, Seyd Movi, Zhenman Fang, Lesley Shannon*

- `2112.13162v1` - [abs](http://arxiv.org/abs/2112.13162v1) - [pdf](http://arxiv.org/pdf/2112.13162v1)

> Recently, deep neural networks (DNNs) have been deployed in safety-critical systems such as autonomous vehicles and medical devices. Shortly after that, the vulnerability of DNNs were revealed by stealthy adversarial examples where crafted inputs -- by adding tiny perturbations to original inputs -- can lead a DNN to generate misclassification outputs. To improve the robustness of DNNs, some algorithmic-based countermeasures against adversarial examples have been introduced thereafter.   In this paper, we propose a new type of stealthy attack on protected DNNs to circumvent the algorithmic defenses: via smart bit flipping in DNN weights, we can reserve the classification accuracy for clean inputs but misclassify crafted inputs even with algorithmic countermeasures. To fool protected DNNs in a stealthy way, we introduce a novel method to efficiently find their most vulnerable weights and flip those bits in hardware. Experimental results show that we can successfully apply our stealthy attack against state-of-the-art algorithmic-protected DNNs.

</details>

<details>

<summary>2021-12-25 03:33:02 - Gradient Leakage Attack Resilient Deep Learning</summary>

- *Wenqi Wei, Ling Liu*

- `2112.13178v1` - [abs](http://arxiv.org/abs/2112.13178v1) - [pdf](http://arxiv.org/pdf/2112.13178v1)

> Gradient leakage attacks are considered one of the wickedest privacy threats in deep learning as attackers covertly spy gradient updates during iterative training without compromising model training quality, and yet secretly reconstruct sensitive training data using leaked gradients with high attack success rate. Although deep learning with differential privacy is a defacto standard for publishing deep learning models with differential privacy guarantee, we show that differentially private algorithms with fixed privacy parameters are vulnerable against gradient leakage attacks. This paper investigates alternative approaches to gradient leakage resilient deep learning with differential privacy (DP). First, we analyze existing implementation of deep learning with differential privacy, which use fixed noise variance to injects constant noise to the gradients in all layers using fixed privacy parameters. Despite the DP guarantee provided, the method suffers from low accuracy and is vulnerable to gradient leakage attacks. Second, we present a gradient leakage resilient deep learning approach with differential privacy guarantee by using dynamic privacy parameters. Unlike fixed-parameter strategies that result in constant noise variance, different dynamic parameter strategies present alternative techniques to introduce adaptive noise variance and adaptive noise injection which are closely aligned to the trend of gradient updates during differentially private model training. Finally, we describe four complementary metrics to evaluate and compare alternative approaches.

</details>

<details>

<summary>2021-12-25 08:27:03 - FMViz: Visualizing Tests Generated by AFL at the Byte-level</summary>

- *Aftab Hussain, Mohammad Amin Alipour*

- `2112.13207v1` - [abs](http://arxiv.org/abs/2112.13207v1) - [pdf](http://arxiv.org/pdf/2112.13207v1)

> Software fuzzing is a strong testing technique that has become the de facto approach for automated software testing and software vulnerability detection in the industry. The random nature of fuzzing makes monitoring and understanding the behavior of fuzzers difficult. In this paper, we report the development of Fuzzer Mutation Visualizer (FMViz), a tool that focuses on visualizing byte-level mutations in fuzzers. In particular, FMViz extends American Fuzzy Lop (AFL) to visualize the generated test inputs and highlight changes between consecutively generated seeds as a fuzzing campaign progresses. The overarching goal of our tool is to help developers and students comprehend the inner-workings of the AFL fuzzer better. In this paper, we present the architecture of FMViz, discuss a sample case study of it, and outline the future work. FMViz is open-source and publicly available at https://github.com/AftabHussain/afl-test-viz.

</details>

<details>

<summary>2021-12-25 23:33:44 - Defending Against Membership Inference Attacks on Beacon Services</summary>

- *Rajagopal Venkatesaramani, Zhiyu Wan, Bradley A. Malin, Yevgeniy Vorobeychik*

- `2112.13301v1` - [abs](http://arxiv.org/abs/2112.13301v1) - [pdf](http://arxiv.org/pdf/2112.13301v1)

> Large genomic datasets are now created through numerous activities, including recreational genealogical investigations, biomedical research, and clinical care. At the same time, genomic data has become valuable for reuse beyond their initial point of collection, but privacy concerns often hinder access. Over the past several years, Beacon services have emerged to broaden accessibility to such data. These services enable users to query for the presence of a particular minor allele in a private dataset, information that can help care providers determine if genomic variation is spurious or has some known clinical indication. However, various studies have shown that even this limited access model can leak if individuals are members in the underlying dataset. Several approaches for mitigating this vulnerability have been proposed, but they are limited in that they 1) typically rely on heuristics and 2) offer probabilistic privacy guarantees, but neglect utility. In this paper, we present a novel algorithmic framework to ensure privacy in a Beacon service setting with a minimal number of query response flips (e.g., changing a positive response to a negative). Specifically, we represent this problem as combinatorial optimization in both the batch setting (where queries arrive all at once), as well as the online setting (where queries arrive sequentially). The former setting has been the primary focus in prior literature, whereas real Beacons allow sequential queries, motivating the latter investigation. We present principled algorithms in this framework with both privacy and, in some cases, worst-case utility guarantees. Moreover, through an extensive experimental evaluation, we show that the proposed approaches significantly outperform the state of the art in terms of privacy and utility.

</details>

<details>

<summary>2021-12-26 12:34:19 - A Large-Scale Security-Oriented Static Analysis of Python Packages in PyPI</summary>

- *Jukka Ruohonen, Kalle Hjerppe, Kalle Rindell*

- `2107.12699v2` - [abs](http://arxiv.org/abs/2107.12699v2) - [pdf](http://arxiv.org/pdf/2107.12699v2)

> Different security issues are a common problem for open source packages archived to and delivered through software ecosystems. These often manifest themselves as software weaknesses that may lead to concrete software vulnerabilities. This paper examines various security issues in Python packages with static analysis. The dataset is based on a snapshot of all packages stored to the Python Package Index (PyPI). In total, over 197 thousand packages and over 749 thousand security issues are covered. Even under the constraints imposed by static analysis, (a) the results indicate prevalence of security issues; at least one issue is present for about 46% of the Python packages. In terms of the issue types, (b) exception handling and different code injections have been the most common issues. The subprocess module stands out in this regard. Reflecting the generally small size of the packages, (c) software size metrics do not predict well the amount of issues revealed through static analysis. With these results and the accompanying discussion, the paper contributes to the field of large-scale empirical studies for better understanding security problems in software ecosystems.

</details>

<details>

<summary>2021-12-26 14:55:54 - Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs</summary>

- *Chen Chen, Xuanli He, Lingjuan Lyu, Fangzhao Wu*

- `2105.10909v2` - [abs](http://arxiv.org/abs/2105.10909v2) - [pdf](http://arxiv.org/pdf/2105.10909v2)

> The collection and availability of big data, combined with advances in pre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive performance of modern natural language processing tasks, ranging from text classification to text generation. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. However, BERT-based APIs have exhibited a series of security and privacy vulnerabilities. For example, prior work has exploited the security issues of the BERT-based APIs through the adversarial examples crafted by the extracted model. However, the privacy leakage problems of the BERT-based APIs through the extracted model have not been well studied. On the other hand, due to the high capacity of BERT-based APIs, the fine-tuned model is easy to be overlearned, but what kind of information can be leaked from the extracted model remains unknown. In this work, we bridge this gap by first presenting an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries. We further develop an effective attribute inference attack which can infer the sensitive attribute of the training data used by the BERT-based APIs. Our extensive experiments on benchmark datasets under various realistic settings validate the potential vulnerabilities of BERT-based APIs. Moreover, we demonstrate that two promising defense methods become ineffective against our attacks, which calls for more effective defense methods.

</details>

<details>

<summary>2021-12-27 03:50:41 - Evaluating Software User Feedback Classifiers on Unseen Apps, Datasets, and Metadata</summary>

- *Peter Devine, Yun Sing Koh, Kelly Blincoe*

- `2112.13497v1` - [abs](http://arxiv.org/abs/2112.13497v1) - [pdf](http://arxiv.org/pdf/2112.13497v1)

> Listening to user's requirements is crucial to building and maintaining high quality software. Online software user feedback has been shown to contain large amounts of information useful to requirements engineering (RE). Previous studies have created machine learning classifiers for parsing this feedback for development insight. While these classifiers report generally good performance when evaluated on a test set, questions remain as to how well they extend to unseen data in various forms.   This study evaluates machine learning classifiers performance on feedback for two common classification tasks (classifying bug reports and feature requests). Using seven datasets from prior research studies, we investigate the performance of classifiers when evaluated on feedback from different apps than those contained in the training set and when evaluated on completely different datasets (coming from different feedback platforms and/or labelled by different researchers). We also measure the difference in performance of using platform-specific metadata as a feature in classification.   We demonstrate that classification performance is similar on feedback from unseen apps compared to seen apps in the majority of cases tested. However, the classifiers do not perform well on unseen datasets. We show that multi-dataset training or zero shot classification approaches can somewhat mitigate this performance decrease. Finally, we find that using metadata as features in classifying bug reports and feature requests does not lead to a statistically significant improvement in the majority of datasets tested. We discuss the implications of these results on developing user feedback classification models to analyse and extract software requirements.

</details>

<details>

<summary>2021-12-27 06:23:43 - Adversarial Attack for Asynchronous Event-based Data</summary>

- *Wooju Lee, Hyun Myung*

- `2112.13534v1` - [abs](http://arxiv.org/abs/2112.13534v1) - [pdf](http://arxiv.org/pdf/2112.13534v1)

> Deep neural networks (DNNs) are vulnerable to adversarial examples that are carefully designed to cause the deep learning model to make mistakes. Adversarial examples of 2D images and 3D point clouds have been extensively studied, but studies on event-based data are limited. Event-based data can be an alternative to a 2D image under high-speed movements, such as autonomous driving. However, the given adversarial events make the current deep learning model vulnerable to safety issues. In this work, we generate adversarial examples and then train the robust models for event-based data, for the first time. Our algorithm shifts the time of the original events and generates additional adversarial events. Additional adversarial events are generated in two stages. First, null events are added to the event-based data to generate additional adversarial events. The perturbation size can be controlled with the number of null events. Second, the location and time of additional adversarial events are set to mislead DNNs in a gradient-based attack. Our algorithm achieves an attack success rate of 97.95\% on the N-Caltech101 dataset. Furthermore, the adversarial training model improves robustness on the adversarial event data compared to the original model.

</details>

<details>

<summary>2021-12-27 09:47:47 - Swarm consensus</summary>

- *Victor Grishchenko, Mikhail Patrakeev, S. Q. Locke III*

- `2112.07065v4` - [abs](http://arxiv.org/abs/2112.07065v4) - [pdf](http://arxiv.org/pdf/2112.07065v4)

> The strength of gnomes lies in their coordinated action. Being small and subtle creatures themselves, the forest gnomes can form large swarms acting as one giant creature. This unusual defense strategy requires a lot of skill and training. Directing a swarm is not an easy task! Initially, gnomes used leader-based control algorithms, although those have been proven to be vulnerable to abuse and failure.   After thorough research and study, gnomes developed their own leaderless consensus algorithm based on very simple rules. It is based on gossip in a network of a known diameter $d$. One of the gnomes proposes a plan which then spreads gnome to gnome. If there is an agreement, gnomes act \emph{all at once}. If there are conflicting plans (an extreme rarity), they try again. The resulting upper bound on the swarm's reaction time is its round-trip time $2dt$, where $t$ is the command relay time. The original algorithm is non-Byzantine; all gnomes must be sane and sober.   While working on the algorithm, gnomes discovered \emph{swarm time}, a sibling concept to L.Lamport's logical time. That led to a Byzantine-ready version of the algorithm.

</details>

<details>

<summary>2021-12-28 16:32:30 - Understanding and Measuring Robustness of Multimodal Learning</summary>

- *Nishant Vishwamitra, Hongxin Hu, Ziming Zhao, Long Cheng, Feng Luo*

- `2112.12792v2` - [abs](http://arxiv.org/abs/2112.12792v2) - [pdf](http://arxiv.org/pdf/2112.12792v2)

> The modern digital world is increasingly becoming multimodal. Although multimodal learning has recently revolutionized the state-of-the-art performance in multimodal tasks, relatively little is known about the robustness of multimodal learning in an adversarial setting. In this paper, we introduce a comprehensive measurement of the adversarial robustness of multimodal learning by focusing on the fusion of input modalities in multimodal models, via a framework called MUROAN (MUltimodal RObustness ANalyzer). We first present a unified view of multimodal models in MUROAN and identify the fusion mechanism of multimodal models as a key vulnerability. We then introduce a new type of multimodal adversarial attacks called decoupling attack in MUROAN that aims to compromise multimodal models by decoupling their fused modalities. We leverage the decoupling attack of MUROAN to measure several state-of-the-art multimodal models and find that the multimodal fusion mechanism in all these models is vulnerable to decoupling attacks. We especially demonstrate that, in the worst case, the decoupling attack of MUROAN achieves an attack success rate of 100% by decoupling just 1.16% of the input space. Finally, we show that traditional adversarial training is insufficient to improve the robustness of multimodal models with respect to decoupling attacks. We hope our findings encourage researchers to pursue improving the robustness of multimodal learning.

</details>

<details>

<summary>2021-12-29 00:35:41 - Super-Efficient Super Resolution for Fast Adversarial Defense at the Edge</summary>

- *Kartikeya Bhardwaj, Dibakar Gope, James Ward, Paul Whatmough, Danny Loh*

- `2112.14340v1` - [abs](http://arxiv.org/abs/2112.14340v1) - [pdf](http://arxiv.org/pdf/2112.14340v1)

> Autonomous systems are highly vulnerable to a variety of adversarial attacks on Deep Neural Networks (DNNs). Training-free model-agnostic defenses have recently gained popularity due to their speed, ease of deployment, and ability to work across many DNNs. To this end, a new technique has emerged for mitigating attacks on image classification DNNs, namely, preprocessing adversarial images using super resolution -- upscaling low-quality inputs into high-resolution images. This defense requires running both image classifiers and super resolution models on constrained autonomous systems. However, super resolution incurs a heavy computational cost. Therefore, in this paper, we investigate the following question: Does the robustness of image classifiers suffer if we use tiny super resolution models? To answer this, we first review a recent work called Super-Efficient Super Resolution (SESR) that achieves similar or better image quality than prior art while requiring 2x to 330x fewer Multiply-Accumulate (MAC) operations. We demonstrate that despite being orders of magnitude smaller than existing models, SESR achieves the same level of robustness as significantly larger networks. Finally, we estimate end-to-end performance of super resolution-based defenses on a commercial Arm Ethos-U55 micro-NPU. Our findings show that SESR achieves nearly 3x higher FPS than a baseline while achieving similar robustness.

</details>

<details>

<summary>2021-12-29 16:11:04 - IoT Security Challenges and Mitigations: An Introduction</summary>

- *Stuart Millar*

- `2112.14618v1` - [abs](http://arxiv.org/abs/2112.14618v1) - [pdf](http://arxiv.org/pdf/2112.14618v1)

> The use of IoT in society is perhaps already ubiquitous, with a vast attack surface offering multiple opportunities for malicious actors. This short paper first presents an introduction to IoT and its security issues, including an overview of IoT layer models and topologies, IoT standardisation efforts and protocols. The focus then moves to IoT vulnerabilities and specific suggestions for mitigations. This work's intended audience are those relatively new to IoT though with existing network-related knowledge. It is concluded that device resource constraints and a lack of IoT standards are significant issues. Research opportunities exist to develop efficient IoT IDS and energy-saving cryptography techniques lightweight enough to reasonably deploy. The need for standardised protocols and channel-based security solutions is clear, underpinned by legislative directives to ensure high standards that prevent cost-cutting on the device manufacturing side.

</details>

<details>

<summary>2021-12-31 14:39:40 - REST API Fuzzing by Coverage Level Guided Blackbox Testing</summary>

- *Chung-Hsuan Tsai, Shi-Chun Tsai, Shih-Kun Huang*

- `2112.15485v1` - [abs](http://arxiv.org/abs/2112.15485v1) - [pdf](http://arxiv.org/pdf/2112.15485v1)

> With the growth of web applications, REST APIs have become the primary communication method between services. In order to ensure system reliability and security, software quality can be assured by effective testing methods. Black box fuzz testing is one of the effective methods to perform tests on a large scale. However, conventional black box fuzz testing generates random data without judging the quality of the input.   We implement a black box fuzz testing method for REST APIs. It resolves the issues of blind mutations without knowing the effectiveness by Test Coverage Level feedback. We also enhance the mutation strategies by reducing the testing complexity for REST APIs, generating more appropriate test cases to cover possible paths.   We evaluate our method by testing two large open-source projects and 89 bugs are reported and confirmed. In addition, we find 351 bugs from 64 remote API services in APIs.guru.   The work is in https://github.com/iasthc/hsuan-fuzz.

</details>

<details>

<summary>2021-12-31 17:32:59 - SOK: On the Analysis of Web Browser Security</summary>

- *Jungwon Lim, Yonghwi Jin, Mansour Alharthi, Xiaokuan Zhang, Jinho Jung, Rajat Gupta, Kuilin Li, Daehee Jang, Taesoo Kim*

- `2112.15561v1` - [abs](http://arxiv.org/abs/2112.15561v1) - [pdf](http://arxiv.org/pdf/2112.15561v1)

> Web browsers are integral parts of everyone's daily life. They are commonly used for security-critical and privacy sensitive tasks, like banking transactions and checking medical records. Unfortunately, modern web browsers are too complex to be bug free (e.g., 25 million lines of code in Chrome), and their role as an interface to the cyberspace makes them an attractive target for attacks. Accordingly, web browsers naturally become an arena for demonstrating advanced exploitation techniques by attackers and state-of-the-art defenses by browser vendors. Web browsers, arguably, are the most exciting place to learn the latest security issues and techniques, but remain as a black art to most security researchers because of their fast-changing characteristics and complex code bases.   To bridge this gap, this paper attempts to systematize the security landscape of modern web browsers by studying the popular classes of security bugs, their exploitation techniques, and deployed defenses. More specifically, we first introduce a unified architecture that faithfully represents the security design of four major web browsers. Second, we share insights from a 10-year longitudinal study on browser bugs. Third, we present a timeline and context of mitigation schemes and their effectiveness. Fourth, we share our lessons from a full-chain exploit used in 2020 Pwn2Own competition. and the implication of bug bounty programs to web browser security. We believe that the key takeaways from this systematization can shed light on how to advance the status quo of modern web browsers, and, importantly, how to create secure yet complex software in the future.

</details>

<details>

<summary>2021-12-31 18:43:46 - Refining Language Models with Compositional Explanations</summary>

- *Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, Xiang Ren*

- `2103.10415v3` - [abs](http://arxiv.org/abs/2103.10415v3) - [pdf](http://arxiv.org/pdf/2103.10415v3)

> Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences in a new domain. Prior work reveals such spurious patterns via post-hoc explanation algorithms which compute the importance of input features. Further, the model is regularized to align the importance scores with human knowledge, so that the unintended model behaviors are eliminated. However, such a regularization technique lacks flexibility and coverage, since only importance scores towards a pre-defined list of features are adjusted, while more complex human knowledge such as feature interaction and pattern generalization can hardly be incorporated. In this work, we propose to refine a learned language model for a target domain by collecting human-provided compositional explanations regarding observed biases. By parsing these explanations into executable logic rules, the human-specified refinement advice from a small set of explanations can be generalized to more training examples. We additionally introduce a regularization term allowing adjustments for both importance and interaction of features to better rectify model behavior. We demonstrate the effectiveness of the proposed approach on two text classification tasks by showing improved performance in target domain as well as improved model fairness after refinement.

</details>

