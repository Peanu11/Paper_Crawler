# 2020

## TOC

- [2020-01](#2020-01)
- [2020-02](#2020-02)
- [2020-03](#2020-03)
- [2020-04](#2020-04)
- [2020-05](#2020-05)
- [2020-06](#2020-06)
- [2020-07](#2020-07)
- [2020-08](#2020-08)
- [2020-09](#2020-09)
- [2020-10](#2020-10)
- [2020-11](#2020-11)
- [2020-12](#2020-12)

## 2020-01

<details>

<summary>2020-01-02 19:34:58 - Detecting Areas of Potential High Prevalence of Chagas in Argentina</summary>

- *Antonio Vazquez Brust, Tomas Olego, German Rosati, Carolina Lang, Guillermo Bozzoli, Diego Weinberg, Roberto Chuit, Martin A. Minnoni, Carlos Sarraute*

- `2001.00604v1` - [abs](http://arxiv.org/abs/2001.00604v1) - [pdf](http://arxiv.org/pdf/2001.00604v1)

> A map of potential prevalence of Chagas disease (ChD) with high spatial disaggregation is presented. It aims to detect areas outside the Gran Chaco ecoregion (hyperendemic for the ChD), characterized by high affinity with ChD and high health vulnerability.   To quantify potential prevalence, we developed several indicators: an Affinity Index which quantifies the degree of linkage between endemic areas of ChD and the rest of the country. We also studied favorable habitability conditions for Triatoma infestans, looking for areas where the predominant materials of floors, roofs and internal ceilings favor the presence of the disease vector.   We studied determinants of a more general nature that can be encompassed under the concept of Health Vulnerability Index. These determinants are associated with access to health providers and the socio-economic level of different segments of the population.   Finally we constructed a Chagas Potential Prevalence Index (ChPPI) which combines the affinity index, the health vulnerability index, and the population density. We show and discuss the maps obtained. These maps are intended to assist public health specialists, decision makers of public health policies and public officials in the development of cost-effective strategies to improve access to diagnosis and treatment of ChD.

</details>

<details>

<summary>2020-01-03 07:12:54 - Improving PKI, BGP, and DNS Using Blockchain: A Systematic Review</summary>

- *Faizan Safdar Ali, Alptekin Kupcu*

- `2001.00747v1` - [abs](http://arxiv.org/abs/2001.00747v1) - [pdf](http://arxiv.org/pdf/2001.00747v1)

> The Internet has many backbone components on top of which the whole world is connected. It is important to make these components, like Border Gateway Protocol (BGP), Domain Name System (DNS), and Public Key Infrastructure (PKI), secure and work without any interruption. All of the aforementioned components have vulnerabilities, mainly because of their dependence on the centralized parties, that should be resolved.   Blockchain is revolutionizing the concept of today's Internet, primarily because of its degree of decentralization and security properties. In this paper, we discuss how blockchain provides nearly complete solutions to the open challenges for these network backbone components.

</details>

<details>

<summary>2020-01-03 17:55:15 - Towards a framework for understanding societal and ethical implications of Artificial Intelligence</summary>

- *Richard Benjamins, Idoia Salazar*

- `2001.09750v1` - [abs](http://arxiv.org/abs/2001.09750v1) - [pdf](http://arxiv.org/pdf/2001.09750v1)

> Artificial Intelligence (AI) is one of the most discussed technologies today. There are many innovative applications such as the diagnosis and treatment of cancer, customer experience, new business, education, contagious diseases propagation and optimization of the management of humanitarian catastrophes. However, with all those opportunities also comes great responsibility to ensure good and fair practice of AI. The objective of this paper is to identify the main societal and ethical challenges implied by a massive uptake of AI. We have surveyed the literature for the most common challenges and classified them in seven groups: 1) Non-desired effects, 2) Liability, 3) Unknown consequences, 4) Relation people-robots, 5) Concentration of power and wealth, 6) Intentional bad uses, and 7) AI for weapons and warfare. The challenges should be dealt with in different ways depending on their origin; some have technological solutions, while others require ethical, societal, or political answers. Depending on the origin, different stakeholders might need to act. Whatever the identified stakeholder, not treating those issues will lead to uncertainty and unforeseen consequences with potentially large negative societal impact, hurting especially the most vulnerable groups of societies. Technology is helping to take better decisions, and AI is promoting data-driven decisions in addition to experience- and intuition-based discussion, with many improvements happening. However, the negative side effects of this technology need to be well understood and acted upon before we launch them massively into the world.

</details>

<details>

<summary>2020-01-04 15:17:11 - Secure Communication Protocol for Smart Transportation Based on Vehicular Cloud</summary>

- *Trupil Limbasiya, Debasis Das, Sanjay K. Sahay*

- `1912.12884v2` - [abs](http://arxiv.org/abs/1912.12884v2) - [pdf](http://arxiv.org/pdf/1912.12884v2)

> The pioneering concept of connected vehicles has transformed the way of thinking for researchers and entrepreneurs by collecting relevant data from nearby objects. However, this data is useful for a specific vehicle only. Moreover, vehicles get a high amount of data (e.g., traffic, safety, and multimedia infotainment) on the road. Thus, vehicles expect adequate storage devices for this data, but it is infeasible to have a large memory in each vehicle. Hence, the vehicular cloud computing (VCC) framework came into the picture to provide a storage facility by connecting a road-side-unit (RSU) with the vehicular cloud (VC). In this, data should be saved in an encrypted form to preserve security, but there is a challenge to search for information over encrypted data. Next, we understand that many of vehicular communication schemes are inefficient for data transmissions due to its poor performance results and vulnerable to different fundamental security attacks. Accordingly, on-device performance is critical, but data damages and secure on-time connectivity are also significant challenges in a public environment. Therefore, we propose reliable data transmission protocols for cutting-edge architecture to search data from the storage, to resist against various security attacks, and provide better performance results. Thus, the proposed data transmission protocol is useful in diverse smart city applications (business, safety, and entertainment) for the benefits of society.

</details>

<details>

<summary>2020-01-04 21:05:15 - Locality-Sensitive Hashing for Efficient Web Application Security Testing</summary>

- *Ilan Ben-Bassat, Erez Rokah*

- `2001.01128v1` - [abs](http://arxiv.org/abs/2001.01128v1) - [pdf](http://arxiv.org/pdf/2001.01128v1)

> Web application security has become a major concern in recent years, as more and more content and services are available online. A useful method for identifying security vulnerabilities is black-box testing, which relies on an automated crawling of web applications. However, crawling Rich Internet Applications (RIAs) is a very challenging task. One of the key obstacles crawlers face is the state similarity problem: how to determine if two client-side states are equivalent. As current methods do not completely solve this problem, a successful scan of many real-world RIAs is still not possible. We present a novel approach to detect redundant content for security testing purposes. The algorithm applies locality-sensitive hashing using MinHash sketches in order to analyze the Document Object Model (DOM) structure of web pages, and to efficiently estimate similarity between them. Our experimental results show that this approach allows a successful scan of RIAs that cannot be crawled otherwise.

</details>

<details>

<summary>2020-01-06 10:28:26 - Fast-UAP: An Algorithm for Speeding up Universal Adversarial Perturbation Generation with Orientation of Perturbation Vectors</summary>

- *Jiazhu Dai, Le Shu*

- `1911.01172v3` - [abs](http://arxiv.org/abs/1911.01172v3) - [pdf](http://arxiv.org/pdf/1911.01172v3)

> Convolutional neural networks (CNN) have become one of the most popular machine learning tools and are being applied in various tasks, however, CNN models are vulnerable to universal perturbations, which are usually human-imperceptible but can cause natural images to be misclassified with high probability. One of the state-of-the-art algorithms to generate universal perturbations is known as UAP. UAP only aggregates the minimal perturbations in every iteration, which will lead to generated universal perturbation whose magnitude cannot rise up efficiently and cause a slow generation. In this paper, we proposed an optimized algorithm to improve the performance of crafting universal perturbations based on orientation of perturbation vectors. At each iteration, instead of choosing minimal perturbation vector with respect to each image, we aggregate the current instance of universal perturbation with the perturbation which has similar orientation to the former so that the magnitude of the aggregation will rise up as large as possible at every iteration. The experiment results show that we get universal perturbations in a shorter time and with a smaller number of training images. Furthermore, we observe in experiments that universal perturbations generated by our proposed algorithm have an average increment of fooling rate by 9% in white-box attacks and black-box attacks comparing with universal perturbations generated by UAP.

</details>

<details>

<summary>2020-01-07 00:39:36 - Protect Edge Privacy in Path Publishing with Differential Privacy</summary>

- *Zhigang Lu, Hong Shen*

- `2001.01825v1` - [abs](http://arxiv.org/abs/2001.01825v1) - [pdf](http://arxiv.org/pdf/2001.01825v1)

> Paths in a given network are a generalised form of time-serial chains in many real-world applications, such as trajectories and Internet flows. Differentially private trajectory publishing concerns publishing path information that is usable to the genuine users yet secure against adversaries to reconstruct the path with maximum background knowledge. The exiting studies all assume this knowledge to be all but one vertex on the path. To prevent the adversaries recovering the missing information, they publish a perturbed path where each vertex is sampled from a pre-defined set with differential privacy (DP) to replace the corresponding vertex in the original path. In this paper, we relax this assumption to be all but one edge on the path, and hence consider the scenario of more powerful adversaries with the maximum background knowledge of the entire network topology and the path (including all the vertices) except one (arbitrary) missing edge. Under such an assumption, the perturbed path produced by the existing work is vulnerable, because the adversary can reconstruct the missing edge from the existence of an edge in the perturbed path. To address this vulnerability and effectively protect edge privacy, instead of publishing a perturbed path, we propose a novel scheme of graph-based path publishing to protect the original path by embedding the path in a graph that contains fake edges and replicated vertices applying the differential privacy technique, such that only the legitimate users who have the full knowledge of the network topology are able to recover the exact vertices and edges of the original path with high probability. We theoretically analyse the performance of our algorithm in differential privacy, utility, and execution efficiency. We also conduct extensive experimental evaluations on a high-performance cluster system to validate our analytical results.

</details>

<details>

<summary>2020-01-07 09:39:38 - A fine-grained policy model for Provenance-based Access Control and Policy Algebras.pdf</summary>

- *Xinyu Fan, Faen Zhang, Jianfei Song, Jingming Guo, Fujie Gao*

- `2001.01945v1` - [abs](http://arxiv.org/abs/2001.01945v1) - [pdf](http://arxiv.org/pdf/2001.01945v1)

> A fine-grained provenance-based access control policy model is proposed in this paper, in order to improve the express performance of existing model. This method employs provenance as conditions to determine whether a piece of data can be accessed because historical operations performed on data could reveal clues about its sensitivity and vulnerability. Particularly, our proposed work provides a four-valued decision set which allows showing status to match a restriction particularly. This framework consists of target policy, access control policy, and policy algebras. With the complete definition and algebra system construction, a practical fine-grained access control policy model is developed.

</details>

<details>

<summary>2020-01-07 13:19:04 - Missing-Class-Robust Domain Adaptation by Unilateral Alignment for Fault Diagnosis</summary>

- *Qin Wang, Gabriel Michau, Olga Fink*

- `2001.02015v1` - [abs](http://arxiv.org/abs/2001.02015v1) - [pdf](http://arxiv.org/pdf/2001.02015v1)

> Domain adaptation aims at improving model performance by leveraging the learned knowledge in the source domain and transferring it to the target domain. Recently, domain adversarial methods have been particularly successful in alleviating the distribution shift between the source and the target domains. However, these methods assume an identical label space between the two domains. This assumption imposes a significant limitation for real applications since the target training set may not contain the complete set of classes. We demonstrate in this paper that the performance of domain adversarial methods can be vulnerable to an incomplete target label space during training. To overcome this issue, we propose a two-stage unilateral alignment approach. The proposed methodology makes use of the inter-class relationships of the source domain and aligns unilaterally the target to the source domain. The benefits of the proposed methodology are first evaluated on the MNIST$\rightarrow$MNIST-M adaptation task. The proposed methodology is also evaluated on a fault diagnosis task, where the problem of missing fault types in the target training dataset is common in practice. Both experiments demonstrate the effectiveness of the proposed methodology.

</details>

<details>

<summary>2020-01-07 22:15:33 - Quantum-Secure Microgrid</summary>

- *Zefan Tang, Yanyuan Qin, Zimin Jiang, Walter O. Krawec, Peng Zhang*

- `2001.02301v1` - [abs](http://arxiv.org/abs/2001.02301v1) - [pdf](http://arxiv.org/pdf/2001.02301v1)

> Existing microgrid communication relies on classical public key systems, which are vulnerable to attacks from quantum computers. This paper uses quantum key distribution (QKD) to solve these quantum-era microgrid challenges. Specifically, this paper makes the following novel contributions: 1) it offers a QKD-based microgrid communication architecture for microgrids; 2) it shows how to build a quantum-secure microgrid testbed in an RTDS environment; 3) it develops a key pool sharing (KPS) strategy to improve the cyberattack resilience of the QKD-based microgrid; and 4) it analyzes the impacts of critical QKD parameters with the testbed. Test results provide insightful resources for building a quantum-secure microgrid.

</details>

<details>

<summary>2020-01-08 01:47:22 - $μ$VulDeePecker: A Deep Learning-Based System for Multiclass Vulnerability Detection</summary>

- *Deqing Zou, Sujuan Wang, Shouhuai Xu, Zhen Li, Hai Jin*

- `2001.02334v1` - [abs](http://arxiv.org/abs/2001.02334v1) - [pdf](http://arxiv.org/pdf/2001.02334v1)

> Fine-grained software vulnerability detection is an important and challenging problem. Ideally, a detection system (or detector) not only should be able to detect whether or not a program contains vulnerabilities, but also should be able to pinpoint the type of a vulnerability in question. Existing vulnerability detection methods based on deep learning can detect the presence of vulnerabilities (i.e., addressing the binary classification or detection problem), but cannot pinpoint types of vulnerabilities (i.e., incapable of addressing multiclass classification). In this paper, we propose the first deep learning-based system for multiclass vulnerability detection, dubbed $\mu$VulDeePecker. The key insight underlying $\mu$VulDeePecker is the concept of code attention, which can capture information that can help pinpoint types of vulnerabilities, even when the samples are small. For this purpose, we create a dataset from scratch and use it to evaluate the effectiveness of $\mu$VulDeePecker. Experimental results show that $\mu$VulDeePecker is effective for multiclass vulnerability detection and that accommodating control-dependence (other than data-dependence) can lead to higher detection capabilities.

</details>

<details>

<summary>2020-01-08 03:41:27 - Blockchain-based Smart-IoT Trust Zone Measurement Architecture</summary>

- *Jawad Ali, Toqeer Ali, Yazed Alsaawy, Ahmad Shahrafidz Khalid, Shahrulniza Musa*

- `2001.03002v1` - [abs](http://arxiv.org/abs/2001.03002v1) - [pdf](http://arxiv.org/pdf/2001.03002v1)

> With a rapid growth in the IT industry, Internet of Things (IoT) has gained a tremendous attention and become a central aspect of our environment. In IoT the things (devices) communicate and exchange the data without the act of human intervention. Such autonomy and proliferation of IoT ecosystem make the devices more vulnerable to attacks. In this paper, we propose a behavior monitor in IoT-Blockchain setup which can provide trust-confidence to outside networks. Behavior monitor extracts the activity of each device and analyzes the behavior using deep auto-encoders. In addition, we also incorporate Trusted Execution Technology (Intel SGX) in order to provide a secure execution environment for applications and data on blockchain. Finally, in evaluation we analyze three IoT devices data that is infected by mirai attack. The evaluation results demonstrate the ability of our proposed method in terms of accuracy and time required for detection.

</details>

<details>

<summary>2020-01-08 15:17:38 - Watching the Weak Link into Your Home: An Inspection and Monitoring Toolkit for TR-069</summary>

- *Maximilian Hils, Rainer Böhme*

- `2001.02564v1` - [abs](http://arxiv.org/abs/2001.02564v1) - [pdf](http://arxiv.org/pdf/2001.02564v1)

> TR-069 is a standard for the remote management of end-user devices by service providers. Despite being implemented in nearly a billion devices, almost no research has been published on the security and privacy aspects of TR-069. The first contribution of this paper is a study of the TR-069 ecosystem and techniques to inspect TR-069 communication. We find that the majority of analyzed providers do not use recommended security measures, such as TLS. Second, we present a TR-069 honeyclient to both analyze TR-069 behavior of providers and test configuration servers for security vulnerabilities. We find that popular open-source configuration servers use insecure methods to authenticate clients. TR-069 implementations based on these servers expose, for instance, their users' internet telephony credentials. Third, we develop components for a distributed system to continuously monitor activities in providers' TR-069 deployments. Our setup consists of inexpensive hardware sensors deployed on customer premises and centralized log collectors. We perform real-world measurements and find that the purported security benefits of TR-069 are not realized as providers' firmware update processes are lacking.

</details>

<details>

<summary>2020-01-09 06:03:09 - Cloud-based Image Classification Service Is Not Robust To Simple Transformations: A Forgotten Battlefield</summary>

- *Dou Goodman, Tao Wei*

- `1906.07997v2` - [abs](http://arxiv.org/abs/1906.07997v2) - [pdf](http://arxiv.org/pdf/1906.07997v2)

> Many recent works demonstrated that Deep Learning models are vulnerable to adversarial examples.Fortunately, generating adversarial examples usually requires white-box access to the victim model, and the attacker can only access the APIs opened by cloud platforms. Thus, keeping models in the cloud can usually give a (false) sense of security.Unfortunately, cloud-based image classification service is not robust to simple transformations such as Gaussian Noise, Salt-and-Pepper Noise, Rotation and Monochromatization. In this paper,(1) we propose one novel attack method called Image Fusion(IF) attack, which achieve a high bypass rate,can be implemented only with OpenCV and is difficult to defend; and (2) we make the first attempt to conduct an extensive empirical study of Simple Transformation (ST) attacks against real-world cloud-based classification services. Through evaluations on four popular cloud platforms including Amazon, Google, Microsoft, Clarifai, we demonstrate that ST attack has a success rate of approximately 100% except Amazon approximately 50%, IF attack have a success rate over 98% among different classification services. (3) We discuss the possible defenses to address these security challenges.Experiments show that our defense technology can effectively defend known ST attacks.

</details>

<details>

<summary>2020-01-09 11:30:50 - Architecture and Security of SCADA Systems: A Review</summary>

- *Geeta Yadav, Kolin Paul*

- `2001.02925v1` - [abs](http://arxiv.org/abs/2001.02925v1) - [pdf](http://arxiv.org/pdf/2001.02925v1)

> Pipeline bursting, production lines shut down, frenzy traffic, trains confrontation, nuclear reactor shut down, disrupted electric supply, interrupted oxygen supply in ICU - these catastrophic events could result because of an erroneous SCADA system/ Industrial Control System(ICS). SCADA systems have become an essential part of automated control and monitoring of many of the Critical Infrastructures (CI). Modern SCADA systems have evolved from standalone systems into sophisticated complex, open systems, connected to the Internet. This geographically distributed modern SCADA system is vulnerable to threats and cyber attacks. In this paper, we first review the SCADA system architectures that have been proposed/implemented followed by attacks on such systems to understand and highlight the evolving security needs for SCADA systems. A short investigation of the current state of intrusion detection techniques in SCADA systems is done , followed by a brief study of testbeds for SCADA systems. The cloud and Internet of things (IoT) based SCADA systems are studied by analysing the architecture of modern SCADA systems. This review paper ends by highlighting the critical research problems that need to be resolved to close the gaps in the security of SCADA systems.

</details>

<details>

<summary>2020-01-09 23:24:21 - The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?</summary>

- *Toby Shevlane, Allan Dafoe*

- `2001.00463v2` - [abs](http://arxiv.org/abs/2001.00463v2) - [pdf](http://arxiv.org/pdf/2001.00463v2)

> There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.

</details>

<details>

<summary>2020-01-10 05:12:22 - Guess First to Enable Better Compression and Adversarial Robustness</summary>

- *Sicheng Zhu, Bang An, Shiyu Niu*

- `2001.03311v1` - [abs](http://arxiv.org/abs/2001.03311v1) - [pdf](http://arxiv.org/pdf/2001.03311v1)

> Machine learning models are generally vulnerable to adversarial examples, which is in contrast to the robustness of humans. In this paper, we try to leverage one of the mechanisms in human recognition and propose a bio-inspired classification framework in which model inference is conditioned on label hypothesis. We provide a class of training objectives for this framework and an information bottleneck regularizer which utilizes the advantage that label information can be discarded during inference. This framework enables better compression of the mutual information between inputs and latent representations without loss of learning capacity, at the cost of tractable inference complexity. Better compression and elimination of label information further bring better adversarial robustness without loss of natural accuracy, which is demonstrated in the experiment.

</details>

<details>

<summary>2020-01-10 16:53:38 - What are the Actual Flaws in Important Smart Contracts (and How Can We Find Them)?</summary>

- *Alex Groce, Josselin Feist, Gustavo Grieco, Michael Colburn*

- `1911.07567v2` - [abs](http://arxiv.org/abs/1911.07567v2) - [pdf](http://arxiv.org/pdf/1911.07567v2)

> An important problem in smart contract security is understanding the likelihood and criticality of discovered, or potential, weaknesses in contracts. In this paper we provide a summary of Ethereum smart contract audits performed for 23 professional stakeholders, avoiding the common problem of reporting issues mostly prevalent in low-quality contracts. These audits were performed at a leading company in blockchain security, using both open-source and proprietary tools, as well as human code analysis performed by professional security engineers. We categorize 246 individual defects, making it possible to compare the severity and frequency of different vulnerability types, compare smart contract and non-smart contract flaws, and to estimate the efficacy of automated vulnerability detection approaches.

</details>

<details>

<summary>2020-01-13 02:15:15 - Heterogeneous network approach to predict individuals' mental health</summary>

- *Shikang Liu, Fatemeh Vahedian, David Hachen, Omar Lizardo, Christian Poellabauer, Aaron Striegel, Tijana Milenkovic*

- `1906.04346v2` - [abs](http://arxiv.org/abs/1906.04346v2) - [pdf](http://arxiv.org/pdf/1906.04346v2)

> Depression and anxiety are critical public health issues affecting millions of people around the world. To identify individuals who are vulnerable to depression and anxiety, predictive models have been built that typically utilize data from one source. Unlike these traditional models, in this study, we leverage a rich heterogeneous data set from the University of Notre Dame's NetHealth study that collected individuals' (student participants') social interaction data via smartphones, health-related behavioral data via wearables (Fitbit), and trait data from surveys. To integrate the different types of information, we model the NetHealth data as a heterogeneous information network (HIN). Then, we redefine the problem of predicting individuals' mental health conditions (depression or anxiety) in a novel manner, as applying to our HIN a popular paradigm of a recommender system (RS), which is typically used to predict the preference that a person would give to an item (e.g., a movie or book). In our case, the items are the individuals' different mental health states. We evaluate four state-of-the-art RS approaches. Also, we model the prediction of individuals' mental health as another problem type - that of node classification (NC) in our HIN, evaluating in the process four node features under logistic regression as a proof-of-concept classifier. We find that our RS and NC network methods produce more accurate predictions than a logistic regression model using the same NetHealth data in the traditional non-network fashion as well as a random-approach. Also, we find that the best of the considered RS approaches outperforms all considered NC approaches. This is the first study to integrate smartphone, wearable sensor, and survey data in an HIN manner and use RS or NC on the HIN to predict individuals' mental health conditions.

</details>

<details>

<summary>2020-01-13 14:50:21 - Formal specification of a security framework for smart contracts</summary>

- *Mikhail Mandrykin, Jake O'Shannessy, Jacob Payne, Ilya Shchepetkov*

- `2001.04314v1` - [abs](http://arxiv.org/abs/2001.04314v1) - [pdf](http://arxiv.org/pdf/2001.04314v1)

> As smart contracts are growing in size and complexity, it becomes harder and harder to ensure their correctness and security. Due to the lack of isolation mechanisms a single mistake or vulnerability in the code can bring the whole system down, and due to this smart contract upgrades can be especially dangerous. Traditional ways to ensure the security of a smart contract, including DSLs, auditing and static analysis, are used before the code is deployed to the blockchain, and thus offer no protection after the deployment. After each upgrade the whole code need to be verified again, which is a difficult and time-consuming process that is prone to errors. To address these issues a security protocol and framework for smart contracts called Cap9 was developed. It provides developers the ability to perform upgrades in a secure and robust manner, and improves isolation and transparency through the use of a low level capability-based security model. We have used Isabelle/HOL to develop a formal specification of the Cap9 framework and prove its consistency. The paper presents a refinement-based approach that we used to create the specification, as well as discussion of some encountered difficulties during this process.

</details>

<details>

<summary>2020-01-13 20:03:14 - Security Vetting Process of Smart-home Assistant Applications: A First Look and Case Studies</summary>

- *Hang Hu, Limin Yang, Shihan Lin, Gang Wang*

- `2001.04520v1` - [abs](http://arxiv.org/abs/2001.04520v1) - [pdf](http://arxiv.org/pdf/2001.04520v1)

> The popularity of smart-home assistant systems such as Amazon Alexa and Google Home leads to a booming third-party application market (over 70,000 applications across the two stores). While existing works have revealed security issues in these systems, it is not well understood how to help application developers to enforce security requirements. In this paper, we perform a preliminary case study to examine the security vetting mechanisms adopted by Amazon Alexa and Google Home app stores. With a focus on the authentication mechanisms between Alexa/Google cloud and third-party application servers (i.e. endpoints), we show the current security vetting is insufficient as developer mistakes can not be effectively detected and notified. A weak authentication would allow attackers to spoof the cloud to insert/retrieve data into/from the application endpoints. We validate the attack through ethical proof-of-concept experiments. To confirm vulnerable applications have indeed passed the security vetting and entered the markets, we develop a heuristic-based searching method. We find 219 real-world Alexa endpoints that carry the vulnerability, many of which are related to critical applications that control smart home devices and electronic cars. We have notified Amazon and Google about our findings and offered our suggestions to mitigate the issue.

</details>

<details>

<summary>2020-01-14 08:28:37 - Montage: A Neural Network Language Model-Guided JavaScript Engine Fuzzer</summary>

- *Suyoung Lee, HyungSeok Han, Sang Kil Cha, Sooel Son*

- `2001.04107v2` - [abs](http://arxiv.org/abs/2001.04107v2) - [pdf](http://arxiv.org/pdf/2001.04107v2)

> JavaScript (JS) engine vulnerabilities pose significant security threats affecting billions of web browsers. While fuzzing is a prevalent technique for finding such vulnerabilities, there have been few studies that leverage the recent advances in neural network language models (NNLMs). In this paper, we present Montage, the first NNLM-guided fuzzer for finding JS engine vulnerabilities. The key aspect of our technique is to transform a JS abstract syntax tree (AST) into a sequence of AST subtrees that can directly train prevailing NNLMs. We demonstrate that Montage is capable of generating valid JS tests, and show that it outperforms previous studies in terms of finding vulnerabilities. Montage found 37 real-world bugs, including three CVEs, in the latest JS engines, demonstrating its efficacy in finding JS engine bugs.

</details>

<details>

<summary>2020-01-14 10:59:22 - ECGadv: Generating Adversarial Electrocardiogram to Misguide Arrhythmia Classification System</summary>

- *Huangxun Chen, Chenyu Huang, Qianyi Huang, Qian Zhang, Wei Wang*

- `1901.03808v4` - [abs](http://arxiv.org/abs/1901.03808v4) - [pdf](http://arxiv.org/pdf/1901.03808v4)

> Deep neural networks (DNNs)-powered Electrocardiogram (ECG) diagnosis systems recently achieve promising progress to take over tedious examinations by cardiologists. However, their vulnerability to adversarial attacks still lack comprehensive investigation. The existing attacks in image domain could not be directly applicable due to the distinct properties of ECGs in visualization and dynamic properties. Thus, this paper takes a step to thoroughly explore adversarial attacks on the DNN-powered ECG diagnosis system. We analyze the properties of ECGs to design effective attacks schemes under two attacks models respectively. Our results demonstrate the blind spots of DNN-powered diagnosis systems under adversarial attacks, which calls attention to adequate countermeasures.

</details>

<details>

<summary>2020-01-15 23:59:17 - Snooping Attacks on Deep Reinforcement Learning</summary>

- *Matthew Inkawhich, Yiran Chen, Hai Li*

- `1905.11832v2` - [abs](http://arxiv.org/abs/1905.11832v2) - [pdf](http://arxiv.org/pdf/1905.11832v2)

> Adversarial attacks have exposed a significant security vulnerability in state-of-the-art machine learning models. Among these models include deep reinforcement learning agents. The existing methods for attacking reinforcement learning agents assume the adversary either has access to the target agent's learned parameters or the environment that the agent interacts with. In this work, we propose a new class of threat models, called snooping threat models, that are unique to reinforcement learning. In these snooping threat models, the adversary does not have the ability to interact with the target agent's environment, and can only eavesdrop on the action and reward signals being exchanged between agent and environment. We show that adversaries operating in these highly constrained threat models can still launch devastating attacks against the target agent by training proxy models on related tasks and leveraging the transferability of adversarial examples.

</details>

<details>

<summary>2020-01-16 21:33:33 - Securing Wireless Sensor Networks Against Denial-of-Sleep Attacks Using RSA Cryptography Algorithm and Interlock Protocol</summary>

- *Reza Fotohi, Somayyeh Firoozi Bari, Mehdi Yusefi*

- `2001.06077v1` - [abs](http://arxiv.org/abs/2001.06077v1) - [pdf](http://arxiv.org/pdf/2001.06077v1)

> Wireless sensor networks (WSNs) have been vastly employed in the collection and transmission of data via wireless networks. This type of network is nowadays used in many applications for surveillance activities in various environments due to its low cost and easy communications. In these networks, the sensors use a limited power source which after its depletion, since it is non-renewable, network lifetime ends. Due to the weaknesses in sensor nodes, they are vulnerable to many threats. One notable attack threating WSN is Denial of Sleep (DoS). DoS attacks denotes the loss of energy in these sensors by keeping the nodes from going into sleep and energy-saving mode. In this paper, the Abnormal Sensor Detection Accuracy (ASDA-RSA) method is utilised to counteract DoS attacks to reducing the amount of energy consumed. The ASDA-RSA schema in this paper consists of two phases to enhancement security in the WSNs. In the first phase, a clustering approach based on energy and distance is used to select the proper cluster head and in the second phase, the RSA cryptography algorithm and interlock protocol are used here along with an authentication method, to prevent DoS attacks. Moreover, ASDA-RSA method is evaluated here via extensive simulations carried out in NS-2. The simulation results indicate that the WSN network performance metrics are improved in terms of average throughput, Packet Delivery Ratio (PDR), network lifetime, detection ratio, and average residual energy.

</details>

<details>

<summary>2020-01-17 23:08:19 - A Cryptanalysis of Two Cancelable Biometric Schemes based on Index-of-Max Hashing</summary>

- *Kevin Atighehchi, Loubna Ghammam, Koray Karabina, Patrick Lacharme*

- `1910.01389v3` - [abs](http://arxiv.org/abs/1910.01389v3) - [pdf](http://arxiv.org/pdf/1910.01389v3)

> Cancelable biometric schemes generate secure biometric templates by combining user specific tokens and biometric data. The main objective is to create irreversible, unlinkable, and revocable templates, with high accuracy in matching. In this paper, we cryptanalyze two recent cancelable biometric schemes based on a particular locality sensitive hashing function, index-of-max (IoM): Gaussian Random Projection-IoM (GRP-IoM) and Uniformly Random Permutation-IoM (URP-IoM). As originally proposed, these schemes were claimed to be resistant against reversibility, authentication, and linkability attacks under the stolen token scenario. We propose several attacks against GRP-IoM and URP-IoM, and argue that both schemes are severely vulnerable against authentication and linkability attacks. We also propose better, but not yet practical, reversibility attacks against GRP-IoM. The correctness and practical impact of our attacks are verified over the same dataset provided by the authors of these two schemes.

</details>

<details>

<summary>2020-01-18 20:11:36 - System-on-Chip Security Assertions</summary>

- *Yangdi Lyu, Prabhat Mishra*

- `2001.06719v1` - [abs](http://arxiv.org/abs/2001.06719v1) - [pdf](http://arxiv.org/pdf/2001.06719v1)

> Assertions are widely used for functional validation as well as coverage analysis for both software and hardware designs. Assertions enable runtime error detection as well as faster localization of errors. While there is a vast literature on both software and hardware assertions for monitoring functional scenarios, there is limited effort in utilizing assertions to monitor System-on-Chip (SoC) security vulnerabilities. In this paper, we identify common SoC security vulnerabilities by analyzing the design. To monitor these vulnerabilities, we define several classes of assertions to enable runtime checking of security vulnerabilities. Our experimental results demonstrate that the security assertions generated by our proposed approach can detect all the inserted vulnerabilities while the functional assertions generated by state-of-the-art assertion generation techniques fail to detect most of them.

</details>

<details>

<summary>2020-01-19 11:15:20 - LEOPARD: Identifying Vulnerable Code for Vulnerability Assessment through Program Metrics</summary>

- *Xiaoning Du, Bihuan Chen, Yuekang Li, Jianmin Guo, Yaqin Zhou, Yang Liu, Yu Jiang*

- `1901.11479v2` - [abs](http://arxiv.org/abs/1901.11479v2) - [pdf](http://arxiv.org/pdf/1901.11479v2)

> Identifying potentially vulnerable locations in a code base is critical as a pre-step for effective vulnerability assessment; i.e., it can greatly help security experts put their time and effort to where it is needed most. Metric-based and pattern-based methods have been presented for identifying vulnerable code. The former relies on machine learning and cannot work well due to the severe imbalance between non-vulnerable and vulnerable code or lack of features to characterize vulnerabilities. The latter needs the prior knowledge of known vulnerabilities and can only identify similar but not new types of vulnerabilities.   In this paper, we propose and implement a generic, lightweight and extensible framework, LEOPARD, to identify potentially vulnerable functions through program metrics. LEOPARD requires no prior knowledge about known vulnerabilities. It has two steps by combining two sets of systematically derived metrics. First, it uses complexity metrics to group the functions in a target application into a set of bins. Then, it uses vulnerability metrics to rank the functions in each bin and identifies the top ones as potentially vulnerable. Our experimental results on 11 real-world projects have demonstrated that, LEOPARD can cover 74.0% of vulnerable functions by identifying 20% of functions as vulnerable and outperform machine learning-based and static analysis-based techniques. We further propose three applications of LEOPARD for manual code review and fuzzing, through which we discovered 22 new bugs in real applications like PHP, radare2 and FFmpeg, and eight of them are new vulnerabilities.

</details>

<details>

<summary>2020-01-20 02:07:37 - Transferability of Adversarial Examples to Attack Cloud-based Image Classifier Service</summary>

- *Dou Goodman*

- `2001.03460v3` - [abs](http://arxiv.org/abs/2001.03460v3) - [pdf](http://arxiv.org/pdf/2001.03460v3)

> In recent years, Deep Learning(DL) techniques have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. While many recent works demonstrated that DL models are vulnerable to adversarial examples. Fortunately, generating adversarial examples usually requires white-box access to the victim model, and real-world cloud-based image classification services are more complex than white-box classifier,the architecture and parameters of DL models on cloud platforms cannot be obtained by the attacker. The attacker can only access the APIs opened by cloud platforms. Thus, keeping models in the cloud can usually give a (false) sense of security. In this paper, we mainly focus on studying the security of real-world cloud-based image classification services. Specifically, (1) We propose a novel attack method, Fast Featuremap Loss PGD (FFL-PGD) attack based on Substitution model, which achieves a high bypass rate with a very limited number of queries. Instead of millions of queries in previous studies, our method finds the adversarial examples using only two queries per image; and (2) we make the first attempt to conduct an extensive empirical study of black-box attacks against real-world cloud-based classification services. Through evaluations on four popular cloud platforms including Amazon, Google, Microsoft, Clarifai, we demonstrate that FFL-PGD attack has a success rate over 90\% among different classification services. (3) We discuss the possible defenses to address these security challenges in cloud-based classification services. Our defense technology is mainly divided into model training stage and image preprocessing stage.

</details>

<details>

<summary>2020-01-20 14:45:21 - Checking Smart Contracts with Structural Code Embedding</summary>

- *Zhipeng Gao, Lingxiao Jiang, Xin Xia, David Lo, John Grundy*

- `2001.07125v1` - [abs](http://arxiv.org/abs/2001.07125v1) - [pdf](http://arxiv.org/pdf/2001.07125v1)

> Smart contracts have been increasingly used together with blockchains to automate financial and business transactions. However, many bugs and vulnerabilities have been identified in many contracts which raises serious concerns about smart contract security, not to mention that the blockchain systems on which the smart contracts are built can be buggy. Thus, there is a significant need to better maintain smart contract code and ensure its high reliability. In this paper, we propose an automated approach to learn characteristics of smart contracts in Solidity, which is useful for clone detection, bug detection and contract validation on smart contracts. Our new approach is based on word embeddings and vector space comparison. We parse smart contract code into word streams with code structural information, convert code elements (e.g., statements, functions) into numerical vectors that are supposed to encode the code syntax and semantics, and compare the similarities among the vectors encoding code and known bugs, to identify potential issues. We have implemented the approach in a prototype, named SmartEmbed. Results show that our tool can effectively identify many repetitive instances of Solidity code, where the clone ratio is around 90\%. Code clones such as type-III or even type-IV semantic clones can also be detected accurately. Our tool can identify more than 1000 clone related bugs based on our bug databases efficiently and accurately. Our tool can also help to efficiently validate any given smart contract against a known set of bugs, which can help to improve the users' confidence in the reliability of the contract.   The anonymous replication packages can be accessed at: https://drive.google.com/file/d/1kauLT3y2IiHPkUlVx4FSTda-dVAyL4za/view?usp=sharing, and evaluated it with more than 22,000 smart contracts collected from the Ethereum blockchain.

</details>

<details>

<summary>2020-01-21 08:12:36 - Secure and Robust Machine Learning for Healthcare: A Survey</summary>

- *Adnan Qayyum, Junaid Qadir, Muhammad Bilal, Ala Al-Fuqaha*

- `2001.08103v1` - [abs](http://arxiv.org/abs/2001.08103v1) - [pdf](http://arxiv.org/pdf/2001.08103v1)

> Recent years have witnessed widespread adoption of machine learning (ML)/deep learning (DL) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from one-dimensional heart signals to computer-aided diagnosis (CADx) using multi-dimensional medical images. Notwithstanding the impressive performance of ML/DL, there are still lingering doubts regarding the robustness of ML/DL in healthcare settings (which is traditionally considered quite challenging due to the myriad security and privacy issues involved), especially in light of recent results that have shown that ML/DL are vulnerable to adversarial attacks. In this paper, we present an overview of various application areas in healthcare that leverage such techniques from security and privacy point of view and present associated challenges. In addition, we present potential methods to ensure secure and privacy-preserving ML for healthcare applications. Finally, we provide insight into the current research challenges and promising directions for future research.

</details>

<details>

<summary>2020-01-21 10:09:17 - Information Leaks via Safari's Intelligent Tracking Prevention</summary>

- *Artur Janc, Krzysztof Kotowicz, Lukas Weichselbaum, Roberto Clapis*

- `2001.07421v1` - [abs](http://arxiv.org/abs/2001.07421v1) - [pdf](http://arxiv.org/pdf/2001.07421v1)

> Intelligent Tracking Prevention (ITP) is a privacy mechanism implemented by Apple's Safari browser, released in October 2017. ITP aims to reduce the cross-site tracking of web users by limiting the capabilities of cookies and other website data. As part of a routine security review, the Information Security Engineering team at Google has identified multiple security and privacy issues in Safari's ITP design. These issues have a number of unexpected consequences, including the disclosure of the user's web browsing habits, allowing persistent cross-site tracking, and enabling cross-site information leaks (including cross-site search). This report is a modestly expanded version of our original vulnerability submission to Apple (WebKit bug #201319), providing additional context and edited for clarity. A number of the issues discussed here have been addressed in Safari 13.0.4 and iOS 13.3, released in December 2019.

</details>

<details>

<summary>2020-01-21 10:10:58 - Investigation of Data Deletion Vulnerabilities in NAND Flash Memory Based Storage</summary>

- *Abhilash Garg, Supriya Chakraborty, Manoj Malik, Devesh Kumar, Satyajeet Singh, Manan Suri*

- `2001.07424v1` - [abs](http://arxiv.org/abs/2001.07424v1) - [pdf](http://arxiv.org/pdf/2001.07424v1)

> Semiconductor NAND Flash based memory technology dominates the electronic Non-Volatile storage media market. Though NAND Flash offers superior performance and reliability over conventional magnetic HDDs, yet it suffers from certain data-security vulnerabilities. Such vulnerabilities can expose sensitive information stored on the media to security risks. It is thus necessary to study in detail the fundamental reasons behind data-security vulnerabilities of NAND Flash for use in critical applications. In this paper, the problem of unreliable data-deletion/sanitization in commercial NAND Flash media is investigated along with the fundamental reasons leading to such vulnerabilities. Exhaustive software based data recovery experiments (multiple iterations) has been carried out on commercial NAND Flash storage media (8 GB and 16 GB) for different types of filesystems (NTFS and FAT) and OS specific delete/Erase instructions. 100 % data recovery is obtained for windows and linux based delete/Erase commands. Inverse effect of performance enhancement techniques like wear levelling, bad block management etc. is also observed with the help of software based recovery experiments.

</details>

<details>

<summary>2020-01-22 09:49:44 - Mutation testing of smart contracts at scale</summary>

- *Pieter Hartel, Richard Schumi*

- `1909.12563v4` - [abs](http://arxiv.org/abs/1909.12563v4) - [pdf](http://arxiv.org/pdf/1909.12563v4)

> It is crucial that smart contracts are tested thoroughly due to their immutable nature. Even small bugs in smart contracts can lead to huge monetary losses. However, testing is not enough; it is also important to ensure the quality and completeness of the tests. There are already several approaches that tackle this challenge with mutation testing, but their effectiveness is questionable since they only considered small contract samples. Hence, we evaluate the quality of smart contract mutation testing at scale. We choose the most promising of the existing (smart contract specific) mutation operators, analyse their effectiveness in terms of killability and highlight severe vulnerabilities that can be injected with the mutations. Moreover, we improve the existing mutation methods by introducing a novel killing condition that is able to detect a deviation in the gas consumption, i.e., in the monetary value that is required to perform transactions.   This paper has a replication package at https://github.com/pieterhartel/Mutation-at-scale

</details>

<details>

<summary>2020-01-23 13:37:24 - Crushing the Wave -- new Z-Wave vulnerabilities exposed</summary>

- *Noureddine Boucif, Frederik Golchert, Alexander Siemer, Patrick Felke, Frederik Gosewehr*

- `2001.08497v1` - [abs](http://arxiv.org/abs/2001.08497v1) - [pdf](http://arxiv.org/pdf/2001.08497v1)

> This paper describes two denial of service attacks against the Z-Wave protocol and their effects on smart home gateways. Both utilize modified unencrypted packets, which are used in the inclusion phase and during normal operation. These are the commands Nonce Get/S2 Nonce Get and Find Nodes In Range. This paper shows how both can be manipulated and used to block a Z-Wave gateway's communication processing which in turn disables the whole Z-Wave network connected to it

</details>

<details>

<summary>2020-01-24 01:06:25 - Privacy for All: Demystify Vulnerability Disparity of Differential Privacy against Membership Inference Attack</summary>

- *Bo Zhang, Ruotong Yu, Haipei Sun, Yanying Li, Jun Xu, Hui Wang*

- `2001.08855v1` - [abs](http://arxiv.org/abs/2001.08855v1) - [pdf](http://arxiv.org/pdf/2001.08855v1)

> Machine learning algorithms, when applied to sensitive data, pose a potential threat to privacy. A growing body of prior work has demonstrated that membership inference attack (MIA) can disclose specific private information in the training data to an attacker. Meanwhile, the algorithmic fairness of machine learning has increasingly caught attention from both academia and industry. Algorithmic fairness ensures that the machine learning models do not discriminate a particular demographic group of individuals (e.g., black and female people). Given that MIA is indeed a learning model, it raises a serious concern if MIA ``fairly'' treats all groups of individuals equally. In other words, whether a particular group is more vulnerable against MIA than the other groups. This paper examines the algorithmic fairness issue in the context of MIA and its defenses. First, for fairness evaluation, it formalizes the notation of vulnerability disparity (VD) to quantify the difference of MIA treatment on different demographic groups. Second, it evaluates VD on four real-world datasets, and shows that VD indeed exists in these datasets. Third, it examines the impacts of differential privacy, as a defense mechanism of MIA, on VD. The results show that although DP brings significant change on VD, it cannot eliminate VD completely. Therefore, fourth, it designs a new mitigation algorithm named FAIRPICK to reduce VD. An extensive set of experimental results demonstrate that FAIRPICK can effectively reduce VD for both with and without the DP deployment.

</details>

<details>

<summary>2020-01-24 05:07:39 - When Wireless Security Meets Machine Learning: Motivation, Challenges, and Research Directions</summary>

- *Yalin E. Sagduyu, Yi Shi, Tugba Erpek, William Headley, Bryse Flowers, George Stantchev, Zhuo Lu*

- `2001.08883v1` - [abs](http://arxiv.org/abs/2001.08883v1) - [pdf](http://arxiv.org/pdf/2001.08883v1)

> Wireless systems are vulnerable to various attacks such as jamming and eavesdropping due to the shared and broadcast nature of wireless medium. To support both attack and defense strategies, machine learning (ML) provides automated means to learn from and adapt to wireless communication characteristics that are hard to capture by hand-crafted features and models. This article discusses motivation, background, and scope of research efforts that bridge ML and wireless security. Motivated by research directions surveyed in the context of ML for wireless security, ML-based attack and defense solutions and emerging adversarial ML techniques in the wireless domain are identified along with a roadmap to foster research efforts in bridging ML and wireless security.

</details>

<details>

<summary>2020-01-24 18:08:25 - Withdrawing the BGP Re-Routing Curtain: Understanding the Security Impact of BGP Poisoning via Real-World Measurements</summary>

- *Jared M. Smith, Kyle Birkeland, Tyler McDaniel, Max Schuchard*

- `1811.03716v6` - [abs](http://arxiv.org/abs/1811.03716v6) - [pdf](http://arxiv.org/pdf/1811.03716v6)

> The security of the Internet's routing infrastructure has underpinned much of the past two decades of distributed systems security research. However, the converse is increasingly true. Routing and path decisions are now important for the security properties of systems built on top of the Internet. In particular, BGP poisoning leverages the de facto routing protocol between Autonomous Systems (ASes) to maneuver the return paths of upstream networks onto previously unusable, new paths. These new paths can be used to avoid congestion, censors, geo-political boundaries, or any feature of the topology which can be expressed at an AS-level. Given the increase in BGP poisoning usage as a security primitive, we set out to evaluate poisoning feasibility in practice beyond simulation.   To that end, using an Internet-scale measurement infrastructure, we capture and analyze over 1,400 instances of BGP poisoning across thousands of ASes as a mechanism to maneuver return paths of traffic. We analyze in detail the performance of steering paths, the graph-theoretic aspects of available paths, and re-evaluate simulated systems with this data. We find that the real-world evidence does not completely support the findings from simulated systems published in the literature. We also analyze filtering of BGP poisoning across types of ASes and ISP working groups. We explore the connectivity concerns when poisoning by reproducing a decade old experiment to uncover the current state of an Internet triple the size. We build predictive models for understanding an ASes' vulnerability to poisoning. Finally, an exhaustive measurement of an upper bound on the maximum path length of the Internet is presented, detailing how security research should react to ASes leveraging poisoned long paths. In total, our results and analysis expose the real-world impact of BGP poisoning on past and future security research.

</details>

<details>

<summary>2020-01-24 18:58:50 - Learning to Catch Security Patches</summary>

- *Arthur D. Sawadogo, Tegawendé F. Bissyandé, Naouel Moha, Kevin Allix, Jacques Klein, Li Li, Yves Le Traon*

- `2001.09148v1` - [abs](http://arxiv.org/abs/2001.09148v1) - [pdf](http://arxiv.org/pdf/2001.09148v1)

> Timely patching is paramount to safeguard users and maintainers against dire consequences of malicious attacks. In practice, patching is prioritized following the nature of the code change that is committed in the code repository. When such a change is labeled as being security-relevant, i.e., as fixing a vulnerability, maintainers rapidly spread the change and users are notified about the need to update to a new version of the library or of the application. Unfortunately, oftentimes, some security-relevant changes go unnoticed as they represent silent fixes of vulnerabilities. In this paper, we propose a Co-Training-based approach to catch security patches as part of an automatic monitoring service of code repositories. Leveraging different classes of features, we empirically show that such automation is feasible and can yield a precision of over 90% in identifying security patches, with an unprecedented recall of over 80%. Beyond such a benchmarking with ground truth data which demonstrates an improvement over the state-of-the-art, we confirmed that our approach can help catch security patches that were not reported as such.

</details>

<details>

<summary>2020-01-26 05:59:40 - A Survey on Smartphones Security: Software Vulnerabilities, Malware, and Attacks</summary>

- *Milad Taleby Ahvanooey, Qianmu Li, Mahdi Rabbani, Ahmed Raza Rajput*

- `2001.09406v1` - [abs](http://arxiv.org/abs/2001.09406v1) - [pdf](http://arxiv.org/pdf/2001.09406v1)

> Nowadays, the usage of smartphones and their applications have become rapidly increasing popular in people's daily life. Over the last decade, availability of mobile money services such as mobile-payment systems and app markets have significantly increased due to the different forms of apps and connectivity provided by mobile devices such as 3G, 4G, GPRS, and Wi-Fi, etc. In the same trend, the number of vulnerabilities targeting these services and communication networks has raised as well. Therefore, smartphones have become ideal target devices for malicious programmers. With increasing the number of vulnerabilities and attacks, there has been a corresponding ascent of the security countermeasures presented by the researchers. Due to these reasons, security of the payment systems is one of the most important issues in mobile payment systems. In this survey, we aim to provide a comprehensive and structured overview of the research on security solutions for smartphone devices. This survey reviews the state of the art on security solutions, threats, and vulnerabilities during the period of 2011-2017, by focusing on software attacks, such those to smartphone applications. We outline some countermeasures aimed at protecting smartphones against these groups of attacks, based on the detection rules, data collections and operating systems, especially focusing on open source applications. With this categorization, we want to provide an easy understanding for users and researchers to improve their knowledge about the security and privacy of smartphones.

</details>

<details>

<summary>2020-01-26 16:22:53 - Secondary Use of Electronic Health Record: Opportunities and Challenges</summary>

- *Shahid Munir Shah, Rizwan Ahmed Khan*

- `2001.09479v1` - [abs](http://arxiv.org/abs/2001.09479v1) - [pdf](http://arxiv.org/pdf/2001.09479v1)

> In present technological era, healthcare providers generate huge amount of clinical data on daily basis. Generated clinical data is stored digitally in the form of Electronic Health Records (EHR) as a central data repository of hospitals. Data contained in EHR is not only used for the patients' primary care but also for various secondary purposes such as clinical research, automated disease surveillance and clinical audits for quality enhancement. Using EHR data for secondary purposes without consent or in some cases even with consent creates privacy issues for individuals. Secondly, EHR data is also made accessible to various stake holders including different government agencies at various geographical sites through wired or wireless networks. Sharing of EHR across multiples agencies makes it vulnerable to cyber attacks and also makes it difficult to implement strict privacy laws as in some cases data is shared with organization that is governed by specific regional law. Privacy of an individual could be severely affected when their sensitive private information contained in EHR is leaked or exposed to public. Data leak can cause financial losses or an individuals may encounter social boycott if their medical condition is exposed in public. To protect patients personal data from such threats, there exists different privacy regulations such as GDPR, HIPAA and MHR. However, continually evolving state-of-the-art techniques in machine learning, data analytics and hacking are making it even more difficult to completely protect individual's / patient's privacy. In this article, we have systematically examined various secondary uses of EHR with the aim to highlight how these secondary uses effect patients' privacy. Secondly, we have critically analyzed GDPR and highlighted possible areas of improvement, considering escalating use of technology and different secondary uses of EHR.

</details>

<details>

<summary>2020-01-27 05:49:32 - Finding Security Vulnerabilities in Network Protocol Implementations</summary>

- *Kaled Alshmrany, Lucas Cordeiro*

- `2001.09592v1` - [abs](http://arxiv.org/abs/2001.09592v1) - [pdf](http://arxiv.org/pdf/2001.09592v1)

> Implementations of network protocols are often prone to vulnerabilities caused by developers' mistakes when accessing memory regions and dealing with arithmetic operations. Finding practical approaches for checking the security of network protocol implementations has proven to be a challenging problem. The main reason is that the protocol software state-space is too large to be explored. Here we propose a novel verification approach that combines fuzzing with symbolic execution to verify intricate properties in network protocol implementations. We use fuzzing for an initial exploration of the network protocol, while symbolic execution explores both the program paths and protocol states, which were uncovered by fuzzing. From this combination, we automatically generate high-coverage test input packets for a network protocol implementation. We surveyed various approaches based on fuzzing and symbolic execution to understand how these techniques can be effectively combined and then choose a suitable tool to develop further our model on top of it. In our preliminary evaluation, we used ESBMC, Map2Check, and KLEE as software verifiers and SPIKE as fuzzer to check their suitability to verify our network protocol implementations. Our experimental results show that ESBMC can be further developed within our verification framework called \textit{FuSeBMC}, to efficiently and effectively detect intricate security vulnerabilities in network protocol implementations.

</details>

<details>

<summary>2020-01-27 07:37:07 - Practical Fast Gradient Sign Attack against Mammographic Image Classifier</summary>

- *Ibrahim Yilmaz*

- `2001.09610v1` - [abs](http://arxiv.org/abs/2001.09610v1) - [pdf](http://arxiv.org/pdf/2001.09610v1)

> Artificial intelligence (AI) has been a topic of major research for many years. Especially, with the emergence of deep neural network (DNN), these studies have been tremendously successful. Today machines are capable of making faster, more accurate decision than human. Thanks to the great development of machine learning (ML) techniques, ML have been used many different fields such as education, medicine, malware detection, autonomous car etc. In spite of having this degree of interest and much successful research, ML models are still vulnerable to adversarial attacks. Attackers can manipulate clean data in order to fool the ML classifiers to achieve their desire target. For instance; a benign sample can be modified as a malicious sample or a malicious one can be altered as benign while this modification can not be recognized by human observer. This can lead to many financial losses, or serious injuries, even deaths. The motivation behind this paper is that we emphasize this issue and want to raise awareness. Therefore, the security gap of mammographic image classifier against adversarial attack is demonstrated. We use mamographic images to train our model then evaluate our model performance in terms of accuracy. Later on, we poison original dataset and generate adversarial samples that missclassified by the model. We then using structural similarity index (SSIM) analyze similarity between clean images and adversarial images. Finally, we show how successful we are to misuse by using different poisoning factors.

</details>

<details>

<summary>2020-01-27 15:03:07 - Verifying Software Vulnerabilities in IoT Cryptographic Protocols</summary>

- *Fatimah Aljaafari, Lucas C. Cordeiro, Mustafa A. Mustafa*

- `2001.09837v1` - [abs](http://arxiv.org/abs/2001.09837v1) - [pdf](http://arxiv.org/pdf/2001.09837v1)

> Internet of Things (IoT) is a system that consists of a large number of smart devices connected through a network. The number of these devices is increasing rapidly, which creates a massive and complex network with a vast amount of data communicated over that network. One way to protect this data in transit, i.e., to achieve data confidentiality, is to use lightweight encryption algorithms for IoT protocols. However, the design and implementation of such protocols is an error-prone task; flaws in the implementation can lead to devastating security vulnerabilities. These vulnerabilities can be exploited by an attacker and affect users' privacy. There exist various techniques to verify software and detect vulnerabilities. Bounded Model Checking (BMC) and Fuzzing are useful techniques to check the correctness of a software system concerning its specifications. Here we describe a framework called Encryption-BMC and Fuzzing (EBF) using combined BMC and fuzzing techniques. We evaluate the application of EBF verification framework on a case study, i.e., the S-MQTT protocol, to check security vulnerabilities in cryptographic protocols for IoT.

</details>

<details>

<summary>2020-01-28 06:36:40 - Adversarial Image Translation: Unrestricted Adversarial Examples in Face Recognition Systems</summary>

- *Kazuya Kakizaki, Kosuke Yoshida*

- `1905.03421v3` - [abs](http://arxiv.org/abs/1905.03421v3) - [pdf](http://arxiv.org/pdf/1905.03421v3)

> Thanks to recent advances in deep neural networks (DNNs), face recognition systems have become highly accurate in classifying a large number of face images. However, recent studies have found that DNNs could be vulnerable to adversarial examples, raising concerns about the robustness of such systems. Adversarial examples that are not restricted to small perturbations could be more serious since conventional certified defenses might be ineffective against them. To shed light on the vulnerability to such adversarial examples, we propose a flexible and efficient method for generating unrestricted adversarial examples using image translation techniques. Our method enables us to translate a source image into any desired facial appearance with large perturbations to deceive target face recognition systems. Our experimental results indicate that our method achieved about $90$ and $80\%$ attack success rates under white- and black-box settings, respectively, and that the translated images are perceptually realistic and maintain the identifiability of the individual while the perturbations are large enough to bypass certified defenses.

</details>

<details>

<summary>2020-01-28 08:05:39 - Membership Inference Attacks Against Object Detection Models</summary>

- *Yeachan Park, Myungjoo Kang*

- `2001.04011v2` - [abs](http://arxiv.org/abs/2001.04011v2) - [pdf](http://arxiv.org/pdf/2001.04011v2)

> Machine learning models can leak information regarding the dataset they have trained. In this paper, we present the first membership inference attack against black-boxed object detection models that determines whether the given data records are used in the training. To attack the object detection model, we devise a novel method named as called a canvas method, in which predicted bounding boxes are drawn on an empty image for the attack model input. Based on the experiments, we successfully reveal the membership status of privately sensitive data trained using one-stage and two-stage detection models. We then propose defense strategies and also conduct a transfer attack between the models and datasets. Our results show that object detection models are also vulnerable to inference attacks like other models.

</details>

<details>

<summary>2020-01-28 23:13:12 - IoT Behavioral Monitoring via Network Traffic Analysis</summary>

- *Arunan Sivanathan*

- `2001.10632v1` - [abs](http://arxiv.org/abs/2001.10632v1) - [pdf](http://arxiv.org/pdf/2001.10632v1)

> Smart homes, enterprises, and cities are increasingly being equipped with a plethora of Internet of Things (IoT), ranging from smart-lights to security cameras. While IoT networks have the potential to benefit our lives, they create privacy and security challenges not seen with traditional IT networks. Due to the lack of visibility, operators of such smart environments are not often aware of their IoT assets, let alone whether each IoT device is functioning properly safe from cyber-attacks. This thesis is the culmination of our efforts to develop techniques to profile the network behavioral pattern of IoTs, automate IoT classification, deduce their operating context, and detect anomalous behavior indicative of cyber-attacks.   We begin this thesis by surveying IoT ecosystem, while reviewing current approaches to vulnerability assessments, intrusion detection, and behavioral monitoring. For our first contribution, we collect traffic traces and characterize the network behavior of IoT devices via attributes from traffic patterns. We develop a robust machine learning-based inference engine trained with these attributes and demonstrate real-time classification of 28 IoT devices with over 99% accuracy. Our second contribution enhances the classification by reducing the cost of attribute extraction while also identifying IoT device states. Prototype implementation and evaluation demonstrate the ability of our supervised machine learning method to detect behavioral changes for five IoT devices. Our third and final contribution develops a modularized unsupervised inference engine that dynamically accommodates the addition of new IoT devices and/or updates to existing ones, without requiring system-wide retraining of the model. We demonstrate via experiments that our model can automatically detect attacks and firmware changes in ten IoT devices with over 94% accuracy.

</details>

<details>

<summary>2020-01-29 03:46:53 - A Survey of Distributed Consensus Protocols for Blockchain Networks</summary>

- *Yang Xiao, Ning Zhang, Wenjing Lou, Y. Thomas Hou*

- `1904.04098v4` - [abs](http://arxiv.org/abs/1904.04098v4) - [pdf](http://arxiv.org/pdf/1904.04098v4)

> Since the inception of Bitcoin, cryptocurrencies and the underlying blockchain technology have attracted an increasing interest from both academia and industry. Among various core components, consensus protocol is the defining technology behind the security and performance of blockchain. From incremental modifications of Nakamoto consensus protocol to innovative alternative consensus mechanisms, many consensus protocols have been proposed to improve the performance of the blockchain network itself or to accommodate other specific application needs.   In this survey, we present a comprehensive review and analysis on the state-of-the-art blockchain consensus protocols. To facilitate the discussion of our analysis, we first introduce the key definitions and relevant results in the classic theory of fault tolerance which help to lay the foundation for further discussion. We identify five core components of a blockchain consensus protocol, namely, block proposal, block validation, information propagation, block finalization, and incentive mechanism. A wide spectrum of blockchain consensus protocols are then carefully reviewed accompanied by algorithmic abstractions and vulnerability analyses. The surveyed consensus protocols are analyzed using the five-component framework and compared with respect to different performance metrics. These analyses and comparisons provide us new insights in the fundamental differences of various proposals in terms of their suitable application scenarios, key assumptions, expected fault tolerance, scalability, drawbacks and trade-offs. We believe this survey will provide blockchain developers and researchers a comprehensive view on the state-of-the-art consensus protocols and facilitate the process of designing future protocols.

</details>

<details>

<summary>2020-01-29 06:51:13 - A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning</summary>

- *Shahbaz Rezaei, Xin Liu*

- `1904.04334v3` - [abs](http://arxiv.org/abs/1904.04334v3) - [pdf](http://arxiv.org/pdf/1904.04334v3)

> Due to insufficient training data and the high computational cost to train a deep neural network from scratch, transfer learning has been extensively used in many deep-neural-network-based applications. A commonly used transfer learning approach involves taking a part of a pre-trained model, adding a few layers at the end, and re-training the new layers with a small dataset. This approach, while efficient and widely used, imposes a security vulnerability because the pre-trained model used in transfer learning is usually publicly available, including to potential attackers. In this paper, we show that without any additional knowledge other than the pre-trained model, an attacker can launch an effective and efficient brute force attack that can craft instances of input to trigger each target class with high confidence. We assume that the attacker has no access to any target-specific information, including samples from target classes, re-trained model, and probabilities assigned by Softmax to each class, and thus making the attack target-agnostic. These assumptions render all previous attack models inapplicable, to the best of our knowledge. To evaluate the proposed attack, we perform a set of experiments on face recognition and speech recognition tasks and show the effectiveness of the attack. Our work reveals a fundamental security weakness of the Softmax layer when used in transfer learning settings.

</details>

<details>

<summary>2020-01-31 11:54:46 - Benchmarking Popular Classification Models' Robustness to Random and Targeted Corruptions</summary>

- *Utkarsh Desai, Srikanth Tamilselvam, Jassimran Kaur, Senthil Mani, Shreya Khare*

- `2002.00754v1` - [abs](http://arxiv.org/abs/2002.00754v1) - [pdf](http://arxiv.org/pdf/2002.00754v1)

> Text classification models, especially neural networks based models, have reached very high accuracy on many popular benchmark datasets. Yet, such models when deployed in real world applications, tend to perform badly. The primary reason is that these models are not tested against sufficient real world natural data. Based on the application users, the vocabulary and the style of the model's input may greatly vary. This emphasizes the need for a model agnostic test dataset, which consists of various corruptions that are natural to appear in the wild. Models trained and tested on such benchmark datasets, will be more robust against real world data. However, such data sets are not easily available. In this work, we address this problem, by extending the benchmark datasets along naturally occurring corruptions such as Spelling Errors, Text Noise and Synonyms and making them publicly available. Through extensive experiments, we compare random and targeted corruption strategies using Local Interpretable Model-Agnostic Explanations(LIME). We report the vulnerabilities in two popular text classification models along these corruptions and also find that targeted corruptions can expose vulnerabilities of a model better than random choices in most cases.

</details>

<details>

<summary>2020-01-31 17:12:52 - Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks</summary>

- *Sanghyun Hong, Michael Davinroy, Yiǧitcan Kaya, Stuart Nevans Locke, Ian Rackow, Kevin Kulda, Dana Dachman-Soled, Tudor Dumitraş*

- `1810.03487v4` - [abs](http://arxiv.org/abs/1810.03487v4) - [pdf](http://arxiv.org/pdf/1810.03487v4)

> Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary's capability to conduct black-box attacks against the model. This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels. First, we define the threat model for these attacks: our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim's deep learning (DL) system is running and passively monitors the accesses of the target functions in the shared framework. Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique. Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim's entire network architecture. In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having observed only one forward propagation. Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting. From this meta-model, we evaluate the importance of the observed attributes in the fingerprinting process. Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker's observations. Our empirical security analysis represents a step toward understanding the DNNs' vulnerability to cache side-channel attacks.

</details>


## 2020-02

<details>

<summary>2020-02-01 06:57:41 - Quantifying (Hyper) Parameter Leakage in Machine Learning</summary>

- *Vasisht Duddu, D. Vijay Rao*

- `1910.14409v2` - [abs](http://arxiv.org/abs/1910.14409v2) - [pdf](http://arxiv.org/pdf/1910.14409v2)

> Machine Learning models, extensively used for various multimedia applications, are offered to users as a blackbox service on the Cloud on a pay-per-query basis. Such blackbox models are commercially valuable to adversaries, making them vulnerable to extraction attacks to reverse engineer the proprietary model thereby violating the model privacy and Intellectual Property. Here, the adversary first extracts the model architecture or hyperparameters through side channel leakage, followed by stealing the functionality of the target model by training the reconstructed architecture on a synthetic dataset. While the attacks proposed in literature are empirical, there is a need for a theoretical framework to measure the information leaked under such extraction attacks. To this extent, in this work, we propose a novel probabilistic framework, Airavata, to estimate the information leakage in such model extraction attacks. This framework captures the fact that extracting the exact target model is difficult due to experimental uncertainty while inferring model hyperparameters and stochastic nature of training to steal the target model functionality. Specifically, we use Bayesian Networks to capture uncertainty in estimating the target model under various extraction attacks based on the subjective notion of probability. We validate the proposed framework under different adversary assumptions commonly adopted in literature to reason about the attack efficacy. This provides a practical tool to infer actionable details about extracting blackbox models and help identify the best attack combination which maximises the knowledge extracted (or information leaked) from the target model.

</details>

<details>

<summary>2020-02-01 12:10:27 - Self-supervised Adversarial Training</summary>

- *Kejiang Chen, Hang Zhou, Yuefeng Chen, Xiaofeng Mao, Yuhong Li, Yuan He, Hui Xue, Weiming Zhang, Nenghai Yu*

- `1911.06470v2` - [abs](http://arxiv.org/abs/1911.06470v2) - [pdf](http://arxiv.org/pdf/1911.06470v2)

> Recent work has demonstrated that neural networks are vulnerable to adversarial examples. To escape from the predicament, many works try to harden the model in various ways, in which adversarial training is an effective way which learns robust feature representation so as to resist adversarial attacks. Meanwhile, the self-supervised learning aims to learn robust and semantic embedding from data itself. With these views, we introduce self-supervised learning to against adversarial examples in this paper. Specifically, the self-supervised representation coupled with k-Nearest Neighbour is proposed for classification. To further strengthen the defense ability, self-supervised adversarial training is proposed, which maximizes the mutual information between the representations of original examples and the corresponding adversarial examples. Experimental results show that the self-supervised representation outperforms its supervised version in respect of robustness and self-supervised adversarial training can further improve the defense ability efficiently.

</details>

<details>

<summary>2020-02-01 14:09:48 - Learning to Detect Malicious Clients for Robust Federated Learning</summary>

- *Suyi Li, Yong Cheng, Wei Wang, Yang Liu, Tianjian Chen*

- `2002.00211v1` - [abs](http://arxiv.org/abs/2002.00211v1) - [pdf](http://arxiv.org/pdf/2002.00211v1)

> Federated learning systems are vulnerable to attacks from malicious clients. As the central server in the system cannot govern the behaviors of the clients, a rogue client may initiate an attack by sending malicious model updates to the server, so as to degrade the learning performance or enforce targeted model poisoning attacks (a.k.a. backdoor attacks). Therefore, timely detecting these malicious model updates and the underlying attackers becomes critically important. In this work, we propose a new framework for robust federated learning where the central server learns to detect and remove the malicious model updates using a powerful detection model, leading to targeted defense. We evaluate our solution in both image classification and sentiment analysis tasks with a variety of machine learning models. Experimental results show that our solution ensures robust federated learning that is resilient to both the Byzantine attacks and the targeted model poisoning attacks.

</details>

<details>

<summary>2020-02-01 14:24:53 - Public Authorities as Defendants: Using Bayesian Networks to determine the Likelihood of Success for Negligence claims in the wake of Oakden</summary>

- *Scott McLachlan, Evangelia Kyrimi, Norman Fenton*

- `2002.05664v1` - [abs](http://arxiv.org/abs/2002.05664v1) - [pdf](http://arxiv.org/pdf/2002.05664v1)

> Several countries are currently investigating issues of neglect, poor quality care and abuse in the aged care sector. In most cases it is the State who license and monitor aged care providers, which frequently introduces a serious conflict of interest because the State also operate many of the facilities where our most vulnerable peoples are cared for. Where issues are raised with the standard of care being provided, the State are seen by many as a deep-pockets defendant and become the target of high-value lawsuits. This paper draws on cases and circumstances from one jurisdiction based on the English legal tradition, Australia, and proposes a Bayesian solution capable of determining probability for success for citizen plaintiffs who bring negligence claims against a public authority defendant. Use of a Bayesian network trained on case audit data shows that even when the plaintiff case meets all requirements for a successful negligence litigation, success is not often assured. Only in around one-fifth of these cases does the plaintiff succeed against a public authority as defendant.

</details>

<details>

<summary>2020-02-01 21:15:52 - Reinforcement Learning with Perturbed Rewards</summary>

- *Jingkang Wang, Yang Liu, Bo Li*

- `1810.01032v4` - [abs](http://arxiv.org/abs/1810.01032v4) - [pdf](http://arxiv.org/pdf/1810.01032v4)

> Recent studies have shown that reinforcement learning (RL) models are vulnerable in various noisy scenarios. For instance, the observed reward channel is often subject to noise in practice (e.g., when rewards are collected through sensors), and is therefore not credible. In addition, for applications such as robotics, a deep reinforcement learning (DRL) algorithm can be manipulated to produce arbitrary errors by receiving corrupted rewards. In this paper, we consider noisy RL problems with perturbed rewards, which can be approximated with a confusion matrix. We develop a robust RL framework that enables agents to learn in noisy environments where only perturbed rewards are observed. Our solution framework builds on existing RL/DRL algorithms and firstly addresses the biased noisy reward setting without any assumptions on the true distribution (e.g., zero-mean Gaussian noise as made in previous works). The core ideas of our solution include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. We prove the convergence and sample complexity of our approach. Extensive experiments on different DRL platforms show that trained policies based on our estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines. For instance, the state-of-the-art PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively.

</details>

<details>

<summary>2020-02-02 18:48:31 - Permissioned Blockchain-Based Security for SDN in IoT Cloud Networks</summary>

- *Safi Faizullah, Muhammad Asad Khan, Ali Alzahrani, Imdadullah Khan*

- `2002.00456v1` - [abs](http://arxiv.org/abs/2002.00456v1) - [pdf](http://arxiv.org/pdf/2002.00456v1)

> The advancement in cloud networks has enabled connectivity of both traditional networked elements and new devices from all walks of life, thereby forming the Internet of Things (IoT). In an IoT setting, improving and scaling network components as well as reducing cost is essential to sustain exponential growth. In this domain, software-defined networking (SDN) is revolutionizing the network infrastructure with a new paradigm. SDN splits the control/routing logic from the data transfer/forwarding. This splitting causes many issues in SDN, such as vulnerabilities of DDoS attacks. Many solutions (including blockchain based) have been proposed to overcome these problems. In this work, we offer a blockchain-based solution that is provided in redundant SDN (load-balanced) to service millions of IoT devices. Blockchain is considered as tamper-proof and impossible to corrupt due to the replication of the ledger and consensus for verification and addition to the ledger. Therefore, it is a perfect fit for SDN in IoT Networks. Blockchain technology provides everyone with a working proof of decentralized trust. The experimental results show gain and efficiency with respect to the accuracy, update process, and bandwidth utilization.

</details>

<details>

<summary>2020-02-02 19:06:33 - Detecting DDoS Attack on SDN Due to Vulnerabilities in OpenFlow</summary>

- *Sarwan Ali, Maria Khalid Alvi, Safi Faizullah, Muhammad Asad Khan, Abdullah Alshanqiti, Imdadullah Khan*

- `1912.12221v4` - [abs](http://arxiv.org/abs/1912.12221v4) - [pdf](http://arxiv.org/pdf/1912.12221v4)

> Software Defined Networking (SDN) is a network paradigm shift that facilitates comprehensive network programmability to cope with emerging new technologies such as cloud computing and big data. SDN facilitates simplified and centralized network management enabling it to operate in dynamic scenarios. Further, SDN uses the OpenFlow protocol for communication between the controller and its switches. The OpenFlow creates vulnerabilities for network attacks especially Distributed Denial of Service (DDoS). DDoS attacks are launched from the compromised hosts connected to the SDN switches. In this paper, we introduce a time- and space-efficient solution for the identification of these compromised hosts. Our solution consumes less computational resources and space and does not require any special equipment.

</details>

<details>

<summary>2020-02-03 02:58:31 - Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks</summary>

- *Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, John E. Hopcroft*

- `1908.06281v5` - [abs](http://arxiv.org/abs/1908.06281v5) - [pdf](http://arxiv.org/pdf/1908.06281v5)

> Deep learning models are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on benign inputs. However, under the black-box setting, most existing adversaries often have a poor transferability to attack other defense models. In this work, from the perspective of regarding the adversarial example generation as an optimization process, we propose two new methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). NI-FGSM aims to adapt Nesterov accelerated gradient into the iterative attacks so as to effectively look ahead and improve the transferability of adversarial examples. While SIM is based on our discovery on the scale-invariant property of deep learning models, for which we leverage to optimize the adversarial perturbations over the scale copies of the input images so as to avoid "overfitting" on the white-box model being attacked and generate more transferable adversarial examples. NI-FGSM and SIM can be naturally integrated to build a robust gradient-based attack to generate more transferable adversarial examples against the defense models. Empirical results on ImageNet dataset demonstrate that our attack methods exhibit higher transferability and achieve higher attack success rates than state-of-the-art gradient-based attacks.

</details>

<details>

<summary>2020-02-04 19:34:34 - Bicycle Attacks Considered Harmful: Quantifying the Damage of Widespread Password Length Leakage</summary>

- *Benjamin Harsha, Robert Morton, Jeremiah Blocki, John Springer, Melissa Dark*

- `2002.01513v1` - [abs](http://arxiv.org/abs/2002.01513v1) - [pdf](http://arxiv.org/pdf/2002.01513v1)

> We examine the issue of password length leakage via encrypted traffic i.e., bicycle attacks. We aim to quantify both the prevalence of password length leakage bugs as well as the potential harm to users. In an observational study, we find that {\em most} of the Alexa top 100 rates sites are vulnerable to bicycle attacks meaning that an eavesdropping attacker can infer the exact length of a password based on the length the encrypted packet containing the password. We discuss several ways in which an eavesdropping attacker could link this password length with a particular user account e.g., a targeted campaign against a smaller group of users or via DNS hijacking for larger scale campaigns. We next use a decision-theoretic model to quantify the extent to which password length leakage might help an attacker to crack user passwords. In our analysis, we consider three different levels of password attackers: hacker, criminal and nation-state. In all cases, we find that such an attacker who knows the length of each user password gains a significant advantage over one without knowing the password length. As part of this analysis, we also release a new differentially private password frequency dataset from the 2016 LinkedIn breach using a differentially private algorithm of Blocki et al. (NDSS 2016) to protect user accounts. The LinkedIn frequency corpus is based on over 170 million passwords making it the largest frequency corpus publicly available to password researchers. While the defense against bicycle attacks is straightforward (i.e., ensure that passwords are always padded before encryption), we discuss several practical challenges organizations may face when attempting to patch this vulnerability. We advocate for a new W3C standard on how password fields are handled which would effectively eliminate most instances of password length leakage.

</details>

<details>

<summary>2020-02-04 22:05:09 - Whose Side are Ethics Codes On? Power, Responsibility and the Social Good</summary>

- *Anne L. Washington, Rachel S. Kuo*

- `2002.01559v1` - [abs](http://arxiv.org/abs/2002.01559v1) - [pdf](http://arxiv.org/pdf/2002.01559v1)

> The moral authority of ethics codes stems from an assumption that they serve a unified society, yet this ignores the political aspects of any shared resource. The sociologist Howard S. Becker challenged researchers to clarify their power and responsibility in the classic essay: Whose Side Are We On. Building on Becker's hierarchy of credibility, we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence, or the "social good", of data technology. The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency. Interviews with community organizers about social change in the digital era supplement the analysis, surfacing the limits of technical solutions to concerns of marginalized communities. Given evidence that highlights the gulf between the documents and lived experiences, we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations. Understanding contested digital resources is central to the emerging field of public interest technology. We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes.

</details>

<details>

<summary>2020-02-05 14:34:22 - Understanding the Decision Boundary of Deep Neural Networks: An Empirical Study</summary>

- *David Mickisch, Felix Assion, Florens Greßner, Wiebke Günther, Mariele Motta*

- `2002.01810v1` - [abs](http://arxiv.org/abs/2002.01810v1) - [pdf](http://arxiv.org/pdf/2002.01810v1)

> Despite achieving remarkable performance on many image classification tasks, state-of-the-art machine learning (ML) classifiers remain vulnerable to small input perturbations. Especially, the existence of adversarial examples raises concerns about the deployment of ML models in safety- and security-critical environments, like autonomous driving and disease detection. Over the last few years, numerous defense methods have been published with the goal of improving adversarial as well as corruption robustness. However, the proposed measures succeeded only to a very limited extent. This limited progress is partly due to the lack of understanding of the decision boundary and decision regions of deep neural networks. Therefore, we study the minimum distance of data points to the decision boundary and how this margin evolves over the training of a deep neural network. By conducting experiments on MNIST, FASHION-MNIST, and CIFAR-10, we observe that the decision boundary moves closer to natural images over training. This phenomenon even remains intact in the late epochs of training, where the classifier already obtains low training and test error rates. On the other hand, adversarial training appears to have the potential to prevent this undesired convergence of the decision boundary.

</details>

<details>

<summary>2020-02-06 09:49:16 - An Analysis of Adversarial Attacks and Defenses on Autonomous Driving Models</summary>

- *Yao Deng, Xi Zheng, Tianyi Zhang, Chen Chen, Guannan Lou, Miryung Kim*

- `2002.02175v1` - [abs](http://arxiv.org/abs/2002.02175v1) - [pdf](http://arxiv.org/pdf/2002.02175v1)

> Nowadays, autonomous driving has attracted much attention from both industry and academia. Convolutional neural network (CNN) is a key component in autonomous driving, which is also increasingly adopted in pervasive computing such as smartphones, wearable devices, and IoT networks. Prior work shows CNN-based classification models are vulnerable to adversarial attacks. However, it is uncertain to what extent regression models such as driving models are vulnerable to adversarial attacks, the effectiveness of existing defense techniques, and the defense implications for system and middleware builders. This paper presents an in-depth analysis of five adversarial attacks and four defense methods on three driving models. Experiments show that, similar to classification models, these models are still highly vulnerable to adversarial attacks. This poses a big security threat to autonomous driving and thus should be taken into account in practice. While these defense methods can effectively defend against different attacks, none of them are able to provide adequate protection against all five attacks. We derive several implications for system and middleware builders: (1) when adding a defense component against adversarial attacks, it is important to deploy multiple defense methods in tandem to achieve a good coverage of various attacks, (2) a blackbox attack is much less effective compared with a white-box attack, implying that it is important to keep model details (e.g., model architecture, hyperparameters) confidential via model obfuscation, and (3) driving models with a complex architecture are preferred if computing resources permit as they are more resilient to adversarial attacks than simple models.

</details>

<details>

<summary>2020-02-07 02:24:50 - Blockchain for Internet of Things: A Survey</summary>

- *Hong-Ning Dai, Zibin Zheng, Yan Zhang*

- `1906.00245v5` - [abs](http://arxiv.org/abs/1906.00245v5) - [pdf](http://arxiv.org/pdf/1906.00245v5)

> Internet of Things (IoT) is reshaping the incumbent industry to smart industry featured with data-driven decision-making. However, intrinsic features of IoT result in a number of challenges such as decentralization, poor interoperability, privacy and security vulnerabilities. Blockchain technology brings the opportunities in addressing the challenges of IoT. In this paper, we investigate the integration of blockchain technology with IoT. We name such synthesis of blockchain and IoT as Blockchain of Things (BCoT). This paper presents an in-depth survey of BCoT and discusses the insights of this new paradigm. In particular, we first briefly introduce IoT and discuss the challenges of IoT. Then we give an overview of blockchain technology. We next concentrate on introducing the convergence of blockchain and IoT and presenting the proposal of BCoT architecture. We further discuss the issues about using blockchain for 5G beyond in IoT as well as industrial applications of BCoT. Finally, we outline the open research directions in this promising area.

</details>

<details>

<summary>2020-02-07 10:54:57 - Formalising and verifying smart contracts with Solidifier: a bounded model checker for Solidity</summary>

- *Pedro Antonino, A. W. Roscoe*

- `2002.02710v1` - [abs](http://arxiv.org/abs/2002.02710v1) - [pdf](http://arxiv.org/pdf/2002.02710v1)

> The exploitation of smart-contract vulnerabilities can have catastrophic consequences such as the loss of millions of pounds worth of crypto assets. Formal verification can be a useful tool in identifying vulnerabilities and proving that they have been fixed. In this paper, we present a formalisation of Solidity and the Ethereum blockchain using the Solid language and its blockchain; a Solid program is obtained by explicating/desugaring a Solidity program. We make some abstractions that over-approximate the way in which Solidity/Ethereum behave. Based on this formalisation, we create Solidifier: a bounded model checker for Solidity. It translates Solid into Boogie, an intermediate verification language, that is later verified using Corral, a bounded model checker for Boogie. Unlike much of the work in this area, we do not try to find specific behavioural/code patterns that might lead to vulnerabilities. Instead, we provide a tool to find errors/bad states, i.e. program states that do not conform with the intent of the developer. Such a bad state, be it a vulnerability or not, might be reached through the execution of specific known code patterns or through behaviours that have not been anticipated.

</details>

<details>

<summary>2020-02-07 13:27:29 - RAID: Randomized Adversarial-Input Detection for Neural Networks</summary>

- *Hasan Ferit Eniser, Maria Christakis, Valentin Wüstholz*

- `2002.02776v1` - [abs](http://arxiv.org/abs/2002.02776v1) - [pdf](http://arxiv.org/pdf/2002.02776v1)

> In recent years, neural networks have become the default choice for image classification and many other learning tasks, even though they are vulnerable to so-called adversarial attacks. To increase their robustness against these attacks, there have emerged numerous detection mechanisms that aim to automatically determine if an input is adversarial. However, state-of-the-art detection mechanisms either rely on being tuned for each type of attack, or they do not generalize across different attack types. To alleviate these issues, we propose a novel technique for adversarial-image detection, RAID, that trains a secondary classifier to identify differences in neuron activation values between benign and adversarial inputs. Our technique is both more reliable and more effective than the state of the art when evaluated against six popular attacks. Moreover, a straightforward extension of RAID increases its robustness against detection-aware adversaries without affecting its effectiveness.

</details>

<details>

<summary>2020-02-07 15:47:50 - Security Certification in Payment Card Industry: Testbeds, Measurements, and Recommendations</summary>

- *Sazzadur Rahaman, Gang Wang, Danfeng, Yao*

- `2002.02855v1` - [abs](http://arxiv.org/abs/2002.02855v1) - [pdf](http://arxiv.org/pdf/2002.02855v1)

> The massive payment card industry (PCI) involves various entities such as merchants, issuer banks, acquirer banks, and card brands. Ensuring security for all entities that process payment card information is a challenging task. The PCI Security Standards Council requires all entities to be compliant with the PCI Data Security Standard (DSS), which specifies a series of security requirements. However, little is known regarding how well PCI DSS is enforced in practice. In this paper, we take a measurement approach to systematically evaluate the PCI DSS certification process for e-commerce websites. We develop an e-commerce web application testbed, BuggyCart, which can flexibly add or remove 35 PCI DSS related vulnerabilities. Then we use the testbed to examine the capability and limitations of PCI scanners and the rigor of the certification process. We find that there is an alarming gap between the security standard and its real-world enforcement. None of the 6 PCI scanners we tested are fully compliant with the PCI scanning guidelines, issuing certificates to merchants that still have major vulnerabilities. To further examine the compliance status of real-world e-commerce websites, we build a new lightweight scanning tool named PciCheckerLite and scan 1,203 e-commerce websites across various business sectors. The results confirm that 86% of the websites have at least one PCI DSS violation that should have disqualified them as non-compliant. Our in-depth accuracy analysis also shows that PciCheckerLite's output is more precise than w3af. We reached out to the PCI Security Council to share our research results to improve the enforcement in practice.

</details>

<details>

<summary>2020-02-08 02:57:24 - IoT Network Behavioral Fingerprint Inference with Limited Network Trace for Cyber Investigation: A Meta Learning Approach</summary>

- *Jonathan Pan*

- `2001.04705v2` - [abs](http://arxiv.org/abs/2001.04705v2) - [pdf](http://arxiv.org/pdf/2001.04705v2)

> The development and adoption of Internet of Things (IoT) devices will grow significantly in the coming years to enable Industry 4.0. Many forms of IoT devices will be developed and used across industry verticals. However, the euphoria of this technology adoption is shadowed by the solemn presence of cyber threats that will follow its growth trajectory. Cyber threats would either embed their malicious code or attack vulnerabilities in IoT that could induce significant consequences in cyber and physical realms. In order to manage such destructive effects, incident responders and cyber investigators require the capabilities to find these rogue IoT and contain them quickly. Such online devices may only leave network activity traces. A collection of relevant traces could be used to infer the IoT's network behaviorial fingerprints and in turn could facilitate investigative find of these IoT. However, the challenge is how to infer these fingerprints when there is limited network activity traces. This research proposes the novel model construct that learns to infer the network behaviorial fingerprint of specific IoT based on limited network activity traces using a One-Card Time Series Meta-Learner called DeepNetPrint. Our research also demonstrates the application of DeepNetPrint to identify IoT devices that performs comparatively well against leading supervised learning models. Our solution would enable cyber investigator to identify specific IoT of interest while overcoming the constraints of having only limited network traces of the IoT.

</details>

<details>

<summary>2020-02-08 04:45:15 - Data-Driven Debugging for Functional Side Channels</summary>

- *Saeid Tizpaz-Niari, Pavol Cerny, Ashutosh Trivedi*

- `1808.10502v2` - [abs](http://arxiv.org/abs/1808.10502v2) - [pdf](http://arxiv.org/pdf/1808.10502v2)

> Information leaks through side channels are a pervasive problem, even in security-critical applications. Functional side channels arise when an attacker knows that a secret value of a server stays fixed for a certain time. Then, the attacker can observe the server executions on a sequence of different public inputs, each paired with the same secret input. Thus for each secret, the attacker observes a function from public inputs to execution time, for instance, and she can compare these functions for different secrets. First, we introduce a notion of noninterference for functional side channels. We focus on the case of noisy observations, where we demonstrate with examples that there is a practical functional side channel in programs that would be deemed information-leak-free or be underestimated using the standard definition. Second, we develop a framework and techniques for debugging programs for functional side channels. We extend evolutionary fuzzing techniques to generate inputs that exploit functional dependencies of response times on public inputs. We adapt existing results and algorithms in functional data analysis to model the functions and discover the existence of side channels. We use a functional extension of standard decision tree learning to pinpoint the code fragments causing a side channel if there is one. We empirically evaluate the performance of our tool FUCHSIA on a series of micro-benchmarks and realistic Java programs. On the set of benchmarks, we show that FUCHSIA outperforms the state-of-the-art techniques in detecting side channel classes. On the realistic programs, we show the scalability of FUCHSIA in analyzing functional side channels in Java programs with thousands of methods. Also, we show the usefulness of FUCHSIA in finding side channels including a zero-day vulnerability in OpenJDK and another vulnerability in Jetty that was since fixed by the developers.

</details>

<details>

<summary>2020-02-08 05:53:21 - Attacking Optical Character Recognition (OCR) Systems with Adversarial Watermarks</summary>

- *Lu Chen, Wei Xu*

- `2002.03095v1` - [abs](http://arxiv.org/abs/2002.03095v1) - [pdf](http://arxiv.org/pdf/2002.03095v1)

> Optical character recognition (OCR) is widely applied in real applications serving as a key preprocessing tool. The adoption of deep neural network (DNN) in OCR results in the vulnerability against adversarial examples which are crafted to mislead the output of the threat model. Different from vanilla colorful images, images of printed text have clear backgrounds usually. However, adversarial examples generated by most of the existing adversarial attacks are unnatural and pollute the background severely. To address this issue, we propose a watermark attack method to produce natural distortion that is in the disguise of watermarks and evade human eyes' detection. Experimental results show that watermark attacks can yield a set of natural adversarial examples attached with watermarks and attain similar attack performance to the state-of-the-art methods in different attack scenarios.

</details>

<details>

<summary>2020-02-09 13:53:29 - Empirical Review of Automated Analysis Tools on 47,587 Ethereum Smart Contracts</summary>

- *Thomas Durieux, João F. Ferreira, Rui Abreu, Pedro Cruz*

- `1910.10601v2` - [abs](http://arxiv.org/abs/1910.10601v2) - [pdf](http://arxiv.org/pdf/1910.10601v2)

> Over the last few years, there has been substantial research on automated analysis, testing, and debugging of Ethereum smart contracts. However, it is not trivial to compare and reproduce that research. To address this, we present an empirical evaluation of 9 state-of-the-art automated analysis tools using two new datasets: i) a dataset of 69 annotated vulnerable smart contracts that can be used to evaluate the precision of analysis tools; and ii) a dataset with all the smart contracts in the Ethereum Blockchain that have Solidity source code available on Etherscan (a total of 47,518 contracts). The datasets are part of SmartBugs, a new extendable execution framework that we created to facilitate the integration and comparison between multiple analysis tools and the analysis of Ethereum smart contracts. We used SmartBugs to execute the 9 automated analysis tools on the two datasets. In total, we ran 428,337 analyses that took approximately 564 days and 3 hours, being the largest experimental setup to date both in the number of tools and in execution time. We found that only 42% of the vulnerabilities from our annotated dataset are detected by all the tools, with the tool Mythril having the higher accuracy (27%). When considering the largest dataset, we observed that 97% of contracts are tagged as vulnerable, thus suggesting a considerable number of false positives. Indeed, only a small number of vulnerabilities (and of only two categories) were detected simultaneously by four or more tools.

</details>

<details>

<summary>2020-02-10 04:15:31 - On the Relationship between Software Complexity and Security</summary>

- *Mamdouh Alenezi, Mohammad Zarour*

- `2002.07135v1` - [abs](http://arxiv.org/abs/2002.07135v1) - [pdf](http://arxiv.org/pdf/2002.07135v1)

> This work aims at discussing the complexity aspect of software while demonstrating its relationship with security. Complexity is an essential part of software; however, numerous studies indicate that they increase the vulnerability of the software systems and introduce bugs in the program. Many developers face difficulty when trying to understand the complex components of software. Complexity in software increases when objects in the software are used to design a more complex object while creating a hierarchical complexity in the system. However, it is necessary for the developers to strive for minimum complexity, as increased complexity introduces security risks in the software, which can cause severe monetary and reputational damage to a government or a private organization. It even causes bodily harm to human beings with various examples found in previous years where security breaches led to severe consequences. Hence it is vital to maintain low complexity and simple design of structure. Various developers tend to introduce deliberate complexities in the system so that they do not have to write the same program twice; however, it is getting problematic for the software organizations as the demands of security are continually increasing.

</details>

<details>

<summary>2020-02-10 19:07:10 - PingPong: Packet-Level Signatures for Smart Home Device Events</summary>

- *Rahmadi Trimananda, Janus Varmarken, Athina Markopoulou, Brian Demsky*

- `1907.11797v3` - [abs](http://arxiv.org/abs/1907.11797v3) - [pdf](http://arxiv.org/pdf/1907.11797v3)

> Smart home devices are vulnerable to passive inference attacks based on network traffic, even in the presence of encryption. In this paper, we present PINGPONG, a tool that can automatically extract packet-level signatures for device events (e.g., light bulb turning ON/OFF) from network traffic. We evaluated PINGPONG on popular smart home devices ranging from smart plugs and thermostats to cameras, voice-activated devices, and smart TVs. We were able to: (1) automatically extract previously unknown signatures that consist of simple sequences of packet lengths and directions; (2) use those signatures to detect the devices or specific events with an average recall of more than 97%; (3) show that the signatures are unique among hundreds of millions of packets of real world network traffic; (4) show that our methodology is also applicable to publicly available datasets; and (5) demonstrate its robustness in different settings: events triggered by local and remote smartphones, as well as by homeautomation systems.

</details>

<details>

<summary>2020-02-10 21:57:24 - A Regularized Attention Mechanism for Graph Attention Networks</summary>

- *Uday Shankar Shanthamallu, Jayaraman J. Thiagarajan, Andreas Spanias*

- `1811.00181v2` - [abs](http://arxiv.org/abs/1811.00181v2) - [pdf](http://arxiv.org/pdf/1811.00181v2)

> Machine learning models that can exploit the inherent structure in data have gained prominence. In particular, there is a surge in deep learning solutions for graph-structured data, due to its wide-spread applicability in several fields. Graph attention networks (GAT), a recent addition to the broad class of feature learning models in graphs, utilizes the attention mechanism to efficiently learn continuous vector representations for semi-supervised learning problems. In this paper, we perform a detailed analysis of GAT models, and present interesting insights into their behavior. In particular, we show that the models are vulnerable to heterogeneous rogue nodes and hence propose novel regularization strategies to improve the robustness of GAT models. Using benchmark datasets, we demonstrate performance improvements on semi-supervised learning, using the proposed robust variant of GAT.

</details>

<details>

<summary>2020-02-12 10:51:12 - ExplFrame: Exploiting Page Frame Cache for Fault Analysis of Block Ciphers</summary>

- *Anirban Chakraborty, Sarani Bhattacharya, Sayandeep Saha, Debdeep Mukhopadhyay*

- `1905.12974v3` - [abs](http://arxiv.org/abs/1905.12974v3) - [pdf](http://arxiv.org/pdf/1905.12974v3)

> Page Frame Cache (PFC) is a purely software cache, present in modern Linux based operating systems (OS), which stores the page frames that are recently being released by the processes running on a particular CPU. In this paper, we show that the page frame cache can be maliciously exploited by an adversary to steer the pages of a victim process to some pre-decided attacker-chosen locations in the memory. We practically demonstrate an end-to-end attack, ExplFrame, where an attacker having only user-level privilege is able to force a victim process's memory pages to vulnerable locations in DRAM and deterministically conduct Rowhammer to induce faults. We further show that these faults can be exploited for extracting the secret key of table-based block cipher implementations. As a case study, we perform a full-key recovery on OpenSSL AES by Rowhammer-induced single bit faults in the T-tables. We propose an improvised fault analysis technique which can exploit any Rowhammer-induced bit-flips in the AES T-tables.

</details>

<details>

<summary>2020-02-12 10:52:38 - A Zero-Shot based Fingerprint Presentation Attack Detection System</summary>

- *Haozhe Liu, Wentian Zhang, Guojie Liu, Feng Liu*

- `2002.04908v1` - [abs](http://arxiv.org/abs/2002.04908v1) - [pdf](http://arxiv.org/pdf/2002.04908v1)

> With the development of presentation attacks, Automated Fingerprint Recognition Systems(AFRSs) are vulnerable to presentation attack. Thus, numerous methods of presentation attack detection(PAD) have been proposed to ensure the normal utilization of AFRS. However, the demand of large-scale presentation attack images and the low-level generalization ability always astrict existing PAD methods' actual performances. Therefore, we propose a novel Zero-Shot Presentation Attack Detection Model to guarantee the generalization of the PAD model. The proposed ZSPAD-Model based on generative model does not utilize any negative samples in the process of establishment, which ensures the robustness for various types or materials based presentation attack. Different from other auto-encoder based model, the Fine-grained Map architecture is proposed to refine the reconstruction error of the auto-encoder networks and a task-specific gaussian model is utilized to improve the quality of clustering. Meanwhile, in order to improve the performance of the proposed model, 9 confidence scores are discussed in this article. Experimental results showed that the ZSPAD-Model is the state of the art for ZSPAD, and the MS-Score is the best confidence score. Compared with existing methods, the proposed ZSPAD-Model performs better than the feature-based method and under the multi-shot setting, the proposed method overperforms the learning based method with little training data. When large training data is available, their results are similar.

</details>

<details>

<summary>2020-02-12 13:54:35 - Adversarial Attacks on GMM i-vector based Speaker Verification Systems</summary>

- *Xu Li, Jinghua Zhong, Xixin Wu, Jianwei Yu, Xunying Liu, Helen Meng*

- `1911.03078v2` - [abs](http://arxiv.org/abs/1911.03078v2) - [pdf](http://arxiv.org/pdf/1911.03078v2)

> This work investigates the vulnerability of Gaussian Mixture Model (GMM) i-vector based speaker verification systems to adversarial attacks, and the transferability of adversarial samples crafted from GMM i-vector based systems to x-vector based systems. In detail, we formulate the GMM i-vector system as a scoring function of enrollment and testing utterance pairs. Then we leverage the fast gradient sign method (FGSM) to optimize testing utterances for adversarial samples generation. These adversarial samples are used to attack both GMM i-vector and x-vector systems. We measure the system vulnerability by the degradation of equal error rate and false acceptance rate. Experiment results show that GMM i-vector systems are seriously vulnerable to adversarial attacks, and the crafted adversarial samples prove to be transferable and pose threats to neuralnetwork speaker embedding based systems (e.g. x-vector systems).

</details>

<details>

<summary>2020-02-12 16:54:00 - QPEP: A QUIC-Based Approach to Encrypted Performance Enhancing Proxies for High-Latency Satellite Broadband</summary>

- *James Pavur, Martin Strohmeier, Vincent Lenders, Ivan Martinovic*

- `2002.05091v1` - [abs](http://arxiv.org/abs/2002.05091v1) - [pdf](http://arxiv.org/pdf/2002.05091v1)

> Satellite broadband services are critical infrastructures enabling advanced technologies to function in the most remote regions of the globe. However, status-quo services are often unencrypted by default and vulnerable to eavesdropping attacks. In this paper, we challenge the historical perception that over-the-air security must trade off with TCP performance in high-latency satellite networks due to the deep-packet inspection requirements of Performance Enhancing Proxies (PEPs).   After considering why prior work in this area has failed to find wide adoption, we present an open-source encrypted-by-default PEP - QPEP - which seeks to address these issues. QPEP is built around the open QUIC standard and designed so individual customers may adopt it without ISP involvement. QPEP's performance is assessed through simulations in a replicable docker-based testbed. Across many benchmarks and network conditions, QPEP is found to avoid the perceived security-encryption trade-off in PEP design. Compared to unencrypted PEP implementations, QPEP reduces average page load times by more than 30% while also offering over-the-air privacy. Compared to the traditional VPN encryption available to customers today, QPEP more than halves average page load times. Together, these experiments lead to the conclusion that QPEP represents a promising new approach to protecting modern satellite broadband connections.

</details>

<details>

<summary>2020-02-13 07:46:38 - The Conditional Entropy Bottleneck</summary>

- *Ian Fischer*

- `2002.05379v1` - [abs](http://arxiv.org/abs/2002.05379v1) - [pdf](http://arxiv.org/pdf/2002.05379v1)

> Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.

</details>

<details>

<summary>2020-02-13 17:35:34 - Over-the-Air Adversarial Attacks on Deep Learning Based Modulation Classifier over Wireless Channels</summary>

- *Brian Kim, Yalin E. Sagduyu, Kemal Davaslioglu, Tugba Erpek, Sennur Ulukus*

- `2002.02400v2` - [abs](http://arxiv.org/abs/2002.02400v2) - [pdf](http://arxiv.org/pdf/2002.02400v2)

> We consider a wireless communication system that consists of a transmitter, a receiver, and an adversary. The transmitter transmits signals with different modulation types, while the receiver classifies its received signals to modulation types using a deep learning-based classifier. In the meantime, the adversary makes over-the-air transmissions that are received as superimposed with the transmitter's signals to fool the classifier at the receiver into making errors. While this evasion attack has received growing interest recently, the channel effects from the adversary to the receiver have been ignored so far such that the previous attack mechanisms cannot be applied under realistic channel effects. In this paper, we present how to launch a realistic evasion attack by considering channels from the adversary to the receiver. Our results show that modulation classification is vulnerable to an adversarial attack over a wireless channel that is modeled as Rayleigh fading with path loss and shadowing. We present various adversarial attacks with respect to availability of information about channel, transmitter input, and classifier architecture. First, we present two types of adversarial attacks, namely a targeted attack (with minimum power) and non-targeted attack that aims to change the classification to a target label or to any other label other than the true label, respectively. Both are white-box attacks that are transmitter input-specific and use channel information. Then we introduce an algorithm to generate adversarial attacks using limited channel information where the adversary only knows the channel distribution. Finally, we present a black-box universal adversarial perturbation (UAP) attack where the adversary has limited knowledge about both channel and transmitter input.

</details>

<details>

<summary>2020-02-13 23:37:00 - Thou Shalt Not Depend on Me: Analysing the Use of Outdated JavaScript Libraries on the Web</summary>

- *Tobias Lauinger, Abdelberi Chaabane, Sajjad Arshad, William Robertson, Christo Wilson, Engin Kirda*

- `1811.00918v2` - [abs](http://arxiv.org/abs/1811.00918v2) - [pdf](http://arxiv.org/pdf/1811.00918v2)

> Web developers routinely rely on third-party Java-Script libraries such as jQuery to enhance the functionality of their sites. However, if not properly maintained, such dependencies can create attack vectors allowing a site to be compromised.   In this paper, we conduct the first comprehensive study of client-side JavaScript library usage and the resulting security implications across the Web. Using data from over 133 k websites, we show that 37% of them include at least one library with a known vulnerability; the time lag behind the newest release of a library is measured in the order of years. In order to better understand why websites use so many vulnerable or outdated libraries, we track causal inclusion relationships and quantify different scenarios. We observe sites including libraries in ad hoc and often transitive ways, which can lead to different versions of the same library being loaded into the same document at the same time. Furthermore, we find that libraries included transitively, or via ad and tracking code, are more likely to be vulnerable. This demonstrates that not only website administrators, but also the dynamic architecture and developers of third-party services are to blame for the Web's poor state of library management.   The results of our work underline the need for more thorough approaches to dependency management, code maintenance and third-party code inclusion on the Web.

</details>

<details>

<summary>2020-02-14 00:07:22 - Large-Scale Analysis of Style Injection by Relative Path Overwrite</summary>

- *Sajjad Arshad, Seyed Ali Mirheidari, Tobias Lauinger, Bruno Crispo, Engin Kirda, William Robertson*

- `1811.00917v2` - [abs](http://arxiv.org/abs/1811.00917v2) - [pdf](http://arxiv.org/pdf/1811.00917v2)

> Relative Path Overwrite (RPO) is a recent technique to inject style directives into sites even when no style sink or markup injection vulnerability is present. It exploits differences in how browsers and web servers interpret relative paths (i.e., path confusion) to make a HTML page reference itself as a stylesheet; a simple text injection vulnerability along with browsers' leniency in parsing CSS resources results in an attacker's ability to inject style directives that will be interpreted by the browser. Even though style injection may appear less serious a threat than script injection, it has been shown that it enables a range of attacks, including secret exfiltration.   In this paper, we present the first large-scale study of the Web to measure the prevalence and significance of style injection using RPO. Our work shows that around 9% of the sites in the Alexa Top 10,000 contain at least one vulnerable page, out of which more than one third can be exploited. We analyze in detail various impediments to successful exploitation, and make recommendations for remediation. In contrast to script injection, relatively simple countermeasures exist to mitigate style injection. However, there appears to be little awareness of this attack vector as evidenced by a range of popular Content Management Systems (CMSes) that we found to be exploitable.

</details>

<details>

<summary>2020-02-14 00:22:59 - Cached and Confused: Web Cache Deception in the Wild</summary>

- *Seyed Ali Mirheidari, Sajjad Arshad, Kaan Onarlioglu, Bruno Crispo, Engin Kirda, William Robertson*

- `1912.10190v2` - [abs](http://arxiv.org/abs/1912.10190v2) - [pdf](http://arxiv.org/pdf/1912.10190v2)

> Web cache deception (WCD) is an attack proposed in 2017, where an attacker tricks a caching proxy into erroneously storing private information transmitted over the Internet and subsequently gains unauthorized access to that cached data. Due to the widespread use of web caches and, in particular, the use of massive networks of caching proxies deployed by content distribution network (CDN) providers as a critical component of the Internet, WCD puts a substantial population of Internet users at risk. We present the first large-scale study that quantifies the prevalence of WCD in 340 high-profile sites among the Alexa Top 5K. Our analysis reveals WCD vulnerabilities that leak private user data as well as secret authentication and authorization tokens that can be leveraged by an attacker to mount damaging web application attacks. Furthermore, we explore WCD in a scientific framework as an instance of the path confusion class of attacks, and demonstrate that variations on the path confusion technique used make it possible to exploit sites that are otherwise not impacted by the original attack. Our findings show that many popular sites remain vulnerable two years after the public disclosure of WCD. Our empirical experiments with popular CDN providers underline the fact that web caches are not plug & play technologies. In order to mitigate WCD, site operators must adopt a holistic view of their web infrastructure and carefully configure cache settings appropriate for their applications.

</details>

<details>

<summary>2020-02-14 00:28:24 - On the Effectiveness of Type-based Control Flow Integrity</summary>

- *Reza Mirzazade Farkhani, Saman Jafari, Sajjad Arshad, William Robertson, Engin Kirda, Hamed Okhravi*

- `1810.10649v2` - [abs](http://arxiv.org/abs/1810.10649v2) - [pdf](http://arxiv.org/pdf/1810.10649v2)

> Control flow integrity (CFI) has received significant attention in the community to combat control hijacking attacks in the presence of memory corruption vulnerabilities. The challenges in creating a practical CFI has resulted in the development of a new type of CFI based on runtime type checking (RTC). RTC-based CFI has been implemented in a number of recent practical efforts such as GRSecurity Reuse Attack Protector (RAP) and LLVM-CFI. While there has been a number of previous efforts that studied the strengths and limitations of other types of CFI techniques, little has been done to evaluate the RTC-based CFI. In this work, we study the effectiveness of RTC from the security and practicality aspects. From the security perspective, we observe that type collisions are abundant in sufficiently large code bases but exploiting them to build a functional attack is not straightforward. Then we show how an attacker can successfully bypass RTC techniques using a variant of ROP attacks that respect type checking (called TROP) and also built two proof-of-concept exploits, one against Nginx web server and the other against Exim mail server. We also discuss practical challenges of implementing RTC. Our findings suggest that while RTC is more practical for applying CFI to large code bases, its policy is not strong enough when facing a motivated attacker.

</details>

<details>

<summary>2020-02-14 12:09:21 - Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets</summary>

- *Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, Xingjun Ma*

- `2002.05990v1` - [abs](http://arxiv.org/abs/2002.05990v1) - [pdf](http://arxiv.org/pdf/2002.05990v1)

> Skip connections are an essential component of current state-of-the-art deep neural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt. Despite their huge success in building deeper and more powerful DNNs, we identify a surprising security weakness of skip connections in this paper. Use of skip connections allows easier generation of highly transferable adversarial examples. Specifically, in ResNet-like (with skip connections) neural networks, gradients can backpropagate through either skip connections or residual modules. We find that using more gradients from the skip connections rather than the residual modules according to a decay factor, allows one to craft adversarial examples with high transferability. Our method is termed Skip Gradient Method(SGM). We conduct comprehensive transfer attacks against state-of-the-art DNNs including ResNets, DenseNets, Inceptions, Inception-ResNet, Squeeze-and-Excitation Network (SENet) and robustly trained DNNs. We show that employing SGM on the gradient flow can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, SGM can be easily combined with existing black-box attack techniques, and obtain high improvements over state-of-the-art transferability methods. Our findings not only motivate new research into the architectural vulnerability of DNNs, but also open up further challenges for the design of secure DNN architectures.

</details>

<details>

<summary>2020-02-14 20:16:22 - HotFuzz: Discovering Algorithmic Denial-of-Service Vulnerabilities Through Guided Micro-Fuzzing</summary>

- *William Blair, Andrea Mambretti, Sajjad Arshad, Michael Weissbacher, William Robertson, Engin Kirda, Manuel Egele*

- `2002.03416v2` - [abs](http://arxiv.org/abs/2002.03416v2) - [pdf](http://arxiv.org/pdf/2002.03416v2)

> Contemporary fuzz testing techniques focus on identifying memory corruption vulnerabilities that allow adversaries to achieve either remote code execution or information disclosure. Meanwhile, Algorithmic Complexity (AC)vulnerabilities, which are a common attack vector for denial-of-service attacks, remain an understudied threat. In this paper, we present HotFuzz, a framework for automatically discovering AC vulnerabilities in Java libraries. HotFuzz uses micro-fuzzing, a genetic algorithm that evolves arbitrary Java objects in order to trigger the worst-case performance for a method under test. We define Small Recursive Instantiation (SRI) as a technique to derive seed inputs represented as Java objects to micro-fuzzing. After micro-fuzzing, HotFuzz synthesizes test cases that triggered AC vulnerabilities into Java programs and monitors their execution in order to reproduce vulnerabilities outside the fuzzing framework. HotFuzz outputs those programs that exhibit high CPU utilization as witnesses for AC vulnerabilities in a Java library. We evaluate HotFuzz over the Java Runtime Environment (JRE), the 100 most popular Java libraries on Maven, and challenges contained in the DARPA Space and Time Analysis for Cybersecurity (STAC) program. We evaluate SRI's effectiveness by comparing the performance of micro-fuzzing with SRI, measured by the number of AC vulnerabilities detected, to simply using empty values as seed inputs. In this evaluation, we verified known AC vulnerabilities, discovered previously unknown AC vulnerabilities that we responsibly reported to vendors, and received confirmation from both IBM and Oracle. Our results demonstrate that micro-fuzzing finds AC vulnerabilities in real-world software, and that micro-fuzzing with SRI-derived seed inputs outperforms using empty values.

</details>

<details>

<summary>2020-02-15 14:04:33 - Analyzing CNN Based Behavioural Malware Detection Techniques on Cloud IaaS</summary>

- *Andrew McDole, Mahmoud Abdelsalam, Maanak Gupta, Sudip Mittal*

- `2002.06383v1` - [abs](http://arxiv.org/abs/2002.06383v1) - [pdf](http://arxiv.org/pdf/2002.06383v1)

> Cloud Infrastructure as a Service (IaaS) is vulnerable to malware due to its exposure to external adversaries, making it a lucrative attack vector for malicious actors. A datacenter infected with malware can cause data loss and/or major disruptions to service for its users. This paper analyzes and compares various Convolutional Neural Networks (CNNs) for online detection of malware in cloud IaaS. The detection is performed based on behavioural data using process level performance metrics including cpu usage, memory usage, disk usage etc. We have used the state of the art DenseNets and ResNets in effectively detecting malware in online cloud system. CNN are designed to extract features from data gathered from a live malware running on a real cloud environment. Experiments are performed on OpenStack (a cloud IaaS software) testbed designed to replicate a typical 3-tier web architecture. Comparative analysis is performed for different metrics for different CNN models used in this research.

</details>

<details>

<summary>2020-02-15 19:03:36 - Undersensitivity in Neural Reading Comprehension</summary>

- *Johannes Welbl, Pasquale Minervini, Max Bartolo, Pontus Stenetorp, Sebastian Riedel*

- `2003.04808v1` - [abs](http://arxiv.org/abs/2003.04808v1) - [pdf](http://arxiv.org/pdf/2003.04808v1)

> Current reading comprehension models generalise well to in-distribution test sets, yet perform poorly on adversarially selected inputs. Most prior work on adversarial inputs studies oversensitivity: semantically invariant text perturbations that cause a model's prediction to change when it should not. In this work we focus on the complementary problem: excessive prediction undersensitivity, where input text is meaningfully changed but the model's prediction does not, even though it should. We formulate a noisy adversarial attack which searches among semantic variations of the question for which a model erroneously predicts the same answer, and with even higher probability. Despite comprising unanswerable questions, both SQuAD2.0 and NewsQA models are vulnerable to this attack. This indicates that although accurate, models tend to rely on spurious patterns and do not fully consider the information specified in a question. We experiment with data augmentation and adversarial training as defences, and find that both substantially decrease vulnerability to attacks on held out data, as well as held out attack spaces. Addressing undersensitivity also improves results on AddSent and AddOneSent, and models furthermore generalise better when facing train/evaluation distribution mismatch: they are less prone to overly rely on predictive cues present only in the training set, and outperform a conventional model by as much as 10.9% F1.

</details>

<details>

<summary>2020-02-15 22:28:07 - Security of HyperLogLog (HLL) Cardinality Estimation: Vulnerabilities and Protection</summary>

- *Pedro Reviriego, Daniel Ting*

- `2002.06463v1` - [abs](http://arxiv.org/abs/2002.06463v1) - [pdf](http://arxiv.org/pdf/2002.06463v1)

> Count distinct or cardinality estimates are widely used in network monitoring for security. They can be used, for example, to detect the malware spread, network scans, or a denial of service attack. There are many algorithms to estimate cardinality. Among those, HyperLogLog (HLL) has been one of the most widely adopted. HLL is simple, provides good cardinality estimates over a wide range of values, requires a small amount of memory, and allows merging of estimates from different sources. However, as HLL is increasingly used to detect attacks, it can itself become the target of attackers that want to avoid being detected. To the best of our knowledge, the security of HLL has not been studied before. In this letter, we take an initial step in its study by first exposing a vulnerability of HLL that allows an attacker to manipulate its estimate. This shows the importance of designing secure HLL implementations. In the second part of the letter, we propose an efficient protection technique to detect and avoid the HLL manipulation. The results presented strongly suggest that the security of HLL should be further studied given that it is widely adopted in many networking and computing applications.

</details>

<details>

<summary>2020-02-16 00:21:16 - Multi-Task Siamese Neural Network for Improving Replay Attack Detection</summary>

- *Patrick von Platen, Fei Tao, Gokhan Tur*

- `2002.07629v1` - [abs](http://arxiv.org/abs/2002.07629v1) - [pdf](http://arxiv.org/pdf/2002.07629v1)

> Automatic speaker verification systems are vulnerable to audio replay attacks which bypass security by replaying recordings of authorized speakers. Replay attack detection (RA) detection systems built upon Residual Neural Networks (ResNet)s have yielded astonishing results on the public benchmark ASVspoof 2019 Physical Access challenge. With most teams using fine-tuned feature extraction pipelines and model architectures, the generalizability of such systems remains questionable though. In this work, we analyse the effect of discriminative feature learning in a multi-task learning (MTL) setting can have on the generalizability and discriminability of RA detection systems. We use a popular ResNet architecture optimized by the cross-entropy criterion as our baseline and compare it to the same architecture optimized by MTL using Siamese Neural Networks (SNN). It can be shown that SNN outperform the baseline by relative 26.8 % Equal Error Rate (EER). We further enhance the model's architecture and demonstrate that SNN with additional reconstruction loss yield another significant improvement of relative 13.8 % EER.

</details>

<details>

<summary>2020-02-16 02:59:41 - Blind Adversarial Network Perturbations</summary>

- *Milad Nasr, Alireza Bahramali, Amir Houmansadr*

- `2002.06495v1` - [abs](http://arxiv.org/abs/2002.06495v1) - [pdf](http://arxiv.org/pdf/2002.06495v1)

> Deep Neural Networks (DNNs) are commonly used for various traffic analysis problems, such as website fingerprinting and flow correlation, as they outperform traditional (e.g., statistical) techniques by large margins. However, deep neural networks are known to be vulnerable to adversarial examples: adversarial inputs to the model that get labeled incorrectly by the model due to small adversarial perturbations. In this paper, for the first time, we show that an adversary can defeat DNN-based traffic analysis techniques by applying \emph{adversarial perturbations} on the patterns of \emph{live} network traffic.

</details>

<details>

<summary>2020-02-16 08:04:16 - On the Feasibility of Sybil Attacks in Shard-Based Permissionless Blockchains</summary>

- *Tayebeh Rajab, Mohammad Hossein Manshaei, Mohammad Dakhilalian, Murtuza Jadliwala, Mohammad Ashiqur Rahman*

- `2002.06531v1` - [abs](http://arxiv.org/abs/2002.06531v1) - [pdf](http://arxiv.org/pdf/2002.06531v1)

> Bitcoin's single leader consensus protocol (Nakamoto consensus) suffers from significant transaction throughput and network scalability issues due to the computational requirements of it Proof-of-Work (PoW) based leader selection strategy. To overcome this, committee-based approaches (e.g., Elastico) that partition the outstanding transaction set into shards and (randomly) select multiple committees to process these transactions in parallel have been proposed and have become very popular. However, by design these committee or shard-based blockchain solutions are easily vulnerable to the Sybil attacks, where an adversary can easily compromise/manipulate the consensus protocol if it has enough computational power to generate multiple Sybil committee members (by generating multiple valid node identifiers). Despite the straightforward nature of these attacks, they have not been systematically analyzed. In this paper, we fill this research gap by modelling and analyzing Sybil attacks in a representative and popular shard-based protocol called Elastico. We show that the PoW technique used for identifier or ID generation in the initial phase of the protocol is vulnerable to Sybil attacks, and a node with high hash-power can generate enough Sybil IDs to successfully compromise Elastico. We analytically derive conditions for two different categories of Sybil attacks and perform numerical simulations to validate our theoretical results under different network and protocol parameters.

</details>

<details>

<summary>2020-02-16 22:19:32 - Massif: Interactive Interpretation of Adversarial Attacks on Deep Learning</summary>

- *Nilaksh Das, Haekyu Park, Zijie J. Wang, Fred Hohman, Robert Firstman, Emily Rogers, Duen Horng Chau*

- `2001.07769v3` - [abs](http://arxiv.org/abs/2001.07769v3) - [pdf](http://arxiv.org/pdf/2001.07769v3)

> Deep neural networks (DNNs) are increasingly powering high-stakes applications such as autonomous cars and healthcare; however, DNNs are often treated as "black boxes" in such applications. Recent research has also revealed that DNNs are highly vulnerable to adversarial attacks, raising serious concerns over deploying DNNs in the real world. To overcome these deficiencies, we are developing Massif, an interactive tool for deciphering adversarial attacks. Massif identifies and interactively visualizes neurons and their connections inside a DNN that are strongly activated or suppressed by an adversarial attack. Massif provides both a high-level, interpretable overview of the effect of an attack on a DNN, and a low-level, detailed description of the affected neurons. These tightly coupled views in Massif help people better understand which input features are most vulnerable or important for correct predictions.

</details>

<details>

<summary>2020-02-17 13:10:43 - A New Methodology for Information Security Risk Assessment for Medical Devices and Its Evaluation</summary>

- *Tom Mahler, Yuval Elovici, Yuval Shahar*

- `2002.06938v1` - [abs](http://arxiv.org/abs/2002.06938v1) - [pdf](http://arxiv.org/pdf/2002.06938v1)

> As technology advances towards more connected and digital environments, medical devices are becoming increasingly connected to hospital networks and to the Internet, which exposes them, and thus the patients using them, to new cybersecurity threats. Currently, there is a lack of a methodology dedicated to information security risk assessment for medical devices.   In this study, we present the Threat identification, ontology-based Likelihood, severity Decomposition, and Risk integration (TLDR) methodology for information security risk assessment for medical devices. The TLDR methodology uses the following steps: (1) identifying the potentially vulnerable components of medical devices, in this case, four different medical imaging devices (MIDs); (2) identifying the potential attacks, in this case, 23 potential attacks on MIDs; (3) mapping the discovered attacks into a known attack ontology - in this case, the Common Attack Pattern Enumeration and Classifications (CAPECs); (4) estimating the likelihood of the mapped CAPECs in the medical domain with the assistance of a panel of senior healthcare Information Security Experts (ISEs); (5) computing the CAPEC-based likelihood estimates of each attack; (6) decomposing each attack into several severity aspects and assigning them weights; (7) assessing the magnitude of the impact of each of the severity aspects for each attack with the assistance of a panel of senior Medical Experts (MEs); (8) computing the composite severity assessments for each attack; and finally, (9) integrating the likelihood and severity of each attack into its risk, and thus prioritizing it. The details of steps six to eight are beyond the scope of the current study; in the current study, we had replaced them by a single step that included asking the panel of MEs [in this case, radiologists], to assess the overall severity for each attack and use it as its severity...

</details>

<details>

<summary>2020-02-17 18:13:09 - Targeted Forgetting and False Memory Formation in Continual Learners through Adversarial Backdoor Attacks</summary>

- *Muhammad Umer, Glenn Dawson, Robi Polikar*

- `2002.07111v1` - [abs](http://arxiv.org/abs/2002.07111v1) - [pdf](http://arxiv.org/pdf/2002.07111v1)

> Artificial neural networks are well-known to be susceptible to catastrophic forgetting when continually learning from sequences of tasks. Various continual (or "incremental") learning approaches have been proposed to avoid catastrophic forgetting, but they are typically adversary agnostic, i.e., they do not consider the possibility of a malicious attack. In this effort, we explore the vulnerability of Elastic Weight Consolidation (EWC), a popular continual learning algorithm for avoiding catastrophic forgetting. We show that an intelligent adversary can bypass the EWC's defenses, and instead cause gradual and deliberate forgetting by introducing small amounts of misinformation to the model during training. We demonstrate such an adversary's ability to assume control of the model via injection of "backdoor" attack samples on both permuted and split benchmark variants of the MNIST dataset. Importantly, once the model has learned the adversarial misinformation, the adversary can then control the amount of forgetting of any task. Equivalently, the malicious actor can create a "false memory" about any task by inserting carefully-designed backdoor samples to any fraction of the test instances of that task. Perhaps most damaging, we show this vulnerability to be very acute; neural network memory can be easily compromised with the addition of backdoor samples into as little as 1% of the training data of even a single task.

</details>

<details>

<summary>2020-02-17 21:20:01 - On the Need for Topology-Aware Generative Models for Manifold-Based Defenses</summary>

- *Uyeong Jang, Susmit Jha, Somesh Jha*

- `1909.03334v4` - [abs](http://arxiv.org/abs/1909.03334v4) - [pdf](http://arxiv.org/pdf/1909.03334v4)

> Machine-learning (ML) algorithms or models, especially deep neural networks (DNNs), have shown significant promise in several areas. However, researchers have recently demonstrated that ML algorithms, especially DNNs, are vulnerable to adversarial examples (slightly perturbed samples that cause misclassification). The existence of adversarial examples has hindered the deployment of ML algorithms in safety-critical sectors, such as security. Several defenses for adversarial examples exist in the literature. One of the important classes of defenses are manifold-based defenses, where a sample is ``pulled back" into the data manifold before classifying. These defenses rely on the assumption that data lie in a manifold of a lower dimension than the input space. These defenses use a generative model to approximate the input distribution. In this paper, we investigate the following question: do the generative models used in manifold-based defenses need to be topology-aware? We suggest the answer is yes, and we provide theoretical and empirical evidence to support our claim.

</details>

<details>

<summary>2020-02-18 00:39:49 - TensorShield: Tensor-based Defense Against Adversarial Attacks on Images</summary>

- *Negin Entezari, Evangelos E. Papalexakis*

- `2002.10252v1` - [abs](http://arxiv.org/abs/2002.10252v1) - [pdf](http://arxiv.org/pdf/2002.10252v1)

> Recent studies have demonstrated that machine learning approaches like deep neural networks (DNNs) are easily fooled by adversarial attacks. Subtle and imperceptible perturbations of the data are able to change the result of deep neural networks. Leveraging vulnerable machine learning methods raises many concerns especially in domains where security is an important factor. Therefore, it is crucial to design defense mechanisms against adversarial attacks. For the task of image classification, unnoticeable perturbations mostly occur in the high-frequency spectrum of the image. In this paper, we utilize tensor decomposition techniques as a preprocessing step to find a low-rank approximation of images which can significantly discard high-frequency perturbations. Recently a defense framework called Shield could "vaccinate" Convolutional Neural Networks (CNN) against adversarial examples by performing random-quality JPEG compressions on local patches of images on the ImageNet dataset. Our tensor-based defense mechanism outperforms the SLQ method from Shield by 14% against FastGradient Descent (FGSM) adversarial attacks, while maintaining comparable speed.

</details>

<details>

<summary>2020-02-18 03:52:13 - ROBin: Known-Plaintext Attack Resistant Orthogonal Blinding via Channel Randomization</summary>

- *Yanjun Pan, Yao Zheng, Ming Li*

- `2002.07355v1` - [abs](http://arxiv.org/abs/2002.07355v1) - [pdf](http://arxiv.org/pdf/2002.07355v1)

> Orthogonal blinding based schemes for wireless physical layer security aim to achieve secure communication by injecting noise into channels orthogonal to the main channel and corrupting the eavesdropper's signal reception. These methods, albeit practical, have been proven vulnerable against multi-antenna eavesdroppers who can filter the message from the noise. The vulnerability is rooted in the fact that the main channel state remains static in spite of the noise injection, which allows an eavesdropper to estimate it promptly via known symbols and filter out the noise. Our proposed scheme leverages a reconfigurable antenna for Alice to rapidly change the channel state during transmission and a compressive sensing based algorithm for her to predict and cancel the changing effects for Bob. As a result, the communication between Alice and Bob remains clear, whereas randomized channel state prevents Eve from launching the known-plaintext attack. We formally analyze the security of the scheme against both single and multi-antenna eavesdroppers and identify its unique anti-eavesdropping properties due to the artificially created fast-changing channel. We conduct extensive simulations and real-world experiments to evaluate its performance. Empirical results show that our scheme can suppress Eve's attack success rate to the level of random guessing, even if she knows all the symbols transmitted through other antenna modes.

</details>

<details>

<summary>2020-02-18 04:30:46 - Collusion Attacks on Decentralized Attributed-Based Encryption: Analyses and a Solution</summary>

- *Ehsan Meamari, Hao Guo, Chien-Chung Shen, Junbeom Hur*

- `2002.07811v1` - [abs](http://arxiv.org/abs/2002.07811v1) - [pdf](http://arxiv.org/pdf/2002.07811v1)

> Attribute-based Encryption (ABE) is an information centric security solution that moves beyond traditional restrictions of point-to-point encryption by allowing for flexible, fine-grain policy-based and content-based access control that is cryptographically enforced. As the original ABE systems are managed by a single authority, several efforts have decentralized different ABE schemes to address the key escrow problem, where the authority can issue secret keys to itself to decrypt all the ciphertext. However, decentralized ABE (DABE) schemes raise the issue of collusion attacks. In this paper, we review two existing types of collusion attacks on DABE systems, and introduce a new type of collusion among authorities and data users. We show that six existing DABE systems are vulnerable to the newly introduced collusion and propose a model to secure one of the DABE schemes.

</details>

<details>

<summary>2020-02-18 11:11:02 - An Empirical Assessment of Security Risks of Global Android Banking Apps</summary>

- *Sen Chen, Lingling Fan, Guozhu Meng, Ting Su, Minhui Xue, Yinxing Xue, Yang Liu, Lihua Xu*

- `1805.05236v5` - [abs](http://arxiv.org/abs/1805.05236v5) - [pdf](http://arxiv.org/pdf/1805.05236v5)

> Mobile banking apps, belonging to the most security-critical app category, render massive and dynamic transactions susceptible to security risks. Given huge potential financial loss caused by vulnerabilities, existing research lacks a comprehensive empirical study on the security risks of global banking apps to provide useful insights and improve the security of banking apps.   Since data-related weaknesses in banking apps are critical and may directly cause serious financial loss, this paper first revisits the state-of-the-art available tools and finds that they have limited capability in identifying data-related security weaknesses of banking apps. To complement the capability of existing tools in data-related weakness detection, we propose a three-phase automated security risk assessment system, named AUSERA, which leverages static program analysis techniques and sensitive keyword identification. By leveraging AUSERA, we collect 2,157 weaknesses in 693 real-world banking apps across 83 countries, which we use as a basis to conduct a comprehensive empirical study from different aspects, such as global distribution and weakness evolution during version updates. We find that apps owned by subsidiary banks are always less secure than or equivalent to those owned by parent banks. In addition, we also track the patching of weaknesses and receive much positive feedback from banking entities so as to improve the security of banking apps in practice. To date, we highlight that 21 banks have confirmed the weaknesses we reported. We also exchange insights with 7 banks, such as HSBC in UK and OCBC in Singapore, via in-person or online meetings to help them improve their apps. We hope that the insights developed in this paper will inform the communities about the gaps among multiple stakeholders, including banks, academic researchers, and third-party security companies.

</details>

<details>

<summary>2020-02-18 15:18:31 - On the Estimation of Complex Circuits Functional Failure Rate by Machine Learning Techniques</summary>

- *Thomas Lange, Aneesh Balakrishnan, Maximilien Glorieux, Dan Alexandrescu, Luca Sterpone*

- `2002.09945v1` - [abs](http://arxiv.org/abs/2002.09945v1) - [pdf](http://arxiv.org/pdf/2002.09945v1)

> De-Rating or Vulnerability Factors are a major feature of failure analysis efforts mandated by today's Functional Safety requirements. Determining the Functional De-Rating of sequential logic cells typically requires computationally intensive fault-injection simulation campaigns. In this paper a new approach is proposed which uses Machine Learning to estimate the Functional De-Rating of individual flip-flops and thus, optimising and enhancing fault injection efforts. Therefore, first, a set of per-instance features is described and extracted through an analysis approach combining static elements (cell properties, circuit structure, synthesis attributes) and dynamic elements (signal activity). Second, reference data is obtained through first-principles fault simulation approaches. Finally, one part of the reference dataset is used to train the Machine Learning algorithm and the remaining is used to validate and benchmark the accuracy of the trained tool. The intended goal is to obtain a trained model able to provide accurate per-instance Functional De-Rating data for the full list of circuit instances, an objective that is difficult to reach using classical methods. The presented methodology is accompanied by a practical example to determine the performance of various Machine Learning models for different training sizes.

</details>

<details>

<summary>2020-02-18 18:38:54 - Machine Learning to Tackle the Challenges of Transient and Soft Errors in Complex Circuits</summary>

- *Thomas Lange, Aneesh Balakrishnan, Maximilien Glorieux, Dan Alexandrescu, Luca Sterpone*

- `2002.08882v1` - [abs](http://arxiv.org/abs/2002.08882v1) - [pdf](http://arxiv.org/pdf/2002.08882v1)

> The Functional Failure Rate analysis of today's complex circuits is a difficult task and requires a significant investment in terms of human efforts, processing resources and tool licenses. Thereby, de-rating or vulnerability factors are a major instrument of failure analysis efforts. Usually computationally intensive fault-injection simulation campaigns are required to obtain a fine-grained reliability metrics for the functional level. Therefore, the use of machine learning algorithms to assist this procedure and thus, optimising and enhancing fault injection efforts, is investigated in this paper. Specifically, machine learning models are used to predict accurate per-instance Functional De-Rating data for the full list of circuit instances, an objective that is difficult to reach using classical methods. The described methodology uses a set of per-instance features, extracted through an analysis approach, combining static elements (cell properties, circuit structure, synthesis attributes) and dynamic elements (signal activity). Reference data is obtained through first-principles fault simulation approaches. One part of this reference dataset is used to train the machine learning model and the remaining is used to validate and benchmark the accuracy of the trained tool. The presented methodology is applied on a practical example and various machine learning models are evaluated and compared.

</details>

<details>

<summary>2020-02-18 21:48:54 - Towards Query-Efficient Black-Box Adversary with Zeroth-Order Natural Gradient Descent</summary>

- *Pu Zhao, Pin-Yu Chen, Siyue Wang, Xue Lin*

- `2002.07891v1` - [abs](http://arxiv.org/abs/2002.07891v1) - [pdf](http://arxiv.org/pdf/2002.07891v1)

> Despite the great achievements of the modern deep neural networks (DNNs), the vulnerability/robustness of state-of-the-art DNNs raises security concerns in many application domains requiring high reliability. Various adversarial attacks are proposed to sabotage the learning performance of DNN models. Among those, the black-box adversarial attack methods have received special attentions owing to their practicality and simplicity. Black-box attacks usually prefer less queries in order to maintain stealthy and low costs. However, most of the current black-box attack methods adopt the first-order gradient descent method, which may come with certain deficiencies such as relatively slow convergence and high sensitivity to hyper-parameter settings. In this paper, we propose a zeroth-order natural gradient descent (ZO-NGD) method to design the adversarial attacks, which incorporates the zeroth-order gradient estimation technique catering to the black-box attack scenario and the second-order natural gradient descent to achieve higher query efficiency. The empirical evaluations on image classification datasets demonstrate that ZO-NGD can obtain significantly lower model query complexities compared with state-of-the-art attack methods.

</details>

<details>

<summary>2020-02-18 23:14:25 - Block Switching: A Stochastic Approach for Deep Learning Security</summary>

- *Xiao Wang, Siyue Wang, Pin-Yu Chen, Xue Lin, Peter Chin*

- `2002.07920v1` - [abs](http://arxiv.org/abs/2002.07920v1) - [pdf](http://arxiv.org/pdf/2002.07920v1)

> Recent study of adversarial attacks has revealed the vulnerability of modern deep learning models. That is, subtly crafted perturbations of the input can make a trained network with high accuracy produce arbitrary incorrect predictions, while maintain imperceptible to human vision system. In this paper, we introduce Block Switching (BS), a defense strategy against adversarial attacks based on stochasticity. BS replaces a block of model layers with multiple parallel channels, and the active channel is randomly assigned in the run time hence unpredictable to the adversary. We show empirically that BS leads to a more dispersed input gradient distribution and superior defense effectiveness compared with other stochastic defenses such as stochastic activation pruning (SAP). Compared to other defenses, BS is also characterized by the following features: (i) BS causes less test accuracy drop; (ii) BS is attack-independent and (iii) BS is compatible with other defenses and can be used jointly with others.

</details>

<details>

<summary>2020-02-19 13:23:07 - Explainability and Adversarial Robustness for RNNs</summary>

- *Alexander Hartl, Maximilian Bachl, Joachim Fabini, Tanja Zseby*

- `1912.09855v2` - [abs](http://arxiv.org/abs/1912.09855v2) - [pdf](http://arxiv.org/pdf/1912.09855v2)

> Recurrent Neural Networks (RNNs) yield attractive properties for constructing Intrusion Detection Systems (IDSs) for network data. With the rise of ubiquitous Machine Learning (ML) systems, malicious actors have been catching up quickly to find new ways to exploit ML vulnerabilities for profit. Recently developed adversarial ML techniques focus on computer vision and their applicability to network traffic is not straightforward: Network packets expose fewer features than an image, are sequential and impose several constraints on their features.   We show that despite these completely different characteristics, adversarial samples can be generated reliably for RNNs. To understand a classifier's potential for misclassification, we extend existing explainability techniques and propose new ones, suitable particularly for sequential data. Applying them shows that already the first packets of a communication flow are of crucial importance and are likely to be targeted by attackers. Feature importance methods show that even relatively unimportant features can be effectively abused to generate adversarial samples. Since traditional evaluation metrics such as accuracy are not sufficient for quantifying the adversarial threat, we propose the Adversarial Robustness Score (ARS) for comparing IDSs, capturing a common notion of adversarial robustness, and show that an adversarial training procedure can significantly and successfully reduce the attack surface.

</details>

<details>

<summary>2020-02-20 21:39:25 - Enhanced Adversarial Strategically-Timed Attacks against Deep Reinforcement Learning</summary>

- *Chao-Han Huck Yang, Jun Qi, Pin-Yu Chen, Yi Ouyang, I-Te Danny Hung, Chin-Hui Lee, Xiaoli Ma*

- `2002.09027v1` - [abs](http://arxiv.org/abs/2002.09027v1) - [pdf](http://arxiv.org/pdf/2002.09027v1)

> Recent deep neural networks based techniques, especially those equipped with the ability of self-adaptation in the system level such as deep reinforcement learning (DRL), are shown to possess many advantages of optimizing robot learning systems (e.g., autonomous navigation and continuous robot arm control.) However, the learning-based systems and the associated models may be threatened by the risks of intentionally adaptive (e.g., noisy sensor confusion) and adversarial perturbations from real-world scenarios. In this paper, we introduce timing-based adversarial strategies against a DRL-based navigation system by jamming in physical noise patterns on the selected time frames. To study the vulnerability of learning-based navigation systems, we propose two adversarial agent models: one refers to online learning; another one is based on evolutionary learning. Besides, three open-source robot learning and navigation control environments are employed to study the vulnerability under adversarial timing attacks. Our experimental results show that the adversarial timing attacks can lead to a significant performance drop, and also suggest the necessity of enhancing the robustness of robot learning systems.

</details>

<details>

<summary>2020-02-21 00:14:44 - Optimizing Vulnerability-Driven Honey Traffic Using Game Theory</summary>

- *Iffat Anjum, Mohammad Sujan Miah, Mu Zhu, Nazia Sharmin, Christopher Kiekintveld, William Enck, Munindar P Singh*

- `2002.09069v1` - [abs](http://arxiv.org/abs/2002.09069v1) - [pdf](http://arxiv.org/pdf/2002.09069v1)

> Enterprises are increasingly concerned about adversaries that slowly and deliberately exploit resources over the course of months or even years. A key step in this kill chain is network reconnaissance, which has historically been active (e.g., network scans) and therefore detectable. However, new networking technology increases the possibility of passive network reconnaissance, which will be largely undetectable by defenders. In this paper, we propose Snaz, a technique that uses deceptively crafted honey traffic to confound the knowledge gained through passive network reconnaissance. We present a two-player non-zero-sum Stackelberg game model that characterizes how a defender should deploy honey traffic in the presence of an adversary who is aware of Snaz. In doing so, we demonstrate the existence of optimal defender strategies that will either dissuade an adversary from acting on the existence of real vulnerabilities observed within network traffic, or reveal the adversary's presence when it attempts to unknowingly attack an intrusion detection node.

</details>

<details>

<summary>2020-02-21 17:13:37 - Robustness from Simple Classifiers</summary>

- *Sharon Qian, Dimitris Kalimeris, Gal Kaplun, Yaron Singer*

- `2002.09422v1` - [abs](http://arxiv.org/abs/2002.09422v1) - [pdf](http://arxiv.org/pdf/2002.09422v1)

> Despite the vast success of Deep Neural Networks in numerous application domains, it has been shown that such models are not robust i.e., they are vulnerable to small adversarial perturbations of the input. While extensive work has been done on why such perturbations occur or how to successfully defend against them, we still do not have a complete understanding of robustness. In this work, we investigate the connection between robustness and simplicity. We find that simpler classifiers, formed by reducing the number of output classes, are less susceptible to adversarial perturbations. Consequently, we demonstrate that decomposing a complex multiclass model into an aggregation of binary models enhances robustness. This behavior is consistent across different datasets and model architectures and can be combined with known defense techniques such as adversarial training. Moreover, we provide further evidence of a disconnect between standard and robust learning regimes. In particular, we show that elaborate label information can help standard accuracy but harm robustness.

</details>

<details>

<summary>2020-02-22 00:28:41 - Polarizing Front Ends for Robust CNNs</summary>

- *Can Bakiskan, Soorya Gopalakrishnan, Metehan Cekic, Upamanyu Madhow, Ramtin Pedarsani*

- `2002.09580v1` - [abs](http://arxiv.org/abs/2002.09580v1) - [pdf](http://arxiv.org/pdf/2002.09580v1)

> The vulnerability of deep neural networks to small, adversarially designed perturbations can be attributed to their "excessive linearity." In this paper, we propose a bottom-up strategy for attenuating adversarial perturbations using a nonlinear front end which polarizes and quantizes the data. We observe that ideal polarization can be utilized to completely eliminate perturbations, develop algorithms to learn approximately polarizing bases for data, and investigate the effectiveness of the proposed strategy on the MNIST and Fashion MNIST datasets.

</details>

<details>

<summary>2020-02-22 05:25:07 - An Empirical Study of Android Security Bulletins in Different Vendors</summary>

- *Sadegh Farhang, Mehmet Bahadir Kirdan, Aron Laszka, Jens Grossklags*

- `2002.09629v1` - [abs](http://arxiv.org/abs/2002.09629v1) - [pdf](http://arxiv.org/pdf/2002.09629v1)

> Mobile devices encroach on almost every part of our lives, including work and leisure, and contain a wealth of personal and sensitive information. It is, therefore, imperative that these devices uphold high security standards. A key aspect is the security of the underlying operating system. In particular, Android plays a critical role due to being the most dominant platform in the mobile ecosystem with more than one billion active devices and due to its openness, which allows vendors to adopt and customize it. Similar to other platforms, Android maintains security by providing monthly security patches and announcing them via the Android security bulletin. To absorb this information successfully across the Android ecosystem, impeccable coordination by many different vendors is required.   In this paper, we perform a comprehensive study of 3,171 Android-related vulnerabilities and study to which degree they are reflected in the Android security bulletin, as well as in the security bulletins of three leading vendors: Samsung, LG, and Huawei. In our analysis, we focus on the metadata of these security bulletins (e.g., timing, affected layers, severity, and CWE data) to better understand the similarities and differences among vendors. We find that (i) the studied vendors in the Android ecosystem have adopted different structures for vulnerability reporting, (ii) vendors are less likely to react with delay for CVEs with Android Git repository references, (iii) vendors handle Qualcomm-related CVEs differently from the rest of external layer CVEs.

</details>

<details>

<summary>2020-02-22 09:50:39 - Network Cooperation with Progressive Disambiguation for Partial Label Learning</summary>

- *Yao Yao, Chen Gong, Jiehui Deng, Jian Yang*

- `2002.11919v1` - [abs](http://arxiv.org/abs/2002.11919v1) - [pdf](http://arxiv.org/pdf/2002.11919v1)

> Partial Label Learning (PLL) aims to train a classifier when each training instance is associated with a set of candidate labels, among which only one is correct but is not accessible during the training phase. The common strategy dealing with such ambiguous labeling information is to disambiguate the candidate label sets. Nonetheless, existing methods ignore the disambiguation difficulty of instances and adopt the single-trend training mechanism. The former would lead to the vulnerability of models to the false positive labels and the latter may arouse error accumulation problem. To remedy these two drawbacks, this paper proposes a novel approach termed "Network Cooperation with Progressive Disambiguation" (NCPD) for PLL. Specifically, we devise a progressive disambiguation strategy of which the disambiguation operations are performed on simple instances firstly and then gradually on more complicated ones. Therefore, the negative impacts brought by the false positive labels of complicated instances can be effectively mitigated as the disambiguation ability of the model has been strengthened via learning from the simple instances. Moreover, by employing artificial neural networks as the backbone, we utilize a network cooperation mechanism which trains two networks collaboratively by letting them interact with each other. As two networks have different disambiguation ability, such interaction is beneficial for both networks to reduce their respective disambiguation errors, and thus is much better than the existing algorithms with single-trend training process. Extensive experimental results on various benchmark and practical datasets demonstrate the superiority of our NCPD to other state-of-the-art PLL methods.

</details>

<details>

<summary>2020-02-22 21:13:00 - Non-Intrusive Detection of Adversarial Deep Learning Attacks via Observer Networks</summary>

- *Kirthi Shankar Sivamani, Rajeev Sahay, Aly El Gamal*

- `2002.09772v1` - [abs](http://arxiv.org/abs/2002.09772v1) - [pdf](http://arxiv.org/pdf/2002.09772v1)

> Recent studies have shown that deep learning models are vulnerable to specifically crafted adversarial inputs that are quasi-imperceptible to humans. In this letter, we propose a novel method to detect adversarial inputs, by augmenting the main classification network with multiple binary detectors (observer networks) which take inputs from the hidden layers of the original network (convolutional kernel outputs) and classify the input as clean or adversarial. During inference, the detectors are treated as a part of an ensemble network and the input is deemed adversarial if at least half of the detectors classify it as so. The proposed method addresses the trade-off between accuracy of classification on clean and adversarial samples, as the original classification network is not modified during the detection process. The use of multiple observer networks makes attacking the detection mechanism non-trivial even when the attacker is aware of the victim classifier. We achieve a 99.5% detection accuracy on the MNIST dataset and 97.5% on the CIFAR-10 dataset using the Fast Gradient Sign Attack in a semi-white box setup. The number of false positive detections is a mere 0.12% in the worst case scenario.

</details>

<details>

<summary>2020-02-23 20:32:38 - Comparing the Effects of DNS, DoT, and DoH on Web Performance</summary>

- *Austin Hounsel, Kevin Borgolte, Paul Schmitt, Jordan Holland, Nick Feamster*

- `1907.08089v3` - [abs](http://arxiv.org/abs/1907.08089v3) - [pdf](http://arxiv.org/pdf/1907.08089v3)

> Nearly every service on the Internet relies on the Domain Name System (DNS), which translates a human-readable name to an IP address before two endpoints can communicate. Today, DNS traffic is unencrypted, leaving users vulnerable to eavesdropping and tampering. Past work has demonstrated that DNS queries can reveal a user's browsing history and even what smart devices they are using at home. In response to these privacy concerns, two new protocols have been proposed: DNS-over-HTTPS (DoH) and DNS-over-TLS (DoT). Instead of sending DNS queries and responses in the clear, DoH and DoT establish encrypted connections between users and resolvers. By doing so, these protocols provide privacy and security guarantees that traditional DNS (Do53) lacks.   In this paper, we measure the effect of Do53, DoT, and DoH on query response times and page load times from five global vantage points. We find that although DoH and DoT response times are generally higher than Do53, both protocols can perform better than Do53 in terms of page load times. However, as throughput decreases and substantial packet loss and latency are introduced, web pages load fastest with Do53. Additionally, web pages successfully load more often with Do53 and DoT than DoH. Based on these results, we provide several recommendations to improve DNS performance, such as opportunistic partial responses and wire format caching.

</details>

<details>

<summary>2020-02-24 05:47:08 - Utilizing a null class to restrict decision spaces and defend against neural network adversarial attacks</summary>

- *Matthew J. Roos*

- `2002.10084v1` - [abs](http://arxiv.org/abs/2002.10084v1) - [pdf](http://arxiv.org/pdf/2002.10084v1)

> Despite recent progress, deep neural networks generally continue to be vulnerable to so-called adversarial examples--input images with small perturbations that can result in changes in the output classifications, despite no such change in the semantic meaning to human viewers. This is true even for seemingly simple challenges such as the MNIST digit classification task. In part, this suggests that these networks are not relying on the same set of object features as humans use to make these classifications. In this paper we examine an additional, and largely unexplored, cause behind this phenomenon--namely, the use of the conventional training paradigm in which the entire input space is parcellated among the training classes. Owing to this paradigm, learned decision spaces for individual classes span excessively large regions of the input space and include images that have no semantic similarity to images in the training set. In this study, we train models that include a null class. That is, models may "opt-out" of classifying an input image as one of the digit classes. During training, null images are created through a variety of methods, in an attempt to create tighter and more semantically meaningful decision spaces for the digit classes. The best performing models classify nearly all adversarial examples as nulls, rather than mistaking them as a member of an incorrect digit class, while simultaneously maintaining high accuracy on the unperturbed test set. The use of a null class and the training paradigm presented herein may provide an effective defense against adversarial attacks for some applications. Code for replicating this study will be made available at https://github.com/mattroos/null_class_adversarial_defense .

</details>

<details>

<summary>2020-02-24 15:50:49 - A Multimodal Deep Network for the Reconstruction of T2W MR Images</summary>

- *Antonio Falvo, Danilo Comminiello, Simone Scardapane, Michele Scarpiniti, Aurelio Uncini*

- `1908.03009v2` - [abs](http://arxiv.org/abs/1908.03009v2) - [pdf](http://arxiv.org/pdf/1908.03009v2)

> Multiple sclerosis is one of the most common chronic neurological diseases affecting the central nervous system. Lesions produced by the MS can be observed through two modalities of magnetic resonance (MR), known as T2W and FLAIR sequences, both providing useful information for formulating a diagnosis. However, long acquisition time makes the acquired MR image vulnerable to motion artifacts. This leads to the need of accelerating the execution of the MR analysis. In this paper, we present a deep learning method that is able to reconstruct subsampled MR images obtained by reducing the k-space data, while maintaining a high image quality that can be used to observe brain lesions. The proposed method exploits the multimodal approach of neural networks and it also focuses on the data acquisition and processing stages to reduce execution time of the MR analysis. Results prove the effectiveness of the proposed method in reconstructing subsampled MR images while saving execution time.

</details>

<details>

<summary>2020-02-24 17:23:06 - Spatial-Temporal Moving Target Defense: A Markov Stackelberg Game Model</summary>

- *Henger Li, Wen Shen, Zizhan Zheng*

- `2002.10390v1` - [abs](http://arxiv.org/abs/2002.10390v1) - [pdf](http://arxiv.org/pdf/2002.10390v1)

> Moving target defense has emerged as a critical paradigm of protecting a vulnerable system against persistent and stealthy attacks. To protect a system, a defender proactively changes the system configurations to limit the exposure of security vulnerabilities to potential attackers. In doing so, the defender creates asymmetric uncertainty and complexity for the attackers, making it much harder for them to compromise the system. In practice, the defender incurs a switching cost for each migration of the system configurations. The switching cost usually depends on both the current configuration and the following configuration. Besides, different system configurations typically require a different amount of time for an attacker to exploit and attack. Therefore, a defender must simultaneously decide both the optimal sequences of system configurations and the optimal timing for switching. In this paper, we propose a Markov Stackelberg Game framework to precisely characterize the defender's spatial and temporal decision-making in the face of advanced attackers. We introduce a relative value iteration algorithm that computes the defender's optimal moving target defense strategies. Empirical evaluation on real-world problems demonstrates the advantages of the Markov Stackelberg game model for spatial-temporal moving target defense.

</details>

<details>

<summary>2020-02-24 19:01:47 - Precise Tradeoffs in Adversarial Training for Linear Regression</summary>

- *Adel Javanmard, Mahdi Soltanolkotabi, Hamed Hassani*

- `2002.10477v1` - [abs](http://arxiv.org/abs/2002.10477v1) - [pdf](http://arxiv.org/pdf/2002.10477v1)

> Despite breakthrough performance, modern learning models are known to be highly vulnerable to small adversarial perturbations in their inputs. While a wide variety of recent \emph{adversarial training} methods have been effective at improving robustness to perturbed inputs (robust accuracy), often this benefit is accompanied by a decrease in accuracy on benign inputs (standard accuracy), leading to a tradeoff between often competing objectives. Complicating matters further, recent empirical evidence suggest that a variety of other factors (size and quality of training data, model size, etc.) affect this tradeoff in somewhat surprising ways. In this paper we provide a precise and comprehensive understanding of the role of adversarial training in the context of linear regression with Gaussian features. In particular, we characterize the fundamental tradeoff between the accuracies achievable by any algorithm regardless of computational power or size of the training data. Furthermore, we precisely characterize the standard/robust accuracy and the corresponding tradeoff achieved by a contemporary mini-max adversarial training approach in a high-dimensional regime where the number of data points and the parameters of the model grow in proportion to each other. Our theory for adversarial training algorithms also facilitates the rigorous study of how a variety of factors (size and quality of training data, model overparametrization etc.) affect the tradeoff between these two competing accuracies.

</details>

<details>

<summary>2020-02-25 02:41:42 - Adversarial Perturbations Prevail in the Y-Channel of the YCbCr Color Space</summary>

- *Camilo Pestana, Naveed Akhtar, Wei Liu, David Glance, Ajmal Mian*

- `2003.00883v1` - [abs](http://arxiv.org/abs/2003.00883v1) - [pdf](http://arxiv.org/pdf/2003.00883v1)

> Deep learning offers state of the art solutions for image recognition. However, deep models are vulnerable to adversarial perturbations in images that are subtle but significantly change the model's prediction. In a white-box attack, these perturbations are generally learned for deep models that operate on RGB images and, hence, the perturbations are equally distributed in the RGB color space. In this paper, we show that the adversarial perturbations prevail in the Y-channel of the YCbCr space. Our finding is motivated from the fact that the human vision and deep models are more responsive to shape and texture rather than color. Based on our finding, we propose a defense against adversarial images. Our defence, coined ResUpNet, removes perturbations only from the Y-channel by exploiting ResNet features in an upsampling framework without the need for a bottleneck. At the final stage, the untouched CbCr-channels are combined with the refined Y-channel to restore the clean image. Note that ResUpNet is model agnostic as it does not modify the DNN structure. ResUpNet is trained end-to-end in Pytorch and the results are compared to existing defence techniques in the input transformation category. Our results show that our approach achieves the best balance between defence against adversarial attacks such as FGSM, PGD and DDN and maintaining the original accuracies of VGG-16, ResNet50 and DenseNet121 on clean images. We perform another experiment to show that learning adversarial perturbations only for the Y-channel results in higher fooling rates for the same perturbation magnitude.

</details>

<details>

<summary>2020-02-25 11:07:36 - HarDNN: Feature Map Vulnerability Evaluation in CNNs</summary>

- *Abdulrahman Mahmoud, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, Stephen W. Keckler*

- `2002.09786v2` - [abs](http://arxiv.org/abs/2002.09786v2) - [pdf](http://arxiv.org/pdf/2002.09786v2)

> As Convolutional Neural Networks (CNNs) are increasingly being employed in safety-critical applications, it is important that they behave reliably in the face of hardware errors. Transient hardware errors may percolate undesirable state during execution, resulting in software-manifested errors which can adversely affect high-level decision making. This paper presents HarDNN, a software-directed approach to identify vulnerable computations during a CNN inference and selectively protect them based on their propensity towards corrupting the inference output in the presence of a hardware error. We show that HarDNN can accurately estimate relative vulnerability of a feature map (fmap) in CNNs using a statistical error injection campaign, and explore heuristics for fast vulnerability assessment. Based on these results, we analyze the tradeoff between error coverage and computational overhead that the system designers can use to employ selective protection. Results show that the improvement in resilience for the added computation is superlinear with HarDNN. For example, HarDNN improves SqueezeNet's resilience by 10x with just 30% additional computations.

</details>

<details>

<summary>2020-02-26 02:53:52 - Is the OWASP Top 10 list comprehensive enough for writing secure code?</summary>

- *Parth Sane*

- `2002.11269v1` - [abs](http://arxiv.org/abs/2002.11269v1) - [pdf](http://arxiv.org/pdf/2002.11269v1)

> The OWASP Top 10 is a list that is published by the Open Web Application Security Project (OWASP). The general purpose is to serve as a watchlist for bugs to avoid while writing code. This paper compares how many of those weakness as described in the top ten list are actually reported in vulnerabilities listed in the National Vulnerability Database (NVD). That way it makes it possible to empirically show whether the OWASP Top 10 list is comprehensive enough or not, for code weaknesses that have been found in the past decade.

</details>

<details>

<summary>2020-02-26 15:19:28 - PointTrackNet: An End-to-End Network For 3-D Object Detection and Tracking From Point Clouds</summary>

- *Sukai Wang, Yuxiang Sun, Chengju Liu, Ming Liu*

- `2002.11559v1` - [abs](http://arxiv.org/abs/2002.11559v1) - [pdf](http://arxiv.org/pdf/2002.11559v1)

> Recent machine learning-based multi-object tracking (MOT) frameworks are becoming popular for 3-D point clouds. Most traditional tracking approaches use filters (e.g., Kalman filter or particle filter) to predict object locations in a time sequence, however, they are vulnerable to extreme motion conditions, such as sudden braking and turning. In this letter, we propose PointTrackNet, an end-to-end 3-D object detection and tracking network, to generate foreground masks, 3-D bounding boxes, and point-wise tracking association displacements for each detected object. The network merely takes as input two adjacent point-cloud frames. Experimental results on the KITTI tracking dataset show competitive results over the state-of-the-arts, especially in the irregularly and rapidly changing scenarios.

</details>

<details>

<summary>2020-02-26 22:12:36 - Improving Robustness of Deep-Learning-Based Image Reconstruction</summary>

- *Ankit Raj, Yoram Bresler, Bo Li*

- `2002.11821v1` - [abs](http://arxiv.org/abs/2002.11821v1) - [pdf](http://arxiv.org/pdf/2002.11821v1)

> Deep-learning-based methods for different applications have been shown vulnerable to adversarial examples. These examples make deployment of such models in safety-critical tasks questionable. Use of deep neural networks as inverse problem solvers has generated much excitement for medical imaging including CT and MRI, but recently a similar vulnerability has also been demonstrated for these tasks. We show that for such inverse problem solvers, one should analyze and study the effect of adversaries in the measurement-space, instead of the signal-space as in previous work. In this paper, we propose to modify the training strategy of end-to-end deep-learning-based inverse problem solvers to improve robustness. We introduce an auxiliary network to generate adversarial examples, which is used in a min-max formulation to build robust image reconstruction networks. Theoretically, we show for a linear reconstruction scheme the min-max formulation results in a singular-value(s) filter regularized solution, which suppresses the effect of adversarial examples occurring because of ill-conditioning in the measurement matrix. We find that a linear network using the proposed min-max learning scheme indeed converges to the same solution. In addition, for non-linear Compressed Sensing (CS) reconstruction using deep networks, we show significant improvement in robustness using the proposed approach over other methods. We complement the theory by experiments for CS on two different datasets and evaluate the effect of increasing perturbations on trained networks. We find the behavior for ill-conditioned and well-conditioned measurement matrices to be qualitatively different.

</details>

<details>

<summary>2020-02-27 02:35:08 - Defense-PointNet: Protecting PointNet Against Adversarial Attacks</summary>

- *Yu Zhang, Gongbo Liang, Tawfiq Salem, Nathan Jacobs*

- `2002.11881v1` - [abs](http://arxiv.org/abs/2002.11881v1) - [pdf](http://arxiv.org/pdf/2002.11881v1)

> Despite remarkable performance across a broad range of tasks, neural networks have been shown to be vulnerable to adversarial attacks. Many works focus on adversarial attacks and defenses on 2D images, but few focus on 3D point clouds. In this paper, our goal is to enhance the adversarial robustness of PointNet, which is one of the most widely used models for 3D point clouds. We apply the fast gradient sign attack method (FGSM) on 3D point clouds and find that FGSM can be used to generate not only adversarial images but also adversarial point clouds. To minimize the vulnerability of PointNet to adversarial attacks, we propose Defense-PointNet. We compare our model with two baseline approaches and show that Defense-PointNet significantly improves the robustness of the network against adversarial samples.

</details>

<details>

<summary>2020-02-27 16:36:19 - QoS-aware Stochastic Spatial PLS Model for Analysing Secrecy Performance under Eavesdropping and Jamming</summary>

- *Bhawna Ahuja, Deepak Mishra, Ranjan Bose*

- `2001.11664v2` - [abs](http://arxiv.org/abs/2001.11664v2) - [pdf](http://arxiv.org/pdf/2001.11664v2)

> Securing wireless communication, being inherently vulnerable to eavesdropping and jamming attacks, becomes more challenging in resource-constrained networks like Internet-of-Things. Towards this, physical layer security (PLS) has gained significant attention due to its low complexity. In this paper, we address the issue of random inter-node distances in secrecy analysis and develop a comprehensive quality-of-service (QoS) aware PLS framework for the analysis of both eavesdropping and jamming capabilities of attacker. The proposed solution covers spatially stochastic deployment of legitimate nodes and attacker. We characterise the secrecy outage performance against both attacks using inter-node distance based probabilistic distribution functions. The model takes into account the practical limits arising out of underlying QoS requirements, which include the maximum distance between legitimate users driven by transmit power and receiver sensitivity. A novel concept of eavesdropping zone is introduced, and relative impact of jamming power is investigated. Closed-form expressions for asymptotic secrecy outage probability are derived offering insights into design of optimal system parameters for desired security level against the attacker's capability of both attacks. Analytical framework, validated by numerical results, establishes that the proposed solution offers potentially accurate characterisation of the PLS performance and key design perspective from point-of-view of both legitimate user and attacker.

</details>

<details>

<summary>2020-02-27 19:00:01 - On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping</summary>

- *Sanghyun Hong, Varun Chandrasekaran, Yiğitcan Kaya, Tudor Dumitraş, Nicolas Papernot*

- `2002.11497v2` - [abs](http://arxiv.org/abs/2002.11497v2) - [pdf](http://arxiv.org/pdf/2002.11497v2)

> Machine learning algorithms are vulnerable to data poisoning attacks. Prior taxonomies that focus on specific scenarios, e.g., indiscriminate or targeted, have enabled defenses for the corresponding subset of known attacks. Yet, this introduces an inevitable arms race between adversaries and defenders. In this work, we study the feasibility of an attack-agnostic defense relying on artifacts that are common to all poisoning attacks. Specifically, we focus on a common element between all attacks: they modify gradients computed to train the model. We identify two main artifacts of gradients computed in the presence of poison: (1) their $\ell_2$ norms have significantly higher magnitudes than those of clean gradients, and (2) their orientation differs from clean gradients. Based on these observations, we propose the prerequisite for a generic poisoning defense: it must bound gradient magnitudes and minimize differences in orientation. We call this gradient shaping. As an exemplar tool to evaluate the feasibility of gradient shaping, we use differentially private stochastic gradient descent (DP-SGD), which clips and perturbs individual gradients during training to obtain privacy guarantees. We find that DP-SGD, even in configurations that do not result in meaningful privacy guarantees, increases the model's robustness to indiscriminate attacks. It also mitigates worst-case targeted attacks and increases the adversary's cost in multi-poison scenarios. The only attack we find DP-SGD to be ineffective against is a strong, yet unrealistic, indiscriminate attack. Our results suggest that, while we currently lack a generic poisoning defense, gradient shaping is a promising direction for future research.

</details>

<details>

<summary>2020-02-27 22:43:49 - Enhancing Adversarial Example Transferability with an Intermediate Level Attack</summary>

- *Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, Ser-Nam Lim*

- `1907.10823v3` - [abs](http://arxiv.org/abs/1907.10823v3) - [pdf](http://arxiv.org/pdf/1907.10823v3)

> Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples are typically overfit to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. We introduce the Intermediate Level Attack (ILA), which attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model, improving upon state-of-the-art methods. We show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability. Additionally, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples using intermediate feature maps. Our code is available at https://github.com/CUVL/Intermediate-Level-Attack.

</details>

<details>

<summary>2020-02-28 15:13:08 - Social and Child Care Provision in Kinship Networks: an Agent-Based Model</summary>

- *Umberto Gostoli, Eric Silverman*

- `2002.05188v2` - [abs](http://arxiv.org/abs/2002.05188v2) - [pdf](http://arxiv.org/pdf/2002.05188v2)

> Providing for the needs of the vulnerable is a critical component of social and health policy-making. In particular, caring for children and for vulnerable older people is vital to the wellbeing of millions of families throughout the world. In most developed countries, this care is provided through both formal and informal means, and is therefore governed by complex policies that interact in non-obvious ways with other areas of policy-making. In this paper we present an agent-based model of social and child care provision in the UK, in which agents can provide informal care or pay for private care for their relatives. Agents make care decisions based on numerous factors including their health status, employment, financial situation, and social and physical distance to those in need. Simulation results show that the model can produce plausible patterns of care need and availability, and therefore can provide an important aid to this complex area of policy-making. We conclude that the model's use of kinship networks for distributing care and the explicit modelling of interactions between social care and child care will enable policy-makers to develop more informed policy interventions in these critical areas.

</details>

<details>

<summary>2020-02-29 04:11:27 - DangKiller: Eliminating Dangling Pointers Efficiently via Implicit Identifier</summary>

- *Daliang Xu, Dongwei Chen, Chun Yang, KangSun, Xu Cheng, Dong Tong*

- `2003.00175v1` - [abs](http://arxiv.org/abs/2003.00175v1) - [pdf](http://arxiv.org/pdf/2003.00175v1)

> Use-After-Free vulnerabilities, allowing the attacker to access unintended memory via dangling pointers, are more threatening. However, most detection schemes can only detect dangling pointers and invalid them, but not provide a tolerance mechanism to repair the errors at runtime. Also, these techniques obtain and manage the metadata inefficiently with complex structures and too much scan (sweep). The goal of this paper is to use compiler instrumentation to eliminate dangling pointers automatically and efficiently. In this paper, we observe that most techniques lack accurate efficient pointer graph metadata maintaining methods, so they need to scan the log to reduce the redundancy and sweep the whole address space to find dangling pointers. Also, they lack a direct, efficiently obtaining metadata approach. The key insight of this paper is that a unique identifier can be used as a key to a hash or direct-map algorithm. Thus, this paper maintains the same implicit identifier with each memory object and its corresponding referent. Associating the unique ID with metadata for memory objects, obtaining and managing the pointer graph metadata can be efficiently. Therefore, with the delayed free technique adopted into C/C++, we present the DangKiller as a novel and lightweight dangling pointer elimination solution. We first demonstrate the MinFat Pointer, which can calculate unique implicit ID for each object and pointer quickly, and use hash algorithm to obtain metadata. Secondly, we propose the Log Cache and Log Compression mechanism based on the ID to decrease the redundancy of dangling pointer candidates. Coupled with the Address Tagging architecture on an ARM64 system, our experiments show that the DangKiller can eliminate use-after-free vulnerabilities at only 11% and 3% runtime overheads for the SPEC CPU2006 and 2017 benchmarks respectively, except for unique cases.

</details>

<details>

<summary>2020-02-29 06:56:36 - Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices</summary>

- *Vincent S. Chen, Sen Wu, Zhenzhen Weng, Alexander Ratner, Christopher Ré*

- `1909.06349v2` - [abs](http://arxiv.org/abs/1909.06349v2) - [pdf](http://arxiv.org/pdf/1909.06349v2)

> In real-world machine learning applications, data subsets correspond to especially critical outcomes: vulnerable cyclist detections are safety-critical in an autonomous driving task, and "question" sentences might be important to a dialogue agent's language understanding for product purposes. While machine learning models can achieve high quality performance on coarse-grained metrics like F1-score and overall accuracy, they may underperform on critical subsets---we define these as slices, the key abstraction in our approach. To address slice-level performance, practitioners often train separate "expert" models on slice subsets or use multi-task hard parameter sharing. We propose Slice-based Learning, a new programming model in which the slicing function (SF), a programming interface, specifies critical data subsets for which the model should commit additional capacity. Any model can leverage SFs to learn slice expert representations, which are combined with an attention mechanism to make slice-aware predictions. We show that our approach maintains a parameter-efficient representation while improving over baselines by up to 19.0 F1 on slices and 4.6 F1 overall on datasets spanning language understanding (e.g. SuperGLUE), computer vision, and production-scale industrial systems.

</details>

<details>

<summary>2020-02-29 07:44:14 - Can Machine Learning Model with Static Features be Fooled: an Adversarial Machine Learning Approach</summary>

- *Rahim Taheri, Reza Javidan, Mohammad Shojafar, Vinod P, Mauro Conti*

- `1904.09433v2` - [abs](http://arxiv.org/abs/1904.09433v2) - [pdf](http://arxiv.org/pdf/1904.09433v2)

> The widespread adoption of smartphones dramatically increases the risk of attacks and the spread of mobile malware, especially on the Android platform. Machine learning-based solutions have been already used as a tool to supersede signature-based anti-malware systems. However, malware authors leverage features from malicious and legitimate samples to estimate statistical difference in-order to create adversarial examples. Hence, to evaluate the vulnerability of machine learning algorithms in malware detection, we propose five different attack scenarios to perturb malicious applications (apps). By doing this, the classification algorithm inappropriately fits the discriminant function on the set of data points, eventually yielding a higher misclassification rate. Further, to distinguish the adversarial examples from benign samples, we propose two defense mechanisms to counter attacks. To validate our attacks and solutions, we test our model on three different benchmark datasets. We also test our methods using various classifier algorithms and compare them with the state-of-the-art data poisoning method using the Jacobian matrix. Promising results show that generated adversarial samples can evade detection with a very high probability. Additionally, evasive variants generated by our attack models when used to harden the developed anti-malware system improves the detection rate up to 50% when using the Generative Adversarial Network (GAN) method.

</details>

<details>

<summary>2020-02-29 09:55:45 - Comparison of Distal Teacher Learning with Numerical and Analytical Methods to Solve Inverse Kinematics for Rigid-Body Mechanisms</summary>

- *Tim von Oehsen, Alexander Fabisch, Shivesh Kumar, Frank Kirchner*

- `2003.00225v1` - [abs](http://arxiv.org/abs/2003.00225v1) - [pdf](http://arxiv.org/pdf/2003.00225v1)

> Several publications are concerned with learning inverse kinematics, however, their evaluation is often limited and none of the proposed methods is of practical relevance for rigid-body kinematics with a known forward model. We argue that for rigid-body kinematics one of the first proposed machine learning (ML) solutions to inverse kinematics -- distal teaching (DT) -- is actually good enough when combined with differentiable programming libraries and we provide an extensive evaluation and comparison to analytical and numerical solutions. In particular, we analyze solve rate, accuracy, sample efficiency and scalability. Further, we study how DT handles joint limits, singularities, unreachable poses, trajectories and provide a comparison of execution times. The three approaches are evaluated on three different rigid body mechanisms with varying complexity. With enough training data and relaxed precision requirements, DT has a better solve rate and is faster than state-of-the-art numerical solvers for a 15-DoF mechanism. DT is not affected by singularities while numerical solutions are vulnerable to them. In all other cases numerical solutions are usually better. Analytical solutions outperform the other approaches by far if they are available.

</details>


## 2020-03

<details>

<summary>2020-03-02 22:27:33 - Logic Bugs in IoT Platforms and Systems: A Review</summary>

- *Wei Zhou, Chen Cao, Dongdong Huo, Kai Cheng, Lan Zhang, Le Guan, Tao Liu, Yaowen Zheng, Yuqing Zhang, Limin Sun, Yazhe Wang, Peng Liu*

- `1912.13410v2` - [abs](http://arxiv.org/abs/1912.13410v2) - [pdf](http://arxiv.org/pdf/1912.13410v2)

> In recent years, IoT platforms and systems have been rapidly emerging. Although IoT is a new technology, new does not mean simpler (than existing networked systems). Contrarily, the complexity (of IoT platforms and systems) is actually being increased in terms of the interactions between the physical world and cyberspace. The increased complexity indeed results in new vulnerabilities. This paper seeks to provide a review of the recently discovered logic bugs that are specific to IoT platforms and systems. In particular, 17 logic bugs and one weakness falling into seven categories of vulnerabilities are reviewed in this survey.

</details>

<details>

<summary>2020-03-03 06:39:00 - Novelty Detection Via Blurring</summary>

- *Sungik Choi, Sae-Young Chung*

- `1911.11943v3` - [abs](http://arxiv.org/abs/1911.11943v3) - [pdf](http://arxiv.org/pdf/1911.11943v3)

> Conventional out-of-distribution (OOD) detection schemes based on variational autoencoder or Random Network Distillation (RND) have been observed to assign lower uncertainty to the OOD than the target distribution. In this work, we discover that such conventional novelty detection schemes are also vulnerable to the blurred images. Based on the observation, we construct a novel RND-based OOD detector, SVD-RND, that utilizes blurred images during training. Our detector is simple, efficient at test time, and outperforms baseline OOD detectors in various domains. Further results show that SVD-RND learns better target distribution representation than the baseline RND algorithm. Finally, SVD-RND combined with geometric transform achieves near-perfect detection accuracy on the CelebA dataset.

</details>

<details>

<summary>2020-03-03 15:27:53 - Analyzing Accuracy Loss in Randomized Smoothing Defenses</summary>

- *Yue Gao, Harrison Rosenberg, Kassem Fawaz, Somesh Jha, Justin Hsu*

- `2003.01595v1` - [abs](http://arxiv.org/abs/2003.01595v1) - [pdf](http://arxiv.org/pdf/2003.01595v1)

> Recent advances in machine learning (ML) algorithms, especially deep neural networks (DNNs), have demonstrated remarkable success (sometimes exceeding human-level performance) on several tasks, including face and speech recognition. However, ML algorithms are vulnerable to \emph{adversarial attacks}, such test-time, training-time, and backdoor attacks. In test-time attacks an adversary crafts adversarial examples, which are specially crafted perturbations imperceptible to humans which, when added to an input example, force a machine learning model to misclassify the given input example. Adversarial examples are a concern when deploying ML algorithms in critical contexts, such as information security and autonomous driving. Researchers have responded with a plethora of defenses. One promising defense is \emph{randomized smoothing} in which a classifier's prediction is smoothed by adding random noise to the input example we wish to classify. In this paper, we theoretically and empirically explore randomized smoothing. We investigate the effect of randomized smoothing on the feasible hypotheses space, and show that for some noise levels the set of hypotheses which are feasible shrinks due to smoothing, giving one reason why the natural accuracy drops after smoothing. To perform our analysis, we introduce a model for randomized smoothing which abstracts away specifics, such as the exact distribution of the noise. We complement our theoretical results with extensive experiments.

</details>

<details>

<summary>2020-03-03 16:30:37 - Gaining a Sense of Touch. Physical Parameters Estimation using a Soft Gripper and Neural Networks</summary>

- *Michał Bednarek, Piotr Kicki, Jakub Bednarek, Krzysztof Walas*

- `2003.00784v2` - [abs](http://arxiv.org/abs/2003.00784v2) - [pdf](http://arxiv.org/pdf/2003.00784v2)

> Soft grippers are gaining significant attention in the manipulation of elastic objects, where it is required to handle soft and unstructured objects which are vulnerable to deformations. A crucial problem is to estimate the physical parameters of a squeezed object to adjust the manipulation procedure, which is considered as a significant challenge. To the best of the authors' knowledge, there is not enough research on physical parameters estimation using deep learning algorithms on measurements from direct interaction with objects using robotic grippers. In our work, we proposed a trainable system for the regression of a stiffness coefficient and provided extensive experiments using the physics simulator environment. Moreover, we prepared the application that works in the real-world scenario. Our system can reliably estimate the stiffness of an object using the Yale OpenHand soft gripper based on readings from Inertial Measurement Units (IMUs) attached to its fingers. Additionally, during the experiments, we prepared three datasets of signals gathered while squeezing objects -- two created in the simulation environment and one composed of real data.

</details>

<details>

<summary>2020-03-03 16:51:46 - Learn2Perturb: an End-to-end Feature Perturbation Learning to Improve Adversarial Robustness</summary>

- *Ahmadreza Jeddi, Mohammad Javad Shafiee, Michelle Karg, Christian Scharfenberger, Alexander Wong*

- `2003.01090v2` - [abs](http://arxiv.org/abs/2003.01090v2) - [pdf](http://arxiv.org/pdf/2003.01090v2)

> While deep neural networks have been achieving state-of-the-art performance across a wide variety of applications, their vulnerability to adversarial attacks limits their widespread deployment for safety-critical applications. Alongside other adversarial defense approaches being investigated, there has been a very recent interest in improving adversarial robustness in deep neural networks through the introduction of perturbations during the training process. However, such methods leverage fixed, pre-defined perturbations and require significant hyper-parameter tuning that makes them very difficult to leverage in a general fashion. In this study, we introduce Learn2Perturb, an end-to-end feature perturbation learning approach for improving the adversarial robustness of deep neural networks. More specifically, we introduce novel perturbation-injection modules that are incorporated at each layer to perturb the feature space and increase uncertainty in the network. This feature perturbation is performed at both the training and the inference stages. Furthermore, inspired by the Expectation-Maximization, an alternating back-propagation training algorithm is introduced to train the network and noise parameters consecutively. Experimental results on CIFAR-10 and CIFAR-100 datasets show that the proposed Learn2Perturb method can result in deep neural networks which are $4-7\%$ more robust on $l_{\infty}$ FGSM and PDG adversarial attacks and significantly outperforms the state-of-the-art against $l_2$ $C\&W$ attack and a wide range of well-known black-box attacks.

</details>

<details>

<summary>2020-03-04 03:20:59 - Type I Attack for Generative Models</summary>

- *Chengjin Sun, Sizhe Chen, Jia Cai, Xiaolin Huang*

- `2003.01872v1` - [abs](http://arxiv.org/abs/2003.01872v1) - [pdf](http://arxiv.org/pdf/2003.01872v1)

> Generative models are popular tools with a wide range of applications. Nevertheless, it is as vulnerable to adversarial samples as classifiers. The existing attack methods mainly focus on generating adversarial examples by adding imperceptible perturbations to input, which leads to wrong result. However, we focus on another aspect of attack, i.e., cheating models by significant changes. The former induces Type II error and the latter causes Type I error. In this paper, we propose Type I attack to generative models such as VAE and GAN. One example given in VAE is that we can change an original image significantly to a meaningless one but their reconstruction results are similar. To implement the Type I attack, we destroy the original one by increasing the distance in input space while keeping the output similar because different inputs may correspond to similar features for the property of deep neural network. Experimental results show that our attack method is effective to generate Type I adversarial examples for generative models on large-scale image datasets.

</details>

<details>

<summary>2020-03-04 05:12:27 - Double Backpropagation for Training Autoencoders against Adversarial Attack</summary>

- *Chengjin Sun, Sizhe Chen, Xiaolin Huang*

- `2003.01895v1` - [abs](http://arxiv.org/abs/2003.01895v1) - [pdf](http://arxiv.org/pdf/2003.01895v1)

> Deep learning, as widely known, is vulnerable to adversarial samples. This paper focuses on the adversarial attack on autoencoders. Safety of the autoencoders (AEs) is important because they are widely used as a compression scheme for data storage and transmission, however, the current autoencoders are easily attacked, i.e., one can slightly modify an input but has totally different codes. The vulnerability is rooted the sensitivity of the autoencoders and to enhance the robustness, we propose to adopt double backpropagation (DBP) to secure autoencoder such as VAE and DRAW. We restrict the gradient from the reconstruction image to the original one so that the autoencoder is not sensitive to trivial perturbation produced by the adversarial attack. After smoothing the gradient by DBP, we further smooth the label by Gaussian Mixture Model (GMM), aiming for accurate and robust classification. We demonstrate in MNIST, CelebA, SVHN that our method leads to a robust autoencoder resistant to attack and a robust classifier able for image transition and immune to adversarial attack if combined with GMM.

</details>

<details>

<summary>2020-03-04 10:11:44 - Risk Management Practices in Information Security: Exploring the Status Quo in the DACH Region</summary>

- *Michael Brunner, Clemens Sauerwein, Michael Felderer, Ruth Breu*

- `2003.07674v1` - [abs](http://arxiv.org/abs/2003.07674v1) - [pdf](http://arxiv.org/pdf/2003.07674v1)

> Information security management aims at ensuring proper protection of information values and information processing systems (i.e. assets). Information security risk management techniques are incorporated to deal with threats and vulnerabilities that impose risks to information security properties of these assets. This paper investigates the current state of risk management practices being used in information security management in the DACH region (Germany, Austria, Switzerland). We used an anonymous online survey targeting strategic and operative information security and risk managers and collected data from 26 organizations. We analyzed general practices, documentation artifacts, patterns of stakeholder collaboration as well as tool types and data sources used by enterprises to conduct information security management activities. Our findings show that the state of practice of information security risk management is in need of improvement. Current industrial practice heavily relies on manual data collection and complex potentially subjective decision processes with multiple stakeholders involved. Dedicated risk management tools and methods are used selectively and neglected in favor of general-purpose documentation tools and direct communication between stakeholders. In light of our results we propose guidelines for the development of risk management practices that are better aligned with the current operational situation in information security management.

</details>

<details>

<summary>2020-03-04 10:51:22 - Vessels Cybersecurity: Issues, Challenges, and the Road Ahead</summary>

- *Maurantonio Caprolu, Roberto Di Pietro, Simone Raponi, Savio Sciancalepore, Pietro Tedeschi*

- `2003.01991v1` - [abs](http://arxiv.org/abs/2003.01991v1) - [pdf](http://arxiv.org/pdf/2003.01991v1)

> Vessels cybersecurity is recently gaining momentum, as a result of a few recent attacks to vessels at sea. These recent attacks have shacked the maritime domain, which was thought to be relatively immune to cyber threats. The cited belief is now over, as proved by recent mandates issued by the International Maritime Organization (IMO). According to these regulations, all vessels should be the subject of a cybersecurity risk analysis, and technical controls should be adopted to mitigate the resulting risks. This initiative is laudable since, despite the recent incidents, the vulnerabilities and threats affecting modern vessels are still unclear to operating entities, leaving the potential for dreadful consequences of further attacks just a matter of "when", not "if". In this contribution, we investigate and systematize the major security weaknesses affecting systems and communication technologies adopted in modern vessels. Specifically, we describe the architecture and main features of the different systems, pointing out their main security issues, and specifying how they were exploited by attackers to cause service disruption and relevant financial losses. We also identify a few countermeasures to the introduced attacks. Finally, we highlight a few research challenges to be addressed by industry and academia to strengthen vessels security.

</details>

<details>

<summary>2020-03-04 15:30:10 - Threats to Federated Learning: A Survey</summary>

- *Lingjuan Lyu, Han Yu, Qiang Yang*

- `2003.02133v1` - [abs](http://arxiv.org/abs/2003.02133v1) - [pdf](http://arxiv.org/pdf/2003.02133v1)

> With the emergence of data silos and popular privacy awareness, the traditional centralized approach of training artificial intelligence (AI) models is facing strong challenges. Federated learning (FL) has recently emerged as a promising solution under this new reality. Existing FL protocol design has been shown to exhibit vulnerabilities which can be exploited by adversaries both within and without the system to compromise data privacy. It is thus of paramount importance to make FL system designers to be aware of the implications of future FL algorithm design on privacy-preservation. Currently, there is no survey on this topic. In this paper, we bridge this important gap in FL literature. By providing a concise introduction to the concept of FL, and a unique taxonomy covering threat models and two major attacks on FL: 1) poisoning attacks and 2) inference attacks, this paper provides an accessible review of this important topic. We highlight the intuitions, key techniques as well as fundamental assumptions adopted by various attacks, and discuss promising future research directions towards more robust privacy preservation in FL.

</details>

<details>

<summary>2020-03-05 14:28:41 - Applying Tensor Decomposition to image for Robustness against Adversarial Attack</summary>

- *Seungju Cho, Tae Joon Jun, Mingu Kang, Daeyoung Kim*

- `2002.12913v2` - [abs](http://arxiv.org/abs/2002.12913v2) - [pdf](http://arxiv.org/pdf/2002.12913v2)

> Nowadays the deep learning technology is growing faster and shows dramatic performance in computer vision areas. However, it turns out a deep learning based model is highly vulnerable to some small perturbation called an adversarial attack. It can easily fool the deep learning model by adding small perturbations. On the other hand, tensor decomposition method widely uses for compressing the tensor data, including data matrix, image, etc. In this paper, we suggest combining tensor decomposition for defending the model against adversarial example. We verify this idea is simple and effective to resist adversarial attack. In addition, this method rarely degrades the original performance of clean data. We experiment on MNIST, CIFAR10 and ImageNet data and show our method robust on state-of-the-art attack methods.

</details>

<details>

<summary>2020-03-06 08:08:54 - Defense against adversarial attacks on spoofing countermeasures of ASV</summary>

- *Haibin Wu, Songxiang Liu, Helen Meng, Hung-yi Lee*

- `2003.03065v1` - [abs](http://arxiv.org/abs/2003.03065v1) - [pdf](http://arxiv.org/pdf/2003.03065v1)

> Various forefront countermeasure methods for automatic speaker verification (ASV) with considerable performance in anti-spoofing are proposed in the ASVspoof 2019 challenge. However, previous work has shown that countermeasure models are vulnerable to adversarial examples indistinguishable from natural data. A good countermeasure model should not only be robust against spoofing audio, including synthetic, converted, and replayed audios; but counteract deliberately generated examples by malicious adversaries. In this work, we introduce a passive defense method, spatial smoothing, and a proactive defense method, adversarial training, to mitigate the vulnerability of ASV spoofing countermeasure models against adversarial examples. This paper is among the first to use defense methods to improve the robustness of ASV spoofing countermeasure models under adversarial attacks. The experimental results show that these two defense methods positively help spoofing countermeasure models counter adversarial examples.

</details>

<details>

<summary>2020-03-06 15:44:54 - Judge, Jury & Encryptioner: Exceptional Device Access with a Social Cost</summary>

- *Sacha Servan-Schreiber, Archer Wheeler*

- `1912.05620v3` - [abs](http://arxiv.org/abs/1912.05620v3) - [pdf](http://arxiv.org/pdf/1912.05620v3)

> We present Judge, Jury and Encryptioner (JJE) an exceptional access scheme for unlocking devices that does not give unilateral power to any single authority. JJE achieves this by placing final approval to unlock a device in the hands of peer devices. JJE distributes maintenance of the protocol across a network of "custodians" such as courts, government agencies, civil rights watchdogs, and academic institutions. Unlock requests, however, can only be approved by a randomly selected set of recently active peer devices that must be physically located by law enforcement in order to gain access to the locked device. This requires that law enforcement expend both human and monetary resources and pay a "social cost" in order to find and request the participation of random device owners in the unlock process. Compared to other proposed exceptional access schemes, we believe that JJE mitigates the risk of mass surveillance, law enforcement abuse, and vulnerability to unlawful attackers. While we propose a concrete construction, our primary goal with JJE is to spur discussion on ethical exceptional access schemes that balance privacy of individuals and the desires for law enforcement. JJE transparently reveals the use of exceptional access to the public and enforces a fixed social cost that, we believe, can be an effective deterrent to mass surveillance and abuse.

</details>

<details>

<summary>2020-03-08 05:12:13 - On the Robustness of Cooperative Multi-Agent Reinforcement Learning</summary>

- *Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, Nicolas Papernot*

- `2003.03722v1` - [abs](http://arxiv.org/abs/2003.03722v1) - [pdf](http://arxiv.org/pdf/2003.03722v1)

> In cooperative multi-agent reinforcement learning (c-MARL), agents learn to cooperatively take actions as a team to maximize a total team reward. We analyze the robustness of c-MARL to adversaries capable of attacking one of the agents on a team. Through the ability to manipulate this agent's observations, the adversary seeks to decrease the total team reward.   Attacking c-MARL is challenging for three reasons: first, it is difficult to estimate team rewards or how they are impacted by an agent mispredicting; second, models are non-differentiable; and third, the feature space is low-dimensional. Thus, we introduce a novel attack. The attacker first trains a policy network with reinforcement learning to find a wrong action it should encourage the victim agent to take. Then, the adversary uses targeted adversarial examples to force the victim to take this action.   Our results on the StartCraft II multi-agent benchmark demonstrate that c-MARL teams are highly vulnerable to perturbations applied to one of their agent's observations. By attacking a single agent, our attack method has highly negative impact on the overall team reward, reducing it from 20 to 9.4. This results in the team's winning rate to go down from 98.9% to 0%.

</details>

<details>

<summary>2020-03-09 16:32:27 - Secure Traffic Lights: Replay Attack Detection for Model-based Smart Traffic Controllers</summary>

- *Pratham Oza, Mahsa Foruhandeh, Ryan Gerdes, Thidapat Chantem*

- `2003.04244v1` - [abs](http://arxiv.org/abs/2003.04244v1) - [pdf](http://arxiv.org/pdf/2003.04244v1)

> Rapid urbanization calls for smart traffic management solutions that incorporate sensors, distributed traffic controllers and V2X communication technologies to provide fine-grained traffic control to mitigate congestion. As in many other cyber-physical systems, smart traffic management systems typically lack security measures. This allows numerous opportunities for adversarial entities to craft attacks on the sensor networks, wireless data sharing and/or the distributed traffic controllers. We show that such vulnerabilities can be exploited to disrupt mobility in a large urban area and cause unsafe conditions for drivers and the pedestrians on the roads. Specifically, in this paper, we look into vulnerabilities in model-based traffic controllers and show that, even with state-of-the-art attack detectors in place, false-data injection can be used to hamper mobility. We demonstrate a replay attack by modeling an isolated intersection in VISSIM, a popular traffic simulator and also discuss countermeasures to thwart such attacks.

</details>

<details>

<summary>2020-03-10 00:01:34 - Retrofitting Fine Grain Isolation in the Firefox Renderer (Extended Version)</summary>

- *Shravan Narayan, Craig Disselkoen, Tal Garfinkel, Nathan Froyd, Eric Rahm, Sorin Lerner, Hovav Shacham, Deian Stefan*

- `2003.00572v2` - [abs](http://arxiv.org/abs/2003.00572v2) - [pdf](http://arxiv.org/pdf/2003.00572v2)

> Firefox and other major browsers rely on dozens of third-party libraries to render audio, video, images, and other content. These libraries are a frequent source of vulnerabilities. To mitigate this threat, we are migrating Firefox to an architecture that isolates these libraries in lightweight sandboxes, dramatically reducing the impact of a compromise.   Retrofitting isolation can be labor-intensive, very prone to security bugs, and requires critical attention to performance. To help, we developed RLBox, a framework that minimizes the burden of converting Firefox to securely and efficiently use untrusted code. To enable this, RLBox employs static information flow enforcement, and lightweight dynamic checks, expressed directly in the C++ type system.   RLBox supports efficient sandboxing through either software-based-fault isolation or multi-core process isolation. Performance overheads are modest and transient, and have only minor impact on page latency. We demonstrate this by sandboxing performance-sensitive image decoding libraries ( libjpeg and libpng ), video decoding libraries ( libtheora and libvpx ), the libvorbis audio decoding library, and the zlib decompression library.   RLBox, using a WebAssembly sandbox, has been integrated into production Firefox to sandbox the libGraphite font shaping library.

</details>

<details>

<summary>2020-03-10 00:19:49 - An abstract semantics of speculative execution for reasoning about security vulnerabilities</summary>

- *Robert J. Colvin, Kirsten Winter*

- `2004.00577v1` - [abs](http://arxiv.org/abs/2004.00577v1) - [pdf](http://arxiv.org/pdf/2004.00577v1)

> Reasoning about correctness and security of software is increasingly difficult due to the complexity of modern microarchitectural features such as out-of-order execution. A class of security vulnerabilities termed Spectre that exploits side effects of speculative, out-of-order execution was announced in 2018 and has since drawn much attention. In this paper we formalise speculative execution and its side effects with the intention of allowing speculation to be reasoned about abstractly at the program level, limiting the exposure to processor-specific or low-level semantics. To this end we encode and expose speculative execution explicitly in the programming language, rather than solely in the operational semantics; as a result the effects of speculative execution are captured by redefining the meaning of a conditional statement, and introducing novel language constructs that model transient execution of an alternative branch. We add an abstract cache to the global state of the system, and derive some general refinement rules that expose cache side effects due to speculative loads. Underlying this extension is a semantic model that is based on instruction-level parallelism. The rules are encoded in a simulation tool, which we use to analyse an abstract specification of a Spectre attack and vulnerable code fragments.

</details>

<details>

<summary>2020-03-10 01:56:20 - Broken Metre: Attacking Resource Metering in EVM</summary>

- *Daniel Perez, Benjamin Livshits*

- `1909.07220v3` - [abs](http://arxiv.org/abs/1909.07220v3) - [pdf](http://arxiv.org/pdf/1909.07220v3)

> Blockchain systems, such as Ethereum, use an approach called "metering" to assign a cost to smart contract execution, an approach which is designed to incentivise miners to operate the network and protect it against DoS attacks. In the past, the imperfections of Ethereum metering allowed several DoS attacks which were countered through modification of the metering mechanism.   This paper presents a new DoS attack on Ethereum which systematically exploits its metering mechanism. We first replay and analyse several months of transactions, during which we discover a number of discrepancies in the metering model, such as significant inconsistencies in the pricing of the instructions. We further demonstrate that there is very little correlation between the execution cost and the utilised resources, such as CPU and memory. Based on these observations, we present a new type of DoS attack we call Resource Exhaustion Attack, which uses these imperfections to generate low-throughput contracts. To do this, we design a genetic algorithm that generates contracts with a throughput on average 200 times slower than typical contracts. We then show that all major Ethereum client implementations are vulnerable and, if running on commodity hardware, would be unable to stay in sync with the network when under attack. We argue that such an attack could be financially attractive not only for Ethereum competitors and speculators, but also for Ethereum miners. Finally, we discuss short-term and potential long-term fixes against such attacks. Our attack has been responsibly disclosed to the Ethereum Foundation and awarded a bug bounty reward of 5,000 USD.

</details>

<details>

<summary>2020-03-10 02:05:13 - Are We Susceptible to Rowhammer? An End-to-End Methodology for Cloud Providers</summary>

- *Lucian Cojocar, Jeremie Kim, Minesh Patel, Lillian Tsai, Stefan Saroiu, Alec Wolman, Onur Mutlu*

- `2003.04498v1` - [abs](http://arxiv.org/abs/2003.04498v1) - [pdf](http://arxiv.org/pdf/2003.04498v1)

> Cloud providers are concerned that Rowhammer poses a potentially critical threat to their servers, yet today they lack a systematic way to test whether the DRAM used in their servers is vulnerable to Rowhammer attacks. This paper presents an end-to-end methodology to determine if cloud servers are susceptible to these attacks. With our methodology, a cloud provider can construct worst-case testing conditions for DRAM.   We apply our methodology to three classes of servers from a major cloud provider. Our findings show that none of the CPU instruction sequences used in prior work to mount Rowhammer attacks create worst-case DRAM testing conditions. To address this limitation, we develop an instruction sequence that leverages microarchitectural side-effects to ``hammer'' DRAM at a near-optimal rate on modern Intel Skylake and Cascade Lake platforms. We also design a DDR4 fault injector that can reverse engineer row adjacency for any DDR4 DIMM. When applied to our cloud provider's DIMMs, we find that DRAM rows do not always follow a linear map.

</details>

<details>

<summary>2020-03-10 03:17:16 - Shielding Collaborative Learning: Mitigating Poisoning Attacks through Client-Side Detection</summary>

- *Lingchen Zhao, Shengshan Hu, Qian Wang, Jianlin Jiang, Chao Shen, Xiangyang Luo, Pengfei Hu*

- `1910.13111v2` - [abs](http://arxiv.org/abs/1910.13111v2) - [pdf](http://arxiv.org/pdf/1910.13111v2)

> Collaborative learning allows multiple clients to train a joint model without sharing their data with each other. Each client performs training locally and then submits the model updates to a central server for aggregation. Since the server has no visibility into the process of generating the updates, collaborative learning is vulnerable to poisoning attacks where a malicious client can generate a poisoned update to introduce backdoor functionality to the joint model. The existing solutions for detecting poisoned updates, however, fail to defend against the recently proposed attacks, especially in the non-IID setting. In this paper, we present a novel defense scheme to detect anomalous updates in both IID and non-IID settings. Our key idea is to realize client-side cross-validation, where each update is evaluated over other clients' local data. The server will adjust the weights of the updates based on the evaluation results when performing aggregation. To adapt to the unbalanced distribution of data in the non-IID setting, a dynamic client allocation mechanism is designed to assign detection tasks to the most suitable clients. During the detection process, we also protect the client-level privacy to prevent malicious clients from stealing the training data of other clients, by integrating differential privacy with our design without degrading the detection performance. Our experimental evaluations on two real-world datasets show that our scheme is significantly robust to two representative poisoning attacks.

</details>

<details>

<summary>2020-03-10 10:10:38 - SpecFuzz: Bringing Spectre-type vulnerabilities to the surface</summary>

- *Oleksii Oleksenko, Bohdan Trach, Mark Silberstein, Christof Fetzer*

- `1905.10311v4` - [abs](http://arxiv.org/abs/1905.10311v4) - [pdf](http://arxiv.org/pdf/1905.10311v4)

> SpecFuzz is the first tool that enables dynamic testing for speculative execution vulnerabilities (e.g., Spectre). The key is a novel concept of speculation exposure: The program is instrumented to simulate speculative execution in software by forcefully executing the code paths that could be triggered due to mispredictions, thereby making the speculative memory accesses visible to integrity checkers (e.g., AddressSanitizer). Combined with the conventional fuzzing techniques, speculation exposure enables more precise identification of potential vulnerabilities compared to state-of-the-art static analyzers.   Our prototype for detecting Spectre V1 vulnerabilities successfully identifies all known variations of Spectre V1 and decreases the mitigation overheads across the evaluated applications, reducing the amount of instrumented branches by up to 77% given a sufficient test coverage.

</details>

<details>

<summary>2020-03-10 10:35:50 - On Isometry Robustness of Deep 3D Point Cloud Models under Adversarial Attacks</summary>

- *Yue Zhao, Yuwei Wu, Caihua Chen, Andrew Lim*

- `2002.12222v2` - [abs](http://arxiv.org/abs/2002.12222v2) - [pdf](http://arxiv.org/pdf/2002.12222v2)

> While deep learning in 3D domain has achieved revolutionary performance in many tasks, the robustness of these models has not been sufficiently studied or explored. Regarding the 3D adversarial samples, most existing works focus on manipulation of local points, which may fail to invoke the global geometry properties, like robustness under linear projection that preserves the Euclidean distance, i.e., isometry. In this work, we show that existing state-of-the-art deep 3D models are extremely vulnerable to isometry transformations. Armed with the Thompson Sampling, we develop a black-box attack with success rate over 95% on ModelNet40 data set. Incorporating with the Restricted Isometry Property, we propose a novel framework of white-box attack on top of spectral norm based perturbation. In contrast to previous works, our adversarial samples are experimentally shown to be strongly transferable. Evaluated on a sequence of prevailing 3D models, our white-box attack achieves success rates from 98.88% to 100%. It maintains a successful attack rate over 95% even within an imperceptible rotation range $[\pm 2.81^{\circ}]$.

</details>

<details>

<summary>2020-03-10 13:29:34 - Streamlining Integrity Tree Updates for Secure Persistent Non-Volatile Memory</summary>

- *Alexander Freij, Shougang Yuan, Huiyang Zhou, Yan Solihin*

- `2003.04693v1` - [abs](http://arxiv.org/abs/2003.04693v1) - [pdf](http://arxiv.org/pdf/2003.04693v1)

> Emerging non-volatile main memory (NVMM) is rapidly being integrated into computer systems. However, NVMM is vulnerable to potential data remanence and replay attacks.   Established security models including split counter mode encryption and Bonsai Merkle tree (BMT) authentication have been introduced against such data integrity attacks. However, these security methods are not readily compatible with NVMM. Recent works on secure NVMM pointed out the need for data and its metadata, including the counter, the message authentication code (MAC), and the BMT to be persisted atomically. However, memory persistency models have been overlooked for secure NVMM, which is essential for crash recoverability.   In this work, we analyze the invariants that need to be ensured in order to support crash recovery for secure NVMM. We highlight that prior research has substantially under-estimated the cost of BMT persistence and propose several optimization techniques to reduce the overhead of atomically persisting updates to BMTs. The optimizations proposed explore the use of pipelining, out-of-order writes, and update coalescing while conforming to strict or epoch persistency models respectively. We evaluate our work and show that our proposed optimizations significantly reduce the performance overhead of secure NVMM with crash recoverability.

</details>

<details>

<summary>2020-03-10 15:17:02 - Predicting the vulnerability of spacecraft components: modelling debris impact effects through vulnerable-zones</summary>

- *Mirko Trisolini, Hugh G. Lewis, Camilla Colombo*

- `2003.05521v1` - [abs](http://arxiv.org/abs/2003.05521v1) - [pdf](http://arxiv.org/pdf/2003.05521v1)

> The space environment around the Earth is populated by more than 130 million objects of 1 mm in size and larger, and future predictions shows that this amount is destined to increase, even if mitigation measures are implemented at a far better rate than today. These objects can hit and damage a spacecraft or its components. It is thus necessary to assess the risk level for a satellite during its mission lifetime. Few software packages perform this analysis, and most of them employ time-consuming ray-tracing methodology, where particles are randomly sampled from relevant distributions. In addition, they tend not to consider the risk associated with the secondary debris clouds. The paper presents the development of a vulnerability assessment model, which relies on a fully statistical procedure: the debris fluxes are directly used combining them with the concept of the vulnerable zone, avoiding the random sampling the debris fluxes. A novel methodology is presented to predict damage to internal components. It models the interaction between the components and the secondary debris cloud through basic geometric operations, considering mutual shielding and shadowing between internal components. The methodologies are tested against state-of-the-art software for relevant test cases, comparing results on external structures and internal components.

</details>

<details>

<summary>2020-03-11 09:42:39 - Scan Correlation -- Revealing distributed scan campaigns</summary>

- *Steffen Haas, Florian Wilkens, Mathias Fischer*

- `2003.05188v1` - [abs](http://arxiv.org/abs/2003.05188v1) - [pdf](http://arxiv.org/pdf/2003.05188v1)

> Public networks are exposed to port scans from the Internet. Attackers search for vulnerable services they can exploit. In large scan campaigns, attackers often utilize different machines to perform distributed scans, which impedes their detection and might also camouflage the actual goal of the scanning campaign. In this paper, we present a correlation algorithm to detect scans, identify potential relations among them, and reassemble them to larger campaigns. We evaluate our approach on real-world Internet traffic and our results indicate that it can summarize and characterize standalone and distributed scan campaigns based on their tools and intention.

</details>

<details>

<summary>2020-03-12 10:10:11 - Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models</summary>

- *Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, Tianle Chen*

- `2001.03274v2` - [abs](http://arxiv.org/abs/2001.03274v2) - [pdf](http://arxiv.org/pdf/2001.03274v2)

> Transfer learning provides an effective solution for feasibly and fast customize accurate \textit{Student} models, by transferring the learned knowledge of pre-trained \textit{Teacher} models over large datasets via fine-tuning. Many pre-trained Teacher models used in transfer learning are publicly available and maintained by public platforms, increasing their vulnerability to backdoor attacks. In this paper, we demonstrate a backdoor threat to transfer learning tasks on both image and time-series data leveraging the knowledge of publicly accessible Teacher models, aimed at defeating three commonly-adopted defenses: \textit{pruning-based}, \textit{retraining-based} and \textit{input pre-processing-based defenses}. Specifically, (A) ranking-based selection mechanism to speed up the backdoor trigger generation and perturbation process while defeating \textit{pruning-based} and/or \textit{retraining-based defenses}. (B) autoencoder-powered trigger generation is proposed to produce a robust trigger that can defeat the \textit{input pre-processing-based defense}, while guaranteeing that selected neuron(s) can be significantly activated. (C) defense-aware retraining to generate the manipulated model using reverse-engineered model inputs.   We launch effective misclassification attacks on Student models over real-world images, brain Magnetic Resonance Imaging (MRI) data and Electrocardiography (ECG) learning systems. The experiments reveal that our enhanced attack can maintain the $98.4\%$ and $97.2\%$ classification accuracy as the genuine model on clean image and time series inputs respectively while improving $27.9\%-100\%$ and $27.1\%-56.1\%$ attack success rate on trojaned image and time series inputs respectively in the presence of pruning-based and/or retraining-based defenses.

</details>

<details>

<summary>2020-03-12 11:00:30 - Inline Detection of DGA Domains Using Side Information</summary>

- *Raaghavi Sivaguru, Jonathan Peck, Femi Olumofin, Anderson Nascimento, Martine De Cock*

- `2003.05703v1` - [abs](http://arxiv.org/abs/2003.05703v1) - [pdf](http://arxiv.org/pdf/2003.05703v1)

> Malware applications typically use a command and control (C&C) server to manage bots to perform malicious activities. Domain Generation Algorithms (DGAs) are popular methods for generating pseudo-random domain names that can be used to establish a communication between an infected bot and the C&C server. In recent years, machine learning based systems have been widely used to detect DGAs. There are several well known state-of-the-art classifiers in the literature that can detect DGA domain names in real-time applications with high predictive performance. However, these DGA classifiers are highly vulnerable to adversarial attacks in which adversaries purposely craft domain names to evade DGA detection classifiers. In our work, we focus on hardening DGA classifiers against adversarial attacks. To this end, we train and evaluate state-of-the-art deep learning and random forest (RF) classifiers for DGA detection using side information that is harder for adversaries to manipulate than the domain name itself. Additionally, the side information features are selected such that they are easily obtainable in practice to perform inline DGA detection. The performance and robustness of these models is assessed by exposing them to one day of real-traffic data as well as domains generated by adversarial attack algorithms. We found that the DGA classifiers that rely on both the domain name and side information have high performance and are more robust against adversaries.

</details>

<details>

<summary>2020-03-12 14:37:57 - Topological Effects on Attacks Against Vertex Classification</summary>

- *Benjamin A. Miller, Mustafa Çamurcu, Alexander J. Gomez, Kevin Chan, Tina Eliassi-Rad*

- `2003.05822v1` - [abs](http://arxiv.org/abs/2003.05822v1) - [pdf](http://arxiv.org/pdf/2003.05822v1)

> Vertex classification is vulnerable to perturbations of both graph topology and vertex attributes, as shown in recent research. As in other machine learning domains, concerns about robustness to adversarial manipulation can prevent potential users from adopting proposed methods when the consequence of action is very high. This paper considers two topological characteristics of graphs and explores the way these features affect the amount the adversary must perturb the graph in order to be successful. We show that, if certain vertices are included in the training set, it is possible to substantially an adversary's required perturbation budget. On four citation datasets, we demonstrate that if the training set includes high degree vertices or vertices that ensure all unlabeled nodes have neighbors in the training set, we show that the adversary's budget often increases by a substantial factor---often a factor of 2 or more---over random training for the Nettack poisoning attack. Even for especially easy targets (those that are misclassified after just one or two perturbations), the degradation of performance is much slower, assigning much lower probabilities to the incorrect classes. In addition, we demonstrate that this robustness either persists when recently proposed defenses are applied, or is competitive with the resulting performance improvement for the defender.

</details>

<details>

<summary>2020-03-12 19:27:26 - ÆGIS: Shielding Vulnerable Smart Contracts Against Attacks</summary>

- *Christof Ferreira Torres, Mathis Baden, Robert Norvill, Beltran Borja Fiz Pontiveros, Hugo Jonker, Sjouke Mauw*

- `2003.05987v1` - [abs](http://arxiv.org/abs/2003.05987v1) - [pdf](http://arxiv.org/pdf/2003.05987v1)

> In recent years, smart contracts have suffered major exploits, costing millions of dollars. Unlike traditional programs, smart contracts are deployed on a blockchain. As such, they cannot be modified once deployed. Though various tools have been proposed to detect vulnerable smart contracts, the majority fails to protect vulnerable contracts that have already been deployed on the blockchain. Only very few solutions have been proposed so far to tackle the issue of post-deployment. However, these solutions suffer from low precision and are not generic enough to prevent any type of attack.   In this work, we introduce {\AE}GIS, a dynamic analysis tool that protects smart contracts from being exploited during runtime. Its capability of detecting new vulnerabilities can easily be extended through so-called attack patterns. These patterns are written in a domain-specific language that is tailored to the execution model of Ethereum smart contracts. The language enables the description of malicious control and data flows. In addition, we propose a novel mechanism to streamline and speed up the process of managing attack patterns. Patterns are voted upon and stored via a smart contract, thus leveraging the benefits of tamper-resistance and transparency provided by the blockchain. We compare {\AE}GIS to current state-of-the-art tools and demonstrate that our solution achieves higher precision in detecting attacks. Finally, we perform a large-scale analysis on the first 4.5 million blocks of the Ethereum blockchain, thereby confirming the occurrences of well reported and yet unreported attacks in the wild.

</details>

<details>

<summary>2020-03-13 15:57:31 - Understanding Adversarial Attacks on Deep Learning Based Medical Image Analysis Systems</summary>

- *Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, Feng Lu*

- `1907.10456v2` - [abs](http://arxiv.org/abs/1907.10456v2) - [pdf](http://arxiv.org/pdf/1907.10456v2)

> Deep neural networks (DNNs) have become popular for medical image analysis tasks like cancer diagnosis and lesion detection. However, a recent study demonstrates that medical deep learning systems can be compromised by carefully-engineered adversarial examples/attacks with small imperceptible perturbations. This raises safety concerns about the deployment of these systems in clinical settings. In this paper, we provide a deeper understanding of adversarial examples in the context of medical images. We find that medical DNN models can be more vulnerable to adversarial attacks compared to models for natural images, according to two different viewpoints. Surprisingly, we also find that medical adversarial attacks can be easily detected, i.e., simple detectors can achieve over 98% detection AUC against state-of-the-art attacks, due to fundamental feature differences compared to normal examples. We believe these findings may be a useful basis to approach the design of more explainable and secure medical deep learning systems.

</details>

<details>

<summary>2020-03-15 13:23:57 - CoinMagic: A Differential Privacy Framework for Ring Signature Schemes</summary>

- *Wangze Ni, Han Wu, Peng Cheng, Lei Chen, Xuemin Lin, Lei Chen, Xin Lai, Xiao Zhang*

- `2003.06826v1` - [abs](http://arxiv.org/abs/2003.06826v1) - [pdf](http://arxiv.org/pdf/2003.06826v1)

> By allowing users to obscure their transactions via including "mixins" (chaff coins), ring signature schemes have been widely used to protect a sender's identity of a transaction in privacy-preserving blockchain systems, like Monero and Bytecoin. However, recent works point out that the existing ring signature scheme is vulnerable to the "chain-reaction" analysis (i.e., the spent coin in a given ring signature can be deduced through elimination). Especially, when the diversity of mixins is low, the spent coin will have a high risk to be detected. To overcome the weakness, the ring signature should be consisted of a set of mixins with high diversity and produce observations having "similar" distributions for any two coins. In this paper, we propose a notion, namely $\epsilon$-coin-indistinguishability ($\epsilon$-CI), to formally define the "similar" distribution guaranteed through a differential privacy scheme. Then, we formally define the CI-aware mixins selection problem with disjoint-superset constraint (CIA-MS-DS), which aims to find a mixin set that has maximal diversity and satisfies the constraints of $\epsilon$-CI and the budget. In CIA-MS-DS, each ring signature is either disjoint with or the superset of its preceding ring signatures. We prove that CIA-MS-DS is NP-hard and thus intractable. To solve the CIA-MS-DS problem, we propose two approximation algorithms, namely the Progressive Algorithm and the Game Theoretic Algorithm, with theoretic guarantees. Through extensive experiments on both real data sets and synthetic data sets, we demonstrate the efficiency and the effectiveness of our approaches.

</details>

<details>

<summary>2020-03-16 14:13:03 - Smoothed Inference for Adversarially-Trained Models</summary>

- *Yaniv Nemcovsky, Evgenii Zheltonozhskii, Chaim Baskin, Brian Chmiel, Maxim Fishman, Alex M. Bronstein, Avi Mendelson*

- `1911.07198v2` - [abs](http://arxiv.org/abs/1911.07198v2) - [pdf](http://arxiv.org/pdf/1911.07198v2)

> Deep neural networks are known to be vulnerable to adversarial attacks. Current methods of defense from such attacks are based on either implicit or explicit regularization, e.g., adversarial training. Randomized smoothing, the averaging of the classifier outputs over a random distribution centered in the sample, has been shown to guarantee the performance of a classifier subject to bounded perturbations of the input. In this work, we study the application of randomized smoothing as a way to improve performance on unperturbed data as well as to increase robustness to adversarial attacks. The proposed technique can be applied on top of any existing adversarial defense, but works particularly well with the randomized approaches. We examine its performance on common white-box (PGD) and black-box (transfer and NAttack) attacks on CIFAR-10 and CIFAR-100, substantially outperforming previous art for most scenarios and comparable on others. For example, we achieve 60.4% accuracy under a PGD attack on CIFAR-10 using ResNet-20, outperforming previous art by 11.7%. Since our method is based on sampling, it lends itself well for trading-off between the model inference complexity and its performance. A reference implementation of the proposed techniques is provided at https://github.com/yanemcovsky/SIAM

</details>

<details>

<summary>2020-03-16 22:03:45 - On the Security of Randomized Defenses Against Adversarial Samples</summary>

- *Kumar Sharad, Giorgia Azzurra Marson, Hien Thi Thu Truong, Ghassan Karame*

- `1812.04293v4` - [abs](http://arxiv.org/abs/1812.04293v4) - [pdf](http://arxiv.org/pdf/1812.04293v4)

> Deep Learning has been shown to be particularly vulnerable to adversarial samples. To combat adversarial strategies, numerous defensive techniques have been proposed. Among these, a promising approach is to use randomness in order to make the classification process unpredictable and presumably harder for the adversary to control. In this paper, we study the effectiveness of randomized defenses against adversarial samples. To this end, we categorize existing state-of-the-art adversarial strategies into three attacker models of increasing strength, namely blackbox, graybox, and whitebox (a.k.a.~adaptive) attackers. We also devise a lightweight randomization strategy for image classification based on feature squeezing, that consists of pre-processing the classifier input by embedding randomness within each feature, before applying feature squeezing. We evaluate the proposed defense and compare it to other randomized techniques in the literature via thorough experiments. Our results indeed show that careful integration of randomness can be effective against both graybox and blackbox attacks without significantly degrading the accuracy of the underlying classifier. However, our experimental results offer strong evidence that in the present form such randomization techniques cannot deter a whitebox adversary that has access to all classifier parameters and has full knowledge of the defense. Our work thoroughly and empirically analyzes the impact of randomization techniques against all classes of adversarial strategies.

</details>

<details>

<summary>2020-03-17 02:07:09 - SMACS: Smart Contract Access Control Service</summary>

- *Bowen Liu, Siwei Sun, Pawel Szalachowski*

- `2003.07495v1` - [abs](http://arxiv.org/abs/2003.07495v1) - [pdf](http://arxiv.org/pdf/2003.07495v1)

> Although blockchain-based smart contracts promise a ``trustless'' way of enforcing agreements even with monetary consequences, they suffer from multiple security issues. Many of these issues could be mitigated via an effective access control system, however, its realization is challenging due to the properties of current blockchain platforms (like lack of privacy, costly on-chain resources, or latency). To address this problem, we propose the SMACS framework, where updatable and sophisticated Access Control Rules (ACRs)} for smart contracts can be realized with low cost. SMACS shifts the burden of expensive ACRs validation and management operations to an off-chain infrastructure, while implementing on-chain only lightweight token-based access control. SMACS is flexible and in addition to simple access control lists can easily implement rules enhancing the runtime security of smart contracts. With dedicated ACRs backed by vulnerability-detection tools, SMACS can protect vulnerable contracts after deployment. We fully implement SMACS and evaluate it.

</details>

<details>

<summary>2020-03-17 08:11:18 - Heat and Blur: An Effective and Fast Defense Against Adversarial Examples</summary>

- *Haya Brama, Tal Grinshpoun*

- `2003.07573v1` - [abs](http://arxiv.org/abs/2003.07573v1) - [pdf](http://arxiv.org/pdf/2003.07573v1)

> The growing incorporation of artificial neural networks (NNs) into many fields, and especially into life-critical systems, is restrained by their vulnerability to adversarial examples (AEs). Some existing defense methods can increase NNs' robustness, but they often require special architecture or training procedures and are irrelevant to already trained models. In this paper, we propose a simple defense that combines feature visualization with input modification, and can, therefore, be applicable to various pre-trained networks. By reviewing several interpretability methods, we gain new insights regarding the influence of AEs on NNs' computation. Based on that, we hypothesize that information about the "true" object is preserved within the NN's activity, even when the input is adversarial, and present a feature visualization version that can extract that information in the form of relevance heatmaps. We then use these heatmaps as a basis for our defense, in which the adversarial effects are corrupted by massive blurring. We also provide a new evaluation metric that can capture the effects of both attacks and defenses more thoroughly and descriptively, and demonstrate the effectiveness of the defense and the utility of the suggested evaluation measurement with VGG19 results on the ImageNet dataset.

</details>

<details>

<summary>2020-03-17 16:55:58 - Revisiting Security Vulnerabilities in Commercial Password Managers</summary>

- *Michael Carr, Siamak F. Shahandashti*

- `2003.01985v2` - [abs](http://arxiv.org/abs/2003.01985v2) - [pdf](http://arxiv.org/pdf/2003.01985v2)

> In this work we analyse five popular commercial password managers for security vulnerabilities. Our analysis is twofold. First, we compile a list of previously disclosed vulnerabilities through a comprehensive review of the academic and non-academic sources and test each password manager against all the previously disclosed vulnerabilities. We find a mixed picture of fixed and persisting vulnerabilities. Then we carry out systematic functionality tests on the considered password managers and find four new vulnerabilities. Notably, one of the new vulnerabilities we identified allows a malicious app to impersonate a legitimate app to two out of five widely-used password managers we tested and as a result steal the user's password for the targeted service. We implement a proof-of-concept attack to show the feasibility of this vulnerability in a real-life scenario. Finally, we report and reflect on our experience of responsible disclosure of the newly discovered vulnerabilities to the corresponding password manager vendors.

</details>

<details>

<summary>2020-03-17 22:02:50 - Liquidity in Credit Networks with Constrained Agents</summary>

- *Geoffrey Ramseyer, Ashish Goel, David Mazieres*

- `1910.02194v3` - [abs](http://arxiv.org/abs/1910.02194v3) - [pdf](http://arxiv.org/pdf/1910.02194v3)

> In order to scale transaction rates for deployment across the global web, many cryptocurrencies have deployed so-called "Layer-2" networks of private payment channels. An idealized payment network behaves like a Credit Network, a model for transactions across a network of bilateral trust relationships. Credit Networks capture many aspects of traditional currencies as well as new virtual currencies and payment mechanisms. In the traditional credit network model, if an agent defaults, every other node that trusted it is vulnerable to loss. In a cryptocurrency context, trust is manufactured by capital deposits, and thus there arises a natural tradeoff between network liquidity (i.e. the fraction of transactions that succeed) and the cost of capital deposits.   In this paper, we introduce constraints that bound the total amount of loss that the rest of the network can suffer if an agent (or a set of agents) were to default - equivalently, how the network changes if agents can support limited solvency guarantees.   We show that these constraints preserve the analytical structure of a credit network. Furthermore, we show that aggregate borrowing constraints greatly simplify the network structure and in the payment network context achieve the optimal tradeoff between liquidity and amount of escrowed capital.

</details>

<details>

<summary>2020-03-17 23:59:46 - Library network, a possible path to explainable neural networks</summary>

- *Jung Hoon Lee*

- `1909.13360v3` - [abs](http://arxiv.org/abs/1909.13360v3) - [pdf](http://arxiv.org/pdf/1909.13360v3)

> Deep neural networks (DNNs) may outperform human brains in complex tasks, but the lack of transparency in their decision-making processes makes us question whether we could fully trust DNNs with high stakes problems. As DNNs' operations rely on a massive number of both parallel and sequential linear/nonlinear computations, predicting their mistakes is nearly impossible. Also, a line of studies suggests that DNNs can be easily deceived by adversarial attacks, indicating that their decisions can easily be corrupted by unexpected factors. Such vulnerability must be overcome if we intend to take advantage of DNNs' efficiency in high stakes problems. Here, we propose an algorithm that can help us better understand DNNs' decision-making processes. Our empirical evaluations suggest that this algorithm can effectively trace DNNs' decision processes from one layer to another and detect adversarial attacks.

</details>

<details>

<summary>2020-03-18 12:33:59 - Vulnerabilities of Connectionist AI Applications: Evaluation and Defence</summary>

- *Christian Berghoff, Matthias Neu, Arndt von Twickel*

- `2003.08837v1` - [abs](http://arxiv.org/abs/2003.08837v1) - [pdf](http://arxiv.org/pdf/2003.08837v1)

> This article deals with the IT security of connectionist artificial intelligence (AI) applications, focusing on threats to integrity, one of the three IT security goals. Such threats are for instance most relevant in prominent AI computer vision applications. In order to present a holistic view on the IT security goal integrity, many additional aspects such as interpretability, robustness and documentation are taken into account. A comprehensive list of threats and possible mitigations is presented by reviewing the state-of-the-art literature. AI-specific vulnerabilities such as adversarial attacks and poisoning attacks as well as their AI-specific root causes are discussed in detail. Additionally and in contrast to former reviews, the whole AI supply chain is analysed with respect to vulnerabilities, including the planning, data acquisition, training, evaluation and operation phases. The discussion of mitigations is likewise not restricted to the level of the AI system itself but rather advocates viewing AI systems in the context of their supply chains and their embeddings in larger IT infrastructures and hardware devices. Based on this and the observation that adaptive attackers may circumvent any single published AI-specific defence to date, the article concludes that single protective measures are not sufficient but rather multiple measures on different levels have to be combined to achieve a minimum level of IT security for AI applications.

</details>

<details>

<summary>2020-03-18 17:14:50 - Survey of Privacy-Preserving Collaborative Filtering</summary>

- *Islam Elnabarawy, Wei Jiang, Donald C. Wunsch II*

- `2003.08343v1` - [abs](http://arxiv.org/abs/2003.08343v1) - [pdf](http://arxiv.org/pdf/2003.08343v1)

> Collaborative filtering recommendation systems provide recommendations to users based on their own past preferences, as well as those of other users who share similar interests. The use of recommendation systems has grown widely in recent years, helping people choose which movies to watch, books to read, and items to buy. However, users are often concerned about their privacy when using such systems, and many users are reluctant to provide accurate information to most online services. Privacy-preserving collaborative filtering recommendation systems aim to provide users with accurate recommendations while maintaining certain guarantees about the privacy of their data. This survey examines the recent literature in privacy-preserving collaborative filtering, providing a broad perspective of the field and classifying the key contributions in the literature using two different criteria: the type of vulnerability they address and the type of approach they use to solve it.

</details>

<details>

<summary>2020-03-19 10:41:34 - Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors</summary>

- *Gilad Cohen, Guillermo Sapiro, Raja Giryes*

- `1909.06872v2` - [abs](http://arxiv.org/abs/1909.06872v2) - [pdf](http://arxiv.org/pdf/1909.06872v2)

> Deep neural networks (DNNs) are notorious for their vulnerability to adversarial attacks, which are small perturbations added to their input images to mislead their prediction. Detection of adversarial examples is, therefore, a fundamental requirement for robust classification frameworks. In this work, we present a method for detecting such adversarial attacks, which is suitable for any pre-trained neural network classifier. We use influence functions to measure the impact of every training sample on the validation set data. From the influence scores, we find the most supportive training samples for any given validation example. A k-nearest neighbor (k-NN) model fitted on the DNN's activation layers is employed to search for the ranking of these supporting training samples. We observe that these samples are highly correlated with the nearest neighbors of the normal inputs, while this correlation is much weaker for adversarial inputs. We train an adversarial detector using the k-NN ranks and distances and show that it successfully distinguishes adversarial examples, getting state-of-the-art results on six attack methods with three datasets. Code is available at https://github.com/giladcohen/NNIF_adv_defense.

</details>

<details>

<summary>2020-03-19 17:28:36 - Automatically Proving Microkernels Free from Privilege Escalation from their Executable</summary>

- *Olivier Nicole, Matthieu Lemerre, Sébastien Bardin, Xavier Rival*

- `2003.08915v1` - [abs](http://arxiv.org/abs/2003.08915v1) - [pdf](http://arxiv.org/pdf/2003.08915v1)

> Operating system kernels are the security keystone of most computer systems, as they provide the core protection mechanisms. Kernels are in particular responsible for their own security, i.e. they must prevent untrusted user tasks from reaching their level of privilege. We demonstrate that proving such absence of privilege escalation is a pre-requisite for any definitive security proof of the kernel. While prior OS kernel formal verifications were performed either on source code or crafted kernels, with manual or semi-automated methods requiring significant human efforts in annotations or proofs, we show that it is possible to compute such kernel security proofs using fully-automated methods and starting from the executable code of an existing microkernel with no modification, thus formally verifying absence of privilege escalation with high confidence for a low cost. We applied our method on two embedded microkernels, including the industrial kernel AnonymOS: with only 58 lines of annotation and less than 10 minutes of computation, our method finds a vulnerability in a first (buggy) version of AnonymOS and verifies absence of privilege escalation in a second (secure) version.

</details>

<details>

<summary>2020-03-20 12:21:56 - Colored Noise Injection for Training Adversarially Robust Neural Networks</summary>

- *Evgenii Zheltonozhskii, Chaim Baskin, Yaniv Nemcovsky, Brian Chmiel, Avi Mendelson, Alex M. Bronstein*

- `2003.02188v2` - [abs](http://arxiv.org/abs/2003.02188v2) - [pdf](http://arxiv.org/pdf/2003.02188v2)

> Even though deep learning has shown unmatched performance on various tasks, neural networks have been shown to be vulnerable to small adversarial perturbations of the input that lead to significant performance degradation. In this work we extend the idea of adding white Gaussian noise to the network weights and activations during adversarial training (PNI) to the injection of colored noise for defense against common white-box and black-box attacks. We show that our approach outperforms PNI and various previous approaches in terms of adversarial accuracy on CIFAR-10 and CIFAR-100 datasets. In addition, we provide an extensive ablation study of the proposed method justifying the chosen configurations.

</details>

<details>

<summary>2020-03-20 16:24:25 - Adversarial Examples and the Deeper Riddle of Induction: The Need for a Theory of Artifacts in Deep Learning</summary>

- *Cameron Buckner*

- `2003.11917v1` - [abs](http://arxiv.org/abs/2003.11917v1) - [pdf](http://arxiv.org/pdf/2003.11917v1)

> Deep learning is currently the most widespread and successful technology in artificial intelligence. It promises to push the frontier of scientific discovery beyond current limits. However, skeptics have worried that deep neural networks are black boxes, and have called into question whether these advances can really be deemed scientific progress if humans cannot understand them. Relatedly, these systems also possess bewildering new vulnerabilities: most notably a susceptibility to "adversarial examples". In this paper, I argue that adversarial examples will become a flashpoint of debate in philosophy and diverse sciences. Specifically, new findings concerning adversarial examples have challenged the consensus view that the networks' verdicts on these cases are caused by overfitting idiosyncratic noise in the training set, and may instead be the result of detecting predictively useful "intrinsic features of the data geometry" that humans cannot perceive (Ilyas et al., 2019). These results should cause us to re-examine responses to one of the deepest puzzles at the intersection of philosophy and science: Nelson Goodman's "new riddle" of induction. Specifically, they raise the possibility that progress in a number of sciences will depend upon the detection and manipulation of useful features that humans find inscrutable. Before we can evaluate this possibility, however, we must decide which (if any) of these inscrutable features are real but available only to "alien" perception and cognition, and which are distinctive artifacts of deep learning-for artifacts like lens flares or Gibbs phenomena can be similarly useful for prediction, but are usually seen as obstacles to scientific theorizing. Thus, machine learning researchers urgently need to develop a theory of artifacts for deep neural networks, and I conclude by sketching some initial directions for this area of research.

</details>

<details>

<summary>2020-03-21 02:40:44 - An Empirical Study on Benchmarks of Artificial Software Vulnerabilities</summary>

- *Sijia Geng, Yuekang Li, Yunlan Du, Jun Xu, Yang Liu, Bing Mao*

- `2003.09561v1` - [abs](http://arxiv.org/abs/2003.09561v1) - [pdf](http://arxiv.org/pdf/2003.09561v1)

> Recently, various techniques (e.g., fuzzing) have been developed for vulnerability detection. To evaluate those techniques, the community has been developing benchmarks of artificial vulnerabilities because of a shortage of ground-truth. However, people have concerns that such vulnerabilities cannot represent reality and may lead to unreliable and misleading results. Unfortunately, there lacks research on handling such concerns.   In this work, to understand how close these benchmarks mirror reality, we perform an empirical study on three artificial vulnerability benchmarks - LAVA-M, Rode0day and CGC (2669 bugs) and various real-world memory-corruption vulnerabilities (80 CVEs). Furthermore, we propose a model to depict the properties of memory-corruption vulnerabilities. Following this model, we conduct intensive experiments and data analyses. Our analytic results reveal that while artificial benchmarks attempt to approach the real world, they still significantly differ from reality. Based on the findings, we propose a set of strategies to improve the quality of artificial benchmarks.

</details>

<details>

<summary>2020-03-21 20:48:09 - Multi-Class classification of vulnerabilities in Smart Contracts using AWD-LSTM, with pre-trained encoder inspired from natural language processing</summary>

- *Ajay K. Gogineni, S. Swayamjyoti, Devadatta Sahoo, Kisor K. Sahu, Raj kishore*

- `2004.00362v1` - [abs](http://arxiv.org/abs/2004.00362v1) - [pdf](http://arxiv.org/pdf/2004.00362v1)

> Vulnerability detection and safety of smart contracts are of paramount importance because of their immutable nature. Symbolic tools like OYENTE and MAIAN are typically used for vulnerability prediction in smart contracts. As these tools are computationally expensive, they are typically used to detect vulnerabilities until some predefined invocation depth. These tools require more search time as the invocation depth increases. Since the number of smart contracts is increasing exponentially, it is difficult to analyze the contracts using these traditional tools. Recently a machine learning technique called Long Short Term Memory (LSTM) has been used for binary classification, i.e., to predict whether a smart contract is vulnerable or not. This technique requires nearly constant search time as the invocation depth increases. In the present article, we have shown a multi-class classification, where we classify a smart contract in Suicidal, Prodigal, Greedy, or Normal categories. We used Average Stochastic Gradient Descent Weight-Dropped LSTM (AWD-LSTM), which is a variant of LSTM, to perform classification. We reduced the class imbalance (a large number of normal contracts as compared to other categories) by considering only the distinct opcode combination for normal contracts. We have achieved a weighted average Fbeta score of 90.0%. Hence, such techniques can be used to analyze a large number of smart contracts and help to improve the security of these contracts.

</details>

<details>

<summary>2020-03-23 18:23:12 - Backflash Light as a Security Vulnerability in Quantum Key Distribution Systems</summary>

- *Ivan Vybornyi, Abderrahmen Trichili, Mohamed-Slim Alouini*

- `2003.10478v1` - [abs](http://arxiv.org/abs/2003.10478v1) - [pdf](http://arxiv.org/pdf/2003.10478v1)

> Based on the fundamental rules of quantum mechanics, two communicating parties can generate and share a secret random key that can be used to encrypt and decrypt messages sent over an insecure channel. This process is known as quantum key distribution (QKD). Contrary to classical encryption schemes, the security of a QKD system does not depend on the computational complexity of specific mathematical problems. However, QKD systems can be subject to different kinds of attacks, exploiting engineering and technical imperfections of the components forming the systems. Here, we review the security vulnerabilities of QKD. We mainly focus on a particular effect known as backflash light, which can be a source of eavesdropping attacks. We equally highlight the methods for quantifying backflash emission and the different ways to mitigate this effect.

</details>

<details>

<summary>2020-03-25 15:08:20 - TEAM: An Taylor Expansion-Based Method for Generating Adversarial Examples</summary>

- *Ya-guan Qian, Xi-Ming Zhang, Wassim Swaileh, Li Wei, Bin Wang, Jian-Hai Chen, Wu-Jie Zhou, Jing-Sheng Lei*

- `2001.08389v2` - [abs](http://arxiv.org/abs/2001.08389v2) - [pdf](http://arxiv.org/pdf/2001.08389v2)

> Although Deep Neural Networks(DNNs) have achieved successful applications in many fields, they are vulnerable to adversarial examples.Adversarial training is one of the most effective methods to improve the robustness of DNNs, and it is generally considered as solving a saddle point problem that minimizes risk and maximizes perturbation.Therefore, powerful adversarial examples can effectively replicate the situation of perturbation maximization to solve the saddle point problem.The method proposed in this paper approximates the output of DNNs in the input neighborhood by using the Taylor expansion, and then optimizes it by using the Lagrange multiplier method to generate adversarial examples. If it is used for adversarial training, the DNNs can be effectively regularized and the defects of the model can be improved.

</details>

<details>

<summary>2020-03-26 01:37:40 - When NAS Meets Robustness: In Search of Robust Architectures against Adversarial Attacks</summary>

- *Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, Dahua Lin*

- `1911.10695v3` - [abs](http://arxiv.org/abs/1911.10695v3) - [pdf](http://arxiv.org/pdf/1911.10695v3)

> Recent advances in adversarial attacks uncover the intrinsic vulnerability of modern deep neural networks. Since then, extensive efforts have been devoted to enhancing the robustness of deep networks via specialized learning algorithms and loss functions. In this work, we take an architectural perspective and investigate the patterns of network architectures that are resilient to adversarial attacks. To obtain the large number of networks needed for this study, we adopt one-shot neural architecture search, training a large network for once and then finetuning the sub-networks sampled therefrom. The sampled architectures together with the accuracies they achieve provide a rich basis for our study. Our "robust architecture Odyssey" reveals several valuable observations: 1) densely connected patterns result in improved robustness; 2) under computational budget, adding convolution operations to direct connection edge is effective; 3) flow of solution procedure (FSP) matrix is a good indicator of network robustness. Based on these observations, we discover a family of robust architectures (RobNets). On various datasets, including CIFAR, SVHN, Tiny-ImageNet, and ImageNet, RobNets exhibit superior robustness performance to other widely used architectures. Notably, RobNets substantially improve the robust accuracy (~5% absolute gains) under both white-box and black-box attacks, even with fewer parameter numbers. Code is available at https://github.com/gmh14/RobNets.

</details>

<details>

<summary>2020-03-26 09:30:47 - Democratic Value and Money for Decentralized Digital Society</summary>

- *Bryan Ford*

- `2003.12375v1` - [abs](http://arxiv.org/abs/2003.12375v1) - [pdf](http://arxiv.org/pdf/2003.12375v1)

> Classical monetary systems regularly subject the most vulnerable majority of the world's population to debilitating financial shocks, and have manifestly allowed uncontrolled global inequality over the long term. Given these basic failures, how can we avoid asking whether mainstream macroeconomic principles are actually compatible with democratic principles such as equality or the protection of human rights and dignity? This idea paper takes a constructive look at this question, by exploring how alternate monetary principles might result in a form of money more compatible with democratic principles -- dare we call it "democratic money"? In this alternative macroeconomic philosophy, both the supply of and the demand for money must be rooted in people, so as to give all people both equal opportunities for economic participation. Money must be designed around equality, not only across all people alive at a given moment, but also across past and future generations of people, guaranteeing that our descendants cannot be enslaved by their ancestors' economic luck or misfortune. Democratic money must reliably give all people a means to enable everyday commerce, investment, and value creation in good times and bad, and must impose hard limits on financial inequality. Democratic money must itself be governed democratically, and must economically facilitate the needs of citizens in a democracy for trustworthy and unbiased information with which to make wise collective decisions. An intriguing approach to implementing and deploying democratic money is via a cryptocurrency built on a proof-of-personhood foundation, giving each opt-in human participant one equal unit of stake. Such a cryptocurrency would have both interesting similarities to, and important differences from, a Universal Basic Income (UBI) denominated in an existing currency.

</details>

<details>

<summary>2020-03-26 16:49:28 - Denial of Service Attacks Detection in Software-Defined Wireless Sensor Networks</summary>

- *Gustavo A. Nunez Segura, Sotiris Skaperas, Arsenia Chorti, Lefteris Mamatas, Cintia Borges Margi*

- `2003.12027v1` - [abs](http://arxiv.org/abs/2003.12027v1) - [pdf](http://arxiv.org/pdf/2003.12027v1)

> Software-defined networking (SDN) is a promising technology to overcome many challenges in wireless sensor networks (WSN), particularly with respect to flexibility and reuse. Conversely, the centralization and the planes' separation turn SDNs vulnerable to new security threats in the general context of distributed denial of service (DDoS) attacks. State-of-the-art approaches to identify DDoS do not always take into consideration restrictions in typical WSNs e.g., computational complexity and power constraints, while further performance improvement is always a target. The objective of this work is to propose a lightweight but very efficient DDoS attack detection approach using change point analysis. Our approach has a high detection rate and linear complexity, so that it is suitable for WSNs. We demonstrate the performance of our detector in software-defined WSNs of 36 and 100 nodes with varying attack intensity (the number of attackers ranges from 5% to 20% of nodes). We use change point detectors to monitor anomalies in two metrics: the data packets delivery rate and the control packets overhead. Our results show that with increasing intensity of attack, our approach can achieve a detection rate close to100% and that the type of attack can also be inferred.

</details>

<details>

<summary>2020-03-27 11:39:43 - Assessing the Security of OPC UA Deployments</summary>

- *Linus Roepert, Markus Dahlmanns, Ina Berenice Fink, Jan Pennekamp, Martin Henze*

- `2003.12341v1` - [abs](http://arxiv.org/abs/2003.12341v1) - [pdf](http://arxiv.org/pdf/2003.12341v1)

> To address the increasing security demands of industrial deployments, OPC UA is one of the first industrial protocols explicitly designed with security in mind. However, deploying it securely requires a thorough configuration of a wide range of options. Thus, assessing the security of OPC UA deployments and their configuration is necessary to ensure secure operation, most importantly confidentiality and integrity of industrial processes. In this work, we present extensions to the popular Metasploit Framework to ease network-based security assessments of OPC UA deployments. To this end, we discuss methods to discover OPC UA servers, test their authentication, obtain their configuration, and check for vulnerabilities. Ultimately, our work enables operators to verify the (security) configuration of their systems and identify potential attack vectors.

</details>

<details>

<summary>2020-03-28 18:28:33 - Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning</summary>

- *Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, Zhangyang Wang*

- `2003.12862v1` - [abs](http://arxiv.org/abs/2003.12862v1) - [pdf](http://arxiv.org/pdf/2003.12862v1)

> Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We introduce adversarial training into self-supervision, to provide general-purpose robust pre-trained models for the first time. We find these robust pre-trained models can benefit the subsequent fine-tuning in two ways: i) boosting final model robustness; ii) saving the computation cost, if proceeding towards adversarial fine-tuning. We conduct extensive experiments to demonstrate that the proposed framework achieves large performance margins (eg, 3.83% on robust accuracy and 1.3% on standard accuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end adversarial training baseline. Moreover, we find that different self-supervised pre-trained models have a diverse adversarial vulnerability. It inspires us to ensemble several pretraining tasks, which boosts robustness more. Our ensemble strategy contributes to a further improvement of 3.59% on robust accuracy, while maintaining a slightly higher standard accuracy on CIFAR-10. Our codes are available at https://github.com/TAMU-VITA/Adv-SS-Pretraining.

</details>

<details>

<summary>2020-03-28 22:30:12 - liOS: Lifting iOS apps for fun and profit</summary>

- *Julian Schütte, Dennis Titze*

- `2003.12901v1` - [abs](http://arxiv.org/abs/2003.12901v1) - [pdf](http://arxiv.org/pdf/2003.12901v1)

> Although iOS is the second most popular mobile operating system and is often considered the more secure one, approaches to automatically analyze iOS applications are scarce and generic app analysis frameworks do not exist. This is on the one hand due to the closed ecosystem putting obstacles in the way of reverse engineers and on the other hand due to the complexity of reverse engineering and analyzing app binaries. Reliably lifting accurate call graphs, control flows, and data dependence graphs from binary code, as well as reconstructing object-oriented high-level concepts is a non-trivial task and the choice of the lifted target representation determines the analysis capabilities. None of the various existing intermediate representations is a perfect fit for all types of analysis, while the detection of vulnerabilities requires techniques ranging from simple pattern matching to complex inter-procedural data flow analyses. We address this gap by introducing liOS, a binary lifting and analysis framework for iOS applications that extracts lifted information from several frontends and unifies them in a "supergraph" representation that tolerates missing parts and is further extended and interlinked by liOS "passes". A static analysis of the binary is then realized in the form of graph traversal queries, which can be considered as an advancement of classic program query languages. We illustrate this approach by means of a typical JavaScript/Objective-C bridge, which can lead to remote code execution in iOS applications.

</details>

<details>

<summary>2020-03-29 00:16:32 - TBT: Targeted Neural Network Attack with Bit Trojan</summary>

- *Adnan Siraj Rakin, Zhezhi He, Deliang Fan*

- `1909.05193v3` - [abs](http://arxiv.org/abs/1909.05193v3) - [pdf](http://arxiv.org/pdf/1909.05193v3)

> Security of modern Deep Neural Networks (DNNs) is under severe scrutiny as the deployment of these models become widespread in many intelligence-based applications. Most recently, DNNs are attacked through Trojan which can effectively infect the model during the training phase and get activated only through specific input patterns (i.e, trigger) during inference. In this work, for the first time, we propose a novel Targeted Bit Trojan(TBT) method, which can insert a targeted neural Trojan into a DNN through the bit-flip attack. Our algorithm efficiently generates a trigger specifically designed to locate certain vulnerable bits of DNN weights stored in main memory (i.e., DRAM). The objective is that once the attacker flips these vulnerable bits, the network still operates with normal inference accuracy with benign input. However, when the attacker activates the trigger by embedding it with any input, the network is forced to classify all inputs to a certain target class. We demonstrate that flipping only several vulnerable bits identified by our method, using available bit-flip techniques (i.e, row-hammer), can transform a fully functional DNN model into a Trojan-infected model. We perform extensive experiments of CIFAR-10, SVHN and ImageNet datasets on both VGG-16 and Resnet-18 architectures. Our proposed TBT could classify 92 % of test images to a target class with as little as 84 bit-flips out of 88 million weight bits on Resnet-18 for CIFAR10 dataset.

</details>

<details>

<summary>2020-03-29 23:58:33 - Analytical Estimation and Localization of Hardware Trojan Vulnerability in RTL Designs</summary>

- *Sheikh Ariful Islam, Love Kumar Sah, Srinivas Katkoori*

- `2003.13164v1` - [abs](http://arxiv.org/abs/2003.13164v1) - [pdf](http://arxiv.org/pdf/2003.13164v1)

> Offshoring the proprietary Intellectual property (IP) has recently increased the threat of malicious logic insertion in the form of Hardware Trojan (HT). A potential and stealthy HT is triggered with nets that switch rarely during regular circuit operation. Detection of HT in the host design requires exhaustive simulation to activate the HT during pre- and postsilicon. Although the nets with variable switching probability less than a threshold are primarily chosen as a good candidate for Trojan triggering, there is no systematic fine-grained approach for earlier detection of rare nets from word-level measures of input signals. In this paper, we propose a high-level technique to estimate the nets with the rare activity of arithmetic modules from word-level information. Specifically, for a given module, we use the knowledge of internal construction of the architecture to detect "low activity" and "local regions" without resorting to expensive RTL and other low-level simulations. The presented heuristic method abstracts away from the low-level details of design and describes the rare activity of bits (modules) in a word (architecture) as a function of signal statistics. The resulting quick estimates of nets in rare regions allows a designer to develop a compact test generation algorithm without the knowledge of the bit-level activity. We determine the effect of different positions of the breakpoint in the input signal to calculate the accuracy of the approach. We conduct a set of experiments on six adder architectures and four multiplier architectures. The average error to calculate the rare nets between RTL simulation and estimated values are below 2% in all architectures.

</details>

<details>

<summary>2020-03-30 07:48:52 - Towards Stable and Comprehensive Domain Alignment: Max-Margin Domain-Adversarial Training</summary>

- *Jianfei Yang, Han Zou, Yuxun Zhou, Lihua Xie*

- `2003.13249v1` - [abs](http://arxiv.org/abs/2003.13249v1) - [pdf](http://arxiv.org/pdf/2003.13249v1)

> Domain adaptation tackles the problem of transferring knowledge from a label-rich source domain to a label-scarce or even unlabeled target domain. Recently domain-adversarial training (DAT) has shown promising capacity to learn a domain-invariant feature space by reversing the gradient propagation of a domain classifier. However, DAT is still vulnerable in several aspects including (1) training instability due to the overwhelming discriminative ability of the domain classifier in adversarial training, (2) restrictive feature-level alignment, and (3) lack of interpretability or systematic explanation of the learned feature space. In this paper, we propose a novel Max-margin Domain-Adversarial Training (MDAT) by designing an Adversarial Reconstruction Network (ARN). The proposed MDAT stabilizes the gradient reversing in ARN by replacing the domain classifier with a reconstruction network, and in this manner ARN conducts both feature-level and pixel-level domain alignment without involving extra network structures. Furthermore, ARN demonstrates strong robustness to a wide range of hyper-parameters settings, greatly alleviating the task of model selection. Extensive empirical results validate that our approach outperforms other state-of-the-art domain alignment methods. Moreover, reconstructing adapted features reveals the domain-invariant feature space which conforms with our intuition.

</details>

<details>

<summary>2020-03-30 07:50:13 - Hold the Door! Fingerprinting Your Car Key to Prevent Keyless Entry Car Theft</summary>

- *Kyungho Joo, Wonsuk Choi, Dong Hoon Lee*

- `2003.13251v1` - [abs](http://arxiv.org/abs/2003.13251v1) - [pdf](http://arxiv.org/pdf/2003.13251v1)

> Recently, the traditional way to unlock car doors has been replaced with a keyless entry system which proves more convenient for automobile owners. When a driver with a key fob is in the vicinity of the vehicle, doors automatically unlock on user command. However, unfortunately, it has been shown that these keyless entry systems are vulnerable to signal relaying attacks. While it is evident that automobile manufacturers incorporate preventative methods to secure these keyless entry systems, they continue to be vulnerable to a range of attacks. Relayed signals result in valid packets that are verified as legitimate, and this makes it is difficult to distinguish a legitimate door unlock request from a malicious signal. In response to this vulnerability, this paper presents an RF fingerprinting method (coined HOld the DOoR, HODOR) to detect attacks on keyless entry systems the first attempt to exploit the RF fingerprint technique in the automotive domain. HODOR is designed as a sub authentication method that supports existing authentication systems for keyless entry systems and does not require any modification of the main system to perform. Through a series of experiments, the results demonstrate that HODOR competently and reliably detects attacks on keyless entry systems. HODOR achieves both an average false positive rate (FPR) of 0.27 percent with a false negative rate (FNR) of 0 percent for the detection of simulated attacks, corresponding to current research on keyless entry car theft.

</details>

<details>

<summary>2020-03-30 18:51:59 - DeepHammer: Depleting the Intelligence of Deep Neural Networks through Targeted Chain of Bit Flips</summary>

- *Fan Yao, Adnan Siraj Rakin, Deliang Fan*

- `2003.13746v1` - [abs](http://arxiv.org/abs/2003.13746v1) - [pdf](http://arxiv.org/pdf/2003.13746v1)

> Security of machine learning is increasingly becoming a major concern due to the ubiquitous deployment of deep learning in many security-sensitive domains. Many prior studies have shown external attacks such as adversarial examples that tamper with the integrity of DNNs using maliciously crafted inputs. However, the security implication of internal threats (i.e., hardware vulnerability) to DNN models has not yet been well understood. In this paper, we demonstrate the first hardware-based attack on quantized deep neural networks-DeepHammer-that deterministically induces bit flips in model weights to compromise DNN inference by exploiting the rowhammer vulnerability. DeepHammer performs aggressive bit search in the DNN model to identify the most vulnerable weight bits that are flippable under system constraints. To trigger deterministic bit flips across multiple pages within reasonable amount of time, we develop novel system-level techniques that enable fast deployment of victim pages, memory-efficient rowhammering and precise flipping of targeted bits. DeepHammer can deliberately degrade the inference accuracy of the victim DNN system to a level that is only as good as random guess, thus completely depleting the intelligence of targeted DNN systems. We systematically demonstrate our attacks on real systems against 12 DNN architectures with 4 different datasets and different application domains. Our evaluation shows that DeepHammer is able to successfully tamper DNN inference behavior at run-time within a few minutes. We further discuss several mitigation techniques from both algorithm and system levels to protect DNNs against such attacks. Our work highlights the need to incorporate security mechanisms in future deep learning system to enhance the robustness of DNN against hardware-based deterministic fault injections.

</details>

<details>

<summary>2020-03-31 05:10:40 - Adversarial Imitation Attack</summary>

- *Mingyi Zhou, Jing Wu, Yipeng Liu, Xiaolin Huang, Shuaicheng Liu, Xiang Zhang, Ce Zhu*

- `2003.12760v2` - [abs](http://arxiv.org/abs/2003.12760v2) - [pdf](http://arxiv.org/pdf/2003.12760v2)

> Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the attacked models. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the attacked model by a two-player game like the generative adversarial networks (GANs). The objective of the generative model is to generate examples that lead the imitation model returning different outputs with the attacked model. The objective of the imitation model is to output the same labels with the attacked model under the same inputs. Then, the adversarial examples generated by the imitation model are utilized to fool the attacked model. Compared with the current substitute attacks, imitation attacks can use less training data to produce a replica of the attacked model and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query.

</details>

<details>

<summary>2020-03-31 06:21:03 - A Thorough Comparison Study on Adversarial Attacks and Defenses for Common Thorax Disease Classification in Chest X-rays</summary>

- *Chendi Rao, Jiezhang Cao, Runhao Zeng, Qi Chen, Huazhu Fu, Yanwu Xu, Mingkui Tan*

- `2003.13969v1` - [abs](http://arxiv.org/abs/2003.13969v1) - [pdf](http://arxiv.org/pdf/2003.13969v1)

> Recently, deep neural networks (DNNs) have made great progress on automated diagnosis with chest X-rays images. However, DNNs are vulnerable to adversarial examples, which may cause misdiagnoses to patients when applying the DNN based methods in disease detection. Recently, there is few comprehensive studies exploring the influence of attack and defense methods on disease detection, especially for the multi-label classification problem. In this paper, we aim to review various adversarial attack and defense methods on chest X-rays. First, the motivations and the mathematical representations of attack and defense methods are introduced in details. Second, we evaluate the influence of several state-of-the-art attack and defense methods for common thorax disease classification in chest X-rays. We found that the attack and defense methods have poor performance with excessive iterations and large perturbations. To address this, we propose a new defense method that is robust to different degrees of perturbations. This study could provide new insights into methodological development for the community.

</details>

<details>

<summary>2020-03-31 11:55:18 - When the Guard failed the Droid: A case study of Android malware</summary>

- *Harel Berger, Chen Hajaj, Amit Dvir*

- `2003.14123v1` - [abs](http://arxiv.org/abs/2003.14123v1) - [pdf](http://arxiv.org/pdf/2003.14123v1)

> Android malware is a persistent threat to billions of users around the world. As a countermeasure, Android malware detection systems are occasionally implemented. However, these systems are often vulnerable to \emph{evasion attacks}, in which an adversary manipulates malicious instances so that they are misidentified as benign. In this paper, we launch various innovative evasion attacks against several Android malware detection systems. The vulnerability inherent to all of these systems is that they are part of Androguard~\cite{desnos2011androguard}, a popular open source library used in Android malware detection systems. Some of the detection systems decrease to a 0\% detection rate after the attack. Therefore, the use of open source libraries in malware detection systems calls for caution.   In addition, we present a novel evaluation scheme for evasion attack generation that exploits the weak spots of known Android malware detection systems. In so doing, we evaluate the functionality and maliciousness of the manipulated instances created by our evasion attacks. We found variations in both the maliciousness and functionality tests of our manipulated apps. We show that non-functional apps, while considered malicious, do not threaten users and are thus useless from an attacker's point of view. We conclude that evasion attacks must be assessed for both functionality and maliciousness to evaluate their impact, a step which is far from commonplace today.

</details>

<details>

<summary>2020-03-31 15:25:06 - DaST: Data-free Substitute Training for Adversarial Attacks</summary>

- *Mingyi Zhou, Jing Wu, Yipeng Liu, Shuaicheng Liu, Ce Zhu*

- `2003.12703v2` - [abs](http://arxiv.org/abs/2003.12703v2) - [pdf](http://arxiv.org/pdf/2003.12703v2)

> Machine learning models are vulnerable to adversarial examples. For the black-box setting, current substitute attacks need pre-trained models to generate adversarial examples. However, pre-trained models are hard to obtain in real-world tasks. In this paper, we propose a data-free substitute training method (DaST) to obtain substitute models for adversarial black-box attacks without the requirement of any real data. To achieve this, DaST utilizes specially designed generative adversarial networks (GANs) to train the substitute models. In particular, we design a multi-branch architecture and label-control loss for the generative model to deal with the uneven distribution of synthetic samples. The substitute model is then trained by the synthetic samples generated by the generative model, which are labeled by the attacked model subsequently. The experiments demonstrate the substitute models produced by DaST can achieve competitive performance compared with the baseline models which are trained by the same train set with attacked models. Additionally, to evaluate the practicability of the proposed method on the real-world task, we attack an online machine learning model on the Microsoft Azure platform. The remote model misclassifies 98.35% of the adversarial examples crafted by our method. To the best of our knowledge, we are the first to train a substitute model for adversarial attacks without any real data.

</details>


## 2020-04

<details>

<summary>2020-04-01 10:07:15 - Research of Caller ID Spoofing Launch, Detection, and Defense</summary>

- *Volodymyr Buriachok, Volodymyr Sokolov, Mahyar TajDini*

- `2004.00318v1` - [abs](http://arxiv.org/abs/2004.00318v1) - [pdf](http://arxiv.org/pdf/2004.00318v1)

> Caller ID parodying produces the valid Caller character, in this manner deciding seem to start from another client. This apparently basic assault strategy has been utilized in the developing communication fake and trick calls, bringing about significant financial trouble. Unfortunately, callerID spoofing is easy to implement but yet it is difficult to have protection against it. In addition, there are not effective and defense solutions available right now. In this research it is suggested the CIVE (Callee Inference & VErification), a compelling and viable guard against Caller ID spoofing. This way it is described how it's possible to lunch call spoofing and between line describe how CIVE approach method can help to prevent somehow this kind of attacks. Caller ID Spoofing could cause huge financial and political issues special nowadays, when many things even sometimes authentication and verification are available by phone call, like banks approving transactions or two factor authentications and many other things. We believe critical industries specially banks and payment service providers should be protected against such vulnerabilities with their system and make an approach to prevent it, also it is very important to learn people specially who has special social place like politicians or celebrities to know such kind of attack are already exist. For this paper we implemented a call from white house to show there is no limitation and no matter whom you try to spoof, but destination which is the victim receive the call and that make this attack vector dangerous. And even modern communication and even devices like 4G and smart phones are not able to prevent or even detect this kind of attack. This study is a demonstration of the vulnerabilities available. All experiments were conducted on isolated mock-ups.

</details>

<details>

<summary>2020-04-01 12:41:45 - An Overview of Federated Deep Learning Privacy Attacks and Defensive Strategies</summary>

- *David Enthoven, Zaid Al-Ars*

- `2004.04676v1` - [abs](http://arxiv.org/abs/2004.04676v1) - [pdf](http://arxiv.org/pdf/2004.04676v1)

> With the increased attention and legislation for data-privacy, collaborative machine learning (ML) algorithms are being developed to ensure the protection of private data used for processing. Federated learning (FL) is the most popular of these methods, which provides privacy preservation by facilitating collaborative training of a shared model without the need to exchange any private data with a centralized server. Rather, an abstraction of the data in the form of a machine learning model update is sent. Recent studies showed that such model updates may still very well leak private information and thus more structured risk assessment is needed. In this paper, we analyze existing vulnerabilities of FL and subsequently perform a literature review of the possible attack methods targetingFL privacy protection capabilities. These attack methods are then categorized by a basic taxonomy. Additionally, we provide a literature study of the most recent defensive strategies and algorithms for FL aimed to overcome these attacks. These defensive strategies are categorized by their respective underlying defence principle. The paper concludes that the application of a single defensive strategy is not enough to provide adequate protection to all available attack methods.

</details>

<details>

<summary>2020-04-01 13:12:57 - Certified Side Channels</summary>

- *Cesar Pereida García, Sohaib ul Hassan, Nicola Tuveri, Iaroslav Gridin, Alejandro Cabrera Aldaya, Billy Bob Brumley*

- `1909.01785v2` - [abs](http://arxiv.org/abs/1909.01785v2) - [pdf](http://arxiv.org/pdf/1909.01785v2)

> We demonstrate that the format in which private keys are persisted impacts Side Channel Analysis (SCA) security. Surveying several widely deployed software libraries, we investigate the formats they support, how they parse these keys, and what runtime decisions they make. We uncover a combination of weaknesses and vulnerabilities, in extreme cases inducing completely disjoint multi-precision arithmetic stacks deep within the cryptosystem level for keys that otherwise seem logically equivalent. Exploiting these vulnerabilities, we design and implement key recovery attacks utilizing signals ranging from electromagnetic (EM) emanations, to granular microarchitecture cache timings, to coarse traditional wall clock timings.

</details>

<details>

<summary>2020-04-01 15:40:36 - Adaptive Adversarial Attack on Scene Text Recognition</summary>

- *Xiaoyong Yuan, Pan He, Xiaolin Andy Li, Dapeng Oliver Wu*

- `1807.03326v3` - [abs](http://arxiv.org/abs/1807.03326v3) - [pdf](http://arxiv.org/pdf/1807.03326v3)

> Recent studies have shown that state-of-the-art deep learning models are vulnerable to the inputs with small perturbations (adversarial examples). We observe two critical obstacles in adversarial examples: (i) Strong adversarial attacks (e.g., C&W attack) require manually tuning hyper-parameters and take a long time to construct an adversarial example, making it impractical to attack real-time systems; (ii) Most of the studies focus on non-sequential tasks, such as image classification, yet only a few consider sequential tasks. In this work, we speed up adversarial attacks, especially on sequential learning tasks. By leveraging the uncertainty of each task, we directly learn the adaptive multi-task weightings, without manually searching hyper-parameters. A unified architecture is developed and evaluated for both non-sequential tasks and sequential ones. To validate the effectiveness, we take the scene text recognition task as a case study. To our best knowledge, our proposed method is the first attempt to adversarial attack for scene text recognition. Adaptive Attack achieves over 99.9\% success rate with 3-6X speedup compared to state-of-the-art adversarial attacks.

</details>

<details>

<summary>2020-04-01 17:59:59 - Evading Deepfake-Image Detectors with White- and Black-Box Attacks</summary>

- *Nicholas Carlini, Hany Farid*

- `2004.00622v1` - [abs](http://arxiv.org/abs/2004.00622v1) - [pdf](http://arxiv.org/pdf/2004.00622v1)

> It is now possible to synthesize highly realistic images of people who don't exist. Such content has, for example, been implicated in the creation of fraudulent social-media profiles responsible for dis-information campaigns. Significant efforts are, therefore, being deployed to detect synthetically-generated content. One popular forensic approach trains a neural network to distinguish real from synthetic content.   We show that such forensic classifiers are vulnerable to a range of attacks that reduce the classifier to near-0% accuracy. We develop five attack case studies on a state-of-the-art classifier that achieves an area under the ROC curve (AUC) of 0.95 on almost all existing image generators, when only trained on one generator. With full access to the classifier, we can flip the lowest bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb 1% of the image area to reduce the classifier's AUC to 0.08; or add a single noise pattern in the synthesizer's latent space to reduce the classifier's AUC to 0.17. We also develop a black-box attack that, with no access to the target classifier, reduces the AUC to 0.22. These attacks reveal significant vulnerabilities of certain image-forensic classifiers.

</details>

<details>

<summary>2020-04-01 23:14:52 - On adversarial patches: real-world attack on ArcFace-100 face recognition system</summary>

- *Mikhail Pautov, Grigorii Melnikov, Edgar Kaziakhmedov, Klim Kireev, Aleksandr Petiushko*

- `1910.07067v3` - [abs](http://arxiv.org/abs/1910.07067v3) - [pdf](http://arxiv.org/pdf/1910.07067v3)

> Recent works showed the vulnerability of image classifiers to adversarial attacks in the digital domain. However, the majority of attacks involve adding small perturbation to an image to fool the classifier. Unfortunately, such procedures can not be used to conduct a real-world attack, where adding an adversarial attribute to the photo is a more practical approach. In this paper, we study the problem of real-world attacks on face recognition systems. We examine security of one of the best public face recognition systems, LResNet100E-IR with ArcFace loss, and propose a simple method to attack it in the physical world. The method suggests creating an adversarial patch that can be printed, added as a face attribute and photographed; the photo of a person with such attribute is then passed to the classifier such that the classifier's recognized class changes from correct to the desired one. Proposed generating procedure allows projecting adversarial patches not only on different areas of the face, such as nose or forehead but also on some wearable accessory, such as eyeglasses.

</details>

<details>

<summary>2020-04-02 11:22:26 - CORSICA: Cross-Origin Web Service Identification</summary>

- *Christian Dresen, Fabian Ising, Damian Poddebniak, Tobias Kappert, Thorsten Holz, Sebastian Schinzel*

- `2004.00939v1` - [abs](http://arxiv.org/abs/2004.00939v1) - [pdf](http://arxiv.org/pdf/2004.00939v1)

> Vulnerabilities in private networks are difficult to detect for attackers outside of the network. While there are known methods for port scanning internal hosts that work by luring unwitting internal users to an external web page that hosts malicious JavaScript code, no such method for detailed and precise service identification is known. The reason is that the Same Origin Policy (SOP) prevents access to HTTP responses of other origins by default. We perform a structured analysis of loopholes in the SOP that can be used to identify web applications across network boundaries. For this, we analyze HTML5, CSS, and JavaScript features of standard-compliant web browsers that may leak sensitive information about cross-origin content. The results reveal several novel techniques, including leaking JavaScript function names or styles of cross-origin requests that are available in all common browsers. We implement and test these techniques in a tool called CORSICA. It can successfully identify 31 of 42 (74%) of web services running on different IoT devices as well as the version numbers of the four most widely used content management systems WordPress, Drupal, Joomla, and TYPO3. CORSICA can also determine the patch level on average down to three versions (WordPress), six versions (Drupal), two versions (Joomla), and four versions (TYPO3) with only ten requests on average. Furthermore, CORSICA is able to identify 48 WordPress plugins containing 65 vulnerabilities. Finally, we analyze mitigation strategies and show that the proposed but not yet implemented strategies Cross-Origin Resource Policy (CORP)} and Sec-Metadata would prevent our identification techniques.

</details>

<details>

<summary>2020-04-03 02:37:21 - Efficient UAV Physical Layer Security based on Deep Learning and Artificial Noise</summary>

- *Behrooz Khadem, Salar Mohebalizadeh*

- `2004.01343v1` - [abs](http://arxiv.org/abs/2004.01343v1) - [pdf](http://arxiv.org/pdf/2004.01343v1)

> Network-connected unmanned aerial vehicle (UAV) communications is a common solution to achieve high-rate image transmission. The broadcast nature of these wireless networks makes this communication vulnerable to eavesdropping. This paper considers the problem of compressed secret image transmission between two nodes, in the presence of a passive eavesdropper. In this paper, we use auto encoder/decoder convolutional neural networks, which by using deep learning algorithms, allow us to compress/decompress images. Also we use network physical layer features to generate high rate artificial noise to secure the data. Using features of the channel with applying artificial noises, reduce the channel capacity of the unauthorized users and prevent eavesdropper from detecting received data. Our simulation experiments show that for received data with SNR fewer than 5 in the authorized node, the MSE is less than 0.05.

</details>

<details>

<summary>2020-04-03 07:17:39 - A "Final" Security Bug</summary>

- *Quan Thoi Minh Nguyen*

- `2004.01403v1` - [abs](http://arxiv.org/abs/2004.01403v1) - [pdf](http://arxiv.org/pdf/2004.01403v1)

> This article discusses a fixed critical security bug in Google Tink's Ed25519 Java implementation. The bug allows remote attackers to extract the private key with only two Ed25519 signatures. The vulnerability comes from the misunderstanding of what "final" in Java programming language means. The bug was discovered during security review before Google Tink was officially released. It reinforces the challenge in writing safe cryptographic code and the importance of the security review process even for the code written by professional cryptographers.

</details>

<details>

<summary>2020-04-03 07:28:01 - Deep Face Representations for Differential Morphing Attack Detection</summary>

- *Ulrich Scherhag, Christian Rathgeb, Johannes Merkle, Christoph Busch*

- `2001.01202v2` - [abs](http://arxiv.org/abs/2001.01202v2) - [pdf](http://arxiv.org/pdf/2001.01202v2)

> The vulnerability of facial recognition systems to face morphing attacks is well known. Many different approaches for morphing attack detection have been proposed in the scientific literature. However, the morphing attack detection algorithms proposed so far have only been trained and tested on datasets whose distributions of image characteristics are either very limited (e.g. only created with a single morphing tool) or rather unrealistic (e.g. no print-scan transformation). As a consequence, these methods easily overfit on certain image types and the results presented cannot be expected to apply to real-world scenarios. For example, the results of the latest NIST Face Recognition Vendor Test MORPH show that the submitted MAD algorithms lack robustness and performance when considering unseen and challenging datasets. In this work, subsets of the FERET and FRGCv2 face databases are used to create a large realistic database for training and testing of morphing attack detection algorithms, containing a large number of ICAO-compliant bona fide facial images, corresponding unconstrained probe images, and morphed images created with four different tools. Furthermore, multiple post-processings are applied on the reference images, e.g. print-scan and JPEG2000 compression. On this database, previously proposed differential morphing algorithms are evaluated and compared. In addition, the application of deep face representations for differential morphing attack detection algorithms is investigated. It is shown that algorithms based on deep face representations can achieve very high detection performance (less than 3\%~\mbox{D-EER}) and robustness with respect to various post-processings. Finally, the limitations of the developed methods are analyzed.

</details>

<details>

<summary>2020-04-03 20:51:16 - Generative Forensics: Procedural Generation and Information Games</summary>

- *Michael Cook*

- `2004.01768v1` - [abs](http://arxiv.org/abs/2004.01768v1) - [pdf](http://arxiv.org/pdf/2004.01768v1)

> Procedural generation is used across game design to achieve a wide variety of ends, and has led to the creation of several game subgenres by injecting variance, surprise or unpredictability into otherwise static designs. Information games are a type of mystery game in which the player is tasked with gathering knowledge and developing an understanding of an event or system. Their reliance on player knowledge leaves them vulnerable to spoilers and hard to replay. In this paper we introduce the notion of generative forensics games, a subgenre of information games that challenge the player to understand the output of a generative system. We introduce information games, show how generative forensics develops the idea, report on two prototype games we created, and evaluate our work on generative forensics so far from a player and a designer perspective.

</details>

<details>

<summary>2020-04-03 23:05:17 - TRRespass: Exploiting the Many Sides of Target Row Refresh</summary>

- *Pietro Frigo, Emanuele Vannacci, Hasan Hassan, Victor van der Veen, Onur Mutlu, Cristiano Giuffrida, Herbert Bos, Kaveh Razavi*

- `2004.01807v1` - [abs](http://arxiv.org/abs/2004.01807v1) - [pdf](http://arxiv.org/pdf/2004.01807v1)

> After a plethora of high-profile RowHammer attacks, CPU and DRAM vendors scrambled to deliver what was meant to be the definitive hardware solution against the RowHammer problem: Target Row Refresh (TRR). A common belief among practitioners is that, for the latest generation of DDR4 systems that are protected by TRR, RowHammer is no longer an issue in practice. However, in reality, very little is known about TRR. In this paper, we demystify the inner workings of TRR and debunk its security guarantees. We show that what is advertised as a single mitigation mechanism is actually a series of different solutions coalesced under the umbrella term TRR. We inspect and disclose, via a deep analysis, different existing TRR solutions and demonstrate that modern implementations operate entirely inside DRAM chips. Despite the difficulties of analyzing in-DRAM mitigations, we describe novel techniques for gaining insights into the operation of these mitigation mechanisms. These insights allow us to build TRRespass, a scalable black-box RowHammer fuzzer. TRRespass shows that even the latest generation DDR4 chips with in-DRAM TRR, immune to all known RowHammer attacks, are often still vulnerable to new TRR-aware variants of RowHammer that we develop. In particular, TRRespass finds that, on modern DDR4 modules, RowHammer is still possible when many aggressor rows are used (as many as 19 in some cases), with a method we generally refer to as Many-sided RowHammer. Overall, our analysis shows that 13 out of the 42 modules from all three major DRAM vendors are vulnerable to our TRR-aware RowHammer access patterns, and thus one can still mount existing state-of-the-art RowHammer attacks. In addition to DDR4, we also experiment with LPDDR4 chips and show that they are susceptible to RowHammer bit flips too. Our results provide concrete evidence that the pursuit of better RowHammer mitigations must continue.

</details>

<details>

<summary>2020-04-04 16:21:13 - The Paradox of Information Access: Growing Isolation in the Age of Sharing</summary>

- *Tarek Abdelzaher, Heng Ji, Jinyang Li, Chaoqi Yang, John Dellaverson, Lixia Zhang, Chao Xu, Boleslaw K. Szymanski*

- `2004.01967v1` - [abs](http://arxiv.org/abs/2004.01967v1) - [pdf](http://arxiv.org/pdf/2004.01967v1)

> Modern online media, such as Twitter, Instagram, and YouTube, enable anyone to become an information producer and to offer online content for potentially global consumption. By increasing the amount of globally accessible real-time information, today's ubiquitous producers contribute to a world, where an individual consumes vanishingly smaller fractions of all produced content. In general, consumers preferentially select information that closely matches their individual views and values. The bias inherent in such selection is further magnified by today's information curation services that maximize user engagement (and thus service revenue) by filtering new content in accordance with observed consumer preferences. Consequently, individuals get exposed to increasingly narrower bands of the ideology spectrum. Societies get fragmented into increasingly ideologically isolated enclaves. These enclaves (or echo-chambers) then become vulnerable to misinformation spread, which in turn further magnifies polarization and bias. We call this dynamic the paradox of information access; a growing ideological fragmentation in the age of sharing. This article describes the technical, economic, and socio-cognitive contributors to this paradox, and explores research directions towards its mitigation.

</details>

<details>

<summary>2020-04-04 21:09:14 - JackHammer: Efficient Rowhammer on Heterogeneous FPGA-CPU Platforms</summary>

- *Zane Weissman, Thore Tiemann, Daniel Moghimi, Evan Custodio, Thomas Eisenbarth, Berk Sunar*

- `1912.11523v3` - [abs](http://arxiv.org/abs/1912.11523v3) - [pdf](http://arxiv.org/pdf/1912.11523v3)

> After years of development, FPGAs are finally making an appearance on multi-tenant cloud servers. These heterogeneous FPGA-CPU architectures break common assumptions about isolation and security boundaries. Since the FPGA and CPU architectures share hardware resources, a new class of vulnerabilities requires us to reassess the security and dependability of these platforms.   In this work, we analyze the memory and cache subsystem and study Rowhammer and cache attacks enabled on two proposed heterogeneous FPGA-CPU platforms by Intel: the Arria 10 GX with an integrated FPGA-CPU platform, and the Arria 10 GX PAC expansion card which connects the FPGA to the CPU via the PCIe interface. We show that while Intel PACs currently are immune to cache attacks from FPGA to CPU, the integrated platform is indeed vulnerable to Prime and Probe style attacks from the FPGA to the CPU's last level cache. Further, we demonstrate JackHammer, a novel and efficient Rowhammer from the FPGA to the host's main memory. Our results indicate that a malicious FPGA can perform twice as fast as a typical Rowhammer attack from the CPU on the same system and causes around four times as many bit flips as the CPU attack. We demonstrate the efficacy of JackHammer from the FPGA through a realistic fault attack on the WolfSSL RSA signing implementation that reliably causes a fault after an average of fifty-eight RSA signatures, 25% faster than a CPU rowhammer attack. In some scenarios our JackHammer attack produces faulty signatures more than three times more often and almost three times faster than a conventional CPU rowhammer attack.

</details>

<details>

<summary>2020-04-06 07:43:00 - Saturation Memory Access: Mitigating Memory Spatial Errors without Terminating Programs</summary>

- *Dongwei Chen, Daliang Xu, Dong Tong, Kang Sun, Xuetao Guan, Chun Yang, Xu Cheng*

- `2002.02831v2` - [abs](http://arxiv.org/abs/2002.02831v2) - [pdf](http://arxiv.org/pdf/2002.02831v2)

> Memory spatial errors, i.e., buffer overflow vulnerabilities, have been a well-known issue in computer security for a long time and remain one of the root causes of exploitable vulnerabilities. Most of the existing mitigation tools adopt a fail-stop strategy to protect programs from intrusions, which means the victim program will be terminated upon detecting a memory safety violation. Unfortunately, the fail-stop strategy harms the availability of software.   In this paper, we propose Saturation Memory Access (SMA), a memory spatial error mitigation mechanism that prevents out-of-bounds access without terminating a program. SMA is based on a key observation that developers generally do not rely on out-of-bounds accesses to implement program logic. SMA modifies dynamic memory allocators and adds paddings to objects to form an enlarged object boundary. By dynamically correcting all the out-of-bounds accesses to operate on the enlarged protecting boundaries, SMA can tolerate out-of-bounds accesses. For the sake of compatibility, we chose tagged pointers to record the boundary metadata of a memory object in the pointer itself, and correct the address upon detecting out-of-bounds access.   We have implemented the prototype of SMA on LLVM 10.0. Our results show that our compiler enables the programs to execute successfully through buffer overflow attacks. Experiments on MiBench show that our prototype incurs an overhead of 78\%. Further optimizations would require ISA supports.

</details>

<details>

<summary>2020-04-06 10:52:25 - Discovering associations in COVID-19 related research papers</summary>

- *Iztok Fister Jr., Karin Fister, Iztok Fister*

- `2004.03397v1` - [abs](http://arxiv.org/abs/2004.03397v1) - [pdf](http://arxiv.org/pdf/2004.03397v1)

> A COVID-19 pandemic has already proven itself to be a global challenge. It proves how vulnerable humanity can be. It has also mobilized researchers from different sciences and different countries in the search for a way to fight this potentially fatal disease. In line with this, our study analyses the abstracts of papers related to COVID-19 and coronavirus-related-research using association rule text mining in order to find the most interestingness words, on the one hand, and relationships between them on the other. Then, a method, called information cartography, was applied for extracting structured knowledge from a huge amount of association rules. On the basis of these methods, the purpose of our study was to show how researchers have responded in similar epidemic/pandemic situations throughout history.

</details>

<details>

<summary>2020-04-06 22:57:23 - Challenges in Forecasting Malicious Events from Incomplete Data</summary>

- *Nazgol Tavabi, Andrés Abeliuk, Negar Mokhberian, Jeremy Abramson, Kristina Lerman*

- `2004.04597v1` - [abs](http://arxiv.org/abs/2004.04597v1) - [pdf](http://arxiv.org/pdf/2004.04597v1)

> The ability to accurately predict cyber-attacks would enable organizations to mitigate their growing threat and avert the financial losses and disruptions they cause. But how predictable are cyber-attacks? Researchers have attempted to combine external data -- ranging from vulnerability disclosures to discussions on Twitter and the darkweb -- with machine learning algorithms to learn indicators of impending cyber-attacks. However, successful cyber-attacks represent a tiny fraction of all attempted attacks: the vast majority are stopped, or filtered by the security appliances deployed at the target. As we show in this paper, the process of filtering reduces the predictability of cyber-attacks. The small number of attacks that do penetrate the target's defenses follow a different generative process compared to the whole data which is much harder to learn for predictive models. This could be caused by the fact that the resulting time series also depends on the filtering process in addition to all the different factors that the original time series depended on. We empirically quantify the loss of predictability due to filtering using real-world data from two organizations. Our work identifies the limits to forecasting cyber-attacks from highly filtered data.

</details>

<details>

<summary>2020-04-07 12:00:40 - Feature Partitioning for Robust Tree Ensembles and their Certification in Adversarial Scenarios</summary>

- *Stefano Calzavara, Claudio Lucchese, Federico Marcuzzi, Salvatore Orlando*

- `2004.03295v1` - [abs](http://arxiv.org/abs/2004.03295v1) - [pdf](http://arxiv.org/pdf/2004.03295v1)

> Machine learning algorithms, however effective, are known to be vulnerable in adversarial scenarios where a malicious user may inject manipulated instances. In this work we focus on evasion attacks, where a model is trained in a safe environment and exposed to attacks at test time. The attacker aims at finding a minimal perturbation of a test instance that changes the model outcome.   We propose a model-agnostic strategy that builds a robust ensemble by training its basic models on feature-based partitions of the given dataset. Our algorithm guarantees that the majority of the models in the ensemble cannot be affected by the attacker. We experimented the proposed strategy on decision tree ensembles, and we also propose an approximate certification method for tree ensembles that efficiently assess the minimal accuracy of a forest on a given dataset avoiding the costly computation of evasion attacks.   Experimental evaluation on publicly available datasets shows that proposed strategy outperforms state-of-the-art adversarial learning algorithms against evasion attacks.

</details>

<details>

<summary>2020-04-07 14:22:10 - Universal Adversarial Perturbations Generative Network for Speaker Recognition</summary>

- *Jiguo Li, Xinfeng Zhang, Chuanmin Jia, Jizheng Xu, Li Zhang, Yue Wang, Siwei Ma, Wen Gao*

- `2004.03428v1` - [abs](http://arxiv.org/abs/2004.03428v1) - [pdf](http://arxiv.org/pdf/2004.03428v1)

> Attacking deep learning based biometric systems has drawn more and more attention with the wide deployment of fingerprint/face/speaker recognition systems, given the fact that the neural networks are vulnerable to the adversarial examples, which have been intentionally perturbed to remain almost imperceptible for human. In this paper, we demonstrated the existence of the universal adversarial perturbations~(UAPs) for the speaker recognition systems. We proposed a generative network to learn the mapping from the low-dimensional normal distribution to the UAPs subspace, then synthesize the UAPs to perturbe any input signals to spoof the well-trained speaker recognition model with high probability. Experimental results on TIMIT and LibriSpeech datasets demonstrate the effectiveness of our model.

</details>

<details>

<summary>2020-04-07 14:29:28 - Learning to fool the speaker recognition</summary>

- *Jiguo Li, Xinfeng Zhang, Jizheng Xu, Li Zhang, Yue Wang, Siwei Ma, Wen Gao*

- `2004.03434v1` - [abs](http://arxiv.org/abs/2004.03434v1) - [pdf](http://arxiv.org/pdf/2004.03434v1)

> Due to the widespread deployment of fingerprint/face/speaker recognition systems, attacking deep learning based biometric systems has drawn more and more attention. Previous research mainly studied the attack to the vision-based system, such as fingerprint and face recognition. While the attack for speaker recognition has not been investigated yet, although it has been widely used in our daily life. In this paper, we attempt to fool the state-of-the-art speaker recognition model and present \textit{speaker recognition attacker}, a lightweight model to fool the deep speaker recognition model by adding imperceptible perturbations onto the raw speech waveform. We find that the speaker recognition system is also vulnerable to the attack, and we achieve a high success rate on the non-targeted attack. Besides, we also present an effective method to optimize the speaker recognition attacker to obtain a trade-off between the attack success rate with the perceptual quality. Experiments on the TIMIT dataset show that we can achieve a sentence error rate of $99.2\%$ with an average SNR $57.2\text{dB}$ and PESQ 4.2 with speed rather faster than real-time.

</details>

<details>

<summary>2020-04-07 22:04:52 - Practical Data Poisoning Attack against Next-Item Recommendation</summary>

- *Hengtong Zhang, Yaliang Li, Bolin Ding, Jing Gao*

- `2004.03728v1` - [abs](http://arxiv.org/abs/2004.03728v1) - [pdf](http://arxiv.org/pdf/2004.03728v1)

> Online recommendation systems make use of a variety of information sources to provide users the items that users are potentially interested in. However, due to the openness of the online platform, recommendation systems are vulnerable to data poisoning attacks. Existing attack approaches are either based on simple heuristic rules or designed against specific recommendations approaches. The former often suffers unsatisfactory performance, while the latter requires strong knowledge of the target system. In this paper, we focus on a general next-item recommendation setting and propose a practical poisoning attack approach named LOKI against blackbox recommendation systems. The proposed LOKI utilizes the reinforcement learning algorithm to train the attack agent, which can be used to generate user behavior samples for data poisoning. In real-world recommendation systems, the cost of retraining recommendation models is high, and the interaction frequency between users and a recommendation system is restricted.Given these real-world restrictions, we propose to let the agent interact with a recommender simulator instead of the target recommendation system and leverage the transferability of the generated adversarial samples to poison the target system. We also propose to use the influence function to efficiently estimate the influence of injected samples on the recommendation results, without re-training the models within the simulator. Extensive experiments on two datasets against four representative recommendation models show that the proposed LOKI achieves better attacking performance than existing methods.

</details>

<details>

<summary>2020-04-07 22:38:49 - Learning to Detect Head Movement in Unconstrained Remote Gaze Estimation in the Wild</summary>

- *Zhecan Wang, Jian Zhao, Cheng Lu, Han Huang, Fan Yang, Lianji Li, Yandong Guo*

- `2004.03737v1` - [abs](http://arxiv.org/abs/2004.03737v1) - [pdf](http://arxiv.org/pdf/2004.03737v1)

> Unconstrained remote gaze estimation remains challenging mostly due to its vulnerability to the large variability in head-pose. Prior solutions struggle to maintain reliable accuracy in unconstrained remote gaze tracking. Among them, appearance-based solutions demonstrate tremendous potential in improving gaze accuracy. However, existing works still suffer from head movement and are not robust enough to handle real-world scenarios. Especially most of them study gaze estimation under controlled scenarios where the collected datasets often cover limited ranges of both head-pose and gaze which introduces further bias. In this paper, we propose novel end-to-end appearance-based gaze estimation methods that could more robustly incorporate different levels of head-pose representations into gaze estimation. Our method could generalize to real-world scenarios with low image quality, different lightings and scenarios where direct head-pose information is not available. To better demonstrate the advantage of our methods, we further propose a new benchmark dataset with the most rich distribution of head-gaze combination reflecting real-world scenarios. Extensive evaluations on several public datasets and our own dataset demonstrate that our method consistently outperforms the state-of-the-art by a significant margin.

</details>

<details>

<summary>2020-04-07 23:02:37 - Towards Evaluating the Robustness of Chinese BERT Classifiers</summary>

- *Boxin Wang, Boyuan Pan, Xin Li, Bo Li*

- `2004.03742v1` - [abs](http://arxiv.org/abs/2004.03742v1) - [pdf](http://arxiv.org/pdf/2004.03742v1)

> Recent advances in large-scale language representation models such as BERT have improved the state-of-the-art performances in many NLP tasks. Meanwhile, character-level Chinese NLP models, including BERT for Chinese, have also demonstrated that they can outperform the existing models. In this paper, we show that, however, such BERT-based models are vulnerable under character-level adversarial attacks. We propose a novel Chinese char-level attack method against BERT-based classifiers. Essentially, we generate "small" perturbation on the character level in the embedding space and guide the character substitution procedure. Extensive experiments show that the classification accuracy on a Chinese news dataset drops from 91.8% to 0% by manipulating less than 2 characters on average based on the proposed attack. Human evaluations also confirm that our generated Chinese adversarial examples barely affect human performance on these NLP tasks.

</details>

<details>

<summary>2020-04-08 01:26:16 - Governance of the Internet of Things (IoT)</summary>

- *Lawrence J. Trautman, Mohammed T. Hussein, Louis Ngamassi, Mason J. Molesky*

- `2004.03765v1` - [abs](http://arxiv.org/abs/2004.03765v1) - [pdf](http://arxiv.org/pdf/2004.03765v1)

> Today's increasing rate of technological change results from the rapid growth in computer processing speed, when combined with the cost decline of processing capacity, and is of historical import. The daily life of billions of individuals worldwide has been forever changed by technology in just the last few years. Costly data breaches continue at an alarming rate. The challenge facing humans as they attempt to govern the process of artificial intelligence, machine learning, and the impact of billions of sensory devices connected to the Internet is the subject of this Article.   We proceed in nine sections. First, we define the Internet of Things (IoT), comment on the explosive growth in sensory devices connected to the Internet, provide examples of IoT devices, and speak to the promise of the IoT. Second, we discuss legal requirements for corporate governance as a foundation for considering the challenge of governing the IoT. Third, we look at potential IoT threats. Fourth, we discuss the Mirai botnet. Fifth, is a look at the IoT threat vector vulnerabilities during times of crisis. Sixth, we discuss the Manufactured Usage Description (MUD) methodology. Seventh, is a discussion of recent regulatory developments. Next, we look at a few recommendations. And finally, we conclude. We believe this Article contributes to our understanding of the widespread exposure to malware associated with IoT and adds to the nascent but emerging literature on governance of enterprise risk, a subject of vital societal importance.

</details>

<details>

<summary>2020-04-08 17:00:22 - Dependency-Based Neural Representations for Classifying Lines of Programs</summary>

- *Shashank Srikant, Nicolas Lesimple, Una-May O'Reilly*

- `2004.10166v1` - [abs](http://arxiv.org/abs/2004.10166v1) - [pdf](http://arxiv.org/pdf/2004.10166v1)

> We investigate the problem of classifying a line of program as containing a vulnerability or not using machine learning. Such a line-level classification task calls for a program representation which goes beyond reasoning from the tokens present in the line. We seek a distributed representation in a latent feature space which can capture the control and data dependencies of tokens appearing on a line of program, while also ensuring lines of similar meaning have similar features. We present a neural architecture, Vulcan, that successfully demonstrates both these requirements. It extracts contextual information about tokens in a line and inputs them as Abstract Syntax Tree (AST) paths to a bi-directional LSTM with an attention mechanism. It concurrently represents the meanings of tokens in a line by recursively embedding the lines where they are most recently defined. In our experiments, Vulcan compares favorably with a state-of-the-art classifier, which requires significant preprocessing of programs, suggesting the utility of using deep learning to model program dependence information.

</details>

<details>

<summary>2020-04-08 23:10:10 - Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment</summary>

- *Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits*

- `1907.11932v6` - [abs](http://arxiv.org/abs/1907.11932v6) - [pdf](http://arxiv.org/pdf/1907.11932v6)

> Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.

</details>

<details>

<summary>2020-04-09 01:36:23 - TOG: Targeted Adversarial Objectness Gradient Attacks on Real-time Object Detection Systems</summary>

- *Ka-Ho Chow, Ling Liu, Mehmet Emre Gursoy, Stacey Truex, Wenqi Wei, Yanzhao Wu*

- `2004.04320v1` - [abs](http://arxiv.org/abs/2004.04320v1) - [pdf](http://arxiv.org/pdf/2004.04320v1)

> The rapid growth of real-time huge data capturing has pushed the deep learning and data analytic computing to the edge systems. Real-time object recognition on the edge is one of the representative deep neural network (DNN) powered edge systems for real-world mission-critical applications, such as autonomous driving and augmented reality. While DNN powered object detection edge systems celebrate many life-enriching opportunities, they also open doors for misuse and abuse. This paper presents three Targeted adversarial Objectness Gradient attacks, coined as TOG, which can cause the state-of-the-art deep object detection networks to suffer from object-vanishing, object-fabrication, and object-mislabeling attacks. We also present a universal objectness gradient attack to use adversarial transferability for black-box attacks, which is effective on any inputs with negligible attack time cost, low human perceptibility, and particularly detrimental to object detection edge systems. We report our experimental measurements using two benchmark datasets (PASCAL VOC and MS COCO) on two state-of-the-art detection algorithms (YOLO and SSD). The results demonstrate serious adversarial vulnerabilities and the compelling need for developing robust object detection systems.

</details>

<details>

<summary>2020-04-09 10:24:35 - Detection and Analysis of Drive-by Downloads and Malicious Websites</summary>

- *Saeed Ibrahim, Nawwaf Al Herami, Ebrahim Al Naqbi, Monther Aldwairi*

- `2002.08007v2` - [abs](http://arxiv.org/abs/2002.08007v2) - [pdf](http://arxiv.org/pdf/2002.08007v2)

> A drive by download is a download that occurs without users action or knowledge. It usually triggers an exploit of vulnerability in a browser to downloads an unknown file. The malicious program in the downloaded file installs itself on the victims machine. Moreover, the downloaded file can be camouflaged as an installer that would further install malicious software. Drive by downloads is a very good example of the exponential increase in malicious activity over the Internet and how it affects the daily use of the web. In this paper, we try to address the problem caused by drive by downloads from different standpoints. We provide in depth understanding of the difficulties in dealing with drive by downloads and suggest appropriate solutions. We propose machine learning and feature selection solutions to remedy the the drive-by download problem. Experimental results reported 98.2% precision, 98.2% F-Measure and 97.2% ROC area.

</details>

<details>

<summary>2020-04-09 11:52:59 - Efficient and Secure Flash-based Gaming CAPTCH</summary>

- *Monther Aldwairi, Suaad Mohammed, Megana Lakshmi Padmanabhan*

- `2004.04497v1` - [abs](http://arxiv.org/abs/2004.04497v1) - [pdf](http://arxiv.org/pdf/2004.04497v1)

> With the growth of connectivity to smart grids, new applications, and the changing interaction between customer and energy clouds, clouds are more vulnerable to denial-of-service attacks. Efficient detection methods are required to authenticate, detect and control attackers. Completely Automated Public Turing test to tell Computers and Humans Apart, CAPTCHA, is one efficient tool to thwart denial of service attacks. The server presents the user with a client puzzle to solve in order to gain access to the service or website. The puzzle should be hard enough for computers, but easy for humans to solve. Several methods have been suggested including the popular image-based, as well as video-based, and text-based CAPTCHAs. In this paper, we present a new Flash-based gaming CAPTCHA to differentiate bots from humans. We propose a drag and drop client puzzle where the user will play a simple game to answer a visual question. Our method turns out to be convenient, easy for users and challenging for bots. Additionally, it has gaming aspect, which makes it interesting to users of all age groups.

</details>

<details>

<summary>2020-04-09 13:24:29 - Network disruption: maximizing disagreement and polarization in social networks</summary>

- *Mayee F. Chen, Miklos Z. Racz*

- `2003.08377v2` - [abs](http://arxiv.org/abs/2003.08377v2) - [pdf](http://arxiv.org/pdf/2003.08377v2)

> Recent years have seen a marked increase in the spread of misinformation, a phenomenon which has been accelerated and amplified by social media such as Facebook and Twitter. While some actors spread misinformation to push a specific agenda, it has also been widely documented that others aim to simply disrupt the network by increasing disagreement and polarization across the network and thereby destabilizing society. Popular social networks are also vulnerable to large-scale attacks. Motivated by this reality, we introduce a simple model of network disruption where an adversary can take over a limited number of user profiles in a social network with the aim of maximizing disagreement and/or polarization in the network.   We investigate this model both theoretically and empirically. We show that the adversary will always change the opinion of a taken-over profile to an extreme in order to maximize disruption. We also prove that an adversary can increase disagreement / polarization at most linearly in the number of user profiles it takes over. Furthermore, we present a detailed empirical study of several natural algorithms for the adversary on both synthetic networks and real world (Reddit and Twitter) data sets. These show that even simple, unsophisticated heuristics, such as targeting centrists, can disrupt a network effectively, causing a large increase in disagreement / polarization. Studying the problem of network disruption through the lens of an adversary thus highlights the seriousness of the problem.

</details>

<details>

<summary>2020-04-09 16:10:22 - The Web is Still Small After More Than a Decade</summary>

- *Nguyen Phong Hoang, Arian Akhavan Niaki, Michalis Polychronakis, Phillipa Gill*

- `2004.04623v1` - [abs](http://arxiv.org/abs/2004.04623v1) - [pdf](http://arxiv.org/pdf/2004.04623v1)

> Understanding web co-location is essential for various reasons. For instance, it can help one to assess the collateral damage that denial-of-service attacks or IP-based blocking can cause to the availability of co-located web sites. However, it has been more than a decade since the first study was conducted in 2007. The Internet infrastructure has changed drastically since then, necessitating a renewed study to comprehend the nature of web co-location.   In this paper, we conduct an empirical study to revisit web co-location using datasets collected from active DNS measurements. Our results show that the web is still small and centralized to a handful of hosting providers. More specifically, we find that more than 60% of web sites are co-located with at least ten other web sites---a group comprising less popular web sites. In contrast, 17.5% of mostly popular web sites are served from their own servers.   Although a high degree of web co-location could make co-hosted sites vulnerable to DoS attacks, our findings show that it is an increasing trend to co-host many web sites and serve them from well-provisioned content delivery networks (CDN) of major providers that provide advanced DoS protection benefits. Regardless of the high degree of web co-location, our analyses of popular block lists indicate that IP-based blocking does not cause severe collateral damage as previously thought.

</details>

<details>

<summary>2020-04-11 18:21:06 - Robust Generalised Quadratic Discriminant Analysis</summary>

- *Abhik Ghosh, Rita SahaRay, Sayan Chakrabarty, Sayan Bhadra*

- `2004.06568v1` - [abs](http://arxiv.org/abs/2004.06568v1) - [pdf](http://arxiv.org/pdf/2004.06568v1)

> Quadratic discriminant analysis (QDA) is a widely used statistical tool to classify observations from different multivariate Normal populations. The generalized quadratic discriminant analysis (GQDA) classification rule/classifier, which generalizes the QDA and the minimum Mahalanobis distance (MMD) classifiers to discriminate between populations with underlying elliptically symmetric distributions competes quite favorably with the QDA classifier when it is optimal and performs much better when QDA fails under non-Normal underlying distributions, e.g. Cauchy distribution. However, the classification rule in GQDA is based on the sample mean vector and the sample dispersion matrix of a training sample, which are extremely non-robust under data contamination. In real world, since it is quite common to face data highly vulnerable to outliers, the lack of robustness of the classical estimators of the mean vector and the dispersion matrix reduces the efficiency of the GQDA classifier significantly, increasing the misclassification errors. The present paper investigates the performance of the GQDA classifier when the classical estimators of the mean vector and the dispersion matrix used therein are replaced by various robust counterparts. Applications to various real data sets as well as simulation studies reveal far better performance of the proposed robust versions of the GQDA classifier. A Comparative study has been made to advocate the appropriate choice of the robust estimators to be used in a specific situation of the degree of contamination of the data sets.

</details>

<details>

<summary>2020-04-13 23:28:27 - ExTru: A Lightweight, Fast, and Secure Expirable Trust for the Internet of Things</summary>

- *Hadi Mardani Kamali, Kimia Zamiri Azar, Shervin Roshanisefat, Ashkan Vakil, Avesta Sasan*

- `2004.06235v1` - [abs](http://arxiv.org/abs/2004.06235v1) - [pdf](http://arxiv.org/pdf/2004.06235v1)

> The resource-constrained nature of the Internet of Things (IoT) devices, poses a challenge in designing a secure, reliable, and particularly high-performance communication for this family of devices. Although side-channel resistant ciphers (either block cipher or stream cipher) are the well-suited solution to establish a guaranteed secure communication, the energy-intensive nature of these ciphers makes them undesirable for particularly lightweight IoT solutions. In this paper, we introduce ExTru, a novel encrypted communication protocol based on stream ciphers that adds a configurable switching & toggling network (CSTN) to not only boost the performance of the communication in lightweight IoT devices, it also consumes far less energy compared with the conventional side-channel resistant ciphers. Although the overall structure of the proposed scheme is leaky against physical attacks, such as side-channel or new scan-based Boolean satisfiability (SAT) attack or algebraic attack, we introduce a dynamic encryption mechanism that removes this vulnerability. We demonstrate how each communicated message in the proposed scheme reduces the level of trust. Accordingly, since a specific number of messages, N, could break the communication and extract the key, by using the dynamic encryption mechanism, ExTru can re-initiate the level of trust periodically after T messages where T<N, to protect the communication against side-channel and scan-based attacks (e.g. SAT attack). Furthermore, we demonstrate that by properly configuring the value of T, ExTru not only increases the strength of security from per "device" to per "message", it also significantly improves energy consumption as well as throughput in comparison with an architecture that only uses a conventional side-channel resistant block/stream cipher.

</details>

<details>

<summary>2020-04-13 23:47:53 - AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning</summary>

- *Jinyuan Jia, Neil Zhenqiang Gong*

- `1805.04810v2` - [abs](http://arxiv.org/abs/1805.04810v2) - [pdf](http://arxiv.org/pdf/1805.04810v2)

> Users in various web and mobile applications are vulnerable to attribute inference attacks, in which an attacker leverages a machine learning classifier to infer a target user's private attributes (e.g., location, sexual orientation, political view) from its public data (e.g., rating scores, page likes). Existing defenses leverage game theory or heuristics based on correlations between the public data and attributes. These defenses are not practical. Specifically, game-theoretic defenses require solving intractable optimization problems, while correlation-based defenses incur large utility loss of users' public data.   In this paper, we present AttriGuard, a practical defense against attribute inference attacks. AttriGuard is computationally tractable and has small utility loss. Our AttriGuard works in two phases. Suppose we aim to protect a user's private attribute. In Phase I, for each value of the attribute, we find a minimum noise such that if we add the noise to the user's public data, then the attacker's classifier is very likely to infer the attribute value for the user. We find the minimum noise via adapting existing evasion attacks in adversarial machine learning. In Phase II, we sample one attribute value according to a certain probability distribution and add the corresponding noise found in Phase I to the user's public data. We formulate finding the probability distribution as solving a constrained convex optimization problem. We extensively evaluate AttriGuard and compare it with existing methods using a real-world dataset. Our results show that AttriGuard substantially outperforms existing methods. Our work is the first one that shows evasion attacks can be used as defensive techniques for privacy protection.

</details>

<details>

<summary>2020-04-14 03:27:35 - Towards Robust Classification with Image Quality Assessment</summary>

- *Yeli Feng, Yiyu Cai*

- `2004.06288v1` - [abs](http://arxiv.org/abs/2004.06288v1) - [pdf](http://arxiv.org/pdf/2004.06288v1)

> Recent studies have shown that deep convolutional neural networks (DCNN) are vulnerable to adversarial examples and sensitive to perceptual quality as well as the acquisition condition of images. These findings raise a big concern for the adoption of DCNN-based applications for critical tasks. In the literature, various defense strategies have been introduced to increase the robustness of DCNN, including re-training an entire model with benign noise injection, adversarial examples, or adding extra layers. In this paper, we investigate the connection between adversarial manipulation and image quality, subsequently propose a protective mechanism that doesnt require re-training a DCNN. Our method combines image quality assessment with knowledge distillation to detect input images that would trigger a DCCN to produce egregiously wrong results. Using the ResNet model trained on ImageNet as an example, we demonstrate that the detector can effectively identify poor quality and adversarial images.

</details>

<details>

<summary>2020-04-14 03:48:34 - Gelato: Feedback-driven and Guided Security Analysis of Client-side Web Applications</summary>

- *Behnaz Hassanshahi, Hyunjun Lee, Paddy Krishnan, Jörn Güy Suß*

- `2004.06292v1` - [abs](http://arxiv.org/abs/2004.06292v1) - [pdf](http://arxiv.org/pdf/2004.06292v1)

> Even though a lot of effort has been invested in analyzing client-side web applications during the past decade, the existing tools often fail to deal with the complexity of modern JavaScript applications. However, from an attacker point of view, the client side of such web applications can reveal invaluable information about the server side. In this paper, first we study the existing tools and enumerate the most crucial features a security-aware client-side analysis should be supporting. Next, we propose GELATO to detect vulnerabilities in modern client-side JavaScript applications that are built upon complex libraries and frameworks. In particular, we take the first step in closing the gap between state-aware crawling and client-side security analysis by proposing a feedback-driven security-aware guided crawler that is able to analyze complex frameworks automatically, and increase the coverage of security-sensitive parts of the program efficiently. Moreover, we propose a new lightweight client-side taint analysis that outperforms the start-of-the-art tools, requires no modification to browsers, and reports non-trivial taint flows on modern JavaScript applications.

</details>

<details>

<summary>2020-04-14 07:46:41 - Towards Adversarial Malware Detection: Lessons Learned from PDF-based Attacks</summary>

- *Davide Maiorca, Battista Biggio, Giorgio Giacinto*

- `1811.00830v3` - [abs](http://arxiv.org/abs/1811.00830v3) - [pdf](http://arxiv.org/pdf/1811.00830v3)

> Malware still constitutes a major threat in the cybersecurity landscape, also due to the widespread use of infection vectors such as documents. These infection vectors hide embedded malicious code to the victim users, facilitating the use of social engineering techniques to infect their machines. Research showed that machine-learning algorithms provide effective detection mechanisms against such threats, but the existence of an arms race in adversarial settings has recently challenged such systems. In this work, we focus on malware embedded in PDF files as a representative case of such an arms race. We start by providing a comprehensive taxonomy of the different approaches used to generate PDF malware, and of the corresponding learning-based detection systems. We then categorize threats specifically targeted against learning-based PDF malware detectors, using a well-established framework in the field of adversarial machine learning. This framework allows us to categorize known vulnerabilities of learning-based PDF malware detectors and to identify novel attacks that may threaten such systems, along with the potential defense mechanisms that can mitigate the impact of such threats. We conclude the paper by discussing how such findings highlight promising research directions towards tackling the more general challenge of designing robust malware detectors in adversarial settings.

</details>

<details>

<summary>2020-04-14 13:11:53 - Multi-stage Jamming Attacks Detection using Deep Learning Combined with Kernelized Support Vector Machine in 5G Cloud Radio Access Networks</summary>

- *Marouane Hachimi, Georges Kaddoum, Ghyslain Gagnon, Poulmanogo Illy*

- `2004.06077v2` - [abs](http://arxiv.org/abs/2004.06077v2) - [pdf](http://arxiv.org/pdf/2004.06077v2)

> In 5G networks, the Cloud Radio Access Network (C-RAN) is considered a promising future architecture in terms of minimizing energy consumption and allocating resources efficiently by providing real-time cloud infrastructures, cooperative radio, and centralized data processing. Recently, given their vulnerability to malicious attacks, the security of C-RAN networks has attracted significant attention. Among various anomaly-based intrusion detection techniques, the most promising one is the machine learning-based intrusion detection as it learns without human assistance and adjusts actions accordingly. In this direction, many solutions have been proposed, but they show either low accuracy in terms of attack classification or they offer just a single layer of attack detection. This research focuses on deploying a multi-stage machine learning-based intrusion detection (ML-IDS) in 5G C-RAN that can detect and classify four types of jamming attacks: constant jamming, random jamming, deceptive jamming, and reactive jamming. This deployment enhances security by minimizing the false negatives in C-RAN architectures. The experimental evaluation of the proposed solution is carried out using WSN-DS (Wireless Sensor Networks DataSet), which is a dedicated wireless dataset for intrusion detection. The final classification accuracy of attacks is 94.51\% with a 7.84\% false negative rate.

</details>

<details>

<summary>2020-04-14 16:51:42 - Weight Poisoning Attacks on Pre-trained Models</summary>

- *Keita Kurita, Paul Michel, Graham Neubig*

- `2004.06660v1` - [abs](http://arxiv.org/abs/2004.06660v1) - [pdf](http://arxiv.org/pdf/2004.06660v1)

> Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct ``weight poisoning'' attacks where pre-trained weights are injected with vulnerabilities that expose ``backdoors'' after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method, which we call RIPPLe, and an initialization procedure, which we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks. Code to reproduce our experiments is available at https://github.com/neulab/RIPPLe.

</details>

<details>

<summary>2020-04-15 09:04:16 - Advanced Evasion Attacks and Mitigations on Practical ML-Based Phishing Website Classifiers</summary>

- *Yusi Lei, Sen Chen, Lingling Fan, Fu Song, Yang Liu*

- `2004.06954v1` - [abs](http://arxiv.org/abs/2004.06954v1) - [pdf](http://arxiv.org/pdf/2004.06954v1)

> Machine learning (ML) based approaches have been the mainstream solution for anti-phishing detection. When they are deployed on the client-side, ML-based classifiers are vulnerable to evasion attacks. However, such potential threats have received relatively little attention because existing attacks destruct the functionalities or appearance of webpages and are conducted in the white-box scenario, making it less practical. Consequently, it becomes imperative to understand whether it is possible to launch evasion attacks with limited knowledge of the classifier, while preserving the functionalities and appearance.   In this work, we show that even in the grey-, and black-box scenarios, evasion attacks are not only effective on practical ML-based classifiers, but can also be efficiently launched without destructing the functionalities and appearance. For this purpose, we propose three mutation-based attacks, differing in the knowledge of the target classifier, addressing a key technical challenge: automatically crafting an adversarial sample from a known phishing website in a way that can mislead classifiers. To launch attacks in the white- and grey-box scenarios, we also propose a sample-based collision attack to gain the knowledge of the target classifier. We demonstrate the effectiveness and efficiency of our evasion attacks on the state-of-the-art, Google's phishing page filter, achieved 100% attack success rate in less than one second per website. Moreover, the transferability attack on BitDefender's industrial phishing page classifier, TrafficLight, achieved up to 81.25% attack success rate. We further propose a similarity-based method to mitigate such evasion attacks, Pelican. We demonstrate that Pelican can effectively detect evasion attacks. Our findings contribute to design more robust phishing website classifiers in practice.

</details>

<details>

<summary>2020-04-15 21:39:33 - Usable, Acceptable, Appropriable: Towards Practicable Privacy</summary>

- *Aakash Gautam*

- `2004.07359v1` - [abs](http://arxiv.org/abs/2004.07359v1) - [pdf](http://arxiv.org/pdf/2004.07359v1)

> A majority of the work on digital privacy and security has focused on users from developed countries who account for only around 20\% of the global population. Moreover, the privacy needs for population that is already marginalized and vulnerable differ from users who have privilege to access a greater social support system. We reflect on our experiences of introducing computers and the Internet to a group of sex-trafficking survivors in Nepal and highlight a few socio-political factors that have influenced the design space around digital privacy. These factors include the population's limited digital and text literacy skills and the fear of stigma against trafficked persons widely prevalent in Nepali society. We underscore the need to widen our perspective by focusing on practicable privacy, that is, privacy practices that are (1) usable, (2) acceptable, and (3) appropriable.

</details>

<details>

<summary>2020-04-16 12:09:05 - A Secure and Improved Multi Server Authentication Protocol Using Fuzzy Commitment</summary>

- *Hafeez Ur Rehman, Anwar Ghani, Shehzad Ashraf Chaudhry, Mohammed H. Alsharif, Narjes Nabipour*

- `2004.07618v1` - [abs](http://arxiv.org/abs/2004.07618v1) - [pdf](http://arxiv.org/pdf/2004.07618v1)

> Very recently, Barman et al. proposed a multi-server authentication protocol using fuzzy commitment. The authors claimed that their protocol provides anonymity while resisting all known attacks. In this paper, we analyze that Barman et al.'s protocol is still vulnerable to anonymity violation attack and impersonation based on the stolen smart attack; moreover, it has scalability issues. We then propose an improved and enhanced protocol to overcome the security weaknesses of Barman et al.'s scheme. The security of the proposed protocol is verified using BAN logic and widely accepted automated AVISPA tool. The BAN logic and automated AVISPA along with the informal analysis ensures the robustness of the scheme against all known attacks

</details>

<details>

<summary>2020-04-16 14:28:21 - Online Social Deception and Its Countermeasures for Trustworthy Cyberspace: A Survey</summary>

- *Zhen Guo, Jin-Hee Cho, Ing-Ray Chen, Srijan Sengupta, Michin Hong, Tanushree Mitra*

- `2004.07678v1` - [abs](http://arxiv.org/abs/2004.07678v1) - [pdf](http://arxiv.org/pdf/2004.07678v1)

> We are living in an era when online communication over social network services (SNSs) have become an indispensable part of people's everyday lives. As a consequence, online social deception (OSD) in SNSs has emerged as a serious threat in cyberspace, particularly for users vulnerable to such cyberattacks. Cyber attackers have exploited the sophisticated features of SNSs to carry out harmful OSD activities, such as financial fraud, privacy threat, or sexual/labor exploitation. Therefore, it is critical to understand OSD and develop effective countermeasures against OSD for building a trustworthy SNSs. In this paper, we conducted an extensive survey, covering (i) the multidisciplinary concepts of social deception; (ii) types of OSD attacks and their unique characteristics compared to other social network attacks and cybercrimes; (iii) comprehensive defense mechanisms embracing prevention, detection, and response (or mitigation) against OSD attacks along with their pros and cons; (iv) datasets/metrics used for validation and verification; and (v) legal and ethical concerns related to OSD research. Based on this survey, we provide insights into the effectiveness of countermeasures and the lessons from existing literature. We conclude this survey paper with an in-depth discussions on the limitations of the state-of-the-art and recommend future research directions in this area.

</details>

<details>

<summary>2020-04-17 00:44:07 - Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study</summary>

- *Dinh-Luan Nguyen, Sunpreet S. Arora, Yuhang Wu, Hao Yang*

- `2003.11145v2` - [abs](http://arxiv.org/abs/2003.11145v2) - [pdf](http://arxiv.org/pdf/2003.11145v2)

> Deep learning-based systems have been shown to be vulnerable to adversarial attacks in both digital and physical domains. While feasible, digital attacks have limited applicability in attacking deployed systems, including face recognition systems, where an adversary typically has access to the input and not the transmission channel. In such setting, physical attacks that directly provide a malicious input through the input channel pose a bigger threat. We investigate the feasibility of conducting real-time physical attacks on face recognition systems using adversarial light projections. A setup comprising a commercially available web camera and a projector is used to conduct the attack. The adversary uses a transformation-invariant adversarial pattern generation method to generate a digital adversarial pattern using one or more images of the target available to the adversary. The digital adversarial pattern is then projected onto the adversary's face in the physical domain to either impersonate a target (impersonation) or evade recognition (obfuscation). We conduct preliminary experiments using two open-source and one commercial face recognition system on a pool of 50 subjects. Our experimental results demonstrate the vulnerability of face recognition systems to light projection attacks in both white-box and black-box attack settings.

</details>

<details>

<summary>2020-04-17 02:15:29 - A Case for Maximal Leakage as a Side Channel Leakage Metric</summary>

- *Benjamin Wu, Aaron B. Wagner, G. Edward Suh*

- `2004.08035v1` - [abs](http://arxiv.org/abs/2004.08035v1) - [pdf](http://arxiv.org/pdf/2004.08035v1)

> Side channels represent a broad class of security vulnerabilities that have been demonstrated to exist in many applications. Because completely eliminating side channels often leads to prohibitively high overhead, there is a need for a principled trade-off between cost and leakage. In this paper, we make a case for the use of maximal leakage to analyze such trade-offs. Maximal leakage is an operationally interpretable leakage metric designed for side channels. We present the most useful theoretical properties of maximal leakage from previous work and demonstrate empirically that conventional metrics such as mutual information and channel capacity underestimate the threat posed by side channels whereas maximal leakage does not. We also study the cost-leakage trade-off as an optimization problem using maximal leakage. We demonstrate that not only can this problem be represented as a linear program, but also that optimal protection can be achieved using a combination of at most two deterministic schemes.

</details>

<details>

<summary>2020-04-17 02:26:30 - MDEA: Malware Detection with Evolutionary Adversarial Learning</summary>

- *Xiruo Wang, Risto Miikkulainen*

- `2002.03331v2` - [abs](http://arxiv.org/abs/2002.03331v2) - [pdf](http://arxiv.org/pdf/2002.03331v2)

> Malware detection have used machine learning to detect malware in programs. These applications take in raw or processed binary data to neural network models to classify as benign or malicious files. Even though this approach has proven effective against dynamic changes, such as encrypting, obfuscating and packing techniques, it is vulnerable to specific evasion attacks where that small changes in the input data cause misclassification at test time. This paper proposes a new approach: MDEA, an Adversarial Malware Detection model uses evolutionary optimization to create attack samples to make the network robust against evasion attacks. By retraining the model with the evolved malware samples, its performance improves a significant margin.

</details>

<details>

<summary>2020-04-17 13:42:24 - Deep Neural Rejection against Adversarial Examples</summary>

- *Angelo Sotgiu, Ambra Demontis, Marco Melis, Battista Biggio, Giorgio Fumera, Xiaoyi Feng, Fabio Roli*

- `1910.00470v3` - [abs](http://arxiv.org/abs/1910.00470v3) - [pdf](http://arxiv.org/pdf/1910.00470v3)

> Despite the impressive performances reported by deep neural networks in different application domains, they remain largely vulnerable to adversarial examples, i.e., input samples that are carefully perturbed to cause misclassification at test time. In this work, we propose a deep neural rejection mechanism to detect adversarial examples, based on the idea of rejecting samples that exhibit anomalous feature representations at different network layers. With respect to competing approaches, our method does not require generating adversarial examples at training time, and it is less computationally demanding. To properly evaluate our method, we define an adaptive white-box attack that is aware of the defense mechanism and aims to bypass it. Under this worst-case setting, we empirically show that our approach outperforms previously-proposed methods that detect adversarial examples by only analyzing the feature representation provided by the output network layer.

</details>

<details>

<summary>2020-04-17 14:00:45 - A Robust Reputation-based Group Ranking System and its Resistance to Bribery</summary>

- *Joao Saude, Guilherme Ramos, Ludovico Boratto, Carlos Caleiro*

- `2004.06223v2` - [abs](http://arxiv.org/abs/2004.06223v2) - [pdf](http://arxiv.org/pdf/2004.06223v2)

> The spread of online reviews and opinions and its growing influence on people's behavior and decisions, boosted the interest to extract meaningful information from this data deluge. Hence, crowdsourced ratings of products and services gained a critical role in business and governments. Current state-of-the-art solutions rank the items with an average of the ratings expressed for an item, with a consequent lack of personalization for the users, and the exposure to attacks and spamming/spurious users. Using these ratings to group users with similar preferences might be useful to present users with items that reflect their preferences and overcome those vulnerabilities. In this paper, we propose a new reputation-based ranking system, utilizing multipartite rating subnetworks, which clusters users by their similarities using three measures, two of them based on Kolmogorov complexity. We also study its resistance to bribery and how to design optimal bribing strategies. Our system is novel in that it reflects the diversity of preferences by (possibly) assigning distinct rankings to the same item, for different groups of users. We prove the convergence and efficiency of the system. By testing it on synthetic and real data, we see that it copes better with spamming/spurious users, being more robust to attacks than state-of-the-art approaches. Also, by clustering users, the effect of bribery in the proposed multipartite ranking system is dimmed, comparing to the bipartite case.

</details>

<details>

<summary>2020-04-18 00:14:27 - The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks</summary>

- *Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, Dawn Song*

- `1911.07135v2` - [abs](http://arxiv.org/abs/1911.07135v2) - [pdf](http://arxiv.org/pdf/1911.07135v2)

> This paper studies model-inversion attacks, in which the access to a model is abused to infer information about the training data. Since its first introduction, such attacks have raised serious concerns given that training data usually contain privacy-sensitive information. Thus far, successful model-inversion attacks have only been demonstrated on simple models, such as linear regression and logistic regression. Previous attempts to invert neural networks, even the ones with simple architectures, have failed to produce convincing results. We present a novel attack method, termed the generative model-inversion attack, which can invert deep neural networks with high success rates. Rather than reconstructing private training data from scratch, we leverage partial public information, which can be very generic, to learn a distributional prior via generative adversarial networks (GANs) and use it to guide the inversion process. Moreover, we theoretically prove that a model's predictive power and its vulnerability to inversion attacks are indeed two sides of the same coin---highly predictive models are able to establish a strong correlation between features and labels, which coincides exactly with what an adversary exploits to mount the attacks. Our extensive experiments demonstrate that the proposed attack improves identification accuracy over the existing work by about 75\% for reconstructing face images from a state-of-the-art face recognition classifier. We also show that differential privacy, in its canonical form, is of little avail to defend against our attacks.

</details>

<details>

<summary>2020-04-18 00:49:47 - Human Factors in Biocybersecurity Wargames</summary>

- *Lucas Potter, Xavier-Lewis Palmer*

- `2005.02135v1` - [abs](http://arxiv.org/abs/2005.02135v1) - [pdf](http://arxiv.org/pdf/2005.02135v1)

> Within the field of biocybersecurity, it is important to understand what vulnerabilities may be uncovered in the processing of biologics as well as how they can be safeguarded as they intersect with cyber and cyberphysical systems, as noted by the Peccoud Lab, to ensure not only product and brand integrity, but protect those served. Recent findings have revealed that biological systems can be used to compromise computer systems and vice versa. While regular and sophisticated attacks are still years away, time is of the essence to better understand ways to deepen critique and grasp intersectional vulnerabilities within bioprocessing as processes involved become increasingly digitally accessible. Wargames have been shown to be successful with-in improving group dynamics in response to anticipated cyber threats, and they can be used towards addressing possible threats within biocybersecurity. Within this paper, we discuss the growing prominence of biocybersecurity, the importance of biocybersecurity to bioprocessing , with respect to domestic and international contexts, and reasons for emphasizing the biological component in the face of explosive growth in biotechnology and thus separating the terms biocybersecurity and cyberbiosecurity. Additionally, a discussion and manual is provided for a simulation towards organizational learning to sense and shore up vulnerabilities that may emerge within an organization's bioprocessing pipeline

</details>

<details>

<summary>2020-04-18 08:59:41 - sFuzz: An Efficient Adaptive Fuzzer for Solidity Smart Contracts</summary>

- *Tai D. Nguyen, Long H. Pham, Jun Sun, Yun Lin, Quang Tran Minh*

- `2004.08563v1` - [abs](http://arxiv.org/abs/2004.08563v1) - [pdf](http://arxiv.org/pdf/2004.08563v1)

> Smart contracts are Turing-complete programs that execute on the infrastructure of the blockchain, which often manage valuable digital assets. Solidity is one of the most popular programming languages for writing smart contracts on the Ethereum platform. Like traditional programs, smart contracts may contain vulnerabilities. Unlike traditional programs, smart contracts cannot be easily patched once they are deployed. It is thus important that smart contracts are tested thoroughly before deployment. In this work, we present an adaptive fuzzer for smart contracts on the Ethereum platform called sFuzz. Compared to existing Solidity fuzzers, sFuzz combines the strategy in the AFL fuzzer and an efficient lightweight multi-objective adaptive strategy targeting those hard-to-cover branches. sFuzz has been applied to more than 4 thousand smart contracts and the experimental results show that (1) sFuzz is efficient, e.g., two orders of magnitude faster than state-of-the-art tools; (2) sFuzz is effective in achieving high code coverage and discovering vulnerabilities; and (3) the different fuzzing strategies in sFuzz complement each other.

</details>

<details>

<summary>2020-04-19 03:45:05 - Data Poisoning Attacks on Federated Machine Learning</summary>

- *Gan Sun, Yang Cong, Jiahua Dong, Qiang Wang, Ji Liu*

- `2004.10020v1` - [abs](http://arxiv.org/abs/2004.10020v1) - [pdf](http://arxiv.org/pdf/2004.10020v1)

> Federated machine learning which enables resource constrained node devices (e.g., mobile phones and IoT devices) to learn a shared model while keeping the training data local, can provide privacy, security and economic benefits by designing an effective communication protocol. However, the communication protocol amongst different nodes could be exploited by attackers to launch data poisoning attacks, which has been demonstrated as a big threat to most machine learning models. In this paper, we attempt to explore the vulnerability of federated machine learning. More specifically, we focus on attacking a federated multi-task learning framework, which is a federated learning framework via adopting a general multi-task learning framework to handle statistical challenges. We formulate the problem of computing optimal poisoning attacks on federated multi-task learning as a bilevel program that is adaptive to arbitrary choice of target nodes and source attacking nodes. Then we propose a novel systems-aware optimization method, ATTack on Federated Learning (AT2FL), which is efficiency to derive the implicit gradients for poisoned data, and further compute optimal attack strategies in the federated machine learning. Our work is an earlier study that considers issues of data poisoning attack for federated learning. To the end, experimental results on real-world datasets show that federated multi-task learning model is very sensitive to poisoning attacks, when the attackers either directly poison the target nodes or indirectly poison the related nodes by exploiting the communication protocol.

</details>

<details>

<summary>2020-04-19 17:11:07 - Secure and Energy-Efficient Key-Agreement Protocol for Multi-Server Architecture</summary>

- *Trupil Limbasiya, Sanjay K. Sahay*

- `2004.10010v1` - [abs](http://arxiv.org/abs/2004.10010v1) - [pdf](http://arxiv.org/pdf/2004.10010v1)

> Authentication schemes are practised globally to verify the legitimacy of users and servers for the exchange of data in different facilities. Generally, the server verifies a user to provide resources for different purposes. But due to the large network system, the authentication process has become complex and therefore, time-to-time different authentication protocols have been proposed for the multi-server architecture. However, most of the protocols are vulnerable to various security attacks and their performance is not efficient. In this paper, we propose a secure and energy-efficient remote user authentication protocol for multi-server systems. The results show that the proposed protocol is comparatively ~44% more efficient and needs ~38% less communication cost. We also demonstrate that with only two-factor authentication, the proposed protocol is more secure from the earlier related authentication schemes.

</details>

<details>

<summary>2020-04-20 10:09:27 - GraN: An Efficient Gradient-Norm Based Detector for Adversarial and Misclassified Examples</summary>

- *Julia Lust, Alexandru Paul Condurache*

- `2004.09179v1` - [abs](http://arxiv.org/abs/2004.09179v1) - [pdf](http://arxiv.org/pdf/2004.09179v1)

> Deep neural networks (DNNs) are vulnerable to adversarial examples and other data perturbations. Especially in safety critical applications of DNNs, it is therefore crucial to detect misclassified samples. The current state-of-the-art detection methods require either significantly more runtime or more parameters than the original network itself. This paper therefore proposes GraN, a time- and parameter-efficient method that is easily adaptable to any DNN.   GraN is based on the layer-wise norm of the DNN's gradient regarding the loss of the current input-output combination, which can be computed via backpropagation. GraN achieves state-of-the-art performance on numerous problem set-ups.

</details>

<details>

<summary>2020-04-20 13:03:14 - MemShield: GPU-assisted software memory encryption</summary>

- *Pierpaolo Santucci, Emiliano Ingrassia, Giulio Picierro, Marco Cesati*

- `2004.09252v1` - [abs](http://arxiv.org/abs/2004.09252v1) - [pdf](http://arxiv.org/pdf/2004.09252v1)

> Cryptographic algorithm implementations are vulnerable to Cold Boot attacks, which consist in exploiting the persistence of RAM cells across reboots or power down cycles to read the memory contents and recover precious sensitive data. The principal defensive weapon against Cold Boot attacks is memory encryption. In this work we propose MemShield, a memory encryption framework for user space applications that exploits a GPU to safely store the master key and perform the encryption/decryption operations. We developed a prototype that is completely transparent to existing applications and does not require changes to the OS kernel. We discuss the design, the related works, the implementation, the security analysis, and the performances of MemShield.

</details>

<details>

<summary>2020-04-20 19:17:59 - FlashFlow: A Secure Speed Test for Tor</summary>

- *Matthew Traudt, Rob Jansen, Aaron Johnson*

- `2004.09583v1` - [abs](http://arxiv.org/abs/2004.09583v1) - [pdf](http://arxiv.org/pdf/2004.09583v1)

> The Tor network uses a measurement system to estimate its relays' forwarding capacity and to balance traffic among them. This system has been shown to be vulnerable to adversarial manipulation. Moreover, its accuracy and effectiveness in benign circumstances has never been fully quantified. We first obtain such a quantification by analyzing Tor metrics data and performing experiments on the live network. Our results show that Tor currently underestimates its true capacity by about 50% and improperly balances its traffic by 15-25%. Then, to solve the problems with security and accuracy, we present FlashFlow, a system to measure the capacity of Tor relays. Our analysis shows that FlashFlow limits a malicious relay to obtaining a capacity estimate at most 1.33 times its true capacity. Through realistic Internet experiments, we find that FlashFlow measures relay capacity with at least 89% accuracy 95% of the time. Through simulation, we find that FlashFlow can measure the entire Tor network in less than 5 hours using 3 measurers with 1 Gbit/s of bandwidth each. Finally, simulations using FlashFlow for load balancing shows that, compared to TorFlow, network weight error decreases by 86%, while the median of 50 KiB, 1 MiB, and 5 MiB transfer times decreases by 15%, 29%, and 37%, respectively. Moreover, FlashFlow yields more consistent client performance: the median rate of transfer timeouts decreases by 100%, while the standard deviation of 50 KiB, 1 MiB, and 5 MiB transfer times decreases by 55%, 61%, and 41%, respectively. We also find that the performance improvements increase relative to TorFlow as the total client-traffic load increases, demonstrating that FlashFlow is better suited to supporting network growth.

</details>

<details>

<summary>2020-04-20 19:56:48 - Why do People Share Misinformation during the COVID-19 Pandemic?</summary>

- *Samuli Laato, A. K. M. Najmul Islam, Muhammad Nazrul Islam, Eoin Whelan*

- `2004.09600v1` - [abs](http://arxiv.org/abs/2004.09600v1) - [pdf](http://arxiv.org/pdf/2004.09600v1)

> The World Health Organization have emphasised that misinformation - spreading rapidly through social media - poses a serious threat to the COVID-19 response. Drawing from theories of health perception and cognitive load, we develop and test a research model hypothesizing why people share unverified COVID-19 information through social media. Our findings suggest a person's trust in online information and perceived information overload are strong predictors of unverified information sharing. Furthermore, these factors, along with a person's perceived COVID-19 severity and vulnerability influence cyberchondria. Females were significantly more likely to suffer from cyberchondria, however, males were more likely to share news without fact checking their source. Our findings suggest that to mitigate the spread of COVID-19 misinformation and cyberchondria, measures should be taken to enhance a healthy skepticism of health news while simultaneously guarding against information overload.

</details>

<details>

<summary>2020-04-20 23:50:43 - Scalable and Secure Architecture for Distributed IoT Systems</summary>

- *Najmeddine Dhieb, Hakim Ghazzai, Hichem Besbes, Yehia Massoud*

- `2005.02456v1` - [abs](http://arxiv.org/abs/2005.02456v1) - [pdf](http://arxiv.org/pdf/2005.02456v1)

> Internet-of-things (IoT) is perpetually revolutionizing our daily life and rapidly transforming physical objects into an ubiquitous connected ecosystem. Due to their massive deployment and moderate security levels, those devices face a lot of security, management, and control challenges. Their classical centralized architecture is still cloaking vulnerabilities and anomalies that can be exploited by hackers for spying, eavesdropping, and taking control of the network. In this paper, we propose to improve the IoT architecture with additional security features using Artificial Intelligence (AI) and blockchain technology. We propose a novel architecture based on permissioned blockchain technology in order to build a scalable and decentralized end-to-end secure IoT system. Furthermore, we enhance the IoT system security with an AI-component at the gateway level to detect and classify suspected activities, malware, and cyber-attacks using machine learning techniques. Simulations and practical implementation show that the proposed architecture delivers high performance against cyber-attacks.

</details>

<details>

<summary>2020-04-21 14:01:53 - Entropy-Based Modeling for Estimating Soft Errors Impact on Binarized Neural Network Inference</summary>

- *Navid Khoshavi, Saman Sargolzaei, Arman Roohi, Connor Broyles, Yu Bi*

- `2004.05089v2` - [abs](http://arxiv.org/abs/2004.05089v2) - [pdf](http://arxiv.org/pdf/2004.05089v2)

> Over past years, the easy accessibility to the large scale datasets has significantly shifted the paradigm for developing highly accurate prediction models that are driven from Neural Network (NN). These models can be potentially impacted by the radiation-induced transient faults that might lead to the gradual downgrade of the long-running expected NN inference accelerator. The crucial observation from our rigorous vulnerability assessment on the NN inference accelerator demonstrates that the weights and activation functions are unevenly susceptible to both single-event upset (SEU) and multi-bit upset (MBU), especially in the first five layers of our selected convolution neural network. In this paper, we present the relatively-accurate statistical models to delineate the impact of both undertaken SEU and MBU across layers and per each layer of the selected NN. These models can be used for evaluating the error-resiliency magnitude of NN topology before adopting them in the safety-critical applications.

</details>

<details>

<summary>2020-04-21 17:17:09 - EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness against Adversarial Attacks</summary>

- *Sanchari Sen, Balaraman Ravindran, Anand Raghunathan*

- `2004.10162v1` - [abs](http://arxiv.org/abs/2004.10162v1) - [pdf](http://arxiv.org/pdf/2004.10162v1)

> Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this limitation to achieve the 'best of both worlds', i.e., the higher unperturbed accuracies of the full precision models combined with the higher robustness of the low precision models, by composing them in an ensemble. Further, as low precision DNN models have significantly lower computational and storage requirements than full precision models, EMPIR models only incur modest compute and memory overheads compared to a single full-precision model (<25% in our evaluations). We evaluate EMPIR across a suite of DNNs for 3 different image recognition tasks (MNIST, CIFAR-10 and ImageNet) and under 4 different adversarial attacks. Our results indicate that EMPIR boosts the average adversarial accuracies by 42.6%, 15.2% and 10.5% for the DNN models trained on the MNIST, CIFAR-10 and ImageNet datasets respectively, when compared to single full-precision models, without sacrificing accuracy on the unperturbed inputs.

</details>

<details>

<summary>2020-04-21 19:38:31 - Certifying Joint Adversarial Robustness for Model Ensembles</summary>

- *Mainuddin Ahmad Jonas, David Evans*

- `2004.10250v1` - [abs](http://arxiv.org/abs/2004.10250v1) - [pdf](http://arxiv.org/pdf/2004.10250v1)

> Deep Neural Networks (DNNs) are often vulnerable to adversarial examples.Several proposed defenses deploy an ensemble of models with the hope that, although the individual models may be vulnerable, an adversary will not be able to find an adversarial example that succeeds against the ensemble. Depending on how the ensemble is used, an attacker may need to find a single adversarial example that succeeds against all, or a majority, of the models in the ensemble. The effectiveness of ensemble defenses against strong adversaries depends on the vulnerability spaces of models in the ensemble being disjoint. We consider the joint vulnerability of an ensemble of models, and propose a novel technique for certifying the joint robustness of ensembles, building upon prior works on single-model robustness certification. We evaluate the robustness of various models ensembles, including models trained using cost-sensitive robustness to be diverse, to improve understanding of the potential effectiveness of ensemble models as a defense against adversarial examples.

</details>

<details>

<summary>2020-04-22 02:11:13 - Scalable Attack on Graph Data by Injecting Vicious Nodes</summary>

- *Jihong Wang, Minnan Luo, Fnu Suya, Jundong Li, Zijiang Yang, Qinghua Zheng*

- `2004.13825v1` - [abs](http://arxiv.org/abs/2004.13825v1) - [pdf](http://arxiv.org/pdf/2004.13825v1)

> Recent studies have shown that graph convolution networks (GCNs) are vulnerable to carefully designed attacks, which aim to cause misclassification of a specific node on the graph with unnoticeable perturbations. However, a vast majority of existing works cannot handle large-scale graphs because of their high time complexity. Additionally, existing works mainly focus on manipulating existing nodes on the graph, while in practice, attackers usually do not have the privilege to modify information of existing nodes. In this paper, we develop a more scalable framework named Approximate Fast Gradient Sign Method (AFGSM) which considers a more practical attack scenario where adversaries can only inject new vicious nodes to the graph while having no control over the original graph. Methodologically, we provide an approximation strategy to linearize the model we attack and then derive an approximate closed-from solution with a lower time cost. To have a fair comparison with existing attack methods that manipulate the original graph, we adapt them to the new attack scenario by injecting vicious nodes. Empirical experimental results show that our proposed attack method can significantly reduce the classification accuracy of GCNs and is much faster than existing methods without jeopardizing the attack performance.

</details>

<details>

<summary>2020-04-22 02:56:08 - Towards Automated Augmentation and Instrumentation of Legacy Cryptographic Executables: Extended Version</summary>

- *Karim Eldefrawy, Michael Locasto, Norrathep Rattanavipanon, Hassen Saidi*

- `2004.09713v2` - [abs](http://arxiv.org/abs/2004.09713v2) - [pdf](http://arxiv.org/pdf/2004.09713v2)

> Implementation flaws in cryptographic libraries, design flaws in underlying cryptographic primitives, and weaknesses in protocols using both, can all lead to exploitable vulnerabilities in software. Manually fixing such issues is challenging and resource consuming, especially when maintaining legacy software that contains broken or outdated cryptography, and for which source code may not be available. While there is existing work on identifying cryptographic primitives (often in the context of malware analysis), none of this prior work has focused on replacing such primitives with stronger (or more secure ones) after they have been identified. This paper explores feasibility of designing and implementing a toolchain for Augmentation and Legacy-software Instrumentation of Cryptographic Executables (ALICE). The key features of ALICE are: (i) automatically detecting and extracting implementations of weak or broken cryptographic primitives from binaries without requiring source code or debugging symbols, (ii) identifying the context and scope in which such primitives are used, and performing program analysis to determine the effects of replacing such implementations with more secure ones, and (iii) replacing implementations of weak primitives with those of stronger or more secure ones. We demonstrate practical feasibility of our approach on cryptographic hash functions with several popular cryptographic libraries and real-world programs of various levels of complexity. Our experimental results show that ALICE can locate and replace insecure hash functions, even in large binaries (we tested ones of size up to 1.5MB), while preserving existing functionality of the original binaries, and while incurring minimal execution-time overhead in the rewritten binaries. We also open source ALICE's code at https://github.com/SRI-CSL/ALICE.

</details>

<details>

<summary>2020-04-22 09:31:45 - Key Protected Classification for Collaborative Learning</summary>

- *Mert Bülent Sarıyıldız, Ramazan Gökberk Cinbiş, Erman Ayday*

- `1908.10172v2` - [abs](http://arxiv.org/abs/1908.10172v2) - [pdf](http://arxiv.org/pdf/1908.10172v2)

> Large-scale datasets play a fundamental role in training deep learning models. However, dataset collection is difficult in domains that involve sensitive information. Collaborative learning techniques provide a privacy-preserving solution, by enabling training over a number of private datasets that are not shared by their owners. However, recently, it has been shown that the existing collaborative learning frameworks are vulnerable to an active adversary that runs a generative adversarial network (GAN) attack. In this work, we propose a novel classification model that is resilient against such attacks by design. More specifically, we introduce a key-based classification model and a principled training scheme that protects class scores by using class-specific private keys, which effectively hide the information necessary for a GAN attack. We additionally show how to utilize high dimensional keys to improve the robustness against attacks without increasing the model complexity. Our detailed experiments demonstrate the effectiveness of the proposed technique. Source code is available at https://github.com/mbsariyildiz/key-protected-classification.

</details>

<details>

<summary>2020-04-22 13:14:48 - Defending Adversarial Attacks via Semantic Feature Manipulation</summary>

- *Shuo Wang, Tianle Chen, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen*

- `2002.02007v2` - [abs](http://arxiv.org/abs/2002.02007v2) - [pdf](http://arxiv.org/pdf/2002.02007v2)

> Machine learning models have demonstrated vulnerability to adversarial attacks, more specifically misclassification of adversarial examples. In this paper, we propose a one-off and attack-agnostic Feature Manipulation (FM)-Defense to detect and purify adversarial examples in an interpretable and efficient manner. The intuition is that the classification result of a normal image is generally resistant to non-significant intrinsic feature changes, e.g., varying thickness of handwritten digits. In contrast, adversarial examples are sensitive to such changes since the perturbation lacks transferability. To enable manipulation of features, a combo-variational autoencoder is applied to learn disentangled latent codes that reveal semantic features. The resistance to classification change over the morphs, derived by varying and reconstructing latent codes, is used to detect suspicious inputs. Further, combo-VAE is enhanced to purify the adversarial examples with good quality by considering both class-shared and class-unique features. We empirically demonstrate the effectiveness of detection and the quality of purified instance. Our experiments on three datasets show that FM-Defense can detect nearly $100\%$ of adversarial examples produced by different state-of-the-art adversarial attacks. It achieves more than $99\%$ overall purification accuracy on the suspicious instances that close the manifold of normal examples.

</details>

<details>

<summary>2020-04-22 18:31:59 - Cyberattacks and Countermeasures For In-Vehicle Networks</summary>

- *Emad Aliwa, Omer Rana, Charith Perera, Peter Burnap*

- `2004.10781v1` - [abs](http://arxiv.org/abs/2004.10781v1) - [pdf](http://arxiv.org/pdf/2004.10781v1)

> As connectivity between and within vehicles increases, so does concern about safety and security. Various automotive serial protocols are used inside vehicles such as Controller Area Network (CAN), Local Interconnect Network (LIN) and FlexRay. CAN bus is the most used in-vehicle network protocol to support exchange of vehicle parameters between Electronic Control Units (ECUs). This protocol lacks security mechanisms by design and is therefore vulnerable to various attacks. Furthermore, connectivity of vehicles has made the CAN bus not only vulnerable from within the vehicle but also from outside. With the rise of connected cars, more entry points and interfaces have been introduced on board vehicles, thereby also leading to a wider potential attack surface. Existing security mechanisms focus on the use of encryption, authentication and vehicle Intrusion Detection Systems (IDS), which operate under various constrains such as low bandwidth, small frame size (e.g. in the CAN protocol), limited availability of computational resources and real-time sensitivity. We survey In-Vehicle Network (IVN) attacks which have been grouped under: direct interfaces-initiated attacks, telematics and infotainment-initiated attacks, and sensor-initiated attacks. We survey and classify current cryptographic and IDS approaches and compare these approaches based on criteria such as real time constrains, types of hardware used, changes in CAN bus behaviour, types of attack mitigation and software/ hardware used to validate these approaches. We conclude with potential mitigation strategies and research challenges for the future.

</details>

<details>

<summary>2020-04-22 19:56:21 - An Efficient Convolutional Neural Network for Coronary Heart Disease Prediction</summary>

- *Aniruddha Dutta, Tamal Batabyal, Meheli Basu, Scott T. Acton*

- `1909.00489v2` - [abs](http://arxiv.org/abs/1909.00489v2) - [pdf](http://arxiv.org/pdf/1909.00489v2)

> This study proposes an efficient neural network with convolutional layers to classify significantly class-imbalanced clinical data. The data are curated from the National Health and Nutritional Examination Survey (NHANES) with the goal of predicting the occurrence of Coronary Heart Disease (CHD). While the majority of the existing machine learning models that have been used on this class of data are vulnerable to class imbalance even after the adjustment of class-specific weights, our simple two-layer CNN exhibits resilience to the imbalance with fair harmony in class-specific performance. In order to obtain significant improvement in classification accuracy under supervised learning settings, it is a common practice to train a neural network architecture with a massive data and thereafter, test the resulting network on a comparatively smaller amount of data. However, given a highly imbalanced dataset, it is often challenging to achieve a high class 1 (true CHD prediction rate) accuracy as the testing data size increases. We adopt a two-step approach: first, we employ least absolute shrinkage and selection operator (LASSO) based feature weight assessment followed by majority-voting based identification of important features. Next, the important features are homogenized by using a fully connected layer, a crucial step before passing the output of the layer to successive convolutional stages. We also propose a training routine per epoch, akin to a simulated annealing process, to boost the classification accuracy. Despite a 35:1 (Non-CHD:CHD) ratio in the NHANES dataset, the investigation confirms that our proposed CNN architecture has the classification power of 77% to correctly classify the presence of CHD and 81.8% the absence of CHD cases on a testing data, which is 85.70% of the total dataset. ( (<1920 characters)Please check the paper for full abstract)

</details>

<details>

<summary>2020-04-22 23:37:38 - Digit Recognition From Wrist Movements and Security Concerns with Smart Wrist Wearable IOT Devices</summary>

- *Lambert T. Leong, Sean Wiere*

- `2004.14777v1` - [abs](http://arxiv.org/abs/2004.14777v1) - [pdf](http://arxiv.org/pdf/2004.14777v1)

> In this paper, we investigate a potential security vulnerability associated with wrist wearable devices. Hardware components on common wearable devices include an accelerometer and gyroscope, among other sensors. We demonstrate that an accelerometer and gyroscope can pick up enough unique wrist movement information to identify digits being written by a user. With a data set of 400 writing samples, of either the digit zero or the digit one, we constructed a machine learning model to correctly identify the digit being written based on the movements of the wrist. Our model's performance on an unseen test set resulted in an area under the receiver operating characteristic (AUROC) curve of 1.00. Loading our model onto our fabricated device resulted in 100% accuracy when predicting ten writing samples in real-time. The model's ability to correctly identify all digits via wrist movement and orientation changes raises security concerns. Our results imply that nefarious individuals may be able to gain sensitive digit based information such as social security, credit card, and medical record numbers from wrist wearable devices.

</details>

<details>

<summary>2020-04-23 01:11:15 - DeepCloak: Adversarial Crafting As a Defensive Measure to Cloak Processes</summary>

- *Mehmet Sinan Inci, Thomas Eisenbarth, Berk Sunar*

- `1808.01352v2` - [abs](http://arxiv.org/abs/1808.01352v2) - [pdf](http://arxiv.org/pdf/1808.01352v2)

> Over the past decade, side-channels have proven to be significant and practical threats to modern computing systems. Recent attacks have all exploited the underlying shared hardware. While practical, mounting such a complicated attack is still akin to listening on a private conversation in a crowded train station. The attacker has to either perform significant manual labor or use AI systems to automate the process. The recent academic literature points to the latter option. With the abundance of cheap computing power and the improvements made in AI, it is quite advantageous to automate such tasks. By using AI systems however, malicious parties also inherit their weaknesses. One such weakness is undoubtedly the vulnerability to adversarial samples.   In contrast to the previous literature, for the first time, we propose the use of adversarial learning as a defensive tool to obfuscate and mask private information. We demonstrate the viability of this approach by first training CNNs and other machine learning classifiers on leakage trace of different processes. After training highly accurate models (99+% accuracy), we investigate their resolve against adversarial learning methods. By applying minimal perturbations to input traces, the adversarial traffic by the defender can run as an attachment to the original process and cloak it against a malicious classifier.   Finally, we investigate whether an attacker can protect her classifier model by employing adversarial defense methods, namely adversarial re-training and defensive distillation. Our results show that even in the presence of an intelligent adversary that employs such techniques, all 10 of the tested adversarial learning methods still manage to successfully craft adversarial perturbations and the proposed cloaking methodology succeeds.

</details>

<details>

<summary>2020-04-23 13:05:16 - Uncovering Vulnerable Industrial Control Systems from the Internet Core</summary>

- *Marcin Nawrocki, Thomas C. Schmidt, Matthias Wählisch*

- `1901.04411v2` - [abs](http://arxiv.org/abs/1901.04411v2) - [pdf](http://arxiv.org/pdf/1901.04411v2)

> Industrial control systems (ICS) are managed remotely with the help of dedicated protocols that were originally designed to work in walled gardens. Many of these protocols have been adapted to Internet transport and support wide-area communication. ICS now exchange insecure traffic on an inter-domain level, putting at risk not only common critical infrastructure but also the Internet ecosystem (e.g., DRDoS~attacks).   In this paper, we uncover unprotected inter-domain ICS traffic at two central Internet vantage points, an IXP and an ISP. This traffic analysis is correlated with data from honeypots and Internet-wide scans to separate industrial from non-industrial ICS traffic. We provide an in-depth view on Internet-wide ICS communication. Our results can be used i) to create precise filters for potentially harmful non-industrial ICS traffic, and ii) to detect ICS sending unprotected inter-domain ICS traffic, being vulnerable to eavesdropping and traffic manipulation attacks.

</details>

<details>

<summary>2020-04-23 19:47:43 - Adversarial Machine Learning in Network Intrusion Detection Systems</summary>

- *Elie Alhajjar, Paul Maxwell, Nathaniel D. Bastian*

- `2004.11898v1` - [abs](http://arxiv.org/abs/2004.11898v1) - [pdf](http://arxiv.org/pdf/2004.11898v1)

> Adversarial examples are inputs to a machine learning system intentionally crafted by an attacker to fool the model into producing an incorrect output. These examples have achieved a great deal of success in several domains such as image recognition, speech recognition and spam detection. In this paper, we study the nature of the adversarial problem in Network Intrusion Detection Systems (NIDS). We focus on the attack perspective, which includes techniques to generate adversarial examples capable of evading a variety of machine learning models. More specifically, we explore the use of evolutionary computation (particle swarm optimization and genetic algorithm) and deep learning (generative adversarial networks) as tools for adversarial example generation. To assess the performance of these algorithms in evading a NIDS, we apply them to two publicly available data sets, namely the NSL-KDD and UNSW-NB15, and we contrast them to a baseline perturbation method: Monte Carlo simulation. The results show that our adversarial example generation techniques cause high misclassification rates in eleven different machine learning models, along with a voting classifier. Our work highlights the vulnerability of machine learning based NIDS in the face of adversarial perturbation.

</details>

<details>

<summary>2020-04-24 13:18:35 - Predicting Vulnerability In Large Codebases With Deep Code Representation</summary>

- *Anshul Tanwar, Krishna Sundaresan, Parmesh Ashwath, Prasanna Ganesan, Sathish Kumar Chandrasekaran, Sriram Ravi*

- `2004.12783v1` - [abs](http://arxiv.org/abs/2004.12783v1) - [pdf](http://arxiv.org/pdf/2004.12783v1)

> Currently, while software engineers write code for various modules, quite often, various types of errors - coding, logic, semantic, and others (most of which are not caught by compilation and other tools) get introduced. Some of these bugs might be found in the later stage of testing, and many times it is reported by customers on production code. Companies have to spend many resources, both money and time in finding and fixing the bugs which would have been avoided if coding was done right. Also, concealed flaws in software can lead to security vulnerabilities that potentially allow attackers to compromise systems and applications. Interestingly, same or similar issues/bugs, which were fixed in the past (although in different modules), tend to get introduced in production code again.   We developed a novel AI-based system which uses the deep representation of Abstract Syntax Tree (AST) created from the source code and also the active feedback loop to identify and alert the potential bugs that could be caused at the time of development itself i.e. as the developer is writing new code (logic and/or function). This tool integrated with IDE as a plugin would work in the background, point out existing similar functions/code-segments and any associated bugs in those functions. The tool would enable the developer to incorporate suggestions right at the time of development, rather than waiting for UT/QA/customer to raise a defect.   We assessed our tool on both open-source code and also on Cisco codebase for C and C++ programing language. Our results confirm that deep representation of source code and the active feedback loop is an assuring approach for predicting security and other vulnerabilities present in the code.

</details>

<details>

<summary>2020-04-25 18:10:26 - On the safety of vulnerable road users by cyclist orientation detection using Deep Learning</summary>

- *Marichelo Garcia-Venegas, Diego A. Mercado-Ravell, Carlos A. Carballo-Monsivais*

- `2004.11909v1` - [abs](http://arxiv.org/abs/2004.11909v1) - [pdf](http://arxiv.org/pdf/2004.11909v1)

> In this work, orientation detection using Deep Learning is acknowledged for a particularly vulnerable class of road users,the cyclists. Knowing the cyclists' orientation is of great relevance since it provides a good notion about their future trajectory, which is crucial to avoid accidents in the context of intelligent transportation systems. Using Transfer Learning with pre-trained models and TensorFlow, we present a performance comparison between the main algorithms reported in the literature for object detection,such as SSD, Faster R-CNN and R-FCN along with MobilenetV2, InceptionV2, ResNet50, ResNet101 feature extractors. Moreover, we propose multi-class detection with eight different classes according to orientations. To do so, we introduce a new dataset called "Detect-Bike", containing 20,229 cyclist instances over 11,103 images, which has been labeled based on cyclist's orientation. Then, the same Deep Learning methods used for detection are trained to determine the target's heading. Our experimental results and vast evaluation showed satisfactory performance of all of the studied methods for the cyclists and their orientation detection, especially using Faster R-CNN with ResNet50 proved to be precise but significantly slower. Meanwhile, SSD using InceptionV2 provided good trade-off between precision and execution time, and is to be preferred for real-time embedded applications.

</details>

<details>

<summary>2020-04-26 15:23:23 - Why is My Secret Leaked? Discovering Vulnerabilities in Device-to-Device File Sharing</summary>

- *Andrei Bytes, Jay Prakash, Jianying Zhou, Tony Q. S. Quek*

- `2002.03144v2` - [abs](http://arxiv.org/abs/2002.03144v2) - [pdf](http://arxiv.org/pdf/2002.03144v2)

> The number of active users of Wi-Fi Direct Device-to-Device file sharing applications on Android has exceeded 1.8 billion. Wi-Fi Direct, also known as Wi-Fi P2P, is commonly used for peer-to-peer, high-speed file transfer between mobile devices, as well as a close proximity connection mode for wireless cameras, network printers, TVs and other IoT and mobile devices. For its end users, such type of direct file transfer does not incur cellular data charges. However, despite the popularity of such applications, we observe that the software vendors tend to prioritize the ease of user flow over the security in their implementations, which leads to serious security flaws. We perform a comprehensive security analysis in the context of security and usability and report our findings in the form of 17 Common Vulnerabilities and Exposures (CVE) which have been disclosed to the corresponding vendors. To address the similar flaws at the early stage of the application design, we propose a joint consideration of security and usability for such applications and their protocols that can be visualized in form of a customised User Journey Map (UJM).

</details>

<details>

<summary>2020-04-26 18:12:24 - SMART: Secure Magnetoelectric AntifeRromagnet-Based Tamper-Proof Non-Volatile Memory</summary>

- *Nikhil Rangarajan, Satwik Patnaik, Johann Knechtel, Ozgur Sinanoglu, Shaloo Rakheja*

- `1902.07792v2` - [abs](http://arxiv.org/abs/1902.07792v2) - [pdf](http://arxiv.org/pdf/1902.07792v2)

> The storage industry is moving toward emerging non-volatile memories (NVMs), including the spin-transfer torque magnetoresistive random-access memory (STT-MRAM) and the phase-change memory (PCM), owing to their high density and low-power operation. In this paper, we demonstrate, for the first time, circuit models and performance benchmarking for the domain wall (DW) reversal-based magnetoelectric-antiferromagnetic random access memory (ME-AFMRAM) at cell-level and at array-level. We also provide perspectives for coherent rotation-based memory switching with topological insulator-driven anomalous Hall read-out. In the coherent rotation regime, the ultra-low power magnetoelectric switching coupled with the terahertz-range antiferromagnetic dynamics result in substantially lower energy-per-bit and latency metrics for the ME-AFMRAM compared to other NVMs including STTMRAM and PCM. After characterizing the novel ME-AFMRAM, we leverage its unique properties to build a dense, on-chip, secure NVM platform, called SMART: A Secure Magnetoelectric Antiferromagnet- Based Tamper-Proof Non-Volatile Memory. New NVM technologies open up challenges and opportunities from a data-security perspective. For example, their sensitivity to magnetic fields and temperature fluctuations, and their data remanence after power-down make NVMs vulnerable to data theft and tampering attacks. The proposed SMART memory is not only resilient against data confidentiality attacks seeking to leak sensitive information but also ensures data integrity and prevents Denial-of-Service (DoS) attacks on the memory. It is impervious to particular power side-channel (PSC) attacks which exploit asymmetric read/write signatures for 0 and 1 logic levels, and photonic side-channel attacks which monitor photo-emission signatures from the chip backside.

</details>

<details>

<summary>2020-04-26 21:50:35 - Security of Distributed Machine Learning: A Game-Theoretic Approach to Design Secure DSVM</summary>

- *Rui Zhang, Quanyan Zhu*

- `2003.04735v2` - [abs](http://arxiv.org/abs/2003.04735v2) - [pdf](http://arxiv.org/pdf/2003.04735v2)

> Distributed machine learning algorithms play a significant role in processing massive data sets over large networks. However, the increasing reliance on machine learning on information and communication technologies (ICTs) makes it inherently vulnerable to cyber threats. This work aims to develop secure distributed algorithms to protect the learning from data poisoning and network attacks. We establish a game-theoretic framework to capture the conflicting goals of a learner who uses distributed support vector machines (SVMs) and an attacker who is capable of modifying training data and labels. We develop a fully distributed and iterative algorithm to capture real-time reactions of the learner at each node to adversarial behaviors. The numerical results show that distributed SVM is prone to fail in different types of attacks, and their impact has a strong dependence on the network structure and attack capabilities.

</details>

<details>

<summary>2020-04-26 22:20:25 - Ensemble Adversarial Training: Attacks and Defenses</summary>

- *Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel*

- `1705.07204v5` - [abs](http://arxiv.org/abs/1705.07204v5) - [pdf](http://arxiv.org/pdf/1705.07204v5)

> Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks. However, subsequent work found that more elaborate black-box attacks could significantly enhance transferability and reduce the accuracy of our models.

</details>

<details>

<summary>2020-04-26 22:30:52 - Bias Busters: Robustifying DL-based Lithographic Hotspot Detectors Against Backdooring Attacks</summary>

- *Kang Liu, Benjamin Tan, Gaurav Rajavendra Reddy, Siddharth Garg, Yiorgos Makris, Ramesh Karri*

- `2004.12492v1` - [abs](http://arxiv.org/abs/2004.12492v1) - [pdf](http://arxiv.org/pdf/2004.12492v1)

> Deep learning (DL) offers potential improvements throughout the CAD tool-flow, one promising application being lithographic hotspot detection. However, DL techniques have been shown to be especially vulnerable to inference and training time adversarial attacks. Recent work has demonstrated that a small fraction of malicious physical designers can stealthily "backdoor" a DL-based hotspot detector during its training phase such that it accurately classifies regular layout clips but predicts hotspots containing a specially crafted trigger shape as non-hotspots. We propose a novel training data augmentation strategy as a powerful defense against such backdooring attacks. The defense works by eliminating the intentional biases introduced in the training data but does not require knowledge of which training samples are poisoned or the nature of the backdoor trigger. Our results show that the defense can drastically reduce the attack success rate from 84% to ~0%.

</details>

<details>

<summary>2020-04-27 08:10:51 - Evaluating Explanation Methods for Deep Learning in Security</summary>

- *Alexander Warnecke, Daniel Arp, Christian Wressnegger, Konrad Rieck*

- `1906.02108v4` - [abs](http://arxiv.org/abs/1906.02108v4) - [pdf](http://arxiv.org/pdf/1906.02108v4)

> Deep learning is increasingly used as a building block of security systems. Unfortunately, neural networks are hard to interpret and typically opaque to the practitioner. The machine learning community has started to address this problem by developing methods for explaining the predictions of neural networks. While several of these approaches have been successfully applied in the area of computer vision, their application in security has received little attention so far. It is an open question which explanation methods are appropriate for computer security and what requirements they need to satisfy. In this paper, we introduce criteria for comparing and evaluating explanation methods in the context of computer security. These cover general properties, such as the accuracy of explanations, as well as security-focused aspects, such as the completeness, efficiency, and robustness. Based on our criteria, we investigate six popular explanation methods and assess their utility in security systems for malware detection and vulnerability discovery. We observe significant differences between the methods and build on these to derive general recommendations for selecting and applying explanation methods in computer security.

</details>

<details>

<summary>2020-04-27 09:28:33 - Robust Algorithms under Adversarial Injections</summary>

- *Paritosh Garg, Sagar Kale, Lars Rohwedder, Ola Svensson*

- `2004.12667v1` - [abs](http://arxiv.org/abs/2004.12667v1) - [pdf](http://arxiv.org/pdf/2004.12667v1)

> In this paper, we study streaming and online algorithms in the context of randomness in the input. For several problems, a random order of the input sequence---as opposed to the worst-case order---appears to be a necessary evil in order to prove satisfying guarantees. However, algorithmic techniques that work under this assumption tend to be vulnerable to even small changes in the distribution. For this reason, we propose a new \emph{adversarial injections} model, in which the input is ordered randomly, but an adversary may inject misleading elements at arbitrary positions. We believe that studying algorithms under this much weaker assumption can lead to new insights and, in particular, more robust algorithms. We investigate two classical combinatorial-optimization problems in this model: Maximum matching and cardinality constrained monotone submodular function maximization. Our main technical contribution is a novel streaming algorithm for the latter that computes a $0.55$-approximation. While the algorithm itself is clean and simple, an involved analysis shows that it emulates a subdivision of the input stream which can be used to greatly limit the power of the adversary.

</details>

<details>

<summary>2020-04-27 15:18:49 - DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking</summary>

- *Christopher Hidey, Tuhin Chakrabarty, Tariq Alhindi, Siddharth Varia, Kriste Krstovski, Mona Diab, Smaranda Muresan*

- `2004.12864v1` - [abs](http://arxiv.org/abs/2004.12864v1) - [pdf](http://arxiv.org/pdf/2004.12864v1)

> The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating end-to-end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction. We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking -- multiple propositions, temporal reasoning, and ambiguity and lexical variation -- and introduce a resource with these types of claims. Then we present a system designed to be resilient to these "attacks" using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions. We find that in handling these attacks we obtain state-of-the-art results on FEVER, largely due to improved evidence retrieval.

</details>

<details>

<summary>2020-04-28 00:43:42 - Modelling and Quantifying Membership Information Leakage in Machine Learning</summary>

- *Farhad Farokhi, Mohamed Ali Kaafar*

- `2001.10648v2` - [abs](http://arxiv.org/abs/2001.10648v2) - [pdf](http://arxiv.org/pdf/2001.10648v2)

> Machine learning models have been shown to be vulnerable to membership inference attacks, i.e., inferring whether individuals' data have been used for training models. The lack of understanding about factors contributing success of these attacks motivates the need for modelling membership information leakage using information theory and for investigating properties of machine learning models and training algorithms that can reduce membership information leakage. We use conditional mutual information leakage to measure the amount of information leakage from the trained machine learning model about the presence of an individual in the training dataset. We devise an upper bound for this measure of information leakage using Kullback--Leibler divergence that is more amenable to numerical computation. We prove a direct relationship between the Kullback--Leibler membership information leakage and the probability of success for a hypothesis-testing adversary examining whether a particular data record belongs to the training dataset of a machine learning model. We show that the mutual information leakage is a decreasing function of the training dataset size and the regularization weight. We also prove that, if the sensitivity of the machine learning model (defined in terms of the derivatives of the fitness with respect to model parameters) is high, more membership information is potentially leaked. This illustrates that complex models, such as deep neural networks, are more susceptible to membership inference attacks in comparison to simpler models with fewer degrees of freedom. We show that the amount of the membership information leakage is reduced by $\mathcal{O}(\log^{1/2}(\delta^{-1})\epsilon^{-1})$ when using Gaussian $(\epsilon,\delta)$-differentially-private additive noises.

</details>

<details>

<summary>2020-04-28 12:16:16 - MuonTrap: Preventing Cross-Domain Spectre-Like Attacks by Capturing Speculative State</summary>

- *Sam Ainsworth, Timothy M. Jones*

- `1911.08384v2` - [abs](http://arxiv.org/abs/1911.08384v2) - [pdf](http://arxiv.org/pdf/1911.08384v2)

> The disclosure of the Spectre speculative-execution attacks in January 2018 has left a severe vulnerability that systems are still struggling with how to patch. The solutions that currently exist tend to have incomplete coverage, perform badly, or have highly undesirable edge cases that cause application domains to break.   MuonTrap allows processors to continue to speculate, avoiding significant reductions in performance, without impacting security. We instead prevent the propagation of any state based on speculative execution, by placing the results of speculative cache accesses into a small, fast L0 filter cache, that is non-inclusive, non-exclusive with the rest of the cache hierarchy. This isolates all parts of the system that can't be quickly cleared on any change in threat domain.   MuonTrap uses these speculative filter caches, which are cleared on context and protection-domain switches, along with a series of extensions to the cache coherence protocol and prefetcher. This renders systems immune to cross-domain information leakage via Spectre and a host of similar attacks based on speculative execution, with low performance impact and few changes to the CPU design.

</details>

<details>

<summary>2020-04-28 19:27:48 - Conspiracy in the Time of Corona: Automatic detection of Covid-19 Conspiracy Theories in Social Media and the News</summary>

- *Shadi Shahsavari, Pavan Holur, Timothy R. Tangherlini, Vwani Roychowdhury*

- `2004.13783v1` - [abs](http://arxiv.org/abs/2004.13783v1) - [pdf](http://arxiv.org/pdf/2004.13783v1)

> Rumors and conspiracy theories thrive in environments of low confidence and low trust. Consequently, it is not surprising that ones related to the Covid-19 pandemic are proliferating given the lack of any authoritative scientific consensus on the virus, its spread and containment, or on the long term social and economic ramifications of the pandemic. Among the stories currently circulating are ones suggesting that the 5G network activates the virus, that the pandemic is a hoax perpetrated by a global cabal, that the virus is a bio-weapon released deliberately by the Chinese, or that Bill Gates is using it as cover to launch a global surveillance regime. While some may be quick to dismiss these stories as having little impact on real-world behavior, recent events including the destruction of property, racially fueled attacks against Asian Americans, and demonstrations espousing resistance to public health orders countermand such conclusions. Inspired by narrative theory, we crawl social media sites and news reports and, through the application of automated machine-learning methods, discover the underlying narrative frameworks supporting the generation of these stories. We show how the various narrative frameworks fueling rumors and conspiracy theories rely on the alignment of otherwise disparate domains of knowledge, and consider how they attach to the broader reporting on the pandemic. These alignments and attachments, which can be monitored in near real-time, may be useful for identifying areas in the news that are particularly vulnerable to reinterpretation by conspiracy theorists. Understanding the dynamics of storytelling on social media and the narrative frameworks that provide the generative basis for these stories may also be helpful for devising methods to disrupt their spread.

</details>

<details>

<summary>2020-04-28 20:11:18 - Minority Reports Defense: Defending Against Adversarial Patches</summary>

- *Michael McCoyd, Won Park, Steven Chen, Neil Shah, Ryan Roggenkemper, Minjune Hwang, Jason Xinyu Liu, David Wagner*

- `2004.13799v1` - [abs](http://arxiv.org/abs/2004.13799v1) - [pdf](http://arxiv.org/pdf/2004.13799v1)

> Deep learning image classification is vulnerable to adversarial attack, even if the attacker changes just a small patch of the image. We propose a defense against patch attacks based on partially occluding the image around each candidate patch location, so that a few occlusions each completely hide the patch. We demonstrate on CIFAR-10, Fashion MNIST, and MNIST that our defense provides certified security against patch attacks of a certain size.

</details>

<details>

<summary>2020-04-29 12:25:32 - Active Subspace of Neural Networks: Structural Analysis and Universal Attacks</summary>

- *Chunfeng Cui, Kaiqi Zhang, Talgat Daulbaev, Julia Gusak, Ivan Oseledets, Zheng Zhang*

- `1910.13025v2` - [abs](http://arxiv.org/abs/1910.13025v2) - [pdf](http://arxiv.org/pdf/1910.13025v2)

> Active subspace is a model reduction method widely used in the uncertainty quantification community. In this paper, we propose analyzing the internal structure and vulnerability and deep neural networks using active subspace. Firstly, we employ the active subspace to measure the number of "active neurons" at each intermediate layer and reduce the number of neurons from several thousands to several dozens. This motivates us to change the network structure and to develop a new and more compact network, referred to as {ASNet}, that has significantly fewer model parameters. Secondly, we propose analyzing the vulnerability of a neural network using active subspace and finding an additive universal adversarial attack vector that can misclassify a dataset with a high probability. Our experiments on CIFAR-10 show that ASNet can achieve 23.98$\times$ parameter and 7.30$\times$ flops reduction. The universal active subspace attack vector can achieve around 20% higher attack ratio compared with the existing approach in all of our numerical experiments. The PyTorch codes for this paper are available online.

</details>

<details>

<summary>2020-04-29 21:16:31 - Adversarial Training for Large Neural Language Models</summary>

- *Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, Jianfeng Gao*

- `2004.08994v2` - [abs](http://arxiv.org/abs/2004.08994v2) - [pdf](http://arxiv.org/pdf/2004.08994v2)

> Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.

</details>

<details>

<summary>2020-04-30 12:50:39 - A Robust Hierarchical Graph Convolutional Network Model for Collaborative Filtering</summary>

- *Shaowen Peng, Tsunenori Mine*

- `2004.14734v1` - [abs](http://arxiv.org/abs/2004.14734v1) - [pdf](http://arxiv.org/pdf/2004.14734v1)

> Graph Convolutional Network (GCN) has achieved great success and has been applied in various fields including recommender systems. However, GCN still suffers from many issues such as training difficulties, over-smoothing, vulnerable to adversarial attacks, etc. Distinct from current GCN-based methods which simply employ GCN for recommendation, in this paper we are committed to build a robust GCN model for collaborative filtering. Firstly, we argue that recursively incorporating messages from different order neighborhood mixes distinct node messages indistinguishably, which increases the training difficulty; instead we choose to separately aggregate different order neighbor messages with a simple GCN model which has been shown effective; then we accumulate them together in a hierarchical way without introducing additional model parameters. Secondly, we propose a solution to alleviate over-smoothing by randomly dropping out neighbor messages at each layer, which also well prevents over-fitting and enhances the robustness. Extensive experiments on three real-world datasets demonstrate the effectiveness and robustness of our model.

</details>


## 2020-05

<details>

<summary>2020-05-01 07:29:34 - Certifying Some Distributional Robustness with Principled Adversarial Training</summary>

- *Aman Sinha, Hongseok Namkoong, Riccardo Volpi, John Duchi*

- `1710.10571v5` - [abs](http://arxiv.org/abs/1710.10571v5) - [pdf](http://arxiv.org/pdf/1710.10571v5)

> Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.

</details>

<details>

<summary>2020-05-01 17:27:27 - Practical Traffic Analysis Attacks on Secure Messaging Applications</summary>

- *Alireza Bahramali, Ramin Soltani, Amir Houmansadr, Dennis Goeckel, Don Towsley*

- `2005.00508v1` - [abs](http://arxiv.org/abs/2005.00508v1) - [pdf](http://arxiv.org/pdf/2005.00508v1)

> Instant Messaging (IM) applications like Telegram, Signal, and WhatsApp have become extremely popular in recent years. Unfortunately, such IM services have been targets of continuous governmental surveillance and censorship, as these services are home to public and private communication channels on socially and politically sensitive topics. To protect their clients, popular IM services deploy state-of-the-art encryption mechanisms. In this paper, we show that despite the use of advanced encryption, popular IM applications leak sensitive information about their clients to adversaries who merely monitor their encrypted IM traffic, with no need for leveraging any software vulnerabilities of IM applications. Specifically, we devise traffic analysis attacks that enable an adversary to identify administrators as well as members of target IM channels (e.g., forums) with high accuracies. We believe that our study demonstrates a significant, real-world threat to the users of such services given the increasing attempts by oppressive governments at cracking down controversial IM channels.   We demonstrate the practicality of our traffic analysis attacks through extensive experiments on real-world IM communications. We show that standard countermeasure techniques such as adding cover traffic can degrade the effectiveness of the attacks we introduce in this paper. We hope that our study will encourage IM providers to integrate effective traffic obfuscation countermeasures into their software. In the meantime, we have designed and deployed an open-source, publicly available countermeasure system, called IMProxy, that can be used by IM clients with no need for any support from IM providers. We have demonstrated the effectiveness of IMProxy through experiments.

</details>

<details>

<summary>2020-05-02 04:57:34 - Differentially Private Collaborative Intrusion Detection Systems For VANETs</summary>

- *Tao Zhang, Quanyan Zhu*

- `2005.00703v1` - [abs](http://arxiv.org/abs/2005.00703v1) - [pdf](http://arxiv.org/pdf/2005.00703v1)

> Vehicular ad hoc network (VANET) is an enabling technology in modern transportation systems for providing safety and valuable information, and yet vulnerable to a number of attacks from passive eavesdropping to active interfering. Intrusion detection systems (IDSs) are important devices that can mitigate the threats by detecting malicious behaviors. Furthermore, the collaborations among vehicles in VANETs can improve the detection accuracy by communicating their experiences between nodes. To this end, distributed machine learning is a suitable framework for the design of scalable and implementable collaborative detection algorithms over VANETs. One fundamental barrier to collaborative learning is the privacy concern as nodes exchange data among them. A malicious node can obtain sensitive information of other nodes by inferring from the observed data. In this paper, we propose a privacy-preserving machine-learning based collaborative IDS (PML-CIDS) for VANETs. The proposed algorithm employs the alternating direction method of multipliers (ADMM) to a class of empirical risk minimization (ERM) problems and trains a classifier to detect the intrusions in the VANETs. We use the differential privacy to capture the privacy notation of the PML-CIDS and propose a method of dual variable perturbation to provide dynamic differential privacy. We analyze theoretical performance and characterize the fundamental tradeoff between the security and privacy of the PML-CIDS. We also conduct numerical experiments using the NSL-KDD dataset to corroborate the results on the detection accuracy, security-privacy tradeoffs, and design.

</details>

<details>

<summary>2020-05-02 06:39:33 - Enhancing network forensics with particle swarm and deep learning: The particle deep framework</summary>

- *Nickolaos Koroniotis, Nour Moustafa*

- `2005.00722v1` - [abs](http://arxiv.org/abs/2005.00722v1) - [pdf](http://arxiv.org/pdf/2005.00722v1)

> The popularity of IoT smart things is rising, due to the automation they provide and its effects on productivity. However, it has been proven that IoT devices are vulnerable to both well established and new IoT-specific attack vectors. In this paper, we propose the Particle Deep Framework, a new network forensic framework for IoT networks that utilised Particle Swarm Optimisation to tune the hyperparameters of a deep MLP model and improve its performance. The PDF is trained and validated using Bot-IoT dataset, a contemporary network-traffic dataset that combines normal IoT and non-IoT traffic, with well known botnet-related attacks. Through experimentation, we show that the performance of a deep MLP model is vastly improved, achieving an accuracy of 99.9% and false alarm rate of close to 0%.

</details>

<details>

<summary>2020-05-02 07:07:01 - Epione: Lightweight Contact Tracing with Strong Privacy</summary>

- *Ni Trieu, Kareem Shehata, Prateek Saxena, Reza Shokri, Dawn Song*

- `2004.13293v3` - [abs](http://arxiv.org/abs/2004.13293v3) - [pdf](http://arxiv.org/pdf/2004.13293v3)

> Contact tracing is an essential tool in containing infectious diseases such as COVID-19. Many countries and research groups have launched or announced mobile apps to facilitate contact tracing by recording contacts between users with some privacy considerations. Most of the focus has been on using random tokens, which are exchanged during encounters and stored locally on users' phones. Prior systems allow users to search over released tokens in order to learn if they have recently been in the proximity of a user that has since been diagnosed with the disease. However, prior approaches do not provide end-to-end privacy in the collection and querying of tokens. In particular, these approaches are vulnerable to either linkage attacks by users using token metadata, linkage attacks by the server, or false reporting by users.   In this work, we introduce Epione, a lightweight system for contact tracing with strong privacy protections. Epione alerts users directly if any of their contacts have been diagnosed with the disease, while protecting the privacy of users' contacts from both central services and other users, and provides protection against false reporting. As a key building block, we present a new cryptographic tool for secure two-party private set intersection cardinality (PSI-CA), which allows two parties, each holding a set of items, to learn the intersection size of two private sets without revealing intersection items. We specifically tailor it to the case of large-scale contact tracing where clients have small input sets and the server's database of tokens is much larger.

</details>

<details>

<summary>2020-05-02 20:10:34 - Security Aspects of Internet of Things aided Smart Grids: a Bibliometric Survey</summary>

- *Jacob Sakhnini, Hadis Karimipour, Ali Dehghantanha, Reza M. Parizi, Gautam Srivastava*

- `2005.00915v1` - [abs](http://arxiv.org/abs/2005.00915v1) - [pdf](http://arxiv.org/pdf/2005.00915v1)

> The integration of sensors and communication technology in power systems, known as the smart grid, is an emerging topic in science and technology. One of the critical issues in the smart grid is its increased vulnerability to cyber threats. As such, various types of threats and defense mechanisms are proposed in literature. This paper offers a bibliometric survey of research papers focused on the security aspects of Internet of Things (IoT) aided smart grids. To the best of the authors' knowledge, this is the very first bibliometric survey paper in this specific field. A bibliometric analysis of all journal articles is performed and the findings are sorted by dates, authorship, and key concepts. Furthermore, this paper also summarizes the types of cyber threats facing the smart grid, the various security mechanisms proposed in literature, as well as the research gaps in the field of smart grid security.

</details>

<details>

<summary>2020-05-03 03:06:12 - Repairing Deep Neural Networks: Fix Patterns and Challenges</summary>

- *Md Johirul Islam, Rangeet Pan, Giang Nguyen, Hridesh Rajan*

- `2005.00972v1` - [abs](http://arxiv.org/abs/2005.00972v1) - [pdf](http://arxiv.org/pdf/2005.00972v1)

> Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack overflow and 555 repairs from Github for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.

</details>

<details>

<summary>2020-05-03 18:59:00 - TOFU: Target-Oriented FUzzer</summary>

- *Zi Wang, Ben Liblit, Thomas Reps*

- `2004.14375v2` - [abs](http://arxiv.org/abs/2004.14375v2) - [pdf](http://arxiv.org/pdf/2004.14375v2)

> Program fuzzing---providing randomly constructed inputs to a computer program---has proved to be a powerful way to uncover bugs, find security vulnerabilities, and generate test inputs that increase code coverage. In many applications, however, one is interested in a target-oriented approach-one wants to find an input that causes the program to reach a specific target point in the program. We have created TOFU (for Target-Oriented FUzzer) to address the directed fuzzing problem. TOFU's search is biased according to a distance metric that scores each input according to how close the input's execution trace gets to the target locations. TOFU is also input-structure aware (i.e., the search makes use of a specification of a superset of the program's allowed inputs).   Our experiments on xmllint show that TOFU is 28% faster than AFLGo, while reaching 45% more targets. Moreover, both distance-guided search and exploitation of knowledge of the input structure contribute significantly to TOFU's performance.

</details>

<details>

<summary>2020-05-04 13:17:06 - Crafting, Communality, and Computing: Building on Existing Strengths To Support a Vulnerable Population</summary>

- *Aakash Gautam, Deborah Tatar, Steve Harrison*

- `2005.01459v1` - [abs](http://arxiv.org/abs/2005.01459v1) - [pdf](http://arxiv.org/pdf/2005.01459v1)

> In Nepal, sex-trafficking survivors and the organizations that support them have limited resources to assist the survivors in their on-going journey towards reintegration. We take an asset-based approach wherein we identify and build on the strengths possessed by such groups. In this work, we present reflections from introducing a voice-annotated web application to a group of survivors. The web application tapped into and built upon two elements of pre-existing strengths possessed by the survivors -- the social bond between them and knowledge of crafting as taught to them by the organization. Our findings provide insight into the array of factors influencing how the survivors act in relation to one another as they created novel use practices and adapted the technology. Experience with the application seemed to open knowledge of computing as a potential source of strength. Finally, we articulate three design desiderata that could help promote communal spaces: make activity perceptible to the group, create appropriable steps, and build in fun choices.

</details>

<details>

<summary>2020-05-06 21:22:35 - Towards Frequency-Based Explanation for Robust CNN</summary>

- *Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal, Zihao Ding*

- `2005.03141v1` - [abs](http://arxiv.org/abs/2005.03141v1) - [pdf](http://arxiv.org/pdf/2005.03141v1)

> Current explanation techniques towards a transparent Convolutional Neural Network (CNN) mainly focuses on building connections between the human-understandable input features with models' prediction, overlooking an alternative representation of the input, the frequency components decomposition. In this work, we present an analysis of the connection between the distribution of frequency components in the input dataset and the reasoning process the model learns from the data. We further provide quantification analysis about the contribution of different frequency components toward the model's prediction. We show that the vulnerability of the model against tiny distortions is a result of the model is relying on the high-frequency features, the target features of the adversarial (black and white-box) attackers, to make the prediction. We further show that if the model develops stronger association between the low-frequency component with true labels, the model is more robust, which is the explanation of why adversarially trained models are more robust against tiny distortions.

</details>

<details>

<summary>2020-05-07 01:51:11 - Enabling Cross-chain Transactions: A Decentralized Cryptocurrency Exchange Protocol</summary>

- *Hangyu Tian, Kaiping Xue, Shaohua Li, Jie Xu, Jianqing Liu, Jun Zhao*

- `2005.03199v1` - [abs](http://arxiv.org/abs/2005.03199v1) - [pdf](http://arxiv.org/pdf/2005.03199v1)

> Inspired by Bitcoin, many different kinds of cryptocurrencies based on blockchain technology have turned up on the market. Due to the special structure of the blockchain, it has been deemed impossible to directly trade between traditional currencies and cryptocurrencies or between different types of cryptocurrencies. Generally, trading between different currencies is conducted through a centralized third-party platform. However, it has the problem of a single point of failure, which is vulnerable to attacks and thus affects the security of the transactions. In this paper, we propose a distributed cryptocurrency trading scheme to solve the problem of centralized exchanges, which can achieve trading between different types of cryptocurrencies. Our scheme is implemented with smart contracts on the Ethereum blockchain and deployed on the Ethereum test network. We not only implement transactions between individual users, but also allow transactions between multiple users. The experimental result proves that the cost of our scheme is acceptable.

</details>

<details>

<summary>2020-05-07 14:59:59 - p for political: Participation Without Agency Is Not Enough</summary>

- *Aakash Gautam, Deborah Tatar*

- `2005.03534v1` - [abs](http://arxiv.org/abs/2005.03534v1) - [pdf](http://arxiv.org/pdf/2005.03534v1)

> Participatory Design's vision of democratic participation assumes participants' feelings of agency in envisioning a collective future. But this assumption may be leaky when dealing with vulnerable populations. We reflect on the results of a series of activities aimed at supporting agentic-future-envisionment with a group of sex-trafficking survivors in Nepal. We observed a growing sense among the survivors that they could play a role in bringing about change in their families. They also became aware of how they could interact with available institutional resources. Reflecting on the observations, we argue that building participant agency on the small and personal interactions is necessary before demanding larger Political participation. In particular, a value of PD, especially for vulnerable populations, can lie in the process itself if it helps participants position themselves as actors in the larger world.

</details>

<details>

<summary>2020-05-07 16:13:12 - Predictive Modeling of ICU Healthcare-Associated Infections from Imbalanced Data. Using Ensembles and a Clustering-Based Undersampling Approach</summary>

- *Fernando Sánchez-Hernández, Juan Carlos Ballesteros-Herráez, Mohamed S. Kraiem, Mercedes Sánchez-Barba, María N. Moreno-García*

- `2005.03582v1` - [abs](http://arxiv.org/abs/2005.03582v1) - [pdf](http://arxiv.org/pdf/2005.03582v1)

> Early detection of patients vulnerable to infections acquired in the hospital environment is a challenge in current health systems given the impact that such infections have on patient mortality and healthcare costs. This work is focused on both the identification of risk factors and the prediction of healthcare-associated infections in intensive-care units by means of machine-learning methods. The aim is to support decision making addressed at reducing the incidence rate of infections. In this field, it is necessary to deal with the problem of building reliable classifiers from imbalanced datasets. We propose a clustering-based undersampling strategy to be used in combination with ensemble classifiers. A comparative study with data from 4616 patients was conducted in order to validate our proposal. We applied several single and ensemble classifiers both to the original dataset and to data preprocessed by means of different resampling methods. The results were analyzed by means of classic and recent metrics specifically designed for imbalanced data classification. They revealed that the proposal is more efficient in comparison with other approaches.

</details>

<details>

<summary>2020-05-08 09:30:19 - Proactive Defense for Internet-of-Things: Integrating Moving Target Defense with Cyberdeception</summary>

- *Mengmeng Ge, Jin-Hee Cho, Dong Seong Kim, Gaurav Dixit, Ing-Ray Chen*

- `2005.04220v1` - [abs](http://arxiv.org/abs/2005.04220v1) - [pdf](http://arxiv.org/pdf/2005.04220v1)

> Resource constrained Internet-of-Things (IoT) devices are highly likely to be compromised by attackers because strong security protections may not be suitable to be deployed. This requires an alternative approach to protect vulnerable components in IoT networks. In this paper, we propose an integrated defense technique to achieve intrusion prevention by leveraging cyberdeception (i.e., a decoy system) and moving target defense (i.e., network topology shuffling). We verify the effectiveness and efficiency of our proposed technique analytically based on a graphical security model in a software defined networking (SDN)-based IoT network. We develop four strategies (i.e., fixed/random and adaptive/hybrid) to address "when" to perform network topology shuffling and three strategies (i.e., genetic algorithm/decoy attack path-based optimization/random) to address "how" to perform network topology shuffling on a decoy-populated IoT network, and analyze which strategy can best achieve a system goal such as prolonging the system lifetime, maximizing deception effectiveness, maximizing service availability, or minimizing defense cost. Our results demonstrate that a software defined IoT network running our intrusion prevention technique at the optimal parameter setting prolongs system lifetime, increases attack complexity of compromising critical nodes, and maintains superior service availability compared with a counterpart IoT network without running our intrusion prevention technique. Further, when given a single goal or a multi-objective goal (e.g., maximizing the system lifetime and service availability while minimizing the defense cost) as input, the best combination of "how" and "how" strategies is identified for executing our proposed technique under which the specified goal can be best achieved.

</details>

<details>

<summary>2020-05-08 13:50:11 - Convergence of IT and SCADA: Associated Security Threats and Vulnerabilities</summary>

- *Michael Smurthwaite, Maumita Bhattacharya*

- `2005.04047v1` - [abs](http://arxiv.org/abs/2005.04047v1) - [pdf](http://arxiv.org/pdf/2005.04047v1)

> As many industries shift towards centralised controlled information systems for monitoring and control, more importance is being placed upon technologies such as Supervisory Control and Data Acquisitions industrial systems (SCADA). This focus on integration and interoperability presents numerous challenges for security personnel and organisational management alike. It becomes paramount therefore to reciprocate this new direction within an organisation with adequate plans and frameworks that ensure protection and security of its SCADA architecture. A clear understanding of the relevant threats and vulnerabilities is critical for adopting/developing appropriate policy and frameworks. To this end, in this research we identify and analyse relevant SCADA security threats and vulnerabilities and present a simple scheme to classify them for better understanding.

</details>

<details>

<summary>2020-05-08 14:34:13 - Avoiding Improper Treatment of Persons with Dementia by Care Robots</summary>

- *Martin Cooney, Sepideh Pashami, Eric Järpe, Awais Ashfaq*

- `2005.06622v1` - [abs](http://arxiv.org/abs/2005.06622v1) - [pdf](http://arxiv.org/pdf/2005.06622v1)

> The phrase "most cruel and revolting crimes" has been used to describe some poor historical treatment of vulnerable impaired persons by precisely those who should have had the responsibility of protecting and helping them. We believe we might be poised to see history repeat itself, as increasingly human-like aware robots become capable of engaging in behavior which we would consider immoral in a human--either unknowingly or deliberately. In the current paper we focus in particular on exploring some potential dangers affecting persons with dementia (PWD), which could arise from insufficient software or external factors, and describe a proposed solution involving rich causal models and accountability measures: Specifically, the Consequences of Needs-driven Dementia-compromised Behaviour model (C-NDB) could be adapted to be used with conversation topic detection, causal networks and multi-criteria decision making, alongside reports, audits, and deterrents. Our aim is that the considerations raised could help inform the design of care robots intended to support well-being in PWD.

</details>

<details>

<summary>2020-05-08 14:54:13 - A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View</summary>

- *Lennart Reiher, Bastian Lampe, Lutz Eckstein*

- `2005.04078v1` - [abs](http://arxiv.org/abs/2005.04078v1) - [pdf](http://arxiv.org/pdf/2005.04078v1)

> Accurate environment perception is essential for automated driving. When using monocular cameras, the distance estimation of elements in the environment poses a major challenge. Distances can be more easily estimated when the camera perspective is transformed to a bird's eye view (BEV). For flat surfaces, Inverse Perspective Mapping (IPM) can accurately transform images to a BEV. Three-dimensional objects such as vehicles and vulnerable road users are distorted by this transformation making it difficult to estimate their position relative to the sensor. This paper describes a methodology to obtain a corrected 360{\deg} BEV image given images from multiple vehicle-mounted cameras. The corrected BEV image is segmented into semantic classes and includes a prediction of occluded areas. The neural network approach does not rely on manually labeled data, but is trained on a synthetic dataset in such a way that it generalizes well to real-world data. By using semantically segmented images as input, we reduce the reality gap between simulated and real-world data and are able to show that our method can be successfully applied in the real world. Extensive experiments conducted on the synthetic data demonstrate the superiority of our approach compared to IPM. Source code and datasets are available at https://github.com/ika-rwth-aachen/Cam2BEV

</details>

<details>

<summary>2020-05-08 19:13:22 - On Designing Secure and Robust Scan Chain for Protecting Obfuscated Logic</summary>

- *Hadi Mardani Kamali, Kimia Zamiri Azar, Houman Homayoun, Avesta Sasan*

- `2005.04262v1` - [abs](http://arxiv.org/abs/2005.04262v1) - [pdf](http://arxiv.org/pdf/2005.04262v1)

> In this paper, we assess the security and testability of the state-of-the-art design-for-security (DFS) architectures in the presence of scan-chain locking/obfuscation, a group of solution that has previously proposed to restrict unauthorized access to the scan chain. We discuss the key leakage vulnerability in the recently published prior-art DFS architectures. This leakage relies on the potential glitches in the DFS architecture that could lead the adversary to make a leakage condition in the circuit. Also, we demonstrate that the state-of-the-art DFS architectures impose some substantial architectural drawbacks that moderately affect both test flow and design constraints. We propose a new DFS architecture for building a secure scan chain architecture while addressing the potential of key leakage. The proposed architecture allows the designer to perform the structural test with no limitation, enabling an untrusted foundry to utilize the scan chain for manufacturing fault testing without needing to access the scan chain. Our proposed solution poses negligible limitation/overhead on the test flow, as well as the design criteria.

</details>

<details>

<summary>2020-05-08 23:54:45 - Real-Time Edge Intelligence in the Making: A Collaborative Learning Framework via Federated Meta-Learning</summary>

- *Sen Lin, Guang Yang, Junshan Zhang*

- `2001.03229v2` - [abs](http://arxiv.org/abs/2001.03229v2) - [pdf](http://arxiv.org/pdf/2001.03229v2)

> Many IoT applications at the network edge demand intelligent decisions in a real-time manner. The edge device alone, however, often cannot achieve real-time edge intelligence due to its constrained computing resources and limited local data. To tackle these challenges, we propose a platform-aided collaborative learning framework where a model is first trained across a set of source edge nodes by a federated meta-learning approach, and then it is rapidly adapted to learn a new task at the target edge node, using a few samples only. Further, we investigate the convergence of the proposed federated meta-learning algorithm under mild conditions on node similarity and the adaptation performance at the target edge. To combat against the vulnerability of meta-learning algorithms to possible adversarial attacks, we further propose a robust version of the federated meta-learning algorithm based on distributionally robust optimization, and establish its convergence under mild conditions. Experiments on different datasets demonstrate the effectiveness of the proposed Federated Meta-Learning based framework.

</details>

<details>

<summary>2020-05-09 08:49:01 - Cloud-based Federated Boosting for Mobile Crowdsensing</summary>

- *Zhuzhu Wang, Yilong Yang, Yang Liu, Ximeng Liu, Brij B. Gupta, Jianfeng Ma*

- `2005.05304v1` - [abs](http://arxiv.org/abs/2005.05304v1) - [pdf](http://arxiv.org/pdf/2005.05304v1)

> The application of federated extreme gradient boosting to mobile crowdsensing apps brings several benefits, in particular high performance on efficiency and classification. However, it also brings a new challenge for data and model privacy protection. Besides it being vulnerable to Generative Adversarial Network (GAN) based user data reconstruction attack, there is not the existing architecture that considers how to preserve model privacy. In this paper, we propose a secret sharing based federated learning architecture FedXGB to achieve the privacy-preserving extreme gradient boosting for mobile crowdsensing. Specifically, we first build a secure classification and regression tree (CART) of XGBoost using secret sharing. Then, we propose a secure prediction protocol to protect the model privacy of XGBoost in mobile crowdsensing. We conduct a comprehensive theoretical analysis and extensive experiments to evaluate the security, effectiveness, and efficiency of FedXGB. The results indicate that FedXGB is secure against the honest-but-curious adversaries and attains less than 1% accuracy loss compared with the original XGBoost model.

</details>

<details>

<summary>2020-05-09 20:09:46 - Lattice-based public key encryption with equality test supporting flexible authorization in standard model</summary>

- *Dung Hoang Duong, Kazuhide Fukushima, Shinsaku Kiyomoto, Partha Sarathi Roy, Arnaud Sipasseuth, Willy Susilo*

- `2005.05308v1` - [abs](http://arxiv.org/abs/2005.05308v1) - [pdf](http://arxiv.org/pdf/2005.05308v1)

> Public key encryption with equality test (PKEET) supports to check whether two ciphertexts encrypted under different public keys contain the same message or not. PKEET has many interesting applications such as keyword search on encrypted data, encrypted data partitioning for efficient encrypted data management, personal health record systems, spam filtering in encrypted email systems and so on. However, the PKEET scheme lacks an authorization mechanism for a user to control the comparison of its ciphertexts with others. In 2015, Ma et al. introduce the notion of PKEET with flexible authorization (PKEET-FA) which strengthens privacy protection. Since 2015, there are several follow-up works on PKEET-FA. But, all are secure in the random-oracle model. Moreover, all are vulnerable to quantum attacks. In this paper, we provide three constructions of quantum-safe PKEET-FA secure in the standard model. Proposed constructions are secure based on the hardness assumptions of integer lattices and ideal lattices. Finally, we implement the PKEET-FA scheme over ideal lattices.

</details>

<details>

<summary>2020-05-09 20:52:18 - Intelligent GPS Spoofing Attack Detection in Power Grids</summary>

- *Mohammad Sabouri, Sara Siamak, Maryam Dehghani, Mohsen Mohammadi, Mohammad Hassan Asemani*

- `2005.04513v1` - [abs](http://arxiv.org/abs/2005.04513v1) - [pdf](http://arxiv.org/pdf/2005.04513v1)

> The GPS is vulnerable to GPS spoofing attack (GSA), which leads to disorder in time and position results of the GPS receiver. In power grids, phasor measurement units (PMUs) use GPS to build time-tagged measurements, so they are susceptible to this attack. As a result of this attack, sampling time and phase angle of the PMU measurements change. In this paper, a neural network GPS spoofing detection (NNGSD) with employing PMU data from the dynamic power system is presented to detect GSAs. Numerical results in different conditions show the real-time performance of the proposed detection method.

</details>

<details>

<summary>2020-05-10 03:45:19 - Class-Aware Domain Adaptation for Improving Adversarial Robustness</summary>

- *Xianxu Hou, Jingxin Liu, Bolei Xu, Xiaolong Wang, Bozhi Liu, Guoping Qiu*

- `2005.04564v1` - [abs](http://arxiv.org/abs/2005.04564v1) - [pdf](http://arxiv.org/pdf/2005.04564v1)

> Recent works have demonstrated convolutional neural networks are vulnerable to adversarial examples, i.e., inputs to machine learning models that an attacker has intentionally designed to cause the models to make a mistake. To improve the adversarial robustness of neural networks, adversarial training has been proposed to train networks by injecting adversarial examples into the training data. However, adversarial training could overfit to a specific type of adversarial attack and also lead to standard accuracy drop on clean images. To this end, we propose a novel Class-Aware Domain Adaptation (CADA) method for adversarial defense without directly applying adversarial training. Specifically, we propose to learn domain-invariant features for adversarial examples and clean images via a domain discriminator. Furthermore, we introduce a class-aware component into the discriminator to increase the discriminative power of the network for adversarial examples. We evaluate our newly proposed approach using multiple benchmark datasets. The results demonstrate that our method can significantly improve the state-of-the-art of adversarial robustness for various attacks and maintain high performances on clean images.

</details>

<details>

<summary>2020-05-10 04:11:19 - BlockRoam: Blockchain-based Roaming Management System for Future Mobile Networks</summary>

- *Cong T. Nguyen, Diep N. Nguyen, Dinh Thai Hoang, Hoang-Anh Pham, Nguyen Huynh Tuong, Yong Xiao, Eryk Dutkiewicz*

- `2005.04571v1` - [abs](http://arxiv.org/abs/2005.04571v1) - [pdf](http://arxiv.org/pdf/2005.04571v1)

> Mobile service providers (MSPs) are particularly vulnerable to roaming frauds, especially ones that exploit the long delay in the data exchange process of the contemporary roaming management systems, causing multi-billion dollars loss each year. In this paper, we introduce BlockRoam, a novel blockchain-based roaming management system that provides an efficient data exchange platform among MSPs and mobile subscribers. Utilizing the Proof-of-Stake (PoS) consensus mechanism and smart contracts, BlockRoam can significantly shorten the information exchanging delay, thereby addressing the roaming fraud problems. Through intensive analysis, we show that the security and performance of such PoS-based blockchain network can be further enhanced by incentivizing more users (e.g., subscribers) to participate in the network. Moreover, users in such networks often join stake pools (e.g., formed by MSPs) to increase their profits. Therefore, we develop an economic model based on Stackelberg game to jointly maximize the profits of the network users and the stake pool, thereby encouraging user participation. We also propose an effective method to guarantee the uniqueness of this game's equilibrium. The performance evaluations show that the proposed economic model helps the MSPs to earn additional profits, attracts more investment to the blockchain network, and enhances the network's security and performance.

</details>

<details>

<summary>2020-05-10 17:31:29 - Sensor-based Continuous Authentication of Smartphones' Users Using Behavioral Biometrics: A Contemporary Survey</summary>

- *Mohammed Abuhamad, Ahmed Abusnaina, DaeHun Nyang, David Mohaisen*

- `2001.08578v2` - [abs](http://arxiv.org/abs/2001.08578v2) - [pdf](http://arxiv.org/pdf/2001.08578v2)

> Mobile devices and technologies have become increasingly popular, offering comparable storage and computational capabilities to desktop computers allowing users to store and interact with sensitive and private information. The security and protection of such personal information are becoming more and more important since mobile devices are vulnerable to unauthorized access or theft. User authentication is a task of paramount importance that grants access to legitimate users at the point-of-entry and continuously through the usage session. This task is made possible with today's smartphones' embedded sensors that enable continuous and implicit user authentication by capturing behavioral biometrics and traits. In this paper, we survey more than 140 recent behavioral biometric-based approaches for continuous user authentication, including motion-based methods (28 studies), gait-based methods (19 studies), keystroke dynamics-based methods (20 studies), touch gesture-based methods (29 studies), voice-based methods (16 studies), and multimodal-based methods (34 studies). The survey provides an overview of the current state-of-the-art approaches for continuous user authentication using behavioral biometrics captured by smartphones' embedded sensors, including insights and open challenges for adoption, usability, and performance.

</details>

<details>

<summary>2020-05-11 03:27:08 - Ethics in the digital era</summary>

- *David Pastor-Escuredo*

- `2003.06530v3` - [abs](http://arxiv.org/abs/2003.06530v3) - [pdf](http://arxiv.org/pdf/2003.06530v3)

> Ethics is an ancient matter for human kind, from the origin of civilizations ethics have been related with the most relevant human concerns and determined cultures. Ethics was initially related to religion, politics and philosophy to then be fragmented into specific communities of practice. The undergoing digital revolution enabled by Artificial Intelligence and Data are bringing ethical wicked problems in the social application of these technologies. However, a broader perspective is also necessary. We now face global and highly dynamics challenges that affect groups and individuals, specially those that are most vulnerable. Individual-oriented ethics are no longer sufficient, the new ethic has to consider the several scales in which the current complex society is organized and the interconnections between different systems. Ethics should also give a response to the systemic changes in behavior produced by external factors and threats. Furthermore, AI and digital technologies are global and make us more connected and smart but also more homogeneous, predictable and ultimately controllable. Ethic must take a stand to preserve and keep promoting individuals rights and uniqueness and cultural heterogeneity. Digital technologies have to the foundation for new models of society and help ensure ethical individual and collective values. For these reasons science has to be at the core of the new ethic as it helps understand the complex world. Finally, AI has advanced through the ambition to humanize matter, so we should expect ethics to give a response to the future status of machines and their interactions with humans.

</details>

<details>

<summary>2020-05-13 09:40:11 - eThor: Practical and Provably Sound Static Analysis of Ethereum Smart Contracts</summary>

- *Clara Schneidewind, Ilya Grishchenko, Markus Scherer, Matteo Maffei*

- `2005.06227v1` - [abs](http://arxiv.org/abs/2005.06227v1) - [pdf](http://arxiv.org/pdf/2005.06227v1)

> Ethereum has emerged as the most popular smart contract development platform, with hundreds of thousands of contracts stored on the blockchain and covering a variety of application scenarios, such as auctions, trading platforms, and so on. Given their financial nature, security vulnerabilities may lead to catastrophic consequences and, even worse, they can be hardly fixed as data stored on the blockchain, including the smart contract code itself, are immutable. An automated security analysis of these contracts is thus of utmost interest, but at the same time technically challenging for a variety of reasons, such as the specific transaction-oriented programming mechanisms, which feature a subtle semantics, and the fact that the blockchain data which the contract under analysis interacts with, including the code of callers and callees, are not statically known.   In this work, we present eThor, the first sound and automated static analyzer for EVM bytecode, which is based on an abstraction of the EVM bytecode semantics based on Horn clauses. In particular, our static analysis supports reachability properties, which we show to be sufficient for capturing interesting security properties for smart contracts (e.g., single-entrancy) as well as contract-specific functional properties. Our analysis is proven sound against a complete semantics of EVM bytecode and an experimental large-scale evaluation on real-world contracts demonstrates that eThor is practical and outperforms the state-of-the-art static analyzers: specifically, eThor is the only one to provide soundness guarantees, terminates on 95% of a representative set of real-world contracts, and achieves an F-measure (which combines sensitivity and specificity) of 89%.

</details>

<details>

<summary>2020-05-13 14:52:03 - Cyclic Bayesian Attack Graphs: A Systematic Computational Approach</summary>

- *Isaac Matthews, John Mace, Sadegh Soudjani, Aad van Moorsel*

- `2005.06350v1` - [abs](http://arxiv.org/abs/2005.06350v1) - [pdf](http://arxiv.org/pdf/2005.06350v1)

> Attack graphs are commonly used to analyse the security of medium-sized to large networks. Based on a scan of the network and likelihood information of vulnerabilities, attack graphs can be transformed into Bayesian Attack Graphs (BAGs). These BAGs are used to evaluate how security controls affect a network and how changes in topology affect security. A challenge with these automatically generated BAGs is that cycles arise naturally, which make it impossible to use Bayesian network theory to calculate state probabilities. In this paper we provide a systematic approach to analyse and perform computations over cyclic Bayesian attack graphs. %thus providing a generic approach to handle cycles as well as unifying the theory of Bayesian attack graphs. Our approach first formally introduces two commonly used versions of Bayesian attack graphs and compares their expressiveness. We then present an interpretation of Bayesian attack graphs based on combinational logic circuits, which facilitates an intuitively attractive systematic treatment of cycles. We prove properties of the associated logic circuit and present an algorithm that computes state probabilities without altering the attack graphs (e.g., remove an arc to remove a cycle). Moreover, our algorithm deals seamlessly with all cycles without the need to identify their types. A set of experiments using synthetically created networks demonstrates the scalability of the algorithm on computer networks with hundreds of machines, each with multiple vulnerabilities.

</details>

<details>

<summary>2020-05-13 15:20:15 - COVID-19 Contact-tracing Apps: a Survey on the Global Deployment and Challenges</summary>

- *Jinfeng Li, Xinyi Guo*

- `2005.03599v2` - [abs](http://arxiv.org/abs/2005.03599v2) - [pdf](http://arxiv.org/pdf/2005.03599v2)

> To address the massive spike in uncertainties triggered by the coronavirus disease (COVID-19), there is an ever-increasing number of national governments that are rolling out contact-tracing Apps to aid the containment of the virus. The first hugely contentious issue facing the Apps is the deployment framework, i.e. centralized or decentralized. Based on this, the debate branches out to the corresponding technologies that underpin these architectures, i.e. GPS, QR codes, and Bluetooth. This work conducts a pioneering review of the above scenarios and contributes a geolocation mapping of the current deployment. The Apps vulnerabilities and the directions of research are identified, with a special focus on the Bluetooth-inspired decentralized paradigm.

</details>

<details>

<summary>2020-05-14 04:19:34 - Prive-HD: Privacy-Preserved Hyperdimensional Computing</summary>

- *Behnam Khaleghi, Mohsen Imani, Tajana Rosing*

- `2005.06716v1` - [abs](http://arxiv.org/abs/2005.06716v1) - [pdf](http://arxiv.org/pdf/2005.06716v1)

> The privacy of data is a major challenge in machine learning as a trained model may expose sensitive information of the enclosed dataset. Besides, the limited computation capability and capacity of edge devices have made cloud-hosted inference inevitable. Sending private information to remote servers makes the privacy of inference also vulnerable because of susceptible communication channels or even untrustworthy hosts. In this paper, we target privacy-preserving training and inference of brain-inspired Hyperdimensional (HD) computing, a new learning algorithm that is gaining traction due to its light-weight computation and robustness particularly appealing for edge devices with tight constraints. Indeed, despite its promising attributes, HD computing has virtually no privacy due to its reversible computation. We present an accuracy-privacy trade-off method through meticulous quantization and pruning of hypervectors, the building blocks of HD, to realize a differentially private model as well as to obfuscate the information sent for cloud-hosted inference. Finally, we show how the proposed techniques can be also leveraged for efficient hardware implementation.

</details>

<details>

<summary>2020-05-14 14:00:56 - DjangoChecker: Applying Extended Taint Tracking and Server Side Parsing for Detection of Context-Sensitive XSS Flaws</summary>

- *Antonín Steinhauser, Petr Tůma*

- `2005.06990v1` - [abs](http://arxiv.org/abs/2005.06990v1) - [pdf](http://arxiv.org/pdf/2005.06990v1)

> Cross-site scripting (XSS) flaws are a class of security flaws that permit the injection of malicious code into a web application. In simple situations, these flaws can be caused by missing input sanitizations. Sometimes, however, all application inputs are sanitized, but the sanitizations are not appropriate for the browser contexts of the sanitized values. Using an incorrect sanitizer can make the application look protected, when it is in fact vulnerable as if no sanitization was used, creating a context-sensitive XSS flaw.   To discover context-sensitive XSS flaws, we introduce DjangoChecker. DjangoChecker combines extended dynamic taint tracking with a model browser for context analysis. We demonstrate the practical application of DjangoChecker on eight mature web applications based on Django, discovering previously unknown flaws in seven of the eight applications, including highly severe flaws that allow arbitrary JavaScript execution in the seven flawed applications.

</details>

<details>

<summary>2020-05-14 20:02:06 - Verification of Deep Convolutional Neural Networks Using ImageStars</summary>

- *Hoang-Dung Tran, Stanley Bak, Weiming Xiang, Taylor T. Johnson*

- `2004.05511v2` - [abs](http://arxiv.org/abs/2004.05511v2) - [pdf](http://arxiv.org/pdf/2004.05511v2)

> Convolutional Neural Networks (CNN) have redefined the state-of-the-art in many real-world applications, such as facial recognition, image classification, human pose estimation, and semantic segmentation. Despite their success, CNNs are vulnerable to adversarial attacks, where slight changes to their inputs may lead to sharp changes in their output in even well-trained networks. Set-based analysis methods can detect or prove the absence of bounded adversarial attacks, which can then be used to evaluate the effectiveness of neural network training methodology. Unfortunately, existing verification approaches have limited scalability in terms of the size of networks that can be analyzed.   In this paper, we describe a set-based framework that successfully deals with real-world CNNs, such as VGG16 and VGG19, that have high accuracy on ImageNet. Our approach is based on a new set representation called the ImageStar, which enables efficient exact and over-approximative analysis of CNNs. ImageStars perform efficient set-based analysis by combining operations on concrete images with linear programming (LP). Our approach is implemented in a tool called NNV, and can verify the robustness of VGG networks with respect to a small set of input states, derived from adversarial attacks, such as the DeepFool attack. The experimental results show that our approach is less conservative and faster than existing zonotope methods, such as those used in DeepZ, and the polytope method used in DeepPoly.

</details>

<details>

<summary>2020-05-14 20:50:09 - MagicPairing: Apple's Take on Securing Bluetooth Peripherals</summary>

- *Dennis Heinze, Jiska Classen, Felix Rohrbach*

- `2005.07255v1` - [abs](http://arxiv.org/abs/2005.07255v1) - [pdf](http://arxiv.org/pdf/2005.07255v1)

> Device pairing in large Internet of Things (IoT) deployments is a challenge for device manufacturers and users. Bluetooth offers a comparably smooth trust on first use pairing experience. Bluetooth, though, is well-known for security flaws in the pairing process. In this paper, we analyze how Apple improves the security of Bluetooth pairing while still maintaining its usability and specification compliance. The proprietary protocol that resides on top of Bluetooth is called MagicPairing. It enables the user to pair a device once with Apple's ecosystem and then seamlessly use it with all their other Apple devices. We analyze both, the security properties provided by this protocol, as well as its implementations. In general, MagicPairing could be adapted by other IoT vendors to improve Bluetooth security. Even though the overall protocol is well-designed, we identified multiple vulnerabilities within Apple's implementations with over-the-air and in-process fuzzing.

</details>

<details>

<summary>2020-05-15 05:10:07 - A Survey on Security and Privacy Issues in Modern Healthcare Systems: Attacks and Defenses</summary>

- *AKM Iqridar Newaz, Amit Kumar Sikder, Mohammad Ashiqur Rahman, A. Selcuk Uluagac*

- `2005.07359v1` - [abs](http://arxiv.org/abs/2005.07359v1) - [pdf](http://arxiv.org/pdf/2005.07359v1)

> The recent advancements in computing systems and wireless communications have made healthcare systems more efficient than before. Modern healthcare devices can monitor and manage different health conditions of the patients automatically without any manual intervention from medical professionals. Additionally, the use of implantable medical devices (IMDs), body area networks (BANs), and Internet of Things (IoT) technologies in healthcare systems improve the overall patient monitoring and treatment process. However, these systems are complex in software and hardware, and optimizing between security, privacy, and treatment is crucial for healthcare systems as any security or privacy violation can lead to severe effects on patients' treatments and overall health conditions. Indeed, the healthcare domain is increasingly facing security challenges and threats due to numerous design flaws and the lack of proper security measures in healthcare devices and applications. In this paper, we explore various security and privacy threats to healthcare systems and discuss the consequences of these threats. We present a detailed survey of different potential attacks and discuss their impacts. Furthermore, we review the existing security measures proposed for healthcare systems and discuss their limitations. Finally, we conclude the paper with future research directions toward securing healthcare systems against common vulnerabilities.

</details>

<details>

<summary>2020-05-15 13:45:12 - A Deep Learning-based Fine-grained Hierarchical Learning Approach for Robust Malware Classification</summary>

- *Ahmed Abusnaina, Mohammed Abuhamad, Hisham Alasmary, Afsah Anwar, Rhongho Jang, Saeed Salem, DaeHun Nyang, David Mohaisen*

- `2005.07145v2` - [abs](http://arxiv.org/abs/2005.07145v2) - [pdf](http://arxiv.org/pdf/2005.07145v2)

> The wide acceptance of Internet of Things (IoT) for both household and industrial applications is accompanied by several security concerns. A major security concern is their probable abuse by adversaries towards their malicious intent. Understanding and analyzing IoT malicious behaviors is crucial, especially with their rapid growth and adoption in wide-range of applications. However, recent studies have shown that machine learning-based approaches are susceptible to adversarial attacks by adding junk codes to the binaries, for example, with an intention to fool those machine learning or deep learning-based detection systems. Realizing the importance of addressing this challenge, this study proposes a malware detection system that is robust to adversarial attacks. To do so, examine the performance of the state-of-the-art methods against adversarial IoT software crafted using the graph embedding and augmentation techniques. In particular, we study the robustness of such methods against two black-box adversarial methods, GEA and SGEA, to generate Adversarial Examples (AEs) with reduced overhead, and keeping their practicality intact. Our comprehensive experimentation with GEA-based AEs show the relation between misclassification and the graph size of the injected sample. Upon optimization and with small perturbation, by use of SGEA, all the IoT malware samples are misclassified as benign. This highlights the vulnerability of current detection systems under adversarial settings. With the landscape of possible adversarial attacks, we then propose DL-FHMC, a fine-grained hierarchical learning approach for malware detection and classification, that is robust to AEs with a capability to detect 88.52% of the malicious AEs.

</details>

<details>

<summary>2020-05-15 17:14:45 - Cyberattack on the Microgrids Through Price Modification</summary>

- *Subhankar Mishra*

- `2005.08757v1` - [abs](http://arxiv.org/abs/2005.08757v1) - [pdf](http://arxiv.org/pdf/2005.08757v1)

> Recent massive failures in the power grid acted as a wake up call for all utilities and consumers. This leads to aggressive pursue a more intelligent grid which addresses the concerns of reliability, efficiency, security, quality and sustainability for the energy consumers and producers alike. One of the many features of the smart grid is a discrete energy system consisting of distributed energy sources capable of operating independently from the main grid known as the microgrid. The main focus of the microgrid is to ensure a reliable and affordable energy security. However, it also can be vulnerable to cyber attack and we study the effect of price modification of electricity attack on the microgrid, given that they are able to operate independently from the main grid. This attack consists of two stages, 1) Separate the microgrids from the main grid (islanding) and 2) Failing the nodes inside the microgrid. Empirical results on IEEE Bus data help us evaluate our approach under various settings of grid parameters.

</details>

<details>

<summary>2020-05-16 20:39:13 - BlurNet: Defense by Filtering the Feature Maps</summary>

- *Ravi Raju, Mikko Lipasti*

- `1908.02256v2` - [abs](http://arxiv.org/abs/1908.02256v2) - [pdf](http://arxiv.org/pdf/1908.02256v2)

> Recently, the field of adversarial machine learning has been garnering attention by showing that state-of-the-art deep neural networks are vulnerable to adversarial examples, stemming from small perturbations being added to the input image. Adversarial examples are generated by a malicious adversary by obtaining access to the model parameters, such as gradient information, to alter the input or by attacking a substitute model and transferring those malicious examples over to attack the victim model. Specifically, one of these attack algorithms, Robust Physical Perturbations ($RP_2$), generates adversarial images of stop signs with black and white stickers to achieve high targeted misclassification rates against standard-architecture traffic sign classifiers. In this paper, we propose BlurNet, a defense against the $RP_2$ attack. First, we motivate the defense with a frequency analysis of the first layer feature maps of the network on the LISA dataset, which shows that high frequency noise is introduced into the input image by the $RP_2$ algorithm. To remove the high frequency noise, we introduce a depthwise convolution layer of standard blur kernels after the first layer. We perform a blackbox transfer attack to show that low-pass filtering the feature maps is more beneficial than filtering the input. We then present various regularization schemes to incorporate this low-pass filtering behavior into the training regime of the network and perform white-box attacks. We conclude with an adaptive attack evaluation to show that the success rate of the attack drops from 90\% to 20\% with total variation regularization, one of the proposed defenses.

</details>

<details>

<summary>2020-05-17 10:25:51 - Lost and Found: Stopping Bluetooth Finders from Leaking Private Information</summary>

- *Mira Weller, Jiska Classen, Fabian Ullrich, Denis Waßmann, Erik Tews*

- `2005.08208v1` - [abs](http://arxiv.org/abs/2005.08208v1) - [pdf](http://arxiv.org/pdf/2005.08208v1)

> A Bluetooth finder is a small battery-powered device that can be attached to important items such as bags, keychains, or bikes. The finder maintains a Bluetooth connection with the user's phone, and the user is notified immediately on connection loss. We provide the first comprehensive security and privacy analysis of current commercial Bluetooth finders. Our analysis reveals several significant security vulnerabilities in those products concerning mobile applications and the corresponding backend services in the cloud. We also show that all analyzed cloud-based products leak more private data than required for their respective cloud services.   Overall, there is a big market for Bluetooth finders, but none of the existing products is privacy-friendly. We close this gap by designing and implementing PrivateFind, which ensures locations of the user are never leaked to third parties. It is designed to run on similar hardware as existing finders, allowing vendors to update their systems using PrivateFind.

</details>

<details>

<summary>2020-05-17 12:20:40 - Multi-Party Timed Commitments</summary>

- *Yael Doweck, Ittay Eyal*

- `2005.04883v2` - [abs](http://arxiv.org/abs/2005.04883v2) - [pdf](http://arxiv.org/pdf/2005.04883v2)

> The problem of obtaining secret commitments from multiple parties and revealing them after a certain time is useful for sealed-bid auctions, games, and other applications. Existing solutions, dating back to Rivest, Shamir and Wagner, either do not scale or rely on synchrony for the commitment phase and trust of $t/n$ parties. We formalize the problem of implementing such commitments with a probabilistic delay and without the aforementioned assumptions as Multi-Party Timed Commitments (MPTC) and present a solution -- the Time-Capsule protocol. Like previous approaches, Time Capsule forms a puzzle whose solution reveals the committed values. But unlike previous solutions, no party has an advantage in solving the puzzle, and individual commitments cannot be revealed before the entire set is committed. A particular application of MPTC realizes an advancement in the study of decentralized systems. The state of the art in decentralized systems is manifested in blockchain systems that utilize Proof of Work to achieve censorship resistance. However, they are still vulnerable to frontrunning, an issue that is plaguing operational systems. By adapting Time Capsule, we allow it to be used for Proof of Work, preventing frontrunning by system operators and tuning the puzzle difficulty using the blockchain mechanism.

</details>

<details>

<summary>2020-05-18 14:16:48 - The Challenges and Impact of Privacy Policy Comprehension</summary>

- *Jana Korunovska, Bernadette Kamleitner, Sarah Spiekermann*

- `2005.08967v1` - [abs](http://arxiv.org/abs/2005.08967v1) - [pdf](http://arxiv.org/pdf/2005.08967v1)

> The new information and communication technology providers collect increasing amounts of personal data, a lot of which is user generated. Unless use policies are privacy-friendly, this leaves users vulnerable to privacy risks such as exposure through public data visibility or intrusive commercialisation of their data through secondary data use. Due to complex privacy policies, many users of online services unwillingly agree to privacy-intruding practices. To give users more control over their privacy, scholars and regulators have pushed for short, simple, and prominent privacy policies. The premise has been that users will see and comprehend such policies, and then rationally adjust their disclosure behaviour. In this paper, on a use case of social network service site, we show that this premise does not hold. We invited 214 regular Facebook users to join a new fictitious social network. We experimentally manipulated the privacy-friendliness of an unavoidable and simple privacy policy. Half of our participants miscomprehended even this transparent privacy policy. When privacy threats of secondary data use were present, users remembered the policies as more privacy-friendly than they actually were and unwittingly uploaded more data. To mitigate such behavioural pitfalls we present design recommendations to improve the quality of informed consent.

</details>

<details>

<summary>2020-05-18 21:33:12 - Is Spiking Secure? A Comparative Study on the Security Vulnerabilities of Spiking and Deep Neural Networks</summary>

- *Alberto Marchisio, Giorgio Nanfa, Faiq Khalid, Muhammad Abdullah Hanif, Maurizio Martina, Muhammad Shafique*

- `1902.01147v2` - [abs](http://arxiv.org/abs/1902.01147v2) - [pdf](http://arxiv.org/pdf/1902.01147v2)

> Spiking Neural Networks (SNNs) claim to present many advantages in terms of biological plausibility and energy efficiency compared to standard Deep Neural Networks (DNNs). Recent works have shown that DNNs are vulnerable to adversarial attacks, i.e., small perturbations added to the input data can lead to targeted or random misclassifications. In this paper, we aim at investigating the key research question: ``Are SNNs secure?'' Towards this, we perform a comparative study of the security vulnerabilities in SNNs and DNNs w.r.t. the adversarial noise. Afterwards, we propose a novel black-box attack methodology, i.e., without the knowledge of the internal structure of the SNN, which employs a greedy heuristic to automatically generate imperceptible and robust adversarial examples (i.e., attack images) for the given SNN. We perform an in-depth evaluation for a Spiking Deep Belief Network (SDBN) and a DNN having the same number of layers and neurons (to obtain a fair comparison), in order to study the efficiency of our methodology and to understand the differences between SNNs and DNNs w.r.t. the adversarial examples. Our work opens new avenues of research towards the robustness of the SNNs, considering their similarities to the human brain's functionality.

</details>

<details>

<summary>2020-05-18 22:23:24 - Don't Mine, Wait in Line: Fair and Efficient Blockchain Consensus with Robust Round Robin</summary>

- *Mansoor Ahmed-Rengers, Kari Kostiainen*

- `1804.07391v3` - [abs](http://arxiv.org/abs/1804.07391v3) - [pdf](http://arxiv.org/pdf/1804.07391v3)

> Proof-of-Stake systems randomly choose, on each round, one of the participants as a consensus leader that extends the chain with the next block such that the selection probability is proportional to the owned stake. However, distributed random number generation is notoriously difficult. Systems that derive randomness from the previous blocks are completely insecure; solutions that provide secure random selection are inefficient due to their high communication complexity; and approaches that balance security and performance exhibit selection bias. When block creation is rewarded with new stake, even a minor bias can have a severe cumulative effect.   In this paper, we propose Robust Round Robin, a new consensus scheme that addresses this selection problem. We create reliable long-term identities by bootstrapping from an existing infrastructure, such as Intel's SGX processors, or by mining them starting from an initial fair distribution. For leader selection we use a deterministic approach. On each round, we select a set of the previously created identities as consensus leader candidates in round robin manner. Because simple round-robin alone is vulnerable to attacks and offers poor liveness, we complement such deterministic selection policy with a lightweight endorsement mechanism that is an interactive protocol between the leader candidates and a small subset of other system participants. Our solution has low good efficiency as it requires no expensive distributed randomness generation and it provides block creation fairness which is crucial in deployments that reward it with new stake.

</details>

<details>

<summary>2020-05-19 02:24:14 - On Intrinsic Dataset Properties for Adversarial Machine Learning</summary>

- *Jeffrey Z. Pan, Nicholas Zufelt*

- `2005.09170v1` - [abs](http://arxiv.org/abs/2005.09170v1) - [pdf](http://arxiv.org/pdf/2005.09170v1)

> Deep neural networks (DNNs) have played a key role in a wide range of machine learning applications. However, DNN classifiers are vulnerable to human-imperceptible adversarial perturbations, which can cause them to misclassify inputs with high confidence. Thus, creating robust DNNs which can defend against malicious examples is critical in applications where security plays a major role. In this paper, we study the effect of intrinsic dataset properties on the performance of adversarial attack and defense methods, testing on five popular image classification datasets - MNIST, Fashion-MNIST, CIFAR10/CIFAR100, and ImageNet. We find that input size and image contrast play key roles in attack and defense success. Our discoveries highlight that dataset design and data preprocessing steps are important to boost the adversarial robustness of DNNs. To our best knowledge, this is the first comprehensive work that studies the effect of intrinsic dataset properties on adversarial machine learning.

</details>

<details>

<summary>2020-05-19 03:52:21 - FrameProv: Towards End-To-End Video Provenance</summary>

- *Mansoor Ahmed-Rengers*

- `2005.09199v1` - [abs](http://arxiv.org/abs/2005.09199v1) - [pdf](http://arxiv.org/pdf/2005.09199v1)

> Video feeds are often deliberately used as evidence, as in the case of CCTV footage; but more often than not, the existence of footage of a supposed event is perceived as proof of fact in the eyes of the public at large. This reliance represents a societal vulnerability given the existence of easy-to-use editing tools and means to fabricate entire video feeds using machine learning. And, as the recent barrage of fake news and fake porn videos have shown, this isn't merely an academic concern, it is actively been exploited. I posit that this exploitation is only going to get more insidious. In this position paper, I introduce a long term project that aims to mitigate some of the most egregious forms of manipulation by embedding trustworthy components in the video transmission chain. Unlike earlier works, I am not aiming to do tamper detection or other forms of forensics -- approaches I think are bound to fail in the face of the reality of necessary editing and compression -- instead, the aim here is to provide a way for the video publisher to prove the integrity of the video feed as well as make explicit any edits they may have performed. To do this, I present a novel data structure, a video-edit specification language and supporting infrastructure that provides end-to-end video provenance, from the camera sensor to the viewer. I have implemented a prototype of this system and am in talks with journalists and video editors to discuss the best ways forward with introducing this idea to the mainstream.

</details>

<details>

<summary>2020-05-19 04:19:48 - A Lightweight Isolation Mechanism for Secure Branch Predictors</summary>

- *Lutan Zhao, Peinan Li, Rui Hou, Michael C. Huang, Jiazhen Li, Lixin Zhang, Xuehai Qian, Dan Meng*

- `2005.08183v2` - [abs](http://arxiv.org/abs/2005.08183v2) - [pdf](http://arxiv.org/pdf/2005.08183v2)

> Recently exposed vulnerabilities reveal the necessity to improve the security of branch predictors. Branch predictors record history about the execution of different programs, and such information from different processes are stored in the same structure and thus accessible to each other. This leaves the attackers with the opportunities for malicious training and malicious perception. Instead of flush-based or physical isolation of hardware resources, we want to achieve isolation of the content in these hardware tables with some lightweight processing using randomization as follows. (1) Content encoding. We propose to use hardware-based thread-private random numbers to encode the contents of the branch predictor tables (both direction and destination histories) which we call XOR-BP. Specifically, the data is encoded by XOR operation with the key before written in the table and decoded after read from the table. Such a mechanism obfuscates the information adding difficulties to cross-process or cross-privilege level analysis and perception. It achieves a similar effect of logical isolation but adds little in terms of space or time overheads. (2) Index encoding. We propose a randomized index mechanism of the branch predictor (Noisy-XOR-BP). Similar to the XOR-BP, another thread-private random number is used together with the branch instruction address as the input to compute the index of the branch predictor. This randomized indexing mechanism disrupts the correspondence between the branch instruction address and the branch predictor entry, thus increases the noise for malicious perception attacks. Our analyses using an FPGA-based RISC-V processor prototype and additional auxiliary simulations suggest that the proposed mechanisms incur a very small performance cost while providing strong protection.

</details>

<details>

<summary>2020-05-19 15:49:23 - Backstabber's Knife Collection: A Review of Open Source Software Supply Chain Attacks</summary>

- *Marc Ohm, Henrik Plate, Arnold Sykosch, Michael Meier*

- `2005.09535v1` - [abs](http://arxiv.org/abs/2005.09535v1) - [pdf](http://arxiv.org/pdf/2005.09535v1)

> A software supply chain attack is characterized by the injection of malicious code into a software package in order to compromise dependent systems further down the chain. Recent years saw a number of supply chain attacks that leverage the increasing use of open source during software development, which is facilitated by dependency managers that automatically resolve, download and install hundreds of open source packages throughout the software life cycle. This paper presents a dataset of 174 malicious software packages that were used in real-world attacks on open source software supply chains, and which were distributed via the popular package repositories npm, PyPI, and RubyGems. Those packages, dating from November 2015 to November 2019, were manually collected and analyzed. The paper also presents two general attack trees to provide a structured overview about techniques to inject malicious code into the dependency tree of downstream users, and to execute such code at different times and under different conditions. This work is meant to facilitate the future development of preventive and detective safeguards by open source and research communities.

</details>

<details>

<summary>2020-05-20 13:52:13 - Smart Contract Repair</summary>

- *Xiao Liang Yu, Omar Al-Bataineh, David Lo, Abhik Roychoudhury*

- `1912.05823v3` - [abs](http://arxiv.org/abs/1912.05823v3) - [pdf](http://arxiv.org/pdf/1912.05823v3)

> Smart contracts are automated or self-enforcing contracts that can be used to exchange assets without having to place trust in third parties. Many commercial transactions use smart contracts due to their potential benefits in terms of secure peer-to-peer transactions independent of external parties. Experience shows that many commonly used smart contracts are vulnerable to serious malicious attacks which may enable attackers to steal valuable assets of involving parties. There is therefore a need to apply analysis and automated repair techniques to detect and repair bugs in smart contracts before being deployed. In this work, we present the first general-purpose automated smart contract repair approach that is also gas-aware. Our repair method is search-based and searches among mutations of the buggy contract. Our method also considers the gas usage of the candidate patches by leveraging our novel notion of gas dominance relationship. We have made our smart contract repair tool SCRepair available open-source, for investigation by the wider community.

</details>

<details>

<summary>2020-05-21 17:08:47 - Authentication and Key Management Automation in Decentralized Secure Email and Messaging via Low-Entropy Secrets</summary>

- *Itzel Vazquez Sandoval, Arash Atashpendar, Gabriele Lenzini*

- `2005.10787v1` - [abs](http://arxiv.org/abs/2005.10787v1) - [pdf](http://arxiv.org/pdf/2005.10787v1)

> We revisit the problem of entity authentication in decentralized end-to-end encrypted email and secure messaging to propose a practical and self-sustaining cryptographic solution based on password-authenticated key exchange (PAKE). This not only allows users to authenticate each other via shared low-entropy secrets, e.g., memorable words, without a public key infrastructure or a trusted third party, but it also paves the way for automation and a series of cryptographic enhancements; improves security by minimizing the impact of human error and potentially improves usability. First, we study a few vulnerabilities in voice-based out-of-band authentication, in particular a combinatorial attack against lazy users, which we analyze in the context of a secure email solution. Next, we propose solving the problem of secure equality test using PAKE to achieve entity authentication and to establish a shared high-entropy secret key. Our solution lends itself to offline settings, compatible with the inherently asynchronous nature of email and modern messaging systems. The suggested approach enables enhancements in key management such as automated key renewal and future key pair authentications, multi-device synchronization, secure secret storage and retrieval, and the possibility of post-quantum security as well as facilitating forward secrecy and deniability in a primarily symmetric-key setting. We also discuss the use of auditable PAKEs for mitigating a class of online guess and abort attacks in authentication protocols.

</details>

<details>

<summary>2020-05-22 08:54:41 - Vulnerability of deep neural networks for detecting COVID-19 cases from chest X-ray images to universal adversarial attacks</summary>

- *Hokuto Hirano, Kazuki Koga, Kazuhiro Takemoto*

- `2005.11061v1` - [abs](http://arxiv.org/abs/2005.11061v1) - [pdf](http://arxiv.org/pdf/2005.11061v1)

> Under the epidemic of the novel coronavirus disease 2019 (COVID-19), chest X-ray computed tomography imaging is being used for effectively screening COVID-19 patients. The development of computer-aided systems based on deep neural networks (DNNs) has been advanced, to rapidly and accurately detect COVID-19 cases, because the need for expert radiologists, who are limited in number, forms a bottleneck for the screening. However, so far, the vulnerability of DNN-based systems has been poorly evaluated, although DNNs are vulnerable to a single perturbation, called universal adversarial perturbation (UAP), which can induce DNN failure in most classification tasks. Thus, we focus on representative DNN models for detecting COVID-19 cases from chest X-ray images and evaluate their vulnerability to UAPs generated using simple iterative algorithms. We consider nontargeted UAPs, which cause a task failure resulting in an input being assigned an incorrect label, and targeted UAPs, which cause the DNN to classify an input into a specific class. The results demonstrate that the models are vulnerable to nontargeted and targeted UAPs, even in case of small UAPs. In particular, 2% norm of the UPAs to the average norm of an image in the image dataset achieves >85% and >90% success rates for the nontargeted and targeted attacks, respectively. Due to the nontargeted UAPs, the DNN models judge most chest X-ray images as COVID-19 cases. The targeted UAPs make the DNN models classify most chest X-ray images into a given target class. The results indicate that careful consideration is required in practical applications of DNNs to COVID-19 diagnosis; in particular, they emphasize the need for strategies to address security concerns. As an example, we show that iterative fine-tuning of the DNN models using UAPs improves the robustness of the DNN models against UAPs.

</details>

<details>

<summary>2020-05-22 13:57:28 - Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics</summary>

- *Donald Martin Jr., Vinodkumar Prabhakaran, Jill Kuhlberg, Andrew Smart, William S. Isaac*

- `2005.07572v3` - [abs](http://arxiv.org/abs/2005.07572v3) - [pdf](http://arxiv.org/pdf/2005.07572v3)

> Recent research on algorithmic fairness has highlighted that the problem formulation phase of ML system development can be a key source of bias that has significant downstream impacts on ML system fairness outcomes. However, very little attention has been paid to methods for improving the fairness efficacy of this critical phase of ML system development. Current practice neither accounts for the dynamic complexity of high-stakes domains nor incorporates the perspectives of vulnerable stakeholders. In this paper we introduce community based system dynamics (CBSD) as an approach to enable the participation of typically excluded stakeholders in the problem formulation phase of the ML system development process and facilitate the deep problem understanding required to mitigate bias during this crucial stage.

</details>

<details>

<summary>2020-05-23 15:49:42 - ChirpOTLE: A Framework for Practical LoRaWAN Security Evaluation</summary>

- *Frank Hessel, Lars Almon, Flor Álvarez*

- `2005.11555v1` - [abs](http://arxiv.org/abs/2005.11555v1) - [pdf](http://arxiv.org/pdf/2005.11555v1)

> Low-power wide-area networks (LPWANs) are becoming an integral part of the Internet of Things. As a consequence, businesses, administration, and, subsequently, society itself depend on the reliability and availability of these communication networks. Released in 2015, LoRaWAN gained popularity and attracted the focus of security research, revealing a number of vulnerabilities. This lead to the revised LoRaWAN 1.1 specification in late 2017. Most of previous work focused on simulation and theoretical approaches. Interoperability and the variety of implementations complicate the risk assessment for a specific LoRaWAN network. In this paper, we address these issues by introducing ChirpOTLE, a LoRa and LoRaWAN security evaluation framework suitable for rapid iteration and testing of attacks in testbeds and assessing the security of real-world networks.We demonstrate the potential of our framework by verifying the applicability of a novel denial-of-service attack targeting the adaptive data rate mechanism in a testbed using common off-the-shelf hardware. Furthermore, we show the feasibility of the Class B beacon spoofing attack, which has not been demonstrated in practice before.

</details>

<details>

<summary>2020-05-23 16:19:47 - Adversarial Attack on Hierarchical Graph Pooling Neural Networks</summary>

- *Haoteng Tang, Guixiang Ma, Yurong Chen, Lei Guo, Wei Wang, Bo Zeng, Liang Zhan*

- `2005.11560v1` - [abs](http://arxiv.org/abs/2005.11560v1) - [pdf](http://arxiv.org/pdf/2005.11560v1)

> Recent years have witnessed the emergence and development of graph neural networks (GNNs), which have been shown as a powerful approach for graph representation learning in many tasks, such as node classification and graph classification. The research on the robustness of these models has also started to attract attentions in the machine learning field. However, most of the existing work in this area focus on the GNNs for node-level tasks, while little work has been done to study the robustness of the GNNs for the graph classification task. In this paper, we aim to explore the vulnerability of the Hierarchical Graph Pooling (HGP) Neural Networks, which are advanced GNNs that perform very well in the graph classification in terms of prediction accuracy. We propose an adversarial attack framework for this task. Specifically, we design a surrogate model that consists of convolutional and pooling operators to generate adversarial samples to fool the hierarchical GNN-based graph classification models. We set the preserved nodes by the pooling operator as our attack targets, and then we perturb the attack targets slightly to fool the pooling operator in hierarchical GNNs so that they will select the wrong nodes to preserve. We show the adversarial samples generated from multiple datasets by our surrogate model have enough transferability to attack current state-of-art graph classification models. Furthermore, we conduct the robust train on the target models and demonstrate that the retrained graph classification models are able to better defend against the attack from the adversarial samples. To the best of our knowledge, this is the first work on the adversarial attack against hierarchical GNN-based graph classification models.

</details>

<details>

<summary>2020-05-23 17:31:07 - Emotion-robust EEG Classification for Motor Imagery</summary>

- *Abdul Moeed*

- `2005.13523v1` - [abs](http://arxiv.org/abs/2005.13523v1) - [pdf](http://arxiv.org/pdf/2005.13523v1)

> Developments in Brain Computer Interfaces (BCIs) are empowering those with severe physical afflictions through their use in assistive systems. Common methods of achieving this is via Motor Imagery (MI), which maps brain signals to code for certain commands. Electroencephalogram (EEG) is preferred for recording brain signal data on account of it being non-invasive. Despite their potential utility, MI-BCI systems are yet confined to research labs. A major cause for this is lack of robustness of such systems. As hypothesized by two teams during Cybathlon 2016, a particular source of the system's vulnerability is the sharp change in the subject's state of emotional arousal. This work aims towards making MI-BCI systems resilient to such emotional perturbations. To do so, subjects are exposed to high and low arousal-inducing virtual reality (VR) environments before recording EEG data. The advent of COVID-19 compelled us to modify our methodology. Instead of training machine learning algorithms to classify emotional arousal, we opt for classifying subjects that serve as proxy for each state. Additionally, MI models are trained for each subject instead of each arousal state. As training subjects to use MI-BCI can be an arduous and time-consuming process, reducing this variability and increasing robustness can considerably accelerate the acceptance and adoption of assistive technologies powered by BCI.

</details>

<details>

<summary>2020-05-23 22:21:55 - How Effective are Smart Contract Analysis Tools? Evaluating Smart Contract Static Analysis Tools Using Bug Injection</summary>

- *Asem Ghaleb, Karthik Pattabiraman*

- `2005.11613v1` - [abs](http://arxiv.org/abs/2005.11613v1) - [pdf](http://arxiv.org/pdf/2005.11613v1)

> Security attacks targeting smart contracts have been on the rise, which have led to financial loss and erosion of trust. Therefore, it is important to enable developers to discover security vulnerabilities in smart contracts before deployment. A number of static analysis tools have been developed for finding security bugs in smart contracts. However, despite the numerous bug-finding tools, there is no systematic approach to evaluate the proposed tools and gauge their effectiveness. This paper proposes SolidiFI, an automated and systematic approach for evaluating smart contract static analysis tools. SolidiFI is based on injecting bugs (i.e., code defects) into all potential locations in a smart contract to introduce targeted security vulnerabilities. SolidiFI then checks the generated buggy contract using the static analysis tools, and identifies the bugs that the tools are unable to detect (false-negatives) along with identifying the bugs reported as false-positives. SolidiFI is used to evaluate six widely-used static analysis tools, namely, Oyente, Securify, Mythril, SmartCheck, Manticore and Slither, using a set of 50 contracts injected by 9369 distinct bugs. It finds several instances of bugs that are not detected by the evaluated tools despite their claims of being able to detect such bugs, and all the tools report many false positives

</details>

<details>

<summary>2020-05-24 00:03:27 - ShapeAdv: Generating Shape-Aware Adversarial 3D Point Clouds</summary>

- *Kibok Lee, Zhuoyuan Chen, Xinchen Yan, Raquel Urtasun, Ersin Yumer*

- `2005.11626v1` - [abs](http://arxiv.org/abs/2005.11626v1) - [pdf](http://arxiv.org/pdf/2005.11626v1)

> We introduce ShapeAdv, a novel framework to study shape-aware adversarial perturbations that reflect the underlying shape variations (e.g., geometric deformations and structural differences) in the 3D point cloud space. We develop shape-aware adversarial 3D point cloud attacks by leveraging the learned latent space of a point cloud auto-encoder where the adversarial noise is applied in the latent space. Specifically, we propose three different variants including an exemplar-based one by guiding the shape deformation with auxiliary data, such that the generated point cloud resembles the shape morphing between objects in the same category. Different from prior works, the resulting adversarial 3D point clouds reflect the shape variations in the 3D point cloud space while still being close to the original one. In addition, experimental evaluations on the ModelNet40 benchmark demonstrate that our adversaries are more difficult to defend with existing point cloud defense methods and exhibit a higher attack transferability across classifiers. Our shape-aware adversarial attacks are orthogonal to existing point cloud based attacks and shed light on the vulnerability of 3D deep neural networks.

</details>

<details>

<summary>2020-05-24 12:12:27 - DeepSQLi: Deep Semantic Learning for Testing SQL Injection</summary>

- *Muyang Liu, Ke Li, Tao Chen*

- `2005.11728v1` - [abs](http://arxiv.org/abs/2005.11728v1) - [pdf](http://arxiv.org/pdf/2005.11728v1)

> Security is unarguably the most serious concern for Web applications, to which SQL injection (SQLi) attack is one of the most devastating attacks. Automatically testing SQLi vulnerabilities is of ultimate importance, yet is unfortunately far from trivial to implement. This is because the existence of a huge, or potentially infinite, number of variants and semantic possibilities of SQL leading to SQLi attacks on various Web applications. In this paper, we propose a deep natural language processing based tool, dubbed DeepSQLi, to generate test cases for detecting SQLi vulnerabilities. Through adopting deep learning based neural language model and sequence of words prediction, DeepSQLi is equipped with the ability to learn the semantic knowledge embedded in SQLi attacks, allowing it to translate user inputs (or a test case) into a new test case, which is semantically related and potentially more sophisticated. Experiments are conducted to compare DeepSQLi with SQLmap, a state-of-the-art SQLi testing automation tool, on six real-world Web applications that are of different scales, characteristics and domains. Empirical results demonstrate the effectiveness and the remarkable superiority of DeepSQLi over SQLmap, such that more SQLi vulnerabilities can be identified by using a less number of test cases, whilst running much faster.

</details>

<details>

<summary>2020-05-25 01:44:16 - The never ending war in the stack and the reincarnation of ROP attacks</summary>

- *Ammari Nader, Joan Calvet, Jose M. Fernandez*

- `2005.11886v1` - [abs](http://arxiv.org/abs/2005.11886v1) - [pdf](http://arxiv.org/pdf/2005.11886v1)

> Return Oriented Programming (ROP) is a technique by which an attacker can induce arbitrary behavior inside a vulnerable program without injecting a malicious code. The continues failure of the currently deployed defenses against ROP has made it again one of the most powerful memory corruption attacks. ROP is also considered as one of the most flexible attacks, its level of flexibility, unlike other code reuse attacks, can reach the Turing completeness. Several efforts have been undertaken to study this threat and to propose better defense mechanisms (mitigation or prevention), yet the majority of them are not deeply reviewed nor officially implemented.Furthermore, similar studies show that the techniques proposed to prevent ROP-based exploits usually yield a high false-negative rate and a higher false-positive rate, not to mention the overhead that they introduce into the protected program. The first part of this research work aims at providing an in-depth analysis of the currently available anti-ROP solutions (deployed and proposed), focusing on inspecting their defense logic and summarizing their weaknesses and problems. The second part of this work aims at introducing our proposed Indicators Of Compromise (IOCs) that could be used to improve the detection rate of ROP attacks. The three suggested indicators could detect these attacks at run-time by checking the presence of some artifacts during the execution of the targeted program.

</details>

<details>

<summary>2020-05-27 00:22:48 - Identifying Vulnerabilities of Industrial Control Systems using Evolutionary Multiobjective Optimisation</summary>

- *Nilufer Tuptuk, Stephen Hailes*

- `2005.13095v1` - [abs](http://arxiv.org/abs/2005.13095v1) - [pdf](http://arxiv.org/pdf/2005.13095v1)

> In this paper we propose a novel methodology to assist in identifying vulnerabilities in a real-world complex heterogeneous industrial control systems (ICS) using two evolutionary multiobjective optimisation (EMO) algorithms, NSGA-II and SPEA2. Our approach is evaluated on a well known benchmark chemical plant simulator, the Tennessee Eastman (TE) process model. We identified vulnerabilities in individual components of the TE model and then made use of these to generate combinatorial attacks to damage the safety of the system, and to cause economic loss. Results were compared against random attacks, and the performance of the EMO algorithms were evaluated using hypervolume, spread and inverted generational distance (IGD) metrics. A defence against these attacks in the form of a novel intrusion detection system was developed, using a number of machine learning algorithms. Designed approach was further tested against the developed detection methods. Results demonstrate that EMO algorithms are a promising tool in the identification of the most vulnerable components of ICS, and weaknesses of any existing detection systems in place to protect the system. The proposed approach can be used by control and security engineers to design security aware control, and test the effectiveness of security mechanisms, both during design, and later during system operation.

</details>

<details>

<summary>2020-05-27 11:52:42 - Enhancing Resilience of Deep Learning Networks by Means of Transferable Adversaries</summary>

- *Moritz Seiler, Heike Trautmann, Pascal Kerschke*

- `2005.13293v1` - [abs](http://arxiv.org/abs/2005.13293v1) - [pdf](http://arxiv.org/pdf/2005.13293v1)

> Artificial neural networks in general and deep learning networks in particular established themselves as popular and powerful machine learning algorithms. While the often tremendous sizes of these networks are beneficial when solving complex tasks, the tremendous number of parameters also causes such networks to be vulnerable to malicious behavior such as adversarial perturbations. These perturbations can change a model's classification decision. Moreover, while single-step adversaries can easily be transferred from network to network, the transfer of more powerful multi-step adversaries has - usually -- been rather difficult. In this work, we introduce a method for generating strong ad-versaries that can easily (and frequently) be transferred between different models. This method is then used to generate a large set of adversaries, based on which the effects of selected defense methods are experimentally assessed. At last, we introduce a novel, simple, yet effective approach to enhance the resilience of neural networks against adversaries and benchmark it against established defense methods. In contrast to the already existing methods, our proposed defense approach is much more efficient as it only requires a single additional forward-pass to achieve comparable performance results.

</details>

<details>

<summary>2020-05-27 16:46:10 - The Fallibility of Contact-Tracing Apps</summary>

- *Piotr Sapiezynski, Johanna Pruessing, Vedran Sekara*

- `2005.11297v3` - [abs](http://arxiv.org/abs/2005.11297v3) - [pdf](http://arxiv.org/pdf/2005.11297v3)

> Since the onset of the COVID-19's global spread we have been following the debate around contact tracing apps -- the tech-enabled response to the pandemic. As corporations, academics, governments, and civil society discuss the right way to implement these apps, we noticed recurring implicit assumptions. The proposed solutions are designed for a world where Internet access and smartphone ownership are a given, people are willing and able to install these apps, and those who receive notifications about potential exposure to the virus have access to testing and can isolate safely. In this work we challenge these assumptions. We not only show that there are not enough smartphones worldwide to reach required adoption thresholds but also highlight a broad lack of internet access, which affects certain groups more: the elderly, those with lower incomes, and those with limited ability to socially distance. Unfortunately, these are also the groups that are at the highest risks from COVID-19. We also report that the contact tracing apps that are already deployed on an opt-in basis show disappointing adoption levels. We warn about the potential consequences of over-extending the existing state and corporate surveillance powers. Finally, we describe a multitude of scenarios where contact tracing apps will not help regardless of access or policy. In this work we call for a comprehensive and equitable policy response that prioritizes the needs of the most vulnerable, protects human rights, and considers long term impact instead of focusing on technology-first fixes.

</details>

<details>

<summary>2020-05-27 23:42:25 - Mitigating Advanced Adversarial Attacks with More Advanced Gradient Obfuscation Techniques</summary>

- *Han Qiu, Yi Zeng, Qinkai Zheng, Tianwei Zhang, Meikang Qiu, Gerard Memmi*

- `2005.13712v1` - [abs](http://arxiv.org/abs/2005.13712v1) - [pdf](http://arxiv.org/pdf/2005.13712v1)

> Deep Neural Networks (DNNs) are well-known to be vulnerable to Adversarial Examples (AEs). A large amount of efforts have been spent to launch and heat the arms race between the attackers and defenders. Recently, advanced gradient-based attack techniques were proposed (e.g., BPDA and EOT), which have defeated a considerable number of existing defense methods. Up to today, there are still no satisfactory solutions that can effectively and efficiently defend against those attacks.   In this paper, we make a steady step towards mitigating those advanced gradient-based attacks with two major contributions. First, we perform an in-depth analysis about the root causes of those attacks, and propose four properties that can break the fundamental assumptions of those attacks. Second, we identify a set of operations that can meet those properties. By integrating these operations, we design two preprocessing functions that can invalidate these powerful attacks. Extensive evaluations indicate that our solutions can effectively mitigate all existing standard and advanced attack techniques, and beat 11 state-of-the-art defense solutions published in top-tier conferences over the past 2 years. The defender can employ our solutions to constrain the attack success rate below 7% for the strongest attacks even the adversary has spent dozens of GPU hours.

</details>

<details>

<summary>2020-05-28 02:08:15 - Model-Based Risk Assessment for Cyber Physical Systems Security</summary>

- *Ashraf Tantawy, Abdelkarim Erradi, Sherif Abdelwahed, Khaled Shaban*

- `2005.13738v1` - [abs](http://arxiv.org/abs/2005.13738v1) - [pdf](http://arxiv.org/pdf/2005.13738v1)

> Traditional techniques for Cyber-Physical Systems (CPS) security design either treat the cyber and physical systems independently, or do not address the specific vulnerabilities of real time embedded controllers and networks used to monitor and control physical processes. In this work, we develop and test an integrated model-based approach for CPS security risk assessment utilizing a CPS testbed with real-world industrial controllers and communication protocols. The testbed monitors and controls an exothermic Continuous Stirred Tank Reactor (CSTR) simulated in real-time. CSTR is a fundamental process unit in many industries, including Oil \& Gas, Petrochemicals, Water treatment, and nuclear industry. In addition, the process is rich in terms of hazardous scenarios that could be triggered by cyber attacks due to the lack of possible mechanical protection. The paper presents an integrated approach to analyze and design the cyber security system for a given CPS where the physical threats are identified first to guide the risk assessment process. A mathematical model is derived for the physical system using a hybrid automaton to enumerate potential hazardous states of the system. The cyber system is then analyzed using network and data flow models to develop the attack scenarios that may lead to the identified hazards. Finally, the attack scenarios are performed on the testbed and observations are obtained on the possible ways to prevent and mitigate the attacks. The insights gained from the experiments result in several key findings, including the expressive power of hybrid automaton in security risk assessment, the hazard development time and its impact on cyber security design, and the tight coupling between the physical and the cyber systems for CPS that requires an integrated design approach to achieve cost-effective and secure designs.

</details>

<details>

<summary>2020-05-28 07:31:05 - Exposure to Social Engagement Metrics Increases Vulnerability to Misinformation</summary>

- *Mihai Avram, Nicholas Micallef, Sameer Patil, Filippo Menczer*

- `2005.04682v2` - [abs](http://arxiv.org/abs/2005.04682v2) - [pdf](http://arxiv.org/pdf/2005.04682v2)

> News feeds in virtually all social media platforms include engagement metrics, such as the number of times each post is liked and shared. We find that exposure to these social engagement signals increases the vulnerability of users to misinformation. This finding has important implications for the design of social media interactions in the misinformation age. To reduce the spread of misinformation, we call for technology platforms to rethink the display of social engagement metrics. Further research is needed to investigate whether and how engagement metrics can be presented without amplifying the spread of low-credibility information.

</details>

<details>

<summary>2020-05-28 19:03:57 - The Impact of a Major Security Event on an Open Source Project: The Case of OpenSSL</summary>

- *James Walden*

- `2005.14242v1` - [abs](http://arxiv.org/abs/2005.14242v1) - [pdf](http://arxiv.org/pdf/2005.14242v1)

> Context: The Heartbleed vulnerability brought OpenSSL to international attention in 2014. The almost moribund project was a key security component in public web servers and over a billion mobile devices. This vulnerability led to new investments in OpenSSL.   Objective: The goal of this study is to determine how the Heartbleed vulnerability changed the software evolution of OpenSSL. We study changes in vulnerabilities, code quality, project activity, and software engineering practices.   Method: We use a mixed methods approach, collecting multiple types of quantitative data and qualitative data from web sites and an interview with a developer who worked on post-Heartbleed changes. We use regression discontinuity analysis to determine changes in levels and slopes of code and project activity metrics resulting from Heartbleed.   Results: The OpenSSL project made tremendous improvements to code quality and security after Heartbleed. By the end of 2016, the number of commits per month had tripled, 91 vulnerabilities were found and fixed, code complexity decreased significantly, and OpenSSL obtained a CII best practices badge, certifying its use of good open source development practices.   Conclusions: The OpenSSL project provides a model of how an open source project can adapt and improve after a security event. The evolution of OpenSSL shows that the number of known vulnerabilities is not a useful indicator of project security. A small number of vulnerabilities may simply indicate that a project does not expend much effort to finding vulnerabilities. This study suggests that project activity and CII badge best practices may be better indicators of code quality and security than vulnerability counts.

</details>

<details>

<summary>2020-05-29 14:12:29 - Revisiting RowHammer: An Experimental Analysis of Modern DRAM Devices and Mitigation Techniques</summary>

- *Jeremie S. Kim, Minesh Patel, A. Giray Yaglikci, Hasan Hassan, Roknoddin Azizi, Lois Orosa, Onur Mutlu*

- `2005.13121v2` - [abs](http://arxiv.org/abs/2005.13121v2) - [pdf](http://arxiv.org/pdf/2005.13121v2)

> In order to shed more light on how RowHammer affects modern and future devices at the circuit-level, we first present an experimental characterization of RowHammer on 1580 DRAM chips (408x DDR3, 652x DDR4, and 520x LPDDR4) from 300 DRAM modules (60x DDR3, 110x DDR4, and 130x LPDDR4) with RowHammer protection mechanisms disabled, spanning multiple different technology nodes from across each of the three major DRAM manufacturers. Our studies definitively show that newer DRAM chips are more vulnerable to RowHammer: as device feature size reduces, the number of activations needed to induce a RowHammer bit flip also reduces, to as few as 9.6k (4.8k to two rows each) in the most vulnerable chip we tested.   We evaluate five state-of-the-art RowHammer mitigation mechanisms using cycle-accurate simulation in the context of real data taken from our chips to study how the mitigation mechanisms scale with chip vulnerability. We find that existing mechanisms either are not scalable or suffer from prohibitively large performance overheads in projected future devices given our observed trends of RowHammer vulnerability. Thus, it is critical to research more effective solutions to RowHammer.

</details>

<details>

<summary>2020-05-30 13:36:29 - False Data Injection Attacks on Hybrid AC/HVDC Interconnected System with Virtual Inertia -- Vulnerability, Impact and Detection</summary>

- *Kaikai Pan, Elyas Rakhshani, Peter Palensky*

- `2001.07068v2` - [abs](http://arxiv.org/abs/2001.07068v2) - [pdf](http://arxiv.org/pdf/2001.07068v2)

> Power systems are moving towards hybrid AC/DC grids with the integration of HVDC links, renewable resources and energy storage modules. New models of frequency control have to consider the complex interactions between these components. Meanwhile, more attention should be paid to cyber security concerns as these control strategies highly depend on data communications which may be exposed to cyber attacks. In this regard, this article aims to analyze the false data injection (FDI) attacks on the AC/DC interconnected system with virtual inertia and develop advanced diagnosis tools to reveal their occurrence. We build an optimization-based framework for the purpose of vulnerability and attack impact analysis. Considering the attack impact on the system frequency stability, it is shown that the hybrid grid with parallel AC/DC links and emulated inertia is more vulnerable to the FDI attacks, compared with the one without virtual inertia and the normal AC system. We then propose a detection approach to detect and isolate each FDI intrusion with a sufficient fast response, and even recover the attack value. In addition to theoretical results, the effectiveness of the proposed methods is validated through simulations on the two-area AC/DC interconnected system with virtual inertia emulation capabilities.

</details>

<details>

<summary>2020-05-30 13:47:09 - Impact of Coastal Hazards on Residents Spatial Accessibility to Health Services</summary>

- *Georgios P. Balomenos, Yujie Hu, Jamie E. Padgett, Kyle Shelton*

- `2006.00271v1` - [abs](http://arxiv.org/abs/2006.00271v1) - [pdf](http://arxiv.org/pdf/2006.00271v1)

> The mobility of residents and their access to essential services can be highly affected by transportation network closures that occur during and after coastal hazard events. Few studies have used geographic information systems coupled with infrastructure vulnerability models to explore how spatial accessibility to goods and services shifts after a hurricane. Models that explore spatial accessibility to health services are particularly lacking. This study provides a framework to examine how the disruption of transportation networks during and after a hurricane can impact a residents ability to access health services over time. Two different bridge closure conditions, inundation and structural failure, along with roadway inundation are used to quantify post-hurricane accessibility at short- and long-term temporal scales. Inundation may close a bridge for hours or days, but a structural failure may close a route for weeks or months. Both forms of closure are incorporated using probabilistic vulnerability models coupled with GIS-based models to assess spatial accessibility in the aftermath of a coastal hazard. Harris County, an area in Southeastern Texas prone to coastal hazards, is used as a case study. The results indicate changes in the accessibility scores of specific areas depending on the temporal scale of interest and intensity of the hazard scenario. Sociodemographic indicators are also examined for the study region, revealing the populations most likely to suffer from lack of accessibility. Overall, the presented framework helps to understand how both short-term functionality loss and long-term damage affect access to critical services such as health care after a hazard. This information, in turn, can shape decisions about future mitigation and planning efforts, while the presented framework can be expanded to other hazard-prone areas.

</details>

<details>

<summary>2020-05-30 13:54:43 - Where are the Dangerous Intersections for Pedestrians and Cyclists: A Colocation-Based Approach</summary>

- *Yujie Hu, Yu Zhang, Kyle Shelton*

- `2006.03131v1` - [abs](http://arxiv.org/abs/2006.03131v1) - [pdf](http://arxiv.org/pdf/2006.03131v1)

> Pedestrians and cyclists are vulnerable road users. They are at greater risk for being killed in a crash than other road users. The percentage of fatal crashes that involve a pedestrian or cyclist is higher than the overall percentage of total trips taken by both modes. Because of this risk, finding ways to minimize problematic street environments is critical. Understanding traffic safety spatial patterns and identifying dangerous locations with significantly high crash risks for pedestrians and cyclists is essential in order to design possible countermeasures to improve road safety. This research develops two indicators for examining spatial correlation patterns between elements of the built environment (intersections) and crashes (pedestrian- or cyclist-involved). The global colocation quotient detects the overall connection in an area while the local colocation quotient identifies the locations of high-risk intersections. To illustrate our approach, we applied the methods to inspect the colocation patterns between pedestrian- or cyclist-vehicle crashes and intersections in Houston, Texas and we identified among many intersections the ones that significantly attract crashes. We also scrutinized those intersections, discussed possible attributes leading to high colocation of crashes and proposed corresponding countermeasures.

</details>


## 2020-06

<details>

<summary>2020-06-01 11:42:04 - Adversarial Attacks on Classifiers for Eye-based User Modelling</summary>

- *Inken Hagestedt, Michael Backes, Andreas Bulling*

- `2006.00860v1` - [abs](http://arxiv.org/abs/2006.00860v1) - [pdf](http://arxiv.org/pdf/2006.00860v1)

> An ever-growing body of work has demonstrated the rich information content available in eye movements for user modelling, e.g. for predicting users' activities, cognitive processes, or even personality traits. We show that state-of-the-art classifiers for eye-based user modelling are highly vulnerable to adversarial examples: small artificial perturbations in gaze input that can dramatically change a classifier's predictions. We generate these adversarial examples using the Fast Gradient Sign Method (FGSM) that linearises the gradient to find suitable perturbations. On the sample task of eye-based document type recognition we study the success of different adversarial attack scenarios: with and without knowledge about classifier gradients (white-box vs. black-box) as well as with and without targeting the attack to a specific class, In addition, we demonstrate the feasibility of defending against adversarial attacks by adding adversarial examples to a classifier's training data.

</details>

<details>

<summary>2020-06-01 18:16:39 - Security Smells in Android</summary>

- *Mohammad Ghafari, Pascal Gadient, Oscar Nierstrasz*

- `2006.01181v1` - [abs](http://arxiv.org/abs/2006.01181v1) - [pdf](http://arxiv.org/pdf/2006.01181v1)

> The ubiquity of smartphones, and their very broad capabilities and usage, make the security of these devices tremendously important. Unfortunately, despite all progress in security and privacy mechanisms, vulnerabilities continue to proliferate. Research has shown that many vulnerabilities are due to insecure programming practices. However, each study has often dealt with a specific issue, making the results less actionable for practitioners. To promote secure programming practices, we have reviewed related research, and identified avoidable vulnerabilities in Android-run devices and the "security code smells" that indicate their presence. In particular, we explain the vulnerabilities, their corresponding smells, and we discuss how they could be eliminated or mitigated during development. Moreover, we develop a lightweight static analysis tool and discuss the extent to which it successfully detects several vulnerabilities in about 46,000 apps hosted by the official Android market.

</details>

<details>

<summary>2020-06-01 18:24:27 - Web APIs in Android through the Lens of Security</summary>

- *Pascal Gadient, Mohammad Ghafari, Marc-Andrea Tarnutzer, Oscar Nierstrasz*

- `2001.00195v2` - [abs](http://arxiv.org/abs/2001.00195v2) - [pdf](http://arxiv.org/pdf/2001.00195v2)

> Web communication has become an indispensable characteristic of mobile apps. However, it is not clear what data the apps transmit, to whom, and what consequences such transmissions have. We analyzed the web communications found in mobile apps from the perspective of security. We first manually studied 160 Android apps to identify the commonly-used communication libraries, and to understand how they are used in these apps. We then developed a tool to statically identify web API URLs used in the apps, and restore the JSON data schemas including the type and value of each parameter. We extracted 9,714 distinct web API URLs that were used in 3,376 apps. We found that developers often use the java.net package for network communication, however, third-party libraries like OkHttp are also used in many apps. We discovered that insecure HTTP connections are seven times more prevalent in closed-source than in open-source apps, and that embedded SQL and JavaScript code is used in web communication in more than 500 different apps. This finding is devastating; it leaves billions of users and API service providers vulnerable to attack.

</details>

<details>

<summary>2020-06-02 08:16:02 - Real time Detection of Spectre and Meltdown Attacks Using Machine Learning</summary>

- *Bilal Ali Ahmad*

- `2006.01442v1` - [abs](http://arxiv.org/abs/2006.01442v1) - [pdf](http://arxiv.org/pdf/2006.01442v1)

> Recently discovered Spectre and meltdown attacks affects almost all processors by leaking confidential information to other processes through side-channel attacks. These vulnerabilities expose design flaws in the architecture of modern CPUs. To fix these design flaws, it is necessary to make changes in the hardware of modern processors which is a non-trivial task. Software mitigation techniques for these vulnerabilities cause significant performance degradation. In order to mitigate against Spectre and Meltdown attacks while retaining the performance benefits of modern processors, in this paper, we present a real-time detection mechanism for Spectre and Meltdown attacks by identifying the misuse of speculative execution and side-channel attacks. We use hardware performance counters and software events to monitor activity related to speculative execution, branch prediction, and cache interference. We use various machine learning models to analyze these events. These events produce a very distinctive pattern while the system is under attack; machine learning models are able to detect Meltdown and Spectre attacks under realistic load conditions with an accuracy of over 99%.

</details>

<details>

<summary>2020-06-02 09:22:18 - Kaya: A Testing Framework for Blockchain-based Decentralized Applications</summary>

- *Zhenhao Wu, Jiashuo Zhang, Jianbo Gao, Yue Li, Qingshan Li, Zhi Guan, Zhong Chen*

- `2006.01476v1` - [abs](http://arxiv.org/abs/2006.01476v1) - [pdf](http://arxiv.org/pdf/2006.01476v1)

> In recent years, many decentralized applications based on blockchain (DApp) have been developed. However, due to inadequate testing, DApps are easily exposed to serious vulnerabilities. We find three main challenges for DApp testing, i.e., the inherent complexity of DApp, inconvenient pre-state setting, and not-so-readable logs. In this paper, we propose a testing framework named Kaya to bridge these gaps. Kaya has three main functions. Firstly, Kaya proposes DApp behavior description language (DBDL) to make writing test cases easier. Test cases written in DBDL can also be automatically executed by Kaya. Secondly, Kaya supports a flexible and convenient way for test engineers to set the blockchain pre-states easily. Thirdly, Kaya transforms incomprehensible addresses into readable variables for easy comprehension. With these functions, Kaya can help test engineers test DApps more easily. Besides, to fit the various application environments, we provide two ways for test engineers to use Kaya, i.e., UI and command-line. Our experimental case demonstrates the potential of Kaya in helping test engineers to test DApps more easily.

</details>

<details>

<summary>2020-06-02 17:52:32 - Securing Your Collaborative Jupyter Notebooks in the Cloud using Container and Load Balancing Services</summary>

- *Haw-minn Lu, Adrian Kwong, Jose Unpingco*

- `2006.01818v1` - [abs](http://arxiv.org/abs/2006.01818v1) - [pdf](http://arxiv.org/pdf/2006.01818v1)

> Jupyter has become the go-to platform for developing data applications but data and security concerns, especially when dealing with healthcare, have become paramount for many institutions and applications dealing with sensitive information. How then can we continue to enjoy the data analysis and machine learning opportunities provided by Jupyter and the Python ecosystem while guaranteeing auditable compliance with security and privacy concerns? We will describe the architecture and implementation of a cloud based platform based on Jupyter that integrates with Amazon Web Services (AWS) and uses containerized services without exposing the platform to the vulnerabilities present in Kubernetes and JupyterHub. This architecture addresses the HIPAA requirements to ensure both security and privacy of data. The architecture uses an AWS service to provide JSON Web Tokens (JWT) for authentication as well as network control. Furthermore, our architecture enables secure collaboration and sharing of Jupyter notebooks. Even though our platform is focused on Jupyter notebooks and JupyterLab, it also supports R-Studio and bespoke applications that share the same authentication mechanisms. Further, the platform can be extended to other cloud services other than AWS.

</details>

<details>

<summary>2020-06-02 19:20:49 - Geometric algorithms for predicting resilience and recovering damage in neural networks</summary>

- *Guruprasad Raghavan, Jiayi Li, Matt Thomson*

- `2005.11603v2` - [abs](http://arxiv.org/abs/2005.11603v2) - [pdf](http://arxiv.org/pdf/2005.11603v2)

> Biological neural networks have evolved to maintain performance despite significant circuit damage. To survive damage, biological network architectures have both intrinsic resilience to component loss and also activate recovery programs that adjust network weights through plasticity to stabilize performance. Despite the importance of resilience in technology applications, the resilience of artificial neural networks is poorly understood, and autonomous recovery algorithms have yet to be developed. In this paper, we establish a mathematical framework to analyze the resilience of artificial neural networks through the lens of differential geometry. Our geometric language provides natural algorithms that identify local vulnerabilities in trained networks as well as recovery algorithms that dynamically adjust networks to compensate for damage. We reveal striking vulnerabilities in commonly used image analysis networks, like MLP's and CNN's trained on MNIST and CIFAR10 respectively. We also uncover high-performance recovery paths that enable the same networks to dynamically re-adjust their parameters to compensate for damage. Broadly, our work provides procedures that endow artificial systems with resilience and rapid-recovery routines to enhance their integration with IoT devices as well as enable their deployment for critical applications.

</details>

<details>

<summary>2020-06-03 10:12:37 - An agent-based self-protective method to secure communication between UAVs in unmanned aerial vehicle networks</summary>

- *Reza Fotohi, Eslam Nazemi, Fereidoon Shams Aliee*

- `2006.09293v1` - [abs](http://arxiv.org/abs/2006.09293v1) - [pdf](http://arxiv.org/pdf/2006.09293v1)

> UAVNs (unmanned aerial vehicle networks) may become vulnerable to threats and attacks due to their characteristic features such as highly dynamic network topology, open-air wireless environments, and high mobility. Since previous work has focused on classical and metaheuristic-based approaches, none of these approaches have a self-adaptive approach. In this paper, the challenges and weaknesses of previous methods are examined in the form of a table. Furthermore, we propose an agent-based self-protective method (ASP-UAVN) for UAVNs that is based on the Human Immune System (HIS). In ASP-UAS, the safest route from the source UAV to the destination UAV is chosen according to a self-protective system. In this method, a multi-agent system using an Artificial Immune System (AIS) is employed to detect the attacking UAV and choose the safest route. In the proposed ASP-UAVN, the route request packet (RREQ) is initially transmitted from the source UAV to the destination UAV to detect the existing routes. Then, once the route reply packet (RREP) is received, a self-protective method using agents and the knowledge base is employed to choose the safest route and detect the attacking UAVs. The proposed ASP-UAVN has been validated and evaluated in two ways: simulation and theoretical analysis. The results of simulation evaluation and theory analysis showed that the ASP-UAS increases the Packet Delivery Rate (PDR) by more than 17.4, 20.8, and 25.91%, and detection rate by more than 17.2, 23.1, and 29.3%, and decreases the Packet Loss Rate (PLR) by more than 14.4, 16.8, and 20.21%, the false-positive and false-negative rate by more than 16.5, 25.3, and 31.21% those of SUAS-HIS, SFA and BRUIDS methods, respectively.

</details>

<details>

<summary>2020-06-03 17:14:44 - SQUIRREL: Testing Database Management Systems with Language Validity and Coverage Feedback</summary>

- *Rui Zhong, Yongheng Chen, Hong Hu, Hangfan Zhang, Wenke Lee, Dinghao Wu*

- `2006.02398v1` - [abs](http://arxiv.org/abs/2006.02398v1) - [pdf](http://arxiv.org/pdf/2006.02398v1)

> Fuzzing is an increasingly popular technique for verifying software functionalities and finding security vulnerabilities. However, current mutation-based fuzzers cannot effectively test database management systems (DBMSs), which strictly check inputs for valid syntax and semantics. Generation-based testing can guarantee the syntax correctness of the inputs, but it does not utilize any feedback, like code coverage, to guide the path exploration.   In this paper, we develop Squirrel, a novel fuzzing framework that considers both language validity and coverage feedback to test DBMSs. We design an intermediate representation (IR) to maintain SQL queries in a structural and informative manner. To generate syntactically correct queries, we perform type-based mutations on IR, including statement insertion, deletion and replacement. To mitigate semantic errors, we analyze each IR to identify the logical dependencies between arguments, and generate queries that satisfy these dependencies. We evaluated Squirrel on four popular DBMSs: SQLite, MySQL, PostgreSQL and MariaDB. Squirrel found 51 bugs in SQLite, 7 in MySQL and 5 in MariaDB. 52 of the bugs are fixed with 12 CVEs assigned. In our experiment, Squirrel achieves 2.4x-243.9x higher semantic correctness than state-of-the-art fuzzers, and explores 2.0x-10.9x more new edges than mutation-based tools. These results show that Squirrel is effective in finding memory errors of database management systems.

</details>

<details>

<summary>2020-06-04 15:17:12 - Data-Flow-Based Extension of the System-Theoretic Process Analysis for Security (STPA-Sec)</summary>

- *Jinghua Yu, Stefan Wagner, Feng Luo*

- `2006.02930v1` - [abs](http://arxiv.org/abs/2006.02930v1) - [pdf](http://arxiv.org/pdf/2006.02930v1)

> Security analysis is an essential activity in security engineering to identify potential system vulnerabilities and achieve security requirements in the early design phases. Due to the increasing complexity of modern systems, traditional approaches, which only consider component failures and simple cause-and-effect linkages, lack the power to identify insecure incidents caused by complex interactions among physical systems, human and social entities. By contrast, a top-down System-Theoretic Process Analysis for Security (STPA-Sec) approach views losses as resulting from interactions, focuses on controlling system vulnerabilities instead of external threats and is applicable for complex socio-technical systems. In this paper, we proposed an extension of STPA-Sec based on data flow structures to overcome STPA-Sec's limitations and achieve security constraints of information-critical systems systematically. We analyzed a Bluetooth digital key system of a vehicle by using both the proposed and the original approach to investigate the relationship and differences between both approaches as well as their applicability and highlights. To conclude, the proposed approach can identify more information-related problems with technical details and be used with other STPA-based approaches to co-design systems in multi-disciplines under the unified STPA process framework.

</details>

<details>

<summary>2020-06-05 09:53:01 - DeepSoCS: A Neural Scheduler for Heterogeneous System-on-Chip (SoC) Resource Scheduling</summary>

- *Tegg Taekyong Sung, Jeongsoo Ha, Jeewoo Kim, Alex Yahja, Chae-Bong Sohn, Bo Ryu*

- `2005.07666v2` - [abs](http://arxiv.org/abs/2005.07666v2) - [pdf](http://arxiv.org/pdf/2005.07666v2)

> In this paper, we~present a novel scheduling solution for a class of System-on-Chip (SoC) systems where heterogeneous chip resources (DSP, FPGA, GPU, etc.) must be efficiently scheduled for continuously arriving hierarchical jobs with their tasks represented by a directed acyclic graph. Traditionally, heuristic algorithms have been widely used for many resource scheduling domains, and Heterogeneous Earliest Finish Time (HEFT) has been a dominating state-of-the-art technique across a broad range of heterogeneous resource scheduling domains over many years. Despite their long-standing popularity, HEFT-like algorithms are known to be vulnerable to a small amount of noise added to the environment. Our Deep Reinforcement Learning (DRL)-based SoC Scheduler (DeepSoCS), capable of learning the "best" task ordering under dynamic environment changes, overcomes the brittleness of rule-based schedulers such as HEFT with significantly higher performance across different types of jobs. We~describe a DeepSoCS design process using a real-time heterogeneous SoC scheduling emulator, discuss major challenges, and present two novel neural network design features that lead to outperforming HEFT: (i) hierarchical job- and task-graph embedding; and (ii) efficient use of real-time task information in the state space. Furthermore, we~introduce effective techniques to address two fundamental challenges present in our environment: delayed consequences and joint actions. Through an extensive simulation study, we~show that our DeepSoCS exhibits the significantly higher performance of job execution time than that of HEFT with a higher level of robustness under realistic noise conditions. We~conclude with a discussion of the potential improvements for our DeepSoCS neural scheduler.

</details>

<details>

<summary>2020-06-06 14:06:38 - Unique properties of adversarially trained linear classifiers on Gaussian data</summary>

- *Jamie Hayes*

- `2006.03873v1` - [abs](http://arxiv.org/abs/2006.03873v1) - [pdf](http://arxiv.org/pdf/2006.03873v1)

> Machine learning models are vulnerable to adversarial perturbations, that when added to an input, can cause high confidence misclassifications. The adversarial learning research community has made remarkable progress in the understanding of the root causes of adversarial perturbations. However, most problems that one may consider important to solve for the deployment of machine learning in safety critical tasks involve high dimensional complex manifolds that are difficult to characterize and study. It is common to develop adversarially robust learning theory on simple problems, in the hope that insights will transfer to `real world datasets'. In this work, we discuss a setting where this approach fails. In particular, we show with a linear classifier, it is always possible to solve a binary classification problem on Gaussian data under arbitrary levels of adversarial corruption during training, and that this property is not observed with non-linear classifiers on the CIFAR-10 dataset.

</details>

<details>

<summary>2020-06-06 15:21:59 - Towards Understanding the Adversarial Vulnerability of Skeleton-based Action Recognition</summary>

- *Tianhang Zheng, Sheng Liu, Changyou Chen, Junsong Yuan, Baochun Li, Kui Ren*

- `2005.07151v2` - [abs](http://arxiv.org/abs/2005.07151v2) - [pdf](http://arxiv.org/pdf/2005.07151v2)

> Skeleton-based action recognition has attracted increasing attention due to its strong adaptability to dynamic circumstances and potential for broad applications such as autonomous and anonymous surveillance. With the help of deep learning techniques, it has also witnessed substantial progress and currently achieved around 90\% accuracy in benign environment. On the other hand, research on the vulnerability of skeleton-based action recognition under different adversarial settings remains scant, which may raise security concerns about deploying such techniques into real-world systems. However, filling this research gap is challenging due to the unique physical constraints of skeletons and human actions. In this paper, we attempt to conduct a thorough study towards understanding the adversarial vulnerability of skeleton-based action recognition. We first formulate generation of adversarial skeleton actions as a constrained optimization problem by representing or approximating the physiological and physical constraints with mathematical formulations. Since the primal optimization problem with equality constraints is intractable, we propose to solve it by optimizing its unconstrained dual problem using ADMM. We then specify an efficient plug-in defense, inspired by recent theories and empirical observations, against the adversarial skeleton actions. Extensive evaluations demonstrate the effectiveness of the attack and defense method under different settings.

</details>

<details>

<summary>2020-06-06 17:56:42 - Bypassing Backdoor Detection Algorithms in Deep Learning</summary>

- *Te Juin Lester Tan, Reza Shokri*

- `1905.13409v2` - [abs](http://arxiv.org/abs/1905.13409v2) - [pdf](http://arxiv.org/pdf/1905.13409v2)

> Deep learning models are vulnerable to various adversarial manipulations of their training data, parameters, and input sample. In particular, an adversary can modify the training data and model parameters to embed backdoors into the model, so the model behaves according to the adversary's objective if the input contains the backdoor features, referred to as the backdoor trigger (e.g., a stamp on an image). The poisoned model's behavior on clean data, however, remains unchanged. Many detection algorithms are designed to detect backdoors on input samples or model parameters, through the statistical difference between the latent representations of adversarial and clean input samples in the poisoned model. In this paper, we design an adversarial backdoor embedding algorithm that can bypass the existing detection algorithms including the state-of-the-art techniques. We design an adaptive adversarial training algorithm that optimizes the original loss function of the model, and also maximizes the indistinguishability of the hidden representations of poisoned data and clean data. This work calls for designing adversary-aware defense mechanisms for backdoor detection.

</details>

<details>

<summary>2020-06-06 18:22:55 - Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning</summary>

- *Milad Nasr, Reza Shokri, Amir Houmansadr*

- `1812.00910v2` - [abs](http://arxiv.org/abs/1812.00910v2) - [pdf](http://arxiv.org/pdf/1812.00910v2)

> Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge.   We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.

</details>

<details>

<summary>2020-06-07 01:40:09 - Mitigating Query-Flooding Parameter Duplication Attack on Regression Models with High-Dimensional Gaussian Mechanism</summary>

- *Xiaoguang Li, Hui Li, Haonan Yan, Zelei Cheng, Wenhai Sun, Hui Zhu*

- `2002.02061v3` - [abs](http://arxiv.org/abs/2002.02061v3) - [pdf](http://arxiv.org/pdf/2002.02061v3)

> Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Differential privacy (DP) has been considered a promising technique to mitigate this attack. However, we find that the vulnerability persists when regression models are being protected by current DP solutions. We show that the adversary can launch a query-flooding parameter duplication (QPD) attack to infer the model information by repeated queries.   To defend against the QPD attack on logistic and linear regression models, we propose a novel High-Dimensional Gaussian (HDG) mechanism to prevent unauthorized information disclosure without interrupting the intended services. In contrast to prior work, the proposed HDG mechanism will dynamically generate the privacy budget and random noise for different queries and their results to enhance the obfuscation. Besides, for the first time, HDG enables an optimal privacy budget allocation that automatically determines the minimum amount of noise to be added per user-desired privacy level on each dimension. We comprehensively evaluate the performance of HDG using real-world datasets and shows that HDG effectively mitigates the QPD attack while satisfying the privacy requirements. We also prepare to open-source the relevant codes to the community for further research.

</details>

<details>

<summary>2020-06-08 14:07:19 - Ensemble-based Feature Selection and Classification Model for DNS Typo-squatting Detection</summary>

- *Abdallah Moubayed, Emad Aqeeli, Abdallah Shami*

- `2006.09272v1` - [abs](http://arxiv.org/abs/2006.09272v1) - [pdf](http://arxiv.org/pdf/2006.09272v1)

> Domain Name System (DNS) plays in important role in the current IP-based Internet architecture. This is because it performs the domain name to IP resolution. However, the DNS protocol has several security vulnerabilities due to the lack of data integrity and origin authentication within it. This paper focuses on one particular security vulnerability, namely typo-squatting. Typo-squatting refers to the registration of a domain name that is extremely similar to that of an existing popular brand with the goal of redirecting users to malicious/suspicious websites. The danger of typo-squatting is that it can lead to information threat, corporate secret leakage, and can facilitate fraud. This paper builds on our previous work in [1], which only proposed majority-voting based classifier, by proposing an ensemble-based feature selection and bagging classification model to detect DNS typo-squatting attack. Experimental results show that the proposed framework achieves high accuracy and precision in identifying the malicious/suspicious typo-squatting domains (a loss of at most 1.5% in accuracy and 5% in precision when compared to the model that used the complete feature set) while having a lower computational complexity due to the smaller feature set (a reduction of more than 50% in feature set size).

</details>

<details>

<summary>2020-06-10 01:09:30 - Adversarial Attacks on Brain-Inspired Hyperdimensional Computing-Based Classifiers</summary>

- *Fangfang Yang, Shaolei Ren*

- `2006.05594v1` - [abs](http://arxiv.org/abs/2006.05594v1) - [pdf](http://arxiv.org/pdf/2006.05594v1)

> Being an emerging class of in-memory computing architecture, brain-inspired hyperdimensional computing (HDC) mimics brain cognition and leverages random hypervectors (i.e., vectors with a dimensionality of thousands or even more) to represent features and to perform classification tasks. The unique hypervector representation enables HDC classifiers to exhibit high energy efficiency, low inference latency and strong robustness against hardware-induced bit errors. Consequently, they have been increasingly recognized as an appealing alternative to or even replacement of traditional deep neural networks (DNNs) for local on device classification, especially on low-power Internet of Things devices. Nonetheless, unlike their DNN counterparts, state-of-the-art designs for HDC classifiers are mostly security-oblivious, casting doubt on their safety and immunity to adversarial inputs. In this paper, we study for the first time adversarial attacks on HDC classifiers and highlight that HDC classifiers can be vulnerable to even minimally-perturbed adversarial samples. Concretely, using handwritten digit classification as an example, we construct a HDC classifier and formulate a grey-box attack problem, where an attacker's goal is to mislead the target HDC classifier to produce erroneous prediction labels while keeping the amount of added perturbation noise as little as possible. Then, we propose a modified genetic algorithm to generate adversarial samples within a reasonably small number of queries. Our results show that adversarial images generated by our algorithm can successfully mislead the HDC classifier to produce wrong prediction labels with a high probability (i.e., 78% when the HDC classifier uses a fixed majority rule for decision). Finally, we also present two defense strategies -- adversarial training and retraining-- to strengthen the security of HDC classifiers.

</details>

<details>

<summary>2020-06-10 19:59:52 - Evaluating the Exploitability of Implicit Interactions in Distributed Systems</summary>

- *Jason Jaskolka*

- `2006.06045v1` - [abs](http://arxiv.org/abs/2006.06045v1) - [pdf](http://arxiv.org/pdf/2006.06045v1)

> Implicit interactions refer to those interactions among the components of a system that may be unintended and/or unforeseen by the system designers. As such, they represent cybersecurity vulnerabilities that can be exploited to mount cyber-attacks causing serious and destabilizing system effects. In this paper, we study implicit interactions in distributed systems specified using the algebraic modeling framework known as Communicating Concurrent Kleene Algebra (C$^2$KA). To identify and defend against a range of possible attack scenarios, we develop a new measure of exploitability for implicit interactions to aid in evaluating the threat posed by the existence of such vulnerabilities in system designs for launching cyber-attacks. The presented approach is based on the modeling and analysis of the influence and response of the system agents and their C$^2$KA specifications. We also demonstrate the applicability of the proposed approach using a prototype tool that supports the automated analysis. The rigorous, practical techniques presented here enable cybersecurity vulnerabilities in the designs of distributed systems to be more easily identified, assessed, and then mitigated, offering significant improvements to overall system resilience, dependability, and security.

</details>

<details>

<summary>2020-06-11 06:54:12 - Predicting Motion of Vulnerable Road Users using High-Definition Maps and Efficient ConvNets</summary>

- *Fang-Chieh Chou, Tsung-Han Lin, Henggang Cui, Vladan Radosavljevic, Thi Nguyen, Tzu-Kuo Huang, Matthew Niedoba, Jeff Schneider, Nemanja Djuric*

- `1906.08469v2` - [abs](http://arxiv.org/abs/1906.08469v2) - [pdf](http://arxiv.org/pdf/1906.08469v2)

> Following detection and tracking of traffic actors, prediction of their future motion is the next critical component of a self-driving vehicle (SDV) technology, allowing the SDV to operate safely and efficiently in its environment. This is particularly important when it comes to vulnerable road users (VRUs), such as pedestrians and bicyclists. These actors need to be handled with special care due to an increased risk of injury, as well as the fact that their behavior is less predictable than that of motorized actors. To address this issue, in the current study we present a deep learning-based method for predicting VRU movement, where we rasterize high-definition maps and actor's surroundings into a bird's-eye view image used as an input to deep convolutional networks. In addition, we propose a fast architecture suitable for real-time inference, and perform an ablation study of various rasterization approaches to find the optimal choice for accurate prediction. The results strongly indicate benefits of using the proposed approach for motion prediction of VRUs, both in terms of accuracy and latency.

</details>

<details>

<summary>2020-06-11 10:14:34 - Vulnerability Analysis of 2500 Docker Hub Images</summary>

- *Katrine Wist, Malene Helsem, Danilo Gligoroski*

- `2006.02932v3` - [abs](http://arxiv.org/abs/2006.02932v3) - [pdf](http://arxiv.org/pdf/2006.02932v3)

> The use of container technology has skyrocketed during the last few years, with Docker as the leading container platform. Docker's online repository for publicly available container images, called Docker Hub, hosts over 3.5 million images at the time of writing, making it the world's largest community of container images. We perform an extensive vulnerability analysis of 2500 Docker images. It is of particular interest to perform this type of analysis because the vulnerability landscape is a rapidly changing category, the vulnerability scanners are constantly developed and updated, new vulnerabilities are discovered, and the volume of images on Docker Hub is increasing every day. Our main findings reveal that (1) the number of newly introduced vulnerabilities on Docker Hub is rapidly increasing; (2) certified images are the most vulnerable; (3) official images are the least vulnerable; (4) there is no correlation between the number of vulnerabilities and image features (i.e., number of pulls, number of stars, and days since the last update); (5) the most severe vulnerabilities originate from two of the most popular scripting languages, JavaScript and Python; and (6) Python 2.x packages and jackson-databind packages contain the highest number of severe vulnerabilities. We perceive our study as the most extensive vulnerability analysis published in the open literature in the last couple of years.

</details>

<details>

<summary>2020-06-11 13:30:35 - DEPOSafe: Demystifying the Fake Deposit Vulnerability in Ethereum Smart Contracts</summary>

- *Ru Ji, Ningyu He, Lei Wu, Haoyu Wang, Guangdong Bai, Yao Guo*

- `2006.06419v1` - [abs](http://arxiv.org/abs/2006.06419v1) - [pdf](http://arxiv.org/pdf/2006.06419v1)

> Cryptocurrency has seen an explosive growth in recent years, thanks to the evolvement of blockchain technology and its economic ecosystem. Besides Bitcoin, thousands of cryptocurrencies have been distributed on blockchains, while hundreds of cryptocurrency exchanges are emerging to facilitate the trading of digital assets. At the same time, it also attracts the attentions of attackers. Fake deposit, as one of the most representative attacks (vulnerabilities) related to exchanges and tokens, has been frequently observed in the blockchain ecosystem, causing large financial losses. However, besides a few security reports, our community lacks of the understanding of this vulnerability, for example its scale and the impacts. In this paper, we take the first step to demystify the fake deposit vulnerability. Based on the essential patterns we have summarized, we implement DEPOSafe, an automated tool to detect and verify (exploit) the fake deposit vulnerability in ERC-20 smart contracts. DEPOSafe incorporates several key techniques including symbolic execution based static analysis and behavior modeling based dynamic verification. By applying DEPOSafe to 176,000 ERC-20 smart contracts, we have identified over 7,000 vulnerable contracts that may suffer from two types of attacks. Our findings demonstrate the urgency to identify and prevent the fake deposit vulnerability.

</details>

<details>

<summary>2020-06-11 20:50:05 - Semantic Robustness of Models of Source Code</summary>

- *Goutham Ramakrishnan, Jordan Henkel, Zi Wang, Aws Albarghouthi, Somesh Jha, Thomas Reps*

- `2002.03043v2` - [abs](http://arxiv.org/abs/2002.03043v2) - [pdf](http://arxiv.org/pdf/2002.03043v2)

> Deep neural networks are vulnerable to adversarial examples - small input perturbations that result in incorrect predictions. We study this problem for models of source code, where we want the network to be robust to source-code modifications that preserve code functionality. (1) We define a powerful adversary that can employ sequences of parametric, semantics-preserving program transformations; (2) we show how to perform adversarial training to learn models robust to such adversaries; (3) we conduct an evaluation on different languages and architectures, demonstrating significant quantitative gains in robustness.

</details>

<details>

<summary>2020-06-11 21:35:24 - Backdoors in Neural Models of Source Code</summary>

- *Goutham Ramakrishnan, Aws Albarghouthi*

- `2006.06841v1` - [abs](http://arxiv.org/abs/2006.06841v1) - [pdf](http://arxiv.org/pdf/2006.06841v1)

> Deep neural networks are vulnerable to a range of adversaries. A particularly pernicious class of vulnerabilities are backdoors, where model predictions diverge in the presence of subtle triggers in inputs. An attacker can implant a backdoor by poisoning the training data to yield a desired target prediction on triggered inputs. We study backdoors in the context of deep-learning for source code. (1) We define a range of backdoor classes for source-code tasks and show how to poison a dataset to install such backdoors. (2) We adapt and improve recent algorithms from robust statistics for our setting, showing that backdoors leave a spectral signature in the learned representation of source code, thus enabling detection of poisoned data. (3) We conduct a thorough evaluation on different architectures and languages, showing the ease of injecting backdoors and our ability to eliminate them.

</details>

<details>

<summary>2020-06-12 04:42:10 - Vulnerable Road User Detection Using Smartphone Sensors and Recurrence Quantification Analysis</summary>

- *Huthaifa I. Ashqar, Mohammed Elhenawy, Mahmoud Masoud, Andry Rakotonirainy, Hesham A. Rakha*

- `2006.06941v1` - [abs](http://arxiv.org/abs/2006.06941v1) - [pdf](http://arxiv.org/pdf/2006.06941v1)

> With the fast advancements of the Autonomous Vehicle (AV) industry, detection of Vulnerable Road Users (VRUs) using smartphones is critical for safety applications of Cooperative Intelligent Transportation Systems (C-ITSs). This study explores the use of low-power smartphone sensors and the Recurrence Quantification Analysis (RQA) features for this task. These features are computed over a thresholded similarity matrix extracted from nine channels: accelerometer, gyroscope, and rotation vector in each direction (x, y, and z). Given the high-power consumption of GPS, GPS data is excluded. RQA features are added to traditional time domain features to investigate the classification accuracy when using binary, four-class, and five-class Random Forest classifiers. Experimental results show a promising performance when only using RQA features with a resulted accuracy of 98. 34% and a 98. 79% by adding time domain features. Results outperform previous reported accuracy, demonstrating that RQA features have high classifying capability with respect to VRU detection.

</details>

<details>

<summary>2020-06-12 08:14:05 - CANOA: CAN Origin Authentication Through Power Side-Channel Monitoring</summary>

- *Shailja Thakur, Carlos Moreno, Sebastian Fischmeister*

- `2006.06993v1` - [abs](http://arxiv.org/abs/2006.06993v1) - [pdf](http://arxiv.org/pdf/2006.06993v1)

> The lack of any sender authentication mechanism in place makes CAN (Controller Area Network) vulnerable to security threats. For instance, an attacker can impersonate an ECU (Electronic Control Unit) on the bus and send spoofed messages unobtrusively with the identifier of the impersonated ECU. To address this problem, we propose a novel sender authentication technique that uses power consumption measurements of the ECU to authenticate the sender of a message. When an ECU is transmitting, its power requirement is affected, and a characteristic pattern appears in its power consumption. Our technique exploits the power consumption of each ECU during the transmission of a message to determine whether the message actually originated from the purported sender. We evaluate our approach in both a lab setup and a real vehicle. We also evaluate our approach against factors that can impact the power consumption measurement of the ECU. The results of the evaluation show that the proposed technique is applicable in a broad range of operating conditions with reasonable computational power requirements and attaining good accuracy.

</details>

<details>

<summary>2020-06-12 08:30:04 - Spatial Firewalls: Quarantining Malware Epidemics in Large Scale Massive Wireless Networks</summary>

- *Hesham Elsawy, Mustafa A. Kishk, Mohamed-Slim Alouini*

- `2006.05059v2` - [abs](http://arxiv.org/abs/2006.05059v2) - [pdf](http://arxiv.org/pdf/2006.05059v2)

> Billions of wireless devices are foreseen to participate in big data aggregation and smart automation in order to interface the cyber and physical worlds. Such large-scale ultra-dense wireless connectivity is vulnerable to malicious software (malware) epidemics. Malware worms can exploit multi-hop wireless connectivity to stealthily diffuse throughout the wireless network without being noticed to security servers at the core network. Compromised devices can then be used by adversaries to remotely launch cyber attacks that cause large-scale critical physical damage and threaten public safety. This article overviews the types, threats, and propagation models for malware epidemics in large-scale wireless networks (LSWN). Then, the article proposes a novel and cost efficient countermeasure against malware epidemics in LSWN, denoted as spatial firewalls. It is shown that equipping a strategically selected small portion (i.e., less than 10\%) of the devices with state-of-the-art security mechanisms is sufficient to create spatially secured zones that quarantine malware epidemics. Quarantined infected devices are then cured by on-demand localized software patching. To this end, several firewall deployment strategies are discussed and compared.

</details>

<details>

<summary>2020-06-12 12:23:50 - Learning Diverse Representations for Fast Adaptation to Distribution Shift</summary>

- *Daniel Pace, Alessandra Russo, Murray Shanahan*

- `2006.07119v1` - [abs](http://arxiv.org/abs/2006.07119v1) - [pdf](http://arxiv.org/pdf/2006.07119v1)

> The i.i.d. assumption is a useful idealization that underpins many successful approaches to supervised machine learning. However, its violation can lead to models that learn to exploit spurious correlations in the training data, rendering them vulnerable to adversarial interventions, undermining their reliability, and limiting their practical application. To mitigate this problem, we present a method for learning multiple models, incorporating an objective that pressures each to learn a distinct way to solve the task. We propose a notion of diversity based on minimizing the conditional total correlation of final layer representations across models given the label, which we approximate using a variational estimator and minimize using adversarial training. To demonstrate our framework's ability to facilitate rapid adaptation to distribution shift, we train a number of simple classifiers from scratch on the frozen outputs of our models using a small amount of data from the shifted distribution. Under this evaluation protocol, our framework significantly outperforms a baseline trained using the empirical risk minimization principle.

</details>

<details>

<summary>2020-06-12 13:51:59 - Data Structure Primitives on Persistent Memory: An Evaluation</summary>

- *Philipp Götze, Arun Kumar Tharanatha, Kai-Uwe Sattler*

- `2001.02172v2` - [abs](http://arxiv.org/abs/2001.02172v2) - [pdf](http://arxiv.org/pdf/2001.02172v2)

> Persistent Memory (PMem), as already available, e.g., with Intel Optane DC Persistent Memory, represents a very promising, next-generation memory solution with a significant impact on database architectures. Several data structures for this new technology and its properties have already been proposed. However, primarily only complete structures are presented and evaluated. Thus, the implications of the individual ideas and PMem features are concealed. Therefore, in this paper, we disassemble the structures presented so far, identify their underlying design primitives, and assign them to appropriate design goals regarding PMem. As a result of our comprehensive experiments on real PM hardware, we can reveal the trade-offs of the primitives for various access patterns. This allowed us to pinpoint their best use cases as well as vulnerabilities. Besides our general insights regarding PMem-based data structure design, we also discovered new combinations not examined in the literature so far.

</details>

<details>

<summary>2020-06-14 14:20:13 - Launching Stealth Attacks using Cloud</summary>

- *Moitrayee Chatterjee, Prerit Datta, Faranak Abri, Akbar Siami Namin, Keith S. Jones*

- `2006.07908v1` - [abs](http://arxiv.org/abs/2006.07908v1) - [pdf](http://arxiv.org/pdf/2006.07908v1)

> Cloud computing offers users scalable platforms and low resource cost. At the same time, the off-site location of the resources of this service model makes it more vulnerable to certain types of adversarial actions. Cloud computing has not only gained major user base, but also, it has the features that attackers can leverage to remain anonymous and stealth. With convenient access to data and technology, cloud has turned into an attack platform among other utilization. This paper reports our study to show that cyber attackers heavily abuse the public cloud platforms to setup their attack environments and launch stealth attacks. The paper first reviews types of attacks launched through cloud environment. It then reports case studies through which the processes of launching cyber attacks using clouds are demonstrated.

</details>

<details>

<summary>2020-06-14 14:47:57 - Vulnerability Coverage for Secure Configuration</summary>

- *Shuvalaxmi Dass, Akbar Siami Namin*

- `2006.08604v1` - [abs](http://arxiv.org/abs/2006.08604v1) - [pdf](http://arxiv.org/pdf/2006.08604v1)

> We present a novel idea on adequacy testing called ``{vulnerability coverage}.'' The introduced coverage measure examines the underlying software for the presence of certain classes of vulnerabilities often found in the National Vulnerability Database (NVD) website. The thoroughness of the test input generation procedure is performed through the adaptation of evolutionary algorithms namely Genetic Algorithms (GA) and Particle Swarm Optimization (PSO). The methodology utilizes the Common Vulnerability Scoring System (CVSS), a free and open industry standard for assessing the severity of computer system security vulnerabilities, as a fitness measure for test inputs generation. The outcomes of these evolutionary algorithms are then evaluated in order to identify the vulnerabilities that match a class of vulnerability patterns for testing purposes.

</details>

<details>

<summary>2020-06-14 15:53:10 - Vulnerability Coverage as an Adequacy Testing Criterion</summary>

- *Shuvalaxmi Dass, Akbar Siami Namin*

- `2006.08606v1` - [abs](http://arxiv.org/abs/2006.08606v1) - [pdf](http://arxiv.org/pdf/2006.08606v1)

> Mainstream software applications and tools are the configurable platforms with an enormous number of parameters along with their values. Certain settings and possible interactions between these parameters may harden (or soften) the security and robustness of these applications against some known vulnerabilities. However, the large number of vulnerabilities reported and associated with these tools make the exhaustive testing of these tools infeasible against these vulnerabilities infeasible. As an instance of general software testing problem, the research question to address is whether the system under test is robust and secure against these vulnerabilities. This paper introduces the idea of ``vulnerability coverage,'' a concept to adequately test a given application for a certain classes of vulnerabilities, as reported by the National Vulnerability Database (NVD). The deriving idea is to utilize the Common Vulnerability Scoring System (CVSS) as a means to measure the fitness of test inputs generated by evolutionary algorithms and then through pattern matching identify vulnerabilities that match the generated vulnerability vectors and then test the system under test for those identified vulnerabilities. We report the performance of two evolutionary algorithms (i.e., Genetic Algorithms and Particle Swarm Optimization) in generating the vulnerability pattern vectors.

</details>

<details>

<summary>2020-06-14 21:12:00 - Vyper: A Security Comparison with Solidity Based on Common Vulnerabilities</summary>

- *Mudabbir Kaleem, Anastasia Mavridou, Aron Laszka*

- `2003.07435v4` - [abs](http://arxiv.org/abs/2003.07435v4) - [pdf](http://arxiv.org/pdf/2003.07435v4)

> Vyper has been proposed as a new high-level language for Ethereum smart contract development due to numerous security vulnerabilities and attacks witnessed on contracts written in Solidity since the system's inception. Vyper aims to address these vulnerabilities by providing a language that focuses on simplicity, auditability and security. We present a survey where we study how well-known and commonly-encountered vulnerabilities in Solidity feature in Vyper's development environment. We analyze all such vulnerabilities individually and classify them into five groups based on their status in Vyper. To the best of our knowledge, our survey is the first attempt to study security vulnerabilities in Vyper.

</details>

<details>

<summary>2020-06-15 00:54:49 - Timely Detection and Mitigation of Stealthy DDoS Attacks via IoT Networks</summary>

- *Keval Doshi, Yasin Yilmaz, Suleyman Uludag*

- `2006.08064v1` - [abs](http://arxiv.org/abs/2006.08064v1) - [pdf](http://arxiv.org/pdf/2006.08064v1)

> Internet of Things (IoT) networks consist of sensors, actuators, mobile and wearable devices that can connect to the Internet. With billions of such devices already in the market which have significant vulnerabilities, there is a dangerous threat to the Internet services and also some cyber-physical systems that are also connected to the Internet. Specifically, due to their existing vulnerabilities IoT devices are susceptible to being compromised and being part of a new type of stealthy Distributed Denial of Service (DDoS) attack, called Mongolian DDoS, which is characterized by its widely distributed nature and small attack size from each source. This study proposes a novel anomaly-based Intrusion Detection System (IDS) that is capable of timely detecting and mitigating this emerging type of DDoS attacks. The proposed IDS's capability of detecting and mitigating stealthy DDoS attacks with even very low attack size per source is demonstrated through numerical and testbed experiments.

</details>

<details>

<summary>2020-06-15 16:05:27 - Learning to map source code to software vulnerability using code-as-a-graph</summary>

- *Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim Laredo, Alessandro Morari*

- `2006.08614v1` - [abs](http://arxiv.org/abs/2006.08614v1) - [pdf](http://arxiv.org/pdf/2006.08614v1)

> We explore the applicability of Graph Neural Networks in learning the nuances of source code from a security perspective. Specifically, whether signatures of vulnerabilities in source code can be learned from its graph representation, in terms of relationships between nodes and edges. We create a pipeline we call AI4VA, which first encodes a sample source code into a Code Property Graph. The extracted graph is then vectorized in a manner which preserves its semantic information. A Gated Graph Neural Network is then trained using several such graphs to automatically extract templates differentiating the graph of a vulnerable sample from a healthy one. Our model outperforms static analyzers, classic machine learning, as well as CNN and RNN-based deep learning models on two of the three datasets we experiment with. We thus show that a code-as-graph encoding is more meaningful for vulnerability detection than existing code-as-photo and linear sequence encoding approaches. (Submitted Oct 2019, Paper #28, ICST)

</details>

<details>

<summary>2020-06-15 16:34:05 - A Suite of Metrics for Calculating the Most Significant Security Relevant Software Flaw Types</summary>

- *Peter Mell, Assane Gueye*

- `2006.08524v1` - [abs](http://arxiv.org/abs/2006.08524v1) - [pdf](http://arxiv.org/pdf/2006.08524v1)

> The Common Weakness Enumeration (CWE) is a prominent list of software weakness types. This list is used by vulnerability databases to describe the underlying security flaws within analyzed vulnerabilities. This linkage opens the possibility of using the analysis of software vulnerabilities to identify the most significant weaknesses that enable those vulnerabilities. We accomplish this through creating mashup views combining CWE weakness taxonomies with vulnerability analysis data. The resulting graphs have CWEs as nodes, edges derived from multiple CWE taxonomies, and nodes adorned with vulnerability analysis information (propagated from children to parents). Using these graphs, we develop a suite of metrics to identify the most significant weakness types (using the perspectives of frequency, impact, exploitability, and overall severity).

</details>

<details>

<summary>2020-06-16 01:00:11 - Continual Learning with Adaptive Weights (CLAW)</summary>

- *Tameem Adel, Han Zhao, Richard E. Turner*

- `1911.09514v2` - [abs](http://arxiv.org/abs/1911.09514v2) - [pdf](http://arxiv.org/pdf/1911.09514v2)

> Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting.

</details>

<details>

<summary>2020-06-16 03:30:23 - DefenseVGAE: Defending against Adversarial Attacks on Graph Data via a Variational Graph Autoencoder</summary>

- *Ao Zhang, Jinwen Ma*

- `2006.08900v1` - [abs](http://arxiv.org/abs/2006.08900v1) - [pdf](http://arxiv.org/pdf/2006.08900v1)

> Graph neural networks (GNNs) achieve remarkable performance for tasks on graph data. However, recent works show they are extremely vulnerable to adversarial structural perturbations, making their outcomes unreliable. In this paper, we propose DefenseVGAE, a novel framework leveraging variational graph autoencoders(VGAEs) to defend GNNs against such attacks. DefenseVGAE is trained to reconstruct graph structure. The reconstructed adjacency matrix can reduce the effects of adversarial perturbations and boost the performance of GCNs when facing adversarial attacks. Our experiments on a number of datasets show the effectiveness of the proposed method under various threat models. Under some settings it outperforms existing defense strategies. Our code has been made publicly available at https://github.com/zhangao520/defense-vgae.

</details>

<details>

<summary>2020-06-16 12:26:13 - New Interpretations of Normalization Methods in Deep Learning</summary>

- *Jiacheng Sun, Xiangyong Cao, Hanwen Liang, Weiran Huang, Zewei Chen, Zhenguo Li*

- `2006.09104v1` - [abs](http://arxiv.org/abs/2006.09104v1) - [pdf](http://arxiv.org/pdf/2006.09104v1)

> In recent years, a variety of normalization methods have been proposed to help train neural networks, such as batch normalization (BN), layer normalization (LN), weight normalization (WN), group normalization (GN), etc. However, mathematical tools to analyze all these normalization methods are lacking. In this paper, we first propose a lemma to define some necessary tools. Then, we use these tools to make a deep analysis on popular normalization methods and obtain the following conclusions: 1) Most of the normalization methods can be interpreted in a unified framework, namely normalizing pre-activations or weights onto a sphere; 2) Since most of the existing normalization methods are scaling invariant, we can conduct optimization on a sphere with scaling symmetry removed, which can help stabilize the training of network; 3) We prove that training with these normalization methods can make the norm of weights increase, which could cause adversarial vulnerability as it amplifies the attack. Finally, a series of experiments are conducted to verify these claims.

</details>

<details>

<summary>2020-06-16 12:34:17 - An STPA-based Approach for Systematic Security Analysis of In-vehicle Diagnostic and Software Update Systems</summary>

- *Jinghua Yu, Stefan Wagner, Feng Luo*

- `2006.09108v1` - [abs](http://arxiv.org/abs/2006.09108v1) - [pdf](http://arxiv.org/pdf/2006.09108v1)

> The in-vehicle diagnostic and software update system, which supports remote diagnostic and Over-The-Air (OTA) software updates, is a critical attack goal in automobiles. Adversaries can inject malicious software into vehicles or steal sensitive information through communication channels. Therefore, security analysis, which identifies potential security issues, needs to be conducted in system design. However, existing security analyses of in-vehicle systems are threat-oriented, which start with threat identification and assess risks by brainstorming. In this paper, a system-oriented approach is proposed on the basis of the System-Theoretic Process Analysis (STPA). The proposed approach extends the original STPA from the perspective of data flows and is applicable for information-flow-based systems. Besides, we propose a general model for in-vehicle diagnostic and software update systems and use it to establish a security analysis guideline. In comparison with threat-oriented approaches, the proposed approach shifts from focusing on threats to system vulnerabilities and seems to be efficient to prevent the system from known or even unknown threats. Furthermore, as an extension of the STPA, which has been proven to be applicable to high level designs, the proposed approach can be well integrated into high-level analyses and perform co-design in different disciplines within a unified STPA framework.

</details>

<details>

<summary>2020-06-16 16:10:09 - How Secure is Distributed Convolutional Neural Network on IoT Edge Devices?</summary>

- *Hawzhin Mohammed, Tolulope A. Odetola, Syed Rafay Hasan*

- `2006.09276v1` - [abs](http://arxiv.org/abs/2006.09276v1) - [pdf](http://arxiv.org/pdf/2006.09276v1)

> Convolutional Neural Networks (CNN) has found successful adoption in many applications. The deployment of CNN on resource-constrained edge devices have proved challenging. CNN distributed deployment across different edge devices has been adopted. In this paper, we propose Trojan attacks on CNN deployed across a distributed edge network across different nodes. We propose five stealthy attack scenarios for distributed CNN inference. These attacks are divided into trigger and payload circuitry. These attacks are tested on deep learning models (LeNet, AlexNet). The results show how the degree of vulnerability of individual layers and how critical they are to the final classification.

</details>

<details>

<summary>2020-06-17 10:42:46 - Never Trust Your Victim: Weaponizing Vulnerabilities in Security Scanners</summary>

- *Andrea Valenza, Gabriele Costa, Alessandro Armando*

- `2006.09769v1` - [abs](http://arxiv.org/abs/2006.09769v1) - [pdf](http://arxiv.org/pdf/2006.09769v1)

> The first step of every attack is reconnaissance, i.e., to acquire information about the target. A common belief is that there is almost no risk in scanning a target from a remote location. In this paper we falsify this belief by showing that scanners are exposed to the same risks as their targets. Our methodology is based on a novel attacker model where the scan author becomes the victim of a counter-strike. We developed a working prototype, called RevOK, and we applied it to 78 scanning systems. Out of them, 36 were found vulnerable to XSS. Remarkably, RevOK also found a severe vulnerability in Metasploit Pro, a mainstream penetration testing tool.

</details>

<details>

<summary>2020-06-17 12:29:14 - Frankenstein: Advanced Wireless Fuzzing to Exploit New Bluetooth Escalation Targets</summary>

- *Jan Ruge, Jiska Classen, Francesco Gringoli, Matthias Hollick*

- `2006.09809v1` - [abs](http://arxiv.org/abs/2006.09809v1) - [pdf](http://arxiv.org/pdf/2006.09809v1)

> Wireless communication standards and implementations have a troubled history regarding security. Since most implementations and firmwares are closed-source, fuzzing remains one of the main methods to uncover Remote Code Execution (RCE) vulnerabilities in deployed systems. Generic over-the-air fuzzing suffers from several shortcomings, such as constrained speed, limited repeatability, and restricted ability to debug. In this paper, we present Frankenstein, a fuzzing framework based on advanced firmware emulation, which addresses these shortcomings. Frankenstein brings firmware dumps "back to life", and provides fuzzed input to the chip's virtual modem. The speed-up of our new fuzzing method is sufficient to maintain interoperability with the attached operating system, hence triggering realistic full-stack behavior. We demonstrate the potential of Frankenstein by finding three zero-click vulnerabilities in the Broadcom and Cypress Bluetooth stack, which is used in most Apple devices, many Samsung smartphones, the Raspberry Pis, and many others.   Given RCE on a Bluetooth chip, attackers may escalate their privileges beyond the chip's boundary. We uncover a Wi-Fi/Bluetooth coexistence issue that crashes multiple operating system kernels and a design flaw in the Bluetooth 5.2 specification that allows link key extraction from the host. Turning off Bluetooth will not fully disable the chip, making it hard to defend against RCE attacks. Moreover, when testing our chip-based vulnerabilities on those devices, we find BlueFrag, a chip-independent Android RCE.

</details>

<details>

<summary>2020-06-18 01:55:53 - EnclaveDom: Privilege Separation for Large-TCB Applications in Trusted Execution Environments</summary>

- *Marcela S. Melara, Michael J. Freedman, Mic Bowman*

- `1907.13245v2` - [abs](http://arxiv.org/abs/1907.13245v2) - [pdf](http://arxiv.org/pdf/1907.13245v2)

> Trusted executions environments (TEEs) such as Intel(R) SGX provide hardware-isolated execution areas in memory, called enclaves. By running only the most trusted application components in the enclave, TEEs enable developers to minimize the TCB of their applications thereby helping to protect sensitive application data. However, porting existing applications to TEEs often requires considerable refactoring efforts, as TEEs provide a restricted interface to standard OS features. To ease development efforts, TEE application developers often choose to run their unmodified application in a library OS container that provides a full in-enclave OS interface. Yet, this large-TCB development approach now leaves sensitive in-enclave data exposed to potential bugs or vulnerabilities in third-party code imported into the application. Importantly, because the TEE libOS and the application run in the same enclave address space, even the libOS management data structures (e.g. file descriptor table) may be vulnerable to attack, where in traditional OSes these data structures may be protected via privilege isolation.   We present EnclaveDom, a privilege separation system for large-TCB TEE applications that partitions an enclave into tagged memory regions, and enforces per-region access rules at the granularity of individual in-enclave functions. EnclaveDom is implemented on Intel SGX using Memory Protection Keys (MPK) for memory tagging. To evaluate the security and performance impact of EnclaveDom, we integrated EnclaveDom with the Graphene-SGX library OS. While no product or component can be absolutely secure, our prototype helps protect internal libOS management data structures against tampering by application-level code. At every libOS system call, EnclaveDom then only grants access to those internal data structures which the syscall needs to perform its task.

</details>

<details>

<summary>2020-06-18 14:50:08 - Lightweight Collaborative Anomaly Detection for the IoT using Blockchain</summary>

- *Yisroel Mirsky, Tomer Golomb, Yuval Elovici*

- `2006.10587v1` - [abs](http://arxiv.org/abs/2006.10587v1) - [pdf](http://arxiv.org/pdf/2006.10587v1)

> Due to their rapid growth and deployment, the Internet of things (IoT) have become a central aspect of our daily lives. Unfortunately, IoT devices tend to have many vulnerabilities which can be exploited by an attacker. Unsupervised techniques, such as anomaly detection, can be used to secure these devices in a plug-and-protect manner.   However, anomaly detection models must be trained for a long time in order to capture all benign behaviors. Furthermore, the anomaly detection model is vulnerable to adversarial attacks since, during the training phase, all observations are assumed to be benign. In this paper, we propose (1) a novel approach for anomaly detection and (2) a lightweight framework that utilizes the blockchain to ensemble an anomaly detection model in a distributed environment.   Blockchain framework incrementally updates a trusted anomaly detection model via self-attestation and consensus among the IoT devices. We evaluate our method on a distributed IoT simulation platform, which consists of 48 Raspberry Pis. The simulation demonstrates how the approach can enhance the security of each device and the security of the network as a whole.

</details>

<details>

<summary>2020-06-19 02:06:15 - Probabilistic Safety for Bayesian Neural Networks</summary>

- *Matthew Wicker, Luca Laurenti, Andrea Patane, Marta Kwiatkowska*

- `2004.10281v2` - [abs](http://arxiv.org/abs/2004.10281v2) - [pdf](http://arxiv.org/pdf/2004.10281v2)

> We study probabilistic safety for Bayesian Neural Networks (BNNs) under adversarial input perturbations. Given a compact set of input points, $T \subseteq \mathbb{R}^m$, we study the probability w.r.t. the BNN posterior that all the points in $T$ are mapped to the same region $S$ in the output space. In particular, this can be used to evaluate the probability that a network sampled from the BNN is vulnerable to adversarial attacks. We rely on relaxation techniques from non-convex optimization to develop a method for computing a lower bound on probabilistic safety for BNNs, deriving explicit procedures for the case of interval and linear function propagation techniques. We apply our methods to BNNs trained on a regression task, airborne collision avoidance, and MNIST, empirically showing that our approach allows one to certify probabilistic safety of BNNs with millions of parameters.

</details>

<details>

<summary>2020-06-19 08:07:09 - Adversarial Attacks for Multi-view Deep Models</summary>

- *Xuli Sun, Shiliang Sun*

- `2006.11004v1` - [abs](http://arxiv.org/abs/2006.11004v1) - [pdf](http://arxiv.org/pdf/2006.11004v1)

> Recent work has highlighted the vulnerability of many deep machine learning models to adversarial examples. It attracts increasing attention to adversarial attacks, which can be used to evaluate the security and robustness of models before they are deployed. However, to our best knowledge, there is no specific research on the adversarial attacks for multi-view deep models. This paper proposes two multi-view attack strategies, two-stage attack (TSA) and end-to-end attack (ETEA). With the mild assumption that the single-view model on which the target multi-view model is based is known, we first propose the TSA strategy. The main idea of TSA is to attack the multi-view model with adversarial examples generated by attacking the associated single-view model, by which state-of-the-art single-view attack methods are directly extended to the multi-view scenario. Then we further propose the ETEA strategy when the multi-view model is provided publicly. The ETEA is applied to accomplish direct attacks on the target multi-view model, where we develop three effective multi-view attack methods. Finally, based on the fact that adversarial examples generalize well among different models, this paper takes the adversarial attack on the multi-view convolutional neural network as an example to validate that the effectiveness of the proposed multi-view attacks. Extensive experimental results demonstrate that our multi-view attack strategies are capable of attacking the multi-view deep models, and we additionally find that multi-view models are more robust than single-view models.

</details>

<details>

<summary>2020-06-19 08:12:25 - Towards an Adversarially Robust Normalization Approach</summary>

- *Muhammad Awais, Fahad Shamshad, Sung-Ho Bae*

- `2006.11007v1` - [abs](http://arxiv.org/abs/2006.11007v1) - [pdf](http://arxiv.org/pdf/2006.11007v1)

> Batch Normalization (BatchNorm) is effective for improving the performance and accelerating the training of deep neural networks. However, it has also shown to be a cause of adversarial vulnerability, i.e., networks without it are more robust to adversarial attacks. In this paper, we investigate how BatchNorm causes this vulnerability and proposed new normalization that is robust to adversarial attacks. We first observe that adversarial images tend to shift the distribution of BatchNorm input, and this shift makes train-time estimated population statistics inaccurate. We hypothesize that these inaccurate statistics make models with BatchNorm more vulnerable to adversarial attacks. We prove our hypothesis by replacing train-time estimated statistics with statistics calculated from the inference-time batch. We found that the adversarial vulnerability of BatchNorm disappears if we use these statistics. However, without estimated batch statistics, we can not use BatchNorm in the practice if large batches of input are not available. To mitigate this, we propose Robust Normalization (RobustNorm); an adversarially robust version of BatchNorm. We experimentally show that models trained with RobustNorm perform better in adversarial settings while retaining all the benefits of BatchNorm. Code is available at \url{https://github.com/awaisrauf/RobustNorm}.

</details>

<details>

<summary>2020-06-19 11:25:36 - Differentiable Language Model Adversarial Attacks on Categorical Sequence Classifiers</summary>

- *I. Fursov, A. Zaytsev, N. Kluchnikov, A. Kravchenko, E. Burnaev*

- `2006.11078v1` - [abs](http://arxiv.org/abs/2006.11078v1) - [pdf](http://arxiv.org/pdf/2006.11078v1)

> An adversarial attack paradigm explores various scenarios for the vulnerability of deep learning models: minor changes of the input can force a model failure. Most of the state of the art frameworks focus on adversarial attacks for images and other structured model inputs, but not for categorical sequences models.   Successful attacks on classifiers of categorical sequences are challenging because the model input is tokens from finite sets, so a classifier score is non-differentiable with respect to inputs, and gradient-based attacks are not applicable. Common approaches deal with this problem working at a token level, while the discrete optimization problem at hand requires a lot of resources to solve.   We instead use a fine-tuning of a language model for adversarial attacks as a generator of adversarial examples. To optimize the model, we define a differentiable loss function that depends on a surrogate classifier score and on a deep learning model that evaluates approximate edit distance. So, we control both the adversability of a generated sequence and its similarity to the initial sequence.   As a result, we obtain semantically better samples. Moreover, they are resistant to adversarial training and adversarial detectors. Our model works for diverse datasets on bank transactions, electronic health records, and NLP datasets.

</details>

<details>

<summary>2020-06-19 17:35:03 - Counting Risk Increments to Make Decisions During an Epidemic</summary>

- *Lucien Hardy*

- `2006.11244v1` - [abs](http://arxiv.org/abs/2006.11244v1) - [pdf](http://arxiv.org/pdf/2006.11244v1)

> I propose a smartphone app that will allow people to participate in the management of their own safety during an epidemic or pandemic such as COVID-19 by enabling them to view, in advance, the risks they would take if they visit some given venue (a cafe, the gym, the workplace, the park,...) and, furthermore, track the accumulation of such risks during the course of any given day or week. This idea can be presented to users of the app as counting points. One point represents some constant probability, $p_\text{point}$, of infection. Then the app would work in a similar way to a calorie counting app (instead of counting calories we count probability increments of being infected). Government could set a maximum recommended number of daily (or weekly) points available to each user in accord with its objectives (bringing the disease under control, allowing essential workers to work, protecting vulnerable individuals, ...). It is posited that this, along with other proposed "levers" would allow government to manage a gradual transition to normalcy. I discuss a circuit framework with wires running between boxes. In this framework the wires represent possible sources of infection, namely individuals and the venues themselves (through deposits of pathogens left at the venue). The boxes represent interactions of these sources (when individuals visit a venue). This circuit framework allows (i) calculation of points cost for visiting venues and (ii) probabilistic contact tracing. The points systems proposed here could complement existing contact tracing apps by adding functionality to permit users to participate in decision making up front.

</details>

<details>

<summary>2020-06-20 00:25:07 - MALOnt: An Ontology for Malware Threat Intelligence</summary>

- *Nidhi Rastogi, Sharmishtha Dutta, Mohammed J. Zaki, Alex Gittens, Charu Aggarwal*

- `2006.11446v1` - [abs](http://arxiv.org/abs/2006.11446v1) - [pdf](http://arxiv.org/pdf/2006.11446v1)

> Malware threat intelligence uncovers deep information about malware, threat actors, and their tactics, Indicators of Compromise(IoC), and vulnerabilities in different platforms from scattered threat sources. This collective information can guide decision making in cyber defense applications utilized by security operation centers(SoCs). In this paper, we introduce an open-source malware ontology - MALOnt that allows the structured extraction of information and knowledge graph generation, especially for threat intelligence. The knowledge graph that uses MALOnt is instantiated from a corpus comprising hundreds of annotated malware threat reports. The knowledge graph enables the analysis, detection, classification, and attribution of cyber threats caused by malware. We also demonstrate the annotation process using MALOnt on exemplar threat intelligence reports. A work in progress, this research is part of a larger effort towards auto-generation of knowledge graphs (KGs)for gathering malware threat intelligence from heterogeneous online resources.

</details>

<details>

<summary>2020-06-20 09:58:07 - Game-Theoretical Analysis of Mining Strategy for Bitcoin-NG Blockchain Protocol</summary>

- *Taotao Wang, Xiaoqian Bai, Hao Wang, Soung Chang Liew, Shengli Zhang*

- `1911.00900v3` - [abs](http://arxiv.org/abs/1911.00900v3) - [pdf](http://arxiv.org/pdf/1911.00900v3)

> Bitcoin-NG, a scalable blockchain protocol, divides each block into a key block and many micro blocks to effectively improve the transaction processing capacity. Bitcoin-NG has a special incentive mechanism (i.e. splitting transaction fees to the current and the next leader) to maintain its security. However, this design of the incentive mechanism ignores the joint effect of transaction fees, mint coins and mining duration lengths on the expected mining reward. In this paper, we identify the advanced mining attack that deliberately ignores micro blocks to enlarge the mining duration length to increase the likelihood of winning the mining race. We first show that an advanced mining attacker can maximize its expected reward by optimizing its mining duration length. We then formulate a game-theoretical model in which multiple mining players perform advanced mining to compete with each other. We analyze the Nash equilibrium for the mining game. Our analytical and simulation results indicate that all mining players in the mining game converge to having advanced mining at the equilibrium and have no incentives for deviating from the equilibrium; the transaction processing capability of the Bitcoin-NG network at the equilibrium is decreased by advanced mining. Therefore, we conclude that the Bitcoin-NG blockchain protocol is vulnerable to advanced mining attack. We discuss how to reduce the negative impact of advanced mining for Bitcoin-NG.

</details>

<details>

<summary>2020-06-20 13:44:48 - Regularisation Can Mitigate Poisoning Attacks: A Novel Analysis Based on Multiobjective Bilevel Optimisation</summary>

- *Javier Carnerero-Cano, Luis Muñoz-González, Phillippa Spencer, Emil C. Lupu*

- `2003.00040v2` - [abs](http://arxiv.org/abs/2003.00040v2) - [pdf](http://arxiv.org/pdf/2003.00040v2)

> Machine Learning (ML) algorithms are vulnerable to poisoning attacks, where a fraction of the training data is manipulated to deliberately degrade the algorithms' performance. Optimal poisoning attacks, which can be formulated as bilevel optimisation problems, help to assess the robustness of learning algorithms in worst-case scenarios. However, current attacks against algorithms with hyperparameters typically assume that these hyperparameters remain constant ignoring the effect the attack has on them. We show that this approach leads to an overly pessimistic view of the robustness of the algorithms. We propose a novel optimal attack formulation that considers the effect of the attack on the hyperparameters by modelling the attack as a multiobjective bilevel optimisation problem. We apply this novel attack formulation to ML classifiers using $L_2$ regularisation and show that, in contrast to results previously reported, $L_2$ regularisation enhances the stability of the learning algorithms and helps to mitigate the attacks. Our empirical evaluation on different datasets confirms the limitations of previous strategies, evidences the benefits of using $L_2$ regularisation to dampen the effect of poisoning attacks and shows how the regularisation hyperparameter increases with the fraction of poisoning points.

</details>

<details>

<summary>2020-06-20 17:39:23 - FaceHack: Triggering backdoored facial recognition systems using facial characteristics</summary>

- *Esha Sarkar, Hadjer Benkraouda, Michail Maniatakos*

- `2006.11623v1` - [abs](http://arxiv.org/abs/2006.11623v1) - [pdf](http://arxiv.org/pdf/2006.11623v1)

> Recent advances in Machine Learning (ML) have opened up new avenues for its extensive use in real-world applications. Facial recognition, specifically, is used from simple friend suggestions in social-media platforms to critical security applications for biometric validation in automated immigration at airports. Considering these scenarios, security vulnerabilities to such ML algorithms pose serious threats with severe outcomes. Recent work demonstrated that Deep Neural Networks (DNNs), typically used in facial recognition systems, are susceptible to backdoor attacks; in other words,the DNNs turn malicious in the presence of a unique trigger. Adhering to common characteristics for being unnoticeable, an ideal trigger is small, localized, and typically not a part of the main im-age. Therefore, detection mechanisms have focused on detecting these distinct trigger-based outliers statistically or through their reconstruction. In this work, we demonstrate that specific changes to facial characteristics may also be used to trigger malicious behavior in an ML model. The changes in the facial attributes maybe embedded artificially using social-media filters or introduced naturally using movements in facial muscles. By construction, our triggers are large, adaptive to the input, and spread over the entire image. We evaluate the success of the attack and validate that it does not interfere with the performance criteria of the model. We also substantiate the undetectability of our triggers by exhaustively testing them with state-of-the-art defenses.

</details>

<details>

<summary>2020-06-20 18:01:16 - Defense against Adversarial Attacks in NLP via Dirichlet Neighborhood Ensemble</summary>

- *Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-wei Chang, Xuanjing Huang*

- `2006.11627v1` - [abs](http://arxiv.org/abs/2006.11627v1) - [pdf](http://arxiv.org/pdf/2006.11627v1)

> Despite neural networks have achieved prominent performance on many natural language processing (NLP) tasks, they are vulnerable to adversarial examples. In this paper, we propose Dirichlet Neighborhood Ensemble (DNE), a randomized smoothing method for training a robust model to defense substitution-based attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models for NLP applications. We demonstrate through extensive experimentation that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.

</details>

<details>

<summary>2020-06-20 20:09:03 - Security Smells in Ansible and Chef Scripts: A Replication Study</summary>

- *Akond Rahman, Md. Rayhanur Rahman, Chris Parnin, Laurie Williams*

- `1907.07159v2` - [abs](http://arxiv.org/abs/1907.07159v2) - [pdf](http://arxiv.org/pdf/1907.07159v2)

> Context: Security smells are recurring coding patterns that are indicative of security weakness, and require further inspection. As infrastructure as code (IaC) scripts, such as Ansible and Chef scripts, are used to provision cloud-based servers and systems at scale, security smells in IaC scripts could be used to enable malicious users to exploit vulnerabilities in the provisioned systems. Goal: The goal of this paper is to help practitioners avoid insecure coding practices while developing infrastructure as code scripts through an empirical study of security smells in Ansible and Chef scripts. Methodology: We conduct a replication study where we apply qualitative analysis with 1,956 IaC scripts to identify security smells for IaC scripts written in two languages: Ansible and Chef. We construct a static analysis tool called Security Linter for Ansible and Chef scripts (SLAC) to automatically identify security smells in 50,323 scripts collected from 813 open source software repositories. We also submit bug reports for 1,000 randomly-selected smell occurrences. Results: We identify two security smells not reported in prior work: missing default in case statement and no integrity check. By applying SLAC we identify 46,600 occurrences of security smells that include 7,849 hard-coded passwords. We observe agreement for 65 of the responded 94 bug reports, which suggests the relevance of security smells for Ansible and Chef scripts amongst practitioners. Conclusion: We observe security smells to be prevalent in Ansible and Chef scripts, similar to that of the Puppet scripts. We recommend practitioners to rigorously inspect the presence of the identified security smells in Ansible and Chef scripts using (i) code review, and (ii) static analysis tools.

</details>

<details>

<summary>2020-06-22 08:12:38 - An In-Depth Security Assessment of Maritime Container Terminal Software Systems</summary>

- *Joseph O. Eichenhofer, Elisa Heymann, Barton P. Miller, Arnold Kang*

- `2006.12056v1` - [abs](http://arxiv.org/abs/2006.12056v1) - [pdf](http://arxiv.org/pdf/2006.12056v1)

> Attacks on software systems occur world-wide on a daily basis targeting individuals, corporations, and governments alike. The systems that facilitate maritime shipping are at risk of serious disruptions, and these disruptions can stem from vulnerabilities in the software and processes used in these systems. These vulnerabilities leave such systems open to cyber-attack. Assessments of the security of maritime shipping systems have focused on identifying risks but have not taken the critical (and expensive) next step of actually identifying vulnerabilities present in these systems. While such risk assessments are important, they have not provided the detailed identification of security issues in the systems that control these ports and their terminals. In response, we formed a key collaboration between an experienced academic cybersecurity team and a well-known commercial software provider that manages maritime shipping. We performed an analysis of the information flow involved in the maritime shipping process, and then executed an in-depth vulnerability assessment of the software that manages freight systems. In this paper, we show the flow of information involved in the freight shipping process and explain how we performed the in-depth assessment, summarizing our findings. Like every large software system, maritime shipping systems have vulnerabilities.

</details>

<details>

<summary>2020-06-23 00:14:15 - Category-wise Attack: Transferable Adversarial Examples for Anchor Free Object Detection</summary>

- *Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song, Xi Wu*

- `2003.04367v4` - [abs](http://arxiv.org/abs/2003.04367v4) - [pdf](http://arxiv.org/pdf/2003.04367v4)

> Deep neural networks have been demonstrated to be vulnerable to adversarial attacks: subtle perturbations can completely change the classification results. Their vulnerability has led to a surge of research in this direction. However, most works dedicated to attacking anchor-based object detection models. In this work, we aim to present an effective and efficient algorithm to generate adversarial examples to attack anchor-free object models based on two approaches. First, we conduct category-wise instead of instance-wise attacks on the object detectors. Second, we leverage the high-level semantic information to generate the adversarial examples. Surprisingly, the generated adversarial examples it not only able to effectively attack the targeted anchor-free object detector but also to be transferred to attack other object detectors, even anchor-based detectors such as Faster R-CNN.

</details>

<details>

<summary>2020-06-23 04:37:24 - Visor: Privacy-Preserving Video Analytics as a Cloud Service</summary>

- *Rishabh Poddar, Ganesh Ananthanarayanan, Srinath Setty, Stavros Volos, Raluca Ada Popa*

- `2006.09628v2` - [abs](http://arxiv.org/abs/2006.09628v2) - [pdf](http://arxiv.org/pdf/2006.09628v2)

> Video-analytics-as-a-service is becoming an important offering for cloud providers. A key concern in such services is privacy of the videos being analyzed. While trusted execution environments (TEEs) are promising options for preventing the direct leakage of private video content, they remain vulnerable to side-channel attacks.   We present Visor, a system that provides confidentiality for the user's video stream as well as the ML models in the presence of a compromised cloud platform and untrusted co-tenants. Visor executes video pipelines in a hybrid TEE that spans both the CPU and GPU. It protects the pipeline against side-channel attacks induced by data-dependent access patterns of video modules, and also addresses leakage in the CPU-GPU communication channel. Visor is up to $1000\times$ faster than na\"ive oblivious solutions, and its overheads relative to a non-oblivious baseline are limited to $2\times$--$6\times$.

</details>

<details>

<summary>2020-06-23 09:34:17 - AdvJND: Generating Adversarial Examples with Just Noticeable Difference</summary>

- *Zifei Zhang, Kai Qiao, Lingyun Jiang, Linyuan Wang, Bin Yan*

- `2002.00179v2` - [abs](http://arxiv.org/abs/2002.00179v2) - [pdf](http://arxiv.org/pdf/2002.00179v2)

> Compared with traditional machine learning models, deep neural networks perform better, especially in image classification tasks. However, they are vulnerable to adversarial examples. Adding small perturbations on examples causes a good-performance model to misclassify the crafted examples, without category differences in the human eyes, and fools deep models successfully. There are two requirements for generating adversarial examples: the attack success rate and image fidelity metrics. Generally, perturbations are increased to ensure the adversarial examples' high attack success rate; however, the adversarial examples obtained have poor concealment. To alleviate the tradeoff between the attack success rate and image fidelity, we propose a method named AdvJND, adding visual model coefficients, just noticeable difference coefficients, in the constraint of a distortion function when generating adversarial examples. In fact, the visual subjective feeling of the human eyes is added as a priori information, which decides the distribution of perturbations, to improve the image quality of adversarial examples. We tested our method on the FashionMNIST, CIFAR10, and MiniImageNet datasets. Adversarial examples generated by our AdvJND algorithm yield gradient distributions that are similar to those of the original inputs. Hence, the crafted noise can be hidden in the original inputs, thus improving the attack concealment significantly.

</details>

<details>

<summary>2020-06-23 14:58:58 - A Deep Learning Pipeline for Patient Diagnosis Prediction Using Electronic Health Records</summary>

- *Leopold Franz, Yash Raj Shrestha, Bibek Paudel*

- `2006.16926v1` - [abs](http://arxiv.org/abs/2006.16926v1) - [pdf](http://arxiv.org/pdf/2006.16926v1)

> Augmentation of disease diagnosis and decision-making in healthcare with machine learning algorithms is gaining much impetus in recent years. In particular, in the current epidemiological situation caused by COVID-19 pandemic, swift and accurate prediction of disease diagnosis with machine learning algorithms could facilitate identification and care of vulnerable clusters of population, such as those having multi-morbidity conditions. In order to build a useful disease diagnosis prediction system, advancement in both data representation and development of machine learning architectures are imperative. First, with respect to data collection and representation, we face severe problems due to multitude of formats and lack of coherency prevalent in Electronic Health Records (EHRs). This causes hindrance in extraction of valuable information contained in EHRs. Currently, no universal global data standard has been established. As a useful solution, we develop and publish a Python package to transform public health dataset into an easy to access universal format. This data transformation to an international health data format facilitates researchers to easily combine EHR datasets with clinical datasets of diverse formats. Second, machine learning algorithms that predict multiple disease diagnosis categories simultaneously remain underdeveloped. We propose two novel model architectures in this regard. First, DeepObserver, which uses structured numerical data to predict the diagnosis categories and second, ClinicalBERT_Multi, that incorporates rich information available in clinical notes via natural language processing methods and also provides interpretable visualizations to medical practitioners. We show that both models can predict multiple diagnoses simultaneously with high accuracy.

</details>

<details>

<summary>2020-06-24 00:26:37 - CoinPolice:Detecting Hidden Cryptojacking Attacks with Neural Networks</summary>

- *Ivan Petrov, Luca Invernizzi, Elie Bursztein*

- `2006.10861v2` - [abs](http://arxiv.org/abs/2006.10861v2) - [pdf](http://arxiv.org/pdf/2006.10861v2)

> Traffic monetization is a crucial component of running most for-profit online businesses. One of its latest incarnations is cryptocurrency mining, where a website instructs the visitor's browser to participate in building a cryptocurrency ledger (e.g., Bitcoin, Monero) in exchange for a small reward in the same currency. In its essence, this practice trades the user's electric bill (or battery level) for cryptocurrency. With user consent, this exchange can be a legitimate funding source - for example, UNICEF has collected over 27k charity donations on a website dedicated to this purpose, thehopepage.org. Regrettably, this practice also easily lends itself to abuse: in this form, called cryptojacking, attacks surreptitiously mine in the users browser, and profits are collected either by website owners or by hackers that planted the mining script into a vulnerable page. Cryptojackers have been bettering their evasion techniques, incorporating in their toolkits domain fluxing, content obfuscation, the use of WebAssembly, and throttling. Whereas most state-of-the-art defenses address multiple of these evasion techniques, none is resistant against all. In this paper, we offer a novel detection method, CoinPolice, that is robust against all of the aforementioned evasion techniques. CoinPolice flips throttling against cryptojackers, artificially varying the browser's CPU power to observe the presence of throttling. Based on a deep neural network classifier, CoinPolice can detect 97.87% of hidden miners with a low false positive rate (0.74%). We compare CoinPolice performance with the current state of the art and show our approach outperforms it when detecting aggressively throttled miners. Finally, we deploy Coinpolice to perform the largest-scale cryptoming investigation to date, identifying 6700 sites that monetize traffic in this fashion.

</details>

<details>

<summary>2020-06-24 10:57:33 - Robustness of Bayesian Neural Networks to Gradient-Based Attacks</summary>

- *Ginevra Carbone, Matthew Wicker, Luca Laurenti, Andrea Patane, Luca Bortolussi, Guido Sanguinetti*

- `2002.04359v3` - [abs](http://arxiv.org/abs/2002.04359v3) - [pdf](http://arxiv.org/pdf/2002.04359v3)

> Vulnerability to adversarial attacks is one of the principal hurdles to the adoption of deep learning in safety-critical applications. Despite significant efforts, both practical and theoretical, the problem remains open. In this paper, we analyse the geometry of adversarial attacks in the large-data, overparametrized limit for Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to gradient-based attacks arises as a result of degeneracy in the data distribution, i.e., when the data lies on a lower-dimensional submanifold of the ambient space. As a direct consequence, we demonstrate that in the limit BNN posteriors are robust to gradient-based adversarial attacks. Experimental results on the MNIST and Fashion MNIST datasets with BNNs trained with Hamiltonian Monte Carlo and Variational Inference support this line of argument, showing that BNNs can display both high accuracy and robustness to gradient based adversarial attacks.

</details>

<details>

<summary>2020-06-24 11:59:36 - Exploring the Security Awareness of the Python and JavaScript Open Source Communities</summary>

- *Gábor Antal, Márton Keleti, Péter Hegedűs*

- `2006.13652v1` - [abs](http://arxiv.org/abs/2006.13652v1) - [pdf](http://arxiv.org/pdf/2006.13652v1)

> Software security is undoubtedly a major concern in today's software engineering. Although the level of awareness of security issues is often high, practical experiences show that neither preventive actions nor reactions to possible issues are always addressed properly in reality. By analyzing large quantities of commits in the open-source communities, we can categorize the vulnerabilities mitigated by the developers and study their distribution, resolution time, etc. to learn and improve security management processes and practices. With the help of the Software Heritage Graph Dataset, we investigated the commits of two of the most popular script languages -- Python and JavaScript -- projects collected from public repositories and identified those that mitigate a certain vulnerability in the code (i.e. vulnerability resolution commits). On the one hand, we identified the types of vulnerabilities (in terms of CWE groups) referred to in commit messages and compared their numbers within the two communities. On the other hand, we examined the average time elapsing between the publish date of a vulnerability and the first reference to it in a commit. We found that there is a large intersection in the vulnerability types mitigated by the two communities, but most prevalent vulnerabilities are specific to language. Moreover, neither the JavaScript nor the Python community reacts very fast to appearing security vulnerabilities in general with only a couple of exceptions for certain CWE groups.

</details>

<details>

<summary>2020-06-24 13:55:50 - Exploring Software Naturalness through Neural Language Models</summary>

- *Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui Zheng, Gaetano Rossiello, Alessandro Morari, Jim Laredo, Veronika Thost, Yufan Zhuang, Giacomo Domeniconi*

- `2006.12641v2` - [abs](http://arxiv.org/abs/2006.12641v2) - [pdf](http://arxiv.org/pdf/2006.12641v2)

> The Software Naturalness hypothesis argues that programming languages can be understood through the same techniques used in natural language processing. We explore this hypothesis through the use of a pre-trained transformer-based language model to perform code analysis tasks. Present approaches to code analysis depend heavily on features derived from the Abstract Syntax Tree (AST) while our transformer-based language models work on raw source code. This work is the first to investigate whether such language models can discover AST features automatically. To achieve this, we introduce a sequence labeling task that directly probes the language models understanding of AST. Our results show that transformer based language models achieve high accuracy in the AST tagging task. Furthermore, we evaluate our model on a software vulnerability identification task. Importantly, we show that our approach obtains vulnerability identification results comparable to graph based approaches that rely heavily on compilers for feature extraction.

</details>

<details>

<summary>2020-06-24 15:38:16 - Lightweight Cryptography for IoT: A State-of-the-Art</summary>

- *Vishal A. Thakor, M. A. Razzaque, Muhammad R. A. Khandaker*

- `2006.13813v1` - [abs](http://arxiv.org/abs/2006.13813v1) - [pdf](http://arxiv.org/pdf/2006.13813v1)

> With the emergence of 5G, Internet of Things (IoT) has become a center of attraction for almost all industries due to its wide range of applications from various domains. The explosive growth of industrial control processes and the industrial IoT, imposes unprecedented vulnerability to cyber threats in critical infrastructure through the interconnected systems. This new security threats could be minimized by lightweight cryptography, a sub-branch of cryptography, especially derived for resource-constrained devices such as RFID tags, smart cards, wireless sensors, etc. More than four dozens of lightweight cryptography algorithms have been proposed, designed for specific application(s). These algorithms exhibit diverse hardware and software performances in different circumstances. This paper presents the performance comparison along with their reported cryptanalysis, mainly for lightweight block ciphers, and further shows new research directions to develop novel algorithms with right balance of cost, performance and security characteristics.

</details>

<details>

<summary>2020-06-24 17:01:59 - Printing and Scanning Attack for Image Counter Forensics</summary>

- *Hailey Joren, Otkrist Gupta, Dan Raviv*

- `2005.02160v2` - [abs](http://arxiv.org/abs/2005.02160v2) - [pdf](http://arxiv.org/pdf/2005.02160v2)

> Examining the authenticity of images has become increasingly important as manipulation tools become more accessible and advanced. Recent work has shown that while CNN-based image manipulation detectors can successfully identify manipulations, they are also vulnerable to adversarial attacks, ranging from simple double JPEG compression to advanced pixel-based perturbation. In this paper we explore another method of highly plausible attack: printing and scanning. We demonstrate the vulnerability of two state-of-the-art models to this type of attack. We also propose a new machine learning model that performs comparably to these state-of-the-art models when trained and validated on printed and scanned images. Of the three models, our proposed model outperforms the others when trained and validated on images from a single printer. To facilitate this exploration, we create a dataset of over 6,000 printed and scanned image blocks. Further analysis suggests that variation between images produced from different printers is significant, large enough that good validation accuracy on images from one printer does not imply similar validation accuracy on identical images from a different printer.

</details>

<details>

<summary>2020-06-24 19:55:33 - Deep-CAPTCHA: a deep learning based CAPTCHA solver for vulnerability assessment</summary>

- *Zahra Noury, Mahdi Rezaei*

- `2006.08296v2` - [abs](http://arxiv.org/abs/2006.08296v2) - [pdf](http://arxiv.org/pdf/2006.08296v2)

> CAPTCHA is a human-centred test to distinguish a human operator from bots, attacking programs, or other computerised agents that tries to imitate human intelligence. In this research, we investigate a way to crack visual CAPTCHA tests by an automated deep learning based solution. The goal of this research is to investigate the weaknesses and vulnerabilities of the CAPTCHA generator systems; hence, developing more robust CAPTCHAs, without taking the risks of manual try and fail efforts. We develop a Convolutional Neural Network called Deep-CAPTCHA to achieve this goal. The proposed platform is able to investigate both numerical and alphanumerical CAPTCHAs. To train and develop an efficient model, we have generated a dataset of 500,000 CAPTCHAs to train our model. In this paper, we present our customised deep neural network model, we review the research gaps, the existing challenges, and the solutions to cope with the issues. Our network's cracking accuracy leads to a high rate of 98.94% and 98.31% for the numerical and the alpha-numerical test datasets, respectively. That means more works is required to develop robust CAPTCHAs, to be non-crackable against automated artificial agents. As the outcome of this research, we identify some efficient techniques to improve the security of the CAPTCHAs, based on the performance analysis conducted on the Deep-CAPTCHA model.

</details>

<details>

<summary>2020-06-25 08:23:45 - Usability, Accessibility and Web Security Assessment of E-government Websites in Tanzania</summary>

- *Noe Elisa*

- `2006.14245v1` - [abs](http://arxiv.org/abs/2006.14245v1) - [pdf](http://arxiv.org/pdf/2006.14245v1)

> In spite of the fact that e-government agency (ega) in Tanzania emphasize on the use of ICT within public institutions in Tanzania, accessibility, usability and web security vulnerabilities are still not considered by the majority of web developers. The main objective of this study is to assess the usability, accessibility and web security vulnerabilities of selected Tanzania e-government websites. Using several automatic diagnostic (evaluation) tools such as pingdom, google speed insight, wave, w3c checker and acunetix, this study assess the usability, accessibility and web security vulnerabilities of 79 selected e-government websites in Tanzania. The results reveal several issues on usability, accessibility and security of Tanzania e-government websites. There is high number of usability problems where 100% of websites were found to have broken links and 52 out of 79 websites have loading time of more than five (5) seconds for their main page. The accessibility results show that all 79 selected websites have accessibility errors and violate w3c Web Content Accessibility Guidelines (WCAG) 1.0. The results on web security vulnerabilities indicate that 40 out of 79 (50.6%) assessed websites have one or more high-severity vulnerability (SQL injection or cross site scripting-XSS) while 51 out of 79 (64.5%) have one or more medium-severity vulnerabilities (Cross site request forgery or Denial of Service). Based on these results, this study provides some recommendations for improving the usability, accessibility and web security vulnerabilities of public institutions in Tanzania.

</details>

<details>

<summary>2020-06-25 09:43:44 - Combining Ensemble Kalman Filter and Reservoir Computing to predict spatio-temporal chaotic systems from imperfect observations and models</summary>

- *Futo Tomizawa, Yohei Sawada*

- `2006.14276v1` - [abs](http://arxiv.org/abs/2006.14276v1) - [pdf](http://arxiv.org/pdf/2006.14276v1)

> Prediction of spatio-temporal chaotic systems is important in various fields, such as Numerical Weather Prediction (NWP). While data assimilation methods have been applied in NWP, machine learning techniques, such as Reservoir Computing (RC), are recently recognized as promising tools to predict spatio-temporal chaotic systems. However, the sensitivity of the skill of the machine learning based prediction to the imperfectness of observations is unclear. In this study, we evaluate the skill of RC with noisy and sparsely distributed observations. We intensively compare the performances of RC and Local Ensemble Transform Kalman Filter (LETKF) by applying them to the prediction of the Lorenz 96 system. Although RC can successfully predict the Lorenz 96 system if the system is perfectly observed, we find that RC is vulnerable to observation sparsity compared with LETKF. To overcome this limitation of RC, we propose to combine LETKF and RC. In our proposed method, the system is predicted by RC that learned the analysis time series estimated by LETKF. Our proposed method can successfully predict the Lorenz 96 system using noisy and sparsely distributed observations. Most importantly, our method can predict better than LETKF when the process-based model is imperfect.

</details>

<details>

<summary>2020-06-25 20:03:17 - On the Feasibility of Exploiting Traffic Collision Avoidance System Vulnerabilities</summary>

- *Paul M. Berges, Basavesh Ammanaghatta Shivakumar, Timothy Graziano, Ryan Gerdes, Z. Berkay Celik*

- `2006.14679v1` - [abs](http://arxiv.org/abs/2006.14679v1) - [pdf](http://arxiv.org/pdf/2006.14679v1)

> Traffic Collision Avoidance Systems (TCAS) are safety-critical systems required on most commercial aircrafts in service today. However, TCAS was not designed to account for malicious actors. While in the past it may have been infeasible for an attacker to craft radio signals to mimic TCAS signals, attackers today have access to open-source digital signal processing software, like GNU Radio, and inexpensive software defined radios (SDR) that enable the transmission of spurious TCAS messages. In this paper, methods, both qualitative and quantitative, for analyzing TCAS from an adversarial perspective are presented. To demonstrate the feasibility of inducing near mid-air collisions between current day TCAS-equipped aircraft, an experimental Phantom Aircraft generator is developed using GNU Radio and an SDR against a realistic threat model.

</details>

<details>

<summary>2020-06-26 01:56:20 - CopyCat: Controlled Instruction-Level Attacks on Enclaves</summary>

- *Daniel Moghimi, Jo Van Bulck, Nadia Heninger, Frank Piessens, Berk Sunar*

- `2002.08437v3` - [abs](http://arxiv.org/abs/2002.08437v3) - [pdf](http://arxiv.org/pdf/2002.08437v3)

> The adversarial model presented by trusted execution environments (TEEs) has prompted researchers to investigate unusual attack vectors. One particularly powerful class of controlled-channel attacks abuses page-table modifications to reliably track enclave memory accesses at a page-level granularity. In contrast to noisy microarchitectural timing leakage, this line of deterministic controlled-channel attacks abuses indispensable architectural interfaces and hence cannot be mitigated by tweaking microarchitectural resources.   We propose an innovative controlled-channel attack, named CopyCat, that deterministically counts the number of instructions executed within a single enclave code page. We show that combining the instruction counts harvested by CopyCat with traditional, coarse-grained page-level leakage allows the accurate reconstruction of enclave control flow at a maximal instruction-level granularity. CopyCat can identify intra-page and intra-cache line branch decisions that ultimately may only differ in a single instruction, underscoring that even extremely subtle control flow deviations can be deterministically leaked from secure enclaves. We demonstrate the improved resolution and practicality of CopyCat on Intel SGX in an extensive study of single-trace and deterministic attacks against cryptographic implementations, and give novel algorithmic attacks to perform single-trace key extraction that exploit subtle vulnerabilities in the latest versions of widely-used cryptographic libraries. Our findings highlight the importance of stricter verification of cryptographic implementations, especially in the context of TEEs.

</details>

<details>

<summary>2020-06-26 04:01:58 - Integrating Tensor Similarity to Enhance Clustering Performance</summary>

- *Hong Peng, Yu Hu, Jiazhou Chen, Haiyan Wang, Yang Li, Hongmin Cai*

- `1905.03920v2` - [abs](http://arxiv.org/abs/1905.03920v2) - [pdf](http://arxiv.org/pdf/1905.03920v2)

> The performance of most the clustering methods hinges on the used pairwise affinity, which is usually denoted by a similarity matrix. However, the pairwise similarity is notoriously known for its vulnerability of noise contamination or the imbalance in samples or features, and thus hinders accurate clustering. To tackle this issue, we propose to use information among samples to boost the clustering performance. We proved that a simplified similarity for pairs, denoted by a fourth order tensor, equals to the Kronecker product of pairwise similarity matrices under decomposable assumption, or provide complementary information for which the pairwise similarity missed under indecomposable assumption. Then a high order similarity matrix is obtained from the tensor similarity via eigenvalue decomposition. The high order similarity capturing spatial information serves as a robust complement for the pairwise similarity. It is further integrated with the popular pairwise similarity, named by IPS2, to boost the clustering performance. Extensive experiments demonstrated that the proposed IPS2 significantly outperformed previous similarity-based methods on real-world datasets and it was capable of handling the clustering task over under-sampled and noisy datasets.

</details>

<details>

<summary>2020-06-26 08:17:44 - Poisoning Attacks on Algorithmic Fairness</summary>

- *David Solans, Battista Biggio, Carlos Castillo*

- `2004.07401v3` - [abs](http://arxiv.org/abs/2004.07401v3) - [pdf](http://arxiv.org/pdf/2004.07401v3)

> Research in adversarial machine learning has shown how the performance of machine learning models can be seriously compromised by injecting even a small fraction of poisoning points into the training data. While the effects on model accuracy of such poisoning attacks have been widely studied, their potential effects on other model performance metrics remain to be evaluated. In this work, we introduce an optimization framework for poisoning attacks against algorithmic fairness, and develop a gradient-based poisoning attack aimed at introducing classification disparities among different groups in the data. We empirically show that our attack is effective not only in the white-box setting, in which the attacker has full access to the target model, but also in a more challenging black-box scenario in which the attacks are optimized against a substitute model and then transferred to the target model. We believe that our findings pave the way towards the definition of an entirely novel set of adversarial attacks targeting algorithmic fairness in different scenarios, and that investigating such vulnerabilities will help design more robust algorithms and countermeasures in the future.

</details>

<details>

<summary>2020-06-26 08:29:05 - Orthogonal Deep Models As Defense Against Black-Box Attacks</summary>

- *Mohammad A. A. K. Jalwana, Naveed Akhtar, Mohammed Bennamoun, Ajmal Mian*

- `2006.14856v1` - [abs](http://arxiv.org/abs/2006.14856v1) - [pdf](http://arxiv.org/pdf/2006.14856v1)

> Deep learning has demonstrated state-of-the-art performance for a variety of challenging computer vision tasks. On one hand, this has enabled deep visual models to pave the way for a plethora of critical applications like disease prognostics and smart surveillance. On the other, deep learning has also been found vulnerable to adversarial attacks, which calls for new techniques to defend deep models against these attacks. Among the attack algorithms, the black-box schemes are of serious practical concern since they only need publicly available knowledge of the targeted model. We carefully analyze the inherent weakness of deep models in black-box settings where the attacker may develop the attack using a model similar to the targeted model. Based on our analysis, we introduce a novel gradient regularization scheme that encourages the internal representation of a deep model to be orthogonal to another, even if the architectures of the two models are similar. Our unique constraint allows a model to concomitantly endeavour for higher accuracy while maintaining near orthogonal alignment of gradients with respect to a reference model. Detailed empirical study verifies that controlled misalignment of gradients under our orthogonality objective significantly boosts a model's robustness against transferable black-box adversarial attacks. In comparison to regular models, the orthogonal models are significantly more robust to a range of $l_p$ norm bounded perturbations. We verify the effectiveness of our technique on a variety of large-scale models.

</details>

<details>

<summary>2020-06-26 16:07:35 - Cleaning the NVD: Comprehensive Quality Assessment, Improvements, and Analyses</summary>

- *Afsah Anwar, Ahmed Abusnaina, Songqing Chen, Frank Li, David Mohaisen*

- `2006.15074v1` - [abs](http://arxiv.org/abs/2006.15074v1) - [pdf](http://arxiv.org/pdf/2006.15074v1)

> Vulnerability databases are vital sources of information on emergent software security concerns. Security professionals, from system administrators to developers to researchers, heavily depend on these databases to track vulnerabilities and analyze security trends. How reliable and accurate are these databases though?   In this paper, we explore this question with the National Vulnerability Database (NVD), the U.S. government's repository of vulnerability information that arguably serves as the industry standard. Through a systematic investigation, we uncover inconsistent or incomplete data in the NVD that can impact its practical uses, affecting information such as the vulnerability publication dates, names of vendors and products affected, vulnerability severity scores, and vulnerability type categorizations. We explore the extent of these discrepancies and identify methods for automated corrections. Finally, we demonstrate the impact that these data issues can pose by comparing analyses using the original and our rectified versions of the NVD. Ultimately, our investigation of the NVD not only produces an improved source of vulnerability information, but also provides important insights and guidance for the security community on the curation and use of such data sources.

</details>

<details>

<summary>2020-06-27 03:54:19 - Software Enabled Security Architecture for Counteracting Attacks in Control Systems</summary>

- *Uday Tupakula, Vijay Varadharajan, Kallol Krishna Karmakar*

- `2006.15272v1` - [abs](http://arxiv.org/abs/2006.15272v1) - [pdf](http://arxiv.org/pdf/2006.15272v1)

> Increasingly Industrial Control Systems (ICS) systems are being connected to the Internet to minimise the operational costs and provide additional flexibility. These control systems such as the ones used in power grids, manufacturing and utilities operate continually and have long lifespans measured in decades rather than years as in the case of IT systems. Such industrial control systems require uninterrupted and safe operation. However, they can be vulnerable to a variety of attacks, as successful attacks on critical control infrastructures could have devastating consequences to the safety of human lives as well as a nation's security and prosperity. Furthermore, there can be a range of attacks that can target ICS and it is not easy to secure these systems against all known attacks let alone unknown ones. In this paper, we propose a software enabled security architecture using Software Defined Networking (SDN) and Network Function Virtualisation (NFV) that can enhance the capability to secure industrial control systems. We have designed such an SDN/NFV enabled security architecture and developed a Control System Security Application (CSSA) in SDN Controller for enhancing security in ICS against certain specific attacks namely denial of service attacks, from unpatched vulnerable control system components and securing the communication flows from the legacy devices that do not support any security functionality. In this paper, we discuss the prototype implementation of the proposed architecture and the results obtained from our analysis.

</details>

<details>

<summary>2020-06-27 03:57:48 - XI Commandments of Kubernetes Security: A Systematization of Knowledge Related to Kubernetes Security Practices</summary>

- *Md. Shazibul Islam Shamim, Farzana Ahamed Bhuiyan, Akond Rahman*

- `2006.15275v1` - [abs](http://arxiv.org/abs/2006.15275v1) - [pdf](http://arxiv.org/pdf/2006.15275v1)

> Kubernetes is an open-source software for automating management of computerized services. Organizations, such as IBM, Capital One and Adidas use Kubernetes to deploy and manage their containers, and have reported benefits related to deployment frequency. Despite reported benefits, Kubernetes deployments are susceptible to security vulnerabilities, such as those that occurred at Tesla in 2018. A systematization of Kubernetes security practices can help practitioners mitigate vulnerabilities in their Kubernetes deployments. The goal of this paper is to help practitioners in securing their Kubernetes installations through a systematization of knowledge related to Kubernetes security practices. We systematize knowledge by applying qualitative analysis on 104 Internet artifacts. We identify 11 security practices that include (i) implementation of role-based access control (RBAC) authorization to provide least privilege, (ii) applying security patches to keep Kubernetes updated, and (iii) implementing pod and network specific security policies.

</details>

<details>

<summary>2020-06-27 13:14:58 - QUANOS- Adversarial Noise Sensitivity Driven Hybrid Quantization of Neural Networks</summary>

- *Priyadarshini Panda*

- `2004.11233v2` - [abs](http://arxiv.org/abs/2004.11233v2) - [pdf](http://arxiv.org/pdf/2004.11233v2)

> Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial attacks, wherein, a model gets fooled by applying slight perturbations on the input. With the advent of Internet-of-Things and the necessity to enable intelligence in embedded devices, low-power and secure hardware implementation of DNNs is vital. In this paper, we investigate the use of quantization to potentially resist adversarial attacks. Several recent studies have reported remarkable results in reducing the energy requirement of a DNN through quantization. However, no prior work has considered the relationship between adversarial sensitivity of a DNN and its effect on quantization. We propose QUANOS- a framework that performs layer-specific hybrid quantization based on Adversarial Noise Sensitivity (ANS). We identify a novel noise stability metric (ANS) for DNNs, i.e., the sensitivity of each layer's computation to adversarial noise. ANS allows for a principled way of determining optimal bit-width per layer that incurs adversarial robustness as well as energy-efficiency with minimal loss in accuracy. Essentially, QUANOS assigns layer significance based on its contribution to adversarial perturbation and accordingly scales the precision of the layers. A key advantage of QUANOS is that it does not rely on a pre-trained model and can be applied in the initial stages of training. We evaluate the benefits of QUANOS on precision scalable Multiply and Accumulate (MAC) hardware architectures with data gating and subword parallelism capabilities. Our experiments on CIFAR10, CIFAR100 datasets show that QUANOS outperforms homogenously quantized 8-bit precision baseline in terms of adversarial robustness (3%-4% higher) while yielding improved compression (>5x) and energy savings (>2x) at iso-accuracy.

</details>

<details>

<summary>2020-06-27 21:57:09 - Graph Structure Learning for Robust Graph Neural Networks</summary>

- *Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, Jiliang Tang*

- `2005.10203v3` - [abs](http://arxiv.org/abs/2005.10203v3) - [pdf](http://arxiv.org/pdf/2005.10203v3)

> Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses (footnote: https://github.com/DSE-MSU/DeepRobust). The specific experimental settings to reproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.

</details>

<details>

<summary>2020-06-28 15:17:15 - FDA3 : Federated Defense Against Adversarial Attacks for Cloud-Based IIoT Applications</summary>

- *Yunfei Song, Tian Liu, Tongquan Wei, Xiangfeng Wang, Zhe Tao, Mingsong Chen*

- `2006.15632v1` - [abs](http://arxiv.org/abs/2006.15632v1) - [pdf](http://arxiv.org/pdf/2006.15632v1)

> Along with the proliferation of Artificial Intelligence (AI) and Internet of Things (IoT) techniques, various kinds of adversarial attacks are increasingly emerging to fool Deep Neural Networks (DNNs) used by Industrial IoT (IIoT) applications. Due to biased training data or vulnerable underlying models, imperceptible modifications on inputs made by adversarial attacks may result in devastating consequences. Although existing methods are promising in defending such malicious attacks, most of them can only deal with limited existing attack types, which makes the deployment of large-scale IIoT devices a great challenge. To address this problem, we present an effective federated defense approach named FDA3 that can aggregate defense knowledge against adversarial examples from different sources. Inspired by federated learning, our proposed cloud-based architecture enables the sharing of defense capabilities against different attacks among IIoT devices. Comprehensive experimental results show that the generated DNNs by our approach can not only resist more malicious attacks than existing attack-specific adversarial training methods, but also can prevent IIoT applications from new attacks.

</details>

<details>

<summary>2020-06-29 06:12:45 - Towards Learning-automation IoT Attack Detection through Reinforcement Learning</summary>

- *Tianbo Gu, Allaukik Abhishek, Hao Fu, Huanle Zhang, Debraj Basu, Prasant Mohapatra*

- `2006.15826v1` - [abs](http://arxiv.org/abs/2006.15826v1) - [pdf](http://arxiv.org/pdf/2006.15826v1)

> As a massive number of the Internet of Things (IoT) devices are deployed, the security and privacy issues in IoT arouse more and more attention. The IoT attacks are causing tremendous loss to the IoT networks and even threatening human safety. Compared to traditional networks, IoT networks have unique characteristics, which make the attack detection more challenging. First, the heterogeneity of platforms, protocols, software, and hardware exposes various vulnerabilities. Second, in addition to the traditional high-rate attacks, the low-rate attacks are also extensively used by IoT attackers to obfuscate the legitimate and malicious traffic. These low-rate attacks are challenging to detect and can persist in the networks. Last, the attackers are evolving to be more intelligent and can dynamically change their attack strategies based on the environment feedback to avoid being detected, making it more challenging for the defender to discover a consistent pattern to identify the attack.   In order to adapt to the new characteristics in IoT attacks, we propose a reinforcement learning-based attack detection model that can automatically learn and recognize the transformation of the attack pattern. Therefore, we can continuously detect IoT attacks with less human intervention. In this paper, we explore the crucial features of IoT traffics and utilize the entropy-based metrics to detect both the high-rate and low-rate IoT attacks. Afterward, we leverage the reinforcement learning technique to continuously adjust the attack detection threshold based on the detection feedback, which optimizes the detection and the false alarm rate. We conduct extensive experiments over a real IoT attack dataset and demonstrate the effectiveness of our IoT attack detection framework.

</details>

<details>

<summary>2020-06-29 06:14:06 - IoTGaze: IoT Security Enforcement via Wireless Context Analysis</summary>

- *Tianbo Gu, Zheng Fang, Allaukik Abhishek, Hao Fu, Pengfei Hu, Prasant Mohapatra*

- `2006.15827v1` - [abs](http://arxiv.org/abs/2006.15827v1) - [pdf](http://arxiv.org/pdf/2006.15827v1)

> Internet of Things (IoT) has become the most promising technology for service automation, monitoring, and interconnection, etc. However, the security and privacy issues caused by IoT arouse concerns. Recent research focuses on addressing security issues by looking inside platform and apps. In this work, we creatively change the angle to consider security problems from a wireless context perspective. We propose a novel framework called IoTGaze, which can discover potential anomalies and vulnerabilities in the IoT system via wireless traffic analysis. By sniffing the encrypted wireless traffic, IoTGaze can automatically identify the sequential interaction of events between apps and devices. We discover the temporal event dependencies and generate the Wireless Context for the IoT system. Meanwhile, we extract the IoT Context, which reflects user's expectation, from IoT apps' descriptions and user interfaces. If the wireless context does not match the expected IoT context, IoTGaze reports an anomaly. Furthermore, IoTGaze can discover the vulnerabilities caused by the inter-app interaction via hidden channels, such as temperature and illuminance. We provide a proof-of-concept implementation and evaluation of our framework on the Samsung SmartThings platform. The evaluation shows that IoTGaze can effectively discover anomalies and vulnerabilities, thereby greatly enhancing the security of IoT systems.

</details>

<details>

<summary>2020-06-29 16:45:15 - Legal Risks of Adversarial Machine Learning Research</summary>

- *Ram Shankar Siva Kumar, Jonathon Penney, Bruce Schneier, Kendra Albert*

- `2006.16179v1` - [abs](http://arxiv.org/abs/2006.16179v1) - [pdf](http://arxiv.org/pdf/2006.16179v1)

> Adversarial Machine Learning is booming with ML researchers increasingly targeting commercial ML systems such as those used in Facebook, Tesla, Microsoft, IBM, Google to demonstrate vulnerabilities. In this paper, we ask, "What are the potential legal risks to adversarial ML researchers when they attack ML systems?" Studying or testing the security of any operational system potentially runs afoul the Computer Fraud and Abuse Act (CFAA), the primary United States federal statute that creates liability for hacking. We claim that Adversarial ML research is likely no different. Our analysis show that because there is a split in how CFAA is interpreted, aspects of adversarial ML attacks, such as model inversion, membership inference, model stealing, reprogramming the ML system and poisoning attacks, may be sanctioned in some jurisdictions and not penalized in others. We conclude with an analysis predicting how the US Supreme Court may resolve some present inconsistencies in the CFAA's application in Van Buren v. United States, an appeal expected to be decided in 2021. We argue that the court is likely to adopt a narrow construction of the CFAA, and that this will actually lead to better adversarial ML security outcomes in the long term.

</details>

<details>

<summary>2020-06-29 20:53:35 - Evaluation of Attack Vectors and Risks in Automobiles and Road Infrastructure</summary>

- *John N. Brewer III, George Dimitoglou*

- `2006.16374v1` - [abs](http://arxiv.org/abs/2006.16374v1) - [pdf](http://arxiv.org/pdf/2006.16374v1)

> The evolution of smart automobiles and vehicles within the Internet of Things (IoT) - particularly as that evolution leads toward a proliferation of completely autonomous vehicles - has sparked considerable interest in the subject of vehicle/automotive security. While the attack surface is wide, there are patterns of exploitable vulnerabilities. In this study we reviewed, classified according to their attack surface, and evaluated some of the common vehicle and infrastructure attack vectors identified in the literature. To remediate these attack vectors, specific technical recommendations have been provided as a way towards secure deployments of smart automobiles and transportation infrastructures.

</details>

<details>

<summary>2020-06-30 05:56:33 - Adversarial Deep Ensemble: Evasion Attacks and Defenses for Malware Detection</summary>

- *Deqiang Li, Qianmu Li*

- `2006.16545v1` - [abs](http://arxiv.org/abs/2006.16545v1) - [pdf](http://arxiv.org/pdf/2006.16545v1)

> Malware remains a big threat to cyber security, calling for machine learning based malware detection. While promising, such detectors are known to be vulnerable to evasion attacks. Ensemble learning typically facilitates countermeasures, while attackers can leverage this technique to improve attack effectiveness as well. This motivates us to investigate which kind of robustness the ensemble defense or effectiveness the ensemble attack can achieve, particularly when they combat with each other. We thus propose a new attack approach, named mixture of attacks, by rendering attackers capable of multiple generative methods and multiple manipulation sets, to perturb a malware example without ruining its malicious functionality. This naturally leads to a new instantiation of adversarial training, which is further geared to enhancing the ensemble of deep neural networks. We evaluate defenses using Android malware detectors against 26 different attacks upon two practical datasets. Experimental results show that the new adversarial training significantly enhances the robustness of deep neural networks against a wide range of attacks, ensemble methods promote the robustness when base classifiers are robust enough, and yet ensemble attacks can evade the enhanced malware detectors effectively, even notably downgrading the VirusTotal service.

</details>

<details>

<summary>2020-06-30 06:30:30 - Security Issues of Low Power Wide Area Networks in the Context of LoRa Networks</summary>

- *Debraj Basu, Tianbo Gu, Prasant Mohapatra*

- `2006.16554v1` - [abs](http://arxiv.org/abs/2006.16554v1) - [pdf](http://arxiv.org/pdf/2006.16554v1)

> Low Power Wide Area Networks (LPWAN) have been used to support low cost and mobile bi-directional communications for the Internet of Things (IoT), smart city and a wide range of industrial applications. A primary security concern of LPWAN technology is the attacks that block legitimate communication between nodes resulting in scenarios like loss of packets, delayed packet arrival, and skewed packet reaching the reporting gateway. LoRa (Long Range) is a promising wireless radio access technology that supports long-range communication at low data rates and low power consumption. LoRa is considered as one of the ideal candidates for building LPWANs. We use LoRa as a reference technology to review the IoT security threats on the air and the applicability of different countermeasures that have been adopted so far. LoRa nodes that are close to the gateway use a small SF than the nodes which are far away. But it also implies long in-the-air transmission time, which makes the transmitted packets vulnerable to different kinds of malicious attacks, especially in the physical and the link layer. Therefore, it is not possible to enforce a fixed set of rules for all LoRa nodes since they have different levels of vulnerabilities. Our survey reveals that there is an urgent need for secure and uninterrupted communication between an end-device and the gateway, especially when the threat models are unknown in advance. We explore the traditional countermeasures and find that most of them are ineffective now, such as frequency hopping and spread spectrum methods. In order to adapt to new threats, the emerging countermeasures using game-theoretic approaches and reinforcement machine learning methods can effectively identify threats and dynamically choose the corresponding actions to resist threats, thereby making secured and reliable communications.

</details>

<details>

<summary>2020-06-30 10:05:09 - CVE based classification of vulnerable IoT systems</summary>

- *Grzegorz J. Blinowski, Paweł Piotrowski*

- `2006.16640v1` - [abs](http://arxiv.org/abs/2006.16640v1) - [pdf](http://arxiv.org/pdf/2006.16640v1)

> Common Vulnerabilities and Exposures database (CVE) is one of the largest publicly available source of software and hardware vulnerability data and reports. In this work we analyze the CVE database in the context of IoT device and system vulnerabilities. We introduce a real-world based classification of IoT systems. Then, we employ a SVM algorithm on selected subset of CVE database to classify "new" vulnerability records in this framework. The subset of interest consists of records that describe vulnerabilities of potential IoT devices of different applications, such as: home, industry, mobile controllers, networking, etc. The purpose of the classification is to develop and test an automatic system for recognition of vulnerable IoT devices and to test completes, sufficiency and reliability of CVE data in this respect.

</details>

<details>

<summary>2020-06-30 17:07:45 - Towards Robust LiDAR-based Perception in Autonomous Driving: General Black-box Adversarial Sensor Attack and Countermeasures</summary>

- *Jiachen Sun, Yulong Cao, Qi Alfred Chen, Z. Morley Mao*

- `2006.16974v1` - [abs](http://arxiv.org/abs/2006.16974v1) - [pdf](http://arxiv.org/pdf/2006.16974v1)

> Perception plays a pivotal role in autonomous driving systems, which utilizes onboard sensors like cameras and LiDARs (Light Detection and Ranging) to assess surroundings. Recent studies have demonstrated that LiDAR-based perception is vulnerable to spoofing attacks, in which adversaries spoof a fake vehicle in front of a victim self-driving car by strategically transmitting laser signals to the victim's LiDAR sensor. However, existing attacks suffer from effectiveness and generality limitations. In this work, we perform the first study to explore the general vulnerability of current LiDAR-based perception architectures and discover that the ignored occlusion patterns in LiDAR point clouds make self-driving cars vulnerable to spoofing attacks. We construct the first black-box spoofing attack based on our identified vulnerability, which universally achieves around 80% mean success rates on all target models. We perform the first defense study, proposing CARLO to mitigate LiDAR spoofing attacks. CARLO detects spoofed data by treating ignored occlusion patterns as invariant physical features, which reduces the mean attack success rate to 5.5%. Meanwhile, we take the first step towards exploring a general architecture for robust LiDAR-based perception, and propose SVF that embeds the neglected physical features into end-to-end learning. SVF further reduces the mean attack success rate to around 2.3%.

</details>

<details>

<summary>2020-06-30 18:49:18 - Autosploit: A Fully Automated Framework for Evaluating the Exploitability of Security Vulnerabilities</summary>

- *Noam Moscovich, Ron Bitton, Yakov Mallah, Masaki Inokuchi, Tomohiko Yagyu, Meir Kalech, Yuval Elovici, Asaf Shabtai*

- `2007.00059v1` - [abs](http://arxiv.org/abs/2007.00059v1) - [pdf](http://arxiv.org/pdf/2007.00059v1)

> The existence of a security vulnerability in a system does not necessarily mean that it can be exploited. In this research, we introduce Autosploit -- an automated framework for evaluating the exploitability of vulnerabilities. Given a vulnerable environment and relevant exploits, Autosploit will automatically test the exploits on different configurations of the environment in order to identify the specific properties necessary for successful exploitation of the existing vulnerabilities. Since testing all possible system configurations is infeasible, we introduce an efficient approach for testing and searching through all possible configurations of the environment. The efficient testing process implemented by Autosploit is based on two algorithms: generalized binary splitting and Barinel, which are used for noiseless and noisy environments respectively. We implemented the proposed framework and evaluated it using real vulnerabilities. The results show that Autosploit is able to automatically identify the system properties that affect the ability to exploit a vulnerability in both noiseless and noisy environments. These important results can be utilized for more accurate and effective risk assessment.

</details>

<details>

<summary>2020-06-30 23:05:12 - Generating Adversarial Examples with an Optimized Quality</summary>

- *Aminollah Khormali, DaeHun Nyang, David Mohaisen*

- `2007.00146v1` - [abs](http://arxiv.org/abs/2007.00146v1) - [pdf](http://arxiv.org/pdf/2007.00146v1)

> Deep learning models are widely used in a range of application areas, such as computer vision, computer security, etc. However, deep learning models are vulnerable to Adversarial Examples (AEs),carefully crafted samples to deceive those models. Recent studies have introduced new adversarial attack methods, but, to the best of our knowledge, none provided guaranteed quality for the crafted examples as part of their creation, beyond simple quality measures such as Misclassification Rate (MR). In this paper, we incorporateImage Quality Assessment (IQA) metrics into the design and generation process of AEs. We propose an evolutionary-based single- and multi-objective optimization approaches that generate AEs with high misclassification rate and explicitly improve the quality, thus indistinguishability, of the samples, while perturbing only a limited number of pixels. In particular, several IQA metrics, including edge analysis, Fourier analysis, and feature descriptors, are leveraged into the process of generating AEs. Unique characteristics of the evolutionary-based algorithm enable us to simultaneously optimize the misclassification rate and the IQA metrics of the AEs. In order to evaluate the performance of the proposed method, we conduct intensive experiments on different well-known benchmark datasets(MNIST, CIFAR, GTSRB, and Open Image Dataset V5), while considering various objective optimization configurations. The results obtained from our experiments, when compared with the exist-ing attack methods, validate our initial hypothesis that the use ofIQA metrics within generation process of AEs can substantially improve their quality, while maintaining high misclassification rate.Finally, transferability and human perception studies are provided, demonstrating acceptable performance.

</details>


## 2020-07

<details>

<summary>2020-07-01 18:00:54 - Dispelling Myths on Superposition Attacks: Formal Security Model and Attack Analyses</summary>

- *Luka Music, Céline Chevalier, Elham Kashefi*

- `2007.00677v1` - [abs](http://arxiv.org/abs/2007.00677v1) - [pdf](http://arxiv.org/pdf/2007.00677v1)

> It is of folkloric belief that the security of classical cryptographic protocols is automatically broken if the Adversary is allowed to perform superposition queries and the honest players forced to perform actions coherently on quantum states. Another widely held intuition is that enforcing measurements on the exchanged messages is enough to protect protocols from these attacks.   However, the reality is much more complex. Security models dealing with superposition attacks only consider unconditional security. Conversely, security models considering computational security assume that all supposedly classical messages are measured, which forbids by construction the analysis of superposition attacks. Boneh and Zhandry have started to study the quantum computational security for classical primitives in their seminal work at Crypto'13, but only in the single-party setting. To the best of our knowledge, an equivalent model in the multiparty setting is still missing.   In this work, we propose the first computational security model considering superposition attacks for multiparty protocols. We show that our new security model is satisfiable by proving the security of the well-known One-Time-Pad protocol and give an attack on a variant of the equally reputable Yao Protocol for Secure Two-Party Computations. The post-mortem of this attack reveals the precise points of failure, yielding highly counter-intuitive results: Adding extra classical communication, which is harmless for classical security, can make the protocol become subject to superposition attacks. We use this newly imparted knowledge to construct the first concrete protocol for Secure Two-Party Computation that is resistant to superposition attacks. Our results show that there is no straightforward answer to provide for either the vulnerabilities of classical protocols to superposition attacks or the adapted countermeasures.

</details>

<details>

<summary>2020-07-01 19:25:34 - ConFoc: Content-Focus Protection Against Trojan Attacks on Neural Networks</summary>

- *Miguel Villarreal-Vasquez, Bharat Bhargava*

- `2007.00711v1` - [abs](http://arxiv.org/abs/2007.00711v1) - [pdf](http://arxiv.org/pdf/2007.00711v1)

> Deep Neural Networks (DNNs) have been applied successfully in computer vision. However, their wide adoption in image-related applications is threatened by their vulnerability to trojan attacks. These attacks insert some misbehavior at training using samples with a mark or trigger, which is exploited at inference or testing time. In this work, we analyze the composition of the features learned by DNNs at training. We identify that they, including those related to the inserted triggers, contain both content (semantic information) and style (texture information), which are recognized as a whole by DNNs at testing time. We then propose a novel defensive technique against trojan attacks, in which DNNs are taught to disregard the styles of inputs and focus on their content only to mitigate the effect of triggers during the classification. The generic applicability of the approach is demonstrated in the context of a traffic sign and a face recognition application. Each of them is exposed to a different attack with a variety of triggers. Results show that the method reduces the attack success rate significantly to values < 1% in all the tested attacks while keeping as well as improving the initial accuracy of the models when processing both benign and adversarial data.

</details>

<details>

<summary>2020-07-02 11:30:07 - Hunting for Re-Entrancy Attacks in Ethereum Smart Contracts via Static Analysis</summary>

- *Yuichiro Chinen, Naoto Yanai, Jason Paul Cruz, Shingo Okamura*

- `2007.01029v1` - [abs](http://arxiv.org/abs/2007.01029v1) - [pdf](http://arxiv.org/pdf/2007.01029v1)

> Ethereum smart contracts are programs that are deployed and executed in a consensus-based blockchain managed by a peer-to-peer network. Several re-entrancy attacks that aim to steal Ether, the cryptocurrency used in Ethereum, stored in deployed smart contracts have been found in the recent years. A countermeasure to such attacks is based on dynamic analysis that executes the smart contracts themselves, but it requires the spending of Ether and knowledge of attack patterns for analysis in advance. In this paper, we present a static analysis tool named \textit{RA (Re-entrancy Analyzer)}, a combination of symbolic execution and equivalence checking by a satisfiability modulo theories solver to analyze smart contract vulnerabilities to re-entrancy attacks. In contrast to existing tools, RA supports analysis of inter-contract behaviors by using only the Etherum Virtual Machine bytecodes of target smart contracts, i.e., even without prior knowledge of attack patterns and without spending Ether. Furthermore, RA can verify existence of vulnerabilities to re-entrancy attacks without execution of smart contracts and it does not provide false positives and false negatives. We also present an implementation of RA to evaluate its performance in analyzing the vulnerability of deployed smart contracts to re-entrancy attacks and show that RA can precisely determine which smart contracts are vulnerable.

</details>

<details>

<summary>2020-07-02 12:31:54 - Zooming Into Video Conferencing Privacy and Security Threats</summary>

- *Dima Kagan, Galit Fuhrmann Alpert, Michael Fire*

- `2007.01059v1` - [abs](http://arxiv.org/abs/2007.01059v1) - [pdf](http://arxiv.org/pdf/2007.01059v1)

> The COVID-19 pandemic outbreak, with its related social distancing and shelter-in-place measures, has dramatically affected ways in which people communicate with each other, forcing people to find new ways to collaborate, study, celebrate special occasions, and meet with family and friends. One of the most popular solutions that have emerged is the use of video conferencing applications to replace face-to-face meetings with virtual meetings. This resulted in unprecedented growth in the number of video conferencing users. In this study, we explored privacy issues that may be at risk by attending virtual meetings. We extracted private information from collage images of meeting participants that are publicly posted on the Web. We used image processing, text recognition tools, as well as social network analysis to explore our web crawling curated dataset of over 15,700 collage images, and over 142,000 face images of meeting participants. We demonstrate that video conference users are facing prevalent security and privacy threats. Our results indicate that it is relatively easy to collect thousands of publicly available images of video conference meetings and extract personal information about the participants, including their face images, age, gender, usernames, and sometimes even full names. This type of extracted data can vastly and easily jeopardize people's security and privacy both in the online and real-world, affecting not only adults but also more vulnerable segments of society, such as young children and older adults. Finally, we show that cross-referencing facial image data with social network data may put participants at additional privacy risks they may not be aware of and that it is possible to identify users that appear in several video conference meetings, thus providing a potential to maliciously aggregate different sources of information about a target individual.

</details>

<details>

<summary>2020-07-02 12:33:31 - CRYLOGGER: Detecting Crypto Misuses Dynamically</summary>

- *Luca Piccolboni, Giuseppe Di Guglielmo, Luca P. Carloni, Simha Sethumadhavan*

- `2007.01061v1` - [abs](http://arxiv.org/abs/2007.01061v1) - [pdf](http://arxiv.org/pdf/2007.01061v1)

> Cryptographic (crypto) algorithms are the essential ingredients of all secure systems: crypto hash functions and encryption algorithms, for example, can guarantee properties such as integrity and confidentiality. Developers, however, can misuse the application programming interfaces (API) of such algorithms by using constant keys and weak passwords. This paper presents CRYLOGGER, the first open-source tool to detect crypto misuses dynamically. CRYLOGGER logs the parameters that are passed to the crypto APIs during the execution and checks their legitimacy offline by using a list of crypto rules. We compare CRYLOGGER with CryptoGuard, one of the most effective static tools to detect crypto misuses. We show that our tool complements the results of CryptoGuard, making the case for combining static and dynamic approaches. We analyze 1780 popular Android apps downloaded from the Google Play Store to show that CRYLOGGER can detect crypto misuses on thousands of apps dynamically and automatically. We reverse-engineer 28 Android apps and confirm the issues flagged by CRYLOGGER. We also disclose the most critical vulnerabilities to app developers and collect their feedback.

</details>

<details>

<summary>2020-07-02 13:47:36 - Adversarial Neural Pruning with Latent Vulnerability Suppression</summary>

- *Divyam Madaan, Jinwoo Shin, Sung Ju Hwang*

- `1908.04355v4` - [abs](http://arxiv.org/abs/1908.04355v4) - [pdf](http://arxiv.org/pdf/1908.04355v4)

> Despite the remarkable performance of deep neural networks on various computer vision tasks, they are known to be susceptible to adversarial perturbations, which makes it challenging to deploy them in real-world safety-critical applications. In this paper, we conjecture that the leading cause of adversarial vulnerability is the distortion in the latent feature space, and provide methods to suppress them effectively. Explicitly, we define \emph{vulnerability} for each latent feature and then propose a new loss for adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to minimize the feature-level vulnerability during training. We further propose a Bayesian framework to prune features with high vulnerability to reduce both vulnerability and loss on adversarial samples. We validate our \emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)} method on multiple benchmark datasets, on which it not only obtains state-of-the-art adversarial robustness but also improves the performance on clean examples, using only a fraction of the parameters used by the full network. Further qualitative analysis suggests that the improvements come from the suppression of feature-level vulnerability.

</details>

<details>

<summary>2020-07-02 17:54:05 - Robust ambiguity for contact tracing</summary>

- *David Mestel*

- `2007.01288v1` - [abs](http://arxiv.org/abs/2007.01288v1) - [pdf](http://arxiv.org/pdf/2007.01288v1)

> A known drawback of `decentralised' contact tracing architectures is that users who have been in contact with an infected person are able to precisely identify the relevant contact, and thereby perhaps identify the infected person. In their proposal, the PACT team discuss a simple DH-based protocol to mitigate this problem, but dismiss it because it is vulnerable to a malicious user who may deviate from the specified behaviour. This note presents a modified protocol which achieves robustness against a fully malicious user, and establishes some simple security properties.

</details>

<details>

<summary>2020-07-02 19:47:47 - SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing</summary>

- *Haonan Qiu, Chaowei Xiao, Lei Yang, Xinchen Yan, Honglak Lee, Bo Li*

- `1906.07927v4` - [abs](http://arxiv.org/abs/1906.07927v4) - [pdf](http://arxiv.org/pdf/1906.07927v4)

> Deep neural networks (DNNs) have achieved great success in various applications due to their strong expressive power. However, recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee "subtle perturbation" by limiting the $L_p$ norm of the perturbation. In this paper, we aim to explore the impact of semantic manipulation on DNNs predictions by manipulating the semantic attributes of images and generate "unrestricted adversarial examples".   In particular, we propose an algorithm \emph{SemanticAdv} which leverages disentangled semantic factors to generate adversarial perturbation by altering controlled semantic attributes to fool the learner towards various "adversarial" targets. We conduct extensive experiments to show that the semantic based adversarial examples can not only fool different learning tasks such as face verification and landmark detection, but also achieve high targeted attack success rate against \emph{real-world black-box} services such as Azure face verification service based on transferability.   To further demonstrate the applicability of \emph{SemanticAdv} beyond face recognition domain, we also generate semantic perturbations on street-view images. Such adversarial examples with controlled semantic manipulation can shed light on further understanding about vulnerabilities of DNNs as well as potential defensive approaches.

</details>

<details>

<summary>2020-07-02 19:51:40 - Decoder-free Robustness Disentanglement without (Additional) Supervision</summary>

- *Yifei Wang, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng Yang*

- `2007.01356v1` - [abs](http://arxiv.org/abs/2007.01356v1) - [pdf](http://arxiv.org/pdf/2007.01356v1)

> Adversarial Training (AT) is proposed to alleviate the adversarial vulnerability of machine learning models by extracting only robust features from the input, which, however, inevitably leads to severe accuracy reduction as it discards the non-robust yet useful features. This motivates us to preserve both robust and non-robust features and separate them with disentangled representation learning. Our proposed Adversarial Asymmetric Training (AAT) algorithm can reliably disentangle robust and non-robust representations without additional supervision on robustness. Empirical results show our method does not only successfully preserve accuracy by combining two representations, but also achieve much better disentanglement than previous work.

</details>

<details>

<summary>2020-07-03 19:37:34 - Detecting Replay Attacks Using Multi-Channel Audio: A Neural Network-Based Method</summary>

- *Yuan Gong, Jian Yang, Christian Poellabauer*

- `2003.08225v3` - [abs](http://arxiv.org/abs/2003.08225v3) - [pdf](http://arxiv.org/pdf/2003.08225v3)

> With the rapidly growing number of security-sensitive systems that use voice as the primary input, it becomes increasingly important to address these systems' potential vulnerability to replay attacks. Previous efforts to address this concern have focused primarily on single-channel audio. In this paper, we introduce a novel neural network-based replay attack detection model that further leverages spatial information of multi-channel audio and is able to significantly improve the replay attack detection performance.

</details>

<details>

<summary>2020-07-03 20:28:37 - Vulnerabilities Mapping based on OWASP-SANS: a Survey for Static Application Security Testing (SAST)</summary>

- *Jinfeng Li*

- `2004.03216v2` - [abs](http://arxiv.org/abs/2004.03216v2) - [pdf](http://arxiv.org/pdf/2004.03216v2)

> The delivery of a framework in place for secure application development is of real value for application development teams to integrate security into their development life cycle, especially when a mobile or web application moves past the scanning stage and focuses increasingly on the remediation or mitigation phase based on static application security testing (SAST). For the first time, to the author's knowledge, the industry-standard Open Web Application Security Project (OWASP) top 10 vulnerabilities and CWE/SANS top 25 most dangerous software errors are synced up in a matrix with Checkmarx vulnerability queries, producing an application security framework that helps development teams review and address code vulnerabilities, minimise false positives discovered in static scans and penetration tests, targeting an increased accuracy of the findings. A case study is conducted for vulnerabilities scanning of a proof-of-concept mobile malware detection app. Mapping the OWASP/SANS with Checkmarx vulnerabilities queries, flaws and vulnerabilities are demonstrated to be mitigated with improved efficiency.

</details>

<details>

<summary>2020-07-04 12:03:04 - A Comprehensive Formal Security Analysis and Revision of the Two-phase Key Exchange Primitive of TPM 2.0</summary>

- *Qianying Zhang, Shijun Zhao*

- `1906.06653v2` - [abs](http://arxiv.org/abs/1906.06653v2) - [pdf](http://arxiv.org/pdf/1906.06653v2)

> The Trusted Platform Module (TPM) version 2.0 provides a two-phase key exchange primitive which can be used to implement three widely-standardized authenticated key exchange protocols: the Full Unified Model, the Full MQV, and the SM2 key exchange protocols. However, vulnerabilities have been found in all of these protocols. Fortunately, it seems that the protections offered by TPM chips can mitigate these vulnerabilities. In this paper, we present a security model which captures TPM's protections on keys and protocols' computation environments and in which multiple protocols can be analyzed in a unified way. Based on the unified security model, we give the first formal security analysis of the key exchange primitive of TPM 2.0, and the analysis results show that, with the help of hardware protections of TPM chips, the key exchange primitive indeed satisfies the well-defined security property of our security model, but unfortunately under some impractical limiting conditions, which would prevent the application of the key exchange primitive in real-world networks. To make TPM 2.0 applicable to real-world networks, we present a revision of the key exchange primitive of TPM 2.0, which can be secure without the limiting conditions. We give a rigorous analysis of our revision, and the results show that our revision achieves not only the basic security property of modern AKE security models but also some further security properties.

</details>

<details>

<summary>2020-07-04 14:04:25 - Privacy Preserving Face Recognition Utilizing Differential Privacy</summary>

- *M. A. P. Chamikara, P. Bertok, I. Khalil, D. Liu, S. Camtepe*

- `2005.10486v2` - [abs](http://arxiv.org/abs/2005.10486v2) - [pdf](http://arxiv.org/pdf/2005.10486v2)

> Facial recognition technologies are implemented in many areas, including but not limited to, citizen surveillance, crime control, activity monitoring, and facial expression evaluation. However, processing biometric information is a resource-intensive task that often involves third-party servers, which can be accessed by adversaries with malicious intent. Biometric information delivered to untrusted third-party servers in an uncontrolled manner can be considered a significant privacy leak (i.e. uncontrolled information release) as biometrics can be correlated with sensitive data such as healthcare or financial records. In this paper, we propose a privacy-preserving technique for "controlled information release", where we disguise an original face image and prevent leakage of the biometric features while identifying a person. We introduce a new privacy-preserving face recognition protocol named PEEP (Privacy using EigEnface Perturbation) that utilizes local differential privacy. PEEP applies perturbation to Eigenfaces utilizing differential privacy and stores only the perturbed data in the third-party servers to run a standard Eigenface recognition algorithm. As a result, the trained model will not be vulnerable to privacy attacks such as membership inference and model memorization attacks. Our experiments show that PEEP exhibits a classification accuracy of around 70% - 90% under standard privacy settings.

</details>

<details>

<summary>2020-07-04 19:28:50 - Learning-Aided Physical Layer Attacks Against Multicarrier Communications in IoT</summary>

- *Alireza Nooraiepour, Waheed U. Bajwa, Narayan B. Mandayam*

- `1908.00195v2` - [abs](http://arxiv.org/abs/1908.00195v2) - [pdf](http://arxiv.org/pdf/1908.00195v2)

> Internet-of-Things (IoT) devices that are limited in power and processing are susceptible to physical layer (PHY) spoofing (signal exploitation) attacks owing to their inability to implement a full-blown protocol stack for security. The overwhelming adoption of multicarrier techniques such as orthogonal frequency division multiplexing (OFDM) for the PHY layer makes IoT devices further vulnerable to PHY spoofing attacks. These attacks which aim at injecting bogus/spurious data into the receiver, involve inferring transmission parameters and finding PHY characteristics of the transmitted signals so as to spoof the received signal. Non-contiguous (NC) OFDM systems have been argued to have low probability of exploitation (LPE) characteristics against classic attacks based on cyclostationary analysis, and the corresponding PHY has been deemed to be secure. However, with the advent of machine learning (ML) algorithms, adversaries can devise data-driven attacks to compromise such systems. It is in this vein that PHY spoofing performance of adversaries equipped with supervised and unsupervised ML tools are investigated in this paper. The supervised ML approach is based on deep neural networks (DNN) while the unsupervised one employs variational autoencoders (VAEs). In particular, VAEs are shown to be capable of learning representations from NC-OFDM signals related to their PHY characteristics such as frequency pattern and modulation scheme, which are useful for PHY spoofing. In addition, a new metric based on the disentanglement principle is proposed to measure the quality of such learned representations. Simulation results demonstrate that the performance of the spoofing adversaries highly depends on the subcarriers' allocation patterns. Particularly, it is shown that utilizing a random subcarrier occupancy pattern secures NC-OFDM systems against ML-based attacks.

</details>

<details>

<summary>2020-07-05 12:12:09 - Challenges in Designing Exploit Mitigations for Deeply Embedded Systems</summary>

- *Ali Abbasi, Jos Wetzels, Thorsten Holz, Sandro Etalle*

- `2007.02307v1` - [abs](http://arxiv.org/abs/2007.02307v1) - [pdf](http://arxiv.org/pdf/2007.02307v1)

> Memory corruption vulnerabilities have been around for decades and rank among the most prevalent vulnerabilities in embedded systems. Yet this constrained environment poses unique design and implementation challenges that significantly complicate the adoption of common hardening techniques. Combined with the irregular and involved nature of embedded patch management, this results in prolonged vulnerability exposure windows and vulnerabilities that are relatively easy to exploit. Considering the sensitive and critical nature of many embedded systems, this situation merits significant improvement. In this work, we present the first quantitative study of exploit mitigation adoption in 42 embedded operating systems, showing the embedded world to significantly lag behind the general-purpose world. To improve the security of deeply embedded systems, we subsequently present {\mu}Armor, an approach to address some of the key gaps identified in our quantitative analysis. {\mu}Armor raises the bar for exploitation of embedded memory corruption vulnerabilities, while being adoptable on the short term without incurring prohibitive extra performance or storage costs.

</details>

<details>

<summary>2020-07-05 12:12:40 - Steroids for DOPed Applications: A Compiler for Automated Data-Oriented Programming</summary>

- *Jannik Pewny, Philipp Koppe, Thorsten Holz*

- `2007.02308v1` - [abs](http://arxiv.org/abs/2007.02308v1) - [pdf](http://arxiv.org/pdf/2007.02308v1)

> The wide-spread adoption of system defenses such as the randomization of code, stack, and heap raises the bar for code-reuse attacks. Thus, attackers utilize a scripting engine in target programs like a web browser to prepare the code-reuse chain, e.g., relocate gadget addresses or perform a just-in-time gadget search. However, many types of programs do not provide such an execution context that an attacker can use. Recent advances in data-oriented programming (DOP) explored an orthogonal way to abuse memory corruption vulnerabilities and demonstrated that an attacker can achieve Turing-complete computations without modifying code pointers in applications. As of now, constructing DOP exploits requires a lot of manual work.   In this paper, we present novel techniques to automate the process of generating DOP exploits. We implemented a compiler called Steroids that compiles our high-level language SLANG into low-level DOP data structures driving malicious computations at run time. This enables an attacker to specify her intent in an application- and vulnerability-independent manner to maximize reusability. We demonstrate the effectiveness of our techniques and prototype implementation by specifying four programs of varying complexity in SLANG that calculate the Levenshtein distance, traverse a pointer chain to steal a private key, relocate a ROP chain, and perform a JIT-ROP attack. Steroids compiles each of those programs to low-level DOP data structures targeted at five different applications including GStreamer, Wireshark, and ProFTPd, which have vastly different vulnerabilities and DOP instances. Ultimately, this shows that our compiler is versatile, can be used for both 32- and 64-bit applications, works across bug classes, and enables highly expressive attacks without conventional code-injection or code-reuse techniques in applications lacking a scripting engine.

</details>

<details>

<summary>2020-07-05 12:32:27 - Static Detection of Uninitialized Stack Variables in Binary Code</summary>

- *Behrad Garmany, Martin Stoffel, Robert Gawlik, Thorsten Holz*

- `2007.02314v1` - [abs](http://arxiv.org/abs/2007.02314v1) - [pdf](http://arxiv.org/pdf/2007.02314v1)

> More than two decades after the first stack smashing attacks, memory corruption vulnerabilities utilizing stack anomalies are still prevalent and play an important role in practice. Among such vulnerabilities, uninitialized variables play an exceptional role due to their unpleasant property of unpredictability: as compilers are tailored to operate fast, costly interprocedural analysis procedures are not used in practice to detect such vulnerabilities. As a result, complex relationships that expose uninitialized memory reads remain undiscovered in binary code. Recent vulnerability reports show the versatility on how uninitialized memory reads are utilized in practice, especially for memory disclosure and code execution. Research in recent years proposed detection and prevention techniques tailored to source code. To date, however, there has not been much attention for these types of software bugs within binary executables.   In this paper, we present a static analysis framework to find uninitialized variables in binary executables. We developed methods to lift the binaries into a knowledge representation which builds the base for specifically crafted algorithms to detect uninitialized reads. Our prototype implementation is capable of detecting uninitialized memory errors in complex binaries such as web browsers and OS kernels, and we detected 7 novel bugs.

</details>

<details>

<summary>2020-07-05 12:55:38 - EvilCoder: Automated Bug Insertion</summary>

- *Jannik Pewny, Thorsten Holz*

- `2007.02326v1` - [abs](http://arxiv.org/abs/2007.02326v1) - [pdf](http://arxiv.org/pdf/2007.02326v1)

> The art of finding software vulnerabilities has been covered extensively in the literature and there is a huge body of work on this topic. In contrast, the intentional insertion of exploitable, security-critical bugs has received little (public) attention yet. Wanting more bugs seems to be counterproductive at first sight, but the comprehensive evaluation of bug-finding techniques suffers from a lack of ground truth and the scarcity of bugs.   In this paper, we propose EvilCoder, a system to automatically find potentially vulnerable source code locations and modify the source code to be actually vulnerable. More specifically, we leverage automated program analysis techniques to find sensitive sinks which match typical bug patterns (e.g., a sensitive API function with a preceding sanity check), and try to find data-flow connections to user-controlled sources. We then transform the source code such that exploitation becomes possible, for example by removing or modifying input sanitization or other types of security checks. Our tool is designed to randomly pick vulnerable locations and possible modifications, such that it can generate numerous different vulnerabilities on the same software corpus. We evaluated our tool on several open-source projects such as for example libpng and vsftpd, where we found between 22 and 158 unique connected source-sink pairs per project. This translates to hundreds of potentially vulnerable data-flow paths and hundreds of bugs we can insert. We hope to support future bug-finding techniques by supplying freshly generated, bug-ridden test corpora so that such techniques can (finally) be evaluated and compared in a comprehensive and statistically meaningful way.

</details>

<details>

<summary>2020-07-05 15:18:54 - Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization</summary>

- *Sicheng Zhu, Xiao Zhang, David Evans*

- `2002.11798v2` - [abs](http://arxiv.org/abs/2002.11798v2) - [pdf](http://arxiv.org/pdf/2002.11798v2)

> Training machine learning models that are robust against adversarial inputs poses seemingly insurmountable challenges. To better understand adversarial robustness, we consider the underlying problem of learning robust representations. We develop a notion of representation vulnerability that captures the maximum change of mutual information between the input and output distributions, under the worst-case input perturbation. Then, we prove a theorem that establishes a lower bound on the minimum adversarial risk that can be achieved for any downstream classifier based on its representation vulnerability. We propose an unsupervised learning method for obtaining intrinsically robust representations by maximizing the worst-case mutual information between the input and output distributions. Experiments on downstream classification tasks support the robustness of the representations found using unsupervised learning with our training principle.

</details>

<details>

<summary>2020-07-06 03:06:26 - Adversarial T-shirt! Evading Person Detectors in A Physical World</summary>

- *Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, Xue Lin*

- `1910.11099v3` - [abs](http://arxiv.org/abs/1910.11099v3) - [pdf](http://arxiv.org/pdf/1910.11099v3)

> It is known that deep neural networks (DNNs) are vulnerable to adversarial attacks. The so-called physical adversarial examples deceive DNN-based decisionmakers by attaching adversarial patches to real objects. However, most of the existing works on physical adversarial attacks focus on static objects such as glass frames, stop signs and images attached to cardboard. In this work, we proposed adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person's pose changes. To the best of our knowledge, this is the first work that models the effect of deformation for designing physical adversarial examples with respect to-rigid objects such as T-shirts. We show that the proposed method achieves74% and 57% attack success rates in the digital and physical worlds respectively against YOLOv2. In contrast, the state-of-the-art physical attack method to fool a person detector only achieves 18% attack success rate. Furthermore, by leveraging min-max optimization, we extend our method to the ensemble attack setting against two object detectors YOLO-v2 and Faster R-CNN simultaneously.

</details>

<details>

<summary>2020-07-06 08:49:00 - Adversarial Ranking Attack and Defense</summary>

- *Mo Zhou, Zhenxing Niu, Le Wang, Qilin Zhang, Gang Hua*

- `2002.11293v3` - [abs](http://arxiv.org/abs/2002.11293v3) - [pdf](http://arxiv.org/pdf/2002.11293v3)

> Deep Neural Network (DNN) classifiers are vulnerable to adversarial attack, where an imperceptible perturbation could result in misclassification. However, the vulnerability of DNN-based image ranking systems remains under-explored. In this paper, we propose two attacks against deep ranking systems, i.e., Candidate Attack and Query Attack, that can raise or lower the rank of chosen candidates by adversarial perturbations. Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like objective function is designed to obtain the optimal perturbation. Conversely, a defense method is also proposed to improve the ranking system robustness, which can mitigate all the proposed attacks simultaneously. Our adversarial ranking attacks and defense are evaluated on datasets including MNIST, Fashion-MNIST, and Stanford-Online-Products. Experimental results demonstrate that a typical deep ranking system can be effectively compromised by our attacks. Meanwhile, the system robustness can be moderately improved with our defense. Furthermore, the transferable and universal properties of our adversary illustrate the possibility of realistic black-box attack.

</details>

<details>

<summary>2020-07-06 10:36:11 - Smart Home, security concerns of IoT</summary>

- *Alessandro Ecclesie Agazzi*

- `2007.02628v1` - [abs](http://arxiv.org/abs/2007.02628v1) - [pdf](http://arxiv.org/pdf/2007.02628v1)

> The IoT (Internet of Things) has become widely popular in the domestic environments. People are renewing their homes into smart homes; however, the privacy concerns of owning many Internet connected devices with always-on environmental sensors remain insufficiently addressed. Default and weak passwords, cheap materials and hardware, and unencrypted communication are identified as the principal threats and vulnerabilities of IoT devices. Solutions and countermeasures are also provided: choosing a strong password, strong authentication mechanisms, check online databases of exposed or default credentials to mitigate the first threat; a selection of smart home devices from reputable companies and the implementation of the SDN for the Dos/DDoS threat; and finally IDS, HTTPS protocol and VPN for eavesdropping. The paper concludes dealing with a further challenge, "the lack of technical support", by which an auto-configuration approach should be analysed; this could both ease the installation/maintenance and enhance the security in the self configuration step of Smart Home devices.

</details>

<details>

<summary>2020-07-06 13:14:21 - Black-box Adversarial Example Generation with Normalizing Flows</summary>

- *Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie*

- `2007.02734v1` - [abs](http://arxiv.org/abs/2007.02734v1) - [pdf](http://arxiv.org/pdf/2007.02734v1)

> Deep neural network classifiers suffer from adversarial vulnerability: well-crafted, unnoticeable changes to the input data can affect the classifier decision. In this regard, the study of powerful adversarial attacks can help shed light on sources of this malicious behavior. In this paper, we propose a novel black-box adversarial attack using normalizing flows. We show how an adversary can be found by searching over a pre-trained flow-based model base distribution. This way, we can generate adversaries that resemble the original data closely as the perturbations are in the shape of the data. We then demonstrate the competitive performance of the proposed approach against well-known black-box adversarial attack methods.

</details>

<details>

<summary>2020-07-06 14:13:51 - TICO-19: the Translation Initiative for Covid-19</summary>

- *Antonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian Federman, Dmitriy Genzel, Francisco Guzmán, Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp Öktem, Eric Paquin, Grace Tang, Sylwia Tur*

- `2007.01788v2` - [abs](http://arxiv.org/abs/2007.01788v2) - [pdf](http://arxiv.org/pdf/2007.01788v2)

> The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid-19 (TICO-19) have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID-19 in these languages. In addition to 9 high-resourced, "pivot" languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and South-East Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages.

</details>

<details>

<summary>2020-07-06 14:18:10 - Certifying Decision Trees Against Evasion Attacks by Program Analysis</summary>

- *Stefano Calzavara, Pietro Ferrara, Claudio Lucchese*

- `2007.02771v1` - [abs](http://arxiv.org/abs/2007.02771v1) - [pdf](http://arxiv.org/pdf/2007.02771v1)

> Machine learning has proved invaluable for a range of different tasks, yet it also proved vulnerable to evasion attacks, i.e., maliciously crafted perturbations of input data designed to force mispredictions. In this paper we propose a novel technique to verify the security of decision tree models against evasion attacks with respect to an expressive threat model, where the attacker can be represented by an arbitrary imperative program. Our approach exploits the interpretability property of decision trees to transform them into imperative programs, which are amenable for traditional program analysis techniques. By leveraging the abstract interpretation framework, we are able to soundly verify the security guarantees of decision tree models trained over publicly available datasets. Our experiments show that our technique is both precise and efficient, yielding only a minimal number of false positives and scaling up to cases which are intractable for a competitor approach.

</details>

<details>

<summary>2020-07-06 14:59:31 - An Exploratory Analysis of Microcode as a Building Block for System Defenses</summary>

- *Benjamin Kollenda, Philipp Koppe, Marc Fyrbiak, Christian Kison, Christof Paar, Thorsten Holz*

- `2007.03549v1` - [abs](http://arxiv.org/abs/2007.03549v1) - [pdf](http://arxiv.org/pdf/2007.03549v1)

> Microcode is an abstraction layer used by modern x86 processors that interprets user-visible CISC instructions to hardware-internal RISC instructions. The capability to update x86 microcode enables a vendor to modify CPU behavior in-field, and thus patch erroneous microarchitectural processes or even implement new features. Most prominently, the recent Spectre and Meltdown vulnerabilities were mitigated by Intel via microcode updates. Unfortunately, microcode is proprietary and closed source, and there is little publicly available information on its inner workings.   In this paper, we present new reverse engineering results that extend and complement the public knowledge of proprietary microcode. Based on these novel insights, we show how modern system defenses and tools can be realized in microcode on a commercial, off-the-shelf AMD x86 CPU. We demonstrate how well-established system security defenses such as timing attack mitigations, hardware-assisted address sanitization, and instruction set randomization can be realized in microcode. We also present a proof-of-concept implementation of a microcode-assisted instrumentation framework. Finally, we show how a secure microcode update mechanism and enclave functionality can be implemented in microcode to realize a small trusted execution environment. All microcode programs and the whole infrastructure needed to reproduce and extend our results are publicly available.

</details>

<details>

<summary>2020-07-06 15:28:05 - Detile: Fine-Grained Information Leak Detection in Script Engines</summary>

- *Robert Gawlik, Philipp Koppe, Benjamin Kollenda, Andre Pawlowski, Behrad Garmany, Thorsten Holz*

- `2007.03550v1` - [abs](http://arxiv.org/abs/2007.03550v1) - [pdf](http://arxiv.org/pdf/2007.03550v1)

> Memory disclosure attacks play an important role in the exploitation of memory corruption vulnerabilities. By analyzing recent research, we observe that bypasses of defensive solutions that enforce control-flow integrity or attempt to detect return-oriented programming require memory disclosure attacks as a fundamental first step. However, research lags behind in detecting such information leaks.   In this paper, we tackle this problem and present a system for fine-grained, automated detection of memory disclosure attacks against scripting engines. The basic insight is as follows: scripting languages, such as JavaScript in web browsers, are strictly sandboxed. They must not provide any insights about the memory layout in their contexts. In fact, any such information potentially represents an ongoing memory disclosure attack. Hence, to detect information leaks, our system creates a clone of the scripting engine process with a re-randomized memory layout. The clone is instrumented to be synchronized with the original process. Any inconsistency in the script contexts of both processes appears when a memory disclosure was conducted to leak information about the memory layout. Based on this detection approach, we have designed and implemented Detile (\underline{det}ection of \underline{i}nformation \underline{le}aks), a prototype for the JavaScript engine in Microsoft's Internet Explorer 10/11 on Windows 8.0/8.1. An empirical evaluation shows that our tool can successfully detect memory disclosure attacks even against this proprietary software.

</details>

<details>

<summary>2020-07-06 15:36:00 - Automated Multi-Architectural Discovery of CFI-Resistant Code Gadgets</summary>

- *Patrick Wollgast, Robert Gawlik, Behrad Garmany, Benjamin Kollenda, Thorsten Holz*

- `2007.04116v1` - [abs](http://arxiv.org/abs/2007.04116v1) - [pdf](http://arxiv.org/pdf/2007.04116v1)

> Memory corruption vulnerabilities are still a severe threat for software systems. To thwart the exploitation of such vulnerabilities, many different kinds of defenses have been proposed in the past. Most prominently, Control-Flow Integrity (CFI) has received a lot of attention recently. Several proposals were published that apply coarse-grained policies with a low performance overhead. However, their security remains questionable as recent attacks have shown.   To ease the assessment of a given CFI implementation, we introduce a framework to discover code gadgets for code-reuse attacks that conform to coarse-grained CFI policies. For this purpose, binary code is extracted and transformed to a symbolic representation in an architecture-independent manner. Additionally, code gadgets are verified to provide the needed functionality for a security researcher. We show that our framework finds more CFI-compatible gadgets compared to other code gadget discovery tools. Furthermore, we demonstrate that code gadgets needed to bypass CFI solutions on the ARM architecture can be discovered by our framework as well.

</details>

<details>

<summary>2020-07-07 08:24:13 - An Advanced Approach for Choosing Security Patterns and Checking their Implementation</summary>

- *Sébastien Salva, Loukmen Regainia*

- `2007.03275v1` - [abs](http://arxiv.org/abs/2007.03275v1) - [pdf](http://arxiv.org/pdf/2007.03275v1)

> This paper tackles the problems of generating concrete test cases for testing whether an application is vulnerable to attacks, and of checking whether security solutions are correctly implemented. The approach proposed in the paper aims at guiding developers towards the implementation of secure applications, from the threat modelling stage up to the testing one. This approach relies on a knowledge base integrating varied security data, e.g., attacks, attack steps, and security patterns that are generic and re-usable solutions to design secure applications. The first stage of the approach consists in assisting developers in the design of Attack Defense Trees expressing the attacker possibilities to compromise an application and the defenses that may be implemented. These defenses are given under the form of security pattern combinations. In the second stage, these trees are used to guide developers in the test case generation. After the test case execution, test verdicts show whether an application is vulnerable to the threats modelled by an ADTree. The last stage of the approach checks whether behavioural properties of security patterns hold in the application traces collected while the test case execution. These properties are formalised with LTL properties, which are generated from the knowledge base. Developers do not have to write LTL properties not to be expert in formal models. We experimented the approach on 10 Web applications to evaluate its testing effectiveness and its performance.

</details>

<details>

<summary>2020-07-07 10:04:28 - Manifold Learning via Manifold Deflation</summary>

- *Daniel Ting, Michael I. Jordan*

- `2007.03315v1` - [abs](http://arxiv.org/abs/2007.03315v1) - [pdf](http://arxiv.org/pdf/2007.03315v1)

> Nonlinear dimensionality reduction methods provide a valuable means to visualize and interpret high-dimensional data. However, many popular methods can fail dramatically, even on simple two-dimensional manifolds, due to problems such as vulnerability to noise, repeated eigendirections, holes in convex bodies, and boundary bias. We derive an embedding method for Riemannian manifolds that iteratively uses single-coordinate estimates to eliminate dimensions from an underlying differential operator, thus "deflating" it. These differential operators have been shown to characterize any local, spectral dimensionality reduction method. The key to our method is a novel, incremental tangent space estimator that incorporates global structure as coordinates are added. We prove its consistency when the coordinates converge to true coordinates. Empirically, we show our algorithm recovers novel and interesting embeddings on real-world and synthetic datasets.

</details>

<details>

<summary>2020-07-07 16:52:56 - Can GAN Generated Morphs Threaten Face Recognition Systems Equally as Landmark Based Morphs? -- Vulnerability and Detection</summary>

- *Sushma Venkatesh, Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, Naser Damer, Christoph Busch*

- `2007.03621v1` - [abs](http://arxiv.org/abs/2007.03621v1) - [pdf](http://arxiv.org/pdf/2007.03621v1)

> The primary objective of face morphing is to combine face images of different data subjects (e.g. a malicious actor and an accomplice) to generate a face image that can be equally verified for both contributing data subjects. In this paper, we propose a new framework for generating face morphs using a newer Generative Adversarial Network (GAN) - StyleGAN. In contrast to earlier works, we generate realistic morphs of both high-quality and high resolution of 1024$\times$1024 pixels. With the newly created morphing dataset of 2500 morphed face images, we pose a critical question in this work. \textit{(i) Can GAN generated morphs threaten Face Recognition Systems (FRS) equally as Landmark based morphs?} Seeking an answer, we benchmark the vulnerability of a Commercial-Off-The-Shelf FRS (COTS) and a deep learning-based FRS (ArcFace). This work also benchmarks the detection approaches for both GAN generated morphs against the landmark based morphs using established Morphing Attack Detection (MAD) schemes.

</details>

<details>

<summary>2020-07-07 18:41:37 - Dragoon: Private Decentralized HITs Made Practical</summary>

- *Yuan Lu, Qiang Tang, Guiling Wang*

- `2003.10074v3` - [abs](http://arxiv.org/abs/2003.10074v3) - [pdf](http://arxiv.org/pdf/2003.10074v3)

> With the rapid popularity of blockchain, decentralized human intelligence tasks (HITs) are proposed to crowdsource human knowledge without relying on vulnerable third-party platforms. However, the inherent limits of blockchain cause decentralized HITs to face a few "new" challenges. For example, the confidentiality of solicited data turns out to be the sine qua non, though it was an arguably dispensable property in the centralized setting. To ensure the "new" requirement of data privacy, existing decentralized HITs use generic zero-knowledge proof frameworks (e.g. SNARK), but scarcely perform well in practice, due to the inherently expensive cost of generality.   We present a practical decentralized protocol for HITs, which also achieves the fairness between requesters and workers. At the core of our contributions, we avoid the powerful yet highly-costly generic zk-proof tools and propose a special-purpose scheme to prove the quality of encrypted data. By various non-trivial statement reformations, proving the quality of encrypted data is reduced to efficient verifiable decryption, thus making decentralized HITs practical. Along the way, we rigorously define the ideal functionality of decentralized HITs and then prove the security due to the ideal-real paradigm.   We further instantiate our protocol to implement a system called Dragoon, an instance of which is deployed atop Ethereum to facilitate an image annotation task used by ImageNet. Our evaluations demonstrate its practicality: the on-chain handling cost of Dragoon is even less than the handling fee of Amazon's Mechanical Turk for the same ImageNet HIT.

</details>

<details>

<summary>2020-07-08 11:07:10 - How benign is benign overfitting?</summary>

- *Amartya Sanyal, Puneet K Dokania, Varun Kanade, Philip H. S. Torr*

- `2007.04028v1` - [abs](http://arxiv.org/abs/2007.04028v1) - [pdf](http://arxiv.org/pdf/2007.04028v1)

> We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting [2, 10]. However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don't fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. Standard training procedures bias neural networks towards learning "simple" classification boundaries, which may be less robust than more complex ones. We observe that adversarial training does produce more complex decision boundaries. We conjecture that in part the need for complex decision boundaries arises from sub-optimal representation learning. By means of simple toy examples, we show theoretically how the choice of representation can drastically affect adversarial robustness.

</details>

<details>

<summary>2020-07-08 12:36:21 - Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence</summary>

- *Shakir Mohamed, Marie-Therese Png, William Isaac*

- `2007.04068v1` - [abs](http://arxiv.org/abs/2007.04068v1) - [pdf](http://arxiv.org/pdf/2007.04068v1)

> This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.

</details>

<details>

<summary>2020-07-08 21:33:00 - Are PETs (Privacy Enhancing Technologies) Giving Protection for Smartphones? -- A Case Study</summary>

- *Tanusree Sharma, Masooda Bashir*

- `2007.04444v1` - [abs](http://arxiv.org/abs/2007.04444v1) - [pdf](http://arxiv.org/pdf/2007.04444v1)

> With smartphone technologies enhanced way of interacting with the world around us, it has also been paving the way for easier access to our private and personal information. This has been amplified by the existence of numerous embedded sensors utilized by millions of apps to users. While mobile apps have positively transformed many aspects of our lives with new functionalities, many of these applications are taking advantage of vast amounts of data, privacy apps, a form of Privacy Enhancing Technology can be an effective privacy management tool for smartphones. To protect against vulnerabilities related to the collection, storage, and sharing of sensitive data, developers are building numerous privacy apps. However, there has been a lack of discretion in this particular area which calls for a proper assessment to understand the far-reaching utilization of these apps among users. During this process we have conducted an evaluation of the most popular privacy apps from our total collection of five hundred and twelve to demonstrate their functionality specific data protections they are claiming to offer, both technologically and conventionally, measuring up to standards. Taking their offered security functionalities as a scale, we conducted forensic experiments to indicate where they are failing to be consistent in maintaining protection. For legitimate validation of security gaps in assessed privacy apps, we have also utilized NIST and OWASP guidelines. We believe this study will be efficacious for continuous improvement and can be considered as a foundation towards a common standard for privacy and security measures for an app's development stage.

</details>

<details>

<summary>2020-07-08 22:48:11 - Proof of Witness Presence: Blockchain Consensus for Augmented Democracy in Smart Cities</summary>

- *Evangelos Pournaras*

- `1907.00498v4` - [abs](http://arxiv.org/abs/1907.00498v4) - [pdf](http://arxiv.org/pdf/1907.00498v4)

> Smart Cities evolve into complex and pervasive urban environments with a citizens' mandate to meet sustainable development goals. Repositioning democratic values of citizens' choices in these complex ecosystems has turned out to be imperative in an era of social media filter bubbles, fake news and opportunities for manipulating electoral results with such means. This paper introduces a new paradigm of augmented democracy that promises actively engaging citizens in a more informed decision-making augmented into public urban space. The proposed concept is inspired by a digital revive of the Ancient Agora of Athens, an arena of public discourse, a Polis where citizens assemble to actively deliberate and collectively decide about public matters. The core contribution of the proposed paradigm is the concept of proving witness presence: making decision-making subject of providing secure evidence and testifying for choices made in the physical space. This paper shows how the challenge of proving witness presence can be tackled with blockchain consensus to empower citizens' trust and overcome security vulnerabilities of GPS localization. Moreover, a novel platform for collective decision-making and crowd-sensing in urban space is introduced: Smart Agora. It is shown how real-time collective measurements over citizens' choices can be made in a fully decentralized and privacy-preserving way. Witness presence is tested by deploying a decentralized system for crowd-sensing the sustainable use of transport means. Furthermore, witness presence of cycling risk is validated using official accident data from public authorities, which are compared against wisdom of the crowd. The paramount role of dynamic consensus, self-governance and ethically aligned artificial intelligence in the augmented democracy paradigm is outlined.

</details>

<details>

<summary>2020-07-09 01:02:13 - Artificial Intelligence and Machine Learning in 5G Network Security: Opportunities, advantages, and future research trends</summary>

- *Noman Haider, Muhammad Zeeshan Baig, Muhammad Imran*

- `2007.04490v1` - [abs](http://arxiv.org/abs/2007.04490v1) - [pdf](http://arxiv.org/pdf/2007.04490v1)

> Recent technological and architectural advancements in 5G networks have proven their worth as the deployment has started over the world. Key performance elevating factor from access to core network are softwareization, cloudification and virtualization of key enabling network functions. Along with the rapid evolution comes the risks, threats and vulnerabilities in the system for those who plan to exploit it. Therefore, ensuring fool proof end-to-end (E2E) security becomes a vital concern. Artificial intelligence (AI) and machine learning (ML) can play vital role in design, modelling and automation of efficient security protocols against diverse and wide range of threats. AI and ML has already proven their effectiveness in different fields for classification, identification and automation with higher accuracy. As 5G networks' primary selling point has been higher data rates and speed, it will be difficult to tackle wide range of threats from different points using typical/traditional protective measures. Therefore, AI and ML can play central role in protecting highly data-driven softwareized and virtualized network components. This article presents AI and ML driven applications for 5G network security, their implications and possible research directions. Also, an overview of key data collection points in 5G architecture for threat classification and anomaly detection are discussed.

</details>

<details>

<summary>2020-07-09 05:35:49 - Efficient detection of adversarial images</summary>

- *Darpan Kumar Yadav, Kartik Mundra, Rahul Modpur, Arpan Chattopadhyay, Indra Narayan Kar*

- `2007.04564v1` - [abs](http://arxiv.org/abs/2007.04564v1) - [pdf](http://arxiv.org/pdf/2007.04564v1)

> In this paper, detection of deception attack on deep neural network (DNN) based image classification in autonomous and cyber-physical systems is considered. Several studies have shown the vulnerability of DNN to malicious deception attacks. In such attacks, some or all pixel values of an image are modified by an external attacker, so that the change is almost invisible to the human eye but significant enough for a DNN-based classifier to misclassify it. This paper first proposes a novel pre-processing technique that facilitates the detection of such modified images under any DNN-based image classifier as well as the attacker model. The proposed pre-processing algorithm involves a certain combination of principal component analysis (PCA)-based decomposition of the image, and random perturbation based detection to reduce computational complexity. Next, an adaptive version of this algorithm is proposed where a random number of perturbations are chosen adaptively using a doubly-threshold policy, and the threshold values are learnt via stochastic approximation in order to minimize the expected number of perturbations subject to constraints on the false alarm and missed detection probabilities. Numerical experiments show that the proposed detection scheme outperforms a competing algorithm while achieving reasonably low computational complexity.

</details>

<details>

<summary>2020-07-09 10:38:59 - Green Lighting ML: Confidentiality, Integrity, and Availability of Machine Learning Systems in Deployment</summary>

- *Abhishek Gupta, Erick Galinkin*

- `2007.04693v1` - [abs](http://arxiv.org/abs/2007.04693v1) - [pdf](http://arxiv.org/pdf/2007.04693v1)

> Security and ethics are both core to ensuring that a machine learning system can be trusted. In production machine learning, there is generally a hand-off from those who build a model to those who deploy a model. In this hand-off, the engineers responsible for model deployment are often not privy to the details of the model and thus, the potential vulnerabilities associated with its usage, exposure, or compromise. Techniques such as model theft, model inversion, or model misuse may not be considered in model deployment, and so it is incumbent upon data scientists and machine learning engineers to understand these potential risks so they can communicate them to the engineers deploying and hosting their models. This is an open problem in the machine learning community and in order to help alleviate this issue, automated systems for validating privacy and security of models need to be developed, which will help to lower the burden of implementing these hand-offs and increasing the ubiquity of their adoption.

</details>

<details>

<summary>2020-07-09 18:09:55 - Node Copying for Protection Against Graph Neural Network Topology Attacks</summary>

- *Florence Regol, Soumyasundar Pal, Mark Coates*

- `2007.06704v1` - [abs](http://arxiv.org/abs/2007.06704v1) - [pdf](http://arxiv.org/pdf/2007.06704v1)

> Adversarial attacks can affect the performance of existing deep learning models. With the increased interest in graph based machine learning techniques, there have been investigations which suggest that these models are also vulnerable to attacks. In particular, corruptions of the graph topology can degrade the performance of graph based learning algorithms severely. This is due to the fact that the prediction capability of these algorithms relies mostly on the similarity structure imposed by the graph connectivity. Therefore, detecting the location of the corruption and correcting the induced errors becomes crucial. There has been some recent work which tackles the detection problem, however these methods do not address the effect of the attack on the downstream learning task. In this work, we propose an algorithm that uses node copying to mitigate the degradation in classification that is caused by adversarial attacks. The proposed methodology is applied only after the model for the downstream task is trained and the added computation cost scales well for large graphs. Experimental results show the effectiveness of our approach for several real world datasets.

</details>

<details>

<summary>2020-07-10 06:02:05 - Sample-based Regularization: A Transfer Learning Strategy Toward Better Generalization</summary>

- *Yunho Jeon, Yongseok Choi, Jaesun Park, Subin Yi, Dongyeon Cho, Jiwon Kim*

- `2007.05181v1` - [abs](http://arxiv.org/abs/2007.05181v1) - [pdf](http://arxiv.org/pdf/2007.05181v1)

> Training a deep neural network with a small amount of data is a challenging problem as it is vulnerable to overfitting. However, one of the practical difficulties that we often face is to collect many samples. Transfer learning is a cost-effective solution to this problem. By using the source model trained with a large-scale dataset, the target model can alleviate the overfitting originated from the lack of training data. Resorting to the ability of generalization of the source model, several methods proposed to use the source knowledge during the whole training procedure. However, this is likely to restrict the potential of the target model and some transferred knowledge from the source can interfere with the training procedure. For improving the generalization performance of the target model with a few training samples, we proposed a regularization method called sample-based regularization (SBR), which does not rely on the source's knowledge during training. With SBR, we suggested a new training framework for transfer learning. Experimental results showed that our framework outperformed existing methods in various configurations.

</details>

<details>

<summary>2020-07-10 08:08:38 - SmartBugs: A Framework to Analyze Solidity Smart Contracts</summary>

- *João F. Ferreira, Pedro Cruz, Thomas Durieux, Rui Abreu*

- `2007.04771v2` - [abs](http://arxiv.org/abs/2007.04771v2) - [pdf](http://arxiv.org/pdf/2007.04771v2)

> Over the last few years, there has been substantial research on automated analysis, testing, and debugging of Ethereum smart contracts. However, it is not trivial to compare and reproduce that research. To address this, we present SmartBugs, an extensible and easy-to-use execution framework that simplifies the execution of analysis tools on smart contracts written in Solidity, the primary language used in Ethereum. SmartBugs is currently distributed with support for 10 tools and two datasets of Solidity contracts. The first dataset can be used to evaluate the precision of analysis tools, as it contains 143 annotated vulnerable contracts with 208 tagged vulnerabilities. The second dataset contains 47,518 unique contracts collected through Etherscan. We discuss how SmartBugs supported the largest experimental setup to date both in the number of tools and in execution time. Moreover, we show how it enables easy integration and comparison of analysis tools by presenting a new extension to the tool SmartCheck that improves substantially the detection of vulnerabilities related to the DASP10 categories Bad Randomness, Time Manipulation, and Access Control (identified vulnerabilities increased from 11% to 24%).

</details>

<details>

<summary>2020-07-10 10:50:21 - Improving Software Defined Cognitive and Secure Networking</summary>

- *Ijaz Ahmad*

- `2007.05296v1` - [abs](http://arxiv.org/abs/2007.05296v1) - [pdf](http://arxiv.org/pdf/2007.05296v1)

> Traditional communication networks consist of large sets of vendor-specific manually configurable devices which are hardwired with specific control logic or algorithms. The resulting networks comprise distributed control plane architectures that are complex in nature, difficult to integrate and operate, and are least efficient in terms of resource usage. However, the rapid increase in data traffic requires an integrated use of diverse access technologies and autonomic network operations with increased efficiency. Therefore, the concepts of Software Defined Networking (SDN) are proposed that decouple the network control plane from the data-forwarding plane. The SDN control plane can integrate a diverse set of devices, and tune them at run-time through vendor-agnostic programmable Application Programming Interfaces (APIs). This thesis proposes software defined cognitive networking to enable intelligent use of network resources. Different radio access technologies, including cognitive radios, are integrated through a common control platform to increase the overall network performance. The architectural framework of software defined cognitive networking is presented alongside the experimental performance evaluation. Since SDN enables applications to change the network behavior and centralizes the network control plane to oversee the whole network, it is highly important to investigate security of SDNs. Therefore, this thesis finds potential security vulnerabilities in SDN, studies proposed security platforms and architectures for those vulnerabilities, and presents future directions for unresolved security vulnerabilities. Furthermore, this thesis also investigates the potential security challenges and their solutions for the enabling technologies of 5G, such as SDN, cloud technologies, and virtual network functions, and provides key insights into increasing the security of 5G networks.

</details>

<details>

<summary>2020-07-10 11:25:31 - Generating Adversarial Inputs Using A Black-box Differential Technique</summary>

- *João Batista Pereira Matos Juúnior, Lucas Carvalho Cordeiro, Marcelo d'Amorim, Xiaowei Huang*

- `2007.05315v1` - [abs](http://arxiv.org/abs/2007.05315v1) - [pdf](http://arxiv.org/pdf/2007.05315v1)

> Neural Networks (NNs) are known to be vulnerable to adversarial attacks. A malicious agent initiates these attacks by perturbing an input into another one such that the two inputs are classified differently by the NN. In this paper, we consider a special class of adversarial examples, which can exhibit not only the weakness of NN models - as do for the typical adversarial examples - but also the different behavior between two NN models. We call them difference-inducing adversarial examples or DIAEs. Specifically, we propose DAEGEN, the first black-box differential technique for adversarial input generation. DAEGEN takes as input two NN models of the same classification problem and reports on output an adversarial example. The obtained adversarial example is a DIAE, so that it represents a point-wise difference in the input space between the two NN models. Algorithmically, DAEGEN uses a local search-based optimization algorithm to find DIAEs by iteratively perturbing an input to maximize the difference of two models on predicting the input. We conduct experiments on a spectrum of benchmark datasets (e.g., MNIST, ImageNet, and Driving) and NN models (e.g., LeNet, ResNet, Dave, and VGG). Experimental results are promising. First, we compare DAEGEN with two existing white-box differential techniques (DeepXplore and DLFuzz) and find that under the same setting, DAEGEN is 1) effective, i.e., it is the only technique that succeeds in generating attacks in all cases, 2) precise, i.e., the adversarial attacks are very likely to fool machines and humans, and 3) efficient, i.e, it requires a reasonable number of classification queries. Second, we compare DAEGEN with state-of-the-art black-box adversarial attack methods (simba and tremba), by adapting them to work on a differential setting. The experimental results show that DAEGEN performs better than both of them.

</details>

<details>

<summary>2020-07-10 13:00:50 - Tensor Convolutional Sparse Coding with Low-Rank activations, an application to EEG analysis</summary>

- *Pierre Humbert, Laurent Oudre, Nivolas Vayatis, Julien Audiffren*

- `2007.02534v2` - [abs](http://arxiv.org/abs/2007.02534v2) - [pdf](http://arxiv.org/pdf/2007.02534v2)

> Recently, there has been growing interest in the analysis of spectrograms of ElectroEncephaloGram (EEG), particularly to study the neural correlates of (un)-consciousness during General Anesthesia (GA). Indeed, it has been shown that order three tensors (channels x frequencies x times) are a natural and useful representation of these signals. However this encoding entails significant difficulties, especially for convolutional sparse coding (CSC) as existing methods do not take advantage of the particularities of tensor representation, such as rank structures, and are vulnerable to the high level of noise and perturbations that are inherent to EEG during medical acts. To address this issue, in this paper we introduce a new CSC model, named Kruskal CSC (K-CSC), that uses the Kruskal decomposition of the activation tensors to leverage the intrinsic low rank nature of these representations in order to extract relevant and interpretable encodings. Our main contribution, TC-FISTA, uses multiple tools to efficiently solve the resulting optimization problem despite the increasing complexity induced by the tensor representation. We then evaluate TC-FISTA on both synthetic dataset and real EEG recorded during GA. The results show that TC-FISTA is robust to noise and perturbations, resulting in accurate, sparse and interpretable encoding of the signals.

</details>

<details>

<summary>2020-07-10 14:02:25 - VRUNet: Multi-Task Learning Model for Intent Prediction of Vulnerable Road Users</summary>

- *Adithya Ranga, Filippo Giruzzi, Jagdish Bhanushali, Emilie Wirbel, Patrick Pérez, Tuan-Hung Vu, Xavier Perrotton*

- `2007.05397v1` - [abs](http://arxiv.org/abs/2007.05397v1) - [pdf](http://arxiv.org/pdf/2007.05397v1)

> Advanced perception and path planning are at the core for any self-driving vehicle. Autonomous vehicles need to understand the scene and intentions of other road users for safe motion planning. For urban use cases it is very important to perceive and predict the intentions of pedestrians, cyclists, scooters, etc., classified as vulnerable road users (VRU). Intent is a combination of pedestrian activities and long term trajectories defining their future motion. In this paper we propose a multi-task learning model to predict pedestrian actions, crossing intent and forecast their future path from video sequences. We have trained the model on naturalistic driving open-source JAAD dataset, which is rich in behavioral annotations and real world scenarios. Experimental results show state-of-the-art performance on JAAD dataset and how we can benefit from jointly learning and predicting actions and trajectories using 2D human pose features and scene context.

</details>

<details>

<summary>2020-07-10 15:12:02 - Adversarial Machine Learning based Partial-model Attack in IoT</summary>

- *Zhengping Luo, Shangqing Zhao, Zhuo Lu, Yalin E. Sagduyu, Jie Xu*

- `2006.14146v2` - [abs](http://arxiv.org/abs/2006.14146v2) - [pdf](http://arxiv.org/pdf/2006.14146v2)

> As Internet of Things (IoT) has emerged as the next logical stage of the Internet, it has become imperative to understand the vulnerabilities of the IoT systems when supporting diverse applications. Because machine learning has been applied in many IoT systems, the security implications of machine learning need to be studied following an adversarial machine learning approach. In this paper, we propose an adversarial machine learning based partial-model attack in the data fusion/aggregation process of IoT by only controlling a small part of the sensing devices. Our numerical results demonstrate the feasibility of this attack to disrupt the decision making in data fusion with limited control of IoT devices, e.g., the attack success rate reaches 83\% when the adversary tampers with only 8 out of 20 IoT devices. These results show that the machine learning engine of IoT system is highly vulnerable to attacks even when the adversary manipulates a small portion of IoT devices, and the outcome of these attacks severely disrupts IoT system operations.

</details>

<details>

<summary>2020-07-10 19:02:24 - Improved Detection of Adversarial Images Using Deep Neural Networks</summary>

- *Yutong Gao, Yi Pan*

- `2007.05573v1` - [abs](http://arxiv.org/abs/2007.05573v1) - [pdf](http://arxiv.org/pdf/2007.05573v1)

> Machine learning techniques are immensely deployed in both industry and academy. Recent studies indicate that machine learning models used for classification tasks are vulnerable to adversarial examples, which limits the usage of applications in the fields with high precision requirements. We propose a new approach called Feature Map Denoising to detect the adversarial inputs and show the performance of detection on the mixed dataset consisting of adversarial examples generated by different attack algorithms, which can be used to associate with any pre-trained DNNs at a low cost. Wiener filter is also introduced as the denoise algorithm to the defense model, which can further improve performance. Experimental results indicate that good accuracy of detecting the adversarial examples can be achieved through our Feature Map Denoising algorithm.

</details>

<details>

<summary>2020-07-11 17:34:17 - ManiGen: A Manifold Aided Black-box Generator of Adversarial Examples</summary>

- *Guanxiong Liu, Issa Khalil, Abdallah Khreishah, Abdulelah Algosaibi, Adel Aldalbahi, Mohammed Alaneem, Abdulaziz Alhumam, Mohammed Anan*

- `2007.05817v1` - [abs](http://arxiv.org/abs/2007.05817v1) - [pdf](http://arxiv.org/pdf/2007.05817v1)

> Machine learning models, especially neural network (NN) classifiers, have acceptable performance and accuracy that leads to their wide adoption in different aspects of our daily lives. The underlying assumption is that these models are generated and used in attack free scenarios. However, it has been shown that neural network based classifiers are vulnerable to adversarial examples. Adversarial examples are inputs with special perturbations that are ignored by human eyes while can mislead NN classifiers. Most of the existing methods for generating such perturbations require a certain level of knowledge about the target classifier, which makes them not very practical. For example, some generators require knowledge of pre-softmax logits while others utilize prediction scores.   In this paper, we design a practical black-box adversarial example generator, dubbed ManiGen. ManiGen does not require any knowledge of the inner state of the target classifier. It generates adversarial examples by searching along the manifold, which is a concise representation of input data. Through extensive set of experiments on different datasets, we show that (1) adversarial examples generated by ManiGen can mislead standalone classifiers by being as successful as the state-of-the-art white-box generator, Carlini, and (2) adversarial examples generated by ManiGen can more effectively attack classifiers with state-of-the-art defenses.

</details>

<details>

<summary>2020-07-11 18:41:47 - Understanding Object Detection Through An Adversarial Lens</summary>

- *Ka-Ho Chow, Ling Liu, Mehmet Emre Gursoy, Stacey Truex, Wenqi Wei, Yanzhao Wu*

- `2007.05828v1` - [abs](http://arxiv.org/abs/2007.05828v1) - [pdf](http://arxiv.org/pdf/2007.05828v1)

> Deep neural networks based object detection models have revolutionized computer vision and fueled the development of a wide range of visual recognition applications. However, recent studies have revealed that deep object detectors can be compromised under adversarial attacks, causing a victim detector to detect no object, fake objects, or mislabeled objects. With object detection being used pervasively in many security-critical applications, such as autonomous vehicles and smart cities, we argue that a holistic approach for an in-depth understanding of adversarial attacks and vulnerabilities of deep object detection systems is of utmost importance for the research community to develop robust defense mechanisms. This paper presents a framework for analyzing and evaluating vulnerabilities of the state-of-the-art object detectors under an adversarial lens, aiming to analyze and demystify the attack strategies, adverse effects, and costs, as well as the cross-model and cross-resolution transferability of attacks. Using a set of quantitative metrics, extensive experiments are performed on six representative deep object detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed framework can serve as a methodical benchmark for analyzing adversarial behaviors and risks in real-time object detection systems. We conjecture that this framework can also serve as a tool to assess the security risks and the adversarial robustness of deep object detectors to be deployed in real-world applications.

</details>

<details>

<summary>2020-07-12 00:45:47 - You shall not pass: Mitigating SQL Injection Attacks on Legacy Web Applications</summary>

- *Rasoul Jahanshahi, Adam Doupé, Manuel Egele*

- `2006.11996v3` - [abs](http://arxiv.org/abs/2006.11996v3) - [pdf](http://arxiv.org/pdf/2006.11996v3)

> SQL injection (SQLi) attacks pose a significant threat to the security of web applications. Existing approaches do not support object-oriented programming that renders these approaches unable to protect the real-world web apps such as Wordpress, Joomla, or Drupal against SQLi attacks. We propose a novel hybrid static-dynamic analysis for PHP web applications that limits each PHP function for accessing the database. Our tool, SQLBlock, reduces the attack surface of the vulnerable PHP functions in a web application to a set of query descriptors that demonstrate the benign functionality of the PHP function. We implement SQLBlock as a plugin for MySQL and PHP. Our approach does not require any modification to the web app. W evaluate SQLBlock on 11 SQLi vulnerabilities in Wordpress, Joomla, Drupal, Magento, and their plugins. We demonstrate that SQLBlock successfully prevents all 11 SQLi exploits with negligible performance overhead (i.e., a maximum of 3% on a heavily-loaded web server)

</details>

<details>

<summary>2020-07-12 07:26:06 - Excessive Invariance Causes Adversarial Vulnerability</summary>

- *Jörn-Henrik Jacobsen, Jens Behrmann, Richard Zemel, Matthias Bethge*

- `1811.00401v4` - [abs](http://arxiv.org/abs/1811.00401v4) - [pdf](http://arxiv.org/pdf/1811.00401v4)

> Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an information-theoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.

</details>

<details>

<summary>2020-07-12 13:53:27 - Radium: Improving Dynamic PoW Targeting</summary>

- *George Bissias*

- `2007.05991v1` - [abs](http://arxiv.org/abs/2007.05991v1) - [pdf](http://arxiv.org/pdf/2007.05991v1)

> Most PoW blockchain protocols operate with a simple mechanism whereby a threshold is set for each block and miners generate block hashes until one of those values falls below the threshold. Although largely effective, this mechanism produces blocks at a highly variable rate and also leaves a blockchain susceptible to chain death, i.e. abandonment in the event that the threshold is set too high to attract any miners. A recent innovation called real-time block rate targeting, or RTT, fixes these problems by reducing the target throughout the mining interval. RTT exhibits much less variable block times and even features the ability to fully adjust the target after each block. However, as we show in this paper, RTT also suffers from a critical vulnerability whereby miners deviate form the protocol to increase their profits. We introduce the Radium protocol, which mitigates this vulnerability in RTT while retaining lower variance block times, responsive target adjustment, and lowering the risk of chain death. We also show that Radium's susceptibility to the doublespend attack and orphaned blocks remains similar to Bitcoin.

</details>

<details>

<summary>2020-07-13 06:03:11 - The Blockchain Based Auditor on Secret key Life Cycle in Reconfigurable Platform</summary>

- *Rourab Paul, Nimisha Ghosh, Amlan Chakrabarti, Prasant Mahapatra*

- `2007.06201v1` - [abs](http://arxiv.org/abs/2007.06201v1) - [pdf](http://arxiv.org/pdf/2007.06201v1)

> The growing sophistication of cyber attacks, vulnerabilities in high computing systems and increasing dependency on cryptography to protect our digital data make it more important to keep secret keys safe and secure. Few major issues on secret keys like incorrect use of keys, inappropriate storage of keys, inadequate protection of keys, insecure movement of keys, lack of audit logging, insider threats and non-destruction of keys can compromise the whole security system dangerously. In this article, we have proposed and implemented an isolated secret key memory which can log life cycle of secret keys cryptographically using blockchain (BC) technology. We have also implemented a special custom bus interconnect which receives custom crypto instruction from Processing Element (PE). During the execution of crypto instructions, the architecture assures that secret key will never come in the processor area and the movement of secret keys to various crypto core is recorded cryptographically after the proper authentication process controlled by proposed hardware based BC. To the best of our knowledge, this is the first work which uses blockchain based solution to address the issues of the life cycle of the secret keys in hardware platform. The additional cost of resource usage and timing complexity we spent to implement the proposed idea is very nominal. We have used Xilinx Vivado EDA tool and Artix 7 FPGA board.

</details>

<details>

<summary>2020-07-13 16:57:12 - Security and Machine Learning in the Real World</summary>

- *Ivan Evtimov, Weidong Cui, Ece Kamar, Emre Kiciman, Tadayoshi Kohno, Jerry Li*

- `2007.07205v1` - [abs](http://arxiv.org/abs/2007.07205v1) - [pdf](http://arxiv.org/pdf/2007.07205v1)

> Machine learning (ML) models deployed in many safety- and business-critical systems are vulnerable to exploitation through adversarial examples. A large body of academic research has thoroughly explored the causes of these blind spots, developed sophisticated algorithms for finding them, and proposed a few promising defenses. A vast majority of these works, however, study standalone neural network models. In this work, we build on our experience evaluating the security of a machine learning software product deployed on a large scale to broaden the conversation to include a systems security view of these vulnerabilities. We describe novel challenges to implementing systems security best practices in software with ML components. In addition, we propose a list of short-term mitigation suggestions that practitioners deploying machine learning modules can use to secure their systems. Finally, we outline directions for new research into machine learning attacks and defenses that can serve to advance the state of ML systems security.

</details>

<details>

<summary>2020-07-13 17:00:58 - Adversarial Perturbations of Opinion Dynamics in Networks</summary>

- *Jason Gaitonde, Jon Kleinberg, Eva Tardos*

- `2003.07010v2` - [abs](http://arxiv.org/abs/2003.07010v2) - [pdf](http://arxiv.org/pdf/2003.07010v2)

> We study the connections between network structure, opinion dynamics, and an adversary's power to artificially induce disagreements. We approach these questions by extending models of opinion formation in the social sciences to represent scenarios, familiar from recent events, in which external actors seek to destabilize communities through sophisticated information warfare tactics via fake news and bots. In many instances, the intrinsic goals of these efforts are not necessarily to shift the overall sentiment of the network, but rather to induce discord. These perturbations diffuse via opinion dynamics on the underlying network, through mechanisms that have been analyzed and abstracted through work in computer science and the social sciences. We investigate the properties of such attacks, considering optimal strategies both for the adversary seeking to create disagreement and for the entities tasked with defending the network from attack. We show that for different formulations of these types of objectives, different regimes of the spectral structure of the network will limit the adversary's capacity to sow discord; this enables us to qualitatively describe which networks are most vulnerable against these perturbations. We then consider the algorithmic task of a network defender to mitigate these sorts of adversarial attacks by insulating nodes heterogeneously; we show that, by considering the geometry of this problem, this optimization task can be efficiently solved via convex programming. Finally, we generalize these results to allow for two network structures, where the opinion dynamics process and the measurement of disagreement become uncoupled, and determine how the adversary's power changes; for instance, this may arise when opinion dynamics are controlled an online community via social media, while disagreement is measured along "real-world" connections.

</details>

<details>

<summary>2020-07-13 18:53:45 - DERauth: A Battery-based Authentication Scheme for Distributed Energy Resources</summary>

- *Ioannis Zografopoulos, Charalambos Konstantinou*

- `2007.06625v1` - [abs](http://arxiv.org/abs/2007.06625v1) - [pdf](http://arxiv.org/pdf/2007.06625v1)

> Over the past decades, power systems have experienced drastic transformations in order to address the growth in energy demand, reduce carbon emissions, and enhance power quality and energy efficiency. This shift to the smart grid concept involves, among others, the utilization of distributed energy resources (DERs) such as rooftop solar panels and storage systems, contributing towards grid decentralization while improving control over power generation. In order to seamlessly integrate DERs into power systems, embedded devices are used to support the communication and control functions of DERs. As a result, vulnerabilities of such components can be ported to the industrial environment. Insecure control networks and protocols further exacerbate the problem. Towards reducing the attack surface, we present an authentication scheme for DERs, DERauth, which leverages the inherent entropy of the DER battery energy storage system (BESS) as a root-of-trust. The DER authentication is achieved using a challenge-reply mechanism that relies on the corresponding DER's BESS state-of-charge (SoC) and voltage measurements. A dynamically updating process ensures that the BESS state is up-to-date. We evaluate our proof-of-concept in a prototype development that uses lithium-ion (li-ion) batteries for the BESS. The robustness of our design is assessed against modeling attacks performed by neural networks.

</details>

<details>

<summary>2020-07-13 19:01:01 - Robin: A Web Security Tool</summary>

- *Guilherme Girotto, Avelino Francisco Zorzo*

- `2007.06629v1` - [abs](http://arxiv.org/abs/2007.06629v1) - [pdf](http://arxiv.org/pdf/2007.06629v1)

> Thanks to the advance of technology, all kinds of applications are becoming more complete and capable of performing complex tasks that save much of our time. But to perform these tasks, applications require that some personal information are shared, for example credit card, bank accounts, email addresses, etc. All these data must be transferred securely between the final user and the institution application. Nonetheless, several applications might contain residual flaws that may be explored by criminals in order to steal users data. Hence, to help information security professionals and developers to perform penetration tests (pentests) on web applications, this paper presents Robin: A Web Security Tool. The tool is also applied to a real case study in which a very dangerous vulnerability was found. This vulnerability is also described in this paper.

</details>

<details>

<summary>2020-07-13 21:01:40 - Surveying Vulnerable Populations: A Case Study of Civil Society Organizations</summary>

- *Nikita Samarin, Alisa Frik, Sean Brooks, Coye Cheshire, Serge Egelman*

- `2003.08580v2` - [abs](http://arxiv.org/abs/2003.08580v2) - [pdf](http://arxiv.org/pdf/2003.08580v2)

> Compared to organizations in other sectors, civil society organizations (CSOs) are particularly vulnerable to security and privacy threats, as they lack adequate resources and expertise to defend themselves. At the same time, their security needs and practices have not gained much attention among researchers, and existing solutions designed for the average users do not consider the contexts in which CSO employees operate. As part of our preliminary work, we conducted an anonymous online survey with 102 CSO employees to collect information about their perceived risks of different security and privacy threats, and their self-reported mitigation strategies. The design of our preliminary survey accounted for the unique requirements of our target population by establishing trust with respondents, using anonymity-preserving incentive strategies, and distributing the survey with the help of a trusted intermediary. However, by carefully examining our methods and the feedback received from respondents, we uncovered several issues with our methodology, including the length of the survey, the framing of the questions, and the design of the recruitment email. We hope that the discussion presented in this paper will inform and assist researchers and practitioners working on understanding and improving the security and privacy of CSOs.

</details>

<details>

<summary>2020-07-14 05:01:41 - Securing the Insecure: A First-Line-of-Defense for Nanoscale Communication Systems Operating in THz Band</summary>

- *Waqas Aman, M. Mahboob Ur Rahman, Hassan T. Abbas, Muhammad Arslan Khalid, Muhammad A. Imran, Akram Alomainy, Qammer H. Abbasi*

- `2007.06818v1` - [abs](http://arxiv.org/abs/2007.06818v1) - [pdf](http://arxiv.org/pdf/2007.06818v1)

> Nanoscale communication systems operating in Ter-ahertz (THz) band are anticipated to revolutionise the healthcaresystems of the future. Global wireless data traffic is undergoinga rapid growth. However, wireless systems, due to their broad-casting nature, are vulnerable to malicious security breaches. Inaddition, advances in quantum computing poses a risk to existingcrypto-based information security. It is of the utmost importanceto make the THz systems resilient to potential active and passiveattacks which may lead to devastating consequences, especiallywhen handling sensitive patient data in healthcare systems. Newstrategies are needed to analyse these malicious attacks and topropose viable countermeasures. In this manuscript, we presenta new authentication mechanism for nanoscale communicationsystems operating in THz band at the physical layer. We assessedan impersonation attack on a THz system. We propose usingpath loss as a fingerprint to conduct authentication via two-stephypothesis testing for a transmission device. We used hiddenMarkov Model (HMM) viterbi algorithm to enhance the outputof hypothesis testing. We also conducted transmitter identificationusing maximum likelihood and Gaussian mixture model (GMM)expectation maximization algorithms. Our simulations showedthat the error probabilities are a decreasing functions of SNR. At 10 dB with 0.2 false alarm, the detection probability was almostone. We further observed that HMM out-performs hypothesistesting at low SNR regime (10% increase in accuracy is recordedat SNR =5 dB) whereas the GMM is useful when groundtruths are noisy. Our work addresses major security gaps facedby communication system either through malicious breachesor quantum computing, enabling new applications of nanoscalesystems for Industry 4.0.

</details>

<details>

<summary>2020-07-14 05:25:15 - Towards robust sensing for Autonomous Vehicles: An adversarial perspective</summary>

- *Apostolos Modas, Ricardo Sanchez-Matilla, Pascal Frossard, Andrea Cavallaro*

- `2007.10115v1` - [abs](http://arxiv.org/abs/2007.10115v1) - [pdf](http://arxiv.org/pdf/2007.10115v1)

> Autonomous Vehicles rely on accurate and robust sensor observations for safety critical decision-making in a variety of conditions. Fundamental building blocks of such systems are sensors and classifiers that process ultrasound, RADAR, GPS, LiDAR and camera signals~\cite{Khan2018}. It is of primary importance that the resulting decisions are robust to perturbations, which can take the form of different types of nuisances and data transformations, and can even be adversarial perturbations (APs). Adversarial perturbations are purposefully crafted alterations of the environment or of the sensory measurements, with the objective of attacking and defeating the autonomous systems. A careful evaluation of the vulnerabilities of their sensing system(s) is necessary in order to build and deploy safer systems in the fast-evolving domain of AVs. To this end, we survey the emerging field of sensing in adversarial settings: after reviewing adversarial attacks on sensing modalities for autonomous systems, we discuss countermeasures and present future research directions.

</details>

<details>

<summary>2020-07-14 07:16:52 - Adversarial Detection of Flash Malware: Limitations and Open Issues</summary>

- *Davide Maiorca, Ambra Demontis, Battista Biggio, Fabio Roli, Giorgio Giacinto*

- `1710.10225v2` - [abs](http://arxiv.org/abs/1710.10225v2) - [pdf](http://arxiv.org/pdf/1710.10225v2)

> During the past four years, Flash malware has become one of the most insidious threats to detect, with almost 600 critical vulnerabilities targeting Adobe Flash disclosed in the wild. Research has shown that machine learning can be successfully used to detect Flash malware by leveraging static analysis to extract information from the structure of the file or its bytecode. However, the robustness of Flash malware detectors against well-crafted evasion attempts - also known as adversarial examples - has never been investigated. In this paper, we propose a security evaluation of a novel, representative Flash detector that embeds a combination of the prominent, static features employed by state-of-the-art tools. In particular, we discuss how to craft adversarial Flash malware examples, showing that it suffices to manipulate the corresponding source malware samples slightly to evade detection. We then empirically demonstrate that popular defense techniques proposed to mitigate evasion attempts, including re-training on adversarial examples, may not always be sufficient to ensure robustness. We argue that this occurs when the feature vectors extracted from adversarial examples become indistinguishable from those of benign data, meaning that the given feature representation is intrinsically vulnerable. In this respect, we are the first to formally define and quantitatively characterize this vulnerability, highlighting when an attack can be countered by solely improving the security of the learning algorithm, or when it requires also considering additional features. We conclude the paper by suggesting alternative research directions to improve the security of learning-based Flash malware detectors.

</details>

<details>

<summary>2020-07-14 09:01:31 - ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech</summary>

- *Xin Wang, Junichi Yamagishi, Massimiliano Todisco, Hector Delgado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vestman, Tomi Kinnunen, Kong Aik Lee, Lauri Juvela, Paavo Alku, Yu-Huai Peng, Hsin-Te Hwang, Yu Tsao, Hsin-Min Wang, Sebastien Le Maguer, Markus Becker, Fergus Henderson, Rob Clark, Yu Zhang, Quan Wang, Ye Jia, Kai Onuma, Koji Mushika, Takashi Kaneda, Yuan Jiang, Li-Juan Liu, Yi-Chiao Wu, Wen-Chin Huang, Tomoki Toda, Kou Tanaka, Hirokazu Kameoka, Ingmar Steiner, Driss Matrouf, Jean-Francois Bonastre, Avashna Govender, Srikanth Ronanki, Jing-Xuan Zhang, Zhen-Hua Ling*

- `1911.01601v4` - [abs](http://arxiv.org/abs/1911.01601v4) - [pdf](http://arxiv.org/pdf/1911.01601v4)

> Automatic speaker verification (ASV) is one of the most natural and convenient means of biometric person recognition. Unfortunately, just like all other biometric systems, ASV is vulnerable to spoofing, also referred to as "presentation attacks." These vulnerabilities are generally unacceptable and call for spoofing countermeasures or "presentation attack detection" systems. In addition to impersonation, ASV systems are vulnerable to replay, speech synthesis, and voice conversion attacks. The ASVspoof 2019 edition is the first to consider all three spoofing attack types within a single challenge. While they originate from the same source database and same underlying protocol, they are explored in two specific use case scenarios. Spoofing attacks within a logical access (LA) scenario are generated with the latest speech synthesis and voice conversion technologies, including state-of-the-art neural acoustic and waveform model techniques. Replay spoofing attacks within a physical access (PA) scenario are generated through carefully controlled simulations that support much more revealing analysis than possible previously. Also new to the 2019 edition is the use of the tandem detection cost function metric, which reflects the impact of spoofing and countermeasures on the reliability of a fixed ASV system. This paper describes the database design, protocol, spoofing attack implementations, and baseline ASV and countermeasure results. It also describes a human assessment on spoofed data in logical access. It was demonstrated that the spoofing data in the ASVspoof 2019 database have varied degrees of perceived quality and similarity to the target speakers, including spoofed data that cannot be differentiated from bona-fide utterances even by human subjects.

</details>

<details>

<summary>2020-07-14 16:14:51 - multiple layers of fuzzy logic to quantify vulnerabilies in iot</summary>

- *Mohammad Shojaeshafiei, Letha Etzkorn, Michael Anderson*

- `2007.07155v1` - [abs](http://arxiv.org/abs/2007.07155v1) - [pdf](http://arxiv.org/pdf/2007.07155v1)

> Quantifying vulnerabilities of network systems has been a highly controversial issue in the fields of network security and IoT. Much research has been conducted on this purpose; however, these have many ambiguities and uncertainties. In this paper, we investigate the quantification of vulnerability in the Department of Transportation (DOT) as our proof of concept. We initiate the analysis of security requirements, using Security Quality Requirements Engineering (SQUARE) for security requirements elicitation. Then we apply published security standards such as NIST SP-800 and ISO 27001 to map our security factors and sub-factors. Finally, we propose our Multi-layered Fuzzy Logic (MFL) approach based on Goal question Metrics (GQM) to quantify network security and IoT (Mobile Devices) vulnerability in DOT.

</details>

<details>

<summary>2020-07-14 16:50:02 - Robustifying Reinforcement Learning Agents via Action Space Adversarial Training</summary>

- *Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Aakanksha, Soumik Sarkar*

- `2007.07176v1` - [abs](http://arxiv.org/abs/2007.07176v1) - [pdf](http://arxiv.org/pdf/2007.07176v1)

> Adoption of machine learning (ML)-enabled cyber-physical systems (CPS) are becoming prevalent in various sectors of modern society such as transportation, industrial, and power grids. Recent studies in deep reinforcement learning (DRL) have demonstrated its benefits in a large variety of data-driven decisions and control applications. As reliance on ML-enabled systems grows, it is imperative to study the performance of these systems under malicious state and actuator attacks. Traditional control systems employ resilient/fault-tolerant controllers that counter these attacks by correcting the system via error observations. However, in some applications, a resilient controller may not be sufficient to avoid a catastrophic failure. Ideally, a robust approach is more useful in these scenarios where a system is inherently robust (by design) to adversarial attacks. While robust control has a long history of development, robust ML is an emerging research area that has already demonstrated its relevance and urgency. However, the majority of robust ML research has focused on perception tasks and not on decision and control tasks, although the ML (specifically RL) models used for control applications are equally vulnerable to adversarial attacks. In this paper, we show that a well-performing DRL agent that is initially susceptible to action space perturbations (e.g. actuator attacks) can be robustified against similar perturbations through adversarial training.

</details>

<details>

<summary>2020-07-15 01:48:03 - Data Sampling on MDS-resistant 10th Generation Intel Core (Ice Lake)</summary>

- *Daniel Moghimi*

- `2007.07428v1` - [abs](http://arxiv.org/abs/2007.07428v1) - [pdf](http://arxiv.org/pdf/2007.07428v1)

> Microarchitectural Data Sampling (MDS) is a set of hardware vulnerabilities in Intel CPUs that allows an attacker to leak bytes of data from memory loads and stores across various security boundaries. On affected CPUs, some of these vulnerabilities were patched via microcode updates. Additionally, Intel announced that the newest microarchitectures, namely Cascade Lake and Ice Lake, were not affected by MDS. While Cascade Lake turned out to be vulnerable to the ZombieLoad v2 MDS attack (also known as TAA), Ice Lake was not affected by this attack.   In this technical report, we show a variant of MSBDS (CVE2018-12126), an MDS attack, also known as Fallout, that works on Ice Lake CPUs. This variant was automatically synthesized using Transynther, a tool to find new variants of Meltdown-type attacks. Based on the findings of Transynther, we analyze different microcodes regarding this issue, showing that only microcode versions after January 2020 prevent exploitation of the vulnerability. These results show that Transynther is a valuable tool to find new variants, and also to test for regressions possibly introduced with microcode updates.

</details>

<details>

<summary>2020-07-15 15:35:17 - Mitigating Sybils in Federated Learning Poisoning</summary>

- *Clement Fung, Chris J. M. Yoon, Ivan Beschastnikh*

- `1808.04866v5` - [abs](http://arxiv.org/abs/1808.04866v5) - [pdf](http://arxiv.org/pdf/1808.04866v5)

> Machine learning (ML) over distributed multi-party data is required for a variety of domains. Existing approaches, such as federated learning, collect the outputs computed by a group of devices at a central aggregator and run iterative algorithms to train a globally shared model. Unfortunately, such approaches are susceptible to a variety of attacks, including model poisoning, which is made substantially worse in the presence of sybils.   In this paper we first evaluate the vulnerability of federated learning to sybil-based poisoning attacks. We then describe \emph{FoolsGold}, a novel defense to this problem that identifies poisoning sybils based on the diversity of client updates in the distributed learning process. Unlike prior work, our system does not bound the expected number of attackers, requires no auxiliary information outside of the learning process, and makes fewer assumptions about clients and their data.   In our evaluation we show that FoolsGold exceeds the capabilities of existing state of the art approaches to countering sybil-based label-flipping and backdoor poisoning attacks. Our results hold for different distributions of client data, varying poisoning targets, and various sybil strategies.   Code can be found at: https://github.com/DistributedML/FoolsGold

</details>

<details>

<summary>2020-07-16 00:05:21 - Understanding the Relationship between Social Distancing Policies, Traffic Volume, Air Quality, and the Prevalence of COVID-19 Outcomes in Urban Neighborhoods</summary>

- *Daniel L. Mendoza, Tabitha M. Benney, Rajive Ganguli, Rambabu Pothina, Benjamin Krick, Cheryl S. Pirozzi, Erik T. Crosman, Yue Zhang*

- `2008.01828v1` - [abs](http://arxiv.org/abs/2008.01828v1) - [pdf](http://arxiv.org/pdf/2008.01828v1)

> In response to the COVID-19 pandemic, governments have implemented policies to curb the spread of the novel virus. Little is known about how these policies impact various groups in society. This paper explores the relationship between social distancing policies, traffic volumes and air quality and how they impact various socioeconomic groups. This study aims to understand how disparate communities respond to Stay-at-Home Orders and other social distancing policies to understand how human behavior in response to policy may play a part in the prevalence of COVID-19 positive cases. We collected data on traffic density, air quality, socio-economic status, and positive cases rates of COVID-19 for each zip code of Salt Lake County, Utah (USA) between February 17 and June 12, 2020. We studied the impact of social distancing policies across three periods of policy implementation. We found that wealthier and whiter zip codes experienced a greater reduction in traffic and air pollution during the Stay-at-Home period. However, air quality did not necessarily follow traffic volumes in every case due to the complexity of interactions between emissions and meteorology. We also found a strong relationship between lower socioeconomic status and positive COVID-19 rates. This study provides initial evidence for social distancing's effectiveness in limiting the spread of COVID-19, while providing insight into how socioeconomic status has compounded vulnerability during this crisis. Behavior restrictions disproportionately benefit whiter and wealthier communities both through protection from spread of COVID-19 and reduction in air pollution. Such findings may be further compounded by the impacts of air pollution, which likely exacerbate COVID-19 transmission and mortality rates. Policy makers need to consider adapting social distancing policies to maximize equity in health protection.

</details>

<details>

<summary>2020-07-16 00:11:08 - A Survey on Security Attacks and Defense Techniques for Connected and Autonomous Vehicles</summary>

- *Minh Pham, Kaiqi Xiong*

- `2007.08041v1` - [abs](http://arxiv.org/abs/2007.08041v1) - [pdf](http://arxiv.org/pdf/2007.08041v1)

> Autonomous Vehicle has been transforming intelligent transportation systems. As telecommunication technology improves, autonomous vehicles are getting connected to each other and to infrastructures, forming Connected and Autonomous Vehicles (CAVs). CAVs will help humans achieve safe, efficient, and autonomous transportation systems. However, CAVs will face significant security challenges because many of their components are vulnerable to attacks, and a successful attack on a CAV may have significant impacts on other CAVs and infrastructures due to their communications. In this paper, we conduct a survey on 184 papers from 2000 to 2020 to understand state-of-the-art CAV attacks and defense techniques. This survey first presents a comprehensive overview of security attacks and their corresponding countermeasures on CAVs. We then discuss the details of attack models based on the targeted CAV components of attacks, access requirements, and attack motives. Finally, we identify some current research challenges and trends from the perspectives of both academic research and industrial development. Based on our studies of academic literature and industrial publications, we have not found any strong connection between academic research and industry's implementation on CAV-related security issues. While efforts from CAV manufacturers to secure CAVs have been reported, there is no evidence to show that CAVs on the market have the ability to defend against some novel attack models that the research community has recently found. This survey may give researchers and engineers a better understanding of the current status and trend of CAV security for CAV future improvement.

</details>

<details>

<summary>2020-07-16 08:29:53 - TeeRex: Discovery and Exploitation of Memory Corruption Vulnerabilities in SGX Enclaves</summary>

- *Tobias Cloosters, Michael Rodler, Lucas Davi*

- `2007.07586v2` - [abs](http://arxiv.org/abs/2007.07586v2) - [pdf](http://arxiv.org/pdf/2007.07586v2)

> Intel's Software Guard Extensions (SGX) introduced new instructions to switch the processor to enclave mode which protects it from introspection. While the enclave mode strongly protects the memory and the state of the processor, it cannot withstand memory corruption errors inside the enclave code. In this paper, we show that the attack surface of SGX enclaves provides new challenges for enclave developers as exploitable memory corruption vulnerabilities are easily introduced into enclave code. We develop TeeRex to automatically analyze enclave binary code for vulnerabilities introduced at the host-to-enclave boundary by means of symbolic execution. Our evaluation on public enclave binaries reveal that many of them suffer from memory corruption errors allowing an attacker to corrupt function pointers or perform arbitrary memory writes. As we will show, TeeRex features a specifically tailored framework for SGX enclaves that allows simple proof-of-concept exploit construction to assess the discovered vulnerabilities. Our findings reveal vulnerabilities in multiple enclaves, including enclaves developed by Intel, Baidu, and WolfSSL, as well as biometric fingerprint software deployed on popular laptop brands.

</details>

<details>

<summary>2020-07-16 09:24:51 - SMEs Confidentiality Issues and Adoption of Good Cybersecurity Practices</summary>

- *Alireza Shojaifar*

- `2007.08201v1` - [abs](http://arxiv.org/abs/2007.08201v1) - [pdf](http://arxiv.org/pdf/2007.08201v1)

> Small and medium-sized enterprises (SME) are considered more vulnerable to cyber-attacks. However, and based on SMEs characteristics, they do not adopt good cybersecurity practices. To address the SMEs security adoption problem, we are designing a do-it-yourself (DIY) security assessment and capability improvement method, CYSEC. In the first validation of CYSEC, we conducted a multi-case study in four SMEs. We observed that confidentiality concerns could influence users decisions to provide CYSEC with relevant and accurate security information. The lack of precise information may impact our DIY assessment method to provide accurate recommendations. In this paper, we explore the importance of dynamic consent and its effect on SMEs trust perception and sharing information. We discuss the lack of trust perception may be addressed by applying dynamic consent. Finally, we describe the results of three interviews with SMEs and present how the new way of communication in CYSEC can help us to understand better SMEs attitudes towards sharing information.

</details>

<details>

<summary>2020-07-16 12:16:06 - AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds</summary>

- *Abdullah Hamdi, Sara Rojas, Ali Thabet, Bernard Ghanem*

- `1912.00461v2` - [abs](http://arxiv.org/abs/1912.00461v2) - [pdf](http://arxiv.org/pdf/1912.00461v2)

> Deep neural networks are vulnerable to adversarial attacks, in which imperceptible perturbations to their input lead to erroneous network predictions. This phenomenon has been extensively studied in the image domain, and has only recently been extended to 3D point clouds. In this work, we present novel data-driven adversarial attacks against 3D point cloud networks. We aim to address the following problems in current 3D point cloud adversarial attacks: they do not transfer well between different networks, and they are easy to defend against via simple statistical methods. To this extent, we develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes. AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-the-art attacks. We test AdvPC using four popular point cloud networks: PointNet, PointNet++ (MSG and SSG), and DGCNN. Our proposed attack increases the attack success rate by up to 40% for those transferred to unseen networks (transferability), while maintaining a high success rate on the attacked network. AdvPC also increases the ability to break defenses by up to 38% as compared to other baselines on the ModelNet40 dataset.

</details>

<details>

<summary>2020-07-16 12:33:28 - Deep ahead-of-threat virtual patching</summary>

- *Fady Copty, Andre Kassis, Sharon Keidar-Barner, Dov Murik*

- `2007.08296v1` - [abs](http://arxiv.org/abs/2007.08296v1) - [pdf](http://arxiv.org/pdf/2007.08296v1)

> Many applications have security vulnerabilities that can be exploited. It is practically impossible to find all of them due to the NP-complete nature of the testing problem. Security solutions provide defenses against these attacks through continuous application testing, fast-patching of vulnerabilities, automatic deployment of patches, and virtual patching detection techniques deployed in network and endpoint security tools. These techniques are limited by the need to find vulnerabilities before the black-hats. We propose an innovative technique to virtually patch vulnerabilities before they are found. We leverage testing techniques for supervised-learning data generation, and show how artificial intelligence techniques can use this data to create predictive deep neural-network models that read an application's input and predict in real time whether it is a potential malicious input. We set up an ahead-of-threat experiment in which we generated data on old versions of an application, and then evaluated the predictive model accuracy on vulnerabilities found years later. Our experiments show ahead-of-threat detection on LibXML2 and LibTIFF vulnerabilities with 91.3% and 93.7% accuracy, respectively. We expect to continue work on this field of research and provide ahead-of-threat virtual patching for more libraries. Success in this research can change the current state of endless racing after application vulnerabilities and put the defenders one step ahead of the attackers

</details>

<details>

<summary>2020-07-16 13:44:38 - Adversarial Robustness Assessment: Why both $L_0$ and $L_\infty$ Attacks Are Necessary</summary>

- *Shashank Kotyan, Danilo Vasconcellos Vargas*

- `1906.06026v3` - [abs](http://arxiv.org/abs/1906.06026v3) - [pdf](http://arxiv.org/pdf/1906.06026v3)

> There exists a vast number of adversarial attacks and defences for machine learning algorithms of various types which makes assessing the robustness of algorithms a daunting task. To make matters worse, there is an intrinsic bias in these adversarial algorithms. Here, we organise the problems faced: a) Model Dependence, b) Insufficient Evaluation, c) False Adversarial Samples, and d) Perturbation Dependent Results). Based on this, we propose a model agnostic dual quality assessment method, together with the concept of robustness levels to tackle them. We validate the dual quality assessment on state-of-the-art neural networks (WideResNet, ResNet, AllConv, DenseNet, NIN, LeNet and CapsNet) as well as adversarial defences for image classification problem. We further show that current networks and defences are vulnerable at all levels of robustness. The proposed robustness assessment reveals that depending on the metric used (i.e., $L_0$ or $L_\infty$), the robustness may vary significantly. Hence, the duality should be taken into account for a correct evaluation. Moreover, a mathematical derivation, as well as a counter-example, suggest that $L_1$ and $L_2$ metrics alone are not sufficient to avoid spurious adversarial samples. Interestingly, the threshold attack of the proposed assessment is a novel $L_\infty$ black-box adversarial method which requires even less perturbation than the One-Pixel Attack (only $12\%$ of One-Pixel Attack's amount of perturbation) to achieve similar results.   Code is available at http://bit.ly/DualQualityAssessment.

</details>

<details>

<summary>2020-07-16 14:49:14 - Representation Quality Of Neural Networks Links To Adversarial Attacks and Defences</summary>

- *Shashank Kotyan, Danilo Vasconcellos Vargas, Moe Matsuki*

- `1906.06627v5` - [abs](http://arxiv.org/abs/1906.06627v5) - [pdf](http://arxiv.org/pdf/1906.06627v5)

> Neural networks have been shown vulnerable to a variety of adversarial algorithms. A crucial step to understanding the rationale for this lack of robustness is to assess the potential of the neural networks' representation to encode the existing features. Here, we propose a method to understand the representation quality of the neural networks using a novel test based on Zero-Shot Learning, entitled Raw Zero-Shot. The principal idea is that, if an algorithm learns rich features, such features should be able to interpret "unknown" classes as an aggregate of previously learned features. This is because unknown classes usually share several regular features with recognised classes, given the features learned are general enough. We further introduce two metrics to assess these learned features to interpret unknown classes. One is based on inter-cluster validation technique (Davies-Bouldin Index), and the other is based on the distance to an approximated ground-truth. Experiments suggest that adversarial defences improve the representation of the classifiers, further suggesting that to improve the robustness of the classifiers, one has to improve the representation quality also. Experiments also reveal a strong association (a high Pearson Correlation and low p-value) between the metrics and adversarial attacks. Interestingly, the results indicate that dynamic routing networks such as CapsNet have better representation while current deeper neural networks are trading off representation quality for accuracy.   Code available at http://bit.ly/RepresentationMetrics.

</details>

<details>

<summary>2020-07-16 17:10:39 - Vulnerability-Aware Resilient Networks: Software Diversity-based Network Adaptation</summary>

- *Qisheng Zhang, Jin-Hee Cho, Terrence J. Moore, Ing-Ray Chen*

- `2007.08469v1` - [abs](http://arxiv.org/abs/2007.08469v1) - [pdf](http://arxiv.org/pdf/2007.08469v1)

> By leveraging the principle of software polyculture to ensure security in a network, we proposed a vulnerability-based software diversity metric to determine how a network topology can be adapted to minimize security vulnerability while maintaining maximum network connectivity. Our proposed software diversity-based adaptation (SDA) scheme estimates a node's software diversity based on the vulnerabilities of software packages installed on other nodes on attack paths reachable to the node and employs it for edge adaptations, such as removing an edge with a neighboring node that exposes high security vulnerability because two connected nodes use the same software packages or a neighboring node may have high software vulnerability or adding an edge with another node with less or no security vulnerability because the two nodes use different software packages or have low vulnerabilities associated with them. To validate the proposed SDA scheme, we conducted extensive experiments comparing the proposed SDA scheme with counterpart baseline schemes in real networks. Our simulation experimental results proved the outperformance of our proposed SDA compared to the existing counterparts and provided insightful findings in terms of the effectiveness and efficiency of the proposed SDA scheme under three real network topologies with vastly different network density.

</details>

<details>

<summary>2020-07-16 23:14:34 - Tiny noise, big mistakes: Adversarial perturbations induce errors in Brain-Computer Interface spellers</summary>

- *Xiao Zhang, Dongrui Wu, Lieyun Ding, Hanbin Luo, Chin-Teng Lin, Tzyy-Ping Jung, Ricardo Chavarriaga*

- `2001.11569v4` - [abs](http://arxiv.org/abs/2001.11569v4) - [pdf](http://arxiv.org/pdf/2001.11569v4)

> An electroencephalogram (EEG) based brain-computer interface (BCI) speller allows a user to input text to a computer by thought. It is particularly useful to severely disabled individuals, e.g., amyotrophic lateral sclerosis patients, who have no other effective means of communication with another person or a computer. Most studies so far focused on making EEG-based BCI spellers faster and more reliable; however, few have considered their security. This study, for the first time, shows that P300 and steady-state visual evoked potential BCI spellers are very vulnerable, i.e., they can be severely attacked by adversarial perturbations, which are too tiny to be noticed when added to EEG signals, but can mislead the spellers to spell anything the attacker wants. The consequence could range from merely user frustration to severe misdiagnosis in clinical applications. We hope our research can attract more attention to the security of EEG-based BCI spellers, and more broadly, EEG-based BCIs, which has received little attention before.

</details>

<details>

<summary>2020-07-17 01:56:28 - Understanding and Diagnosing Vulnerability under Adversarial Attacks</summary>

- *Haizhong Zheng, Ziqi Zhang, Honglak Lee, Atul Prakash*

- `2007.08716v1` - [abs](http://arxiv.org/abs/2007.08716v1) - [pdf](http://arxiv.org/pdf/2007.08716v1)

> Deep Neural Networks (DNNs) are known to be vulnerable to adversarial attacks. Currently, there is no clear insight into how slight perturbations cause such a large difference in classification results and how we can design a more robust model architecture. In this work, we propose a novel interpretability method, InterpretGAN, to generate explanations for features used for classification in latent variables. Interpreting the classification process of adversarial examples exposes how adversarial perturbations influence features layer by layer as well as which features are modified by perturbations. Moreover, we design the first diagnostic method to quantify the vulnerability contributed by each layer, which can be used to identify vulnerable parts of model architectures. The diagnostic results show that the layers introducing more information loss tend to be more vulnerable than other layers. Based on the findings, our evaluation results on MNIST and CIFAR10 datasets suggest that average pooling layers, with lower information loss, are more robust than max pooling layers for the network architectures studied in this paper.

</details>

<details>

<summary>2020-07-17 03:00:45 - Senior Living Communities: Made Safer by AI</summary>

- *Ashutosh Saxena, David R Cheriton*

- `2007.05129v3` - [abs](http://arxiv.org/abs/2007.05129v3) - [pdf](http://arxiv.org/pdf/2007.05129v3)

> There is a historically unprecedented shift in demographics towards seniors, which will result in significant housing development over the coming decade. This is an enormous opportunity for real-estate operators to innovate and address the demand in this growing market. However, investments in this area are fraught with risk. Seniors often have more health issues, and Covid-19 has exposed just how vulnerable they are -- especially those living in close proximity. Conventionally, most services for seniors are "high-touch", requiring close physical contact with trained caregivers. Not only are trained caregivers short in supply, but the pandemic has made it evident that conventional high-touch approaches to senior care are high-cost and greater risk. There are not enough caregivers to meet the needs of this emerging demographic, and even fewer who want to undertake the additional training and risk of working in a senior facility, especially given the current pandemic. In this article, we rethink the design of senior living facilities to mitigate the risks and costs using automation. With AI-enabled pervasive automation, we claim there is an opportunity, if not an urgency, to go from high-touch to almost "no touch" while dramatically reducing risk and cost. Although our vision goes beyond the current reality, we cite measurements from Caspar AI-enabled senior properties that show the potential benefit of this approach.

</details>

<details>

<summary>2020-07-17 07:39:21 - INDRA: Intrusion Detection using Recurrent Autoencoders in Automotive Embedded Systems</summary>

- *Vipin Kumar Kukkala, Sooryaa Vignesh Thiruloga, Sudeep Pasricha*

- `2007.08795v1` - [abs](http://arxiv.org/abs/2007.08795v1) - [pdf](http://arxiv.org/pdf/2007.08795v1)

> Today's vehicles are complex distributed embedded systems that are increasingly being connected to various external systems. Unfortunately, this increased connectivity makes the vehicles vulnerable to security attacks that can be catastrophic. In this work, we present a novel Intrusion Detection System (IDS) called INDRA that utilizes a Gated Recurrent Unit (GRU) based recurrent autoencoder to detect anomalies in Controller Area Network (CAN) bus-based automotive embedded systems. We evaluate our proposed framework under different attack scenarios and also compare it with the best known prior works in this area.

</details>

<details>

<summary>2020-07-17 09:15:29 - Speculative Leakage in ARM Cortex-A53</summary>

- *Hamed Nemati, Roberto Guanciale, Pablo Buiras, Andreas Lindner*

- `2007.06865v2` - [abs](http://arxiv.org/abs/2007.06865v2) - [pdf](http://arxiv.org/pdf/2007.06865v2)

> The recent Spectre attacks have demonstrated that modern microarchitectural optimizations can make software insecure. These attacks use features like pipelining, out-of-order and speculation to extract information about the memory contents of a process via side-channels. In this paper we demonstrate that Cortex-A53 is affected by speculative leakage even if the microarchitecture does not support out-of-order execution. We named this new class of vulnerabilities SiSCloak.

</details>

<details>

<summary>2020-07-17 12:56:58 - Always-On 674uW @ 4GOP/s Error Resilient Binary Neural Networks with Aggressive SRAM Voltage Scaling on a 22nm IoT End-Node</summary>

- *Alfio Di Mauro, Francesco Conti, Pasquale Davide Schiavone, Davide Rossi, Luca Benini*

- `2007.08952v1` - [abs](http://arxiv.org/abs/2007.08952v1) - [pdf](http://arxiv.org/pdf/2007.08952v1)

> Binary Neural Networks (BNNs) have been shown to be robust to random bit-level noise, making aggressive voltage scaling attractive as a power-saving technique for both logic and SRAMs. In this work, we introduce the first fully programmable IoT end-node system-on-chip (SoC) capable of executing software-defined, hardware-accelerated BNNs at ultra-low voltage. Our SoC exploits a hybrid memory scheme where error-vulnerable SRAMs are complemented by reliable standard-cell memories to safely store critical data under aggressive voltage scaling. On a prototype in 22nm FDX technology, we demonstrate that both the logic and SRAM voltage can be dropped to 0.5Vwithout any accuracy penalty on a BNN trained for the CIFAR-10 dataset, improving energy efficiency by 2.2X w.r.t. nominal conditions. Furthermore, we show that the supply voltage can be dropped to 0.42V (50% of nominal) while keeping more than99% of the nominal accuracy (with a bit error rate ~1/1000). In this operating point, our prototype performs 4Gop/s (15.4Inference/s on the CIFAR-10 dataset) by computing up to 13binary ops per pJ, achieving 22.8 Inference/s/mW while keeping within a peak power envelope of 674uW - low enough to enable always-on operation in ultra-low power smart cameras, long-lifetime environmental sensors, and insect-sized pico-drones.

</details>

<details>

<summary>2020-07-17 13:01:44 - Constraint-Based Software Diversification for Efficient Mitigation of Code-Reuse Attacks</summary>

- *Rodothea Myrsini Tsoupidi, Roberto Castañeda Lozano, Benoit Baudry*

- `2007.08955v1` - [abs](http://arxiv.org/abs/2007.08955v1) - [pdf](http://arxiv.org/pdf/2007.08955v1)

> Modern software deployment process produces software that is uniform, and hence vulnerable to large-scale code-reuse attacks. Compiler-based diversification improves the resilience and security of software systems by automatically generating different assembly code versions of a given program. Existing techniques are efficient but do not have a precise control over the quality of the generated code variants.   This paper introduces Diversity by Construction (DivCon), a constraint-based compiler approach to software diversification. Unlike previous approaches, DivCon allows users to control and adjust the conflicting goals of diversity and code quality. A key enabler is the use of Large Neighborhood Search (LNS) to generate highly diverse assembly code efficiently.   Experiments using two popular compiler benchmark suites confirm that there is a trade-off between quality of each assembly code version and diversity of the entire pool of versions. Our results show that DivCon allows users to trade between these two properties by generating diverse assembly code for a range of quality bounds. In particular, the experiments show that DivCon is able to mitigate code-reuse attacks effectively while delivering near-optimal code (< 10% optimality gap).   For constraint programming researchers and practitioners, this paper demonstrates that LNS is a valuable technique for finding diverse solutions. For security researchers and software engineers, DivCon extends the scope of compiler-based diversification to performance-critical and resource-constrained applications.

</details>

<details>

<summary>2020-07-17 23:00:39 - HARMer: Cyber-attacks Automation and Evaluation</summary>

- *Simon Yusuf Enoch, Zhibin Huang, Chun Yong Moon, Donghwan Lee, Myung Kil Ahn, Dong Seong Kim*

- `2006.14352v3` - [abs](http://arxiv.org/abs/2006.14352v3) - [pdf](http://arxiv.org/pdf/2006.14352v3)

> With the increasing growth of cyber-attack incidences, it is important to develop innovative and effective techniques to assess and defend networked systems against cyber attacks. One of the well-known techniques for this is performing penetration testing which is carried by a group of security professionals (i.e, red team). Penetration testing is also known to be effective to find existing and new vulnerabilities, however, the quality of security assessment can be depending on the quality of the red team members and their time and devotion to the penetration testing. In this paper, we propose a novel automation framework for cyber-attacks generation named `HARMer' to address the challenges with respect to manual attack execution by the red team. Our novel proposed framework, design, and implementation is based on a scalable graphical security model called Hierarchical Attack Representation Model (HARM). (1) We propose the requirements and the key phases for the automation framework. (2) We propose security metrics-based attack planning strategies along with their algorithms. (3) We conduct experiments in a real enterprise network and Amazon Web Services. The results show how the different phases of the framework interact to model the attackers' operations. This framework will allow security administrators to automatically assess the impact of various threats and attacks in an automated manner.

</details>

<details>

<summary>2020-07-18 13:53:24 - Building a COVID-19 Vulnerability Index</summary>

- *Dave DeCaprio, Joseph Gartner, Thadeus Burgess, Kristian Garcia, Sarthak Kothari, Shaayan Sayed, Carol J. McCall*

- `2003.07347v3` - [abs](http://arxiv.org/abs/2003.07347v3) - [pdf](http://arxiv.org/pdf/2003.07347v3)

> COVID-19 is an acute respiratory disease that has been classified as a pandemic by the World Health Organization. Characterization of this disease is still in its early stages. However, it is known to have high mortality rates, particularly among individuals with preexisting medical conditions. Creating models to identify individuals who are at the greatest risk for severe complications due to COVID-19 will be useful for outreach campaigns to help mitigate the disease's worst effects. While information specific to COVID-19 is limited, a model using complications due to other upper respiratory infections can be used as a proxy to help identify those individuals who are at the greatest risk. We present the results for three models predicting such complications, with each model increasing predictive effectiveness at the expense of ease of implementation.

</details>

<details>

<summary>2020-07-18 16:07:35 - Attributional Robustness Training using Input-Gradient Spatial Alignment</summary>

- *Mayank Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, Vineeth N Balasubramanian, Balaji Krishnamurthy*

- `1911.13073v4` - [abs](http://arxiv.org/abs/1911.13073v4) - [pdf](http://arxiv.org/pdf/1911.13073v4)

> Interpretability is an emerging area of research in trustworthy machine learning. Safe deployment of machine learning system mandates that the prediction and its explanation be reliable and robust. Recently, it has been shown that the explanations could be manipulated easily by adding visually imperceptible perturbations to the input while keeping the model's prediction intact. In this work, we study the problem of attributional robustness (i.e. models having robust explanations) by showing an upper bound for attributional vulnerability in terms of spatial correlation between the input image and its explanation map. We propose a training methodology that learns robust features by minimizing this upper bound using soft-margin triplet loss. Our methodology of robust attribution training (\textit{ART}) achieves the new state-of-the-art attributional robustness measure by a margin of $\approx$ 6-18 $\%$ on several standard datasets, ie. SVHN, CIFAR-10 and GTSRB. We further show the utility of the proposed robust training technique (\textit{ART}) in the downstream task of weakly supervised object localization by achieving the new state-of-the-art performance on CUB-200 dataset.

</details>

<details>

<summary>2020-07-20 22:55:26 - Manipulating Reinforcement Learning: Poisoning Attacks on Cost Signals</summary>

- *Yunhan Huang, Quanyan Zhu*

- `2002.03827v2` - [abs](http://arxiv.org/abs/2002.03827v2) - [pdf](http://arxiv.org/pdf/2002.03827v2)

> This chapter studies emerging cyber-attacks on reinforcement learning (RL) and introduces a quantitative approach to analyze the vulnerabilities of RL. Focusing on adversarial manipulation on the cost signals, we analyze the performance degradation of TD($\lambda$) and $Q$-learning algorithms under the manipulation. For TD($\lambda$), the approximation learned from the manipulated costs has an approximation error bound proportional to the magnitude of the attack. The effect of the adversarial attacks on the bound does not depend on the choice of $\lambda$. In $Q$-learning, we show that $Q$-learning algorithms converge under stealthy attacks and bounded falsifications on cost signals. We characterize the relation between the falsified cost and the $Q$-factors as well as the policy learned by the learning agent which provides fundamental limits for feasible offensive and defensive moves. We propose a robust region in terms of the cost within which the adversary can never achieve the targeted policy. We provide conditions on the falsified cost which can mislead the agent to learn an adversary's favored policy. A case study of TD($\lambda$) learning is provided to corroborate the results.

</details>

<details>

<summary>2020-07-21 11:11:35 - On Analyzing Antisocial Behaviors Amid COVID-19 Pandemic</summary>

- *Md Rabiul Awal, Rui Cao, Sandra Mitrovic, Roy Ka-Wei Lee*

- `2007.10712v1` - [abs](http://arxiv.org/abs/2007.10712v1) - [pdf](http://arxiv.org/pdf/2007.10712v1)

> The COVID-19 pandemic has developed to be more than a bio-crisis as global news has reported a sharp rise in xenophobia and discrimination in both online and offline communities. Such toxic behaviors take a heavy toll on society, especially during these daunting times. Despite the gravity of the issue, very few studies have studied online antisocial behaviors amid the COVID-19 pandemic. In this paper, we fill the research gap by collecting and annotating a large dataset of over 40 million COVID-19 related tweets. Specially, we propose an annotation framework to annotate the antisocial behavior tweets automatically. We also conduct an empirical analysis of our annotated dataset and found that new abusive lexicons are introduced amid the COVID-19 pandemic. Our study also identified the vulnerable targets of antisocial behaviors and the factors that influence the spreading of online antisocial content.

</details>

<details>

<summary>2020-07-21 12:33:29 - Authentication against Man-in-the-Middle Attack with a Time-variant Reconfigurable Dual-LFSR-based Arbiter PUF</summary>

- *Yao Wang, Zhengtai Chang*

- `2007.10755v1` - [abs](http://arxiv.org/abs/2007.10755v1) - [pdf](http://arxiv.org/pdf/2007.10755v1)

> With the expansion of the Internet of Things industry, the information security of Internet of Things devices attracts much attention. Traditional encryption algorithms require sensitive information such as keys to be stored in memory, and also need the support of operating system, which is obviously unacceptable for resource-constrained Internet of Things terminals. Physical not cloning function by extracting the chip is inevitable in the process of manufacturing process deviation, the introduction of the corresponding function relationship between incentive and response, not to need the storage user sensitive information, and only when electricity will respond, in power response immediately disappear, this can save a lot of resources of equipment and the power consumption. However, PUF is vulnerable to modeling attacks, and the traditional methods such as the challenge obfuscation method are time-invariant, which is equivalent to adding a fixed function to the front stage of a traditional APUF circuit. Therefore, it can be potentially modelling attacked with sufficient CRPs. In order to further enhance APUF circuit resistance to modelling attack, this paper proposes a dual-LFSR-based APUF circuit with time-variant challenge obfuscation. Besides, traditional authentication scheme generally adopts the one-time key scheme to enhance resistance to man-in-the-middle attack. The two-time authentication scheme proposed in this paper can improve the ability of RFID system to resist man-in-the-middle attack without sacrificing CRPs.

</details>

<details>

<summary>2020-07-21 17:42:56 - SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems</summary>

- *Hadi Abdullah, Kevin Warren, Vincent Bindschaedler, Nicolas Papernot, Patrick Traynor*

- `2007.06622v3` - [abs](http://arxiv.org/abs/2007.06622v3) - [pdf](http://arxiv.org/pdf/2007.06622v3)

> Speech and speaker recognition systems are employed in a variety of applications, from personal assistants to telephony surveillance and biometric authentication. The wide deployment of these systems has been made possible by the improved accuracy in neural networks. Like other systems based on neural networks, recent research has demonstrated that speech and speaker recognition systems are vulnerable to attacks using manipulated inputs. However, as we demonstrate in this paper, the end-to-end architecture of speech and speaker systems and the nature of their inputs make attacks and defenses against them substantially different than those in the image space. We demonstrate this first by systematizing existing research in this space and providing a taxonomy through which the community can evaluate future work. We then demonstrate experimentally that attacks against these models almost universally fail to transfer. In so doing, we argue that substantial additional work is required to provide adequate mitigations in this space.

</details>

<details>

<summary>2020-07-22 05:45:43 - Exploiting Behavioral Side-Channels in Observation Resilient Cognitive Authentication Schemes</summary>

- *Benjamin Zi Hao Zhao, Hassan Jameel Asghar, Mohamed Ali Kaafar, Francesca Trevisan, Haiyue Yuan*

- `2007.11210v1` - [abs](http://arxiv.org/abs/2007.11210v1) - [pdf](http://arxiv.org/pdf/2007.11210v1)

> Observation Resilient Authentication Schemes (ORAS) are a class of shared secret challenge-response identification schemes where a user mentally computes the response via a cognitive function to authenticate herself such that eavesdroppers cannot readily extract the secret. Security evaluation of ORAS generally involves quantifying information leaked via observed challenge-response pairs. However, little work has evaluated information leaked via human behavior while interacting with these schemes. A common way to achieve observation resilience is by including a modulus operation in the cognitive function. This minimizes the information leaked about the secret due to the many-to-one map from the set of possible secrets to a given response. In this work, we show that user behavior can be used as a side-channel to obtain the secret in such ORAS. Specifically, the user's eye-movement patterns and associated timing information can deduce whether a modulus operation was performed (a fundamental design element), to leak information about the secret. We further show that the secret can still be retrieved if the deduction is erroneous, a more likely case in practice. We treat the vulnerability analytically, and propose a generic attack algorithm that iteratively obtains the secret despite the "faulty" modulus information. We demonstrate the attack on five ORAS, and show that the secret can be retrieved with considerably less challenge-response pairs than non-side-channel attacks (e.g., algebraic/statistical attacks). In particular, our attack is applicable on Mod10, a one-time-pad based scheme, for which no non-side-channel attack exists. We field test our attack with a small-scale eye-tracking user study.

</details>

<details>

<summary>2020-07-23 04:12:39 - PThammer: Cross-User-Kernel-Boundary Rowhammer through Implicit Accesses</summary>

- *Zhi Zhang, Yueqiang Cheng, Dongxi Liu, Surya Nepal, Zhi Wang, Yuval Yarom*

- `2007.08707v2` - [abs](http://arxiv.org/abs/2007.08707v2) - [pdf](http://arxiv.org/pdf/2007.08707v2)

> Rowhammer is a hardware vulnerability in DRAM memory, where repeated access to memory can induce bit flips in neighboring memory locations. Being a hardware vulnerability, rowhammer bypasses all of the system memory protection, allowing adversaries to compromise the integrity and confidentiality of data. Rowhammer attacks have shown to enable privilege escalation, sandbox escape, and cryptographic key disclosures. Recently, several proposals suggest exploiting the spatial proximity between the accessed memory location and the location of the bit flip for a defense against rowhammer. These all aim to deny the attacker's permission to access memory locations near sensitive data. In this paper, we question the core assumption underlying these defenses. We present PThammer, a confused-deputy attack that causes accesses to memory locations that the attacker is not allowed to access. Specifically, PThammer exploits the address translation process of modern processors, inducing the processor to generate frequent accesses to protected memory locations. We implement PThammer, demonstrating that it is a viable attack, resulting in a system compromise (e.g., kernel privilege escalation). We further evaluate the effectiveness of proposed software-only defenses showing that PThammer can overcome those.

</details>

<details>

<summary>2020-07-24 05:19:46 - A Case Study on Software Vulnerability Coordination</summary>

- *Jukka Ruohonen, Sampsa Rauti, Sami Hyrynsalmi, Ville Leppänen*

- `2007.12356v1` - [abs](http://arxiv.org/abs/2007.12356v1) - [pdf](http://arxiv.org/pdf/2007.12356v1)

> Context: Coordination is a fundamental tenet of software engineering. Coordination is required also for identifying discovered and disclosed software vulnerabilities with Common Vulnerabilities and Exposures (CVEs). Motivated by recent practical challenges, this paper examines the coordination of CVEs for open source projects through a public mailing list. Objective: The paper observes the historical time delays between the assignment of CVEs on a mailing list and the later appearance of these in the National Vulnerability Database (NVD). Drawing from research on software engineering coordination, software vulnerabilities, and bug tracking, the delays are modeled through three dimensions: social networks and communication practices, tracking infrastructures, and the technical characteristics of the CVEs coordinated. Method: Given a period between 2008 and 2016, a sample of over five thousand CVEs is used to model the delays with nearly fifty explanatory metrics. Regression analysis is used for the modeling. Results: The results show that the CVE coordination delays are affected by different abstractions for noise and prerequisite constraints. These abstractions convey effects from the social network and infrastructure dimensions. Particularly strong effect sizes are observed for annual and monthly control metrics, a control metric for weekends, the degrees of the nodes in the CVE coordination networks, and the number of references given in NVD for the CVEs archived. Smaller but visible effects are present for metrics measuring the entropy of the emails exchanged, traces to bug tracking systems, and other related aspects. The empirical signals are weaker for the technical characteristics. Conclusion: [...]

</details>

<details>

<summary>2020-07-24 06:01:08 - Defending against Adversarial Attack towards Deep Neural Networks via Collaborative Multi-task Training</summary>

- *Derek Wang, Chaoran Li, Sheng Wen, Surya Nepal, Yang Xiang*

- `1803.05123v4` - [abs](http://arxiv.org/abs/1803.05123v4) - [pdf](http://arxiv.org/pdf/1803.05123v4)

> Deep neural networks (DNNs) are known to be vulnerable to adversarial examples which contain human-imperceptible perturbations. A series of defending methods, either proactive defence or reactive defence, have been proposed in the recent years. However, most of the methods can only handle specific attacks. For example, proactive defending methods are invalid against grey-box or white-box attacks, while reactive defending methods are challenged by low-distortion adversarial examples or transferring adversarial examples. This becomes a critical problem since a defender usually does not have the type of the attack as a priori knowledge. Moreover, existing two-pronged defences (e.g., MagNet), which take advantages of both proactive and reactive methods, have been reported as broken under transferring attacks. To address this problem, this paper proposed a novel defensive framework based on collaborative multi-task training, aiming at providing defence for different types of attacks. The proposed defence first encodes training labels into label pairs and counters black-box attacks leveraging adversarial training supervised by the encoded label pairs. The defence further constructs a detector to identify and reject high-confidence adversarial examples that bypass the black-box defence. In addition, the proposed collaborative architecture can prevent adversaries from finding valid adversarial examples when the defence strategy is exposed. In the experiments, we evaluated our defence against four state-of-the-art attacks on $MNIST$ and $CIFAR10$ datasets. The results showed that our defending method achieved up to $96.3\%$ classification accuracy on black-box adversarial examples, and detected up to $98.7\%$ of the high confidence adversarial examples. It only decreased the model accuracy on benign example classification by $2.1\%$ for the $CIFAR10$ dataset.

</details>

<details>

<summary>2020-07-24 14:33:24 - On Manually Reverse Engineering Communication Protocols of Linux Based IoT Systems</summary>

- *Kaizheng Liu, Ming Yang, Zhen Ling, Huaiyu Yan, Yue Zhang, Xinwen Fu, Wei Zhao*

- `2007.11981v2` - [abs](http://arxiv.org/abs/2007.11981v2) - [pdf](http://arxiv.org/pdf/2007.11981v2)

> IoT security and privacy has raised grave concerns. Efforts have been made to design tools to identify and understand vulnerabilities of IoT systems. Most of the existing protocol security analysis techniques rely on a well understanding of the underlying communication protocols. In this paper, we systematically present the first manual reverse engineering framework for discovering communication protocols of embedded Linux based IoT systems. We have successfully applied our framework to reverse engineer a number of IoT systems. As an example, we present a detailed use of the framework reverse-engineering the WeMo smart plug communication protocol by extracting the firmware from the flash, performing static and dynamic analysis of the firmware and analyzing network traffic. The discovered protocol exposes severe design flaws that allow attackers to control or deny the service of victim plugs. Our manual reverse engineering framework is generic and can be applied to both read-only and writable Embedded Linux filesystems.

</details>

<details>

<summary>2020-07-24 17:10:18 - Hide-and-Seek Privacy Challenge</summary>

- *James Jordon, Daniel Jarrett, Jinsung Yoon, Tavian Barnes, Paul Elbers, Patrick Thoral, Ari Ercole, Cheng Zhang, Danielle Belgrave, Mihaela van der Schaar*

- `2007.12087v2` - [abs](http://arxiv.org/abs/2007.12087v2) - [pdf](http://arxiv.org/pdf/2007.12087v2)

> The clinical time-series setting poses a unique combination of challenges to data modeling and sharing. Due to the high dimensionality of clinical time series, adequate de-identification to preserve privacy while retaining data utility is difficult to achieve using common de-identification techniques. An innovative approach to this problem is synthetic data generation. From a technical perspective, a good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between high-dimensional variables across time. From the privacy perspective, the model should prevent patient re-identification by limiting vulnerability to membership inference attacks. The NeurIPS 2020 Hide-and-Seek Privacy Challenge is a novel two-tracked competition to simultaneously accelerate progress in tackling both problems. In our head-to-head format, participants in the synthetic data generation track (i.e. "hiders") and the patient re-identification track (i.e. "seekers") are directly pitted against each other by way of a new, high-quality intensive care time-series dataset: the AmsterdamUMCdb dataset. Ultimately, we seek to advance generative techniques for dense and high-dimensional temporal data streams that are (1) clinically meaningful in terms of fidelity and predictivity, as well as (2) capable of minimizing membership privacy risks in terms of the concrete notion of patient re-identification.

</details>

<details>

<summary>2020-07-25 06:25:58 - Data and Model Dependencies of Membership Inference Attack</summary>

- *Shakila Mahjabin Tonni, Dinusha Vatsalan, Farhad Farokhi, Dali Kaafar, Zhigang Lu, Gioacchino Tangari*

- `2002.06856v5` - [abs](http://arxiv.org/abs/2002.06856v5) - [pdf](http://arxiv.org/pdf/2002.06856v5)

> Machine learning (ML) models have been shown to be vulnerable to Membership Inference Attacks (MIA), which infer the membership of a given data point in the target dataset by observing the prediction output of the ML model. While the key factors for the success of MIA have not yet been fully understood, existing defense mechanisms such as using L2 regularization \cite{10shokri2017membership} and dropout layers \cite{salem2018ml} take only the model's overfitting property into consideration. In this paper, we provide an empirical analysis of the impact of both the data and ML model properties on the vulnerability of ML techniques to MIA. Our results reveal the relationship between MIA accuracy and properties of the dataset and training model in use. In particular, we show that the size of shadow dataset, the class and feature balance and the entropy of the target dataset, the configurations and fairness of the training model are the most influential factors. Based on those experimental findings, we conclude that along with model overfitting, multiple properties jointly contribute to MIA success instead of any single property. Building on our experimental findings, we propose using those data and model properties as regularizers to protect ML models against MIA. Our results show that the proposed defense mechanisms can reduce the MIA accuracy by up to 25\% without sacrificing the ML model prediction utility.

</details>

<details>

<summary>2020-07-25 13:40:28 - A Review of Cybersecurity Incidents in the Water Sector</summary>

- *Amin Hassanzadeh, Amin Rasekh, Stefano Galelli, Mohsen Aghashahi, Riccardo Taormina, Avi Ostfeld, Katherine Banks*

- `2001.11144v2` - [abs](http://arxiv.org/abs/2001.11144v2) - [pdf](http://arxiv.org/pdf/2001.11144v2)

> This study presents a critical review of disclosed, documented, and malicious cybersecurity incidents in the water sector to inform safeguarding efforts against cybersecurity threats. The review is presented within a technical context of industrial control system architectures, attack-defense models, and security solutions. Fifteen incidents were selected and analyzed through a search strategy that included a variety of public information sources ranging from federal investigation reports to scientific papers. For each individual incident, the situation, response, remediation, and lessons learned were compiled and described. The findings of this review indicate an increase in the frequency, diversity, and complexity of cyberthreats to the water sector. Although the emergence of new threats, such as ransomware or cryptojacking, was found, a recurrence of similar vulnerabilities and threats, such as insider threats, was also evident, emphasizing the need for an adaptive, cooperative, and comprehensive approach to water cyberdefense.

</details>

<details>

<summary>2020-07-27 00:46:21 - A Survey of COVID-19 Contact Tracing Apps</summary>

- *Nadeem Ahmed, Regio A. Michelin, Wanli Xue, Sushmita Ruj, Robert Malaney, Salil S. Kanhere, Aruna Seneviratne, Wen Hu, Helge Janicke, Sanjay Jha*

- `2006.10306v3` - [abs](http://arxiv.org/abs/2006.10306v3) - [pdf](http://arxiv.org/pdf/2006.10306v3)

> The recent outbreak of COVID-19 has taken the world by surprise, forcing lockdowns and straining public health care systems. COVID-19 is known to be a highly infectious virus, and infected individuals do not initially exhibit symptoms, while some remain asymptomatic. Thus, a non-negligible fraction of the population can, at any given time, be a hidden source of transmissions. In response, many governments have shown great interest in smartphone contact tracing apps that help automate the difficult task of tracing all recent contacts of newly identified infected individuals. However, tracing apps have generated much discussion around their key attributes, including system architecture, data management, privacy, security, proximity estimation, and attack vulnerability. In this article, we provide the first comprehensive review of these much-discussed tracing app attributes. We also present an overview of many proposed tracing app examples, some of which have been deployed countrywide, and discuss the concerns users have reported regarding their usage. We close by outlining potential research directions for next-generation app design, which would facilitate improved tracing and security performance, as well as wide adoption by the population at large.

</details>

<details>

<summary>2020-07-27 10:15:37 - Testing And Hardening IoT Devices Against the Mirai Botnet</summary>

- *Christopher Kelly, Nikolaos Pitropakis, Sean McKeown, Costas Lambrinoudakis*

- `2007.13410v1` - [abs](http://arxiv.org/abs/2007.13410v1) - [pdf](http://arxiv.org/pdf/2007.13410v1)

> A large majority of cheap Internet of Things (IoT) devices that arrive brand new, and are configured with out-of-the-box settings, are not being properly secured by the manufactures, and are vulnerable to existing malware lurking on the Internet. Among them is the Mirai botnet which has had its source code leaked to the world, allowing any malicious actor to configure and unleash it. A combination of software assets not being utilised safely and effectively are exposing consumers to a full compromise. We configured and attacked 4 different IoT devices using the Mirai libraries. Our experiments concluded that three out of the four devices were vulnerable to the Mirai malware and became infected when deployed using their default configuration. This demonstrates that the original security configurations are not sufficient to provide acceptable levels of protection for consumers, leaving their devices exposed and vulnerable. By analysing the Mirai libraries and its attack vectors, we were able to determine appropriate device configuration countermeasures to harden the devices against this botnet, which were successfully validated through experimentation.

</details>

<details>

<summary>2020-07-27 14:00:31 - Attacking and Defending Machine Learning Applications of Public Cloud</summary>

- *Dou Goodman, Hao Xin*

- `2008.02076v1` - [abs](http://arxiv.org/abs/2008.02076v1) - [pdf](http://arxiv.org/pdf/2008.02076v1)

> Adversarial attack breaks the boundaries of traditional security defense. For adversarial attack and the characteristics of cloud services, we propose Security Development Lifecycle for Machine Learning applications, e.g., SDL for ML. The SDL for ML helps developers build more secure software by reducing the number and severity of vulnerabilities in ML-as-a-service, while reducing development cost.

</details>

<details>

<summary>2020-07-27 14:16:56 - Uncovering socioeconomic gaps in mobility reduction during the COVID-19 pandemic using location data</summary>

- *Samuel P. Fraiberger, Pablo Astudillo, Lorenzo Candeago, Alex Chunet, Nicholas K. W. Jones, Maham Faisal Khan, Bruno Lepri, Nancy Lozano Gracia, Lorenzo Lucchini, Emanuele Massaro, Aleister Montfort*

- `2006.15195v2` - [abs](http://arxiv.org/abs/2006.15195v2) - [pdf](http://arxiv.org/pdf/2006.15195v2)

> Using smartphone location data from Colombia, Mexico, and Indonesia, we investigate how non-pharmaceutical policy interventions intended to mitigate the spread of the COVID-19 pandemic impact human mobility. In all three countries, we find that following the implementation of mobility restriction measures, human movement decreased substantially. Importantly, we also uncover large and persistent differences in mobility reduction between wealth groups: on average, users in the top decile of wealth reduced their mobility up to twice as much as users in the bottom decile. For decision-makers seeking to efficiently allocate resources to response efforts, these findings highlight that smartphone location data can be leveraged to tailor policies to the needs of specific socioeconomic groups, especially the most vulnerable.

</details>

<details>

<summary>2020-07-28 01:46:05 - Soft Computing Techniques for Dependable Cyber-Physical Systems</summary>

- *Muhammad Atif, Siddique Latif, Rizwan Ahmad, Adnan Khalid Kiani, Junaid Qadir, Adeel Baig, Hisao Ishibuchi, Waseem Abbas*

- `1801.10472v2` - [abs](http://arxiv.org/abs/1801.10472v2) - [pdf](http://arxiv.org/pdf/1801.10472v2)

> Cyber-Physical Systems (CPS) allow us to manipulate objects in the physical world by providing a communication bridge between computation and actuation elements. In the current scheme of things, this sought-after control is marred by limitations inherent in the underlying communication network(s) as well as by the uncertainty found in the physical world. These limitations hamper fine-grained control of elements that may be separated by large-scale distances. In this regard, soft computing is an emerging paradigm that can help to overcome the vulnerabilities, and unreliability of CPS by using techniques including fuzzy systems, neural network, evolutionary computation, probabilistic reasoning and rough sets. In this paper, we present a comprehensive contemporary review of soft computing techniques for CPS dependability modeling, analysis, and improvement. This paper provides an overview of CPS applications, explores the foundations of dependability engineering, and highlights the potential role of soft computing techniques for CPS dependability with various case studies, while identifying common pitfalls and future directions. In addition, this paper provides a comprehensive survey on the use of various soft computing techniques for making CPS dependable.

</details>

<details>

<summary>2020-07-28 07:45:25 - Derivation of Information-Theoretically Optimal Adversarial Attacks with Applications to Robust Machine Learning</summary>

- *Jirong Yi, Raghu Mudumbai, Weiyu Xu*

- `2007.14042v1` - [abs](http://arxiv.org/abs/2007.14042v1) - [pdf](http://arxiv.org/pdf/2007.14042v1)

> We consider the theoretical problem of designing an optimal adversarial attack on a decision system that maximally degrades the achievable performance of the system as measured by the mutual information between the degraded signal and the label of interest. This problem is motivated by the existence of adversarial examples for machine learning classifiers. By adopting an information theoretic perspective, we seek to identify conditions under which adversarial vulnerability is unavoidable i.e. even optimally designed classifiers will be vulnerable to small adversarial perturbations. We present derivations of the optimal adversarial attacks for discrete and continuous signals of interest, i.e., finding the optimal perturbation distributions to minimize the mutual information between the degraded signal and a signal following a continuous or discrete distribution. In addition, we show that it is much harder to achieve adversarial attacks for minimizing mutual information when multiple redundant copies of the input signal are available. This provides additional support to the recently proposed ``feature compression" hypothesis as an explanation for the adversarial vulnerability of deep learning classifiers. We also report on results from computational experiments to illustrate our theoretical results.

</details>

<details>

<summary>2020-07-28 15:40:35 - Coding Practices and Recommendations of Spring Security for Enterprise Applications</summary>

- *Mazharul Islam, Sazzadur Rahaman, Na Meng, Behnaz Hassanshahi, Padmanabhan Krishnan, Danfeng, Yao*

- `2007.14319v1` - [abs](http://arxiv.org/abs/2007.14319v1) - [pdf](http://arxiv.org/pdf/2007.14319v1)

> Spring security is tremendously popular among practitioners for its ease of use to secure enterprise applications. In this paper, we study the application framework misconfiguration vulnerabilities in the light of Spring security, which is relatively understudied in the existing literature. Towards that goal, we identify 6 types of security anti-patterns and 4 insecure vulnerable defaults by conducting a measurement-based approach on 28 Spring applications. Our analysis shows that security risks associated with the identified security anti-patterns and insecure defaults can leave the enterprise application vulnerable to a wide range of high-risk attacks. To prevent these high-risk attacks, we also provide recommendations for practitioners. Consequently, our study has contributed one update to the official Spring security documentation while other security issues identified in this study are being considered for future major releases by Spring security community.

</details>

<details>

<summary>2020-07-28 21:36:06 - When Attackers Meet AI: Learning-empowered Attacks in Cooperative Spectrum Sensing</summary>

- *Zhengping Luo, Shangqing Zhao, Zhuo Lu, Jie Xu, Yalin E. Sagduyu*

- `1905.01430v2` - [abs](http://arxiv.org/abs/1905.01430v2) - [pdf](http://arxiv.org/pdf/1905.01430v2)

> Defense strategies have been well studied to combat Byzantine attacks that aim to disrupt cooperative spectrum sensing by sending falsified versions of spectrum sensing data to a fusion center. However, existing studies usually assume network or attackers as passive entities, e.g., assuming the prior knowledge of attacks is known or fixed. In practice, attackers can actively adopt arbitrary behaviors and avoid pre-assumed patterns or assumptions used by defense strategies. In this paper, we revisit this security vulnerability as an adversarial machine learning problem and propose a novel learning-empowered attack framework named Learning-Evaluation-Beating (LEB) to mislead the fusion center. Based on the black-box nature of the fusion center in cooperative spectrum sensing, our new perspective is to make the adversarial use of machine learning to construct a surrogate model of the fusion center's decision model. We propose a generic algorithm to create malicious sensing data using this surrogate model. Our real-world experiments show that the LEB attack is effective to beat a wide range of existing defense strategies with an up to 82% of success ratio. Given the gap between the proposed LEB attack and existing defenses, we introduce a non-invasive method named as influence-limiting defense, which can coexist with existing defenses to defend against LEB attack or other similar attacks. We show that this defense is highly effective and reduces the overall disruption ratio of LEB attack by up to 80%.

</details>

<details>

<summary>2020-07-29 04:23:34 - Rademacher Complexity for Adversarially Robust Generalization</summary>

- *Dong Yin, Kannan Ramchandran, Peter Bartlett*

- `1810.11914v4` - [abs](http://arxiv.org/abs/1810.11914v4) - [pdf](http://arxiv.org/pdf/1810.11914v4)

> Many machine learning models are vulnerable to adversarial attacks; for example, adding adversarial perturbations that are imperceptible to humans can often make machine learning models produce wrong predictions with high confidence. Moreover, although we may obtain robust models on the training dataset via adversarial training, in some problems the learned models cannot generalize well to the test data. In this paper, we focus on $\ell_\infty$ attacks, and study the adversarially robust generalization problem through the lens of Rademacher complexity. For binary linear classifiers, we prove tight bounds for the adversarial Rademacher complexity, and show that the adversarial Rademacher complexity is never smaller than its natural counterpart, and it has an unavoidable dimension dependence, unless the weight vector has bounded $\ell_1$ norm. The results also extend to multi-class linear classifiers. For (nonlinear) neural networks, we show that the dimension dependence in the adversarial Rademacher complexity also exists. We further consider a surrogate adversarial loss for one-hidden layer ReLU network and prove margin bounds for this setting. Our results indicate that having $\ell_1$ norm constraints on the weight matrices might be a potential way to improve generalization in the adversarial setting. We demonstrate experimental results that validate our theoretical findings.

</details>

<details>

<summary>2020-07-29 06:44:07 - SeMPE: Secure Multi Path Execution Architecture for Removing Conditional Branch Side Channels</summary>

- *Andrea Mondelli, Paul Gazzillo, Yan Solihin*

- `2006.16345v2` - [abs](http://arxiv.org/abs/2006.16345v2) - [pdf](http://arxiv.org/pdf/2006.16345v2)

> One of the most prevalent source of side channel vulnerabilities is the secret-dependent behavior of conditional branches (SDBCB). The state-of-the-art solution relies on Constant-Time Expressions, which require high programming effort and incur high performance overheads. In this paper, we propose SeMPE, an approach that relies on architecture support to eliminate SDBCB without requiring much programming effort while incurring low performance overheads. The key idea is that when a secret-dependent branch is encountered, the SeMPE microarchitecture fetches, executes, and commits both paths of the branch, preventing the adversary from inferring secret values from the branching behavior of the program. To enable that, SeMPE relies on an architecture that is capable of safely executing both branch paths sequentially. Through microbenchmarks and an evaluation of a real-world library, we show that SeMPE incurs near ideal execution time overheads, which is the sum of the execution time of all branch paths of secret-dependent branches. SeMPE outperforms code generated by FaCT, a constant-time expression language, by up to a factor of 18x.

</details>

<details>

<summary>2020-07-29 17:51:29 - Adversarial Robustness for Machine Learning Cyber Defenses Using Log Data</summary>

- *Kai Steverson, Jonathan Mullin, Metin Ahiskali*

- `2007.14983v1` - [abs](http://arxiv.org/abs/2007.14983v1) - [pdf](http://arxiv.org/pdf/2007.14983v1)

> There has been considerable and growing interest in applying machine learning for cyber defenses. One promising approach has been to apply natural language processing techniques to analyze logs data for suspicious behavior. A natural question arises to how robust these systems are to adversarial attacks. Defense against sophisticated attack is of particular concern for cyber defenses. In this paper, we develop a testing framework to evaluate adversarial robustness of machine learning cyber defenses, particularly those focused on log data. Our framework uses techniques from deep reinforcement learning and adversarial natural language processing. We validate our framework using a publicly available dataset and demonstrate that our adversarial attack does succeed against the target systems, revealing a potential vulnerability. We apply our framework to analyze the influence of different levels of dropout regularization and find that higher dropout levels increases robustness. Moreover 90% dropout probability exhibited the highest level of robustness by a significant margin, which suggests unusually high dropout may be necessary to properly protect against adversarial attacks.

</details>

<details>

<summary>2020-07-30 06:37:37 - Exploiting ML algorithms for Efficient Detection and Prevention of JavaScript-XSS Attacks in Android Based Hybrid Applications</summary>

- *Usama Khalid, Muhammad Abdullah, Kashif Inayat*

- `2006.07350v2` - [abs](http://arxiv.org/abs/2006.07350v2) - [pdf](http://arxiv.org/pdf/2006.07350v2)

> The development and analysis of mobile applications in term of security have become an active research area from many years as many apps are vulnerable to different attacks. Especially the concept of hybrid applications has emerged in the last three years where applications are developed in both native and web languages because the use of web languages raises certain security risks in hybrid mobile applications as it creates possible channels where malicious code can be injected inside the application. WebView is an important component in hybrid mobile applications which used to implements a sandbox mechanism to protect the local resources of smartphone devices from un-authorized access of JavaScript. However, the WebView application program interfaces (APIs) also have security issues. For example, an attacker can attack the hybrid application via JavaScript code by bypassing the sandbox security through accessing the public methods of the applications. Cross-site scripting (XSS) is one of the most popular malicious code injection technique for accessing the public methods of the application through JavaScript. This research proposes a framework for detection and prevention of XSS attacks in hybrid applications using state-of-the-art machine learning (ML) algorithms. The detection of the attacks have been perform by exploiting the registered Java object features. The dataset and the sample hybrid applications have been developed using the android studio. Then the widely used toolkit, RapidMiner, has been used for empirical analysis. The results reveal that the ensemble based Random Forest algorithm outperforms other algorithms and achieves both the accuracy and F-measures as high as of 99%.

</details>

<details>

<summary>2020-07-30 08:06:53 - A Data Augmentation-based Defense Method Against Adversarial Attacks in Neural Networks</summary>

- *Yi Zeng, Han Qiu, Gerard Memmi, Meikang Qiu*

- `2007.15290v1` - [abs](http://arxiv.org/abs/2007.15290v1) - [pdf](http://arxiv.org/pdf/2007.15290v1)

> Deep Neural Networks (DNNs) in Computer Vision (CV) are well-known to be vulnerable to Adversarial Examples (AEs), namely imperceptible perturbations added maliciously to cause wrong classification results. Such variability has been a potential risk for systems in real-life equipped DNNs as core components. Numerous efforts have been put into research on how to protect DNN models from being tackled by AEs. However, no previous work can efficiently reduce the effects caused by novel adversarial attacks and be compatible with real-life constraints at the same time. In this paper, we focus on developing a lightweight defense method that can efficiently invalidate full whitebox adversarial attacks with the compatibility of real-life constraints. From basic affine transformations, we integrate three transformations with randomized coefficients that fine-tuned respecting the amount of change to the defended sample. Comparing to 4 state-of-art defense methods published in top-tier AI conferences in the past two years, our method demonstrates outstanding robustness and efficiency. It is worth highlighting that, our model can withstand advanced adaptive attack, namely BPDA with 50 rounds, and still helps the target model maintain an accuracy around 80 %, meanwhile constraining the attack success rate to almost zero.

</details>

<details>

<summary>2020-07-30 09:08:54 - Security Analysis of EOSIO Smart Contracts</summary>

- *Ningyu He, Ruiyi Zhang, Lei Wu, Haoyu Wang, Xiapu Luo, Yao Guo, Ting Yu, Xuxian Jiang*

- `2003.06568v2` - [abs](http://arxiv.org/abs/2003.06568v2) - [pdf](http://arxiv.org/pdf/2003.06568v2)

> The EOSIO blockchain, one of the representative Delegated Proof-of-Stake (DPoS) blockchain platforms, has grown rapidly recently. Meanwhile, a number of vulnerabilities and high-profile attacks against top EOSIO DApps and their smart contracts have also been discovered and observed in the wild, resulting in serious financial damages. Most of EOSIO's smart contracts are not open-sourced and they are typically compiled to WebAssembly (Wasm) bytecode, thus making it challenging to analyze and detect the presence of possible vulnerabilities. In this paper, we propose EOSAFE, the first static analysis framework that can be used to automatically detect vulnerabilities in EOSIO smart contracts at the bytecode level. Our framework includes a practical symbolic execution engine for Wasm, a customized library emulator for EOSIO smart contracts, and four heuristics-driven detectors to identify the presence of four most popular vulnerabilities in EOSIO smart contracts. Experiment results suggest that EOSAFE achieves promising results in detecting vulnerabilities, with an F1-measure of 98%. We have applied EOSAFE to all active 53,666 smart contracts in the ecosystem (as of November 15, 2019). Our results show that over 25% of the smart contracts are vulnerable. We further analyze possible exploitation attempts against these vulnerable smart contracts and identify 48 in-the-wild attacks (25 of them have been confirmed by DApp developers), resulting in financial loss of at least 1.7 million USD.

</details>

<details>

<summary>2020-07-31 10:27:20 - MUZZ: Thread-aware Grey-box Fuzzing for Effective Bug Hunting in Multithreaded Programs</summary>

- *Hongxu Chen, Shengjian Guo, Yinxing Xue, Yulei Sui, Cen Zhang, Yuekang Li, Haijun Wang, Yang Liu*

- `2007.15943v1` - [abs](http://arxiv.org/abs/2007.15943v1) - [pdf](http://arxiv.org/pdf/2007.15943v1)

> Grey-box fuzz testing has revealed thousands of vulnerabilities in real-world software owing to its lightweight instrumentation, fast coverage feedback, and dynamic adjusting strategies. However, directly applying grey-box fuzzing to input-dependent multithreaded programs can be extremely inefficient. In practice, multithreading-relevant bugs are usually buried in sophisticated program flows. Meanwhile, the existing grey-box fuzzing techniques do not stress thread-interleavings which affect execution states in multithreaded programs. Therefore, mainstream grey-box fuzzers cannot effectively test problematic segments in multithreaded programs despite they might obtain high code coverage statistics.   To this end, we propose MUZZ, a new grey-box fuzzing technique that hunts for bugs in multithreaded programs. MUZZ owns three novel thread-aware instrumentations, namely coverage-oriented instrumentation, thread-context instrumentation, and schedule-intervention instrumentation. During fuzzing, these instrumentations engender runtime feedback to stress execution states caused by thread interleavings. By leveraging the feedback in the dynamic seed selection and execution strategies, MUZZ preserves more valuable seeds that expose bugs in a multithreading context.   We evaluate MUZZ on 12 real-world software programs. Experiments show that MUZZ outperforms AFL in both multithreading-relevant seed generation and concurrency-vulnerability detection. Further, by replaying the target programs against the generated seeds, MUZZ also reveals more concurrency-bugs (e.g., data-races, thread-leaks) than AFL. In total, MUZZ detected 8 new concurrency-vulnerabilities and 19 new concurrency-bugs. At the time of writing, 4 CVE IDs have been assigned to the reported issues.

</details>

<details>

<summary>2020-07-31 17:00:50 - Hardware/Software Obfuscation against Timing Side-channel Attack on a GPU</summary>

- *Elmira Karimi, Yunsi Fei, David Kaeli*

- `2007.16175v1` - [abs](http://arxiv.org/abs/2007.16175v1) - [pdf](http://arxiv.org/pdf/2007.16175v1)

> GPUs are increasingly being used in security applications, especially for accelerating encryption/decryption. While GPUs are an attractive platform in terms of performance, the security of these devices raises a number of concerns. One vulnerability is the data-dependent timing information, which can be exploited by adversary to recover the encryption key. Memory system features are frequently exploited since they create detectable timing variations. In this paper, our attack model is a coalescing attack, which leverages a critical GPU microarchitectural feature -- the coalescing unit. As multiple concurrent GPU memory requests can refer to the same cache block, the coalescing unit collapses them into a single memory transaction. The access time of an encryption kernel is dependent on the number of transactions. Correlation between a guessed key value and the associated timing samples can be exploited to recover the secret key. In this paper, a series of hardware/software countermeasures are proposed to obfuscate the memory timing side channel, making the GPU more resilient without impacting performance. Our hardware-based approach attempts to randomize the width of the coalescing unit to lower the signal-to-noise ratio. We present a hierarchical Miss Status Holding Register (MSHR) design that can merge transactions across different warps. This feature boosts performance, while, at the same time, secures the execution. We also present a software-based approach to permute the organization of critical data structures, significantly changing the coalescing behavior and introducing a high degree of randomness. Equipped with our new protections, the effort to launch a successful attack is increased up to 1433X . 178X, while also improving encryption/decryption performance up to 7%.

</details>


## 2020-08

<details>

<summary>2020-08-01 00:58:54 - Vulnerability Under Adversarial Machine Learning: Bias or Variance?</summary>

- *Hossein Aboutalebi, Mohammad Javad Shafiee, Michelle Karg, Christian Scharfenberger, Alexander Wong*

- `2008.00138v1` - [abs](http://arxiv.org/abs/2008.00138v1) - [pdf](http://arxiv.org/pdf/2008.00138v1)

> Prior studies have unveiled the vulnerability of the deep neural networks in the context of adversarial machine learning, leading to great recent attention into this area. One interesting question that has yet to be fully explored is the bias-variance relationship of adversarial machine learning, which can potentially provide deeper insights into this behaviour. The notion of bias and variance is one of the main approaches to analyze and evaluate the generalization and reliability of a machine learning model. Although it has been extensively used in other machine learning models, it is not well explored in the field of deep learning and it is even less explored in the area of adversarial machine learning.   In this study, we investigate the effect of adversarial machine learning on the bias and variance of a trained deep neural network and analyze how adversarial perturbations can affect the generalization of a network. We derive the bias-variance trade-off for both classification and regression applications based on two main loss functions: (i) mean squared error (MSE), and (ii) cross-entropy. Furthermore, we perform quantitative analysis with both simulated and real data to empirically evaluate consistency with the derived bias-variance tradeoffs. Our analysis sheds light on why the deep neural networks have poor performance under adversarial perturbation from a bias-variance point of view and how this type of perturbation would change the performance of a network. Moreover, given these new theoretical findings, we introduce a new adversarial machine learning algorithm with lower computational complexity than well-known adversarial machine learning strategies (e.g., PGD) while providing a high success rate in fooling deep neural networks in lower perturbation magnitudes.

</details>

<details>

<summary>2020-08-01 09:58:22 - Standardized Green View Index and Quantification of Different Metrics of Urban Green Vegetation</summary>

- *Yusuke Kumakoshi, Sau Yee Chan, Hideki Koizumi, Xiaojiang Li, Yuji Yoshimura*

- `2008.00229v1` - [abs](http://arxiv.org/abs/2008.00229v1) - [pdf](http://arxiv.org/pdf/2008.00229v1)

> Urban greenery is considered an important factor in relation to sustainable development and people's quality of life in the city. Although ways to measure urban greenery have been proposed, the characteristics of each metric have not been fully established, rendering previous researches vulnerable to changes in greenery metrics. To make estimation more robust, this study aims to (1) propose an improved indicator of greenery visibility for analytical use (standardized GVI; sGVI), and (2) quantify the relation between sGVI and other greenery metrics. Analyzing a data set for Yokohama city, Japan, it is shown that the sGVI, a weighted form of GVI aggregated to an area, mitigates the bias of densely located measurement sites. Also, by comparing sGVI and NDVI at city block level, we found that sGVI captures the presence of vegetation better in the city center, whereas NDVI is better in capturing vegetation in parks and forests. These tools provide a foundation for accessing the effect of vegetation in urban landscapes in a more robust matter, enabling comparison on any arbitrary geographical scale.

</details>

<details>

<summary>2020-08-02 06:33:47 - Blackbox Trojanising of Deep Learning Models : Using non-intrusive network structure and binary alterations</summary>

- *Jonathan Pan*

- `2008.00408v1` - [abs](http://arxiv.org/abs/2008.00408v1) - [pdf](http://arxiv.org/pdf/2008.00408v1)

> Recent advancements in Artificial Intelligence namely in Deep Learning has heightened its adoption in many applications. Some are playing important roles to the extent that we are heavily dependent on them for our livelihood. However, as with all technologies, there are vulnerabilities that malicious actors could exploit. A form of exploitation is to turn these technologies, intended for good, to become dual-purposed instruments to support deviant acts like malicious software trojans. As part of proactive defense, researchers are proactively identifying such vulnerabilities so that protective measures could be developed subsequently. This research explores a novel blackbox trojanising approach using a simple network structure modification to any deep learning image classification model that would transform a benign model into a deviant one with a simple manipulation of the weights to induce specific types of errors. Propositions to protect the occurrence of such simple exploits are discussed in this research. This research highlights the importance of providing sufficient safeguards to these models so that the intended good of AI innovation and adoption may be protected.

</details>

<details>

<summary>2020-08-02 10:15:50 - Detecting malicious PDF using CNN</summary>

- *Raphael Fettaya, Yishay Mansour*

- `2007.12729v2` - [abs](http://arxiv.org/abs/2007.12729v2) - [pdf](http://arxiv.org/pdf/2007.12729v2)

> Malicious PDF files represent one of the biggest threats to computer security. To detect them, significant research has been done using handwritten signatures or machine learning based on manual feature extraction. Those approaches are both time-consuming, require significant prior knowledge and the list of features has to be updated with each newly discovered vulnerability. In this work, we propose a novel algorithm that uses an ensemble of Convolutional Neural Network (CNN) on the byte level of the file, without any handcrafted features. We show, using a data set of 90000 files downloadable online, that our approach maintains a high detection rate (94%) of PDF malware and even detects new malicious files, still undetected by most antiviruses. Using automatically generated features from our CNN network, and applying a clustering algorithm, we also obtain high similarity between the antiviruses' labels and the resulting clusters.

</details>

<details>

<summary>2020-08-02 16:37:01 - Detecting Adversarial Examples for Speech Recognition via Uncertainty Quantification</summary>

- *Sina Däubener, Lea Schönherr, Asja Fischer, Dorothea Kolossa*

- `2005.14611v2` - [abs](http://arxiv.org/abs/2005.14611v2) - [pdf](http://arxiv.org/pdf/2005.14611v2)

> Machine learning systems and also, specifically, automatic speech recognition (ASR) systems are vulnerable against adversarial attacks, where an attacker maliciously changes the input. In the case of ASR systems, the most interesting cases are targeted attacks, in which an attacker aims to force the system into recognizing given target transcriptions in an arbitrary audio sample. The increasing number of sophisticated, quasi imperceptible attacks raises the question of countermeasures. In this paper, we focus on hybrid ASR systems and compare four acoustic models regarding their ability to indicate uncertainty under attack: a feed-forward neural network and three neural networks specifically designed for uncertainty quantification, namely a Bayesian neural network, Monte Carlo dropout, and a deep ensemble. We employ uncertainty measures of the acoustic model to construct a simple one-class classification model for assessing whether inputs are benign or adversarial. Based on this approach, we are able to detect adversarial examples with an area under the receiving operator curve score of more than 0.99. The neural networks for uncertainty quantification simultaneously diminish the vulnerability to the attack, which is reflected in a lower recognition accuracy of the malicious target text in comparison to a standard hybrid ASR system.

</details>

<details>

<summary>2020-08-03 20:19:06 - Bankrupt Covert Channel: Turning Network Predictability into Vulnerability</summary>

- *Dmitrii Ustiugov, Plamen Petrov, M. R. Siavash Katebzadeh, Boris Grot*

- `2006.03854v2` - [abs](http://arxiv.org/abs/2006.03854v2) - [pdf](http://arxiv.org/pdf/2006.03854v2)

> Recent years have seen a surge in the number of data leaks despite aggressive information-containment measures deployed by cloud providers. When attackers acquire sensitive data in a secure cloud environment, covert communication channels are a key tool to exfiltrate the data to the outside world. While the bulk of prior work focused on covert channels within a single CPU, they require the spy (transmitter) and the receiver to share the CPU, which might be difficult to achieve in a cloud environment with hundreds or thousands of machines.   This work presents Bankrupt, a high-rate highly clandestine channel that enables covert communication between the spy and the receiver running on different nodes in an RDMA network. In Bankrupt, the spy communicates with the receiver by issuing RDMA network packets to a private memory region allocated to it on a different machine (an intermediary). The receiver similarly allocates a separate memory region on the same intermediary, also accessed via RDMA. By steering RDMA packets to a specific set of remote memory addresses, the spy causes deep queuing at one memory bank, which is the finest addressable internal unit of main memory. This exposes a timing channel that the receiver can listen on by issuing probe packets to addresses mapped to the same bank but in its own private memory region. Bankrupt channel delivers 74Kb/s throughput in CloudLab's public cloud while remaining undetectable to the existing monitoring capabilities, such as CPU and NIC performance counters.

</details>

<details>

<summary>2020-08-03 21:55:41 - Hardware Accelerator for Adversarial Attacks on Deep Learning Neural Networks</summary>

- *Haoqiang Guo, Lu Peng, Jian Zhang, Fang Qi, Lide Duan*

- `2008.01219v1` - [abs](http://arxiv.org/abs/2008.01219v1) - [pdf](http://arxiv.org/pdf/2008.01219v1)

> Recent studies identify that Deep learning Neural Networks (DNNs) are vulnerable to subtle perturbations, which are not perceptible to human visual system but can fool the DNN models and lead to wrong outputs. A class of adversarial attack network algorithms has been proposed to generate robust physical perturbations under different circumstances. These algorithms are the first efforts to move forward secure deep learning by providing an avenue to train future defense networks, however, the intrinsic complexity of them prevents their broader usage.   In this paper, we propose the first hardware accelerator for adversarial attacks based on memristor crossbar arrays. Our design significantly improves the throughput of a visual adversarial perturbation system, which can further improve the robustness and security of future deep learning systems. Based on the algorithm uniqueness, we propose four implementations for the adversarial attack accelerator ($A^3$) to improve the throughput, energy efficiency, and computational efficiency.

</details>

<details>

<summary>2020-08-04 07:41:15 - Learning Invariant Feature Representation to Improve Generalization across Chest X-ray Datasets</summary>

- *Sandesh Ghimire, Satyananda Kashyap, Joy T. Wu, Alexandros Karargyris, Mehdi Moradi*

- `2008.04152v1` - [abs](http://arxiv.org/abs/2008.04152v1) - [pdf](http://arxiv.org/pdf/2008.04152v1)

> Chest radiography is the most common medical image examination for screening and diagnosis in hospitals. Automatic interpretation of chest X-rays at the level of an entry-level radiologist can greatly benefit work prioritization and assist in analyzing a larger population. Subsequently, several datasets and deep learning-based solutions have been proposed to identify diseases based on chest X-ray images. However, these methods are shown to be vulnerable to shift in the source of data: a deep learning model performing well when tested on the same dataset as training data, starts to perform poorly when it is tested on a dataset from a different source. In this work, we address this challenge of generalization to a new source by forcing the network to learn a source-invariant representation. By employing an adversarial training strategy, we show that a network can be forced to learn a source-invariant representation. Through pneumonia-classification experiments on multi-source chest X-ray datasets, we show that this algorithm helps in improving classification accuracy on a new source of X-ray dataset.

</details>

<details>

<summary>2020-08-04 13:45:05 - Who Is Charging My Phone? Identifying Wireless Chargers via Fingerprinting</summary>

- *Zhiyun Wang, Jiayu Zhang, Xiaoyu Ji, Wenyuan Xu, Gang Qu, Minjian Zhao*

- `2007.15348v2` - [abs](http://arxiv.org/abs/2007.15348v2) - [pdf](http://arxiv.org/pdf/2007.15348v2)

> With the increasing popularity of the Internet of Things(IoT) devices, the demand for fast and convenient battery charging services grows rapidly. Wireless charging is a promising technology for such a purpose and its usage has become ubiquitous. However, the close distance between the charger and the device being charged not only makes proximity-based and near field communication attacks possible, but also introduces a new type of vulnerabilities. In this paper, we propose to create fingerprints for wireless chargers based on the intrinsic non-linear distortion effects of the underlying charging circuit. Using such fingerprints, we design the WirelessID system to detect potential short-range malicious wireless charging attacks. WirelessID collects signals in the standby state of the charging process and sends them to a trusted server, which can extract the fingerprint and then identify the charger.

</details>

<details>

<summary>2020-08-04 15:41:24 - Exposure Density and Neighborhood Disparities in COVID-19 Infection Risk: Using Large-scale Geolocation Data to Understand Burdens on Vulnerable Communities</summary>

- *Boyeong Hong, Bartosz Bonczak, Arpit Gupta, Lorna Thorpe, Constantine E. Kontokosta*

- `2008.01650v1` - [abs](http://arxiv.org/abs/2008.01650v1) - [pdf](http://arxiv.org/pdf/2008.01650v1)

> This study develops a new method to quantify neighborhood activity levels at high spatial and temporal resolutions and test whether, and to what extent, behavioral responses to social distancing policies vary with socioeconomic and demographic characteristics. We define exposure density as a measure of both the localized volume of activity in a defined area and the proportion of activity occurring in non-residential and outdoor land uses. We utilize this approach to capture inflows/outflows of people as a result of the pandemic and changes in mobility behavior for those that remain. First, we develop a generalizable method for assessing neighborhood activity levels by land use type using smartphone geolocation data over a three-month period covering more than 12 million unique users within the Greater New York area. Second, we measure and analyze disparities in community social distancing by identifying patterns in neighborhood activity levels and characteristics before and after the stay-at-home order. Finally, we evaluate the effect of social distancing in neighborhoods on COVID-19 infection rates and outcomes associated with localized demographic, socioeconomic, and infrastructure characteristics in order to identify disparities in health outcomes related to exposure risk. Our findings provide insight into the timely evaluation of the effectiveness of social distancing for individual neighborhoods and support a more equitable allocation of resources to support vulnerable and at-risk communities. Our findings demonstrate distinct patterns of activity pre- and post-COVID across neighborhoods. The variation in exposure density has a direct and measurable impact on the risk of infection.

</details>

<details>

<summary>2020-08-04 18:04:03 - Statistical Model Checking for Hyperproperties</summary>

- *Yu Wang, Siddhartha Nalluri, Borzoo Bonakdarpour, Miroslav Pajic*

- `1902.04111v5` - [abs](http://arxiv.org/abs/1902.04111v5) - [pdf](http://arxiv.org/pdf/1902.04111v5)

> Hyperproperties have shown to be a powerful tool for expressing and reasoning about information-flow security policies. In this paper, we investigate the problem of statistical model checking (SMC) for hyperproperties. Unlike exhaustive model checking, SMC works based on drawing samples from the system at hand and evaluate the specification with statistical confidence. The main benefit of applying SMC over exhaustive techniques is its efficiency and scalability. To reason about probabilistic hyperproperties, we first propose the temporal logic HyperPCLT* that extends PCTL* and HyperPCTL. We show that HyperPCLT* can express important probabilistic information-flow security policies that cannot be expressed with HyperPCTL. Then, we introduce SMC algorithms for verifying HyperPCLT* formulas on discrete-time Markov chains, based on sequential probability ratio tests (SPRT) with a new notion of multi-dimensional indifference region. Our SMC algorithms can handle both non-nested and nested probability operators for any desired significance level. To show the effectiveness of our technique, we evaluate our SMC algorithms on four case studies focused on information security: timing side-channel vulnerability in encryption, probabilistic anonymity in dining cryptographers, probabilistic noninterference of parallel programs, and the performance of a randomized cache replacement policy that acts as a countermeasure against cache flush attacks.

</details>

<details>

<summary>2020-08-05 02:34:44 - A Large Scale Analysis of Android-Web Hybridization</summary>

- *Abhishek Tiwari, Jyoti Prakash, Sascha Gross, Christian Hammer*

- `2008.01725v2` - [abs](http://arxiv.org/abs/2008.01725v2) - [pdf](http://arxiv.org/pdf/2008.01725v2)

> Many Android applications embed webpages via WebView components and execute JavaScript code within Android. Hybrid applications leverage dedicated APIs to load a resource and render it in a WebView. Furthermore, Android objects can be shared with the JavaScript world. However, bridging the interfaces of the Android and JavaScript world might also incur severe security threats: Potentially untrusted webpages and their JavaScript might interfere with the Android environment and its access to native features. No general analysis is currently available to assess the implications of such hybrid apps bridging the two worlds. To understand the semantics and effects of hybrid apps, we perform a large-scale study on the usage of the hybridization APIs in the wild. We analyze and categorize the parameters to hybridization APIs for 7,500 randomly selected and the 196 most popular applications from the Google Playstore as well as 1000 malware samples. Our results advance the general understanding of hybrid applications, as well as implications for potential program analyses, and the current security situation: We discovered thousands of flows of sensitive data from Android to JavaScript, the vast majority of which could flow to potentially untrustworthy code. Our analysis identified numerous web pages embedding vulnerabilities, which we exemplarily exploited. Additionally, we discovered a multitude of applications in which potentially untrusted JavaScript code may interfere with (trusted) Android objects, both in benign and malign applications.

</details>

<details>

<summary>2020-08-05 06:47:52 - Randomized Last-Level Caches Are Still Vulnerable to Cache Side-Channel Attacks! But We Can Fix It</summary>

- *Wei Song, Boya Li, Zihan Xue, Zhenzhen Li, Wenhao Wang, Peng Liu*

- `2008.01957v1` - [abs](http://arxiv.org/abs/2008.01957v1) - [pdf](http://arxiv.org/pdf/2008.01957v1)

> Cache randomization has recently been revived as a promising defense against conflict-based cache side-channel attacks. As two of the latest implementations, CEASER-S and ScatterCache both claim to thwart conflict-based cache side-channel attacks using randomized skewed caches. Unfortunately, our experiments show that an attacker can easily find a usable eviction set within the chosen remap period of CEASER-S and increasing the number of partitions without dynamic remapping, such as ScatterCache, cannot eliminate the threat. By quantitatively analyzing the access patterns left by various attacks in the LLC, we have newly discovered several problems with the hypotheses and implementations of randomized caches, which are also overlooked by the research on conflict-based cache side-channel attack.   However, cache randomization is not a false hope and it is an effective defense that should be widely adopted in future processors. The newly discovered problems are corresponding to flaws associated with the existing implementation of cache randomization and are fixable. Several new defense techniques are proposed in this paper. our experiments show that all the newly discovered vulnerabilities of existing randomized caches are fixed within the current performance budget. We also argue that randomized set-associative caches can be sufficiently strengthened and possess a better chance to be actually adopted in commercial processors than their skewed counterparts as they introduce less overhaul to the existing cache structure.

</details>

<details>

<summary>2020-08-05 15:08:17 - WANA: Symbolic Execution of Wasm Bytecode for Cross-Platform Smart Contract Vulnerability Detection</summary>

- *Dong Wang, Bo Jiang, W. K. Chan*

- `2007.15510v2` - [abs](http://arxiv.org/abs/2007.15510v2) - [pdf](http://arxiv.org/pdf/2007.15510v2)

> Many popular blockchain platforms are supporting smart contracts for building decentralized applications. However, the vulnerabilities within smart contracts have led to serious financial loss to their end users. For the EOSIO blockchain platform, effective vulnerability detectors are still limited. Furthermore, existing vulnerability detection tools can only support one blockchain platform. In this work, we present WANA, a cross-platform smart contract vulnerability detection tool based on the symbolic execution of WebAssembly bytecode. Furthermore, WANA proposes a set of test oracles to detect the vulnerabilities in EOSIO and Ethereum smart contracts based on WebAssembly bytecode analysis. Our experimental analysis shows that WANA can effectively detect vulnerabilities in both EOSIO and Ethereum smart contracts with high efficiency.

</details>

<details>

<summary>2020-08-06 00:43:53 - EOSFuzzer: Fuzzing EOSIO Smart Contracts for Vulnerability Detection</summary>

- *Yuhe Huang, Bo Jiang, W. K. Chan*

- `2007.14903v3` - [abs](http://arxiv.org/abs/2007.14903v3) - [pdf](http://arxiv.org/pdf/2007.14903v3)

> EOSIO is one typical public blockchain platform. It is scalable in terms of transaction speeds and has a growing ecosystem supporting smart contracts and decentralized applications. However, the vulnerabilities within the EOSIO smart contracts have led to serious attacks, which caused serious financial loss to its end users. In this work, we systematically analyzed three typical EOSIO smart contract vulnerabilities and their related attacks. Then we presented EOSFuzzer, a general black-box fuzzing framework to detect vulnerabilities within EOSIO smart contracts. In particular, EOSFuzzer proposed effective attacking scenarios and test oracles for EOSIO smart contract fuzzing. Our fuzzing experiment on 3963 EOSIO smart contracts shows that EOSFuzzer is both effective and efficient to detect EOSIO smart contract vulnerabilities with high accuracy.

</details>

<details>

<summary>2020-08-06 03:53:52 - Security and Privacy in IoT Using Machine Learning and Blockchain: Threats & Countermeasures</summary>

- *Nazar Waheed, Xiangjian He, Muhammad Ikram, Muhammad Usman, Saad Sajid Hashmi, Muhammad Usman*

- `2002.03488v4` - [abs](http://arxiv.org/abs/2002.03488v4) - [pdf](http://arxiv.org/pdf/2002.03488v4)

> Security and privacy of the users have become significant concerns due to the involvement of the Internet of things (IoT) devices in numerous applications. Cyber threats are growing at an explosive pace making the existing security and privacy measures inadequate. Hence, everyone on the Internet is a product for hackers. Consequently, Machine Learning (ML) algorithms are used to produce accurate outputs from large complex databases, where the generated outputs can be used to predict and detect vulnerabilities in IoT-based systems. Furthermore, Blockchain (BC) techniques are becoming popular in modern IoT applications to solve security and privacy issues. Several studies have been conducted on either ML algorithms or BC techniques. However, these studies target either security or privacy issues using ML algorithms or BC techniques, thus posing a need for a combined survey on efforts made in recent years addressing both security and privacy issues using ML algorithms and BC techniques. In this paper, we provide a summary of research efforts made in the past few years, starting from 2008 to 2019, addressing security and privacy issues using ML algorithms and BCtechniques in the IoT domain. First, we discuss and categorize various security and privacy threats reported in the past twelve years in the IoT domain. Then, we classify the literature on security and privacy efforts based on ML algorithms and BC techniques in the IoT domain. Finally, we identify and illuminate several challenges and future research directions in using ML algorithms and BC techniques to address security and privacy issues in the IoT domain.

</details>

<details>

<summary>2020-08-06 04:53:33 - Predicting Missing Information of Key Aspects in Vulnerability Reports</summary>

- *Hao Guo, Zhenchang Xing, Xiaohong Li*

- `2008.02456v1` - [abs](http://arxiv.org/abs/2008.02456v1) - [pdf](http://arxiv.org/pdf/2008.02456v1)

> Software vulnerabilities have been continually disclosed and documented. An important practice in documenting vulnerabilities is to describe the key vulnerability aspects, such as vulnerability type, root cause, affected product, impact, attacker type and attack vector, for the effective search and management of fast-growing vulnerabilities. We investigate 120,103 vulnerability reports in the Common Vulnerabilities and Exposures (CVE) over the past 20 years. We find that 56%, 85%, 38% and 28% of CVEs miss vulnerability type, root causes, attack vector and attacker type respectively. To help to complete the missing information of these vulnerability aspects, we propose a neural-network based approach for predicting the missing information of a key aspect of a vulnerability based on the known aspects of the vulnerability. We explore the design space of the neural network models and empirically identify the most effective model design. Using a large-scale vulnerability datas\-et from CVE, we show that we can effectively train a neural-network based classifier with less than 20% of historical CVEs. Our model achieves the prediction accuracy 94%, 79%, 89%and 70% for vulnerability type, root cause, attacker type and attack vector, respectively. Our ablation study reveals the prominent correlations among vulnerability aspects and further confirms the practicality of our approach.

</details>

<details>

<summary>2020-08-06 10:51:56 - An Intelligent Non-Invasive Real Time Human Activity Recognition System for Next-Generation Healthcare</summary>

- *William Taylor, Syed Aziz Shah, Kia Dashtipour, Adnan Zahid, Qammer H. Abbasi, Muhammad Ali Imran*

- `2008.02567v1` - [abs](http://arxiv.org/abs/2008.02567v1) - [pdf](http://arxiv.org/pdf/2008.02567v1)

> Human motion detection is getting considerable attention in the field of Artificial Intelligence (AI) driven healthcare systems. Human motion can be used to provide remote healthcare solutions for vulnerable people by identifying particular movements such as falls, gait and breathing disorders. This can allow people to live more independent lifestyles and still have the safety of being monitored if more direct care is needed. At present wearable devices can provide real time monitoring by deploying equipment on a person's body. However, putting devices on a person's body all the time make it uncomfortable and the elderly tends to forget it to wear as well in addition to the insecurity of being tracked all the time. This paper demonstrates how human motions can be detected in quasi-real-time scenario using a non-invasive method. Patterns in the wireless signals presents particular human body motions as each movement induces a unique change in the wireless medium. These changes can be used to identify particular body motions. This work produces a dataset that contains patterns of radio wave signals obtained using software defined radios (SDRs) to establish if a subject is standing up or sitting down as a test case. The dataset was used to create a machine learning model, which was used in a developed application to provide a quasi-real-time classification of standing or sitting state. The machine learning model was able to achieve 96.70 % accuracy using the Random Forest algorithm using 10 fold cross validation. A benchmark dataset of wearable devices was compared to the proposed dataset and results showed the proposed dataset to have similar accuracy of nearly 90 %. The machine learning models developed in this paper are tested for two activities but the developed system is designed and applicable for detecting and differentiating x number of activities.

</details>

<details>

<summary>2020-08-06 18:12:36 - Implementing the Exponential Mechanism with Base-2 Differential Privacy</summary>

- *Christina Ilvento*

- `1912.04222v3` - [abs](http://arxiv.org/abs/1912.04222v3) - [pdf](http://arxiv.org/pdf/1912.04222v3)

> Despite excellent theoretical support, Differential Privacy (DP) can still be a challenge to implement in practice. In part, this challenge is due to the concerns associated with translating arbitrary- or infinite-precision theoretical mechanisms to the reality of floating point or fixed-precision. Beginning with the troubling result of Mironov demonstrating the security issues of using floating point for implementing the Laplace mechanism, there have been many reasonable questions raised concerning the vulnerabilities of real-world implementations of DP.   In this work, we examine the practicalities of implementing the exponential mechanism of McSherry and Talwar. We demonstrate that naive or malicious implementations can result in catastrophic privacy failures. To address these problems, we show that the mechanism can be implemented exactly for a rich set of values of the privacy parameter $\varepsilon$ and utility functions with limited practical overhead in running time and minimal code complexity.   How do we achieve this result? We employ a simple trick of switching from base $e$ to base $2$, allowing us to perform precise base $2$ arithmetic. A short, precise expression is always available for $\varepsilon$, and the only approximation error we incur is the conversion of the base-2 privacy parameter back to base $e$ for reporting purposes. The core base $2$ arithmetic of the mechanism can be simply and efficiently implemented using open-source high precision arithmetic libraries. Furthermore, the exact nature of the implementation lends itself to simple monitoring of correctness and proofs of privacy.

</details>

<details>

<summary>2020-08-06 21:36:12 - Stronger and Faster Wasserstein Adversarial Attacks</summary>

- *Kaiwen Wu, Allen Houze Wang, Yaoliang Yu*

- `2008.02883v1` - [abs](http://arxiv.org/abs/2008.02883v1) - [pdf](http://arxiv.org/pdf/2008.02883v1)

> Deep models, while being extremely flexible and accurate, are surprisingly vulnerable to "small, imperceptible" perturbations known as adversarial attacks. While the majority of existing attacks focus on measuring perturbations under the $\ell_p$ metric, Wasserstein distance, which takes geometry in pixel space into account, has long been known to be a suitable metric for measuring image quality and has recently risen as a compelling alternative to the $\ell_p$ metric in adversarial attacks. However, constructing an effective attack under the Wasserstein metric is computationally much more challenging and calls for better optimization algorithms. We address this gap in two ways: (a) we develop an exact yet efficient projection operator to enable a stronger projected gradient attack; (b) we show that the Frank-Wolfe method equipped with a suitable linear minimization oracle works extremely fast under Wasserstein constraints. Our algorithms not only converge faster but also generate much stronger attacks. For instance, we decrease the accuracy of a residual network on CIFAR-10 to $3.4\%$ within a Wasserstein perturbation ball of radius $0.005$, in contrast to $65.6\%$ using the previous Wasserstein attack based on an \emph{approximate} projection operator. Furthermore, employing our stronger attacks in adversarial training significantly improves the robustness of adversarially trained models.

</details>

<details>

<summary>2020-08-07 08:49:17 - Key Generation for Internet of Things: A Contemporary Survey</summary>

- *Weitao Xu, Junqing Zhang, Shunqi Huang, Chengwen Luo, Wei Li*

- `2007.15956v2` - [abs](http://arxiv.org/abs/2007.15956v2) - [pdf](http://arxiv.org/pdf/2007.15956v2)

> Key generation is a promising technique to bootstrap secure communications for the Internet of Things (IoT) devices that have no prior knowledge between each other. In the past few years, a variety of key generation protocols and systems have been proposed. In this survey, we review and categorise recent key generation systems based on a novel taxonomy. Then, we provide both quantitative and qualitative comparisons of existing approaches. We also discuss the security vulnerabilities of key generation schemes and possible countermeasures. Finally, we discuss the current challenges and point out several potential research directions.

</details>

<details>

<summary>2020-08-07 09:04:51 - When Deep Learning Meets Smart Contracts</summary>

- *Zhipeng Gao*

- `2008.04093v1` - [abs](http://arxiv.org/abs/2008.04093v1) - [pdf](http://arxiv.org/pdf/2008.04093v1)

> Ethereum has become a widely used platform to enable secure, Blockchain-based financial and business transactions. However, many identified bugs and vulnerabilities in smart contracts have led to serious financial losses, which raises serious concerns about smart contract security. Thus, there is a significant need to better maintain smart contract code and ensure its high reliability. In this research: (1) Firstly, we propose an automated deep learning based approach to learn structural code embeddings of smart contracts in Solidity, which is useful for clone detection, bug detection and contract validation on smart contracts. We apply our approach to more than 22K solidity contracts collected from the Ethereum blockchain, results show that the clone ratio of solidity code is at around 90%, much higher than traditional software. We collect a list of 52 known buggy smart contracts belonging to 10 kinds of common vulnerabilities as our bug database. Our approach can identify more than 1000 clone related bugs based on our bug databases efficiently and accurately. (2) Secondly, according to developers' feedback, we have implemented the approach in a web-based tool, named SmartEmbed, to facilitate Solidity developers for using our approach. Our tool can assist Solidity developers to efficiently identify repetitive smart contracts in the existing Ethereum blockchain, as well as checking their contract against a known set of bugs, which can help to improve the users' confidence in the reliability of the contract. We optimize the implementations of SmartEmbed which is sufficient in supporting developers in real-time for practical uses. The Ethereum ecosystem as well as the individual Solidity developer can both benefit from our research.

</details>

<details>

<summary>2020-08-07 14:55:05 - Database Traffic Interception for Graybox Detection of Stored and Context-Sensitive XSS</summary>

- *Antonín Steinhauser, Petr Tůma*

- `2005.03322v2` - [abs](http://arxiv.org/abs/2005.03322v2) - [pdf](http://arxiv.org/pdf/2005.03322v2)

> XSS is a security vulnerability that permits injecting malicious code into the client side of a web application. In the simplest situations, XSS vulnerabilities arise when a web application includes the user input in the web output without due sanitization. Such simple XSS vulnerabilities can be detected fairly reliably with blackbox scanners, which inject malicious payload into sensitive parts of HTTP requests and look for the reflected values in the web output.   Contemporary blackbox scanners are not effective against stored XSS vulnerabilities, where the malicious payload in an HTTP response originates from the database storage of the web application, rather than from the associated HTTP request. Similarly, many blackbox scanners do not systematically handle context-sensitive XSS vulnerabilities, where the user input is included in the web output after a transformation that prevents the scanner from recognizing the original value, but does not sanitize the value sufficiently. Among the combination of two basic data sources (stored vs reflected) and two basic vulnerability patterns (context sensitive vs not so), only one is therefore tested systematically by state-of-the-art blackbox scanners.   Our work focuses on systematic coverage of the three remaining combinations. We present a graybox mechanism that extends a general purpose database to cooperate with our XSS scanner, reporting and injecting the test inputs at the boundary between the database and the web application. Furthermore, we design a mechanism for identifying the injected inputs in the web output even after encoding by the web application, and check whether the encoding sanitizes the injected inputs correctly in the respective browser context. We evaluate our approach on eight mature and technologically diverse web applications, discovering previously unknown and exploitable XSS flaws in each of those applications.

</details>

<details>

<summary>2020-08-07 17:52:11 - A Novel Tampering Attack on AES Cores with Hardware Trojans</summary>

- *Ayush Jain, Ujjwal Guin*

- `2008.03290v1` - [abs](http://arxiv.org/abs/2008.03290v1) - [pdf](http://arxiv.org/pdf/2008.03290v1)

> The implementation of cryptographic primitives in integrated circuits (ICs) continues to increase over the years due to the recent advancement of semiconductor manufacturing and reduction of cost per transistors. The hardware implementation makes cryptographic operations faster and more energy-efficient. However, various hardware attacks have been proposed aiming to extract the secret key in order to undermine the security of these primitives. In this paper, we focus on the widely used advanced encryption standard (AES) block cipher and demonstrate its vulnerability against tampering attack. Our proposed attack relies on implanting a hardware Trojan in the netlist by an untrusted foundry, which can design and implement such a Trojan as it has access to the design layout and mask information. The hardware Trojan's activation modifies a particular round's input data by preventing the effect of all previous rounds' key-dependent computation. We propose to use a sequential hardware Trojan to deliver the payload at the input of an internal round for achieving this modification of data. All the internal subkeys, and finally, the secret key can be computed from the observed ciphertext once the Trojan is activated. We implement our proposed tampering attack with a sequential hardware Trojan inserted into a 128-bit AES design from OpenCores benchmark suite and report the area overhead to demonstrate the feasibility of the proposed tampering attack.

</details>

<details>

<summary>2020-08-08 07:14:40 - Audio Spoofing Verification using Deep Convolutional Neural Networks by Transfer Learning</summary>

- *Rahul T P, P R Aravind, Ranjith C, Usamath Nechiyil, Nandakumar Paramparambath*

- `2008.03464v1` - [abs](http://arxiv.org/abs/2008.03464v1) - [pdf](http://arxiv.org/pdf/2008.03464v1)

> Automatic Speaker Verification systems are gaining popularity these days; spoofing attacks are of prime concern as they make these systems vulnerable. Some spoofing attacks like Replay attacks are easier to implement but are very hard to detect thus creating the need for suitable countermeasures. In this paper, we propose a speech classifier based on deep-convolutional neural network to detect spoofing attacks. Our proposed methodology uses acoustic time-frequency representation of power spectral densities on Mel frequency scale (Mel-spectrogram), via deep residual learning (an adaptation of ResNet-34 architecture). Using a single model system, we have achieved an equal error rate (EER) of 0.9056% on the development and 5.32% on the evaluation dataset of logical access scenario and an equal error rate (EER) of 5.87% on the development and 5.74% on the evaluation dataset of physical access scenario of ASVspoof 2019.

</details>

<details>

<summary>2020-08-08 20:47:40 - PolyScope: Multi-Policy Access Control Analysis to Triage Android Systems</summary>

- *Yu-Tsung Lee, William Enck, Haining Chen, Hayawardh Vijayakumar, Ninghui Li, Daimeng Wang, Zhiyun Qian, Giuseppe Petracca, Trent Jaeger*

- `2008.03593v1` - [abs](http://arxiv.org/abs/2008.03593v1) - [pdf](http://arxiv.org/pdf/2008.03593v1)

> Android filesystem access control provides a foundation for Android system integrity. Android utilizes a combination of mandatory (e.g., SEAndroid) and discretionary (e.g., UNIX permissions) access control, both to protect the Android platform from Android/OEM services and to protect Android/OEM services from third-party apps. However, OEMs often create vulnerabilities when they introduce market-differentiating features because they err when re-configuring this complex combination of Android policies. In this paper, we propose the PolyScope tool to triage the combination of Android filesystem access control policies to vet releases for vulnerabilities. The PolyScope approach leverages two main insights: (1) adversaries may exploit the coarse granularity of mandatory policies and the flexibility of discretionary policies to increase the permissions available to launch attacks, which we call permission expansion, and (2) system configurations may limit the ways adversaries may use their permissions to launch attacks, motivating computation of attack operations. We apply PolyScope to three Google and five OEM Android releases to compute the attack operations accurately to vet these releases for vulnerabilities, finding that permission expansion increases the permissions available to launch attacks, sometimes by more than 10X, but a significant fraction of these permissions (about 15-20%) are not convertible into attack operations. Using PolyScope, we find two previously unknown vulnerabilities, showing how PolyScope helps OEMs triage the complex combination of access control policies down to attack operations worthy of testing.

</details>

<details>

<summary>2020-08-09 00:40:54 - Consumer UAV Cybersecurity Vulnerability Assessment Using Fuzzing Tests</summary>

- *David Rudo, Kai Zeng*

- `2008.03621v1` - [abs](http://arxiv.org/abs/2008.03621v1) - [pdf](http://arxiv.org/pdf/2008.03621v1)

> Unmanned Aerial Vehicles (UAVs) are remote-controlled vehicles capable of flight and are present in a variety of environments from military operations to domestic enjoyment. These vehicles are great assets, but just as their pilot can control them remotely, cyberattacks can be executed in a similar manner. Cyber attacks on UAVs can bring a plethora of issues to physical and virtual systems. Such malfunctions are capable of giving an attacker the ability to steal data, incapacitate the UAV, or hijack the UAV. To mitigate such attacks, it is necessary to identify and patch vulnerabilities that may be maliciously exploited. In this paper, a new UAV vulnerability is explored with related UAV security practices identified for possible exploitation using large streams of data sent at specific ports. The more in-depth model involves strings of data involving FTP-specific keywords sent to the UAV's FTP port in the form of a fuzzing test and launching thousands of packets at other ports on the UAV as well. During these tests, virtual and physical systems are monitored extensively to identify specific patterns and vulnerabilities. This model is applied to a Parrot Bebop 2, which accurately portrays a UAV that had their network compromised by an attacker and portrays many lower-end UAV models for domestic use. During testings, the Parrot Bebop 2 is monitored for degradation in GPS performance, video speed, the UAV's reactivity to the pilot, motor function, and the accuracy of the UAV's sensor data. All these points of monitoring give a comprehensive view of the UAV's reaction to each individual test. In this paper, countermeasures to combat the exploitation of this vulnerability will be discussed as well as possible attacks that can branch from the fuzzing tests.

</details>

<details>

<summary>2020-08-09 07:04:06 - Enhancing Robustness Against Adversarial Examples in Network Intrusion Detection Systems</summary>

- *Mohammad J. Hashemi, Eric Keller*

- `2008.03677v1` - [abs](http://arxiv.org/abs/2008.03677v1) - [pdf](http://arxiv.org/pdf/2008.03677v1)

> The increase of cyber attacks in both the numbers and varieties in recent years demands to build a more sophisticated network intrusion detection system (NIDS). These NIDS perform better when they can monitor all the traffic traversing through the network like when being deployed on a Software-Defined Network (SDN). Because of the inability to detect zero-day attacks, signature-based NIDS which were traditionally used for detecting malicious traffic are beginning to get replaced by anomaly-based NIDS built on neural networks. However, recently it has been shown that such NIDS have their own drawback namely being vulnerable to the adversarial example attack. Moreover, they were mostly evaluated on the old datasets which don't represent the variety of attacks network systems might face these days. In this paper, we present Reconstruction from Partial Observation (RePO) as a new mechanism to build an NIDS with the help of denoising autoencoders capable of detecting different types of network attacks in a low false alert setting with an enhanced robustness against adversarial example attack. Our evaluation conducted on a dataset with a variety of network attacks shows denoising autoencoders can improve detection of malicious traffic by up to 29% in a normal setting and by up to 45% in an adversarial setting compared to other recently proposed anomaly detectors.

</details>

<details>

<summary>2020-08-10 01:38:11 - TEAM: We Need More Powerful Adversarial Examples for DNNs</summary>

- *Yaguan Qian, Ximin Zhang, Bin Wang, Wei Li, Zhaoquan Gu, Haijiang Wang, Wassim Swaileh*

- `2007.15836v2` - [abs](http://arxiv.org/abs/2007.15836v2) - [pdf](http://arxiv.org/pdf/2007.15836v2)

> Although deep neural networks (DNNs) have achieved success in many application fields, it is still vulnerable to imperceptible adversarial examples that can lead to misclassification of DNNs easily. To overcome this challenge, many defensive methods are proposed. Indeed, a powerful adversarial example is a key benchmark to measure these defensive mechanisms. In this paper, we propose a novel method (TEAM, Taylor Expansion-Based Adversarial Methods) to generate more powerful adversarial examples than previous methods. The main idea is to craft adversarial examples by minimizing the confidence of the ground-truth class under untargeted attacks or maximizing the confidence of the target class under targeted attacks. Specifically, we define the new objective functions that approximate DNNs by using the second-order Taylor expansion within a tiny neighborhood of the input. Then the Lagrangian multiplier method is used to obtain the optimize perturbations for these objective functions. To decrease the amount of computation, we further introduce the Gauss-Newton (GN) method to speed it up. Finally, the experimental result shows that our method can reliably produce adversarial examples with 100% attack success rate (ASR) while only by smaller perturbations. In addition, the adversarial example generated with our method can defeat defensive distillation based on gradient masking.

</details>

<details>

<summary>2020-08-10 19:12:21 - Exploring Navigation Styles in a FutureLearn MOOC</summary>

- *Lei Shi, Alexandra I. Cristea, Armando M. Toda, Wilk Oliveira*

- `2008.04373v1` - [abs](http://arxiv.org/abs/2008.04373v1) - [pdf](http://arxiv.org/pdf/2008.04373v1)

> This paper presents for the first time a detailed analysis of fine-grained navigation style identification in MOOCs backed by a large number of active learners. The result shows 1) whilst the sequential style is clearly in evidence, the global style is less prominent; 2) the majority of the learners do not belong to either category; 3) navigation styles are not as stable as believed in the literature; and 4) learners can, and do, swap between navigation styles with detrimental effects. The approach is promising, as it provides insight into online learners' temporal engagement, as well as a tool to identify vulnerable learners, which potentially benefit personalised interventions (from teachers or automatic help) in Intelligent Tutoring Systems (ITS).

</details>

<details>

<summary>2020-08-10 19:27:34 - An Automated, End-to-End Framework for Modeling Attacks From Vulnerability Descriptions</summary>

- *Hodaya Binyamini, Ron Bitton, Masaki Inokuchi, Tomohiko Yagyu, Yuval Elovici, Asaf Shabtai*

- `2008.04377v1` - [abs](http://arxiv.org/abs/2008.04377v1) - [pdf](http://arxiv.org/pdf/2008.04377v1)

> Attack graphs are one of the main techniques used to automate the risk assessment process. In order to derive a relevant attack graph, up-to-date information on known attack techniques should be represented as interaction rules. Designing and creating new interaction rules is not a trivial task and currently performed manually by security experts. However, since the number of new security vulnerabilities and attack techniques continuously and rapidly grows, there is a need to frequently update the rule set of attack graph tools with new attack techniques to ensure that the set of interaction rules is always up-to-date. We present a novel, end-to-end, automated framework for modeling new attack techniques from textual description of a security vulnerability. Given a description of a security vulnerability, the proposed framework first extracts the relevant attack entities required to model the attack, completes missing information on the vulnerability, and derives a new interaction rule that models the attack; this new rule is integrated within MulVAL attack graph tool. The proposed framework implements a novel pipeline that includes a dedicated cybersecurity linguistic model trained on the the NVD repository, a recurrent neural network model used for attack entity extraction, a logistic regression model used for completing the missing information, and a novel machine learning-based approach for automatically modeling the attacks as MulVAL's interaction rule. We evaluated the performance of each of the individual algorithms, as well as the complete framework and demonstrated its effectiveness.

</details>

<details>

<summary>2020-08-10 20:26:05 - Secure IoT Data Analytics in Cloud via Intel SGX</summary>

- *Md Shihabul Islam, Mustafa Safa Ozdayi, Latifur Khan, Murat Kantarcioglu*

- `2008.05286v1` - [abs](http://arxiv.org/abs/2008.05286v1) - [pdf](http://arxiv.org/pdf/2008.05286v1)

> The growing adoption of IoT devices in our daily life is engendering a data deluge, mostly private information that needs careful maintenance and secure storage system to ensure data integrity and protection. Also, the prodigious IoT ecosystem has provided users with opportunities to automate systems by interconnecting their devices and other services with rule-based programs. The cloud services that are used to store and process sensitive IoT data turn out to be vulnerable to outside threats. Hence, sensitive IoT data and rule-based programs need to be protected against cyberattacks. To address this important challenge, in this paper, we propose a framework to maintain confidentiality and integrity of IoT data and rule-based program execution. We design the framework to preserve data privacy utilizing Trusted Execution Environment (TEE) such as Intel SGX, and end-to-end data encryption mechanism. We evaluate the framework by executing rule-based programs in the SGX securely with both simulated and real IoT device data.

</details>

<details>

<summary>2020-08-11 05:22:11 - Localizing Patch Points From One Exploit</summary>

- *Shiqi Shen, Aashish Kolluri, Zhen Dong, Prateek Saxena, Abhik Roychoudhury*

- `2008.04516v1` - [abs](http://arxiv.org/abs/2008.04516v1) - [pdf](http://arxiv.org/pdf/2008.04516v1)

> Automatic patch generation can significantly reduce the window of exposure after a vulnerability is disclosed. Towards this goal, a long-standing problem has been that of patch localization: to find a program point at which a patch can be synthesized. We present PatchLoc, one of the first systems which automatically identifies such a location in a vulnerable binary, given just one exploit, with high accuracy. PatchLoc does not make any assumptions about the availability of source code, test suites, or specialized knowledge of the vulnerability. PatchLoc pinpoints valid patch locations in large real-world applications with high accuracy for about 88% of 43 CVEs we study. These results stem from a novel approach to automatically synthesizing a test-suite which enables probabilistically ranking and effectively differentiating between candidate program patch locations.

</details>

<details>

<summary>2020-08-11 07:57:58 - Code-based Vulnerability Detection in Node.js Applications: How far are we?</summary>

- *Bodin Chinthanet, Serena Elisa Ponta, Henrik Plate, Antonino Sabetta, Raula Gaikovina Kula, Takashi Ishio, Kenichi Matsumoto*

- `2008.04568v1` - [abs](http://arxiv.org/abs/2008.04568v1) - [pdf](http://arxiv.org/pdf/2008.04568v1)

> With one of the largest available collection of reusable packages, the JavaScript runtime environment Node.js is one of the most popular programming application. With recent work showing evidence that known vulnerabilities are prevalent in both open source and industrial software, we propose and implement a viable code-based vulnerability detection tool for Node.js applications. Our case study lists the challenges encountered while implementing our Node.js vulnerable code detector.

</details>

<details>

<summary>2020-08-11 16:10:57 - On Security Measures for Containerized Applications Imaged with Docker</summary>

- *Samuel P. Mullinix, Erikton Konomi, Renee Davis Townsend, Reza M. Parizi*

- `2008.04814v1` - [abs](http://arxiv.org/abs/2008.04814v1) - [pdf](http://arxiv.org/pdf/2008.04814v1)

> Linux containers have risen in popularity in the last few years, making their way to commercial IT service offerings (such as PaaS), application deployments, and Continuous Delivery/Integration pipelines within various development teams. Along with the wide adoption of Docker, security vulnerabilities and concerns have also surfaced. In this survey, we examine the state of security for the most popular container system at the moment: Docker. We will also look into its origins stemming from the Linux technologies built into the OS itself; examine intrinsic vulnerabilities, such as the Docker Image implementation; and provide an analysis of current tools and modern methodologies used in the field to evaluate and enhance its security. For each section, we pinpoint metrics of interest, as they have been revealed by researchers and experts in the domain and summarize their findings to paint a holistic picture of the efforts behind those findings. Lastly, we look at tools utilized in the industry to streamline Docker security scanning and analytics which provide built-in aggregation of key metrics.

</details>

<details>

<summary>2020-08-12 11:44:01 - Learning to Learn from Mistakes: Robust Optimization for Adversarial Noise</summary>

- *Alex Serban, Erik Poll, Joost Visser*

- `2008.05247v1` - [abs](http://arxiv.org/abs/2008.05247v1) - [pdf](http://arxiv.org/pdf/2008.05247v1)

> Sensitivity to adversarial noise hinders deployment of machine learning algorithms in security-critical applications. Although many adversarial defenses have been proposed, robustness to adversarial noise remains an open problem. The most compelling defense, adversarial training, requires a substantial increase in processing time and it has been shown to overfit on the training data. In this paper, we aim to overcome these limitations by training robust models in low data regimes and transfer adversarial knowledge between different models. We train a meta-optimizer which learns to robustly optimize a model using adversarial examples and is able to transfer the knowledge learned to new models, without the need to generate new adversarial examples. Experimental results show the meta-optimizer is consistent across different architectures and data sets, suggesting it is possible to automatically patch adversarial vulnerabilities.

</details>

<details>

<summary>2020-08-12 17:14:00 - Drift with Devil: Security of Multi-Sensor Fusion based Localization in High-Level Autonomous Driving under GPS Spoofing (Extended Version)</summary>

- *Junjie Shen, Jun Yeon Won, Zeyuan Chen, Qi Alfred Chen*

- `2006.10318v3` - [abs](http://arxiv.org/abs/2006.10318v3) - [pdf](http://arxiv.org/pdf/2006.10318v3)

> For high-level Autonomous Vehicles (AV), localization is highly security and safety critical. One direct threat to it is GPS spoofing, but fortunately, AV systems today predominantly use Multi-Sensor Fusion (MSF) algorithms that are generally believed to have the potential to practically defeat GPS spoofing. However, no prior work has studied whether today's MSF algorithms are indeed sufficiently secure under GPS spoofing, especially in AV settings. In this work, we perform the first study to fill this critical gap. As the first study, we focus on a production-grade MSF with both design and implementation level representativeness, and identify two AV-specific attack goals, off-road and wrong-way attacks.   To systematically understand the security property, we first analyze the upper-bound attack effectiveness, and discover a take-over effect that can fundamentally defeat the MSF design principle. We perform a cause analysis and find that such vulnerability only appears dynamically and non-deterministically. Leveraging this insight, we design FusionRipper, a novel and general attack that opportunistically captures and exploits take-over vulnerabilities. We evaluate it on 6 real-world sensor traces, and find that FusionRipper can achieve at least 97% and 91.3% success rates in all traces for off-road and wrong-way attacks respectively. We also find that it is highly robust to practical factors such as spoofing inaccuracies. To improve the practicality, we further design an offline method that can effectively identify attack parameters with over 80% average success rates for both attack goals, with the cost of at most half a day. We also discuss promising defense directions.

</details>

<details>

<summary>2020-08-13 09:34:22 - IMDfence: Architecting a Secure Protocol for Implantable Medical Devices</summary>

- *Muhammad Ali Siddiqi, Christian Doerr, Christos Strydis*

- `2002.09546v4` - [abs](http://arxiv.org/abs/2002.09546v4) - [pdf](http://arxiv.org/pdf/2002.09546v4)

> Over the past decade, focus on the security and privacy aspects of implantable medical devices (IMDs) has intensified, driven by the multitude of cybersecurity vulnerabilities found in various existing devices. However, due to their strict computational, energy and physical constraints, conventional security protocols are not directly applicable to IMDs. Custom-tailored schemes have been proposed instead which, however, fail to cover the full spectrum of security features that modern IMDs and their ecosystems so critically require. In this paper we propose IMDfence, a security protocol for IMD ecosystems that provides a comprehensive yet practical security portfolio, which includes availability, non-repudiation, access control, entity authentication, remote monitoring and system scalability. The protocol also allows emergency access that results in the graceful degradation of offered services without compromising security and patient safety. The performance of the security protocol as well as its feasibility and impact on modern IMDs are extensively analyzed and evaluated. We find that IMDfence achieves the above security requirements at a mere less than 7% increase in total IMD energy consumption, and less than 14 ms and 9 kB increase in system delay and memory footprint, respectively.

</details>

<details>

<summary>2020-08-13 10:08:27 - Detecting Abnormal Traffic in Large-Scale Networks</summary>

- *Mahmoud Said Elsayed, Nhien-An Le-Khac, Soumyabrata Dev, Anca Delia Jurcut*

- `2008.05791v1` - [abs](http://arxiv.org/abs/2008.05791v1) - [pdf](http://arxiv.org/pdf/2008.05791v1)

> With the rapid technological advancements, organizations need to rapidly scale up their information technology (IT) infrastructure viz. hardware, software, and services, at a low cost. However, the dynamic growth in the network services and applications creates security vulnerabilities and new risks that can be exploited by various attacks. For example, User to Root (U2R) and Remote to Local (R2L) attack categories can cause a significant damage and paralyze the entire network system. Such attacks are not easy to detect due to the high degree of similarity to normal traffic. While network anomaly detection systems are being widely used to classify and detect malicious traffic, there are many challenges to discover and identify the minority attacks in imbalanced datasets. In this paper, we provide a detailed and systematic analysis of the existing Machine Learning (ML) approaches that can tackle most of these attacks. Furthermore, we propose a Deep Learning (DL) based framework using Long Short Term Memory (LSTM) autoencoder that can accurately detect malicious traffics in network traffic. We perform our experiments in a publicly available dataset of Intrusion Detection Systems (IDSs). We obtain a significant improvement in attack detection, as compared to other benchmarking methods. Hence, our method provides great confidence in securing these networks from malicious traffic.

</details>

<details>

<summary>2020-08-13 16:45:51 - Déjà Vu: Side-Channel Analysis of Mozilla's NSS</summary>

- *Sohaib ul Hassan, Iaroslav Gridin, Ignacio M. Delgado-Lozano, Cesar Pereida García, Jesús-Javier Chi-Domínguez, Alejandro Cabrera Aldaya, Billy Bob Brumley*

- `2008.06004v1` - [abs](http://arxiv.org/abs/2008.06004v1) - [pdf](http://arxiv.org/pdf/2008.06004v1)

> Recent work on Side Channel Analysis (SCA) targets old, well-known vulnerabilities, even previously exploited, reported, and patched in high-profile cryptography libraries. Nevertheless, researchers continue to find and exploit the same vulnerabilities in old and new products, highlighting a big issue among vendors: effectively tracking and fixing security vulnerabilities when disclosure is not done directly to them. In this work, we present another instance of this issue by performing the first library-wide SCA security evaluation of Mozilla's NSS security library. We use a combination of two independently-developed SCA security frameworks to identify and test security vulnerabilities. Our evaluation uncovers several new vulnerabilities in NSS affecting DSA, ECDSA, and RSA cryptosystems. We exploit said vulnerabilities and implement key recovery attacks using signals---extracted through different techniques such as timing, microarchitecture, and EM---and improved lattice methods.

</details>

<details>

<summary>2020-08-14 07:49:20 - Graph Neural Networks with Continual Learning for Fake News Detection from Social Media</summary>

- *Yi Han, Shanika Karunasekera, Christopher Leckie*

- `2007.03316v2` - [abs](http://arxiv.org/abs/2007.03316v2) - [pdf](http://arxiv.org/pdf/2007.03316v2)

> Although significant effort has been applied to fact-checking, the prevalence of fake news over social media, which has profound impact on justice, public trust and our society, remains a serious problem. In this work, we focus on propagation-based fake news detection, as recent studies have demonstrated that fake news and real news spread differently online. Specifically, considering the capability of graph neural networks (GNNs) in dealing with non-Euclidean data, we use GNNs to differentiate between the propagation patterns of fake and real news on social media. In particular, we concentrate on two questions: (1) Without relying on any text information, e.g., tweet content, replies and user descriptions, how accurately can GNNs identify fake news? Machine learning models are known to be vulnerable to adversarial attacks, and avoiding the dependence on text-based features can make the model less susceptible to the manipulation of advanced fake news fabricators. (2) How to deal with new, unseen data? In other words, how does a GNN trained on a given dataset perform on a new and potentially vastly different dataset? If it achieves unsatisfactory performance, how do we solve the problem without re-training the model on the entire data from scratch? We study the above questions on two datasets with thousands of labelled news items, and our results show that: (1) GNNs can achieve comparable or superior performance without any text information to state-of-the-art methods. (2) GNNs trained on a given dataset may perform poorly on new, unseen data, and direct incremental training cannot solve the problem---this issue has not been addressed in the previous work that applies GNNs for fake news detection. In order to solve the problem, we propose a method that achieves balanced performance on both existing and new datasets, by using techniques from continual learning to train GNNs incrementally.

</details>

<details>

<summary>2020-08-15 12:35:28 - Adversarial Robustness for Code</summary>

- *Pavol Bielik, Martin Vechev*

- `2002.04694v2` - [abs](http://arxiv.org/abs/2002.04694v2) - [pdf](http://arxiv.org/pdf/2002.04694v2)

> Machine learning and deep learning in particular has been recently used to successfully address many tasks in the domain of code such as finding and fixing bugs, code completion, decompilation, type inference and many others. However, the issue of adversarial robustness of models for code has gone largely unnoticed. In this work, we explore this issue by: (i) instantiating adversarial attacks for code (a domain with discrete and highly structured inputs), (ii) showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks, and (iii) combining existing and novel techniques to improve robustness while preserving high accuracy.

</details>

<details>

<summary>2020-08-15 17:26:48 - Tackling COVID-19 through Responsible AI Innovation: Five Steps in the Right Direction</summary>

- *David Leslie*

- `2008.06755v1` - [abs](http://arxiv.org/abs/2008.06755v1) - [pdf](http://arxiv.org/pdf/2008.06755v1)

> Innovations in data science and AI/ML have a central role to play in supporting global efforts to combat COVID-19. The versatility of AI/ML technologies enables scientists and technologists to address an impressively broad range of biomedical, epidemiological, and socioeconomic challenges. This wide-reaching scientific capacity, however, also raises a diverse array of ethical challenges. The need for researchers to act quickly and globally in tackling SARS-CoV-2 demands unprecedented practices of open research and responsible data sharing at a time when innovation ecosystems are hobbled by proprietary protectionism, inequality, and a lack of public trust. Moreover, societally impactful interventions like digital contact tracing are raising fears of surveillance creep and are challenging widely held commitments to privacy, autonomy, and civil liberties. Prepandemic concerns that data-driven innovations may function to reinforce entrenched dynamics of societal inequity have likewise intensified given the disparate impact of the virus on vulnerable social groups and the life-and-death consequences of biased and discriminatory public health outcomes. To address these concerns, I offer five steps that need to be taken to encourage responsible research and innovation. These provide a practice-based path to responsible AI/ML design and discovery centered on open, accountable, equitable, and democratically governed processes and products. When taken from the start, these steps will not only enhance the capacity of innovators to tackle COVID-19 responsibly, they will, more broadly, help to better equip the data science and AI/ML community to cope with future pandemics and to support a more humane, rational, and just society.

</details>

<details>

<summary>2020-08-16 11:56:30 - Efficient, Flexible and Secure Group Key Management Protocol for Dynamic IoT Settings</summary>

- *Adhirath Kabra, Sumit Kumar, Gaurav S. Kasbekar*

- `2008.06890v1` - [abs](http://arxiv.org/abs/2008.06890v1) - [pdf](http://arxiv.org/pdf/2008.06890v1)

> Many Internet of Things (IoT) scenarios require communication to and data acquisition from multiple devices with similar functionalities. For such scenarios, group communication in the form of multicasting and broadcasting has proven to be effective. Group Key Management (GKM) involves the handling, revocation, updation and distribution of cryptographic keys to members of various groups. Classical GKM schemes perform inefficiently in dynamic IoT environments, which are those wherein nodes frequently leave or join a network or migrate from one group to another over time. Recently, the `GroupIt' scheme has been proposed for GKM in dynamic IoT environments. However, this scheme has several limitations such as vulnerability to collusion attacks, the use of computationally expensive asymmetric encryption and threats to the backward secrecy of the system. In this paper, we present a highly efficient and secure GKM protocol for dynamic IoT settings, which maintains forward and backward secrecy at all times. Our proposed protocol uses only symmetric encryption, and is completely resistant to collusion attacks. Also, our protocol is highly flexible and can handle several new scenarios in which device or user dynamics may take place, e.g., allowing a device group to join or leave the network or creation or dissolution of a user group, which are not handled by schemes proposed in prior literature. We evaluate the performance of the proposed protocol via extensive mathematical analysis and numerical computations, and show that it outperforms the GroupIt scheme in terms of the communication and computation costs incurred by users and devices.

</details>

<details>

<summary>2020-08-16 14:40:39 - Discouraging Pool Block Withholding Attacks in Bitcoins</summary>

- *Zhihuai Chen, Bo Li, Xiaohan Shan, Xiaoming Sun, Jialin Zhang*

- `2008.06923v1` - [abs](http://arxiv.org/abs/2008.06923v1) - [pdf](http://arxiv.org/pdf/2008.06923v1)

> The arisen of Bitcoin has led to much enthusiasm for blockchain research and block mining, and the extensive existence of mining pools helps its participants (i.e., miners) gain reward more frequently. Recently, the mining pools are proved to be vulnerable for several possible attacks, and pool block withholding attack is one of them: one strategic pool manager sends some of her miners to other pools and these miners pretend to work on the puzzles but actually do nothing. And these miners still get reward since the pool manager can not recognize these malicious miners. In this work, we revisit the game-theoretic model for pool block withholding attacks and propose a revised approach to reallocate the reward to the miners. Fortunately, in the new model, the pool managers have strong incentive to not launch such attacks. We show that for any number of mining pools, no-pool-attacks is always a Nash equilibrium. Moreover, with only two minority mining pools participating, no-pool-attacks is actually the unique Nash equilibrium.

</details>

<details>

<summary>2020-08-17 01:09:34 - Adversarial Reinforcement Learning under Partial Observability in Autonomous Computer Network Defence</summary>

- *Yi Han, David Hubczenko, Paul Montague, Olivier De Vel, Tamas Abraham, Benjamin I. P. Rubinstein, Christopher Leckie, Tansu Alpcan, Sarah Erfani*

- `1902.09062v3` - [abs](http://arxiv.org/abs/1902.09062v3) - [pdf](http://arxiv.org/pdf/1902.09062v3)

> Recent studies have demonstrated that reinforcement learning (RL) agents are susceptible to adversarial manipulation, similar to vulnerabilities previously demonstrated in the supervised learning setting. While most existing work studies the problem in the context of computer vision or console games, this paper focuses on reinforcement learning in autonomous cyber defence under partial observability. We demonstrate that under the black-box setting, where the attacker has no direct access to the target RL model, causative attacks---attacks that target the training process---can poison RL agents even if the attacker only has partial observability of the environment. In addition, we propose an inversion defence method that aims to apply the opposite perturbation to that which an attacker might use to generate their adversarial samples. Our experimental results illustrate that the countermeasure can effectively reduce the impact of the causative attack, while not significantly affecting the training process in non-attack scenarios.

</details>

<details>

<summary>2020-08-17 03:20:05 - Putting the Semantics into Semantic Versioning</summary>

- *Patrick Lam, Jens Dietrich, David J. Pearce*

- `2008.07069v1` - [abs](http://arxiv.org/abs/2008.07069v1) - [pdf](http://arxiv.org/pdf/2008.07069v1)

> The long-standing aspiration for software reuse has made astonishing strides in the past few years. Many modern software development ecosystems now come with rich sets of publicly-available components contributed by the community. Downstream developers can leverage these upstream components, boosting their productivity.   However, components evolve at their own pace. This imposes obligations on and yields benefits for downstream developers, especially since changes can be breaking, requiring additional downstream work to adapt to. Upgrading too late leaves downstream vulnerable to security issues and missing out on useful improvements; upgrading too early results in excess work. Semantic versioning has been proposed as an elegant mechanism to communicate levels of compatibility, enabling downstream developers to automate dependency upgrades.   While it is questionable whether a version number can adequately characterize version compatibility in general, we argue that developers would greatly benefit from tools such as semantic version calculators to help them upgrade safely. The time is now for the research community to develop such tools: large component ecosystems exist and are accessible, component interactions have become observable through automated builds, and recent advances in program analysis make the development of relevant tools feasible. In particular, contracts (both traditional and lightweight) are a promising input to semantic versioning calculators, which can suggest whether an upgrade is likely to be safe.

</details>

<details>

<summary>2020-08-17 08:33:50 - Binary-level Directed Fuzzing for Use-After-Free Vulnerabilities</summary>

- *Manh-Dung Nguyen, Sébastien Bardin, Richard Bonichon, Roland Groz, Matthieu Lemerre*

- `2002.10751v2` - [abs](http://arxiv.org/abs/2002.10751v2) - [pdf](http://arxiv.org/pdf/2002.10751v2)

> Directed fuzzing focuses on automatically testing specific parts of the code by taking advantage of additional information such as (partial) bug stack trace, patches or risky operations. Key applications include bug reproduction, patch testing and static analysis report verification. Although directed fuzzing has received a lot of attention recently, hard-to-detect vulnerabilities such as Use-After-Free (UAF) are still not well addressed, especially at the binary level. We propose UAFuzz, the first (binary-level) directed greybox fuzzer dedicated to UAF bugs. The technique features a fuzzing engine tailored to UAF specifics, a lightweight code instrumentation and an efficient bug triage step. Experimental evaluation for bug reproduction on real cases demonstrates that UAFuzz significantly outperforms state-of-the-art directed fuzzers in terms of fault detection rate, time to exposure and bug triaging. UAFuzz has also been proven effective in patch testing, leading to the discovery of 30 new bugs (7 CVEs) in programs such as Perl, GPAC and GNU Patch. Finally, we provide to the community a large fuzzing benchmark dedicated to UAF, built on both real codes and real bugs.

</details>

<details>

<summary>2020-08-17 13:06:20 - InSpectre: Breaking and Fixing Microarchitectural Vulnerabilities by Formal Analysis</summary>

- *Roberto Guanciale, Musard Balliu, Mads Dam*

- `1911.00868v2` - [abs](http://arxiv.org/abs/1911.00868v2) - [pdf](http://arxiv.org/pdf/1911.00868v2)

> The recent Spectre attacks has demonstrated the fundamental insecurity of current computer microarchitecture. The attacks use features like pipelining, out-of-order and speculation to extract arbitrary information about the memory contents of a process. A comprehensive formal microarchitectural model capable of representing the forms of out-of-order and speculative behavior that can meaningfully be implemented in a high performance pipelined architecture has not yet emerged. Such a model would be very useful, as it would allow the existence and non-existence of vulnerabilities, and soundness of countermeasures to be formally established.   In this paper we present such a model targeting single core processors. The model is intentionally very general and provides an infrastructure to define models of real CPUs. It incorporates microarchitectural features that underpin all known Spectre vulnerabilities. We use the model to elucidate the security of existing and new vulnerabilities, as well as to formally analyze the effectiveness of proposed countermeasures. Specifically, we discover three new (potential) vulnerabilities, including a new variant of Spectre v4, a vulnerability on speculative fetching, and a vulnerability on out-of-order execution, and analyze the effectiveness of three existing countermeasures: constant time, Retpoline, and ARM's Speculative Store Bypass Safe (SSBS).

</details>

<details>

<summary>2020-08-18 00:58:19 - Adversarial Attack and Defense Strategies for Deep Speaker Recognition Systems</summary>

- *Arindam Jati, Chin-Cheng Hsu, Monisankha Pal, Raghuveer Peri, Wael AbdAlmageed, Shrikanth Narayanan*

- `2008.07685v1` - [abs](http://arxiv.org/abs/2008.07685v1) - [pdf](http://arxiv.org/pdf/2008.07685v1)

> Robust speaker recognition, including in the presence of malicious attacks, is becoming increasingly important and essential, especially due to the proliferation of several smart speakers and personal agents that interact with an individual's voice commands to perform diverse, and even sensitive tasks. Adversarial attack is a recently revived domain which is shown to be effective in breaking deep neural network-based classifiers, specifically, by forcing them to change their posterior distribution by only perturbing the input samples by a very small amount. Although, significant progress in this realm has been made in the computer vision domain, advances within speaker recognition is still limited. The present expository paper considers several state-of-the-art adversarial attacks to a deep speaker recognition system, employing strong defense methods as countermeasures, and reporting on several ablation studies to obtain a comprehensive understanding of the problem. The experiments show that the speaker recognition systems are vulnerable to adversarial attacks, and the strongest attacks can reduce the accuracy of the system from 94% to even 0%. The study also compares the performances of the employed defense methods in detail, and finds adversarial training based on Projected Gradient Descent (PGD) to be the best defense method in our setting. We hope that the experiments presented in this paper provide baselines that can be useful for the research community interested in further studying adversarial robustness of speaker recognition systems.

</details>

<details>

<summary>2020-08-18 21:59:57 - Clustering and Analysis of Vulnerabilities Present in Different Robot Types</summary>

- *Chinwe Ekenna, Bharvee Acharya*

- `2008.08166v1` - [abs](http://arxiv.org/abs/2008.08166v1) - [pdf](http://arxiv.org/pdf/2008.08166v1)

> Due to the new advancements in automation using Artificial Intelligence, Robotics and Internet of Things it has become crucial to pay attention to possible vulnerabilities in order to avoid cyber attack and hijacking that can occur which can be catastrophic. There have been many consequences of disasters due to vulnerabilities in Robotics, these vulnerabilities need to be analyzed to target the severe ones before they cause cataclysm. This paper aims to highlight the areas and severity of each type of vulnerability by analyzing issues categorized under the type of vulnerability. This we achieve by careful analysis of the data and application of information retrieval techniques like Term Frequency - Inverse Document Frequency, dimension reduction techniques like Principal Component Analysis and Clustering using Machine Learning techniques like K-means. By performing this analysis, the severity of robotic issues in different domains and the severity of the issue based on type of issue is detected.

</details>

<details>

<summary>2020-08-19 03:54:27 - Identifying Implicit Vulnerabilities through Personas as Goal Models</summary>

- *Shamal Faily, Claudia Iacob, Raian Ali, Duncan Ki-Aries*

- `2008.04773v2` - [abs](http://arxiv.org/abs/2008.04773v2) - [pdf](http://arxiv.org/pdf/2008.04773v2)

> When used in requirements processes and tools, personas have the potential to identify vulnerabilities resulting from misalignment between user expectations and system goals. Typically, however, this potential is unfulfilled as personas and system goals are captured with different mindsets, by different teams, and for different purposes. If personas are visualised as goal models, it may be easier for stakeholders to see implications of their goals being satisfied or denied, and designers to incorporate the creation and analysis of such models into the broader RE tool-chain. This paper outlines a tool-supported approach for finding implicit vulnerabilities from user and system goals by reframing personas as social goal models. We illustrate this approach with a case study where previously hidden vulnerabilities based on human behaviour were identified.

</details>

<details>

<summary>2020-08-19 08:46:39 - Toward Smart Security Enhancement of Federated Learning Networks</summary>

- *Junjie Tan, Ying-Chang Liang, Nguyen Cong Luong, Dusit Niyato*

- `2008.08330v1` - [abs](http://arxiv.org/abs/2008.08330v1) - [pdf](http://arxiv.org/pdf/2008.08330v1)

> As traditional centralized learning networks (CLNs) are facing increasing challenges in terms of privacy preservation, communication overheads, and scalability, federated learning networks (FLNs) have been proposed as a promising alternative paradigm to support the training of machine learning (ML) models. In contrast to the centralized data storage and processing in CLNs, FLNs exploit a number of edge devices (EDs) to store data and perform training distributively. In this way, the EDs in FLNs can keep training data locally, which preserves privacy and reduces communication overheads. However, since the model training within FLNs relies on the contribution of all EDs, the training process can be disrupted if some of the EDs upload incorrect or falsified training results, i.e., poisoning attacks. In this paper, we review the vulnerabilities of FLNs, and particularly give an overview of poisoning attacks and mainstream countermeasures. Nevertheless, the existing countermeasures can only provide passive protection and fail to consider the training fees paid for the contributions of the EDs, resulting in a unnecessarily high training cost. Hence, we present a smart security enhancement framework for FLNs. In particular, a verify-before-aggregate (VBA) procedure is developed to identify and remove the non-benign training results from the EDs. Afterward, deep reinforcement learning (DRL) is applied to learn the behaving patterns of the EDs and to actively select the EDs that can provide benign training results and charge low training fees. Simulation results reveal that the proposed framework can protect FLNs effectively and efficiently.

</details>

<details>

<summary>2020-08-19 11:42:56 - PASCAL: Timing SCA Resistant Design and Verification Flow</summary>

- *Xinhui Lai, Maksim Jenihhin, Jaan Raik, Kolin Paul*

- `2002.11108v2` - [abs](http://arxiv.org/abs/2002.11108v2) - [pdf](http://arxiv.org/pdf/2002.11108v2)

> A large number of crypto accelerators are being deployed with the widespread adoption of IoT. It is vitally important that these accelerators and other security hardware IPs are provably secure. Security is an extra functional requirement and hence many security verification tools are not mature. We propose an approach/flow-PASCAL-that works on RTL designs and discovers potential Timing Side-Channel Attack(SCA) vulnerabilities in them. Based on information flow analysis, this is able to identify Timing Disparate Security Paths that could lead to information leakage. This flow also (automatically) eliminates the information leakage caused by the timing channel. The insertion of a lightweight Compensator Block as balancing or compliance FSM removes the timing channel with minimum modifications to the design with no impact on the clock cycle time or combinational delay of the critical path in the circuit.

</details>

<details>

<summary>2020-08-19 11:52:50 - Security in Energy Harvesting Networks: A Survey of Current Solutions and Research Challenges</summary>

- *Pietro Tedeschi, Savio Sciancalepore, Roberto Di Pietro*

- `2004.10394v3` - [abs](http://arxiv.org/abs/2004.10394v3) - [pdf](http://arxiv.org/pdf/2004.10394v3)

> The recent advancements in hardware miniaturization capabilities have boosted the diffusion of systems based on Energy Harvesting (EH) technologies, as a means to power embedded wireless devices in a sustainable and low-cost fashion. Despite the undeniable management advantages, the intermittent availability of the energy source and the limited power supply has led to challenging system trade-offs, resulting in an increased attack surface and a general relaxation of the available security services.   In this paper, we survey the security issues, applications, techniques, and challenges arising in wireless networks powered via EH technologies. We explore the vulnerabilities of EH networks, and we provide a comprehensive overview of the scientific literature, including attack vectors, cryptography techniques, physical-layer security schemes for data secrecy, and additional physical-layer countermeasures. For each of the identified macro-areas, we compare the scientific contributions across a range of shared features, indicating the pros and cons of the described techniques, the research challenges, and a few future directions. Finally, we also provide an overview of the emerging topics in the area, such as Non-Orthogonal Multiple Access (NOMA) and Rate-Splitting Multiple Access (RSMA) schemes, and Intelligent Reconfigurable Surfaces, that could trigger the interest of industry and academia and unleash the full potential of pervasive EH wireless networks.

</details>

<details>

<summary>2020-08-19 12:59:12 - Early RTL Analysis for SCA Vulnerability in Fuzzy Extractors of Memory-Based PUF Enabled Devices</summary>

- *Xinhui Lai, Maksim Jenihhin, Georgios Selimis, Sven Goossens, Roel Maes, Kolin Paul*

- `2008.08409v1` - [abs](http://arxiv.org/abs/2008.08409v1) - [pdf](http://arxiv.org/pdf/2008.08409v1)

> Physical Unclonable Functions (PUFs) are gaining attention in the cryptography community because of the ability to efficiently harness the intrinsic variability in the manufacturing process. However, this means that they are noisy devices and require error correction mechanisms, e.g., by employing Fuzzy Extractors (FEs). Recent works demonstrated that applying FEs for error correction may enable new opportunities to break the PUFs if no countermeasures are taken. In this paper, we address an attack model on FEs hardware implementations and provide a solution for early identification of the timing Side-Channel Attack (SCA) vulnerabilities which can be exploited by physical fault injection. The significance of this work stems from the fact that FEs are an essential building block in the implementations of PUF-enabled devices. The information leaked through the timing side-channel during the error correction process can reveal the FE input data and thereby can endanger revealing secrets. Therefore, it is very important to identify the potential leakages early in the process during RTL design. Experimental results based on RTL analysis of several Bose-Chaudhuri-Hocquenghem (BCH) and Reed-Solomon decoders for PUF-enabled devices with FEs demonstrate the feasibility of the proposed methodology.

</details>

<details>

<summary>2020-08-21 03:02:58 - Defending Regression Learners Against Poisoning Attacks</summary>

- *Sandamal Weerasinghe, Sarah M. Erfani, Tansu Alpcan, Christopher Leckie, Justin Kopacz*

- `2008.09279v1` - [abs](http://arxiv.org/abs/2008.09279v1) - [pdf](http://arxiv.org/pdf/2008.09279v1)

> Regression models, which are widely used from engineering applications to financial forecasting, are vulnerable to targeted malicious attacks such as training data poisoning, through which adversaries can manipulate their predictions. Previous works that attempt to address this problem rely on assumptions about the nature of the attack/attacker or overestimate the knowledge of the learner, making them impractical. We introduce a novel Local Intrinsic Dimensionality (LID) based measure called N-LID that measures the local deviation of a given data point's LID with respect to its neighbors. We then show that N-LID can distinguish poisoned samples from normal samples and propose an N-LID based defense approach that makes no assumptions of the attacker. Through extensive numerical experiments with benchmark datasets, we show that the proposed defense mechanism outperforms the state of the art defenses in terms of prediction accuracy (up to 76% lower MSE compared to an undefended ridge model) and running time.

</details>

<details>

<summary>2020-08-21 03:11:23 - Defending Distributed Classifiers Against Data Poisoning Attacks</summary>

- *Sandamal Weerasinghe, Tansu Alpcan, Sarah M. Erfani, Christopher Leckie*

- `2008.09284v1` - [abs](http://arxiv.org/abs/2008.09284v1) - [pdf](http://arxiv.org/pdf/2008.09284v1)

> Support Vector Machines (SVMs) are vulnerable to targeted training data manipulations such as poisoning attacks and label flips. By carefully manipulating a subset of training samples, the attacker forces the learner to compute an incorrect decision boundary, thereby cause misclassifications. Considering the increased importance of SVMs in engineering and life-critical applications, we develop a novel defense algorithm that improves resistance against such attacks. Local Intrinsic Dimensionality (LID) is a promising metric that characterizes the outlierness of data samples. In this work, we introduce a new approximation of LID called K-LID that uses kernel distance in the LID calculation, which allows LID to be calculated in high dimensional transformed spaces. We introduce a weighted SVM against such attacks using K-LID as a distinguishing characteristic that de-emphasizes the effect of suspicious data samples on the SVM decision boundary. Each sample is weighted on how likely its K-LID value is from the benign K-LID distribution rather than the attacked K-LID distribution. We then demonstrate how the proposed defense can be applied to a distributed SVM framework through a case study on an SDR-based surveillance system. Experiments with benchmark data sets show that the proposed defense reduces classification error rates substantially (10% on average).

</details>

<details>

<summary>2020-08-21 07:19:32 - IoT Network Security: Requirements, Threats, and Countermeasures</summary>

- *Ayyoob Hamza, Hassan Habibi Gharakheili, Vijay Sivaraman*

- `2008.09339v1` - [abs](http://arxiv.org/abs/2008.09339v1) - [pdf](http://arxiv.org/pdf/2008.09339v1)

> IoT devices are increasingly utilized in critical infrastructure, enterprises, and households. There are several sophisticated cyber-attacks that have been reported and many networks have proven vulnerable to both active and passive attacks by leaking private information, allowing unauthorized access, and being open to denial of service attacks.   This paper aims firstly, to assist network operators to understand the need for an IoT network security solution, and then secondly, to survey IoT network attack vectors, cyber threats, and countermeasures with a focus on improving the robustness of existing security solutions. Our first contribution highlights viewpoints on IoT security from the perspective of stakeholders such as manufacturers, service providers, consumers, and authorities. We discuss the differences between IoT and IT systems, the need for IoT security solutions, and we highlight the key components required for IoT network security system architecture. For our second contribution, we survey the types of IoT attacks by grouping them based on their impact. We discuss various attack techniques, threats, and shortfalls of existing countermeasures with an intention to enable future research into improving IoT network security.

</details>

<details>

<summary>2020-08-21 13:49:54 - Applying Transparency in Artificial Intelligence based Personalization Systems</summary>

- *Laura Schelenz, Avi Segal, Kobi Gal*

- `2004.00935v2` - [abs](http://arxiv.org/abs/2004.00935v2) - [pdf](http://arxiv.org/pdf/2004.00935v2)

> Artificial Intelligence based systems increasingly use personalization to provide users with relevant content, products, and solutions. Personalization is intended to support users and address their respective needs and preferences. However, users are becoming increasingly vulnerable to online manipulation due to algorithmic advancements and lack of transparency. Such manipulation decreases users' levels of trust, autonomy, and satisfaction concerning the systems with which they interact. Increasing transparency is an important goal for personalization based systems. Unfortunately, system designers lack guidance in assessing and implementing transparency in their developed systems.   In this work we combine insights from technology ethics and computer science to generate a list of transparency best practices for machine generated personalization. Based on these best practices, we develop a checklist to be used by designers wishing to evaluate and increase the transparency of their algorithmic systems. Adopting a designer perspective, we apply the checklist to prominent online services and discuss its advantages and shortcomings. We encourage researchers to adopt the checklist in various environments and to work towards a consensus-based tool for measuring transparency in the personalization community.

</details>

<details>

<summary>2020-08-21 19:34:35 - Genome Reconstruction Attacks Against Genomic Data-Sharing Beacons</summary>

- *Kerem Ayoz, Erman Ayday, A. Ercument Cicek*

- `2001.08852v2` - [abs](http://arxiv.org/abs/2001.08852v2) - [pdf](http://arxiv.org/pdf/2001.08852v2)

> Sharing genome data in a privacy-preserving way stands as a major bottleneck in front of the scientific progress promised by the big data era in genomics. A community-driven protocol named genomic data-sharing beacon protocol has been widely adopted for sharing genomic data. The system aims to provide a secure, easy to implement, and standardized interface for data sharing by only allowing yes/no queries on the presence of specific alleles in the dataset. However, beacon protocol was recently shown to be vulnerable against membership inference attacks. In this paper, we show that privacy threats against genomic data sharing beacons are not limited to membership inference. We identify and analyze a novel vulnerability of genomic data-sharing beacons: genome reconstruction. We show that it is possible to successfully reconstruct a substantial part of the genome of a victim when the attacker knows the victim has been added to the beacon in a recent update. We also show that even if multiple individuals are added to the beacon during the same update, it is possible to identify the victim's genome with high confidence using traits that are easily accessible by the attacker (e.g., eye and hair color). Moreover, we show how the reconstructed genome using a beacon that is not associated with a sensitive phenotype can be used for membership inference attacks to beacons with sensitive phenotypes (i.e., HIV+). The outcome of this work will guide beacon operators on when and how to update the content of the beacon. Thus, this work will be an important attempt at helping beacon operators and participants make informed decisions.

</details>

<details>

<summary>2020-08-21 23:12:53 - One Exploit to Rule them All? On the Security of Drop-in Replacement and Counterfeit Microcontrollers</summary>

- *Johannes Obermaier, Marc Schink, Kosma Moczek*

- `2008.09710v1` - [abs](http://arxiv.org/abs/2008.09710v1) - [pdf](http://arxiv.org/pdf/2008.09710v1)

> With the increasing complexity of embedded systems, the firmware has become a valuable asset. At the same time, pressure for cost reductions in hardware is imminent. These two aspects are united at the heart of the system, i.e., the microcontroller. It runs and protects its firmware, but simultaneously has to prevail against cheaper alternatives. For the very popular STM32F1 microcontroller series, this has caused the emergence of many competitors in the last few years who offer drop-in replacements or even sell counterfeit devices at a fraction of the original price. Thus, the question emerges whether the replacements are silicon-level clones and, if not, whether they provide better, equal, or less security. In this paper, we analyze a total of six devices by four manufacturers, including the original device, in depth. Via a low-level analysis, we identify all of them as being individually developed devices. We further put the focus on debug and hardware security, discovering several novel vulnerabilities in all devices, causing the exposure of the entire firmware. All of the presented vulnerabilities, including invasive ones, are on a Do it Yourself (DiY) level without the demand for a sophisticated lab -- thereby underlining the urgency for hardware fixes. To facilitate further research, reproduction, and testing of other devices, we provide a comprehensive description of all vulnerabilities in this paper and code for proofs-of-concepts online.

</details>

<details>

<summary>2020-08-22 05:19:35 - MLD: An Intelligent Memory Leak Detection Scheme Based on Defect Modes in Smart Grids</summary>

- *Ling Yuan, Siyuan Zhou, Neal Xiong*

- `2008.09758v1` - [abs](http://arxiv.org/abs/2008.09758v1) - [pdf](http://arxiv.org/pdf/2008.09758v1)

> With the expansion of the software scale and complexity of smart grid systems, the detection of smart grid software defects has become a research hotspot. Because of the large scale of the existing smart grid software code, the efficiency and accuracy of the existing smart grid defect detection algorithms are not high. We propose an intelligent memory leak detection scheme based on defect modes MLD in smart grid. Based on the analysis of existing memory leak defect modes, we summarize memory operation behaviors (allocation, release and transfer) and present a state machine model. We employ a fuzzy matching algorithm based on regular expression to determine the memory operation behaviors and then analyze the change in the state machine to assess the vulnerability in the source code. To improve the efficiency of detection and solve the problem of repeated detection at the function call point, we propose a function summary method for memory operation behaviors. The experimental results demonstrate that the method we proposed has high detection speed and accuracy. The algorithm we proposed can identify the defects of the smart grid operation software and ensure the safe operation of the grid.

</details>

<details>

<summary>2020-08-23 00:46:03 - Blockchain-enabled Internet of Medical Things to Combat COVID-19</summary>

- *Hong-Ning Dai, Muhammad Imran, Noman Haider*

- `2008.09933v1` - [abs](http://arxiv.org/abs/2008.09933v1) - [pdf](http://arxiv.org/pdf/2008.09933v1)

> We are experiencing an unprecedented healthcare crisis caused by newly-discovered corona-virus disease (COVID-19). The outbreaks of COVID-19 reveal the frailties of existing healthcare systems. Therefore, the digital transformation of healthcare systems becomes an inevitable trend. During this process, the Internet of Medical Things (IoMT) plays a crucial role while intrinsic vulnerabilities of security and privacy deter the wide adoption of IoMT. In this article, we present a blockchain-enabled IoMT to address the security and privacy concerns of IoMT systems. We also discuss the solutions brought by blockchain-enabled IoMT to COVID-19 from five different perspectives. Moreover, we outline the open challenges and future directions of blockchain-enabled IoMT.

</details>

<details>

<summary>2020-08-23 03:37:51 - Vulnerability of Face Recognition Systems Against Composite Face Reconstruction Attack</summary>

- *Hadi Mansourifar, Weidong Shi*

- `2009.02286v1` - [abs](http://arxiv.org/abs/2009.02286v1) - [pdf](http://arxiv.org/pdf/2009.02286v1)

> Rounding confidence score is considered trivial but a simple and effective countermeasure to stop gradient descent based image reconstruction attacks. However, its capability in the face of more sophisticated reconstruction attacks is an uninvestigated research area. In this paper, we prove that, the face reconstruction attacks based on composite faces can reveal the inefficiency of rounding policy as countermeasure. We assume that, the attacker takes advantage of face composite parts which helps the attacker to get access to the most important features of the face or decompose it to the independent segments. Afterwards, decomposed segments are exploited as search parameters to create a search path to reconstruct optimal face. Face composition parts enable the attacker to violate the privacy of face recognition models even with a blind search. However, we assume that, the attacker may take advantage of random search to reconstruct the target face faster. The algorithm is started with random composition of face parts as initial face and confidence score is considered as fitness value. Our experiments show that, since the rounding policy as countermeasure can't stop the random search process, current face recognition systems are extremely vulnerable against such sophisticated attacks. To address this problem, we successfully test Face Detection Score Filtering (FDSF) as a countermeasure to protect the privacy of training data against proposed attack.

</details>

<details>

<summary>2020-08-25 15:04:32 - Two Sides of the Same Coin: White-box and Black-box Attacks for Transfer Learning</summary>

- *Yinghua Zhang, Yangqiu Song, Jian Liang, Kun Bai, Qiang Yang*

- `2008.11089v1` - [abs](http://arxiv.org/abs/2008.11089v1) - [pdf](http://arxiv.org/pdf/2008.11089v1)

> Transfer learning has become a common practice for training deep learning models with limited labeled data in a target domain. On the other hand, deep models are vulnerable to adversarial attacks. Though transfer learning has been widely applied, its effect on model robustness is unclear. To figure out this problem, we conduct extensive empirical evaluations to show that fine-tuning effectively enhances model robustness under white-box FGSM attacks. We also propose a black-box attack method for transfer learning models which attacks the target model with the adversarial examples produced by its source model. To systematically measure the effect of both white-box and black-box attacks, we propose a new metric to evaluate how transferable are the adversarial examples produced by a source model to a target model. Empirical results show that the adversarial examples are more transferable when fine-tuning is used than they are when the two networks are trained independently.

</details>

<details>

<summary>2020-08-25 22:51:51 - Likelihood Landscapes: A Unifying Principle Behind Many Adversarial Defenses</summary>

- *Fu Lin, Rohit Mittapalli, Prithvijit Chattopadhyay, Daniel Bolya, Judy Hoffman*

- `2008.11300v1` - [abs](http://arxiv.org/abs/2008.11300v1) - [pdf](http://arxiv.org/pdf/2008.11300v1)

> Convolutional Neural Networks have been shown to be vulnerable to adversarial examples, which are known to locate in subspaces close to where normal data lies but are not naturally occurring and of low probability. In this work, we investigate the potential effect defense techniques have on the geometry of the likelihood landscape - likelihood of the input images under the trained model. We first propose a way to visualize the likelihood landscape leveraging an energy-based model interpretation of discriminative classifiers. Then we introduce a measure to quantify the flatness of the likelihood landscape. We observe that a subset of adversarial defense techniques results in a similar effect of flattening the likelihood landscape. We further explore directly regularizing towards a flat landscape for adversarial robustness.

</details>

<details>

<summary>2020-08-26 13:05:32 - Non-linearity identification for construction workers' personality-safety behaviour predictive relationship using neural network and linear regression modelling</summary>

- *Yifan Gao, Vicente A. Gonzalez, Tak Wing Yiu, Guillermo Cabrera-Guerrerod*

- `1912.05944v3` - [abs](http://arxiv.org/abs/1912.05944v3) - [pdf](http://arxiv.org/pdf/1912.05944v3)

> The prediction of workers' safety behaviour can help identify vulnerable workers who intend to undertake unsafe behaviours and be useful in the design of management practices to minimise the occurrence of accidents. The latest literature has evidenced that there is within-population diversity that leads people's intended safety behaviours in the workplace, which are found to vary among individuals as a function of their personality traits. In this study, an innovative forecasting model, which employs neural network algorithms, is developed to numerically simulate the predictive relationship between construction workers' personality traits and their intended safety behaviour. The data-driven nature of neural network enabled a reliable estimate of the relationship, which allowed this research to find that a nonlinear effect exists in the relationship. This research has practical implications. The neural network developed is shown to have highly satisfactory prediction accuracy and is thereby potentially useful for assisting project decision-makers to assess how prone workers are to carry out unsafe behaviours in the workplace.

</details>

<details>

<summary>2020-08-26 15:56:55 - Defending Water Treatment Networks: Exploiting Spatio-temporal Effects for Cyber Attack Detection</summary>

- *Dongjie Wang, Pengyang Wang, Jingbo Zhou, Leilei Sun, Bowen Du, Yanjie Fu*

- `2008.12618v1` - [abs](http://arxiv.org/abs/2008.12618v1) - [pdf](http://arxiv.org/pdf/2008.12618v1)

> While Water Treatment Networks (WTNs) are critical infrastructures for local communities and public health, WTNs are vulnerable to cyber attacks. Effective detection of attacks can defend WTNs against discharging contaminated water, denying access, destroying equipment, and causing public fear. While there are extensive studies in WTNs attack detection, they only exploit the data characteristics partially to detect cyber attacks. After preliminary exploring the sensing data of WTNs, we find that integrating spatio-temporal knowledge, representation learning, and detection algorithms can improve attack detection accuracy. To this end, we propose a structured anomaly detection framework to defend WTNs by modeling the spatio-temporal characteristics of cyber attacks in WTNs. In particular, we propose a spatio-temporal representation framework specially tailored to cyber attacks after separating the sensing data of WTNs into a sequence of time segments. This framework has two key components. The first component is a temporal embedding module to preserve temporal patterns within a time segment by projecting the time segment of a sensor into a temporal embedding vector. We then construct Spatio-Temporal Graphs (STGs), where a node is a sensor and an attribute is the temporal embedding vector of the sensor, to describe the state of the WTNs. The second component is a spatial embedding module, which learns the final fused embedding of the WTNs from STGs. In addition, we devise an improved one class-SVM model that utilizes a new designed pairwise kernel to detect cyber attacks. The devised pairwise kernel augments the distance between normal and attack patterns in the fused embedding space. Finally, we conducted extensive experimental evaluations with real-world data to demonstrate the effectiveness of our framework.

</details>

<details>

<summary>2020-08-26 18:12:53 - Using Name Confusion to Enhance Security</summary>

- *Mohamed Tarek Ibn Ziad, Miguel A. Arroyo, Evgeny Manzhosov, Vasileios P. Kemerlis, Simha Sethumadhavan*

- `1911.02038v3` - [abs](http://arxiv.org/abs/1911.02038v3) - [pdf](http://arxiv.org/pdf/1911.02038v3)

> We introduce a novel concept, called Name Confusion, and demonstrate how it can be employed to thwart multiple classes of code-reuse attacks. By building upon Name Confusion, we derive Phantom Name System (PNS): a security protocol that provides multiple names (addresses) to program instructions. Unlike the conventional model of virtual memory with a one-to-one mapping between instructions and virtual memory addresses, PNS creates N mappings for the same instruction, and randomly switches between them at runtime. PNS achieves fast randomization, at the granularity of basic blocks, which mitigates a class of attacks known as (just-in-time) code-reuse.   If an attacker uses a memory safety-related vulnerability to cause any of the instruction addresses to be different from the one chosen during a fetch, the exploited program will crash. We quantitatively evaluate how PNS mitigates real-world code-reuse attacks by reducing the success probability of typical exploits to approximately $10^{-12}$. We implement PNS and validate it by running SPEC CPU2017 benchmark suite. We further verify its practicality by adding it to a RISC-V core on an FPGA. Lastly, PNS is mainly designed for resource constrained (wimpy) devices and has negligible performance overhead, compared to commercially-available, state-of-the-art, hardware-based protections.

</details>

<details>

<summary>2020-08-26 23:19:21 - Advbox: a toolbox to generate adversarial examples that fool neural networks</summary>

- *Dou Goodman, Hao Xin, Wang Yang, Wu Yuesheng, Xiong Junfeng, Zhang Huan*

- `2001.05574v5` - [abs](http://arxiv.org/abs/2001.05574v5) - [pdf](http://arxiv.org/pdf/2001.05574v5)

> In recent years, neural networks have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. Recent studies have shown that they are all vulnerable to the attack of adversarial examples. Small and often imperceptible perturbations to the input images are sufficient to fool the most powerful neural networks. \emph{Advbox} is a toolbox to generate adversarial examples that fool neural networks in PaddlePaddle, PyTorch, Caffe2, MxNet, Keras, TensorFlow and it can benchmark the robustness of machine learning models. Compared to previous work, our platform supports black box attacks on Machine-Learning-as-a-service, as well as more attack scenarios, such as Face Recognition Attack, Stealth T-shirt, and DeepFake Face Detect. The code is licensed under the Apache 2.0 and is openly available at https://github.com/advboxes/AdvBox. Advbox now supports Python 3.

</details>

<details>

<summary>2020-08-27 11:49:25 - Propensity-to-Pay: Machine Learning for Estimating Prediction Uncertainty</summary>

- *Md Abul Bashar, Astin-Walmsley Kieren, Heath Kerina, Richi Nayak*

- `2008.12065v1` - [abs](http://arxiv.org/abs/2008.12065v1) - [pdf](http://arxiv.org/pdf/2008.12065v1)

> Predicting a customer's propensity-to-pay at an early point in the revenue cycle can provide organisations many opportunities to improve the customer experience, reduce hardship and reduce the risk of impaired cash flow and occurrence of bad debt. With the advancements in data science; machine learning techniques can be used to build models to accurately predict a customer's propensity-to-pay. Creating effective machine learning models without access to large and detailed datasets presents some significant challenges. This paper presents a case-study, conducted on a dataset from an energy organisation, to explore the uncertainty around the creation of machine learning models that are able to predict residential customers entering financial hardship which then reduces their ability to pay energy bills. Incorrect predictions can result in inefficient resource allocation and vulnerable customers not being proactively identified. This study investigates machine learning models' ability to consider different contexts and estimate the uncertainty in the prediction. Seven models from four families of machine learning algorithms are investigated for their novel utilisation. A novel concept of utilising a Baysian Neural Network to the binary classification problem of propensity-to-pay energy bills is proposed and explored for deployment.

</details>

<details>

<summary>2020-08-27 15:31:09 - CACHE SNIPER : Accurate timing control of cache evictions</summary>

- *Samira Briongos, Ida Bruhns, Pedro Malagón, Thomas Eisenbarth, José M. Moya*

- `2008.12188v1` - [abs](http://arxiv.org/abs/2008.12188v1) - [pdf](http://arxiv.org/pdf/2008.12188v1)

> Microarchitectural side channel attacks have been very prominent in security research over the last few years. Caches have been an outstanding covert channel, as they provide high resolution and generic cross-core leakage even with simple user-mode code execution privileges. To prevent these generic cross-core attacks, all major cryptographic libraries now provide countermeasures to hinder key extraction via cross-core cache attacks, for instance avoiding secret dependent access patterns and prefetching data. In this paper, we show that implementations protected by 'good-enough' countermeasures aimed at preventing simple cache attacks are still vulnerable. We present a novel attack that uses a special timing technique to determine when an encryption has started and then evict the data precisely at the desired instant. This new attack does not require special privileges nor explicit synchronization between the attacker and the victim. One key improvement of our attack is a method to evict data from the cache with a single memory access and in absence of shared memory by leveraging the transient capabilities of TSX and relying on the recently reverse-engineered L3 replacement policy. We demonstrate the efficiency by performing an asynchronous last level cache attack to extract an RSA key from the latest wolfSSL library, which has been especially adapted to avoid leaky access patterns, and by extracting an AES key from the S-Box implementation included in OpenSSL bypassing the per round prefetch intended as a protection against cache attacks.

</details>

<details>

<summary>2020-08-27 20:50:48 - Zero-Bias Deep Learning for Accurate Identification of Internet of Things (IoT) Devices</summary>

- *Yongxin Liu, Jian Wang, Jianqiang Li, Houbing Song, Thomas Yang, Shuteng Niu, Zhong Ming*

- `2009.02267v1` - [abs](http://arxiv.org/abs/2009.02267v1) - [pdf](http://arxiv.org/pdf/2009.02267v1)

> The Internet of Things (IoT) provides applications and services that would otherwise not be possible. However, the open nature of IoT make it vulnerable to cybersecurity threats. Especially, identity spoofing attacks, where an adversary passively listens to existing radio communications and then mimic the identity of legitimate devices to conduct malicious activities. Existing solutions employ cryptographic signatures to verify the trustworthiness of received information. In prevalent IoT, secret keys for cryptography can potentially be disclosed and disable the verification mechanism. Non-cryptographic device verification is needed to ensure trustworthy IoT. In this paper, we propose an enhanced deep learning framework for IoT device identification using physical layer signals. Specifically, we enable our framework to report unseen IoT devices and introduce the zero-bias layer to deep neural networks to increase robustness and interpretability. We have evaluated the effectiveness of the proposed framework using real data from ADS-B (Automatic Dependent Surveillance-Broadcast), an application of IoT in aviation. The proposed framework has the potential to be applied to accurate identification of IoT devices in a variety of IoT applications and services. Codes and data are available in IEEE Dataport.

</details>

<details>

<summary>2020-08-28 21:51:53 - Multi-Model Resilient Observer under False Data Injection Attacks</summary>

- *Olugbenga Moses Anubi, Charalambos Konstantinou, Carlos A. Wong, Satish Vedula*

- `2008.12859v1` - [abs](http://arxiv.org/abs/2008.12859v1) - [pdf](http://arxiv.org/pdf/2008.12859v1)

> In this paper, we present the concept of boosting the resiliency of optimization-based observers for cyber-physical systems (CPS) using auxiliary sources of information. Due to the tight coupling of physics, communication and computation, a malicious agent can exploit multiple inherent vulnerabilities in order to inject stealthy signals into the measurement process. The problem setting considers the scenario in which an attacker strategically corrupts portions of the data in order to force wrong state estimates which could have catastrophic consequences. The goal of the proposed observer is to compute the true states in-spite of the adversarial corruption. In the formulation, we use a measurement prior distribution generated by the auxiliary model to refine the feasible region of a traditional compressive sensing-based regression problem. A constrained optimization-based observer is developed using l1-minimization scheme. Numerical experiments show that the solution of the resulting problem recovers the true states of the system. The developed algorithm is evaluated through a numerical simulation example of the IEEE 14-bus system.

</details>

<details>

<summary>2020-08-30 16:45:07 - Reinforcement Learning Based Penetration Testing of a Microgrid Control Algorithm</summary>

- *Christopher Neal, Hanane Dagdougui, Andrea Lodi, José Fernandez*

- `2008.13212v1` - [abs](http://arxiv.org/abs/2008.13212v1) - [pdf](http://arxiv.org/pdf/2008.13212v1)

> Microgrids (MGs) are small-scale power systems which interconnect distributed energy resources and loads within clearly defined regions. However, the digital infrastructure used in an MG to relay sensory information and perform control commands can potentially be compromised due to a cyberattack from a capable adversary. An MG operator is interested in knowing the inherent vulnerabilities in their system and should regularly perform Penetration Testing (PT) activities to prepare for such an event. PT generally involves looking for defensive coverage blindspots in software and hardware infrastructure, however the logic in control algorithms which act upon sensory information should also be considered in PT activities. This paper demonstrates a case study of PT for an MG control algorithm by using Reinforcement Learning (RL) to uncover malicious input which compromises the effectiveness of the controller. Through trial-and-error episodic interactions with a simulated MG, we train an RL agent to find malicious input which reduces the effectiveness of the MG controller.

</details>

<details>

<summary>2020-08-30 19:30:35 - SearchFromFree: Adversarial Measurements for Machine Learning-based Energy Theft Detection</summary>

- *Jiangnan Li, Yingyuan Yang, Jinyuan Stella Sun*

- `2006.03504v2` - [abs](http://arxiv.org/abs/2006.03504v2) - [pdf](http://arxiv.org/pdf/2006.03504v2)

> Energy theft causes large economic losses to utility companies around the world. In recent years, energy theft detection approaches based on machine learning (ML) techniques, especially neural networks, become popular in the research literature and achieve state-of-the-art detection performance. However, in this work, we demonstrate that the well-perform ML models for energy theft detection are highly vulnerable to adversarial attacks. In particular, we design an adversarial measurement generation algorithm that enables the attacker to report extremely low power consumption measurements to the utilities while bypassing the ML energy theft detection. We evaluate our approach with three kinds of neural networks based on a real-world smart meter dataset. The evaluation result demonstrates that our approach can significantly decrease the ML models' detection accuracy, even for black-box attackers.

</details>

<details>

<summary>2020-08-30 20:03:35 - Benchmarking adversarial attacks and defenses for time-series data</summary>

- *Shoaib Ahmed Siddiqui, Andreas Dengel, Sheraz Ahmed*

- `2008.13261v1` - [abs](http://arxiv.org/abs/2008.13261v1) - [pdf](http://arxiv.org/pdf/2008.13261v1)

> The adversarial vulnerability of deep networks has spurred the interest of researchers worldwide. Unsurprisingly, like images, adversarial examples also translate to time-series data as they are an inherent weakness of the model itself rather than the modality. Several attempts have been made to defend against these adversarial attacks, particularly for the visual modality. In this paper, we perform detailed benchmarking of well-proven adversarial defense methodologies on time-series data. We restrict ourselves to the $L_{\infty}$ threat model. We also explore the trade-off between smoothness and clean accuracy for regularization-based defenses to better understand the trade-offs that they offer. Our analysis shows that the explored adversarial defenses offer robustness against both strong white-box as well as black-box attacks. This paves the way for future research in the direction of adversarial attacks and defenses, particularly for time-series data.

</details>

<details>

<summary>2020-08-31 02:12:02 - Share Withholding Attack in Blockchain Mining: Technical Report</summary>

- *Sang-Yoon Chang*

- `2008.13317v1` - [abs](http://arxiv.org/abs/2008.13317v1) - [pdf](http://arxiv.org/pdf/2008.13317v1)

> Cryptocurrency achieves distributed consensus using proof of work (PoW). Prior research in blockchain security identified financially incentivized attacks based on withholding blocks which have the attacker compromise a victim pool and pose as a PoW contributor by submitting the shares (earning credit for mining) but withholding the blocks (no actual contributions to the pool). We advance such threats to generate greater reward advantage to the attackers while undermining the other miners and introduce the share withholding attack (SWH). SWH withholds shares to increase the attacker's reward payout within the pool, in contrast to the prior threats withholding blocks, and rather builds on the block-withholding threats in order to exploit the information about the impending block submission timing, challenging the popularly established assumption that the block submission time is completely random and unknown to miners. We analyze SWH's incentive compatibility and the vulnerability scope by identifying the critical systems and environmental parameters which determine the attack's impact. Our results show that SWH in conjunction with block withholding yield unfair reward advantage at the expense of the protocol-complying victim miners and that a rational miner will selfishly launch SWH to maximize its reward profit. We inform the blockchain and cryptocurrency research of the novel SWH threat and include the potential countermeasure directions to facilitate such research and development.

</details>

<details>

<summary>2020-08-31 04:14:46 - Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization</summary>

- *Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, Xinpeng Zhang*

- `1909.02742v3` - [abs](http://arxiv.org/abs/1909.02742v3) - [pdf](http://arxiv.org/pdf/1909.02742v3)

> Deep neural networks (DNNs) have been proven vulnerable to backdoor attacks, where hidden features (patterns) trained to a normal model, which is only activated by some specific input (called triggers), trick the model into producing unexpected behavior. In this paper, we create covert and scattered triggers for backdoor attacks, invisible backdoors, where triggers can fool both DNN models and human inspection. We apply our invisible backdoors through two state-of-the-art methods of embedding triggers for backdoor attacks. The first approach on Badnets embeds the trigger into DNNs through steganography. The second approach of a trojan attack uses two types of additional regularization terms to generate the triggers with irregular shape and size. We use the Attack Success Rate and Functionality to measure the performance of our attacks. We introduce two novel definitions of invisibility for human perception; one is conceptualized by the Perceptual Adversarial Similarity Score (PASS) and the other is Learned Perceptual Image Patch Similarity (LPIPS). We show that the proposed invisible backdoors can be fairly effective across various DNN models as well as four datasets MNIST, CIFAR-10, CIFAR-100, and GTSRB, by measuring their attack success rates for the adversary, functionality for the normal users, and invisibility scores for the administrators. We finally argue that the proposed invisible backdoor attacks can effectively thwart the state-of-the-art trojan backdoor detection approaches, such as Neural Cleanse and TABOR.

</details>

<details>

<summary>2020-08-31 09:29:22 - Scalable Online Vetting of Android Apps for Measuring Declared SDK Versions and Their Consistency with API Calls</summary>

- *Daoyuan Wu, Debin Gao, David Lo*

- `1912.12982v3` - [abs](http://arxiv.org/abs/1912.12982v3) - [pdf](http://arxiv.org/pdf/1912.12982v3)

> Android has been the most popular smartphone system with multiple platform versions active in the market. To manage the application's compatibility with one or more platform versions, Android allows apps to declare the supported platform SDK versions in their manifest files. In this paper, we conduct a systematic study of this modern software mechanism. Our objective is to measure the current practice of declared SDK versions (which we term as DSDK versions afterwards) in real apps, and the (in)consistency between DSDK versions and their host apps' API calls. To successfully analyze a modern dataset of 22,687 popular apps (with an average app size of 25MB), we design a scalable approach that operates on the Android bytecode level and employs a lightweight bytecode search for app analysis. This approach achieves a good performance suitable for online vetting in app markets, requiring only around 5 seconds to process an app on average. Besides shedding light on the characteristics of DSDK in the wild, our study quantitatively measures two side effects of inappropriate DSDK versions: (i) around 35% apps under-set the minimum DSDK versions and could incur runtime crashes, but fortunately, only 11.3% apps could crash on Android 6.0 and above; (ii) around 2% apps, due to under-claiming the targeted DSDK versions, are potentially exploitable by remote code execution, and half of them invoke the vulnerable API via embedded third-party libraries. These results indicate the importance and difficulty of declaring correct DSDK, and our work can help developers fulfill this goal.

</details>


## 2020-09

<details>

<summary>2020-09-01 21:25:04 - When the Differences in Frequency Domain are Compensated: Understanding and Defeating Modulated Replay Attacks on Automatic Speech Recognition</summary>

- *Shu Wang, Jiahao Cao, Xu He, Kun Sun, Qi Li*

- `2009.00718v1` - [abs](http://arxiv.org/abs/2009.00718v1) - [pdf](http://arxiv.org/pdf/2009.00718v1)

> Automatic speech recognition (ASR) systems have been widely deployed in modern smart devices to provide convenient and diverse voice-controlled services. Since ASR systems are vulnerable to audio replay attacks that can spoof and mislead ASR systems, a number of defense systems have been proposed to identify replayed audio signals based on the speakers' unique acoustic features in the frequency domain. In this paper, we uncover a new type of replay attack called modulated replay attack, which can bypass the existing frequency domain based defense systems. The basic idea is to compensate for the frequency distortion of a given electronic speaker using an inverse filter that is customized to the speaker's transform characteristics. Our experiments on real smart devices confirm the modulated replay attacks can successfully escape the existing detection mechanisms that rely on identifying suspicious features in the frequency domain. To defeat modulated replay attacks, we design and implement a countermeasure named DualGuard. We discover and formally prove that no matter how the replay audio signals could be modulated, the replay attacks will either leave ringing artifacts in the time domain or cause spectrum distortion in the frequency domain. Therefore, by jointly checking suspicious features in both frequency and time domains, DualGuard can successfully detect various replay attacks including the modulated replay attacks. We implement a prototype of DualGuard on a popular voice interactive platform, ReSpeaker Core v2. The experimental results show DualGuard can achieve 98% accuracy on detecting modulated replay attacks.

</details>

<details>

<summary>2020-09-01 22:24:44 - What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge</summary>

- *Kyle Richardson, Ashish Sabharwal*

- `1912.13337v2` - [abs](http://arxiv.org/abs/1912.13337v2) - [pdf](http://arxiv.org/pdf/1912.13337v2)

> Open-domain question answering (QA) is known to involve several underlying knowledge and reasoning challenges, but are models actually learning such knowledge when trained on benchmark tasks? To investigate this, we introduce several new challenge tasks that probe whether state-of-the-art QA models have general knowledge about word definitions and general taxonomic reasoning, both of which are fundamental to more complex forms of reasoning and are widespread in benchmark datasets. As an alternative to expensive crowd-sourcing, we introduce a methodology for automatically building datasets from various types of expert knowledge (e.g., knowledge graphs and lexical taxonomies), allowing for systematic control over the resulting probes and for a more comprehensive evaluation. We find automatically constructing probes to be vulnerable to annotation artifacts, which we carefully control for. Our evaluation confirms that transformer-based QA models are already predisposed to recognize certain types of structural lexical knowledge. However, it also reveals a more nuanced picture: their performance degrades substantially with even a slight increase in the number of hops in the underlying taxonomic hierarchy, or as more challenging distractor candidate answers are introduced. Further, even when these models succeed at the standard instance-level evaluation, they leave much room for improvement when assessed at the level of clusters of semantically connected probes (e.g., all Isa questions about a concept).

</details>

<details>

<summary>2020-09-02 04:35:33 - Open-set Adversarial Defense</summary>

- *Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel*

- `2009.00814v1` - [abs](http://arxiv.org/abs/2009.00814v1) - [pdf](http://arxiv.org/pdf/2009.00814v1)

> Open-set recognition and adversarial defense study two key aspects of deep learning that are vital for real-world deployment. The objective of open-set recognition is to identify samples from open-set classes during testing, while adversarial defense aims to defend the network against images with imperceptible adversarial perturbations. In this paper, we show that open-set recognition systems are vulnerable to adversarial attacks. Furthermore, we show that adversarial defense mechanisms trained on known classes do not generalize well to open-set samples. Motivated by this observation, we emphasize the need of an Open-Set Adversarial Defense (OSAD) mechanism. This paper proposes an Open-Set Defense Network (OSDN) as a solution to the OSAD problem. The proposed network uses an encoder with feature-denoising layers coupled with a classifier to learn a noise-free latent feature representation. Two techniques are employed to obtain an informative latent feature space with the objective of improving open-set performance. First, a decoder is used to ensure that clean images can be reconstructed from the obtained latent features. Then, self-supervision is used to ensure that the latent features are informative enough to carry out an auxiliary task. We introduce a testing protocol to evaluate OSAD performance and show the effectiveness of the proposed method in multiple object classification datasets. The implementation code of the proposed method is available at: https://github.com/rshaojimmy/ECCV2020-OSAD.

</details>

<details>

<summary>2020-09-02 14:41:25 - Robustness to Programmable String Transformations via Augmented Abstract Training</summary>

- *Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni*

- `2002.09579v4` - [abs](http://arxiv.org/abs/2002.09579v4) - [pdf](http://arxiv.org/pdf/2002.09579v4)

> Deep neural networks for natural language processing tasks are vulnerable to adversarial input perturbations. In this paper, we present a versatile language for programmatically specifying string transformations -- e.g., insertions, deletions, substitutions, swaps, etc. -- that are relevant to the task at hand. We then present an approach to adversarially training models that are robust to such user-defined string transformations. Our approach combines the advantages of search-based techniques for adversarial training with abstraction-based techniques. Specifically, we show how to decompose a set of user-defined string transformations into two component specifications, one that benefits from search and another from abstraction. We use our technique to train models on the AG and SST2 datasets and show that the resulting models are robust to combinations of user-defined transformations mimicking spelling mistakes and other meaning-preserving transformations.

</details>

<details>

<summary>2020-09-02 15:00:41 - Flow-based detection and proxy-based evasion of encrypted malware C2 traffic</summary>

- *Carlos Novo, Ricardo Morla*

- `2009.01122v1` - [abs](http://arxiv.org/abs/2009.01122v1) - [pdf](http://arxiv.org/pdf/2009.01122v1)

> State of the art deep learning techniques are known to be vulnerable to evasion attacks where an adversarial sample is generated from a malign sample and misclassified as benign. Detection of encrypted malware command and control traffic based on TCP/IP flow features can be framed as a learning task and is thus vulnerable to evasion attacks. However, unlike e.g. in image processing where generated adversarial samples can be directly mapped to images, going from flow features to actual TCP/IP packets requires crafting the sequence of packets, with no established approach for such crafting and a limitation on the set of modifiable features that such crafting allows. In this paper we discuss learning and evasion consequences of the gap between generated and crafted adversarial samples. We exemplify with a deep neural network detector trained on a public C2 traffic dataset, white-box adversarial learning, and a proxy-based approach for crafting longer flows. Our results show 1) the high evasion rate obtained by using generated adversarial samples on the detector can be significantly reduced when using crafted adversarial samples; 2) robustness against adversarial samples by model hardening varies according to the crafting approach and corresponding set of modifiable features that the attack allows for; 3) incrementally training hardened models with adversarial samples can produce a level playing field where no detector is best against all attacks and no attack is best against all detectors, in a given set of attacks and detectors. To the best of our knowledge this is the first time that level playing field feature set- and iteration-hardening are analyzed in encrypted C2 malware traffic detection.

</details>

<details>

<summary>2020-09-02 15:03:14 - Yet Meta Learning Can Adapt Fast, It Can Also Break Easily</summary>

- *Han Xu, Yaxin Li, Xiaorui Liu, Hui Liu, Jiliang Tang*

- `2009.01672v1` - [abs](http://arxiv.org/abs/2009.01672v1) - [pdf](http://arxiv.org/pdf/2009.01672v1)

> Meta learning algorithms have been widely applied in many tasks for efficient learning, such as few-shot image classification and fast reinforcement learning. During meta training, the meta learner develops a common learning strategy, or experience, from a variety of learning tasks. Therefore, during meta test, the meta learner can use the learned strategy to quickly adapt to new tasks even with a few training samples. However, there is still a dark side about meta learning in terms of reliability and robustness. In particular, is meta learning vulnerable to adversarial attacks? In other words, would a well-trained meta learner utilize its learned experience to build wrong or likely useless knowledge, if an adversary unnoticeably manipulates the given training set? Without the understanding of this problem, it is extremely risky to apply meta learning in safety-critical applications. Thus, in this paper, we perform the initial study about adversarial attacks on meta learning under the few-shot classification problem. In particular, we formally define key elements of adversarial attacks unique to meta learning and propose the first attacking algorithm against meta learning under various settings. We evaluate the effectiveness of the proposed attacking strategy as well as the robustness of several representative meta learning algorithms. Experimental results demonstrate that the proposed attacking strategy can easily break the meta learner and meta learning is vulnerable to adversarial attacks. The implementation of the proposed framework will be released upon the acceptance of this paper.

</details>

<details>

<summary>2020-09-03 14:15:46 - The Sound of Silence: Mining Security Vulnerabilities from Secret Integration Channels in Open-Source Projects</summary>

- *Ralf Ramsauer, Lukas Bulwahn, Daniel Lohmann, Wolfgang Mauerer*

- `2009.01694v1` - [abs](http://arxiv.org/abs/2009.01694v1) - [pdf](http://arxiv.org/pdf/2009.01694v1)

> Public development processes are a key characteristic of open source projects. However, fixes for vulnerabilities are usually discussed privately among a small group of trusted maintainers, and integrated without prior public involvement. This is supposed to prevent early disclosure, and cope with embargo and non-disclosure agreement (NDA) rules. While regular development activities leave publicly available traces, fixes for vulnerabilities that bypass the standard process do not.   We present a data-mining based approach to detect code fragments that arise from such infringements of the standard process. By systematically mapping public development artefacts to source code repositories, we can exclude regular process activities, and infer irregularities that stem from non-public integration channels. For the Linux kernel, the most crucial component of many systems, we apply our method to a period of seven months before the release of Linux 5.4. We find 29 commits that address 12 vulnerabilities. For these vulnerabilities, our approach provides a temporal advantage of 2 to 179 days to design exploits before public disclosure takes place, and fixes are rolled out.   Established responsible disclosure approaches in open development processes are supposed to limit premature visibility of security vulnerabilities. However, our approach shows that, instead, they open additional possibilities to uncover such changes that thwart the very premise. We conclude by discussing implications and partial countermeasures.

</details>

<details>

<summary>2020-09-03 14:45:11 - Deep Learning based Vulnerability Detection: Are We There Yet?</summary>

- *Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, Baishakhi Ray*

- `2009.07235v1` - [abs](http://arxiv.org/abs/2009.07235v1) - [pdf](http://arxiv.org/pdf/2009.07235v1)

> Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask, "how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario?". To our surprise, we find that their performance drops by more than 50%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline: up to 33.57% boost in precision and 128.38% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://git.io/Jf6IA.

</details>

<details>

<summary>2020-09-03 18:16:06 - Developing Enterprise Cyber Situational Awareness</summary>

- *Christopher L Gorham*

- `2009.01864v1` - [abs](http://arxiv.org/abs/2009.01864v1) - [pdf](http://arxiv.org/pdf/2009.01864v1)

> The topic will focus on the U.S. Department of Defense strategy towards improving their network security defenses for the department and the steps they have taken at the agency level where components under DOD such as The Defense Information Systems Agency are working towards adding tools that provides additional capabilities in the cyber space. This approach will be analyzed to determine if DOD goals address any of their vulnerabilities towards protecting their networks. One of the agencies under the DOD umbrella called The Defense Information Systems Agency provides DOD a template on how to build a network that relies upon layers of security to help it combat cyber attacks against its network. Whether that provides an effective solution to DOD remains a question due to the many components that operate under its direction. Managing these networks is the principle responsibilities for the Department of Defense. Nevertheless, it does demonstrates that there are tools available to help DOD build an strong enterprise cyber network of situational awareness that strengthens the ability to protect their network infrastructure.

</details>

<details>

<summary>2020-09-03 20:31:16 - Attack Graph Convolutional Networks by Adding Fake Nodes</summary>

- *Xiaoyun Wang, Minhao Cheng, Joe Eaton, Cho-Jui Hsieh, Felix Wu*

- `1810.10751v4` - [abs](http://arxiv.org/abs/1810.10751v4) - [pdf](http://arxiv.org/pdf/1810.10751v4)

> In this paper, we study the robustness of graph convolutional networks (GCNs). Previous work have shown that GCNs are vulnerable to adversarial perturbation on adjacency or feature matrices of existing nodes; however, such attacks are usually unrealistic in real applications. For instance, in social network applications, the attacker will need to hack into either the client or server to change existing links or features. In this paper, we propose a new type of "fake node attacks" to attack GCNs by adding malicious fake nodes. This is much more realistic than previous attacks; in social network applications, the attacker only needs to register a set of fake accounts and link to existing ones. To conduct fake node attacks, a greedy algorithm is proposed to generate edges of malicious nodes and their corresponding features aiming to minimize the classification accuracy on the target nodes. In addition, we introduce a discriminator to classify malicious nodes from real nodes, and propose a Greedy-GAN attack to simultaneously update the discriminator and the attacker, to make malicious nodes indistinguishable from the real ones. Our non-targeted attack decreases the accuracy of GCN down to 0.03, and our targeted attack reaches a success rate of 78% on a group of 100 nodes, and 90% on average for attacking a single target node.

</details>

<details>

<summary>2020-09-05 18:17:34 - RayS: A Ray Searching Method for Hard-label Adversarial Attack</summary>

- *Jinghui Chen, Quanquan Gu*

- `2006.12792v2` - [abs](http://arxiv.org/abs/2006.12792v2) - [pdf](http://arxiv.org/pdf/2006.12792v2)

> Deep neural networks are vulnerable to adversarial attacks. Among different attack settings, the most challenging yet the most practical one is the hard-label setting where the attacker only has access to the hard-label output (prediction label) of the target model. Previous attempts are neither effective enough in terms of attack success rate nor efficient enough in terms of query complexity under the widely used $L_\infty$ norm threat model. In this paper, we present the Ray Searching attack (RayS), which greatly improves the hard-label attack effectiveness as well as efficiency. Unlike previous works, we reformulate the continuous problem of finding the closest decision boundary into a discrete problem that does not require any zeroth-order gradient estimation. In the meantime, all unnecessary searches are eliminated via a fast check step. This significantly reduces the number of queries needed for our hard-label attack. Moreover, interestingly, we found that the proposed RayS attack can also be used as a sanity check for possible "falsely robust" models. On several recently proposed defenses that claim to achieve the state-of-the-art robust accuracy, our attack method demonstrates that the current white-box/black-box attacks could still give a false sense of security and the robust accuracy drop between the most popular PGD attack and RayS attack could be as large as $28\%$. We believe that our proposed RayS attack could help identify falsely robust models that beat most white-box/black-box attacks.

</details>

<details>

<summary>2020-09-05 21:54:53 - Security Survey and Analysis of Vote-by-Mail Systems</summary>

- *Jenny Blessing, Julian Gomez, McCoy Patiño, Tran Nguyen*

- `2005.08427v2` - [abs](http://arxiv.org/abs/2005.08427v2) - [pdf](http://arxiv.org/pdf/2005.08427v2)

> Voting by mail has been gaining traction for decades in the United States and has emerged as the preferred voting method during the COVID-19 pandemic. In this paper, we examine the security of electronic systems used in the process of voting by mail, including online voter registration and online ballot tracking systems. The goals of these systems, to facilitate voter registration and increase public confidence in elections, are laudable. They indisputably provide a critical public good. It is for these reasons that understanding the security and privacy posture of the mail-in voting process is paramount. We find that online voter registration systems in some states have vulnerabilities that allow adversaries to alter or effectively prevent a voter's registration. We additionally find that ballot tracking systems raise serious privacy questions surrounding ease of access to voter data. While the vulnerabilities discussed here are unlikely to enable an adversary to modify votes, several could have the effect of disenfranchising voters and reducing voter confidence in U.S. elections infrastructure, thereby undermining the very purpose of these systems.

</details>

<details>

<summary>2020-09-06 13:57:17 - Detection Defense Against Adversarial Attacks with Saliency Map</summary>

- *Dengpan Ye, Chuanxi Chen, Changrui Liu, Hao Wang, Shunzhi Jiang*

- `2009.02738v1` - [abs](http://arxiv.org/abs/2009.02738v1) - [pdf](http://arxiv.org/pdf/2009.02738v1)

> It is well established that neural networks are vulnerable to adversarial examples, which are almost imperceptible on human vision and can cause the deep models misbehave. Such phenomenon may lead to severely inestimable consequences in the safety and security critical applications. Existing defenses are trend to harden the robustness of models against adversarial attacks, e.g., adversarial training technology. However, these are usually intractable to implement due to the high cost of re-training and the cumbersome operations of altering the model architecture or parameters. In this paper, we discuss the saliency map method from the view of enhancing model interpretability, it is similar to introducing the mechanism of the attention to the model, so as to comprehend the progress of object identification by the deep networks. We then propose a novel method combined with additional noises and utilize the inconsistency strategy to detect adversarial examples. Our experimental results of some representative adversarial attacks on common datasets including ImageNet and popular models show that our method can detect all the attacks with high detection success rate effectively. We compare it with the existing state-of-the-art technique, and the experiments indicate that our method is more general.

</details>

<details>

<summary>2020-09-08 02:38:11 - Bluff: Interactively Deciphering Adversarial Attacks on Deep Neural Networks</summary>

- *Nilaksh Das, Haekyu Park, Zijie J. Wang, Fred Hohman, Robert Firstman, Emily Rogers, Duen Horng Chau*

- `2009.02608v2` - [abs](http://arxiv.org/abs/2009.02608v2) - [pdf](http://arxiv.org/pdf/2009.02608v2)

> Deep neural networks (DNNs) are now commonly used in many domains. However, they are vulnerable to adversarial attacks: carefully crafted perturbations on data inputs that can fool a model into making incorrect predictions. Despite significant research on developing DNN attack and defense techniques, people still lack an understanding of how such attacks penetrate a model's internals. We present Bluff, an interactive system for visualizing, characterizing, and deciphering adversarial attacks on vision-based neural networks. Bluff allows people to flexibly visualize and compare the activation pathways for benign and attacked images, revealing mechanisms that adversarial attacks employ to inflict harm on a model. Bluff is open-sourced and runs in modern web browsers.

</details>

<details>

<summary>2020-09-08 19:20:02 - Technical Report: Gone in 20 Seconds -- Overview of a Password Vulnerability in Siemens HMIs</summary>

- *Joseph Gardiner, Awais Rashid*

- `2009.03961v1` - [abs](http://arxiv.org/abs/2009.03961v1) - [pdf](http://arxiv.org/pdf/2009.03961v1)

> Siemens produce a range of industrial human machine interface (HMI) screens which allow operators to both view information about and control physical processes. For scenarios where an operator cannot physically access the screen, Siemens provide the SM@rtServer features on HMIs, which when activated provides remote access either through their own Sm@rtClient application, or through third party VNC client software. Through analysing this server, we discovered a lack of protection against brute-force password attacks on basic devices. On advanced devices which include a brute-force protection mechanism, we discovered an attacker strategy that is able to evade the mechanism allowing for unlimited password guess attempts with minimal effect on the guess rate. This vulnerability has been assigned two CVEs - CVE-2020-15786 and CVE-2020-157867. In this report, we provide an overview of this vulnerability, discuss the impact of a successful exploitation and propose mitigations to provide protection against this vulnerability. This report accompanies a demo presented at CPSIoTSec 2020.

</details>

<details>

<summary>2020-09-09 01:41:57 - Going Deep: Using deep learning techniques with simplified mathematical models against XOR BR and TBR PUFs (Attacks and Countermeasures)</summary>

- *Mahmoud Khalafalla, Mahmoud A. Elmohr, Catherine Gebotys*

- `2009.04063v1` - [abs](http://arxiv.org/abs/2009.04063v1) - [pdf](http://arxiv.org/pdf/2009.04063v1)

> This paper contributes to the study of PUFs vulnerability against modeling attacks by evaluating the security of XOR BR PUFs, XOR TBR PUFs, and obfuscated architectures of XOR BR PUF using a simplified mathematical model and deep learning (DL) techniques. Obtained results show that DL modeling attacks could easily break the security of 4-input XOR BR PUFs and 4-input XOR TBR PUFs with modeling accuracy $\sim$ 99%. Similar attacks were executed using single-layer neural networks (NN) and support vector machines (SVM) with polynomial kernel and the obtained results showed that single NNs failed to break the PUF security. Furthermore, SVM results confirmed the same modeling accuracy reported in previous research ($\sim$ 50%). For the first time, this research empirically shows that DL networks can be used as powerful modeling techniques against these complex PUF architectures for which previous conventional machine learning techniques had failed. Furthermore, a detailed scalability analysis is conducted on the DL networks with respect to PUFs' stage size and complexity. The analysis shows that the number of layers and hidden neurons inside every layer has a linear relationship with PUFs' stage size, which agrees with the theoretical findings in deep learning. Consequently, A new obfuscated architecture is introduced as a first step to counter DL modeling attacks and it showed significant resistance against such attacks (16% - 40% less accuracy). This research provides an important step towards prioritizing the efforts to introduce new PUF architectures that are more secure and invulnerable to modeling attacks. Moreover, it triggers future discussions on the removal of influential bits and the level of obfuscation needed to confirm that a specific PUF architecture is resistant against powerful DL modeling attacks.

</details>

<details>

<summary>2020-09-09 07:49:01 - Individual Calibration with Randomized Forecasting</summary>

- *Shengjia Zhao, Tengyu Ma, Stefano Ermon*

- `2006.10288v3` - [abs](http://arxiv.org/abs/2006.10288v3) - [pdf](http://arxiv.org/pdf/2006.10288v3)

> Machine learning applications often require calibrated predictions, e.g. a 90\% credible interval should contain the true outcome 90\% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. We show that calibration for individual samples is possible in the regression setup if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.

</details>

<details>

<summary>2020-09-09 13:48:28 - Hidden in Plain Sight: Obfuscated Strings Threatening Your Privacy</summary>

- *Leonid Glanz, Patrick Müller, Lars Baumgärtner, Michael Reif, Sven Amann, Pauline Anthonysamy, Mira Mezini*

- `2002.04540v2` - [abs](http://arxiv.org/abs/2002.04540v2) - [pdf](http://arxiv.org/pdf/2002.04540v2)

> String obfuscation is an established technique used by proprietary, closed-source applications to protect intellectual property. Furthermore, it is also frequently used to hide spyware or malware in applications. In both cases, the techniques range from bit-manipulation over XOR operations to AES encryption. However, string obfuscation techniques/tools suffer from one shared weakness: They generally have to embed the necessary logic to deobfuscate strings into the app code.   In this paper, we show that most of the string obfuscation techniques found in malicious and benign applications for Android can easily be broken in an automated fashion. We developed StringHound, an open-source tool that uses novel techniques that identify obfuscated strings and reconstruct the originals using slicing.   We evaluated StringHound on both benign and malicious Android apps. In summary, we deobfuscate almost 30 times more obfuscated strings than other string deobfuscation tools. Additionally, we analyzed 100,000 Google Play Store apps and found multiple obfuscated strings that hide vulnerable cryptographic usages, insecure internet accesses, API keys, hard-coded passwords, and exploitation of privileges without the awareness of the developer. Furthermore, our analysis reveals that not only malware uses string obfuscation but also benign apps make extensive use of string obfuscation.

</details>

<details>

<summary>2020-09-10 01:58:06 - TRIER: Template-Guided Neural Networks for Robust and Interpretable Sleep Stage Identification from EEG Recordings</summary>

- *Taeheon Lee, Jeonghwan Hwang, Honggu Lee*

- `2009.05407v1` - [abs](http://arxiv.org/abs/2009.05407v1) - [pdf](http://arxiv.org/pdf/2009.05407v1)

> Neural networks often obtain sub-optimal representations during training, which degrade robustness as well as classification performances. This is a severe problem in applying deep learning to bio-medical domains, since models are vulnerable to being harmed by irregularities and scarcities in data. In this study, we propose a pre-training technique that handles this challenge in sleep staging tasks. Inspired by conventional methods that experienced physicians have used to classify sleep states from the existence of characteristic waveform shapes, or template patterns, our method introduces a cosine similarity based convolutional neural network to extract representative waveforms from training data. Afterwards, these features guide a model to construct representations based on template patterns. Through extensive experiments, we demonstrated that guiding a neural network with template patterns is an effective approach for sleep staging, since (1) classification performances are significantly enhanced and (2) robustness in several aspects are improved. Last but not least, interpretations on models showed that notable features exploited by trained experts are correctly addressed during prediction in the proposed method.

</details>

<details>

<summary>2020-09-10 14:14:22 - Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning</summary>

- *Yang Zou, Zhikun Zhang, Michael Backes, Yang Zhang*

- `2009.04872v1` - [abs](http://arxiv.org/abs/2009.04872v1) - [pdf](http://arxiv.org/pdf/2009.04872v1)

> While being deployed in many critical applications as core components, machine learning (ML) models are vulnerable to various security and privacy attacks. One major privacy attack in this domain is membership inference, where an adversary aims to determine whether a target data sample is part of the training set of a target ML model. So far, most of the current membership inference attacks are evaluated against ML models trained from scratch. However, real-world ML models are typically trained following the transfer learning paradigm, where a model owner takes a pretrained model learned from a different dataset, namely teacher model, and trains her own student model by fine-tuning the teacher model with her own data.   In this paper, we perform the first systematic evaluation of membership inference attacks against transfer learning models. We adopt the strategy of shadow model training to derive the data for training our membership inference classifier. Extensive experiments on four real-world image datasets show that membership inference can achieve effective performance. For instance, on the CIFAR100 classifier transferred from ResNet20 (pretrained with Caltech101), our membership inference achieves $95\%$ attack AUC. Moreover, we show that membership inference is still effective when the architecture of target model is unknown. Our results shed light on the severity of membership risks stemming from machine learning models in practice.

</details>

<details>

<summary>2020-09-10 14:55:42 - Cyberattacks on Miniature Brain Implants to Disrupt Spontaneous Neural Signaling</summary>

- *Sergio López Bernal, Alberto Huertas Celdrán, Lorenzo Fernández Maimó, Michael Taynnan Barros, Sasitharan Balasubramaniam, Gregorio Martínez Pérez*

- `2007.09466v2` - [abs](http://arxiv.org/abs/2007.09466v2) - [pdf](http://arxiv.org/pdf/2007.09466v2)

> Brain-Computer Interfaces (BCI) arose as systems that merge computing systems with the human brain to facilitate recording, stimulation, and inhibition of neural activity. Over the years, the development of BCI technologies has shifted towards miniaturization of devices that can be seamlessly embedded into the brain and can target single neuron or small population sensing and control. We present a motivating example highlighting vulnerabilities of two promising micron-scale BCI technologies, demonstrating the lack of security and privacy principles in existing solutions. This situation opens the door to a novel family of cyberattacks, called neuronal cyberattacks, affecting neuronal signaling. This paper defines the first two neural cyberattacks, Neuronal Flooding (FLO) and Neuronal Scanning (SCA), where each threat can affect the natural activity of neurons. This work implements these attacks in a neuronal simulator to determine their impact over the spontaneous neuronal behavior, defining three metrics: number of spikes, percentage of shifts, and dispersion of spikes. Several experiments demonstrate that both cyberattacks produce a reduction of spikes compared to spontaneous behavior, generating a rise in temporal shifts and a dispersion increase. Mainly, SCA presents a higher impact than FLO in the metrics focused on the number of spikes and dispersion, where FLO is slightly more damaging, considering the percentage of shifts. Nevertheless, the intrinsic behavior of each attack generates a differentiation on how they alter neuronal signaling. FLO is adequate to generate an immediate impact on the neuronal activity, whereas SCA presents higher effectiveness for damages to the neural signaling in the long-term.

</details>

<details>

<summary>2020-09-10 17:41:24 - Fault-Tolerant Edge-Disjoint Paths -- Beyond Uniform Faults</summary>

- *David Adjiashvili, Felix Hommelsheim, Moritz Mühlenthaler, Oliver Schaudt*

- `2009.05382v1` - [abs](http://arxiv.org/abs/2009.05382v1) - [pdf](http://arxiv.org/pdf/2009.05382v1)

> The overwhelming majority of survivable (fault-tolerant) network design models assume a uniform fault model. Such a model assumes that every subset of the network resources (edges or vertices) of a given cardinality $k$ may fail. While this approach yields problems with clean combinatorial structure and good algorithms, it often fails to capture the true nature of the scenario set coming from applications. One natural refinement of the uniform model is obtained by partitioning the set of resources into vulnerable and safe resources. The scenario set contains every subset of at most $k$ faulty resources. This work studies the Fault-Tolerant Path (FTP) problem, the counterpart of the Shortest Path problem in this fault model and the Fault-Tolerant Flow problem (FTF), the counterpart of the $\ell$-disjoint Shortest $s$-$t$ Path problem. We present complexity results alongside exact and approximation algorithms for both models. We emphasize the vast increase in the complexity of the problem with respect to the uniform analogue, the Edge-Disjoint Paths problem.

</details>

<details>

<summary>2020-09-11 02:03:46 - Multitask Learning Strengthens Adversarial Robustness</summary>

- *Chengzhi Mao, Amogh Gupta, Vikram Nitin, Baishakhi Ray, Shuran Song, Junfeng Yang, Carl Vondrick*

- `2007.07236v2` - [abs](http://arxiv.org/abs/2007.07236v2) - [pdf](http://arxiv.org/pdf/2007.07236v2)

> Although deep networks achieve strong accuracy on a range of computer vision benchmarks, they remain vulnerable to adversarial attacks, where imperceptible input perturbations fool the network. We present both theoretical and empirical analyses that connect the adversarial robustness of a model to the number of tasks that it is trained on. Experiments on two datasets show that attack difficulty increases as the number of target tasks increase. Moreover, our results suggest that when models are trained on multiple tasks at once, they become more robust to adversarial attacks on individual tasks. While adversarial defense remains an open challenge, our results suggest that deep networks are vulnerable partly because they are trained on too few tasks.

</details>

<details>

<summary>2020-09-11 04:22:34 - MTFuzz: Fuzzing with a Multi-Task Neural Network</summary>

- *Dongdong She, Rahul Krishna, Lu Yan, Suman Jana, Baishakhi Ray*

- `2005.12392v2` - [abs](http://arxiv.org/abs/2005.12392v2) - [pdf](http://arxiv.org/pdf/2005.12392v2)

> Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation. Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations. In recent years, machine learning (ML) based mutation strategies have reported promising results. However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data. As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model. In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e., predicting for different types of coverage). The compact embedding can guide the mutation process by focusing most of the mutations on the parts of the embedding where the gradient is high. \tool uncovers $11$ previously unseen bugs and achieves an average of $2\times$ more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs.

</details>

<details>

<summary>2020-09-11 17:07:02 - Smart Jamming Attacks in 5G New Radio: A Review</summary>

- *Youness Arjoune, Saleh Faruque*

- `2009.05531v1` - [abs](http://arxiv.org/abs/2009.05531v1) - [pdf](http://arxiv.org/pdf/2009.05531v1)

> The fifth generation of wireless cellular networks (5G) is expected to be the infrastructure for emergency services, natural disasters rescue, public safety, and military communications. 5G, as any previous wireless cellular network, is vulnerable to jamming attacks, which create deliberate interference to hinder the communication of legitimate users. Therefore, jamming 5G networks can be a real threat to public safety. Thus, there is a strong need to investigate to what extent these networks are vulnerable to jamming attacks. For this investigation, we consider the 3GPP standard released in 2017, which is widely accepted as the primary reference for the deployment of these networks. First, we describe the key elements of 5G New Radio (NR) architecture, such as different channels and signals exchanged between the base station and user equipment. Second, we conduct an in-depth review of the jamming attack models and we assess the 5G NR vulnerabilities to these jamming attacks. Then, we present the state-of-the-art detection and mitigation techniques, and we discuss their suitability to defeat smart jammers in 5G wireless networks. Finally, we provide some recommendations and future research directions at the end of this paper.

</details>

<details>

<summary>2020-09-11 21:53:50 - Quantifying Membership Inference Vulnerability via Generalization Gap and Other Model Metrics</summary>

- *Jason W. Bentley, Daniel Gibney, Gary Hoppenworth, Sumit Kumar Jha*

- `2009.05669v1` - [abs](http://arxiv.org/abs/2009.05669v1) - [pdf](http://arxiv.org/pdf/2009.05669v1)

> We demonstrate how a target model's generalization gap leads directly to an effective deterministic black box membership inference attack (MIA). This provides an upper bound on how secure a model can be to MIA based on a simple metric. Moreover, this attack is shown to be optimal in the expected sense given access to only certain likely obtainable metrics regarding the network's training and performance. Experimentally, this attack is shown to be comparable in accuracy to state-of-art MIAs in many cases.

</details>

<details>

<summary>2020-09-12 20:58:04 - Reinforcement Learning-based Black-Box Evasion Attacks to Link Prediction in Dynamic Graphs</summary>

- *Houxiang Fan, Binghui Wang, Pan Zhou, Ang Li, Meng Pang, Zichuan Xu, Cai Fu, Hai Li, Yiran Chen*

- `2009.00163v2` - [abs](http://arxiv.org/abs/2009.00163v2) - [pdf](http://arxiv.org/pdf/2009.00163v2)

> Link prediction in dynamic graphs (LPDG) is an important research problem that has diverse applications such as online recommendations, studies on disease contagion, organizational studies, etc. Various LPDG methods based on graph embedding and graph neural networks have been recently proposed and achieved state-of-the-art performance. In this paper, we study the vulnerability of LPDG methods and propose the first practical black-box evasion attack. Specifically, given a trained LPDG model, our attack aims to perturb the graph structure, without knowing to model parameters, model architecture, etc., such that the LPDG model makes as many wrong predicted links as possible. We design our attack based on a stochastic policy-based RL algorithm. Moreover, we evaluate our attack on three real-world graph datasets from different application domains. Experimental results show that our attack is both effective and efficient.

</details>

<details>

<summary>2020-09-12 22:18:54 - Certified Robustness of Graph Classification against Topology Attack with Randomized Smoothing</summary>

- *Zhidong Gao, Rui Hu, Yanmin Gong*

- `2009.05872v1` - [abs](http://arxiv.org/abs/2009.05872v1) - [pdf](http://arxiv.org/pdf/2009.05872v1)

> Graph classification has practical applications in diverse fields. Recent studies show that graph-based machine learning models are especially vulnerable to adversarial perturbations due to the non i.i.d nature of graph data. By adding or deleting a small number of edges in the graph, adversaries could greatly change the graph label predicted by a graph classification model. In this work, we propose to build a smoothed graph classification model with certified robustness guarantee. We have proven that the resulting graph classification model would output the same prediction for a graph under $l_0$ bounded adversarial perturbation. We also evaluate the effectiveness of our approach under graph convolutional network (GCN) based multi-class graph classification model.

</details>

<details>

<summary>2020-09-14 09:16:09 - Machine learning predicts early onset of fever from continuous physiological data of critically ill patients</summary>

- *Aditya Singh, Akram Mohammed, Lokesh Chinthala, Rishikesan Kamaleswaran*

- `2009.07103v1` - [abs](http://arxiv.org/abs/2009.07103v1) - [pdf](http://arxiv.org/pdf/2009.07103v1)

> Fever can provide valuable information for diagnosis and prognosis of various diseases such as pneumonia, dengue, sepsis, etc., therefore, predicting fever early can help in the effectiveness of treatment options and expediting the treatment process. This study aims to develop novel algorithms that can accurately predict fever onset in critically ill patients by applying machine learning technique on continuous physiological data. We analyzed continuous physiological data collected every 5-minute from a cohort of over 200,000 critically ill patients admitted to an Intensive Care Unit (ICU) over a 2-year period. Each episode of fever from the same patient were considered as an independent event, with separations of at least 24 hours. We extracted descriptive statistical features from six physiological data streams, including heart rate, respiration, systolic and diastolic blood pressure, mean arterial pressure, and oxygen saturation, and use these features to independently predict the onset of fever. Using a bootstrap aggregation method, we created a balanced dataset of 7,801 afebrile and febrile patients and analyzed features up to 4 hours before the fever onset. We found that supervised machine learning methods can predict fever up to 4 hours before onset in critically ill patients with high recall, precision, and F1-score. This study demonstrates the viability of using machine learning to predict fever among hospitalized adults. The discovery of salient physiomarkers through machine learning and deep learning techniques has the potential to further accelerate the development and implementation of innovative care delivery protocols and strategies for medically vulnerable patients.

</details>

<details>

<summary>2020-09-14 14:58:13 - Revealing the Weaknesses of File Sharing System on Cloud Storages</summary>

- *Fauzi Adi Rafrastara, Qi DeYu*

- `2009.07099v1` - [abs](http://arxiv.org/abs/2009.07099v1) - [pdf](http://arxiv.org/pdf/2009.07099v1)

> Cloud storage provides the simpler way to share the files privately and publicly. A good Cloud Storage Provider (SCP) is not only measured by the access speed or file size that can be shared to others, but also regarding the security issues in file sharing itself. In this paper, we analyze the security of file sharing in 3 Chinese CSPs, which are: Baidu, Weiyun and Kanbox. Those CSPs have their own vulnerabilities that successfully revealed. We also provide some suggestions to countermeasure the weaknesses so that they can maintain the quality while improving the security.

</details>

<details>

<summary>2020-09-14 17:20:01 - Robust Deep Learning Ensemble against Deception</summary>

- *Wenqi Wei, Ling Liu*

- `2009.06589v1` - [abs](http://arxiv.org/abs/2009.06589v1) - [pdf](http://arxiv.org/pdf/2009.06589v1)

> Deep neural network (DNN) models are known to be vulnerable to maliciously crafted adversarial examples and to out-of-distribution inputs drawn sufficiently far away from the training data. How to protect a machine learning model against deception of both types of destructive inputs remains an open challenge. This paper presents XEnsemble, a diversity ensemble verification methodology for enhancing the adversarial robustness of DNN models against deception caused by either adversarial examples or out-of-distribution inputs. XEnsemble by design has three unique capabilities. First, XEnsemble builds diverse input denoising verifiers by leveraging different data cleaning techniques. Second, XEnsemble develops a disagreement-diversity ensemble learning methodology for guarding the output of the prediction model against deception. Third, XEnsemble provides a suite of algorithms to combine input verification and output verification to protect the DNN prediction models from both adversarial examples and out of distribution inputs. Evaluated using eleven popular adversarial attacks and two representative out-of-distribution datasets, we show that XEnsemble achieves a high defense success rate against adversarial examples and a high detection success rate against out-of-distribution data inputs, and outperforms existing representative defense methods with respect to robustness and defensibility.

</details>

<details>

<summary>2020-09-14 20:04:40 - Sparsity Turns Adversarial: Energy and Latency Attacks on Deep Neural Networks</summary>

- *Sarada Krithivasan, Sanchari Sen, Anand Raghunathan*

- `2006.08020v3` - [abs](http://arxiv.org/abs/2006.08020v3) - [pdf](http://arxiv.org/pdf/2006.08020v3)

> Adversarial attacks have exposed serious vulnerabilities in Deep Neural Networks (DNNs) through their ability to force misclassifications through human-imperceptible perturbations to DNN inputs. We explore a new direction in the field of adversarial attacks by suggesting attacks that aim to degrade the computational efficiency of DNNs rather than their classification accuracy. Specifically, we propose and demonstrate sparsity attacks, which adversarial modify a DNN's inputs so as to reduce sparsity (or the presence of zero values) in its internal activation values. In resource-constrained systems, a wide range of hardware and software techniques have been proposed that exploit sparsity to improve DNN efficiency. The proposed attack increases the execution time and energy consumption of sparsity-optimized DNN implementations, raising concern over their deployment in latency and energy-critical applications.   We propose a systematic methodology to generate adversarial inputs for sparsity attacks by formulating an objective function that quantifies the network's activation sparsity, and minimizing this function using iterative gradient-descent techniques. We launch both white-box and black-box versions of adversarial sparsity attacks on image recognition DNNs and demonstrate that they decrease activation sparsity by up to 1.82x. We also evaluate the impact of the attack on a sparsity-optimized DNN accelerator and demonstrate degradations up to 1.59x in latency, and also study the performance of the attack on a sparsity-optimized general-purpose processor. Finally, we evaluate defense techniques such as activation thresholding and input quantization and demonstrate that the proposed attack is able to withstand them, highlighting the need for further efforts in this new direction within the field of adversarial machine learning.

</details>

<details>

<summary>2020-09-14 22:06:13 - Private data sharing between decentralized users through the privGAN architecture</summary>

- *Jean-Francois Rajotte, Raymond T Ng*

- `2009.06764v1` - [abs](http://arxiv.org/abs/2009.06764v1) - [pdf](http://arxiv.org/pdf/2009.06764v1)

> More data is almost always beneficial for analysis and machine learning tasks. In many realistic situations however, an enterprise cannot share its data, either to keep a competitive advantage or to protect the privacy of the data sources, the enterprise's clients for example. We propose a method for data owners to share synthetic or fake versions of their data without sharing the actual data, nor the parameters of models that have direct access to the data. The method proposed is based on the privGAN architecture where local GANs are trained on their respective data subsets with an extra penalty from a central discriminator aiming to discriminate the origin of a given fake sample. We demonstrate that this approach, when applied to subsets of various sizes, leads to better utility for the owners than the utility from their real small datasets. The only shared pieces of information are the parameter updates of the central discriminator. The privacy is demonstrated with white-box attacks on the most vulnerable elments of the architecture and the results are close to random guessing. This method would apply naturally in a federated learning setting.

</details>

<details>

<summary>2020-09-15 01:58:17 - Certified Robustness of Community Detection against Adversarial Structural Perturbation via Randomized Smoothing</summary>

- *Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong*

- `2002.03421v2` - [abs](http://arxiv.org/abs/2002.03421v2) - [pdf](http://arxiv.org/pdf/2002.03421v2)

> Community detection plays a key role in understanding graph structure. However, several recent studies showed that community detection is vulnerable to adversarial structural perturbation. In particular, via adding or removing a small number of carefully selected edges in a graph, an attacker can manipulate the detected communities. However, to the best of our knowledge, there are no studies on certifying robustness of community detection against such adversarial structural perturbation. In this work, we aim to bridge this gap. Specifically, we develop the first certified robustness guarantee of community detection against adversarial structural perturbation. Given an arbitrary community detection method, we build a new smoothed community detection method via randomly perturbing the graph structure. We theoretically show that the smoothed community detection method provably groups a given arbitrary set of nodes into the same community (or different communities) when the number of edges added/removed by an attacker is bounded. Moreover, we show that our certified robustness is tight. We also empirically evaluate our method on multiple real-world graphs with ground truth communities.

</details>

<details>

<summary>2020-09-16 03:49:46 - Intentional Control of Type I Error over Unconscious Data Distortion: a Neyman-Pearson Approach to Text Classification</summary>

- *Lucy Xia, Richard Zhao, Yanhui Wu, Xin Tong*

- `1802.02558v3` - [abs](http://arxiv.org/abs/1802.02558v3) - [pdf](http://arxiv.org/pdf/1802.02558v3)

> This paper addresses the challenges in classifying textual data obtained from open online platforms, which are vulnerable to distortion. Most existing classification methods minimize the overall classification error and may yield an undesirably large type I error (relevant textual messages are classified as irrelevant), particularly when available data exhibit an asymmetry between relevant and irrelevant information. Data distortion exacerbates this situation and often leads to fallacious prediction. To deal with inestimable data distortion, we propose the use of the Neyman-Pearson (NP) classification paradigm, which minimizes type II error under a user-specified type I error constraint. Theoretically, we show that the NP oracle is unaffected by data distortion when the class conditional distributions remain the same. Empirically, we study a case of classifying posts about worker strikes obtained from a leading Chinese microblogging platform, which are frequently prone to extensive, unpredictable and inestimable censorship. We demonstrate that, even though the training and test data are susceptible to different distortion and therefore potentially follow different distributions, our proposed NP methods control the type I error on test data at the targeted level. The methods and implementation pipeline proposed in our case study are applicable to many other problems involving data distortion.

</details>

<details>

<summary>2020-09-16 15:32:39 - Enhancing Robustness of Deep Neural Networks Against Adversarial Malware Samples: Principles, Framework, and AICS'2019 Challenge</summary>

- *Deqiang Li, Qianmu Li, Yanfang Ye, Shouhuai Xu*

- `1812.08108v3` - [abs](http://arxiv.org/abs/1812.08108v3) - [pdf](http://arxiv.org/pdf/1812.08108v3)

> Malware continues to be a major cyber threat, despite the tremendous effort that has been made to combat them. The number of malware in the wild steadily increases over time, meaning that we must resort to automated defense techniques. This naturally calls for machine learning based malware detection. However, machine learning is known to be vulnerable to adversarial evasion attacks that manipulate a small number of features to make classifiers wrongly recognize a malware sample as a benign one. The state-of-the-art is that there are no effective countermeasures against these attacks. Inspired by the AICS'2019 Challenge, we systematize a number of principles for enhancing the robustness of neural networks against adversarial malware evasion attacks. Some of these principles have been scattered in the literature, but others are proposed in this paper for the first time. Under the guidance of these principles, we propose a framework and an accompanying training algorithm, which are then applied to the AICS'2019 challenge. Our experimental results have been submitted to the challenge organizer for evaluation.

</details>

<details>

<summary>2020-09-16 21:13:56 - Post Quantum Secure Command and Control of Mobile Agents : Inserting quantum-resistant encryption schemes in the Secure Robot Operating System</summary>

- *Richa Varma, Chris Melville, Claudio Pinello, Tuhin Sahai*

- `2009.07937v1` - [abs](http://arxiv.org/abs/2009.07937v1) - [pdf](http://arxiv.org/pdf/2009.07937v1)

> The secure command and control (C&C) of mobile agents arises in various settings including unmanned aerial vehicles, single pilot operations in commercial settings, and mobile robots to name a few. As more and more of these applications get integrated into aerospace and defense use cases, the security of the communication channel between the ground station and the mobile agent is of increasing importance. The development of quantum computing devices poses a unique threat to secure communications due to the vulnerability of asymmetric ciphers to Shor's algorithm. Given the active development of new quantum resistant encryption techniques, we report the first integration of post-quantum secure encryption schemes with the robot operating system (ROS) and C&C of mobile agents, in general. We integrate these schemes in the application and network layers, and study the performance of these methods by comparing them to present day security schemes such as the widely used RSA algorithm.

</details>

<details>

<summary>2020-09-16 22:16:24 - Security Analysis Methods on Ethereum Smart Contract Vulnerabilities: A Survey</summary>

- *Purathani Praitheeshan, Lei Pan, Jiangshan Yu, Joseph Liu, Robin Doss*

- `1908.08605v3` - [abs](http://arxiv.org/abs/1908.08605v3) - [pdf](http://arxiv.org/pdf/1908.08605v3)

> Smart contracts are software programs featuring both traditional applications and distributed data storage on blockchains. Ethereum is a prominent blockchain platform with the support of smart contracts. The smart contracts act as autonomous agents in critical decentralized applications and hold a significant amount of cryptocurrency to perform trusted transactions and agreements. Millions of dollars as part of the assets held by the smart contracts were stolen or frozen through the notorious attacks just between 2016 and 2018, such as the DAO attack, Parity Multi-Sig Wallet attack, and the integer underflow/overflow attacks. These attacks were caused by a combination of technical flaws in designing and implementing software codes. However, many more vulnerabilities of less severity are to be discovered because of the scripting natures of the Solidity language and the non-updateable feature of blockchains. Hence, we surveyed 16 security vulnerabilities in smart contract programs, and some vulnerabilities do not have a proper solution. This survey aims to identify the key vulnerabilities in smart contracts on Ethereum in the perspectives of their internal mechanisms and software security vulnerabilities. By correlating 16 Ethereum vulnerabilities and 19 software security issues, we predict that many attacks are yet to be exploited. And we have explored many software tools to detect the security vulnerabilities of smart contracts in terms of static analysis, dynamic analysis, and formal verification. This survey presents the security problems in smart contracts together with the available analysis tools and the detection methods. We also investigated the limitations of the tools or analysis methods with respect to the identified security vulnerabilities of the smart contracts.

</details>

<details>

<summary>2020-09-17 06:37:15 - An Extension of Fano's Inequality for Characterizing Model Susceptibility to Membership Inference Attacks</summary>

- *Sumit Kumar Jha, Susmit Jha, Rickard Ewetz, Sunny Raj, Alvaro Velasquez, Laura L. Pullum, Ananthram Swami*

- `2009.08097v1` - [abs](http://arxiv.org/abs/2009.08097v1) - [pdf](http://arxiv.org/pdf/2009.08097v1)

> Deep neural networks have been shown to be vulnerable to membership inference attacks wherein the attacker aims to detect whether specific input data were used to train the model. These attacks can potentially leak private or proprietary data. We present a new extension of Fano's inequality and employ it to theoretically establish that the probability of success for a membership inference attack on a deep neural network can be bounded using the mutual information between its inputs and its activations. This enables the use of mutual information to measure the susceptibility of a DNN model to membership inference attacks. In our empirical evaluation, we show that the correlation between the mutual information and the susceptibility of the DNN model to membership inference attacks is 0.966, 0.996, and 0.955 for CIFAR-10, SVHN and GTSRB models, respectively.

</details>

<details>

<summary>2020-09-17 10:50:42 - Generating Label Cohesive and Well-Formed Adversarial Claims</summary>

- *Pepa Atanasova, Dustin Wright, Isabelle Augenstein*

- `2009.08205v1` - [abs](http://arxiv.org/abs/2009.08205v1) - [pdf](http://arxiv.org/pdf/2009.08205v1)

> Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimising the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.

</details>

<details>

<summary>2020-09-17 23:35:12 - On Primes, Log-Loss Scores and (No) Privacy</summary>

- *Abhinav Aggarwal, Zekun Xu, Oluwaseyi Feyisetan, Nathanael Teissier*

- `2009.08559v1` - [abs](http://arxiv.org/abs/2009.08559v1) - [pdf](http://arxiv.org/pdf/2009.08559v1)

> Membership Inference Attacks exploit the vulnerabilities of exposing models trained on customer data to queries by an adversary. In a recently proposed implementation of an auditing tool for measuring privacy leakage from sensitive datasets, more refined aggregates like the Log-Loss scores are exposed for simulating inference attacks as well as to assess the total privacy leakage based on the adversary's predictions. In this paper, we prove that this additional information enables the adversary to infer the membership of any number of datapoints with full accuracy in a single query, causing complete membership privacy breach. Our approach obviates any attack model training or access to side knowledge with the adversary. Moreover, our algorithms are agnostic to the model under attack and hence, enable perfect membership inference even for models that do not memorize or overfit. In particular, our observations provide insight into the extent of information leakage from statistical aggregates and how they can be exploited.

</details>

<details>

<summary>2020-09-18 02:43:53 - Toward Optimal Adversarial Policies in the Multiplicative Learning System with a Malicious Expert</summary>

- *S. Rasoul Etesami, Negar Kiyavash, Vincent Leon, H. Vincent Poor*

- `2001.00543v2` - [abs](http://arxiv.org/abs/2001.00543v2) - [pdf](http://arxiv.org/pdf/2001.00543v2)

> We consider a learning system based on the conventional multiplicative weight (MW) rule that combines experts' advice to predict a sequence of true outcomes. It is assumed that one of the experts is malicious and aims to impose the maximum loss on the system. The loss of the system is naturally defined to be the aggregate absolute difference between the sequence of predicted outcomes and the true outcomes. We consider this problem under both offline and online settings. In the offline setting where the malicious expert must choose its entire sequence of decisions a priori, we show somewhat surprisingly that a simple greedy policy of always reporting false prediction is asymptotically optimal with an approximation ratio of $1+O(\sqrt{\frac{\ln N}{N}})$, where $N$ is the total number of prediction stages. In particular, we describe a policy that closely resembles the structure of the optimal offline policy. For the online setting where the malicious expert can adaptively make its decisions, we show that the optimal online policy can be efficiently computed by solving a dynamic program in $O(N^3)$. Our results provide a new direction for vulnerability assessment of commonly used learning algorithms to adversarial attacks where the threat is an integral part of the system.

</details>

<details>

<summary>2020-09-18 18:45:21 - On the Threat of npm Vulnerable Dependencies in Node.js Applications</summary>

- *Mahmoud Alfadel, Diego Elias Costa, Mouafak Mokhallalati, Emad Shihab, Bram Adams*

- `2009.09019v1` - [abs](http://arxiv.org/abs/2009.09019v1) - [pdf](http://arxiv.org/pdf/2009.09019v1)

> Software vulnerabilities have a large negative impact on the software systems that we depend on daily. Reports on software vulnerabilities always paint a grim picture, with some reports showing that 83% of organizations depend on vulnerable software. However, our experience leads us to believe that, in the grand scheme of things, these software vulnerabilities may have less impact than what is reported. Therefore, we perform a study to better understand the threat of npm vulnerable packages used in Node.js applications. We define three threat levels for vulnerabilities in packages, based on their lifecycle, where a package vulnerability is assigned a low threat level if it was hidden or still unknown at the time it was used in the dependent application (t), medium threat level if the vulnerability was reported but not yet published at t, and high if it was publicly announced at t. Then, we perform an empirical study involving 6,673 real-world, active, and mature open source Node.js applications. Our findings show that although 67.93% of the examined applications depend on at least one vulnerable package, 94.91% of the vulnerable packages in those affected applications are classified as having low threat. Moreover, we find that in the case of vulnerable packages classified as having high threat, it is the application's lack of updating that makes them vulnerable, i.e., it is not the existence of the vulnerability that is the real problem. Furthermore, we verify our findings at different stages of the application's lifetime and find that our findings still hold. Our study argues that when it comes to software vulnerabilities, things may not be as bad as they seem and that considering vulnerability threat is key.

</details>

<details>

<summary>2020-09-19 03:37:44 - SecDD: Efficient and Secure Method for Remotely Training Neural Networks</summary>

- *Ilia Sucholutsky, Matthias Schonlau*

- `2009.09155v1` - [abs](http://arxiv.org/abs/2009.09155v1) - [pdf](http://arxiv.org/pdf/2009.09155v1)

> We leverage what are typically considered the worst qualities of deep learning algorithms - high computational cost, requirement for large data, no explainability, high dependence on hyper-parameter choice, overfitting, and vulnerability to adversarial perturbations - in order to create a method for the secure and efficient training of remotely deployed neural networks over unsecured channels.

</details>

<details>

<summary>2020-09-19 16:56:29 - On the Influence of Ageing on Face Morph Attacks: Vulnerability and Detection</summary>

- *Sushma Venkatesh, Kiran Raja, Raghavendra Ramachandra, Christoph Busch*

- `2007.02684v2` - [abs](http://arxiv.org/abs/2007.02684v2) - [pdf](http://arxiv.org/pdf/2007.02684v2)

> Face morphing attacks have raised critical concerns as they demonstrate a new vulnerability of Face Recognition Systems (FRS), which are widely deployed in border control applications. The face morphing process uses the images from multiple data subjects and performs an image blending operation to generate a morphed image of high quality. The generated morphed image exhibits similar visual characteristics corresponding to the biometric characteristics of the data subjects that contributed to the composite image and thus making it difficult for both humans and FRS, to detect such attacks. In this paper, we report a systematic investigation on the vulnerability of the Commercial-Off-The-Shelf (COTS) FRS when morphed images under the influence of ageing are presented. To this extent, we have introduced a new morphed face dataset with ageing derived from the publicly available MORPH II face dataset, which we refer to as MorphAge dataset. The dataset has two bins based on age intervals, the first bin - MorphAge-I dataset has 1002 unique data subjects with the age variation of 1 year to 2 years while the MorphAge-II dataset consists of 516 data subjects whose age intervals are from 2 years to 5 years. To effectively evaluate the vulnerability for morphing attacks, we also introduce a new evaluation metric, namely the Fully Mated Morphed Presentation Match Rate (FMMPMR), to quantify the vulnerability effectively in a realistic scenario. Extensive experiments are carried out by using two different COTS FRS (COTS I - Cognitec and COTS II - Neurotechnology) to quantify the vulnerability with ageing. Further, we also evaluate five different Morph Attack Detection (MAD) techniques to benchmark their detection performance with ageing.

</details>

<details>

<summary>2020-09-21 14:46:17 - Feature Distillation With Guided Adversarial Contrastive Learning</summary>

- *Tao Bai, Jinnan Chen, Jun Zhao, Bihan Wen, Xudong Jiang, Alex Kot*

- `2009.09922v1` - [abs](http://arxiv.org/abs/2009.09922v1) - [pdf](http://arxiv.org/pdf/2009.09922v1)

> Deep learning models are shown to be vulnerable to adversarial examples. Though adversarial training can enhance model robustness, typical approaches are computationally expensive. Recent works proposed to transfer the robustness to adversarial attacks across different tasks or models with soft labels.Compared to soft labels, feature contains rich semantic information and holds the potential to be applied to different downstream tasks. In this paper, we propose a novel approach called Guided Adversarial Contrastive Distillation (GACD), to effectively transfer adversarial robustness from teacher to student with features. We first formulate this objective as contrastive learning and connect it with mutual information. With a well-trained teacher model as an anchor, students are expected to extract features similar to the teacher. Then considering the potential errors made by teachers, we propose sample reweighted estimation to eliminate the negative effects from teachers. With GACD, the student not only learns to extract robust features, but also captures structural knowledge from the teacher. By extensive experiments evaluating over popular datasets such as CIFAR-10, CIFAR-100 and STL-10, we demonstrate that our approach can effectively transfer robustness across different models and even different tasks, and achieve comparable or better results than existing methods. Besides, we provide a detailed analysis of various methods, showing that students produced by our approach capture more structural knowledge from teachers and learn more robust features under adversarial attacks.

</details>

<details>

<summary>2020-09-21 16:08:41 - SPAM: Stateless Permutation of Application Memory</summary>

- *Mohamed Tarek Ibn Ziad, Miguel A. Arroyo, Simha Sethumadhavan*

- `2007.13808v3` - [abs](http://arxiv.org/abs/2007.13808v3) - [pdf](http://arxiv.org/pdf/2007.13808v3)

> In this paper, we propose the Stateless Permutation of Application Memory (SPAM), a software defense that enables fine-grained data permutation for C programs. The key benefits include resilience against attacks that directly exploit software errors (i.e., spatial and temporal memory safety violations) in addition to attacks that exploit hardware vulnerabilities such as ColdBoot, RowHammer or hardware side-channels to disclose or corrupt memory using a single cohesive technique. Unlike prior work, SPAM is stateless by design making it automatically applicable to multi-threaded applications.   We implement SPAM as an LLVM compiler pass with an extension to the compiler-rt runtime. We evaluate it on the C subset of the SPEC2017 benchmark suite and three real-world applications: the Nginx web server, the Duktape Javascript interpreter, and the WolfSSL cryptographic library. We further show SPAM's scalability by running a multi-threaded benchmark suite. SPAM has greater security coverage and comparable performance overheads to state-of-the-art software techniques for memory safety on contemporary x86_64 processors. Our security evaluation confirms SPAM's effectiveness in preventing intra/inter spatial/temporal memory violations by making the attacker success chances as low as 1/16!.

</details>

<details>

<summary>2020-09-21 18:51:49 - Modeling Techniques for Logic Locking</summary>

- *Joseph Sweeney, Marijn J. H. Heule, Lawrence Pileggi*

- `2009.10131v1` - [abs](http://arxiv.org/abs/2009.10131v1) - [pdf](http://arxiv.org/pdf/2009.10131v1)

> Logic locking is a method to prevent intellectual property (IP) piracy. However, under a reasonable attack model, SAT-based methods have proven to be powerful in obtaining the secret key. In response, many locking techniques have been developed to specifically resist this form of attack. In this paper, we demonstrate two SAT modeling techniques that can provide many orders of magnitude speed up in discovering the correct key. Specifically, we consider relaxed encodings and symmetry breaking. To demonstrate their impact, we model and attack a state-of-the-art logic locking technique, Full-Lock. We show that circuits previously unbreakable within 15 days of run time can be solved in seconds. Consequently, in assessing the strength of any given locking, it is imperative that these modeling techniques be considered. To remedy this vulnerability in the considered locking technique, we demonstrate an extended version, logic-enhanced Banyan locking, that is resistant to our proposed modeling techniques.

</details>

<details>

<summary>2020-09-21 20:11:53 - Proposal of a Novel Bug Bounty Implementation Using Gamification</summary>

- *Jamie O'Hare, Lynsay A. Shepherd*

- `2009.10158v1` - [abs](http://arxiv.org/abs/2009.10158v1) - [pdf](http://arxiv.org/pdf/2009.10158v1)

> Despite significant popularity, the bug bounty process has remained broadly unchanged since its inception, with limited implementation of gamification aspects. Existing literature recognises that current methods generate intensive resource demands, and can encounter issues impacting program effectiveness. This paper proposes a novel bug bounty process aiming to alleviate resource demands and mitigate inherent issues. Through the additional crowdsourcing of report verification where fellow hackers perform vulnerability verification and reproduction, the client organisation can reduce overheads at the cost of rewarding more participants. The incorporation of gamification elements provides a substitute for monetary rewards, as well as presenting possible mitigation of bug bounty program effectiveness issues. Collectively, traits of the proposed process appear appropriate for resource and budget-constrained organisations - such Higher Education institutions.

</details>

<details>

<summary>2020-09-21 20:48:41 - A Technical Review of Wireless security for the Internet of things: Software Defined Radio perspective</summary>

- *Jose de Jesus Rugeles, Edward Paul Guillen, Leonardo S Cardoso*

- `2009.10171v1` - [abs](http://arxiv.org/abs/2009.10171v1) - [pdf](http://arxiv.org/pdf/2009.10171v1)

> The increase of cyberattacks using IoT devices has exposed the vulnerabilities in the infrastructures that make up the IoT and have shown how small devices can affect networks and services functioning. This paper presents a review of the vulnerabilities of the wireless technologies that bear the IoT and assessing the experiences in implementing wireless attacks targeting the Internet of Things using Software-Defined Radio (SDR) technologies. A systematic literature review was conducted. The types of vulnerabilities and attacks that can affect the wireless technologies that stand the IoT ecosystem and SDR radio platforms were compared. On the IoT system model layer, perception layer was identified as the most vulnerable. Most attacks at this level occur due to limitations in hardware, physical exposure of devices, and heterogeneity of technologies. Future cybersecurity systems based on SDR radios have notable advantages due to their flexibility to adapt to new communication technologies and their potential for the development of advanced tools. However, cybersecurity challenges for the Internet of Things are so complex that it is needed to merge SDR hardware with cognitive techniques and intelligent techniques such as deep learning to adapt to rapid technological changes.

</details>

<details>

<summary>2020-09-22 14:17:18 - Adversarial Attack Based Countermeasures against Deep Learning Side-Channel Attacks</summary>

- *Ruizhe Gu, Ping Wang, Mengce Zheng, Honggang Hu, Nenghai Yu*

- `2009.10568v1` - [abs](http://arxiv.org/abs/2009.10568v1) - [pdf](http://arxiv.org/pdf/2009.10568v1)

> Numerous previous works have studied deep learning algorithms applied in the context of side-channel attacks, which demonstrated the ability to perform successful key recoveries. These studies show that modern cryptographic devices are increasingly threatened by side-channel attacks with the help of deep learning. However, the existing countermeasures are designed to resist classical side-channel attacks, and cannot protect cryptographic devices from deep learning based side-channel attacks. Thus, there arises a strong need for countermeasures against deep learning based side-channel attacks. Although deep learning has the high potential in solving complex problems, it is vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrectly.   In this paper, we propose a kind of novel countermeasures based on adversarial attacks that is specifically designed against deep learning based side-channel attacks. We estimate several models commonly used in deep learning based side-channel attacks to evaluate the proposed countermeasures. It shows that our approach can effectively protect cryptographic devices from deep learning based side-channel attacks in practice. In addition, our experiments show that the new countermeasures can also resist classical side-channel attacks.

</details>

<details>

<summary>2020-09-22 20:48:19 - Using Machine Learning to Develop a Novel COVID-19 Vulnerability Index (C19VI)</summary>

- *Anuj Tiwari, Arya V. Dadhania, Vijay Avin Balaji Ragunathrao, Edson R. A. Oliveira*

- `2009.10808v1` - [abs](http://arxiv.org/abs/2009.10808v1) - [pdf](http://arxiv.org/pdf/2009.10808v1)

> COVID19 is now one of the most leading causes of death in the United States. Systemic health, social and economic disparities have put the minorities and economically poor communities at a higher risk than others. There is an immediate requirement to develop a reliable measure of county-level vulnerabilities that can capture the heterogeneity of both vulnerable communities and the COVID19 pandemic. This study reports a COVID19 Vulnerability Index (C19VI) for identification and mapping of vulnerable counties in the United States. We proposed a Random Forest machine learning based COVID19 vulnerability model using CDC sociodemographic and COVID19-specific themes. An innovative COVID19 Impact Assessment algorithm was also developed using homogeneity and trend assessment technique for evaluating severity of the pandemic in all counties and train RF model. Developed C19VI was statistically validated and compared with the CDC COVID19 Community Vulnerability Index (CCVI). Finally, using C19VI along with census data, we explored racial inequalities and economic disparities in COVID19 health outcomes amongst different regions in the United States. Our C19VI index indicates that 18.30% of the counties falls into very high vulnerability class, 24.34% in high, 23.32% in moderate, 22.34% in low, and 11.68% in very low. Furthermore, C19VI reveals that 75.57% of racial minorities and 82.84% of economically poor communities are very high or high COVID19 vulnerable regions. The proposed approach of vulnerability modeling takes advantage of both the well-established field of statistical analysis and the fast-evolving domain of machine learning. C19VI provides an accurate and more reliable way to measure county level vulnerability in the United States. This index aims at helping emergency planners to develop more effective mitigation strategies especially for the disproportionately impacted communities.

</details>

<details>

<summary>2020-09-23 03:15:20 - Pocket Diagnosis: Secure Federated Learning against Poisoning Attack in the Cloud</summary>

- *Zhuoran Ma, Jianfeng Ma, Yinbin Miao, Ximeng Liu, Kim-Kwang Raymond Choo, Robert H. Deng*

- `2009.10918v1` - [abs](http://arxiv.org/abs/2009.10918v1) - [pdf](http://arxiv.org/pdf/2009.10918v1)

> Federated learning has become prevalent in medical diagnosis due to its effectiveness in training a federated model among multiple health institutions (i.e. Data Islands (DIs)). However, increasingly massive DI-level poisoning attacks have shed light on a vulnerability in federated learning, which inject poisoned data into certain DIs to corrupt the availability of the federated model. Previous works on federated learning have been inadequate in ensuring the privacy of DIs and the availability of the final federated model. In this paper, we design a secure federated learning mechanism with multiple keys to prevent DI-level poisoning attacks for medical diagnosis, called SFPA. Concretely, SFPA provides privacy-preserving random forest-based federated learning by using the multi-key secure computation, which guarantees the confidentiality of DI-related information. Meanwhile, a secure defense strategy over encrypted locally-submitted models is proposed to defense DI-level poisoning attacks. Finally, our formal security analysis and empirical tests on a public cloud platform demonstrate the security and efficiency of SFPA as well as its capability of resisting DI-level poisoning attacks.

</details>

<details>

<summary>2020-09-23 15:31:46 - Dual-SLAM: A framework for robust single camera navigation</summary>

- *Huajian Huang, Wen-Yan Lin, Siying Liu, Dong Zhang, Sai-Kit Yeung*

- `2009.11219v1` - [abs](http://arxiv.org/abs/2009.11219v1) - [pdf](http://arxiv.org/pdf/2009.11219v1)

> SLAM (Simultaneous Localization And Mapping) seeks to provide a moving agent with real-time self-localization. To achieve real-time speed, SLAM incrementally propagates position estimates. This makes SLAM fast but also makes it vulnerable to local pose estimation failures. As local pose estimation is ill-conditioned, local pose estimation failures happen regularly, making the overall SLAM system brittle. This paper attempts to correct this problem. We note that while local pose estimation is ill-conditioned, pose estimation over longer sequences is well-conditioned. Thus, local pose estimation errors eventually manifest themselves as mapping inconsistencies. When this occurs, we save the current map and activate two new SLAM threads. One processes incoming frames to create a new map and the other, recovery thread, backtracks to link new and old maps together. This creates a Dual-SLAM framework that maintains real-time performance while being robust to local pose estimation failures. Evaluation on benchmark datasets shows Dual-SLAM can reduce failures by a dramatic $88\%$.

</details>

<details>

<summary>2020-09-23 17:35:34 - The Agent Web Model -- Modelling web hacking for reinforcement learning</summary>

- *Laszlo Erdodi, Fabio Massimo Zennaro*

- `2009.11274v1` - [abs](http://arxiv.org/abs/2009.11274v1) - [pdf](http://arxiv.org/pdf/2009.11274v1)

> Website hacking is a frequent attack type used by malicious actors to obtain confidential information, modify the integrity of web pages or make websites unavailable. The tools used by attackers are becoming more and more automated and sophisticated, and malicious machine learning agents seems to be the next development in this line. In order to provide ethical hackers with similar tools, and to understand the impact and the limitations of artificial agents, we present in this paper a model that formalizes web hacking tasks for reinforcement learning agents. Our model, named Agent Web Model, considers web hacking as a capture-the-flag style challenge, and it defines reinforcement learning problems at seven different levels of abstraction. We discuss the complexity of these problems in terms of actions and states an agent has to deal with, and we show that such a model allows to represent most of the relevant web vulnerabilities. Aware that the driver of advances in reinforcement learning is the availability of standardized challenges, we provide an implementation for the first three abstraction layers, in the hope that the community would consider these challenges in order to develop intelligent web hacking agents.

</details>

<details>

<summary>2020-09-24 06:04:56 - ThreatZoom: CVE2CWE using Hierarchical Neural Network</summary>

- *Ehsan Aghaei, Waseem Shadid, Ehab Al-Shaer*

- `2009.11501v1` - [abs](http://arxiv.org/abs/2009.11501v1) - [pdf](http://arxiv.org/pdf/2009.11501v1)

> The Common Vulnerabilities and Exposures (CVE) represent standard means for sharing publicly known information security vulnerabilities. One or more CVEs are grouped into the Common Weakness Enumeration (CWE) classes for the purpose of understanding the software or configuration flaws and potential impacts enabled by these vulnerabilities and identifying means to detect or prevent exploitation. As the CVE-to-CWE classification is mostly performed manually by domain experts, thousands of critical and new CVEs remain unclassified, yet they are unpatchable. This significantly limits the utility of CVEs and slows down proactive threat mitigation. This paper presents the first automatic tool to classify CVEs to CWEs. ThreatZoom uses a novel learning algorithm that employs an adaptive hierarchical neural network which adjusts its weights based on text analytic scores and classification errors. It automatically estimates the CWE classes corresponding to a CVE instance using both statistical and semantic features extracted from the description of a CVE. This tool is rigorously tested by various datasets provided by MITRE and the National Vulnerability Database (NVD). The accuracy of classifying CVE instances to their correct CWE classes are 92% (fine-grain) and 94% (coarse-grain) for NVD dataset, and 75% (fine-grain) and 90% (coarse-grain) for MITRE dataset, despite the small corpus.

</details>

<details>

<summary>2020-09-24 15:55:41 - A Systematic Study of Lattice-based NIST PQC Algorithms: from Reference Implementations to Hardware Accelerators</summary>

- *Malik Imran, Zain Ul Abideen, Samuel Pagliarini*

- `2009.07091v3` - [abs](http://arxiv.org/abs/2009.07091v3) - [pdf](http://arxiv.org/pdf/2009.07091v3)

> Security of currently deployed public key cryptography algorithms is foreseen to be vulnerable against quantum computer attacks. Hence, a community effort exists to develop post-quantum cryptography (PQC) algorithms, i.e., algorithms that are resistant to quantum attacks. In this work, we have investigated how lattice-based candidate algorithms from the NIST PQC standardization competition fare when conceived as hardware accelerators. To achieve this, we have assessed the reference implementations of selected algorithms with the goal of identifying what are their basic building blocks. We assume the hardware accelerators will be implemented in application specific integrated circuit (ASIC) and the targeted technology in our experiments is a commercial 65nm node. In order to estimate the characteristics of each algorithm, we have assessed their memory requirements, use of multipliers, and how each algorithm employs hashing functions. Furthermore, for these building blocks, we have collected area and power figures for 12 candidate algorithms. For memories, we make use of a commercial memory compiler. For logic, we make use of a standard cell library. In order to compare the candidate algorithms fairly, we select a reference frequency of operation of 500MHz. Our results reveal that our area and power numbers are comparable to the state of the art, despite targeting a higher frequency of operation and a higher security level in our experiments. The comprehensive investigation of lattice-based NIST PQC algorithms performed in this paper can be used for guiding ASIC designers when selecting an appropriate algorithm while respecting requirements and design constraints.

</details>

<details>

<summary>2020-09-24 19:09:37 - Adversarial Examples in Deep Learning for Multivariate Time Series Regression</summary>

- *Gautam Raj Mode, Khaza Anuarul Hoque*

- `2009.11911v1` - [abs](http://arxiv.org/abs/2009.11911v1) - [pdf](http://arxiv.org/pdf/2009.11911v1)

> Multivariate time series (MTS) regression tasks are common in many real-world data mining applications including finance, cybersecurity, energy, healthcare, prognostics, and many others. Due to the tremendous success of deep learning (DL) algorithms in various domains including image recognition and computer vision, researchers started adopting these techniques for solving MTS data mining problems, many of which are targeted for safety-critical and cost-critical applications. Unfortunately, DL algorithms are known for their susceptibility to adversarial examples which also makes the DL regression models for MTS forecasting also vulnerable to those attacks. To the best of our knowledge, no previous work has explored the vulnerability of DL MTS regression models to adversarial time series examples, which is an important step, specifically when the forecasting from such models is used in safety-critical and cost-critical applications. In this work, we leverage existing adversarial attack generation techniques from the image classification domain and craft adversarial multivariate time series examples for three state-of-the-art deep learning regression models, specifically Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). We evaluate our study using Google stock and household power consumption dataset. The obtained results show that all the evaluated DL regression models are vulnerable to adversarial attacks, transferable, and thus can lead to catastrophic consequences in safety-critical and cost-critical domains, such as energy and finance.

</details>

<details>

<summary>2020-09-24 20:12:14 - Advancing the Research and Development of Assured Artificial Intelligence and Machine Learning Capabilities</summary>

- *Tyler J. Shipp, Daniel J. Clouse, Michael J. De Lucia, Metin B. Ahiskali, Kai Steverson, Jonathan M. Mullin, Nathaniel D. Bastian*

- `2009.13250v1` - [abs](http://arxiv.org/abs/2009.13250v1) - [pdf](http://arxiv.org/pdf/2009.13250v1)

> Artificial intelligence (AI) and machine learning (ML) have become increasingly vital in the development of novel defense and intelligence capabilities across all domains of warfare. An adversarial AI (A2I) and adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is imperative that AI/ML models can defend against these attacks. A2I/AML defenses will help provide the necessary assurance of these advanced capabilities that use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research and development of assured AI/ML capabilities via new A2I/AML defenses by fostering a collaborative environment across the U.S. Department of Defense and U.S. Intelligence Community. The A2IWG aims to identify specific challenges that it can help solve or address more directly, with initial focus on three topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture Vulnerabilities.

</details>

<details>

<summary>2020-09-27 11:59:17 - Deep Latent Defence</summary>

- *Giulio Zizzo, Chris Hankin, Sergio Maffeis, Kevin Jones*

- `1910.03916v2` - [abs](http://arxiv.org/abs/1910.03916v2) - [pdf](http://arxiv.org/pdf/1910.03916v2)

> Deep learning methods have shown state of the art performance in a range of tasks from computer vision to natural language processing. However, it is well known that such systems are vulnerable to attackers who craft inputs in order to cause misclassification. The level of perturbation an attacker needs to introduce in order to cause such a misclassification can be extremely small, and often imperceptible. This is of significant security concern, particularly where misclassification can cause harm to humans.   We thus propose Deep Latent Defence, an architecture which seeks to combine adversarial training with a detection system. At its core Deep Latent Defence has a adversarially trained neural network. A series of encoders take the intermediate layer representation of data as it passes though the network and project it to a latent space which we use for detecting adversarial samples via a $k$-nn classifier. We present results using both grey and white box attackers, as well as an adaptive $L_{\infty}$ bounded attack which was constructed specifically to try and evade our defence. We find that even under the strongest attacker model that we have investigated our defence is able to offer significant defensive benefits.

</details>

<details>

<summary>2020-09-27 17:53:53 - ODE guided Neural Data Augmentation Techniques for Time Series Data and its Benefits on Robustness</summary>

- *Anindya Sarkar, Anirudh Sunder Raj, Raghu Sesha Iyengar*

- `1910.06813v3` - [abs](http://arxiv.org/abs/1910.06813v3) - [pdf](http://arxiv.org/pdf/1910.06813v3)

> Exploring adversarial attack vectors and studying their effects on machine learning algorithms has been of interest to researchers. Deep neural networks working with time series data have received lesser interest compared to their image counterparts in this context. In a recent finding, it has been revealed that current state-of-the-art deep learning time series classifiers are vulnerable to adversarial attacks. In this paper, we introduce two local gradient based and one spectral density based time series data augmentation techniques. We show that a model trained with data obtained using our techniques obtains state-of-the-art classification accuracy on various time series benchmarks. In addition, it improves the robustness of the model against some of the most common corruption techniques,such as Fast Gradient Sign Method (FGSM) and Basic Iterative Method (BIM).

</details>

<details>

<summary>2020-09-28 04:32:23 - Gotta Catch 'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks</summary>

- *Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, Ben Y. Zhao*

- `1904.08554v6` - [abs](http://arxiv.org/abs/1904.08554v6) - [pdf](http://arxiv.org/pdf/1904.08554v6)

> Deep neural networks (DNN) are known to be vulnerable to adversarial attacks. Numerous efforts either try to patch weaknesses in trained models, or try to make it difficult or costly to compute adversarial examples that exploit them. In our work, we explore a new "honeypot" approach to protect DNN models. We intentionally inject trapdoors, honeypot weaknesses in the classification manifold that attract attackers searching for adversarial examples. Attackers' optimization algorithms gravitate towards trapdoors, leading them to produce attacks similar to trapdoors in the feature space. Our defense then identifies attacks by comparing neuron activation signatures of inputs to those of trapdoors. In this paper, we introduce trapdoors and describe an implementation of a trapdoor-enabled defense. First, we analytically prove that trapdoors shape the computation of adversarial attacks so that attack inputs will have feature representations very similar to those of trapdoors. Second, we experimentally show that trapdoor-protected models can detect, with high accuracy, adversarial examples generated by state-of-the-art attacks (PGD, optimization-based CW, Elastic Net, BPDA), with negligible impact on normal classification. These results generalize across classification domains, including image, facial, and traffic-sign recognition. We also present significant results measuring trapdoors' robustness against customized adaptive attacks (countermeasures).

</details>

<details>

<summary>2020-09-28 15:26:35 - Crafting Adversarial Examples for Deep Learning Based Prognostics (Extended Version)</summary>

- *Gautam Raj Mode, Khaza Anuarul Hoque*

- `2009.10149v2` - [abs](http://arxiv.org/abs/2009.10149v2) - [pdf](http://arxiv.org/pdf/2009.10149v2)

> In manufacturing, unexpected failures are considered a primary operational risk, as they can hinder productivity and can incur huge losses. State-of-the-art Prognostics and Health Management (PHM) systems incorporate Deep Learning (DL) algorithms and Internet of Things (IoT) devices to ascertain the health status of equipment, and thus reduce the downtime, maintenance cost and increase the productivity. Unfortunately, IoT sensors and DL algorithms, both are vulnerable to cyber attacks, and hence pose a significant threat to PHM systems. In this paper, we adopt the adversarial example crafting techniques from the computer vision domain and apply them to the PHM domain. Specifically, we craft adversarial examples using the Fast Gradient Sign Method (FGSM) and Basic Iterative Method (BIM) and apply them on the Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Convolutional Neural Network (CNN) based PHM models. We evaluate the impact of adversarial attacks using NASA's turbofan engine dataset. The obtained results show that all the evaluated PHM models are vulnerable to adversarial attacks and can cause a serious defect in the remaining useful life estimation. The obtained results also show that the crafted adversarial examples are highly transferable and may cause significant damages to PHM systems.

</details>

<details>

<summary>2020-09-29 06:13:02 - On $\ell_p$-norm Robustness of Ensemble Stumps and Trees</summary>

- *Yihan Wang, Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh*

- `2008.08755v2` - [abs](http://arxiv.org/abs/2008.08755v2) - [pdf](http://arxiv.org/pdf/2008.08755v2)

> Recent papers have demonstrated that ensemble stumps and trees could be vulnerable to small input perturbations, so robustness verification and defense for those models have become an important research problem. However, due to the structure of decision trees, where each node makes decision purely based on one feature value, all the previous works only consider the $\ell_\infty$ norm perturbation. To study robustness with respect to a general $\ell_p$ norm perturbation, one has to consider the correlation between perturbations on different features, which has not been handled by previous algorithms. In this paper, we study the problem of robustness verification and certified defense with respect to general $\ell_p$ norm perturbations for ensemble decision stumps and trees. For robustness verification of ensemble stumps, we prove that complete verification is NP-complete for $p\in(0, \infty)$ while polynomial time algorithms exist for $p=0$ or $\infty$. For $p\in(0, \infty)$ we develop an efficient dynamic programming based algorithm for sound verification of ensemble stumps. For ensemble trees, we generalize the previous multi-level robustness verification algorithm to $\ell_p$ norm. We demonstrate the first certified defense method for training ensemble stumps and trees with respect to $\ell_p$ norm perturbations, and verify its effectiveness empirically on real datasets.

</details>

<details>

<summary>2020-09-29 08:46:26 - Intrusion Detection Framework for SQL Injection</summary>

- *Israr Ali, Syed Hasan Adil, Mansoor Ebrahim*

- `2009.13868v1` - [abs](http://arxiv.org/abs/2009.13868v1) - [pdf](http://arxiv.org/pdf/2009.13868v1)

> In this era of internet, E-Business and e-commerce applications are using Databases as their integral part. These Databases irrespective of the technology used are vulnerable to SQL injection attacks. These Attacks are considered very dangerous as well as very easy to use for attackers and intruders. In this paper, we are proposing a new approach to detect intrusion from attackers by using SQL injection. The main idea of our proposed solution is to create trusted user profiles fetched from the Queries submitted by authorized users by using association rules. After that we will use a hybrid (anomaly + misuse) detection model which will depend on data mining techniques to detect queries that deviates from our normal behavior profile. The normal behavior profile will be created in XML format. In this way we can minimize false positive alarms.

</details>

<details>

<summary>2020-09-29 12:42:09 - NXNSAttack: Recursive DNS Inefficiencies and Vulnerabilities</summary>

- *Yehuda Afek, Anat Bremler-Barr, Lior Shafir*

- `2005.09107v2` - [abs](http://arxiv.org/abs/2005.09107v2) - [pdf](http://arxiv.org/pdf/2005.09107v2)

> This paper exposes a new vulnerability and introduces a corresponding attack, the NoneXistent Name Server Attack (NXNSAttack), that disrupts and may paralyze the DNS system, making it difficult or impossible for Internet users to access websites, web e-mail, online video chats, or any other online resource. The NXNSAttack generates a storm of packets between DNS resolvers and DNS authoritative name servers. The storm is produced by the response of resolvers to unrestricted referral response messages of authoritative name servers. The attack is significantly more destructive than NXDomain attacks (e.g., the Mirai attack): i) It reaches an amplification factor of more than 1620x on the number of packets exchanged by the recursive resolver. ii) In addition to the negative cache, the attack also saturates the 'NS' section of the resolver caches. To mitigate the attack impact, we propose an enhancement to the recursive resolver algorithm, MaxFetch(k), that prevents unnecessary proactive fetches. We implemented the MaxFetch(1) mitigation enhancement on a BIND resolver and tested it on real-world DNS query datasets. Our results show that MaxFetch(1) degrades neither the recursive resolver throughput nor its latency. Following the discovery of the attack, a responsible disclosure procedure was carried out, and several DNS vendors and public providers have issued a CVE and patched their systems.

</details>

<details>

<summary>2020-09-29 14:28:06 - Optimizing Information Loss Towards Robust Neural Networks</summary>

- *Philip Sperl, Konstantin Böttinger*

- `2008.03072v2` - [abs](http://arxiv.org/abs/2008.03072v2) - [pdf](http://arxiv.org/pdf/2008.03072v2)

> Neural Networks (NNs) are vulnerable to adversarial examples. Such inputs differ only slightly from their benign counterparts yet provoke misclassifications of the attacked NNs. The required perturbations to craft the examples are often negligible and even human imperceptible. To protect deep learning-based systems from such attacks, several countermeasures have been proposed with adversarial training still being considered the most effective. Here, NNs are iteratively retrained using adversarial examples forming a computational expensive and time consuming process often leading to a performance decrease. To overcome the downsides of adversarial training while still providing a high level of security, we present a new training approach we call \textit{entropic retraining}. Based on an information-theoretic-inspired analysis, entropic retraining mimics the effects of adversarial training without the need of the laborious generation of adversarial examples. We empirically show that entropic retraining leads to a significant increase in NNs' security and robustness while only relying on the given original data. With our prototype implementation we validate and show the effectiveness of our approach for various NN architectures and data sets.

</details>

<details>

<summary>2020-09-30 00:57:33 - Ethically Collecting Multi-Modal Spontaneous Conversations with People that have Cognitive Impairments</summary>

- *Angus Addlesee, Pierre Albert*

- `2009.14361v1` - [abs](http://arxiv.org/abs/2009.14361v1) - [pdf](http://arxiv.org/pdf/2009.14361v1)

> In order to make spoken dialogue systems (such as Amazon Alexa or Google Assistant) more accessible and naturally interactive for people with cognitive impairments, appropriate data must be obtainable. Recordings of multi-modal spontaneous conversations with vulnerable user groups are scarce however and this valuable data is challenging to collect. Researchers that call for this data are commonly inexperienced in ethical and legal issues around working with vulnerable participants. Additionally, standard recording equipment is insecure and should not be used to capture sensitive data. We spent a year consulting experts on how to ethically capture and share recordings of multi-modal spontaneous conversations with vulnerable user groups. In this paper we provide guidance, collated from these experts, on how to ethically collect such data and we present a new system - "CUSCO" - to capture, transport and exchange sensitive data securely. This framework is intended to be easily followed and implemented to encourage further publications of similar corpora. Using this guide and secure recording system, researchers can review and refine their ethical measures.

</details>

<details>

<summary>2020-09-30 05:29:42 - Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks</summary>

- *Uday Shankar Shanthamallu, Jayaraman J. Thiagarajan, Andreas Spanias*

- `2009.14455v1` - [abs](http://arxiv.org/abs/2009.14455v1) - [pdf](http://arxiv.org/pdf/2009.14455v1)

> Graph Neural Networks (GNNs), a generalization of neural networks to graph-structured data, are often implemented using message passes between entities of a graph. While GNNs are effective for node classification, link prediction and graph classification, they are vulnerable to adversarial attacks, i.e., a small perturbation to the structure can lead to a non-trivial performance degradation. In this work, we propose Uncertainty Matching GNN (UM-GNN), that is aimed at improving the robustness of GNN models, particularly against poisoning attacks to the graph structure, by leveraging epistemic uncertainties from the message passing framework. More specifically, we propose to build a surrogate predictor that does not directly access the graph structure, but systematically extracts reliable knowledge from a standard GNN through a novel uncertainty-matching strategy. Interestingly, this uncoupling makes UM-GNN immune to evasion attacks by design, and achieves significantly improved robustness against poisoning attacks. Using empirical studies with standard benchmarks and a suite of global and target attacks, we demonstrate the effectiveness of UM-GNN, when compared to existing baselines including the state-of-the-art robust GCN.

</details>


## 2020-10

<details>

<summary>2020-10-01 06:58:07 - CoVer: Collaborative Light-Node-Only Verification and Data Availability for Blockchains</summary>

- *Steven Cao, Swanand Kadhe, Kannan Ramchandran*

- `2010.00217v1` - [abs](http://arxiv.org/abs/2010.00217v1) - [pdf](http://arxiv.org/pdf/2010.00217v1)

> Validating a blockchain incurs heavy computation, communication, and storage costs. As a result, clients with limited resources, called light nodes, cannot verify transactions independently and must trust full nodes, making them vulnerable to security attacks. Motivated by this problem, we ask a fundamental question: can light nodes securely validate without any full nodes? We answer affirmatively by proposing CoVer, a decentralized protocol that allows a group of light nodes to collaboratively verify blocks even under a dishonest majority, achieving the same level of security for block validation as full nodes while only requiring a fraction of the work. In particular, work per node scales down proportionally with the number of participants (up to a log factor), resulting in computation, communication, and storage requirements that are sublinear in block size. Our main contributions are light-node-only protocols for fraud proofs and data availability.

</details>

<details>

<summary>2020-10-01 18:07:16 - Biocybersecurity -- A Converging Threat as an Auxiliary to War</summary>

- *Lucas Potter, Orlando Ayala, Xavier-Lewis Palmer*

- `2010.00624v1` - [abs](http://arxiv.org/abs/2010.00624v1) - [pdf](http://arxiv.org/pdf/2010.00624v1)

> Biodefense is the discipline of ensuring biosecurity with respect to select groups of organisms and limiting their spread. This field has increasingly been challenged by novel threats from nature that have been weaponized such as SARS, Anthrax, and similar pathogens, but has emerged victorious through collaboration of national and world health groups. However, it may come under additional stress in the 21st century as the field intersects with the cyberworld -- a world where governments have already been struggling to keep up with cyber attacks from small to state-level actors as cyberthreats have been relied on to level the playing field in international disputes. Disruptions to military logistics and economies through cyberattacks have been able to be done at a mere fraction of economic and moral costs through conventional military means, making it an increasingly tempting means of disruption. In the field of biocybersecurity (BCS), the strengths within biotechnology and cybersecurity merge, along with many of their vulnerabilities, and this could spell increased trouble for biodefense, as novel threats can be synthesized and disseminated in ways that fuse the routes of attacks seen in biosecurity and cybersecurity. Herein, we offer an exploration of how threats in the domain of biocybersecurity may emerge through less foreseen routes as it might be an attractive auxiliary to conventional war. This is done through an analysis of potential payload and delivery methods to develop notional threat vectorizations. We conclude with several paradigms through which to view BCS-based threats.

</details>

<details>

<summary>2020-10-02 11:38:36 - EVMPatch: Timely and Automated Patching of Ethereum Smart Contracts</summary>

- *Michael Rodler, Wenting Li, Ghassan O. Karame, Lucas Davi*

- `2010.00341v2` - [abs](http://arxiv.org/abs/2010.00341v2) - [pdf](http://arxiv.org/pdf/2010.00341v2)

> Recent attacks exploiting errors in smart contract code had devastating consequences thereby questioning the benefits of this technology. It is currently highly challenging to fix errors and deploy a patched contract in time. Instant patching is especially important since smart contracts are always online due to the distributed nature of blockchain systems. They also manage considerable amounts of assets, which are at risk and often beyond recovery after an attack. Existing solutions to upgrade smart contracts depend on manual and error-prone processes. This paper presents a framework, called EVMPatch, to instantly and automatically patch faulty smart contracts. EVMPatch features a bytecode rewriting engine for the popular Ethereum blockchain, and transparently/automatically rewrites common off-the-shelf contracts to upgradable contracts. The proof-of-concept implementation of EVMPatch automatically hardens smart contracts that are vulnerable to integer over/underflows and access control errors, but can be easily extended to cover more bug classes. Our extensive evaluation on 14,000 real-world (vulnerable) contracts demonstrate that our approach successfully blocks attack transactions launched on these contracts, while keeping the intended functionality of the contract intact. We perform a study with experienced software developers, showing that EVMPatch is practical, and reduces the time for converting a given Solidity smart contract to an upgradable contract by 97.6 %, while ensuring functional equivalence to the original contract.

</details>

<details>

<summary>2020-10-02 14:47:17 - Understanding Realistic Attacks on Airborne Collision Avoidance Systems</summary>

- *Matthew Smith, Martin Strohmeier, Vincent Lenders, Ivan Martinovic*

- `2010.01034v1` - [abs](http://arxiv.org/abs/2010.01034v1) - [pdf](http://arxiv.org/pdf/2010.01034v1)

> Airborne collision avoidance systems provide an onboard safety net should normal air traffic control procedures fail to keep aircraft separated. These systems are widely deployed and have been constantly refined over the past three decades, usually in response to near misses or mid-air collisions. Recent years have seen security research increasingly focus on aviation, identifying that key wireless links---some of which are used in collision avoidance---are vulnerable to attack. In this paper, we go one step further to understand whether an attacker can remotely trigger false collision avoidance alarms. Primarily considering the next-generation Airborne Collision Avoidance System X (ACAS X), we adopt a modelling approach to extract attacker constraints from technical standards before simulating collision avoidance attacks against standardized ACAS X code. We find that in 44% of cases, an attacker can successfully trigger a collision avoidance alert which on average results in a 590 ft altitude deviation; when the aircraft is at lower altitudes, this success rate rises considerably to 79%. Furthermore, we show how our simulation approach can be used to help defend against attacks by identifying where attackers are most likely to be successful.

</details>

<details>

<summary>2020-10-04 22:44:04 - IoT Malware Network Traffic Classification using Visual Representation and Deep Learning</summary>

- *Gueltoum Bendiab, Stavros Shiaeles, Abdulrahman Alruban, Nicholas Kolokotronis*

- `2010.01712v1` - [abs](http://arxiv.org/abs/2010.01712v1) - [pdf](http://arxiv.org/pdf/2010.01712v1)

> With the increase of IoT devices and technologies coming into service, Malware has risen as a challenging threat with increased infection rates and levels of sophistication. Without strong security mechanisms, a huge amount of sensitive data is exposed to vulnerabilities, and therefore, easily abused by cybercriminals to perform several illegal activities. Thus, advanced network security mechanisms that are able of performing a real-time traffic analysis and mitigation of malicious traffic are required. To address this challenge, we are proposing a novel IoT malware traffic analysis approach using deep learning and visual representation for faster detection and classification of new malware (zero-day malware). The detection of malicious network traffic in the proposed approach works at the package level, significantly reducing the time of detection with promising results due to the deep learning technologies used. To evaluate our proposed method performance, a dataset is constructed which consists of 1000 pcap files of normal and malware traffic that are collected from different network traffic sources. The experimental results of Residual Neural Network (ResNet50) are very promising, providing a 94.50% accuracy rate for detection of malware traffic.

</details>

<details>

<summary>2020-10-05 05:25:32 - UNIFUZZ: A Holistic and Pragmatic Metrics-Driven Platform for Evaluating Fuzzers</summary>

- *Yuwei Li, Shouling Ji, Yuan Chen, Sizhuang Liang, Wei-Han Lee, Yueyao Chen, Chenyang Lyu, Chunming Wu, Raheem Beyah, Peng Cheng, Kangjie Lu, Ting Wang*

- `2010.01785v1` - [abs](http://arxiv.org/abs/2010.01785v1) - [pdf](http://arxiv.org/pdf/2010.01785v1)

> A flurry of fuzzing tools (fuzzers) have been proposed in the literature, aiming at detecting software vulnerabilities effectively and efficiently. To date, it is however still challenging to compare fuzzers due to the inconsistency of the benchmarks, performance metrics, and/or environments for evaluation, which buries the useful insights and thus impedes the discovery of promising fuzzing primitives. In this paper, we design and develop UNIFUZZ, an open-source and metrics-driven platform for assessing fuzzers in a comprehensive and quantitative manner. Specifically, UNIFUZZ to date has incorporated 35 usable fuzzers, a benchmark of 20 real-world programs, and six categories of performance metrics. We first systematically study the usability of existing fuzzers, find and fix a number of flaws, and integrate them into UNIFUZZ. Based on the study, we propose a collection of pragmatic performance metrics to evaluate fuzzers from six complementary perspectives. Using UNIFUZZ, we conduct in-depth evaluations of several prominent fuzzers including AFL [1], AFLFast [2], Angora [3], Honggfuzz [4], MOPT [5], QSYM [6], T-Fuzz [7] and VUzzer64 [8]. We find that none of them outperforms the others across all the target programs, and that using a single metric to assess the performance of a fuzzer may lead to unilateral conclusions, which demonstrates the significance of comprehensive metrics. Moreover, we identify and investigate previously overlooked factors that may significantly affect a fuzzer's performance, including instrumentation methods and crash analysis tools. Our empirical results show that they are critical to the evaluation of a fuzzer. We hope that our findings can shed light on reliable fuzzing evaluation, so that we can discover promising fuzzing primitives to effectively facilitate fuzzer designs in the future.

</details>

<details>

<summary>2020-10-05 13:47:45 - Adversarial Boot Camp: label free certified robustness in one epoch</summary>

- *Ryan Campbell, Chris Finlay, Adam M Oberman*

- `2010.02508v1` - [abs](http://arxiv.org/abs/2010.02508v1) - [pdf](http://arxiv.org/pdf/2010.02508v1)

> Machine learning models are vulnerable to adversarial attacks. One approach to addressing this vulnerability is certification, which focuses on models that are guaranteed to be robust for a given perturbation size. A drawback of recent certified models is that they are stochastic: they require multiple computationally expensive model evaluations with random noise added to a given input. In our work, we present a deterministic certification approach which results in a certifiably robust model. This approach is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We achieve certified models on ImageNet-1k by retraining a model with this loss for one epoch without the use of label information.

</details>

<details>

<summary>2020-10-06 01:37:47 - Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior</summary>

- *Hu Zhang, Linchao Zhu, Yi Zhu, Yi Yang*

- `2003.07637v2` - [abs](http://arxiv.org/abs/2003.07637v2) - [pdf](http://arxiv.org/pdf/2003.07637v2)

> Deep neural networks are known to be susceptible to adversarial noise, which are tiny and imperceptible perturbations. Most of previous work on adversarial attack mainly focus on image models, while the vulnerability of video models is less explored. In this paper, we aim to attack video models by utilizing intrinsic movement pattern and regional relative motion among video frames. We propose an effective motion-excited sampler to obtain motion-aware noise prior, which we term as sparked prior. Our sparked prior underlines frame correlations and utilizes video dynamics via relative motion. By using the sparked prior in gradient estimation, we can successfully attack a variety of video classification models with fewer number of queries. Extensive experimental results on four benchmark datasets validate the efficacy of our proposed method.

</details>

<details>

<summary>2020-10-06 13:03:49 - Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder</summary>

- *Alvin Chan, Yi Tay, Yew-Soon Ong, Aston Zhang*

- `2010.02684v1` - [abs](http://arxiv.org/abs/2010.02684v1) - [pdf](http://arxiv.org/pdf/2010.02684v1)

> This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a 'backdoor poisoning' attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned training samples by poison injection in latent space. Just by adding 1% poisoned data, our experiments show that a victim BERT finetuned classifier's predictions can be steered to the poison target class with success rates of >80% when the input hypothesis is injected with the poison signature, demonstrating that NLI and text classification systems face a huge security risk.

</details>

<details>

<summary>2020-10-06 22:56:22 - Adversarial Patch Attacks on Monocular Depth Estimation Networks</summary>

- *Koichiro Yamanaka, Ryutaroh Matsumoto, Keita Takahashi, Toshiaki Fujii*

- `2010.03072v1` - [abs](http://arxiv.org/abs/2010.03072v1) - [pdf](http://arxiv.org/pdf/2010.03072v1)

> Thanks to the excellent learning capability of deep convolutional neural networks (CNN), monocular depth estimation using CNNs has achieved great success in recent years. However, depth estimation from a monocular image alone is essentially an ill-posed problem, and thus, it seems that this approach would have inherent vulnerabilities. To reveal this limitation, we propose a method of adversarial patch attack on monocular depth estimation. More specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Our method can be implemented in the real world by physically placing the printed patterns in real scenes. We also analyze the behavior of monocular depth estimation under attacks by visualizing the activation levels of the intermediate layers and the regions potentially affected by the adversarial attack.

</details>

<details>

<summary>2020-10-07 04:05:00 - Robust and Interpretable Grounding of Spatial References with Relation Networks</summary>

- *Tsung-Yen Yang, Andrew S. Lan, Karthik Narasimhan*

- `2005.00696v2` - [abs](http://arxiv.org/abs/2005.00696v2) - [pdf](http://arxiv.org/pdf/2005.00696v2)

> Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for spatial concepts. However, the lack of explicit reasoning over entities makes such approaches vulnerable to noise in input text or state observations. In this paper, we develop effective models for understanding spatial references in text that are robust and interpretable, without sacrificing performance. We design a text-conditioned \textit{relation network} whose parameters are dynamically computed with a cross-modal attention module to capture fine-grained spatial relations between entities. This design choice provides interpretability of learned intermediate outputs. Experiments across three tasks demonstrate that our model achieves superior performance, with a 17\% improvement in predicting goal locations and a 15\% improvement in robustness compared to state-of-the-art systems.

</details>

<details>

<summary>2020-10-07 08:50:10 - Assessing Robustness of Text Classification through Maximal Safe Radius Computation</summary>

- *Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn, Marta Kwiatkowska*

- `2010.02004v2` - [abs](http://arxiv.org/abs/2010.02004v2) - [pdf](http://arxiv.org/pdf/2010.02004v2)

> Neural network NLP models are vulnerable to small modifications of the input that maintain the original meaning but result in a different prediction. In this paper, we focus on robustness of text classification against word substitutions, aiming to provide guarantees that the model prediction does not change if a word is replaced with a plausible alternative, such as a synonym. As a measure of robustness, we adopt the notion of the maximal safe radius for a given input text, which is the minimum distance in the embedding space to the decision boundary. Since computing the exact maximal safe radius is not feasible in practice, we instead approximate it by computing a lower and upper bound. For the upper bound computation, we employ Monte Carlo Tree Search in conjunction with syntactic filtering to analyse the effect of single and multiple word substitutions. The lower bound computation is achieved through an adaptation of the linear bounding techniques implemented in tools CNN-Cert and POPQORN, respectively for convolutional and recurrent network models. We evaluate the methods on sentiment analysis and news classification models for four datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and provide an analysis of robustness trends. We also apply our framework to interpretability analysis and compare it with LIME.

</details>

<details>

<summary>2020-10-07 09:08:51 - Double Targeted Universal Adversarial Perturbations</summary>

- *Philipp Benz, Chaoning Zhang, Tooba Imtiaz, In So Kweon*

- `2010.03288v1` - [abs](http://arxiv.org/abs/2010.03288v1) - [pdf](http://arxiv.org/pdf/2010.03288v1)

> Despite their impressive performance, deep neural networks (DNNs) are widely known to be vulnerable to adversarial attacks, which makes it challenging for them to be deployed in security-sensitive applications, such as autonomous driving. Image-dependent perturbations can fool a network for one specific image, while universal adversarial perturbations are capable of fooling a network for samples from all classes without selection. We introduce a double targeted universal adversarial perturbations (DT-UAPs) to bridge the gap between the instance-discriminative image-dependent perturbations and the generic universal perturbations. This universal perturbation attacks one targeted source class to sink class, while having a limited adversarial effect on other non-targeted source classes, for avoiding raising suspicions. Targeting the source and sink class simultaneously, we term it double targeted attack (DTA). This provides an attacker with the freedom to perform precise attacks on a DNN model while raising little suspicion. We show the effectiveness of the proposed DTA algorithm on a wide range of datasets and also demonstrate its potential as a physical attack.

</details>

<details>

<summary>2020-10-07 15:43:26 - Adversarial Attacks and Defenses: An Interpretation Perspective</summary>

- *Ninghao Liu, Mengnan Du, Ruocheng Guo, Huan Liu, Xia Hu*

- `2004.11488v2` - [abs](http://arxiv.org/abs/2004.11488v2) - [pdf](http://arxiv.org/pdf/2004.11488v2)

> Despite the recent advances in a wide spectrum of applications, machine learning models, especially deep neural networks, have been shown to be vulnerable to adversarial attacks. Attackers add carefully-crafted perturbations to input, where the perturbations are almost imperceptible to humans, but can cause models to make wrong predictions. Techniques to protect models against adversarial input are called adversarial defense methods. Although many approaches have been proposed to study adversarial attacks and defenses in different scenarios, an intriguing and crucial challenge remains that how to really understand model vulnerability? Inspired by the saying that "if you know yourself and your enemy, you need not fear the battles", we may tackle the aforementioned challenge after interpreting machine learning models to open the black-boxes. The goal of model interpretation, or interpretable machine learning, is to extract human-understandable terms for the working mechanism of models. Recently, some approaches start incorporating interpretation into the exploration of adversarial attacks and defenses. Meanwhile, we also observe that many existing methods of adversarial attacks and defenses, although not explicitly claimed, can be understood from the perspective of interpretation. In this paper, we review recent work on adversarial attacks and defenses, particularly from the perspective of machine learning interpretation. We categorize interpretation into two types, feature-level interpretation and model-level interpretation. For each type of interpretation, we elaborate on how it could be used for adversarial attacks and defenses. We then briefly illustrate additional correlations between interpretation and adversaries. Finally, we discuss the challenges and future directions along tackling adversary issues with interpretation.

</details>

<details>

<summary>2020-10-07 18:24:29 - Downscaling Attack and Defense: Turning What You See Back Into What You Get</summary>

- *Andrew J. Lohn*

- `2010.02456v2` - [abs](http://arxiv.org/abs/2010.02456v2) - [pdf](http://arxiv.org/pdf/2010.02456v2)

> The resizing of images, which is typically a required part of preprocessing for computer vision systems, is vulnerable to attack. Images can be created such that the image is completely different at machine-vision scales than at other scales and the default settings for some common computer vision and machine learning systems are vulnerable. We show that defenses exist and are trivial to administer provided that defenders are aware of the threat. These attacks and defenses help to establish the role of input sanitization in machine learning.

</details>

<details>

<summary>2020-10-07 21:55:10 - Adversarial Attacks to Machine Learning-Based Smart Healthcare Systems</summary>

- *AKM Iqtidar Newaz, Nur Imtiazul Haque, Amit Kumar Sikder, Mohammad Ashiqur Rahman, A. Selcuk Uluagac*

- `2010.03671v1` - [abs](http://arxiv.org/abs/2010.03671v1) - [pdf](http://arxiv.org/pdf/2010.03671v1)

> The increasing availability of healthcare data requires accurate analysis of disease diagnosis, progression, and realtime monitoring to provide improved treatments to the patients. In this context, Machine Learning (ML) models are used to extract valuable features and insights from high-dimensional and heterogeneous healthcare data to detect different diseases and patient activities in a Smart Healthcare System (SHS). However, recent researches show that ML models used in different application domains are vulnerable to adversarial attacks. In this paper, we introduce a new type of adversarial attacks to exploit the ML classifiers used in a SHS. We consider an adversary who has partial knowledge of data distribution, SHS model, and ML algorithm to perform both targeted and untargeted attacks. Employing these adversarial capabilities, we manipulate medical device readings to alter patient status (disease-affected, normal condition, activities, etc.) in the outcome of the SHS. Our attack utilizes five different adversarial ML algorithms (HopSkipJump, Fast Gradient Method, Crafting Decision Tree, Carlini & Wagner, Zeroth Order Optimization) to perform different malicious activities (e.g., data poisoning, misclassify outputs, etc.) on a SHS. Moreover, based on the training and testing phase capabilities of an adversary, we perform white box and black box attacks on a SHS. We evaluate the performance of our work in different SHS settings and medical devices. Our extensive evaluation shows that our proposed adversarial attack can significantly degrade the performance of a ML-based SHS in detecting diseases and normal activities of the patients correctly, which eventually leads to erroneous treatment.

</details>

<details>

<summary>2020-10-08 03:39:19 - An Optimal Graph-Search Method for Secure State Estimation</summary>

- *Xusheng Luo, Miroslav Pajic, Michael M. Zavlanos*

- `1903.10620v4` - [abs](http://arxiv.org/abs/1903.10620v4) - [pdf](http://arxiv.org/pdf/1903.10620v4)

> The growing complexity of modern Cyber-Physical Systems (CPS) and the frequent communication between their components make them vulnerable to malicious attacks. As a result, secure state estimation is a critical requirement for the control of these systems. Many existing secure state estimation methods suffer from combinatorial complexity which grows with the number of states and sensors in the system. This complexity can be mitigated using optimization-based methods that relax the original state estimation problem, although at the cost of optimality as these methods often identify attack-free sensors as attacked. In this paper, we propose a new optimal graph-search algorithm to correctly identify malicious attacks and to securely estimate the states even in large-scale CPS modeled as linear time-invariant systems. The graph consists of layers, each one containing two nodes capturing a truth assignment of any given sensor, and directed edges connecting adjacent layers only. Then, our algorithm searches the layers of this graph incrementally, favoring directions at higher layers with more attack-free assignments, while actively managing a repository of nodes to be expanded at later iterations. The proposed search bias and the ability to revisit nodes in the repository and self-correct, allow our graph-search algorithm to reach the optimal assignment faster and tackle larger problems. We show that our algorithm is complete and optimal provided that process and measurement noises do not dominate the attack signal. Moreover, we provide numerical simulations that demonstrate the ability of our algorithm to correctly identify attacked sensors and securely reconstruct the state. Our simulations show that our method outperforms existing algorithms both in terms of optimality and execution time.

</details>

<details>

<summary>2020-10-08 08:57:57 - Improve Adversarial Robustness via Weight Penalization on Classification Layer</summary>

- *Cong Xu, Dan Li, Min Yang*

- `2010.03844v1` - [abs](http://arxiv.org/abs/2010.03844v1) - [pdf](http://arxiv.org/pdf/2010.03844v1)

> It is well-known that deep neural networks are vulnerable to adversarial attacks. Recent studies show that well-designed classification parts can lead to better robustness. However, there is still much space for improvement along this line. In this paper, we first prove that, from a geometric point of view, the robustness of a neural network is equivalent to some angular margin condition of the classifier weights. We then explain why ReLU type function is not a good choice for activation under this framework. These findings reveal the limitations of the existing approaches and lead us to develop a novel light-weight-penalized defensive method, which is simple and has a good scalability. Empirical results on multiple benchmark datasets demonstrate that our method can effectively improve the robustness of the network without requiring too much additional computation, while maintaining a high classification precision for clean data.

</details>

<details>

<summary>2020-10-08 14:48:14 - Upper Esophageal Sphincter Opening Segmentation with Convolutional Recurrent Neural Networks in High Resolution Cervical Auscultation</summary>

- *Yassin Khalifa, Cara Donohue, James L. Coyle, Ervin Sejdić*

- `2010.04541v1` - [abs](http://arxiv.org/abs/2010.04541v1) - [pdf](http://arxiv.org/pdf/2010.04541v1)

> Upper esophageal sphincter is an important anatomical landmark of the swallowing process commonly observed through the kinematic analysis of radiographic examinations that are vulnerable to subjectivity and clinical feasibility issues. Acting as the doorway of esophagus, upper esophageal sphincter allows the transition of ingested materials from pharyngeal into esophageal stages of swallowing and a reduced duration of opening can lead to penetration/aspiration and/or pharyngeal residue. Therefore, in this study we consider a non-invasive high resolution cervical auscultation-based screening tool to approximate the human ratings of upper esophageal sphincter opening and closure. Swallows were collected from 116 patients and a deep neural network was trained to produce a mask that demarcates the duration of upper esophageal sphincter opening. The proposed method achieved more than 90\% accuracy and similar values of sensitivity and specificity when compared to human ratings even when tested over swallows from an independent clinical experiment. Moreover, the predicted opening and closure moments surprisingly fell within an inter-human comparable error of their human rated counterparts which demonstrates the clinical significance of high resolution cervical auscultation in replacing ionizing radiation-based evaluation of swallowing kinematics.

</details>

<details>

<summary>2020-10-08 15:03:29 - Texture-based Presentation Attack Detection for Automatic Speaker Verification</summary>

- *Lazaro J. Gonzalez-Soler, Jose Patino, Marta Gomez-Barrero, Massimiliano Todisco, Christoph Busch, Nicholas Evans*

- `2010.04038v1` - [abs](http://arxiv.org/abs/2010.04038v1) - [pdf](http://arxiv.org/pdf/2010.04038v1)

> Biometric systems are nowadays employed across a broad range of applications. They provide high security and efficiency and, in many cases, are user friendly. Despite these and other advantages, biometric systems in general and Automatic speaker verification (ASV) systems in particular can be vulnerable to attack presentations. The most recent ASVSpoof 2019 competition showed that most forms of attacks can be detected reliably with ensemble classifier-based presentation attack detection (PAD) approaches. These, though, depend fundamentally upon the complementarity of systems in the ensemble. With the motivation to increase the generalisability of PAD solutions, this paper reports our exploration of texture descriptors applied to the analysis of speech spectrogram images. In particular, we propose a common fisher vector feature space based on a generative model. Experimental results show the soundness of our approach: at most, 16 in 100 bona fide presentations are rejected whereas only one in 100 attack presentations are accepted.

</details>

<details>

<summary>2020-10-08 18:59:19 - Affine-Invariant Robust Training</summary>

- *Oriol Barbany Mayor*

- `2010.04216v1` - [abs](http://arxiv.org/abs/2010.04216v1) - [pdf](http://arxiv.org/pdf/2010.04216v1)

> The field of adversarial robustness has attracted significant attention in machine learning. Contrary to the common approach of training models that are accurate in average case, it aims at training models that are accurate for worst case inputs, hence it yields more robust and reliable models. Put differently, it tries to prevent an adversary from fooling a model. The study of adversarial robustness is largely focused on $\ell_p-$bounded adversarial perturbations, i.e. modifications of the inputs, bounded in some $\ell_p$ norm. Nevertheless, it has been shown that state-of-the-art models are also vulnerable to other more natural perturbations such as affine transformations, which were already considered in machine learning within data augmentation. This project reviews previous work in spatial robustness methods and proposes evolution strategies as zeroth order optimization algorithms to find the worst affine transforms for each input. The proposed method effectively yields robust models and allows introducing non-parametric adversarial perturbations.

</details>

<details>

<summary>2020-10-08 22:03:18 - Comments on the "Generalized" KLJN Key Exchanger with Arbitrary Resistors: Power, Impedance, Security</summary>

- *Shahriar Ferdous, Christiana Chamon, Laszlo B. Kish*

- `2010.04280v1` - [abs](http://arxiv.org/abs/2010.04280v1) - [pdf](http://arxiv.org/pdf/2010.04280v1)

> In (Nature) Science Report 5 (2015) 13653, Vadai, Mingesz and Gingl (VMG) introduce a new Kirchhoff-law-Johnson-noise (KLJN) secure key exchanger that operates with 4 arbitrary resistors (instead of 2 arbitrary resistance values forming 2 identical resistor pairs in the original system). They state that in this new, VMG-KLJN, non-equilibrium system with nonzero power flow, the security during the exchange of the two (HL and LH) bit values is as strong as in the original KLJN scheme. Moreover, they claim that, at practical conditions, their VMG-KLJN protocol "supports more robust protection against attacks". First, we investigate the power flow and thermal equilibrium issues of the VMG-KLJN system with 4 arbitrary resistors. Then we introduce a new KLJN protocol that allows the arbitrary choice of 3 resistors from the 4, while it still operates with zero power flow during the exchange of single bits by utilizing a specific value of the 4th resistor and a binary temperature set for the exchanged (HL and LH) bit values. Then we show that, in general, the KLJN schemes with more than 2 arbitrary resistors (including our new protocol mentioned above) are prone to 4 new passive attacks utilizing the parasitic capacitance and inductance in the cable, while the original KLJN scheme is naturally immune against these new attacks. The core of the security vulnerability exploited by these attacks is the different line resistances in the HL and LH cases. Therefore, on the contrary of the statement and claim cited above, the practical VMG-KLJN system is less secure than the original KLJN scheme. We introduce another 2, modified, non-equilibrium KLJN systems to eliminate the vulnerability against some - but not all - of these attacks. However the price for that is the loss of arbitrariness of the selection of the 4th resistor and the information leak still remains greater than zero.

</details>

<details>

<summary>2020-10-08 23:12:25 - You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion</summary>

- *Roei Schuster, Congzheng Song, Eran Tromer, Vitaly Shmatikov*

- `2007.02220v3` - [abs](http://arxiv.org/abs/2007.02220v3) - [pdf](http://arxiv.org/pdf/2007.02220v3)

> Code autocompletion is an integral feature of modern code editors and IDEs. The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely (not just statically feasible) completions given the current context.   We demonstrate that neural code autocompleters are vulnerable to poisoning attacks. By adding a few specially-crafted files to the autocompleter's training corpus (data poisoning), or else by directly fine-tuning the autocompleter on these files (model poisoning), the attacker can influence its suggestions for attacker-chosen contexts. For example, the attacker can "teach" the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption. Moreover, we show that these attacks can be targeted: an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer.   We quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2. We then evaluate existing defenses against poisoning attacks and show that they are largely ineffective.

</details>

<details>

<summary>2020-10-09 04:33:45 - On the Security of Group Communication Schemes</summary>

- *Shouhuai Xu*

- `2010.05692v1` - [abs](http://arxiv.org/abs/2010.05692v1) - [pdf](http://arxiv.org/pdf/2010.05692v1)

> Secure group communications are a mechanism facilitating protected transmission of messages from a sender to multiple receivers, and many emerging applications in both wired and wireless networks need the support of such a mechanism. There have been many secure group communication schemes in wired networks, which can be directly adopted in, or appropriately adapted to, wireless networks such as mobile ad hoc networks (MANETs) and sensor networks. In this paper we show that the popular group communication schemes that we have examined are vulnerable to the following attack: An outside adversary who compromises a certain legitimate group member could obtain {\em all} past and present group keys (and thus all the messages protected by them); this is in sharp contrast to the widely-accepted belief that a such adversary can only obtain the present group key (and thus the messages protected by it). In order to understand and deal with the attack, we formalize two security models for stateful and stateless group communication schemes. We show that some practical methods can make a {\em subclass} of existing group communication schemes immune to the attack.

</details>

<details>

<summary>2020-10-09 09:32:31 - Facebook Ads: Politics of Migration in Italy</summary>

- *Arthur Capozzi, Gianmarco De Francisci Morales, Yelena Mejova, Corrado Monti, Andre Panisson, Daniela Paolotti*

- `2010.04458v1` - [abs](http://arxiv.org/abs/2010.04458v1) - [pdf](http://arxiv.org/pdf/2010.04458v1)

> Targeted online advertising is on the forefront of political communication, allowing hyper-local advertising campaigns around elections and issues. In this study, we employ a new resource for political ad monitoring -- Facebook Ads Library -- to examine advertising concerning the issue of immigration in Italy. A crucial topic in Italian politics, it has recently been a focus of several populist movements, some of which have adopted social media as a powerful tool for voter engagement. Indeed, we find evidence of targeting by the parties both in terms of geography and demographics (age and gender). For instance, Five Star Movement reaches a younger audience when advertising about immigration, while other parties' ads have a more male audience when advertising on this issue. We also notice a marked rise in advertising volume around elections, as well as a shift to more general audience. Thus, we illustrate political advertising targeting that likely has an impact on public opinion on a topic involving potentially vulnerable populations, and urge the research community to include online advertising in the monitoring of public discourse.

</details>

<details>

<summary>2020-10-11 10:13:10 - SIGNED: A Challenge-Response Based Interrogation Scheme for Simultaneous Watermarking and Trojan Detection</summary>

- *Abhishek Nair, Patanjali SLPSK, Chester Rebeiro, Swarup Bhunia*

- `2010.05209v1` - [abs](http://arxiv.org/abs/2010.05209v1) - [pdf](http://arxiv.org/pdf/2010.05209v1)

> The emergence of distributed manufacturing ecosystems for electronic hardware involving untrusted parties has given rise to diverse trust issues. In particular, IP piracy, overproduction, and hardware Trojan attacks pose significant threats to digital design manufacturers. Watermarking has been one of the solutions employed by the semiconductor industry to overcome many of the trust issues. However, current watermarking techniques have low coverage, incur hardware overheads, and are vulnerable to removal or tampering attacks. Additionally, these watermarks cannot detect Trojan implantation attacks where an adversary alters a design for malicious purposes. We address these issues in our framework called SIGNED: Secure Lightweight Watermarking Scheme for Digital Designs.   SIGNED relies on a challenge-response protocol based interrogation scheme for generating the watermark. SIGNED identifies sensitive regions in the target netlist and samples them to form a compact signature that is representative of the functional and structural characteristics of a design. We show that this signature can be used to simultaneously verify, in a robust manner, the provenance of a design, as well as any malicious alterations to it at any stage during design process. We evaluate SIGNED on the ISCAS85 and ITC benchmark circuits and obtain a detection accuracy of 87.61\% even for modifications as low as 5-gates. We further demonstrate that SIGNED can benefit from integration with a logic locking solution, where it can achieve increased protection against removal/tempering attacks and incurs lower overhead through judicious reuse of the locking logic for watermark creation.

</details>

<details>

<summary>2020-10-12 08:43:41 - Boundary Value Exploration for Software Analysis</summary>

- *Felix Dobslaw, Francisco Gomes de Oliveira Neto, Robert Feldt*

- `2001.06652v2` - [abs](http://arxiv.org/abs/2001.06652v2) - [pdf](http://arxiv.org/pdf/2001.06652v2)

> For software to be reliable and resilient, it is widely accepted that tests must be created and maintained alongside the software itself. One safeguard from vulnerabilities and failures in code is to ensure correct behavior on the boundaries between the input space sub-domains. So-called boundary value analysis (BVA) and boundary value testing (BVT) techniques aim to exercise those boundaries and increase test effectiveness. However, the concepts of BVA and BVT themselves are not generally well defined, and it is not clear how to identify relevant sub-domains, and thus the boundaries delineating them, given a specification. This has limited adoption and hindered automation. We clarify BVA and BVT and introduce Boundary Value Exploration (BVE) to describe techniques that support them by helping to detect and identify boundary inputs. Additionally, we propose two concrete BVE techniques based on information-theoretic distance functions: (i) an algorithm for boundary detection and (ii) the usage of software visualization to explore the behavior of the software under test and identify its boundary behavior. As an initial evaluation, we apply these techniques on a much used and well-tested date handling library. Our results reveal questionable behavior at boundaries highlighted by our techniques. In conclusion, we argue that the boundary value exploration that our techniques enable is a step towards automated boundary value analysis and testing, fostering their wider use and improving test effectiveness and efficiency.

</details>

<details>

<summary>2020-10-12 12:55:37 - PoisonIvy: (In)secure Practices of Enterprise IoT Systems in Smart Buildings</summary>

- *Luis Puche Rondon, Leonardo Babun, Ahmet Aris, Kemal Akkaya, A. Selcuk Uluagac*

- `2010.05658v1` - [abs](http://arxiv.org/abs/2010.05658v1) - [pdf](http://arxiv.org/pdf/2010.05658v1)

> The rise of IoT devices has led to the proliferation of smart buildings, offices, and homes worldwide. Although commodity IoT devices are employed by ordinary end-users, complex environments such as smart buildings, smart offices, conference rooms, or hospitality require customized and highly reliable solutions. Those systems called Enterprise Internet of Things (EIoT) connect such environments to the Internet and are professionally managed solutions usually offered by dedicated vendors. As EIoT systems require specialized training, software, and equipment to deploy, this has led to very little research investigating the security of EIoT systems and their components. In effect, EIoT systems in smart settings such as smart buildings present an unprecedented and unexplored threat vector for an attacker. In this work, we explore EIoT system vulnerabilities and insecure development practices. Specifically, focus on the usage of drivers as an attack mechanism, and introduce PoisonIvy, a number of novel attacks that demonstrate an attacker can easily compromise EIoT system controllers using malicious drivers. Specifically, we show how drivers used to integrate third-party devices to EIoT systems can be misused in a systematic fashion. To demonstrate the capabilities of attackers, we implement and evaluate PoisonIvy using a testbed of real EIoT devices. We show that an attacker can perform DoS attacks, gain remote control, and maliciously abuse system resources of EIoT systems. To the best of our knowledge, this is the first work to analyze the (in)securities of EIoT deployment practices and demonstrate the associated vulnerabilities in this ecosystem. With this work, we raise awareness on the (in)secure development practices used for EIoT systems, the consequences of which can largely impact the security, privacy, reliability, and performance of millions of EIoT systems worldwide.

</details>

<details>

<summary>2020-10-12 18:36:43 - Adversarial Examples for Models of Code</summary>

- *Noam Yefet, Uri Alon, Eran Yahav*

- `1910.07517v5` - [abs](http://arxiv.org/abs/1910.07517v5) - [pdf](http://arxiv.org/pdf/1910.07517v5)

> Neural models of code have shown impressive results when performing tasks such as predicting method names and identifying certain kinds of bugs. We show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code using adversarial examples. The main idea of our approach is to force a given trained model to make an incorrect prediction, as specified by the adversary, by introducing small perturbations that do not change the program's semantics, thereby creating an adversarial example. To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs (DAMP). DAMP works by deriving the desired prediction with respect to the model's inputs, while holding the model weights constant, and following the gradients to slightly modify the input code. We show that our DAMP attack is effective across three neural architectures: code2vec, GGNN, and GNN-FiLM, in both Java and C#. Our evaluations demonstrate that DAMP has up to 89% success rate in changing a prediction to the adversary's choice (a targeted attack) and a success rate of up to 94% in changing a given prediction to any incorrect prediction (a non-targeted attack). To defend a model against such attacks, we empirically examine a variety of possible defenses and discuss their trade-offs. We show that some of these defenses can dramatically drop the success rate of the attacker, with a minor penalty of 2% relative degradation in accuracy when they are not performing under attack. Our code, data, and trained models are available at https://github.com/tech-srl/adversarial-examples .

</details>

<details>

<summary>2020-10-13 00:28:21 - CurbScan: Curb Detection and Tracking Using Multi-Sensor Fusion</summary>

- *Iljoo Baek, Tzu-Chieh Tai, Manoj Bhat, Karun Ellango, Tarang Shah, Kamal Fuseini, Ragunathan, Rajkumar*

- `2010.04837v2` - [abs](http://arxiv.org/abs/2010.04837v2) - [pdf](http://arxiv.org/pdf/2010.04837v2)

> Reliable curb detection is critical for safe autonomous driving in urban contexts. Curb detection and tracking are also useful in vehicle localization and path planning. Past work utilized a 3D LiDAR sensor to determine accurate distance information and the geometric attributes of curbs. However, such an approach requires dense point cloud data and is also vulnerable to false positives from obstacles present on both road and off-road areas. In this paper, we propose an approach to detect and track curbs by fusing together data from multiple sensors: sparse LiDAR data, a mono camera and low-cost ultrasonic sensors. The detection algorithm is based on a single 3D LiDAR and a mono camera sensor used to detect candidate curb features and it effectively removes false positives arising from surrounding static and moving obstacles. The detection accuracy of the tracking algorithm is boosted by using Kalman filter-based prediction and fusion with lateral distance information from low-cost ultrasonic sensors. We next propose a line-fitting algorithm that yields robust results for curb locations. Finally, we demonstrate the practical feasibility of our solution by testing in different road environments and evaluating our implementation in a real vehicle\footnote{Demo video clips demonstrating our algorithm have been uploaded to Youtube: https://www.youtube.com/watch?v=w5MwsdWhcy4, https://www.youtube.com/watch?v=Gd506RklfG8.}. Our algorithm maintains over 90\% accuracy within 4.5-22 meters and 0-14 meters for the KITTI dataset and our dataset respectively, and its average processing time per frame is approximately 10 ms on Intel i7 x86 and 100ms on NVIDIA Xavier board.

</details>

<details>

<summary>2020-10-13 09:33:36 - A Framework of Randomized Selection Based Certified Defenses Against Data Poisoning Attacks</summary>

- *Ruoxin Chen, Jie Li, Chentao Wu, Bin Sheng, Ping Li*

- `2009.08739v2` - [abs](http://arxiv.org/abs/2009.08739v2) - [pdf](http://arxiv.org/pdf/2009.08739v2)

> Neural network classifiers are vulnerable to data poisoning attacks, as attackers can degrade or even manipulate their predictions thorough poisoning only a few training samples. However, the robustness of heuristic defenses is hard to measure. Random selection based defenses can achieve certified robustness by averaging the classifiers' predictions on the sub-datasets sampled from the training set. This paper proposes a framework of random selection based certified defenses against data poisoning attacks. Specifically, we prove that the random selection schemes that satisfy certain conditions are robust against data poisoning attacks. We also derive the analytical form of the certified radius for the qualified random selection schemes. The certified radius of bagging derived by our framework is tighter than the previous work. Our framework allows users to improve robustness by leveraging prior knowledge about the training set and the poisoning model. Given higher level of prior knowledge, we can achieve higher certified accuracy both theoretically and practically. According to the experiments on three benchmark datasets: MNIST 1/7, MNIST, and CIFAR-10, our method outperforms the state-of-the-art.

</details>

<details>

<summary>2020-10-13 15:11:33 - Who Watches the Watchmen? A Review of Subjective Approaches for Sybil-resistance in Proof of Personhood Protocols</summary>

- *Divya Siddarth, Sergey Ivliev, Santiago Siri, Paula Berman*

- `2008.05300v5` - [abs](http://arxiv.org/abs/2008.05300v5) - [pdf](http://arxiv.org/pdf/2008.05300v5)

> Most current self-sovereign identity systems may be categorized as strictly objective, consisting of cryptographically signed statements issued by trusted third party attestors. This failure to provide an input for subjectivity accounts for a central challenge: the inability to address the question of "Who verifies the verifier?". Instead, these protocols outsource their legitimacy to mechanisms beyond their internal structure, relying on traditional centralized institutions such as national ID issuers and KYC providers to verify the claims they hold. This reliance has been employed to safeguard applications from a vulnerability previously thought to be impossible to address in distributed systems: the Sybil attack problem, which describes the abuse of an online system by creating many illegitimate virtual personas. Inspired by the progress in cryptocurrencies and blockchain technology, there has recently been a surge in networked protocols that make use of subjective inputs such as voting, vouching, and interpreting, to arrive at a decentralized and sybil-resistant consensus for identity. In this article, we will outline the approaches of these new and natively digital sources of authentication -- their attributes, methodologies strengths, and weaknesses -- and sketch out possible directions for future developments.

</details>

<details>

<summary>2020-10-13 22:35:42 - On the relationship between class selectivity, dimensionality, and robustness</summary>

- *Matthew L. Leavitt, Ari S. Morcos*

- `2007.04440v2` - [abs](http://arxiv.org/abs/2007.04440v2) - [pdf](http://arxiv.org/pdf/2007.04440v2)

> While the relative trade-offs between sparse and distributed representations in deep neural networks (DNNs) are well-studied, less is known about how these trade-offs apply to representations of semantically-meaningful information. Class selectivity, the variability of a unit's responses across data classes or dimensions, is one way of quantifying the sparsity of semantic representations. Given recent evidence showing that class selectivity can impair generalization, we sought to investigate whether it also confers robustness (or vulnerability) to perturbations of input data. We found that mean class selectivity predicts vulnerability to naturalistic corruptions; networks regularized to have lower levels of class selectivity are more robust to corruption, while networks with higher class selectivity are more vulnerable to corruption, as measured using Tiny ImageNetC and CIFAR10C. In contrast, we found that class selectivity increases robustness to multiple types of gradient-based adversarial attacks. To examine this difference, we studied the dimensionality of the change in the representation due to perturbation, finding that decreasing class selectivity increases the dimensionality of this change for both corruption types, but with a notably larger increase for adversarial attacks. These results demonstrate the causal relationship between selectivity and robustness and provide new insights into the mechanisms of this relationship.

</details>

<details>

<summary>2020-10-14 06:08:15 - Equitable Allocation of Healthcare Resources with Fair Cox Models</summary>

- *Kamrun Naher Keya, Rashidul Islam, Shimei Pan, Ian Stockwell, James R. Foulds*

- `2010.06820v1` - [abs](http://arxiv.org/abs/2010.06820v1) - [pdf](http://arxiv.org/pdf/2010.06820v1)

> Healthcare programs such as Medicaid provide crucial services to vulnerable populations, but due to limited resources, many of the individuals who need these services the most languish on waiting lists. Survival models, e.g. the Cox proportional hazards model, can potentially improve this situation by predicting individuals' levels of need, which can then be used to prioritize the waiting lists. Providing care to those in need can prevent institutionalization for those individuals, which both improves quality of life and reduces overall costs. While the benefits of such an approach are clear, care must be taken to ensure that the prioritization process is fair or independent of demographic information-based harmful stereotypes. In this work, we develop multiple fairness definitions for survival models and corresponding fair Cox proportional hazards models to ensure equitable allocation of healthcare resources. We demonstrate the utility of our methods in terms of fairness and predictive accuracy on two publicly available survival datasets.

</details>

<details>

<summary>2020-10-14 07:25:17 - A Review of Cyber-Ranges and Test-Beds: Current and Future Trends</summary>

- *Elochukwu Ukwandu, Mohamed Amine Ben Farah, Hanan Hindy, David Brosset, Dimitris Kavallieros, Robert Atkinson, Christos Tachtatzis, Miroslav Bures, Ivan Andonovic, Xavier Bellekens*

- `2010.06850v1` - [abs](http://arxiv.org/abs/2010.06850v1) - [pdf](http://arxiv.org/pdf/2010.06850v1)

> Cyber situational awareness has been proven to be of value in forming a comprehensive understanding of threats and vulnerabilities within organisations, as the degree of exposure is governed by the prevailing levels of cyber-hygiene and established processes. A more accurate assessment of the security provision informs on the most vulnerable environments that necessitate more diligent management. The rapid proliferation in the automation of cyber-attacks is reducing the gap between information and operational technologies and the need to review the current levels of robustness against new sophisticated cyber-attacks, trends, technologies and mitigation countermeasures has become pressing. A deeper characterisation is also the basis with which to predict future vulnerabilities in turn guiding the most appropriate deployment technologies. Thus, refreshing established practices and the scope of the training to support the decision making of users and operators. The foundation of the training provision is the use of Cyber-Ranges (CRs) and Test-Beds (TBs), platforms/tools that help inculcate a deeper understanding of the evolution of an attack and the methodology to deploy the most impactful countermeasures to arrest breaches. In this paper, an evaluation of documented CR and TB platforms is evaluated. CRs and TBs are segmented by type, technology, threat scenarios, applications and the scope of attainable training. To enrich the analysis of documented CR and TB research and cap the study, a taxonomy is developed to provide a broader comprehension of the future of CRs and TBs. The taxonomy elaborates on the CRs/TBs different dimensions, as well as, highlighting a diminishing differentiation between application areas.

</details>

<details>

<summary>2020-10-14 13:52:20 - Exploiting Interfaces of Secure Encrypted Virtual Machines</summary>

- *Martin Radev, Mathias Morbitzer*

- `2010.07094v1` - [abs](http://arxiv.org/abs/2010.07094v1) - [pdf](http://arxiv.org/pdf/2010.07094v1)

> Cloud computing is a convenient model for processing data remotely. However, users must trust their cloud provider with the confidentiality and integrity of the stored and processed data. To increase the protection of virtual machines, AMD introduced SEV, a hardware feature which aims to protect code and data in a virtual machine. This allows to store and process sensitive data in cloud environments without the need to trust the cloud provider or the underlying software. However, the virtual machine still depends on the hypervisor for performing certain activities, such as the emulation of special CPU instructions, or the emulation of devices. Yet, most code that runs in virtual machines was not written with an attacker model which considers the hypervisor as malicious. In this work, we introduce a new class of attacks in which a malicious hypervisor manipulates external interfaces of an SEV or SEV-ES virtual machine to make it act against its own interests. We start by showing how we can make use of virtual devices to extract encryption keys and secret data of a virtual machine. We then show how we can reduce the entropy of probabilistic kernel defenses in the virtual machine by carefully manipulating the results of the CPUID and RDTSC instructions. We continue by showing an approach for secret data exfiltration and code injection based on the forgery of MMIO regions over the VM's address space. Finally, we show another attack which forces decryption of the VM's stack and uses Return Oriented Programming to execute arbitrary code inside the VM. While our approach is also applicable to traditional virtualization environments, its severity significantly increases with the attacker model of SEV-ES, which aims to protect a virtual machine from a benign but vulnerable hypervisor.

</details>

<details>

<summary>2020-10-15 14:57:02 - Federated Learning in Adversarial Settings</summary>

- *Raouf Kerkouche, Gergely Ács, Claude Castelluccia*

- `2010.07808v1` - [abs](http://arxiv.org/abs/2010.07808v1) - [pdf](http://arxiv.org/pdf/2010.07808v1)

> Federated Learning enables entities to collaboratively learn a shared prediction model while keeping their training data locally. It prevents data collection and aggregation and, therefore, mitigates the associated privacy risks. However, it still remains vulnerable to various security attacks where malicious participants aim at degrading the generated model, inserting backdoors, or inferring other participants' training data. This paper presents a new federated learning scheme that provides different trade-offs between robustness, privacy, bandwidth efficiency, and model accuracy. Our scheme uses biased quantization of model updates and hence is bandwidth efficient. It is also robust against state-of-the-art backdoor as well as model degradation attacks even when a large proportion of the participant nodes are malicious. We propose a practical differentially private extension of this scheme which protects the whole dataset of participating entities. We show that this extension performs as efficiently as the non-private but robust scheme, even with stringent privacy requirements but are less robust against model degradation and backdoor attacks. This suggests a possible fundamental trade-off between Differential Privacy and robustness.

</details>

<details>

<summary>2020-10-15 15:09:38 - Adversarially Robust Few-Shot Learning: A Meta-Learning Approach</summary>

- *Micah Goldblum, Liam Fowl, Tom Goldstein*

- `1910.00982v3` - [abs](http://arxiv.org/abs/1910.00982v3) - [pdf](http://arxiv.org/pdf/1910.00982v3)

> Previous work on adversarially robust neural networks for image classification requires large training sets and computationally expensive training procedures. On the other hand, few-shot learning methods are highly vulnerable to adversarial examples. The goal of our work is to produce networks which both perform well at few-shot classification tasks and are simultaneously robust to adversarial examples. We develop an algorithm, called Adversarial Querying (AQ), for producing adversarially robust meta-learners, and we thoroughly investigate the causes for adversarial vulnerability. Moreover, our method achieves far superior robust performance on few-shot image classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer learning.

</details>

<details>

<summary>2020-10-16 02:25:22 - Can Advanced Type Systems Be Usable? An Empirical Study of Ownership, Assets, and Typestate in Obsidian</summary>

- *Michael Coblenz, Jonathan Aldrich, Joshua Sunshine, Brad A. Myers*

- `2003.12209v2` - [abs](http://arxiv.org/abs/2003.12209v2) - [pdf](http://arxiv.org/pdf/2003.12209v2)

> Some blockchain programs (smart contracts) have included serious security vulnerabilities. Obsidian is a new typestate-oriented programming language that uses a strong type system to rule out some of these vulnerabilities. Although Obsidian was designed to promote usability to make it as easy as possible to write programs, strong type systems can cause a language to be difficult to use. In particular, ownership, typestate, and assets, which Obsidian uses to provide safety guarantees, have not seen broad adoption together in popular languages and result in significant usability challenges. We performed an empirical study with 20 participants comparing Obsidian to Solidity, which is the language most commonly used for writing smart contracts today. We observed that Obsidian participants were able to successfully complete more of the programming tasks than the Solidity participants. We also found that the Solidity participants commonly inserted asset-related bugs, which Obsidian detects at compile time.

</details>

<details>

<summary>2020-10-16 02:25:40 - Exploiting Vulnerabilities of Deep Learning-based Energy Theft Detection in AMI through Adversarial Attacks</summary>

- *Jiangnan Li, Yingyuan Yang, Jinyuan Stella Sun*

- `2010.09212v1` - [abs](http://arxiv.org/abs/2010.09212v1) - [pdf](http://arxiv.org/pdf/2010.09212v1)

> Effective detection of energy theft can prevent revenue losses of utility companies and is also important for smart grid security. In recent years, enabled by the massive fine-grained smart meter data, deep learning (DL) approaches are becoming popular in the literature to detect energy theft in the advanced metering infrastructure (AMI). However, as neural networks are shown to be vulnerable to adversarial examples, the security of the DL models is of concern.   In this work, we study the vulnerabilities of DL-based energy theft detection through adversarial attacks, including single-step attacks and iterative attacks. From the attacker's point of view, we design the \textit{SearchFromFree} framework that consists of 1) a randomly adversarial measurement initialization approach to maximize the stolen profit and 2) a step-size searching scheme to increase the performance of black-box iterative attacks. The evaluation based on three types of neural networks shows that the adversarial attacker can report extremely low consumption measurements to the utility without being detected by the DL models. We finally discuss the potential defense mechanisms against adversarial attacks in energy theft detection.

</details>

<details>

<summary>2020-10-16 04:48:24 - DPAttack: Diffused Patch Attacks against Universal Object Detection</summary>

- *Shudeng Wu, Tao Dai, Shu-Tao Xia*

- `2010.11679v1` - [abs](http://arxiv.org/abs/2010.11679v1) - [pdf](http://arxiv.org/pdf/2010.11679v1)

> Recently, deep neural networks (DNNs) have been widely and successfully used in Object Detection, e.g. Faster RCNN, YOLO, CenterNet. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. Adversarial attacks against object detection can be divided into two categories, whole-pixel attacks and patch attacks. While these attacks add perturbations to a large number of pixels in images, we proposed a diffused patch attack (\textbf{DPAttack}) to successfully fool object detectors by diffused patches of asteroid-shaped or grid-shape, which only change a small number of pixels. Experiments show that our DPAttack can successfully fool most object detectors with diffused patches and we get the second place in the Alibaba Tianchi competition: Alibaba-Tsinghua Adversarial Challenge on Object Detection. Our code can be obtained from https://github.com/Wu-Shudeng/DPAttack.

</details>

<details>

<summary>2020-10-16 08:43:33 - An Energy Efficient Authentication Scheme using Chebyshev Chaotic Map for Smart Grid Environment</summary>

- *Liping Zhang, Yue Zhu, Wei Ren, Yinghan Wang, Neal N. Xiong*

- `2008.11366v3` - [abs](http://arxiv.org/abs/2008.11366v3) - [pdf](http://arxiv.org/pdf/2008.11366v3)

> As one of the important applications of Smart grid, charging between electric vehicles has attracted much attention. However, authentication between vehicle users and an aggregator may be vulnerable to various attacks due to the usage of wireless communications. In order to reduce the computational costs yet preserve required security, the Chebyshev chaotic map based authentication schemes are proposed. However, the security requirements of Chebyshev polynomials bring a new challenge to the design of authentication schemes based on Chebyshev chaotic maps. To solve this issue, we propose a practical Chebyshev polynomial algorithm by using a binary exponentiation algorithm based on square matrix to achieve secure and efficient Chebyshev polynomial computation. We further apply the proposed algorithm to construct an energy-efficient authentication and key agreement scheme for smart grid environments. Compared with state-of-the-art schemes, the proposed authentication scheme effectively reduces the computational costs and communication costs by adopting the proposed Chebyshev polynomial algorithm. Furthermore, the ProVerif tool is employed to analyze the security of the proposed authentication scheme. Our experimental results justified that our proposed authentication scheme can outperform state-of-the-art schemes in terms of the computational overhead while achieving privacy protection.

</details>

<details>

<summary>2020-10-16 18:26:59 - Prediction-Based GNSS Spoofing Attack Detection for Autonomous Vehicles</summary>

- *Sagar Dasgupta, Mizanur Rahman, Mhafuzul Islam, Mashrur Chowdhury*

- `2010.11722v1` - [abs](http://arxiv.org/abs/2010.11722v1) - [pdf](http://arxiv.org/pdf/2010.11722v1)

> Global Navigation Satellite System (GNSS) provides Positioning, Navigation, and Timing (PNT) services for autonomous vehicles (AVs) using satellites and radio communications. Due to the lack of encryption, open-access of the coarse acquisition (C/A) codes, and low strength of the signal, GNSS is vulnerable to spoofing attacks compromising the navigational capability of the AV. A spoofed attack is difficult to detect as a spoofer (attacker who performs spoofing attack) can mimic the GNSS signal and transmit inaccurate location coordinates to an AV. In this study, we have developed a prediction-based spoofing attack detection strategy using the long short-term memory (LSTM) model, a recurrent neural network model. The LSTM model is used to predict the distance traveled between two consecutive locations of an autonomous vehicle. In order to develop the LSTM prediction model, we have used a publicly available real-world comma2k19 driving dataset. The training dataset contains different features (i.e., acceleration, steering wheel angle, speed, and distance traveled between two consecutive locations) extracted from the controlled area network (CAN), GNSS, and inertial measurement unit (IMU) sensors of AVs. Based on the predicted distance traveled between the current location and the immediate future location of an autonomous vehicle, a threshold value is established using the positioning error of the GNSS device and prediction error (i.e., maximum absolute error) related to distance traveled between the current location and the immediate future location. Our analysis revealed that the prediction-based spoofed attack detection strategy can successfully detect the attack in real-time.

</details>

<details>

<summary>2020-10-17 14:36:44 - Smart Contract Vulnerabilities: Vulnerable Does Not Imply Exploited</summary>

- *Daniel Perez, Benjamin Livshits*

- `1902.06710v5` - [abs](http://arxiv.org/abs/1902.06710v5) - [pdf](http://arxiv.org/pdf/1902.06710v5)

> In recent years, we have seen a great deal of both academic and practical interest in the topic of vulnerabilities in smart contracts, particularly those developed for the Ethereum blockchain. While most of the work has focused on detecting *vulnerable* contracts, in this paper, we focus on finding how many of these vulnerable contracts have actually been *exploited*. We survey the 23,327 vulnerable contracts reported by six recent academic projects and find that, despite the amounts at stake, only 1.98% of them have been exploited since deployment. This corresponds to at most 8,487 ETH (~1.7 million USD), or only 0.27% of the 3 million ETH (600 million USD) at stake. We explain these results by demonstrating that the funds are very concentrated in a small number of contracts which are *not exploitable* in practice.

</details>

<details>

<summary>2020-10-18 16:35:25 - DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles</summary>

- *Huanrui Yang, Jingyang Zhang, Hongliang Dong, Nathan Inkawhich, Andrew Gardner, Andrew Touchet, Wesley Wilkes, Heath Berry, Hai Li*

- `2009.14720v2` - [abs](http://arxiv.org/abs/2009.14720v2) - [pdf](http://arxiv.org/pdf/2009.14720v2)

> Recent research finds CNN models for image classification demonstrate overlapped adversarial vulnerabilities: adversarial attacks can mislead CNN models with small perturbations, which can effectively transfer between different models trained on the same dataset. Adversarial training, as a general robustness improvement technique, eliminates the vulnerability in a single model by forcing it to learn robust features. The process is hard, often requires models with large capacity, and suffers from significant loss on clean data accuracy. Alternatively, ensemble methods are proposed to induce sub-models with diverse outputs against a transfer adversarial example, making the ensemble robust against transfer attacks even if each sub-model is individually non-robust. Only small clean accuracy drop is observed in the process. However, previous ensemble training methods are not efficacious in inducing such diversity and thus ineffective on reaching robust ensemble. We propose DVERGE, which isolates the adversarial vulnerability in each sub-model by distilling non-robust features, and diversifies the adversarial vulnerability to induce diverse outputs against a transfer attack. The novel diversity metric and training procedure enables DVERGE to achieve higher robustness against transfer attacks comparing to previous ensemble methods, and enables the improved robustness when more sub-models are added to the ensemble. The code of this work is available at https://github.com/zjysteven/DVERGE

</details>

<details>

<summary>2020-10-18 22:00:11 - FADER: Fast Adversarial Example Rejection</summary>

- *Francesco Crecchi, Marco Melis, Angelo Sotgiu, Davide Bacciu, Battista Biggio*

- `2010.09119v1` - [abs](http://arxiv.org/abs/2010.09119v1) - [pdf](http://arxiv.org/pdf/2010.09119v1)

> Deep neural networks are vulnerable to adversarial examples, i.e., carefully-crafted inputs that mislead classification at test time. Recent defenses have been shown to improve adversarial robustness by detecting anomalous deviations from legitimate training samples at different layer representations - a behavior normally exhibited by adversarial attacks. Despite technical differences, all aforementioned methods share a common backbone structure that we formalize and highlight in this contribution, as it can help in identifying promising research directions and drawbacks of existing methods. The first main contribution of this work is the review of these detection methods in the form of a unifying framework designed to accommodate both existing defenses and newer ones to come. In terms of drawbacks, the overmentioned defenses require comparing input samples against an oversized number of reference prototypes, possibly at different representation layers, dramatically worsening the test-time efficiency. Besides, such defenses are typically based on ensembling classifiers with heuristic methods, rather than optimizing the whole architecture in an end-to-end manner to better perform detection. As a second main contribution of this work, we introduce FADER, a novel technique for speeding up detection-based methods. FADER overcome the issues above by employing RBF networks as detectors: by fixing the number of required prototypes, the runtime complexity of adversarial examples detectors can be controlled. Our experiments outline up to 73x prototypes reduction compared to analyzed detectors for MNIST dataset and up to 50x for CIFAR10 dataset respectively, without sacrificing classification accuracy on both clean and adversarial data.

</details>

<details>

<summary>2020-10-19 09:20:14 - The Impact of DNS Insecurity on Time</summary>

- *Philipp Jeitner, Haya Shulman, Michael Waidner*

- `2010.09338v1` - [abs](http://arxiv.org/abs/2010.09338v1) - [pdf](http://arxiv.org/pdf/2010.09338v1)

> We demonstrate the first practical off-path time shifting attacks against NTP as well as against Man-in-the-Middle (MitM) secure Chronos-enhanced NTP. Our attacks exploit the insecurity of DNS allowing us to redirect the NTP clients to attacker controlled servers. We perform large scale measurements of the attack surface in NTP clients and demonstrate the threats to NTP due to vulnerable DNS.

</details>

<details>

<summary>2020-10-19 14:08:47 - Anomaly Detection with Convolutional Autoencoders for Fingerprint Presentation Attack Detection</summary>

- *Jascha Kolberg, Marcel Grimmer, Marta Gomez-Barrero, Christoph Busch*

- `2008.07989v2` - [abs](http://arxiv.org/abs/2008.07989v2) - [pdf](http://arxiv.org/pdf/2008.07989v2)

> In recent years, the popularity of fingerprint-based biometric authentication systems significantly increased. However, together with many advantages, biometric systems are still vulnerable to presentation attacks (PAs). In particular, this applies for unsupervised applications, where new attacks unknown to the system operator may occur. Therefore, presentation attack detection (PAD) methods are used to determine whether samples stem from a bona fide subject or from a presentation attack instrument (PAI). In this context, most works are dedicated to solve PAD as a two-class classification problem, which includes training a model on both bona fide and PA samples. In spite of the good detection rates reported, these methods still face difficulties detecting PAIs from unknown materials. To address this issue, we propose a new PAD technique based on autoencoders (AEs) trained only on bona fide samples (i.e. one-class), which are captured in the short wave infrared domain. On the experimental evaluation over a database of 19,711 bona fide and 4,339 PA images including 45 different PAI species, a detection equal error rate (D-EER) of 2.00% was achieved. Additionally, our best performing AE model is compared to further one-class classifiers (support vector machine, Gaussian mixture model). The results show the effectiveness of the AE model as it significantly outperforms the previously proposed methods.

</details>

<details>

<summary>2020-10-19 15:41:22 - Motion Sensor-based Privacy Attack on Smartphones</summary>

- *S Abhishek Anand, Chen Wang, Jian Liu, Nitesh Saxena, Yingying Chen*

- `1907.05972v3` - [abs](http://arxiv.org/abs/1907.05972v3) - [pdf](http://arxiv.org/pdf/1907.05972v3)

> In this paper, we build a speech privacy attack that exploits speech reverberations generated from a smartphone's in-built loudspeaker captured via a zero-permission motion sensor (accelerometer). We design our attack Spearphone2, and demonstrate that speech reverberations from inbuilt loudspeakers, at an appropriate loudness, can impact the accelerometer, leaking sensitive information about the speech. In particular, we show that by exploiting the affected accelerometer readings and carefully selecting feature sets along with off-the-shelf machine learning techniques, Spearphone can successfully perform gender classification (accuracy over 90%) and speaker identification (accuracy over 80%) for any audio/video playback on the smartphone. Our results with testing the attack on a voice call and voice assistant response were also encouraging, showcasing the impact of the proposed attack. In addition, we perform speech recognition and speech reconstruction to extract more information about the eavesdropped speech to an extent. Our work brings to light a fundamental design vulnerability in many currently-deployed smartphones, which may put people's speech privacy at risk while using the smartphone in the loudspeaker mode during phone calls, media playback or voice assistant interactions.

</details>

<details>

<summary>2020-10-19 17:16:38 - A Survey of Machine Learning Techniques in Adversarial Image Forensics</summary>

- *Ehsan Nowroozi, Ali Dehghantanha, Reza M. Parizi, Kim-Kwang Raymond Choo*

- `2010.09680v1` - [abs](http://arxiv.org/abs/2010.09680v1) - [pdf](http://arxiv.org/pdf/2010.09680v1)

> Image forensic plays a crucial role in both criminal investigations (e.g., dissemination of fake images to spread racial hate or false narratives about specific ethnicity groups) and civil litigation (e.g., defamation). Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches, for example how to detect adversarial (image) examples, with real-world consequences (e.g., inadmissible evidence, or wrongful conviction). Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios.

</details>

<details>

<summary>2020-10-20 13:05:48 - Adversarial Item Promotion: Vulnerabilities at the Core of Top-N Recommenders that Use Images to Address Cold Start</summary>

- *Zhuoran Liu, Martha Larson*

- `2006.01888v3` - [abs](http://arxiv.org/abs/2006.01888v3) - [pdf](http://arxiv.org/pdf/2006.01888v3)

> E-commerce platforms provide their customers with ranked lists of recommended items matching the customers' preferences. Merchants on e-commerce platforms would like their items to appear as high as possible in the top-N of these ranked lists. In this paper, we demonstrate how unscrupulous merchants can create item images that artificially promote their products, improving their rankings. Recommender systems that use images to address the cold start problem are vulnerable to this security risk. We describe a new type of attack, Adversarial Item Promotion (AIP), that strikes directly at the core of Top-N recommenders: the ranking mechanism itself. Existing work on adversarial images in recommender systems investigates the implications of conventional attacks, which target deep learning classifiers. In contrast, our AIP attacks are embedding attacks that seek to push features representations in a way that fools the ranker (not a classifier) and directly lead to item promotion. We introduce three AIP attacks insider attack, expert attack, and semantic attack, which are defined with respect to three successively more realistic attack models. Our experiments evaluate the danger of these attacks when mounted against three representative visually-aware recommender algorithms in a framework that uses images to address cold start. We also evaluate potential defenses, including adversarial training and find that common, currently-existing, techniques do not eliminate the danger of AIP attacks. In sum, we show that using images to address cold start opens recommender systems to potential threats with clear practical implications.

</details>

<details>

<summary>2020-10-20 19:17:25 - Mitigating Sybil Attacks on Differential Privacy based Federated Learning</summary>

- *Yupeng Jiang, Yong Li, Yipeng Zhou, Xi Zheng*

- `2010.10572v1` - [abs](http://arxiv.org/abs/2010.10572v1) - [pdf](http://arxiv.org/pdf/2010.10572v1)

> In federated learning, machine learning and deep learning models are trained globally on distributed devices. The state-of-the-art privacy-preserving technique in the context of federated learning is user-level differential privacy. However, such a mechanism is vulnerable to some specific model poisoning attacks such as Sybil attacks. A malicious adversary could create multiple fake clients or collude compromised devices in Sybil attacks to mount direct model updates manipulation. Recent works on novel defense against model poisoning attacks are difficult to detect Sybil attacks when differential privacy is utilized, as it masks clients' model updates with perturbation. In this work, we implement the first Sybil attacks on differential privacy based federated learning architectures and show their impacts on model convergence. We randomly compromise some clients by manipulating different noise levels reflected by the local privacy budget epsilon of differential privacy on the local model updates of these Sybil clients such that the global model convergence rates decrease or even leads to divergence. We apply our attacks to two recent aggregation defense mechanisms, called Krum and Trimmed Mean. Our evaluation results on the MNIST and CIFAR-10 datasets show that our attacks effectively slow down the convergence of the global models. We then propose a method to keep monitoring the average loss of all participants in each round for convergence anomaly detection and defend our Sybil attacks based on the prediction cost reported from each client. Our empirical study demonstrates that our defense approach effectively mitigates the impact of our Sybil attacks on model convergence.

</details>

<details>

<summary>2020-10-20 22:20:53 - Towards Understanding the Dynamics of the First-Order Adversaries</summary>

- *Zhun Deng, Hangfeng He, Jiaoyang Huang, Weijie J. Su*

- `2010.10650v1` - [abs](http://arxiv.org/abs/2010.10650v1) - [pdf](http://arxiv.org/pdf/2010.10650v1)

> An acknowledged weakness of neural networks is their vulnerability to adversarial perturbations to the inputs. To improve the robustness of these models, one of the most popular defense mechanisms is to alternatively maximize the loss over the constrained perturbations (or called adversaries) on the inputs using projected gradient ascent and minimize over weights. In this paper, we analyze the dynamics of the maximization step towards understanding the experimentally observed effectiveness of this defense mechanism. Specifically, we investigate the non-concave landscape of the adversaries for a two-layer neural network with a quadratic loss. Our main result proves that projected gradient ascent finds a local maximum of this non-concave problem in a polynomial number of iterations with high probability. To our knowledge, this is the first work that provides a convergence analysis of the first-order adversaries. Moreover, our analysis demonstrates that, in the initial phase of adversarial training, the scale of the inputs matters in the sense that a smaller input scale leads to faster convergence of adversarial training and a "more regular" landscape. Finally, we show that these theoretical findings are in excellent agreement with a series of experiments.

</details>

<details>

<summary>2020-10-21 07:01:32 - "Are you home alone?" "Yes" Disclosing Security and Privacy Vulnerabilities in Alexa Skills</summary>

- *Dan Su, Jiqiang Liu, Sencun Zhu, Xiaoyang Wang, Wei Wang*

- `2010.10788v1` - [abs](http://arxiv.org/abs/2010.10788v1) - [pdf](http://arxiv.org/pdf/2010.10788v1)

> The home voice assistants such as Amazon Alexa have become increasingly popular due to many interesting voice-activated services provided through special applications called skills. These skills, though useful, have also introduced new security and privacy challenges. Prior work has verified that Alexa is vulnerable to multiple types of voice attacks, but the security and privacy risk of using skills has not been fully investigated. In this work, we study an adversary model that covers three severe privacy-related vulnerabilities, namely,over-privileged resource access, hidden code-manipulation and hidden content-manipulation. By exploiting these vulnerabilities, malicious skills can not only bypass the security tests in the vetting process, but also surreptitiously change their original functions in an attempt to steal users' personal information. What makes the situation even worse is that the attacks can be extended from virtual networks to the physical world. We systematically study the security issues from the feasibility and implementation of the attacks to the design of countermeasures. We also made a comprehensive survey study of 33,744 skills in Alex Skills Store.

</details>

<details>

<summary>2020-10-21 13:14:17 - Amnesiac Machine Learning</summary>

- *Laura Graves, Vineel Nagisetty, Vijay Ganesh*

- `2010.10981v1` - [abs](http://arxiv.org/abs/2010.10981v1) - [pdf](http://arxiv.org/pdf/2010.10981v1)

> The Right to be Forgotten is part of the recently enacted General Data Protection Regulation (GDPR) law that affects any data holder that has data on European Union residents. It gives EU residents the ability to request deletion of their personal data, including training records used to train machine learning models. Unfortunately, Deep Neural Network models are vulnerable to information leaking attacks such as model inversion attacks which extract class information from a trained model and membership inference attacks which determine the presence of an example in a model's training data. If a malicious party can mount an attack and learn private information that was meant to be removed, then it implies that the model owner has not properly protected their user's rights and their models may not be compliant with the GDPR law. In this paper, we present two efficient methods that address this question of how a model owner or data holder may delete personal data from models in such a way that they may not be vulnerable to model inversion and membership inference attacks while maintaining model efficacy. We start by presenting a real-world threat model that shows that simply removing training data is insufficient to protect users. We follow that up with two data removal methods, namely Unlearning and Amnesiac Unlearning, that enable model owners to protect themselves against such attacks while being compliant with regulations. We provide extensive empirical analysis that show that these methods are indeed efficient, safe to apply, effectively remove learned information about sensitive data from trained models while maintaining model efficacy.

</details>

<details>

<summary>2020-10-21 14:02:27 - Token Drop mechanism for Neural Machine Translation</summary>

- *Huaao Zhang, Shigui Qiu, Xiangyu Duan, Min Zhang*

- `2010.11018v1` - [abs](http://arxiv.org/abs/2010.11018v1) - [pdf](http://arxiv.org/pdf/2010.11018v1)

> Neural machine translation with millions of parameters is vulnerable to unfamiliar inputs. We propose Token Drop to improve generalization and avoid overfitting for the NMT model. Similar to word dropout, whereas we replace dropped token with a special token instead of setting zero to words. We further introduce two self-supervised objectives: Replaced Token Detection and Dropped Token Prediction. Our method aims to force model generating target translation with less information, in this way the model can learn textual representation better. Experiments on Chinese-English and English-Romanian benchmark demonstrate the effectiveness of our approach and our model achieves significant improvements over a strong Transformer baseline.

</details>

<details>

<summary>2020-10-21 18:55:15 - Uncovering the Hidden Dangers: Finding Unsafe Go Code in the Wild</summary>

- *Johannes Lauinger, Lars Baumgärtner, Anna-Katharina Wickert, Mira Mezini*

- `2010.11242v1` - [abs](http://arxiv.org/abs/2010.11242v1) - [pdf](http://arxiv.org/pdf/2010.11242v1)

> The Go programming language aims to provide memory and thread safety through measures such as automated memory management with garbage collection and a strict type system. However, it also offers a way of circumventing this safety net through the use of the unsafe package. While there are legitimate use cases for unsafe, developers must exercise caution to avoid introducing vulnerabilities like buffer overflows or memory corruption in general. Using go-geiger, we conducted a study on the usage of unsafe in the top 500 most popular open-source Go projects on GitHub, including a manual analysis of 1,400 code samples on how unsafe is used. From the projects using Go's module system, 38% directly contain at least one unsafe usage, and 91% contain at least one unsafe usage in the project itself or one of its transitive dependencies. Based on the usage patterns found, we present possible exploit vectors in different scenarios. Finally, we present go-safer, a novel static analysis tool to identify dangerous and common usage patterns that were previously undetected with existing tools.

</details>

<details>

<summary>2020-10-22 06:02:44 - MixCon: Adjusting the Separability of Data Representations for Harder Data Recovery</summary>

- *Xiaoxiao Li, Yangsibo Huang, Binghui Peng, Zhao Song, Kai Li*

- `2010.11463v1` - [abs](http://arxiv.org/abs/2010.11463v1) - [pdf](http://arxiv.org/pdf/2010.11463v1)

> To address the issue that deep neural networks (DNNs) are vulnerable to model inversion attacks, we design an objective function, which adjusts the separability of the hidden data representations, as a way to control the trade-off between data utility and vulnerability to inversion attacks. Our method is motivated by the theoretical insights of data separability in neural networking training and results on the hardness of model inversion. Empirically, by adjusting the separability of data representation, we show that there exist sweet-spots for data separability such that it is difficult to recover data during inference while maintaining data utility.

</details>

<details>

<summary>2020-10-22 07:14:44 - Understanding Static Code Warnings: an Incremental AI Approach</summary>

- *Xueqi Yang, Zhe Yu, Junjie Wang, Tim Menzies*

- `1911.01387v3` - [abs](http://arxiv.org/abs/1911.01387v3) - [pdf](http://arxiv.org/pdf/1911.01387v3)

> Knowledge-based systems reason over some knowledge base. Hence, an important issue for such systems is how to acquire the knowledge needed for their inference. This paper assesses active learning methods for acquiring knowledge for "static code warnings".   Static code analysis is a widely-used method for detecting bugs and security vulnerabilities in software systems. As software becomes more complex, analysis tools also report lists of increasingly complex warnings that developers need to address on a daily basis. Such static code analysis tools are usually over-cautious; i.e. they often offer many warnings about spurious issues. Previous research work shows that about 35% to 91% of warnings reported as bugs by SA tools are actually unactionable (i.e., warnings that would not be acted on by developers because they are falsely suggested as bugs).   Experienced developers know which errors are important and which can be safely ignored. How can we capture that experience? This paper reports on an incremental AI tool that watches humans reading false alarm reports. Using an incremental support vector machine mechanism, this AI tool can quickly learn to distinguish spurious false alarms from more serious matters that deserve further attention.   In this work, nine open-source projects are employed to evaluate our proposed model on the features extracted by previous researchers and identify the actionable warnings in a priority order given by our algorithm. We observe that our model can identify over 90% of actionable warnings when our methods tell humans to ignore 70 to 80% of the warnings.

</details>

<details>

<summary>2020-10-22 07:18:41 - Integrated Model-Driven Engineering of Blockchain Applications for Business Processes and Asset Management</summary>

- *Qinghua Lu, An Binh Tran, Ingo Weber, Hugo O'Connor, Paul Rimba, Xiwei Xu, Mark Staples, Liming Zhu, Ross Jeffery*

- `2005.12685v2` - [abs](http://arxiv.org/abs/2005.12685v2) - [pdf](http://arxiv.org/pdf/2005.12685v2)

> Blockchain has attracted broad interests to build decentralised applications. Blockchain has attracted broad interests to build decentralised applications. However, developing such applications without introducing vulnerabilities is hard for developers, not the least because the deployed code is immutable and can be called by anyone with access to the network. Model-driven engineering (MDE) helps to reduce those risks, by combining proven code snippets as per the model specification, which is easier to understand than source code. Therefore, in this paper, we present an approach for integrated MDE across business processes and asset management (e.g. for settlement). Our approach includes methods for fungible/non-fungible asset registration, escrow for conditional payment, and asset swap. The proposed MDE approach is implemented in a smart contract generation tool called Lorikeet, and evaluated in terms of feasibility, functional correctness, and cost effectiveness.

</details>

<details>

<summary>2020-10-22 13:08:04 - HRFA: High-Resolution Feature-based Attack</summary>

- *Zhixing Ye, Sizhe Chen, Peidong Zhang, Chengjin Sun, Xiaolin Huang*

- `2001.07631v2` - [abs](http://arxiv.org/abs/2001.07631v2) - [pdf](http://arxiv.org/pdf/2001.07631v2)

> Adversarial attacks have long been developed for revealing the vulnerability of Deep Neural Networks (DNNs) by adding imperceptible perturbations to the input. Most methods generate perturbations like normal noise, which is not interpretable and without semantic meaning. In this paper, we propose High-Resolution Feature-based Attack (HRFA), yielding authentic adversarial examples with up to $1024 \times 1024$ resolution. HRFA exerts attack by modifying the latent feature representation of the image, i.e., the gradients back propagate not only through the victim DNN, but also through the generative model that maps the feature space to the image space. In this way, HRFA generates adversarial examples that are in high-resolution, realistic, noise-free, and hence is able to evade several denoising-based defenses. In the experiment, the effectiveness of HRFA is validated by attacking the object classification and face verification tasks with BigGAN and StyleGAN, respectively. The advantages of HRFA are verified from the high quality, high authenticity, and high attack success rate faced with defenses.

</details>

<details>

<summary>2020-10-22 14:57:42 - Adversarial Attacks on Binary Image Recognition Systems</summary>

- *Eric Balkanski, Harrison Chase, Kojin Oshiba, Alexander Rilee, Yaron Singer, Richard Wang*

- `2010.11782v1` - [abs](http://arxiv.org/abs/2010.11782v1) - [pdf](http://arxiv.org/pdf/2010.11782v1)

> We initiate the study of adversarial attacks on models for binary (i.e. black and white) image classification. Although there has been a great deal of work on attacking models for colored and grayscale images, little is known about attacks on models for binary images. Models trained to classify binary images are used in text recognition applications such as check processing, license plate recognition, invoice processing, and many others. In contrast to colored and grayscale images, the search space of attacks on binary images is extremely restricted and noise cannot be hidden with minor perturbations in each pixel. Thus, the optimization landscape of attacks on binary images introduces new fundamental challenges.   In this paper we introduce a new attack algorithm called SCAR, designed to fool classifiers of binary images. We show that SCAR significantly outperforms existing $L_0$ attacks applied to the binary setting and use it to demonstrate the vulnerability of real-world text recognition systems. SCAR's strong performance in practice contrasts with the existence of classifiers that are provably robust to large perturbations. In many cases, altering a single pixel is sufficient to trick Tesseract, a popular open-source text recognition system, to misclassify a word as a different word in the English dictionary. We also license software from providers of check processing systems to most of the major US banks and demonstrate the vulnerability of check recognitions for mobile deposits. These systems are substantially harder to fool since they classify both the handwritten amounts in digits and letters, independently. Nevertheless, we generalize SCAR to design attacks that fool state-of-the-art check processing systems using unnoticeable perturbations that lead to misclassification of deposit amounts. Consequently, this is a powerful method to perform financial fraud.

</details>

<details>

<summary>2020-10-22 17:55:35 - Smoothed Geometry for Robust Attribution</summary>

- *Zifan Wang, Haofan Wang, Shakul Ramkumar, Matt Fredrikson, Piotr Mardziel, Anupam Datta*

- `2006.06643v2` - [abs](http://arxiv.org/abs/2006.06643v2) - [pdf](http://arxiv.org/pdf/2006.06643v2)

> Feature attributions are a popular tool for explaining the behavior of Deep Neural Networks (DNNs), but have recently been shown to be vulnerable to attacks that produce divergent explanations for nearby inputs. This lack of robustness is especially problematic in high-stakes applications where adversarially-manipulated explanations could impair safety and trustworthiness. Building on a geometric understanding of these attacks presented in recent work, we identify Lipschitz continuity conditions on models' gradient that lead to robust gradient-based attributions, and observe that smoothness may also be related to the ability of an attack to transfer across multiple attribution methods. To mitigate these attacks in practice, we propose an inexpensive regularization method that promotes these conditions in DNNs, as well as a stochastic smoothing technique that does not require re-training. Our experiments on a range of image models demonstrate that both of these mitigations consistently improve attribution robustness, and confirm the role that smooth geometry plays in these attacks on real, large-scale models.

</details>

<details>

<summary>2020-10-23 00:36:25 - AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing Flows</summary>

- *Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie*

- `2007.07435v2` - [abs](http://arxiv.org/abs/2007.07435v2) - [pdf](http://arxiv.org/pdf/2007.07435v2)

> Deep learning classifiers are susceptible to well-crafted, imperceptible variations of their inputs, known as adversarial attacks. In this regard, the study of powerful attack models sheds light on the sources of vulnerability in these classifiers, hopefully leading to more robust ones. In this paper, we introduce AdvFlow: a novel black-box adversarial attack method on image classifiers that exploits the power of normalizing flows to model the density of adversarial examples around a given target image. We see that the proposed method generates adversaries that closely follow the clean data distribution, a property which makes their detection less likely. Also, our experimental results show competitive performance of the proposed approach with some of the existing attack methods on defended classifiers. The code is available at https://github.com/hmdolatabadi/AdvFlow.

</details>

<details>

<summary>2020-10-23 03:44:03 - DeFuzz: Deep Learning Guided Directed Fuzzing</summary>

- *Xiaogang Zhu, Shigang Liu, Xian Li, Sheng Wen, Jun Zhang, Camtepe Seyit, Yang Xiang*

- `2010.12149v1` - [abs](http://arxiv.org/abs/2010.12149v1) - [pdf](http://arxiv.org/pdf/2010.12149v1)

> Fuzzing is one of the most effective technique to identify potential software vulnerabilities. Most of the fuzzers aim to improve the code coverage, and there is lack of directedness (e.g., fuzz the specified path in a software). In this paper, we proposed a deep learning (DL) guided directed fuzzing for software vulnerability detection, named DeFuzz. DeFuzz includes two main schemes: (1) we employ a pre-trained DL prediction model to identify the potentially vulnerable functions and the locations (i.e., vulnerable addresses). Precisely, we employ Bidirectional-LSTM (BiLSTM) to identify attention words, and the vulnerabilities are associated with these attention words in functions. (2) then we employ directly fuzzing to fuzz the potential vulnerabilities by generating inputs that tend to arrive the predicted locations. To evaluate the effectiveness and practical of the proposed DeFuzz technique, we have conducted experiments on real-world data sets. Experimental results show that our DeFuzz can discover coverage more and faster than AFL. Moreover, DeFuzz exposes 43 more bugs than AFL on real-world applications.

</details>

<details>

<summary>2020-10-23 04:37:23 - Improving robustness against common corruptions by covariate shift adaptation</summary>

- *Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, Matthias Bethge*

- `2006.16971v2` - [abs](http://arxiv.org/abs/2006.16971v2) - [pdf](http://arxiv.org/pdf/2006.16971v2)

> Today's state-of-the-art machine vision models are vulnerable to image corruptions like blurring or compression artefacts, limiting their performance in many real-world applications. We here argue that popular benchmarks to measure model robustness against common corruptions (like ImageNet-C) underestimate model robustness in many (but not all) application scenarios. The key insight is that in many scenarios, multiple unlabeled examples of the corruptions are available and can be used for unsupervised online adaptation. Replacing the activation statistics estimated by batch normalization on the training set with the statistics of the corrupted images consistently improves the robustness across 25 different popular computer vision models. Using the corrected statistics, ResNet-50 reaches 62.2% mCE on ImageNet-C compared to 76.7% without adaptation. With the more robust DeepAugment+AugMix model, we improve the state of the art achieved by a ResNet50 model up to date from 53.6% mCE to 45.4% mCE. Even adapting to a single sample improves robustness for the ResNet-50 and AugMix models, and 32 samples are sufficient to improve the current state of the art for a ResNet-50 architecture. We argue that results with adapted statistics should be included whenever reporting scores in corruption benchmarks and other out-of-distribution generalization settings.

</details>

<details>

<summary>2020-10-23 06:31:17 - On Evaluating Neural Network Backdoor Defenses</summary>

- *Akshaj Veldanda, Siddharth Garg*

- `2010.12186v1` - [abs](http://arxiv.org/abs/2010.12186v1) - [pdf](http://arxiv.org/pdf/2010.12186v1)

> Deep neural networks (DNNs) demonstrate superior performance in various fields, including scrutiny and security. However, recent studies have shown that DNNs are vulnerable to backdoor attacks. Several defenses were proposed in the past to defend DNNs against such backdoor attacks. In this work, we conduct a critical analysis and identify common pitfalls in these existing defenses, prepare a comprehensive database of backdoor attacks, conduct a side-by-side evaluation of existing defenses against this database. Finally, we layout some general guidelines to help researchers develop more robust defenses in the future and avoid common mistakes from the past.

</details>

<details>

<summary>2020-10-23 13:43:45 - Towards Efficiently Establishing Mutual Distrust Between Host Application and Enclave for SGX</summary>

- *Yuan Chen, Jiaqi Li, Guorui Xu, Yajin Zhou, Zhi Wang, Cong Wang, Kui Ren*

- `2010.12400v1` - [abs](http://arxiv.org/abs/2010.12400v1) - [pdf](http://arxiv.org/pdf/2010.12400v1)

> Since its debut, SGX has been used in many applications, e.g., secure data processing. However, previous systems usually assume a trusted enclave and ignore the security issues caused by an untrusted enclave. For instance, a vulnerable (or even malicious) third-party enclave can be exploited to attack the host application and the rest of the system. In this paper, we propose an efficient mechanism to confine an untrusted enclave's behaviors. The threats of an untrusted enclave come from the enclave-host asymmetries. They can be abused to access arbitrary memory regions of its host application, jump to any code location after leaving the enclave and forge the stack register to manipulate the saved context. Our solution breaks such asymmetries and establishes mutual distrust between the host application and the enclave. It leverages Intel MPK for efficient memory isolation and the x86 single-step debugging mechanism to capture the event when an enclave is existing. It then performs the integrity check for the jump target and the stack pointer. We have solved two practical challenges and implemented a prototype system. The evaluation with multiple micro-benchmarks and representative real-world applications demonstrated the efficiency of our system, with less than 4% performance overhead.

</details>

<details>

<summary>2020-10-23 16:22:05 - Improving Robustness by Augmenting Training Sentences with Predicate-Argument Structures</summary>

- *Nafise Sadat Moosavi, Marcel de Boer, Prasetya Ajie Utama, Iryna Gurevych*

- `2010.12510v1` - [abs](http://arxiv.org/abs/2010.12510v1) - [pdf](http://arxiv.org/pdf/2010.12510v1)

> Existing NLP datasets contain various biases, and models tend to quickly learn those biases, which in turn limits their robustness. Existing approaches to improve robustness against dataset biases mostly focus on changing the training objective so that models learn less from biased examples. Besides, they mostly focus on addressing a specific bias, and while they improve the performance on adversarial evaluation sets of the targeted bias, they may bias the model in other ways, and therefore, hurt the overall robustness. In this paper, we propose to augment the input sentences in the training data with their corresponding predicate-argument structures, which provide a higher-level abstraction over different realizations of the same meaning and help the model to recognize important parts of sentences. We show that without targeting a specific bias, our sentence augmentation improves the robustness of transformer models against multiple biases. In addition, we show that models can still be vulnerable to the lexical overlap bias, even when the training data does not contain this bias, and that the sentence augmentation also improves the robustness in this scenario. We will release our adversarial datasets to evaluate bias in such a scenario as well as our augmentation scripts at https://github.com/UKPLab/data-augmentation-for-robustness.

</details>

<details>

<summary>2020-10-23 20:02:48 - Avoiding Occupancy Detection from Smart Meter using Adversarial Machine Learning</summary>

- *ibrahim Yilmaz, Ambareen Siraj*

- `2010.12640v1` - [abs](http://arxiv.org/abs/2010.12640v1) - [pdf](http://arxiv.org/pdf/2010.12640v1)

> More and more conventional electromechanical meters are being replaced with smart meters because of their substantial benefits such as providing faster bi-directional communication between utility services and end users, enabling direct load control for demand response, energy saving, and so on. However, the fine-grained usage data provided by smart meter brings additional vulnerabilities from users to companies. Occupancy detection is one such example which causes privacy violation of smart meter users. Detecting the occupancy of a home is straightforward with time of use information as there is a strong correlation between occupancy and electricity usage. In this work, our major contributions are twofold. First, we validate the viability of an occupancy detection attack based on a machine learning technique called Long Short Term Memory (LSTM) method and demonstrate improved results. In addition, we introduce an Adversarial Machine Learning Occupancy Detection Avoidance (AMLODA) framework as a counter attack in order to prevent abuse of energy consumption. Essentially, the proposed privacy-preserving framework is designed to mask real-time or near real-time electricity usage information using calculated optimum noise without compromising users' billing systems functionality. Our results show that the proposed privacy-aware billing technique upholds users' privacy strongly.

</details>

<details>

<summary>2020-10-23 23:13:16 - Learning Assisted Side Channel Delay Test for Detection of Recycled ICs</summary>

- *Ashkan Vakil, Farzad Niknia, Ali Mirzaeian, Avesta Sasan, Naghmeh Karimi*

- `2010.12704v1` - [abs](http://arxiv.org/abs/2010.12704v1) - [pdf](http://arxiv.org/pdf/2010.12704v1)

> With the outsourcing of design flow, ensuring the security and trustworthiness of integrated circuits has become more challenging. Among the security threats, IC counterfeiting and recycled ICs have received a lot of attention due to their inferior quality, and in turn, their negative impact on the reliability and security of the underlying devices. Detecting recycled ICs is challenging due to the effect of process variations and process drift occurring during the chip fabrication. Moreover, relying on a golden chip as a basis for comparison is not always feasible. Accordingly, this paper presents a recycled IC detection scheme based on delay side-channel testing. The proposed method relies on the features extracted during the design flow and the sample delays extracted from the target chip to build a Neural Network model using which the target chip can be truly identified as new or recycled. The proposed method classifies the timing paths of the target chip into two groups based on their vulnerability to aging using the information collected from the design and detects the recycled ICs based on the deviation of the delay of these two sets from each other.

</details>

<details>

<summary>2020-10-24 14:05:03 - ATRO: Adversarial Training with a Rejection Option</summary>

- *Masahiro Kato, Zhenghang Cui, Yoshihiro Fukuhara*

- `2010.12905v1` - [abs](http://arxiv.org/abs/2010.12905v1) - [pdf](http://arxiv.org/pdf/2010.12905v1)

> This paper proposes a classification framework with a rejection option to mitigate the performance deterioration caused by adversarial examples. While recent machine learning algorithms achieve high prediction performance, they are empirically vulnerable to adversarial examples, which are slightly perturbed data samples that are wrongly classified. In real-world applications, adversarial attacks using such adversarial examples could cause serious problems. To this end, various methods are proposed to obtain a classifier that is robust against adversarial examples. Adversarial training is one of them, which trains a classifier to minimize the worst-case loss under adversarial attacks. In this paper, in order to acquire a more reliable classifier against adversarial attacks, we propose the method of Adversarial Training with a Rejection Option (ATRO). Applying the adversarial training objective to both a classifier and a rejection function simultaneously, classifiers trained by ATRO can choose to abstain from classification when it has insufficient confidence to classify a test data point. We examine the feasibility of the framework using the surrogate maximum hinge loss and establish a generalization bound for linear models. Furthermore, we empirically confirmed the effectiveness of ATRO using various models and real-world datasets.

</details>

<details>

<summary>2020-10-24 21:20:35 - Are Adversarial Examples Created Equal? A Learnable Weighted Minimax Risk for Robustness under Non-uniform Attacks</summary>

- *Huimin Zeng, Chen Zhu, Tom Goldstein, Furong Huang*

- `2010.12989v1` - [abs](http://arxiv.org/abs/2010.12989v1) - [pdf](http://arxiv.org/pdf/2010.12989v1)

> Adversarial Training is proved to be an efficient method to defend against adversarial examples, being one of the few defenses that withstand strong attacks. However, traditional defense mechanisms assume a uniform attack over the examples according to the underlying data distribution, which is apparently unrealistic as the attacker could choose to focus on more vulnerable examples. We present a weighted minimax risk optimization that defends against non-uniform attacks, achieving robustness against adversarial examples under perturbed test data distributions. Our modified risk considers importance weights of different adversarial examples and focuses adaptively on harder examples that are wrongly classified or at higher risk of being classified incorrectly. The designed risk allows the training process to learn a strong defense through optimizing the importance weights. The experiments show that our model significantly improves state-of-the-art adversarial accuracy under non-uniform attacks without a significant drop under uniform attacks.

</details>

<details>

<summary>2020-10-25 08:55:40 - Dynamic Adversarial Patch for Evading Object Detection Models</summary>

- *Shahar Hoory, Tzvika Shapira, Asaf Shabtai, Yuval Elovici*

- `2010.13070v1` - [abs](http://arxiv.org/abs/2010.13070v1) - [pdf](http://arxiv.org/pdf/2010.13070v1)

> Recent research shows that neural networks models used for computer vision (e.g., YOLO and Fast R-CNN) are vulnerable to adversarial evasion attacks. Most of the existing real-world adversarial attacks against object detectors use an adversarial patch which is attached to the target object (e.g., a carefully crafted sticker placed on a stop sign). This method may not be robust to changes in the camera's location relative to the target object; in addition, it may not work well when applied to nonplanar objects such as cars. In this study, we present an innovative attack method against object detectors applied in a real-world setup that addresses some of the limitations of existing attacks. Our method uses dynamic adversarial patches which are placed at multiple predetermined locations on a target object. An adversarial learning algorithm is applied in order to generate the patches used. The dynamic attack is implemented by switching between optimized patches dynamically, according to the camera's position (i.e., the object detection system's position). In order to demonstrate our attack in a real-world setup, we implemented the patches by attaching flat screens to the target object; the screens are used to present the patches and switch between them, depending on the current camera location. Thus, the attack is dynamic and adjusts itself to the situation to achieve optimal results. We evaluated our dynamic patch approach by attacking the YOLOv2 object detector with a car as the target object and succeeded in misleading it in up to 90% of the video frames when filming the car from a wide viewing angle range. We improved the attack by generating patches that consider the semantic distance between the target object and its classification. We also examined the attack's transferability among different car models and were able to mislead the detector 71% of the time.

</details>

<details>

<summary>2020-10-25 22:42:27 - Generalized Iris Presentation Attack Detection Algorithm under Cross-Database Settings</summary>

- *Mehak Gupta, Vishal Singh, Akshay Agarwal, Mayank Vatsa, Richa Singh*

- `2010.13244v1` - [abs](http://arxiv.org/abs/2010.13244v1) - [pdf](http://arxiv.org/pdf/2010.13244v1)

> Presentation attacks are posing major challenges to most of the biometric modalities. Iris recognition, which is considered as one of the most accurate biometric modality for person identification, has also been shown to be vulnerable to advanced presentation attacks such as 3D contact lenses and textured lens. While in the literature, several presentation attack detection (PAD) algorithms are presented; a significant limitation is the generalizability against an unseen database, unseen sensor, and different imaging environment. To address this challenge, we propose a generalized deep learning-based PAD network, MVANet, which utilizes multiple representation layers. It is inspired by the simplicity and success of hybrid algorithm or fusion of multiple detection networks. The computational complexity is an essential factor in training deep neural networks; therefore, to reduce the computational complexity while learning multiple feature representation layers, a fixed base model has been used. The performance of the proposed network is demonstrated on multiple databases such as IIITD-WVU MUIPA and IIITD-CLI databases under cross-database training-testing settings, to assess the generalizability of the proposed algorithm.

</details>

<details>

<summary>2020-10-25 23:01:13 - MixNet for Generalized Face Presentation Attack Detection</summary>

- *Nilay Sanghvi, Sushant Kumar Singh, Akshay Agarwal, Mayank Vatsa, Richa Singh*

- `2010.13246v1` - [abs](http://arxiv.org/abs/2010.13246v1) - [pdf](http://arxiv.org/pdf/2010.13246v1)

> The non-intrusive nature and high accuracy of face recognition algorithms have led to their successful deployment across multiple applications ranging from border access to mobile unlocking and digital payments. However, their vulnerability against sophisticated and cost-effective presentation attack mediums raises essential questions regarding its reliability. In the literature, several presentation attack detection algorithms are presented; however, they are still far behind from reality. The major problem with existing work is the generalizability against multiple attacks both in the seen and unseen setting. The algorithms which are useful for one kind of attack (such as print) perform unsatisfactorily for another type of attack (such as silicone masks). In this research, we have proposed a deep learning-based network termed as \textit{MixNet} to detect presentation attacks in cross-database and unseen attack settings. The proposed algorithm utilizes state-of-the-art convolutional neural network architectures and learns the feature mapping for each attack category. Experiments are performed using multiple challenging face presentation attack databases such as SMAD and Spoof In the Wild (SiW-M) databases. Extensive experiments and comparison with existing state of the art algorithms show the effectiveness of the proposed algorithm.

</details>

<details>

<summary>2020-10-26 00:41:27 - Blockchain-Empowered Socially Optimal Transactive Energy System: Framework and Implementation</summary>

- *Qing Yang, Hao Wang*

- `2010.13264v1` - [abs](http://arxiv.org/abs/2010.13264v1) - [pdf](http://arxiv.org/pdf/2010.13264v1)

> Transactive energy plays a key role in the operation and energy management of future power systems. However, the conventional operational mechanism, which follows a centralized design, is often less secure, vulnerable to malicious behaviors, and suffers from privacy leakage. In this work, we introduce blockchain technology in transactive energy to address these challenges. Specifically, we develop a novel blockchain-based transactive energy framework for prosumers and design a decentralized energy trading algorithm that matches the operation of the underlying blockchain system. We prove that the trading algorithm improves the individual benefit and guarantees the socially optimal performance, and thus incentivizes prosumers to join the transactive energy platform. Moreover, we evaluate the feasibility of the transactive energy platform throughout the implementation of a small-scale network of Internet of Things (IoT) devices and extensive simulations using real-world data. Our results show that this blockchain-based transactive energy platform is feasible in practice, and the decentralized trading algorithm reduces the user's individual cost by up to 77% and lowers the overall cost by 24%.

</details>

<details>

<summary>2020-10-26 04:51:06 - PTAuth: Temporal Memory Safety via Robust Points-to Authentication</summary>

- *Reza Mirzazade Farkhani, Mansour Ahmadi, Long Lu*

- `2002.07936v3` - [abs](http://arxiv.org/abs/2002.07936v3) - [pdf](http://arxiv.org/pdf/2002.07936v3)

> Temporal memory corruptions are commonly exploited software vulnerabilities that can lead to powerful attacks. Despite significant progress made by decades of research on mitigation techniques, existing countermeasures fall short due to either limited coverage or overly high overhead. Furthermore, they require external mechanisms (e.g., spatial memory safety) to protect their metadata. Otherwise, their protection can be bypassed or disabled. To address these limitations, we present robust points-to authentication, a novel runtime scheme for detecting all kinds of temporal memory corruptions. We built a prototype system, called PTAuth, that realizes this scheme on ARM architectures. PTAuth contains a customized compiler for code analysis and instrumentation and a runtime library for performing the points-to authentication as a protected program runs. PTAuth leverages the Pointer Authentication Code (PAC) feature, provided by the ARMv8.3 and later CPUs, which serves as a simple hardware-based encryption primitive. PTAuth uses minimal in-memory metadata and protects its metadata without requiring spatial memory safety. We report our evaluation of PTAuth in terms of security, robustness and performance using 150 vulnerable programs from Juliet test suite and the SPEC CPU2006 benchmarks. PTAuth detects all three categories of heap-based temporal memory corruptions, generates zero false alerts, and slows down program execution by 26% (this number was measured based on software-emulated PAC; it is expected to decrease to 20% when using hardware-based PAC). We also show that PTAuth incurs 2% memory overhead thanks to the efficient use of metadata.

</details>

<details>

<summary>2020-10-27 10:04:38 - The Geometry of Information Cocoon: Analyzing the Cultural Space with Word Embedding Models</summary>

- *Huimin Xu, Zhicong Chen, Ruiqi Li, Cheng-Jun Wang*

- `2007.10083v3` - [abs](http://arxiv.org/abs/2007.10083v3) - [pdf](http://arxiv.org/pdf/2007.10083v3)

> Accompanied by the development of digital media, the threat of information cocoon has become a significant issue. However, little is known about the measure of information cocoon as a cultural space and its relationship with social class. This study addresses this problem by constructing the cultural space with word embedding models and random shuffling methods among three large-scale digital media use datasets. In the light of field theory of cultural production, we investigate the information cocoon effect on different social classes among 979 computer users, 100,000 smartphone users, and 159,373 mobile reading application users. Our analysis reveals that information cocoons widely exist in the daily use of digital media. Moreover, people of lower social class have a higher probability of getting stuck in the information cocoon filled with the entertainment content. In contrast, the people of higher social class have more capability to stride over the constraints of the information cocoon. The results suggest that the disadvantages for vulnerable groups in acquiring knowledge may further widen social inequality.

</details>

<details>

<summary>2020-10-27 23:12:58 - Shredder: Learning Noise Distributions to Protect Inference Privacy</summary>

- *Fatemehsadat Mireshghallah, Mohammadkazem Taram, Prakash Ramrakhyani, Dean Tullsen, Hadi Esmaeilzadeh*

- `1905.11814v3` - [abs](http://arxiv.org/abs/1905.11814v3) - [pdf](http://arxiv.org/pdf/1905.11814v3)

> A wide variety of deep neural applications increasingly rely on the cloud to perform their compute-heavy inference. This common practice requires sending private and privileged data over the network to remote servers, exposing it to the service provider and potentially compromising its privacy. Even if the provider is trusted, the data can still be vulnerable over communication channels or via side-channel attacks in the cloud. To that end, this paper aims to reduce the information content of the communicated data with as little as possible compromise on the inference accuracy by making the sent data noisy. An undisciplined addition of noise can significantly reduce the accuracy of inference, rendering the service unusable. To address this challenge, this paper devises Shredder, an end-to-end framework, that, without altering the topology or the weights of a pre-trained network, learns additive noise distributions that significantly reduce the information content of communicated data while maintaining the inference accuracy. The key idea is finding the additive noise distributions by casting it as a disjoint offline learning process with a loss function that strikes a balance between accuracy and information degradation. The loss function also exposes a knob for a disciplined and controlled asymmetric trade-off between privacy and accuracy. Experimentation with six real-world DNNs from text processing and image classification shows that Shredder reduces the mutual information between the input and the communicated data to the cloud by 74.70% compared to the original execution while only sacrificing 1.58% loss in accuracy. On average, Shredder also offers a speedup of 1.79x over Wi-Fi and 2.17x over LTE compared to cloud-only execution when using an off-the-shelf mobile GPU (Tegra X2) on the edge.

</details>

<details>

<summary>2020-10-28 04:50:38 - EC-SVC: Secure CAN Bus In-Vehicle Communications with Fine-grained Access Control Based on Edge Computing</summary>

- *Donghyun Yu, Ruei-Hau Hsu, Jemin Lee*

- `2010.14747v1` - [abs](http://arxiv.org/abs/2010.14747v1) - [pdf](http://arxiv.org/pdf/2010.14747v1)

> In-vehicle communications are not designed for message exchange between the vehicles and outside systems originally. Thus, the security design of message protection is insufficient. Moreover, the internal devices do not have enough resources to process the additional security operations. Nonetheless, due to the characteristic of the in-vehicle network in which messages are broadcast, secure message transmission to specific receivers must be ensured. With consideration of the facts aforementioned, this work addresses resource problems by offloading secure operations to high-performance devices, and uses attribute-based access control to ensure the confidentiality of messages from attackers and unauthorized users. In addition, we reconfigure existing access control based cryptography to address new vulnerabilities arising from the use of edge computing and attribute-based access control. Thus, this paper proposes an edge computing-based security protocol with fine-grained attribute-based encryption using a hash function, symmetric-based cryptography, and reconfigured cryptographic scheme. In addition, this work formally proves the reconfigured cryptographic scheme and security protocol, and evaluates the feasibility of the proposed security protocol in various aspects using the CANoe software.

</details>

<details>

<summary>2020-10-28 13:14:44 - CoinWatch: A Clone-Based Approach For Detecting Vulnerabilities in Cryptocurrencies</summary>

- *Qingze Hum, Wei Jin Tan, Shi Ying Tey, Latasha Lenus, Ivan Homoliak, Yun Lin, Jun Sun*

- `2006.10280v2` - [abs](http://arxiv.org/abs/2006.10280v2) - [pdf](http://arxiv.org/pdf/2006.10280v2)

> Cryptocurrencies have become very popular in recent years. Thousands of new cryptocurrencies have emerged, proposing new and novel techniques that improve on Bitcoin's core innovation of the blockchain data structure and consensus mechanism. However, cryptocurrencies are a major target for cyber-attacks, as they can be sold on exchanges anonymously and most cryptocurrencies have their codebases publicly available. One particular issue is the prevalence of code clones in cryptocurrencies, which may amplify security threats. If a vulnerability is found in one cryptocurrency, it might be propagated into other cloned cryptocurrencies. In this work, we propose a systematic remedy to this problem, and we propose CoinWatch (CW). Given a reported vulnerability at the input, CW uses the code evolution analysis and a clone detection technique for indication of cryptocurrencies that might be vulnerable. We applied CW on 1094 cryptocurrencies using 4 CVEs and obtained 786 true vulnerabilities present in 384 projects, which were confirmed with developers and successfully reported as CVE extensions.

</details>

<details>

<summary>2020-10-28 13:25:39 - The Security Reference Architecture for Blockchains: Towards a Standardized Model for Studying Vulnerabilities, Threats, and Defenses</summary>

- *Ivan Homoliak, Sarad Venugopalan, Qingze Hum, Daniel Reijsbergen, Richard Schumi, Pawel Szalachowski*

- `1910.09775v3` - [abs](http://arxiv.org/abs/1910.09775v3) - [pdf](http://arxiv.org/pdf/1910.09775v3)

> Blockchains are distributed systems, in which security is a critical factor for their success. However, despite their increasing popularity and adoption, there is a lack of standardized models that study blockchain-related security threats. To fill this gap, the main focus of our work is to systematize and extend the knowledge about the security and privacy aspects of blockchains and contribute to the standardization of this domain.   We propose the security reference architecture (SRA) for blockchains, which adopts a stacked model (similar to the ISO/OSI) describing the nature and hierarchy of various security and privacy aspects. The SRA contains four layers: (1) the network layer, (2) the consensus layer, (3) the replicated state machine layer, and (4) the application layer. At each of these layers, we identify known security threats, their origin, and countermeasures, while we also analyze several cross-layer dependencies. Next, to enable better reasoning about security aspects of blockchains by the practitioners, we propose a blockchain-specific version of the threat-risk assessment standard ISO/IEC 15408 by embedding the stacked model into this standard. Finally, we provide designers of blockchain platforms and applications with a design methodology following the model of SRA and its hierarchy.

</details>

<details>

<summary>2020-10-29 13:40:22 - Minimalistic Attacks: How Little it Takes to Fool a Deep Reinforcement Learning Policy</summary>

- *Xinghua Qu, Zhu Sun, Yew-Soon Ong, Abhishek Gupta, Pengfei Wei*

- `1911.03849v5` - [abs](http://arxiv.org/abs/1911.03849v5) - [pdf](http://arxiv.org/pdf/1911.03849v5)

> Recent studies have revealed that neural network-based policies can be easily fooled by adversarial examples. However, while most prior works analyze the effects of perturbing every pixel of every frame assuming white-box policy access, in this paper we take a more restrictive view towards adversary generation - with the goal of unveiling the limits of a model's vulnerability. In particular, we explore minimalistic attacks by defining three key settings: (1) black-box policy access: where the attacker only has access to the input (state) and output (action probability) of an RL policy; (2) fractional-state adversary: where only several pixels are perturbed, with the extreme case being a single-pixel adversary; and (3) tactically-chanced attack: where only significant frames are tactically chosen to be attacked. We formulate the adversarial attack by accommodating the three key settings and explore their potency on six Atari games by examining four fully trained state-of-the-art policies. In Breakout, for example, we surprisingly find that: (i) all policies showcase significant performance degradation by merely modifying 0.01% of the input state, and (ii) the policy trained by DQN is totally deceived by perturbation to only 1% frames.

</details>

<details>

<summary>2020-10-29 22:50:31 - Examining the Relationship of Code and Architectural Smells with Software Vulnerabilities</summary>

- *Kazi Zakia Sultana, Zadia Codabux, Byron Williams*

- `2010.15978v1` - [abs](http://arxiv.org/abs/2010.15978v1) - [pdf](http://arxiv.org/pdf/2010.15978v1)

> Context: Security is vital to software developed for commercial or personal use. Although more organizations are realizing the importance of applying secure coding practices, in many of them, security concerns are not known or addressed until a security failure occurs. The root cause of security failures is vulnerable code. While metrics have been used to predict software vulnerabilities, we explore the relationship between code and architectural smells with security weaknesses. As smells are surface indicators of a deeper problem in software, determining the relationship between smells and software vulnerabilities can play a significant role in vulnerability prediction models. Objective: This study explores the relationship between smells and software vulnerabilities to identify the smells. Method: We extracted the class, method, file, and package level smells for three systems: Apache Tomcat, Apache CXF, and Android. We then compared their occurrences in the vulnerable classes which were reported to contain vulnerable code and in the neutral classes (non-vulnerable classes where no vulnerability had yet been reported). Results: We found that a vulnerable class is more likely to have certain smells compared to a non-vulnerable class. God Class, Complex Class, Large Class, Data Class, Feature Envy, Brain Class have a statistically significant relationship with software vulnerabilities. We found no significant relationship between architectural smells and software vulnerabilities. Conclusion: We can conclude that for all the systems examined, there is a statistically significant correlation between software vulnerabilities and some smells.

</details>

<details>

<summary>2020-10-30 07:17:12 - Perception Improvement for Free: Exploring Imperceptible Black-box Adversarial Attacks on Image Classification</summary>

- *Yongwei Wang, Mingquan Feng, Rabab Ward, Z. Jane Wang, Lanjun Wang*

- `2011.05254v1` - [abs](http://arxiv.org/abs/2011.05254v1) - [pdf](http://arxiv.org/pdf/2011.05254v1)

> Deep neural networks are vulnerable to adversarial attacks. White-box adversarial attacks can fool neural networks with small adversarial perturbations, especially for large size images. However, keeping successful adversarial perturbations imperceptible is especially challenging for transfer-based black-box adversarial attacks. Often such adversarial examples can be easily spotted due to their unpleasantly poor visual qualities, which compromises the threat of adversarial attacks in practice. In this study, to improve the image quality of black-box adversarial examples perceptually, we propose structure-aware adversarial attacks by generating adversarial images based on psychological perceptual models. Specifically, we allow higher perturbations on perceptually insignificant regions, while assigning lower or no perturbation on visually sensitive regions. In addition to the proposed spatial-constrained adversarial perturbations, we also propose a novel structure-aware frequency adversarial attack method in the discrete cosine transform (DCT) domain. Since the proposed attacks are independent of the gradient estimation, they can be directly incorporated with existing gradient-based attacks. Experimental results show that, with the comparable attack success rate (ASR), the proposed methods can produce adversarial examples with considerably improved visual quality for free. With the comparable perceptual quality, the proposed approaches achieve higher attack success rates: particularly for the frequency structure-aware attacks, the average ASR improves more than 10% over the baseline attacks.

</details>

<details>

<summary>2020-10-31 03:18:36 - Evaluation of Inference Attack Models for Deep Learning on Medical Data</summary>

- *Maoqiang Wu, Xinyue Zhang, Jiahao Ding, Hien Nguyen, Rong Yu, Miao Pan, Stephen T. Wong*

- `2011.00177v1` - [abs](http://arxiv.org/abs/2011.00177v1) - [pdf](http://arxiv.org/pdf/2011.00177v1)

> Deep learning has attracted broad interest in healthcare and medical communities. However, there has been little research into the privacy issues created by deep networks trained for medical applications. Recently developed inference attack algorithms indicate that images and text records can be reconstructed by malicious parties that have the ability to query deep networks. This gives rise to the concern that medical images and electronic health records containing sensitive patient information are vulnerable to these attacks. This paper aims to attract interest from researchers in the medical deep learning community to this important problem. We evaluate two prominent inference attack models, namely, attribute inference attack and model inversion attack. We show that they can reconstruct real-world medical images and clinical reports with high fidelity. We then investigate how to protect patients' privacy using defense mechanisms, such as label perturbation and model perturbation. We provide a comparison of attack results between the original and the medical deep learning models with defenses. The experimental evaluations show that our proposed defense approaches can effectively reduce the potential privacy leakage of medical deep learning from the inference attacks.

</details>


## 2020-11

<details>

<summary>2020-11-01 04:21:48 - Monitoring-based Differential Privacy Mechanism Against Query-Flooding Parameter Duplication Attack</summary>

- *Haonan Yan, Xiaoguang Li, Hui Li, Jiamin Li, Wenhai Sun, Fenghua Li*

- `2011.00418v1` - [abs](http://arxiv.org/abs/2011.00418v1) - [pdf](http://arxiv.org/pdf/2011.00418v1)

> Public intelligent services enabled by machine learning algorithms are vulnerable to model extraction attacks that can steal confidential information of the learning models through public queries. Though there are some protection options such as differential privacy (DP) and monitoring, which are considered promising techniques to mitigate this attack, we still find that the vulnerability persists. In this paper, we propose an adaptive query-flooding parameter duplication (QPD) attack. The adversary can infer the model information with black-box access and no prior knowledge of any model parameters or training data via QPD. We also develop a defense strategy using DP called monitoring-based DP (MDP) against this new attack. In MDP, we first propose a novel real-time model extraction status assessment scheme called Monitor to evaluate the situation of the model. Then, we design a method to guide the differential privacy budget allocation called APBA adaptively. Finally, all DP-based defenses with MDP could dynamically adjust the amount of noise added in the model response according to the result from Monitor and effectively defends the QPD attack. Furthermore, we thoroughly evaluate and compare the QPD attack and MDP defense performance on real-world models with DP and monitoring protection.

</details>

<details>

<summary>2020-11-01 17:17:10 - LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of Point Cloud-based Deep Networks</summary>

- *Hang Zhou, Dongdong Chen, Jing Liao, Weiming Zhang, Kejiang Chen, Xiaoyi Dong, Kunlin Liu, Gang Hua, Nenghai Yu*

- `2011.00566v1` - [abs](http://arxiv.org/abs/2011.00566v1) - [pdf](http://arxiv.org/pdf/2011.00566v1)

> Deep neural networks have made tremendous progress in 3D point-cloud recognition. Recent works have shown that these 3D recognition networks are also vulnerable to adversarial samples produced from various attack methods, including optimization-based 3D Carlini-Wagner attack, gradient-based iterative fast gradient method, and skeleton-detach based point-dropping. However, after a careful analysis, these methods are either extremely slow because of the optimization/iterative scheme, or not flexible to support targeted attack of a specific category. To overcome these shortcomings, this paper proposes a novel label guided adversarial network (LG-GAN) for real-time flexible targeted point cloud attack. To the best of our knowledge, this is the first generation based 3D point cloud attack method. By feeding the original point clouds and target attack label into LG-GAN, it can learn how to deform the point clouds to mislead the recognition network into the specific label only with a single forward pass. In detail, LGGAN first leverages one multi-branch adversarial network to extract hierarchical features of the input point clouds, then incorporates the specified label information into multiple intermediate features using the label encoder. Finally, the encoded features will be fed into the coordinate reconstruction decoder to generate the target adversarial sample. By evaluating different point-cloud recognition models (e.g., PointNet, PointNet++ and DGCNN), we demonstrate that the proposed LG-GAN can support flexible targeted attack on the fly while guaranteeing good attack performance and higher efficiency simultaneously.

</details>

<details>

<summary>2020-11-02 06:52:33 - Ridesharing Services and Car-Seats: Technological Perceptions and Usage Patterns</summary>

- *Subasish Das*

- `2011.02277v1` - [abs](http://arxiv.org/abs/2011.02277v1) - [pdf](http://arxiv.org/pdf/2011.02277v1)

> Children are one of the most vulnerable groups in traffic crashes. Child safety seats (CSSs) can decrease the severity of crash outcomes for children. The usage of CSSs has significantly improved in the U.S. over the last 40 years, but it is anticipated that the usage of CSSs in popular ridesharing services (RSSs), such as Uber and Lyft, is not widespread. This paper used a publicly available nationwide internet survey that was designed to gain an understanding of riders and drivers perception toward child passenger safety in regard to technological perception on RSSs. This study performed a rigorous exploratory data analysis to identify the key psychological insights of the survey participants. Additionally, a recently developed dimension-reduction method has been applied to understand the co-occurrence patterns of the responses to gain intuitive insights. It is found that urban-dwelling parents with higher education degrees eventually use RSSs often due to their familiarity of the technological advantages. On the other hand, non-urban and moderately educated parents and guardians are dismissive in using RSSs while having kids with them to ride due to less trust on the technology.

</details>

<details>

<summary>2020-11-02 10:29:31 - Total Eclipse of the Heart -- Disrupting the InterPlanetary File System</summary>

- *Bernd Prünster, Alexander Marsalek, Thomas Zefferer*

- `2011.00874v1` - [abs](http://arxiv.org/abs/2011.00874v1) - [pdf](http://arxiv.org/pdf/2011.00874v1)

> Peer-to-peer networks are an attractive alternative to classical client-server architectures in several fields of application such as voice-over-IP telephony and file sharing. Recently, a new peer-to-peer solution called the InterPlanetary File System (IPFS) has attracted attention, which promises to re-decentralise the Web. Being increasingly used as a stand-alone application, IPFS has also emerged as the technical backbone of various other decentralised solutions and was even used to evade censorship. Decentralised applications serving millions of users rely on IPFS as one of their crucial building blocks. This popularity makes IPFS attractive for large-scale attacks. We have identified a conceptual issue in one of IPFS's core libraries and demonstrate their exploitation by means of a successful end-to-end attack. We evaluated this attack against the IPFS reference implementation on the public IPFS network, which is used by the average user to share and consume IPFS content. Results obtained from mounting this attack on live IPFS nodes show that arbitrary IPFS nodes can be eclipsed, i.e. isolated from the network, with moderate effort and limited resources. Compared to similar works, we show that our attack scales linearly even beyond current network sizes and can disrupt the entire public IPFS network with alarmingly low effort. The vulnerability set described in this paper has been assigned CVE-2020-10937. Responsible disclosure procedures are currently being carried out and have led to mitigations being deployed, with additional fixes to be rolled out in future releases.

</details>

<details>

<summary>2020-11-02 16:56:56 - Contact Tracing Made Un-relay-able</summary>

- *Marco Casagrande, Mauro Conti, Eleonora Losiouk*

- `2010.12641v2` - [abs](http://arxiv.org/abs/2010.12641v2) - [pdf](http://arxiv.org/pdf/2010.12641v2)

> Automated contact tracing is a key solution to control the spread of airborne transmittable diseases: it traces contacts among individuals in order to alert people about their potential risk of being infected. The current SARS-CoV-2 pandemic put a heavy strain on the healthcare system of many countries. Governments chose different approaches to face the spread of the virus and the contact tracing apps were considered the most effective ones. In particular, by leveraging on the Bluetooth Low-Energy technology, mobile apps allow to achieve a privacy-preserving contact tracing of citizens. While researchers proposed several contact tracing approaches, each government developed its own national contact tracing app.   In this paper, we demonstrate that many popular contact tracing apps (e.g., the ones promoted by the Italian, French, Swiss government) are vulnerable to relay attacks. Through such attacks people might get misleadingly diagnosed as positive to SARS-CoV-2, thus being enforced to quarantine and eventually leading to a breakdown of the healthcare system. To tackle this vulnerability, we propose a novel and lightweight solution that prevents relay attacks, while providing the same privacy-preserving features as the current approaches. To evaluate the feasibility of both the relay attack and our novel defence mechanism, we developed a proof of concept against the Italian contact tracing app (i.e., Immuni). The design of our defence allows it to be integrated into any contact tracing app.

</details>

<details>

<summary>2020-11-03 13:20:28 - Characterising attacks targeting low-cost routers: a MikroTik case study (Extended)</summary>

- *Joao M. Ceron, Christian Scholten, Aiko Pras, Elmer Lastdrager, Jair Santanna*

- `2011.01685v1` - [abs](http://arxiv.org/abs/2011.01685v1) - [pdf](http://arxiv.org/pdf/2011.01685v1)

> Attacks targeting network infrastructure devices pose a threat to the security of the internet. An attack targeting such devices can affect an entire autonomous system. In recent years, malware such as VPNFilter, Navidade, and SonarDNS has been used to compromise low-cost routers and commit all sorts of cybercrimes from DDoS attacks to ransomware deployments. Routers of the type concerned are used both to provide last-mile access for home users and to manage interdomain routing (BGP). MikroTik is a particular brand of low-cost router. In our previous research, we found more than 4 million MikroTik routers available on the internet. We have shown that these devices are also popular in Internet Exchange infrastructures. Despite their popularity, these devices are known to have numerous vulnerabilities. In this paper, we extend our previous analysis by presenting a long-term investigation of MikroTik-targeted attacks. By using a highly interactive honeypot that we developed, we collected more than 44 million packets over 120 days, from sensors deployed in Australia, Brazil, China, India, the Netherlands, and the United States. The incoming traffic was classified on the basis of Common Vulnerabilities and Exposures to detect attacks targeting MikroTik devices. That enabled us to identify a wide range of activities on the system, such as cryptocurrency mining, DNS server redirection, and more than 3,000 successfully established tunnels used for eavesdropping. Although this research focuses on Mikrotik devices, both the methodology and the publicly available scripts can be easily applied to any other type of network device.

</details>

<details>

<summary>2020-11-03 15:50:04 - A novel group based cryptosystem based on electromagnetic rotor machine</summary>

- *Ashish Kumar, N S Raghava*

- `2011.01803v1` - [abs](http://arxiv.org/abs/2011.01803v1) - [pdf](http://arxiv.org/pdf/2011.01803v1)

> In this paper, an algorithm is aimed to make a cryptosystem for gray level images based on voice features, secret sharing scheme and electromagnetic rotor machine. Here, Shamir secret sharing (k n) threshold scheme is used to secure a key along with voice features of (n k) users. Keystream is molded by coefficients of a voice sample, using this key stream, rotor machines rotating cylinders positions are initialized and internal wiring is decided by pseudo random number of Henon chaotic map, where initial seed for chaotic system is chosen from keystream. And furthermore, shares of key stream are distributed among users. Speech processing is fused with electromagnetic machine to provide authentication as well as group based encryption. Perceptual linear predication (PLP) coefficients are utilized for formation of secret key. Simulation experiments and statistical analysis demonstrate that the proposed algorithm is sensitive to initial secret keystream, entropy, mean value analysis and histogram of the encrypted image is admirable. Hence, the proposed scheme is resistible to any vulnerable situation.

</details>

<details>

<summary>2020-11-03 17:01:44 - Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks</summary>

- *Denis Emelin, Ivan Titov, Rico Sennrich*

- `2011.01846v1` - [abs](http://arxiv.org/abs/2011.01846v1) - [pdf](http://arxiv.org/pdf/2011.01846v1)

> Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.

</details>

<details>

<summary>2020-11-03 21:50:56 - Online Discoverability and Vulnerabilities of ICS/SCADA Devices in the Netherlands</summary>

- *Joao M. Ceron, Justyna J. Chromik, Jair Santanna, Aiko Pras*

- `2011.02019v1` - [abs](http://arxiv.org/abs/2011.02019v1) - [pdf](http://arxiv.org/pdf/2011.02019v1)

> On a regular basis, we read in the news about cyber-attacks on critical infrastructures, such as power plants. Such infrastructures rely on the so-called Industrial Control Systems (ICS) / Supervisory Control And Data Acquisition (SCADA) networks. By hacking the devices in such systems and networks, attackers may take over the control of critical infrastructures, with potentially devastating consequences. This report focusses on critical infrastructures in the Netherlands and investigates three main questions: 1) How many ICS/SCADA devices located in the Netherlands can be easily found by potential attackers?, 2) How many of these devices are vulnerable to cyber-attacks?, and 3) What measures should be taken to prevent these devices from being hacked?

</details>

<details>

<summary>2020-11-03 22:36:27 - Face Morphing Attack Generation & Detection: A Comprehensive Survey</summary>

- *Sushma Venkatesh, Raghavendra Ramachandra, Kiran Raja, Christoph Busch*

- `2011.02045v1` - [abs](http://arxiv.org/abs/2011.02045v1) - [pdf](http://arxiv.org/pdf/2011.02045v1)

> The vulnerability of Face Recognition System (FRS) to various kind of attacks (both direct and in-direct attacks) and face morphing attacks has received a great interest from the biometric community. The goal of a morphing attack is to subvert the FRS at Automatic Border Control (ABC) gates by presenting the Electronic Machine Readable Travel Document (eMRTD) or e-passport that is obtained based on the morphed face image. Since the application process for the e-passport in the majority countries requires a passport photo to be presented by the applicant, a malicious actor and the accomplice can generate the morphed face image and to obtain the e-passport. An e-passport with a morphed face images can be used by both the malicious actor and the accomplice to cross the border as the morphed face image can be verified against both of them. This can result in a significant threat as a malicious actor can cross the border without revealing the track of his/her criminal background while the details of accomplice are recorded in the log of the access control system. This survey aims to present a systematic overview of the progress made in the area of face morphing in terms of both morph generation and morph detection. In this paper, we describe and illustrate various aspects of face morphing attacks, including different techniques for generating morphed face images but also the state-of-the-art regarding Morph Attack Detection (MAD) algorithms based on a stringent taxonomy and finally the availability of public databases, which allow to benchmark new MAD algorithms in a reproducible manner. The outcomes of competitions/benchmarking, vulnerability assessments and performance evaluation metrics are also provided in a comprehensive manner. Furthermore, we discuss the open challenges and potential future works that need to be addressed in this evolving field of biometrics.

</details>

<details>

<summary>2020-11-04 17:08:54 - Identity and Personhood in Digital Democracy: Evaluating Inclusion, Equality, Security, and Privacy in Pseudonym Parties and Other Proofs of Personhood</summary>

- *Bryan Ford*

- `2011.02412v1` - [abs](http://arxiv.org/abs/2011.02412v1) - [pdf](http://arxiv.org/pdf/2011.02412v1)

> Digital identity seems like a prerequisite for digital democracy: how can we ensure "one person, one vote" online without identifying voters? But digital identity solutions - ID checking, biometrics, self-sovereign identity, and trust networks - all present flaws, leaving users vulnerable to exclusion, identity loss or theft, and coercion. These flaws may be insurmountable because digital identity is a cart pulling the horse. We cannot achieve digital identity secure enough for the weight of digital democracy, until we build it on a solid foundation of "digital personhood." While identity is about distinguishing one person from another through attributes or affiliations, personhood is about giving all real people inalienable digital participation rights independent of identity, including protection against erosion of their democratic rights through identity loss, theft, coercion, or fakery.   We explore and analyze alternative approaches to "proof of personhood" that may provide this missing foundation. Pseudonym parties marry the transparency of periodic physical-world events with the power of digital tokens between events. These tokens represent limited-term but renewable claims usable for purposes such as online voting or liquid democracy, sampled juries or deliberative polls, abuse-resistant social communication, or minting universal basic income in a permissionless cryptocurrency. Enhancing pseudonym parties to provide participants a moment of enforced physical security and privacy can address coercion and vote-buying risks that plague today's E-voting systems. We also examine other proposed approaches to proof of personhood, some of which offer conveniences such as all-online participation. These alternatives currently fall short of satisfying all the key digital personhood goals, unfortunately, but offer valuable insights into the challenges we face.

</details>

<details>

<summary>2020-11-04 21:48:52 - BDoS: Blockchain Denial of Service</summary>

- *Michael Mirkin, Yan Ji, Jonathan Pang, Ariah Klages-Mundt, Ittay Eyal, Ari Juels*

- `1912.07497v4` - [abs](http://arxiv.org/abs/1912.07497v4) - [pdf](http://arxiv.org/pdf/1912.07497v4)

> Proof-of-work (PoW) cryptocurrency blockchains like Bitcoin secure vast amounts of money. Their operators, called miners, expend resources to generate blocks and receive monetary rewards for their effort. Blockchains are, in principle, attractive targets for Denial-of-Service (DoS) attacks: There is fierce competition among coins, as well as potential gains from short selling. Classical DoS attacks, however, typically target a few servers and cannot scale to systems with many nodes. There have been no successful DoS attacks to date against prominent cryptocurrencies. We present Blockchain DoS (BDoS), the first incentive-based DoS attack that targets PoW cryptocurrencies. Unlike classical DoS, BDoS targets the system's mechanism design: It exploits the reward mechanism to discourage miner participation. Previous DoS attacks against PoW blockchains require an adversary's mining power to match that of all other miners. In contrast, BDoS can cause a blockchain to grind to a halt with significantly fewer resources, e.g., 21% as of March 2020 in Bitcoin, according to our empirical study. We find that Bitcoin's vulnerability to BDoS increases rapidly as the mining industry matures and profitability drops. BDoS differs from known attacks like Selfish Mining in its aim not to increase an adversary's revenue, but to disrupt the system. Although it bears some algorithmic similarity to those attacks, it introduces a new adversarial model, goals, algorithm, and game-theoretic analysis. Beyond its direct implications for operational blockchains, BDoS introduces the novel idea that an adversary can manipulate miners' incentives by proving the existence of blocks without actually publishing them.

</details>

<details>

<summary>2020-11-04 23:55:25 - An Overview of UPnP-based IoT Security: Threats, Vulnerabilities, and Prospective Solutions</summary>

- *Golam Kayas, Mahmud Hossain, Jamie Payton, S. M. Riazul Islam*

- `2011.02587v1` - [abs](http://arxiv.org/abs/2011.02587v1) - [pdf](http://arxiv.org/pdf/2011.02587v1)

> Advances in the development and increased availability of smart devices ranging from small sensors to complex cloud infrastructures as well as various networking technologies and communication protocols have supported the rapid expansion of Internet of Things deployments. The Universal Plug and Play (UPnP) protocol has been widely accepted and used in the IoT domain to support interactions among heterogeneous IoT devices, in part due to zero configuration implementation which makes it feasible for use in large-scale networks. The popularity and ubiquity of UPnP to support IoT systems necessitate an exploration of security risks associated with the use of the protocol for IoT deployments. In this work, we analyze security vulnerabilities of UPnP-based IoT systems and identify attack opportunities by the adversaries leveraging the vulnerabilities. Finally, we propose prospective solutions to secure UPnP-based IoT systems from adversarial operations.

</details>

<details>

<summary>2020-11-05 08:43:12 - A Black-Box Attack Model for Visually-Aware Recommender Systems</summary>

- *Rami Cohen, Oren Sar Shalom, Dietmar Jannach, Amihood Amir*

- `2011.02701v1` - [abs](http://arxiv.org/abs/2011.02701v1) - [pdf](http://arxiv.org/pdf/2011.02701v1)

> Due to the advances in deep learning, visually-aware recommender systems (RS) have recently attracted increased research interest. Such systems combine collaborative signals with images, usually represented as feature vectors outputted by pre-trained image models. Since item catalogs can be huge, recommendation service providers often rely on images that are supplied by the item providers. In this work, we show that relying on such external sources can make an RS vulnerable to attacks, where the goal of the attacker is to unfairly promote certain pushed items. Specifically, we demonstrate how a new visual attack model can effectively influence the item scores and rankings in a black-box approach, i.e., without knowing the parameters of the model. The main underlying idea is to systematically create small human-imperceptible perturbations of the pushed item image and to devise appropriate gradient approximation methods to incrementally raise the pushed item's score. Experimental evaluations on two datasets show that the novel attack model is effective even when the contribution of the visual features to the overall performance of the recommender system is modest.

</details>

<details>

<summary>2020-11-05 16:36:58 - Street to Cloud: Improving Flood Maps With Crowdsourcing and Semantic Segmentation</summary>

- *Veda Sunkara, Matthew Purri, Bertrand Le Saux, Jennifer Adams*

- `2011.08010v1` - [abs](http://arxiv.org/abs/2011.08010v1) - [pdf](http://arxiv.org/pdf/2011.08010v1)

> To address the mounting destruction caused by floods in climate-vulnerable regions, we propose Street to Cloud, a machine learning pipeline for incorporating crowdsourced ground truth data into the segmentation of satellite imagery of floods. We propose this approach as a solution to the labor-intensive task of generating high-quality, hand-labeled training data, and demonstrate successes and failures of different plausible crowdsourcing approaches in our model. Street to Cloud leverages community reporting and machine learning to generate novel, near-real time insights into the extent of floods to be used for emergency response.

</details>

<details>

<summary>2020-11-05 21:59:37 - Evaluating the Performance of Twitter-based Exploit Detectors</summary>

- *Daniel Alves de Sousa, Elaine Ribeiro de Faria, Rodrigo Sanches Miani*

- `2011.03113v1` - [abs](http://arxiv.org/abs/2011.03113v1) - [pdf](http://arxiv.org/pdf/2011.03113v1)

> Patch prioritization is a crucial aspect of information systems security, and knowledge of which vulnerabilities were exploited in the wild is a powerful tool to help systems administrators accomplish this task. The analysis of social media for this specific application can enhance the results and bring more agility by collecting data from online discussions and applying machine learning techniques to detect real-world exploits. In this paper, we use a technique that combines Twitter data with public database information to classify vulnerabilities as exploited or not-exploited. We analyze the behavior of different classifying algorithms, investigate the influence of different antivirus data as ground truth, and experiment with various time window sizes. Our findings suggest that using a Light Gradient Boosting Machine (LightGBM) can benefit the results, and for most cases, the statistics related to a tweet and the users who tweeted are more meaningful than the text tweeted. We also demonstrate the importance of using ground-truth data from security companies not mentioned in previous works.

</details>

<details>

<summary>2020-11-05 23:29:41 - Evaluation of vulnerability reproducibility in container-based Cyber Range</summary>

- *Ryotaro Nakata, Akira Otsuka*

- `2010.16024v2` - [abs](http://arxiv.org/abs/2010.16024v2) - [pdf](http://arxiv.org/pdf/2010.16024v2)

> A cyber range, a practical and highly educational information security exercise system, is difficult to implement in educational institutions because of the high cost of implementing and maintaining it. Therefore, there is a need for a cyber range that can be adopted and maintained at a low cost. Recently, container type virtualization is gaining attention as it can create a high-speed and high-density exercise environment. However, existing researches have not clearly shown the advantages of container virtualization for building exercise environments. And it is not clear whether the sufficient vulnerabilities are reproducible, which is required to conduct incident scenarios in cyber range. In this paper, we compare container virtualization with existing virtualization type and confirm that the amount of memory, CPU, and storage consumption can be reduced to less than 1/10 of the conventional virtualization methods. We also compare and verify the reproducibility of the vulnerabilities used in common exercise scenarios and confirm that 99.3% of the vulnerabilities are reproducible. The container-based cyber range can be used as a new standard to replace existing methods.

</details>

<details>

<summary>2020-11-06 13:27:07 - Mind the GAP: Security & Privacy Risks of Contact Tracing Apps</summary>

- *Lars Baumgärtner, Alexandra Dmitrienko, Bernd Freisleben, Alexander Gruler, Jonas Höchst, Joshua Kühlberg, Mira Mezini, Richard Mitev, Markus Miettinen, Anel Muhamedagic, Thien Duc Nguyen, Alvar Penning, Dermot Frederik Pustelnik, Filipp Roos, Ahmad-Reza Sadeghi, Michael Schwarz, Christian Uhl*

- `2006.05914v2` - [abs](http://arxiv.org/abs/2006.05914v2) - [pdf](http://arxiv.org/pdf/2006.05914v2)

> Google and Apple have jointly provided an API for exposure notification in order to implement decentralized contract tracing apps using Bluetooth Low Energy, the so-called "Google/Apple Proposal", which we abbreviate by "GAP". We demonstrate that in real-world scenarios the current GAP design is vulnerable to (i) profiling and possibly de-anonymizing infected persons, and (ii) relay-based wormhole attacks that basically can generate fake contacts with the potential of affecting the accuracy of an app-based contact tracing system. For both types of attack, we have built tools that can easily be used on mobile phones or Raspberry Pis (e.g., Bluetooth sniffers). The goal of our work is to perform a reality check towards possibly providing empirical real-world evidence for these two privacy and security risks. We hope that our findings provide valuable input for developing secure and privacy-preserving digital contact tracing systems.

</details>

<details>

<summary>2020-11-06 17:07:34 - A survey on practical adversarial examples for malware classifiers</summary>

- *Daniel Park, Bülent Yener*

- `2011.05973v1` - [abs](http://arxiv.org/abs/2011.05973v1) - [pdf](http://arxiv.org/pdf/2011.05973v1)

> Machine learning based solutions have been very helpful in solving problems that deal with immense amounts of data, such as malware detection and classification. However, deep neural networks have been found to be vulnerable to adversarial examples, or inputs that have been purposefully perturbed to result in an incorrect label. Researchers have shown that this vulnerability can be exploited to create evasive malware samples. However, many proposed attacks do not generate an executable and instead generate a feature vector. To fully understand the impact of adversarial examples on malware detection, we review practical attacks against malware classifiers that generate executable adversarial malware examples. We also discuss current challenges in this area of research, as well as suggestions for improvement and future research directions.

</details>

<details>

<summary>2020-11-07 01:52:13 - Privacy in Deep Learning: A Survey</summary>

- *Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, Ramesh Raskar, Hadi Esmaeilzadeh*

- `2004.12254v5` - [abs](http://arxiv.org/abs/2004.12254v5) - [pdf](http://arxiv.org/pdf/2004.12254v5)

> The ever-growing advances of deep learning in many areas including vision, recommendation systems, natural language processing, etc., have led to the adoption of Deep Neural Networks (DNNs) in production systems. The availability of large datasets and high computational power are the main contributors to these advances. The datasets are usually crowdsourced and may contain sensitive information. This poses serious privacy concerns as this data can be misused or leaked through various vulnerabilities. Even if the cloud provider and the communication link is trusted, there are still threats of inference attacks where an attacker could speculate properties of the data used for training, or find the underlying model architecture and parameters. In this survey, we review the privacy concerns brought by deep learning, and the mitigating techniques introduced to tackle these issues. We also show that there is a gap in the literature regarding test-time inference privacy, and propose possible future research directions.

</details>

<details>

<summary>2020-11-07 02:57:50 - Defense-friendly Images in Adversarial Attacks: Dataset and Metrics for Perturbation Difficulty</summary>

- *Camilo Pestana, Wei Liu, David Glance, Ajmal Mian*

- `2011.02675v2` - [abs](http://arxiv.org/abs/2011.02675v2) - [pdf](http://arxiv.org/pdf/2011.02675v2)

> Dataset bias is a problem in adversarial machine learning, especially in the evaluation of defenses. An adversarial attack or defense algorithm may show better results on the reported dataset than can be replicated on other datasets. Even when two algorithms are compared, their relative performance can vary depending on the dataset. Deep learning offers state-of-the-art solutions for image recognition, but deep models are vulnerable even to small perturbations. Research in this area focuses primarily on adversarial attacks and defense algorithms. In this paper, we report for the first time, a class of robust images that are both resilient to attacks and that recover better than random images under adversarial attacks using simple defense techniques. Thus, a test dataset with a high proportion of robust images gives a misleading impression about the performance of an adversarial attack or defense. We propose three metrics to determine the proportion of robust images in a dataset and provide scoring to determine the dataset bias. We also provide an ImageNet-R dataset of 15000+ robust images to facilitate further research on this intriguing phenomenon of image strength under attack. Our dataset, combined with the proposed metrics, is valuable for unbiased benchmarking of adversarial attack and defense algorithms.

</details>

<details>

<summary>2020-11-08 20:10:37 - An Approach for the Identification of Information Leakage in Automotive Infotainment systems</summary>

- *Abdul Moiz, Manar H. Alalfi*

- `2011.04066v1` - [abs](http://arxiv.org/abs/2011.04066v1) - [pdf](http://arxiv.org/pdf/2011.04066v1)

> The advancements in the digitization world has revolutionized the automotive industry. Today's modern cars are equipped with internet, computers that can provide autonomous driving functionalities as well as infotainment systems that can run mobile operating systems, like Android Auto and Apple CarPlay. Android Automotive is Google's android operating system tailored to run natively on vehicle's infotainment systems, it allows third party apps to be installed and run on vehicle's infotainment systems. Such apps may raise security concerns related to user's safety, security and privacy. This paper investigates security concerns of in-vehicle apps, specifically, those related to inter component communication (ICC) among these apps. ICC allows apps to share information via inter or intra apps components through a messaging object called intent. In case of insecure communication, Intent can be hijacked or spoofed by malicious apps and user's sensitive information can be leaked to hacker's database. We investigate the attack surface and vulnerabilities in these apps and provide a static analysis approach and a tool to find data leakage vulnerabilities. The approach can also provide hints to mitigate these leaks. We evaluate our approach by analyzing a set of Android Auto apps downloaded from Google Play store, and we report our validated results on vulnerabilities identified on those apps.

</details>

<details>

<summary>2020-11-09 05:52:03 - Watch out! Motion is Blurring the Vision of Your Deep Neural Networks</summary>

- *Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Jian Wang, Bing Yu, Wei Feng, Yang Liu*

- `2002.03500v3` - [abs](http://arxiv.org/abs/2002.03500v3) - [pdf](http://arxiv.org/pdf/2002.03500v3)

> The state-of-the-art deep neural networks (DNNs) are vulnerable against adversarial examples with additive random-like noise perturbations. While such examples are hardly found in the physical world, the image blurring effect caused by object motion, on the other hand, commonly occurs in practice, making the study of which greatly important especially for the widely adopted real-time image processing tasks (e.g., object detection, tracking). In this paper, we initiate the first step to comprehensively investigate the potential hazards of the blur effect for DNN, caused by object motion. We propose a novel adversarial attack method that can generate visually natural motion-blurred adversarial examples, named motion-based adversarial blur attack (ABBA). To this end, we first formulate the kernel-prediction-based attack where an input image is convolved with kernels in a pixel-wise way, and the misclassification capability is achieved by tuning the kernel weights. To generate visually more natural and plausible examples, we further propose the saliency-regularized adversarial kernel prediction, where the salient region serves as a moving object, and the predicted kernel is regularized to achieve naturally visual effects. Besides, the attack is further enhanced by adaptively tuning the translations of object and background. A comprehensive evaluation on the NeurIPS'17 adversarial competition dataset demonstrates the effectiveness of ABBA by considering various kernel sizes, translations, and regions. The in-depth study further confirms that our method shows more effective penetrating capability to the state-of-the-art GAN-based deblurring mechanisms compared with other blurring methods. We release the code to https://github.com/tsingqguo/ABBA.

</details>

<details>

<summary>2020-11-09 18:44:29 - Automated Adversary Emulation for Cyber-Physical Systems via Reinforcement Learning</summary>

- *Arnab Bhattacharya, Thiagarajan Ramachandran, Sandeep Banik, Chase P. Dowling, Shaunak D. Bopardikar*

- `2011.04635v1` - [abs](http://arxiv.org/abs/2011.04635v1) - [pdf](http://arxiv.org/pdf/2011.04635v1)

> Adversary emulation is an offensive exercise that provides a comprehensive assessment of a system's resilience against cyber attacks. However, adversary emulation is typically a manual process, making it costly and hard to deploy in cyber-physical systems (CPS) with complex dynamics, vulnerabilities, and operational uncertainties. In this paper, we develop an automated, domain-aware approach to adversary emulation for CPS. We formulate a Markov Decision Process (MDP) model to determine an optimal attack sequence over a hybrid attack graph with cyber (discrete) and physical (continuous) components and related physical dynamics. We apply model-based and model-free reinforcement learning (RL) methods to solve the discrete-continuous MDP in a tractable fashion. As a baseline, we also develop a greedy attack algorithm and compare it with the RL procedures. We summarize our findings through a numerical study on sensor deception attacks in buildings to compare the performance and solution quality of the proposed algorithms.

</details>

<details>

<summary>2020-11-09 20:42:01 - Adversarial Semantic Collisions</summary>

- *Congzheng Song, Alexander M. Rush, Vitaly Shmatikov*

- `2011.04743v1` - [abs](http://arxiv.org/abs/2011.04743v1) - [pdf](http://arxiv.org/pdf/2011.04743v1)

> We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts-- including paraphrase identification, document retrieval, response suggestion, and extractive summarization-- are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at https://github.com/csong27/collision-bert.

</details>

<details>

<summary>2020-11-10 00:32:30 - Classifier-independent Lower-Bounds for Adversarial Robustness</summary>

- *Elvis Dohmatob*

- `2006.09989v6` - [abs](http://arxiv.org/abs/2006.09989v6) - [pdf](http://arxiv.org/pdf/2006.09989v6)

> We theoretically analyse the limits of robustness to test-time adversarial and noisy examples in classification. Our work focuses on deriving bounds which uniformly apply to all classifiers (i.e all measurable functions from features to labels) for a given problem. Our contributions are two-fold. (1) We use optimal transport theory to derive variational formulae for the Bayes-optimal error a classifier can make on a given classification problem, subject to adversarial attacks. The optimal adversarial attack is then an optimal transport plan for a certain binary cost-function induced by the specific attack model, and can be computed via a simple algorithm based on maximal matching on bipartite graphs. (2) We derive explicit lower-bounds on the Bayes-optimal error in the case of the popular distance-based attacks. These bounds are universal in the sense that they depend on the geometry of the class-conditional distributions of the data, but not on a particular classifier. Our results are in sharp contrast with the existing literature, wherein adversarial vulnerability of classifiers is derived as a consequence of nonzero ordinary test error.

</details>

<details>

<summary>2020-11-10 01:44:32 - Visual Spoofing in content based spam detection</summary>

- *Mark Sokolov, Kehinde Olufowobi, Nic Herndon*

- `2004.05265v2` - [abs](http://arxiv.org/abs/2004.05265v2) - [pdf](http://arxiv.org/pdf/2004.05265v2)

> Although the problem of spam classification seems to be solved, there are still vulnerabilities in the current spam filters that could be easily exploited. We present one such vulnerability, in which one could replace some characters with corresponding characters from a different alphabet. These characters are visually similar, yet have a different Unicode encoding. With this approach spammers can create messages that bypass existing spam filters. Moreover, we show that this approach can be used to avoid plagiarism detection, and in other applications that use natural language processing for automatic analysis of text documents.

</details>

<details>

<summary>2020-11-10 07:56:48 - Characterization and Automatic Update of Deprecated Machine-Learning API Usages</summary>

- *Stefanus Agus Haryono, Ferdian Thung, David Lo, Julia Lawall, Lingxiao Jiang*

- `2011.04962v1` - [abs](http://arxiv.org/abs/2011.04962v1) - [pdf](http://arxiv.org/pdf/2011.04962v1)

> Due to the rise of AI applications, machine learning libraries have become far more accessible, with Python being the most common programming language to write them. Machine learning libraries tend to be updated periodically, which may deprecate existing APIs, making it necessary for developers to update their usages. However, updating usages of deprecated APIs are typically not a priority for developers, leading to widespread usages of deprecated APIs which expose library users to vulnerability issues. In this paper, we built a tool to automate these updates. We first conducted an empirical study to seek a better understanding on how updates of deprecated machine-learning API usages in Python can be done. The study involved a dataset of 112 deprecated APIs from Scikit-Learn, TensorFlow, and PyTorch. We found dimensions of deprecated API migration related to its update operation (i.e., the required operation to perform the migration), API mapping (i.e., the number of deprecated and its corresponding updated APIs),and context dependency (i.e., whether we need to consider surrounding contexts when performing the migration). Guided by the findings on our empirical study, we created MLCatchUp, a tool to automate the update of Python deprecated API usage that automatically infers the API migration transformation through comparison of the deprecated and updated API signatures. These transformations are expressed in a Domain Specific Language (DSL). We evaluated MLCatchUp using test dataset containing 258 files with 514 API usages that we collected from public GitHub repositories. In this evaluation, MLCatchUp achieves a precision of 86.19%. We further improve the precision of MLCatchUp by adding a feature that allows it to accept additional user input to specify the transformation constraints in the DSL for context-dependent API migration, where MLCatchUp achieves a precision of 93.58%.

</details>

<details>

<summary>2020-11-10 08:29:39 - Neural Networks with Recurrent Generative Feedback</summary>

- *Yujia Huang, James Gornet, Sihui Dai, Zhiding Yu, Tan Nguyen, Doris Y. Tsao, Anima Anandkumar*

- `2007.09200v2` - [abs](http://arxiv.org/abs/2007.09200v2) - [pdf](http://arxiv.org/pdf/2007.09200v2)

> Neural networks are vulnerable to input perturbations such as additive noise and adversarial attacks. In contrast, human perception is much more robust to such perturbations. The Bayesian brain hypothesis states that human brains use an internal generative model to update the posterior beliefs of the sensory input. This mechanism can be interpreted as a form of self-consistency between the maximum a posteriori (MAP) estimation of an internal generative model and the external environment. Inspired by such hypothesis, we enforce self-consistency in neural networks by incorporating generative recurrent feedback. We instantiate this design on convolutional neural networks (CNNs). The proposed framework, termed Convolutional Neural Networks with Feedback (CNN-F), introduces a generative feedback with latent variables to existing CNN architectures, where consistent predictions are made through alternating MAP inference under a Bayesian framework. In the experiments, CNN-F shows considerably improved adversarial robustness over conventional feedforward CNNs on standard benchmarks.

</details>

<details>

<summary>2020-11-10 10:48:34 - A survey on Adversarial Recommender Systems: from Attack/Defense strategies to Generative Adversarial Networks</summary>

- *Yashar Deldjoo, Tommaso Di Noia, Felice Antonio Merra*

- `2005.10322v2` - [abs](http://arxiv.org/abs/2005.10322v2) - [pdf](http://arxiv.org/pdf/2005.10322v2)

> Latent-factor models (LFM) based on collaborative filtering (CF), such as matrix factorization (MF) and deep CF methods, are widely used in modern recommender systems (RS) due to their excellent performance and recommendation accuracy. However, success has been accompanied with a major new arising challenge: many applications of machine learning (ML) are adversarial in nature. In recent years, it has been shown that these methods are vulnerable to adversarial examples, i.e., subtle but non-random perturbations designed to force recommendation models to produce erroneous outputs.   The goal of this survey is two-fold: (i) to present recent advances on adversarial machine learning (AML) for the security of RS (i.e., attacking and defense recommendation models), (ii) to show another successful application of AML in generative adversarial networks (GANs) for generative applications, thanks to their ability for learning (high-dimensional) data distributions. In this survey, we provide an exhaustive literature review of 74 articles published in major RS and ML journals and conferences. This review serves as a reference for the RS community, working on the security of RS or on generative models using GANs to improve their quality.

</details>

<details>

<summary>2020-11-10 13:11:03 - Compression Boosts Differentially Private Federated Learning</summary>

- *Raouf Kerkouche, Gergely Ács, Claude Castelluccia, Pierre Genevès*

- `2011.05578v1` - [abs](http://arxiv.org/abs/2011.05578v1) - [pdf](http://arxiv.org/pdf/2011.05578v1)

> Federated Learning allows distributed entities to train a common model collaboratively without sharing their own data. Although it prevents data collection and aggregation by exchanging only parameter updates, it remains vulnerable to various inference and reconstruction attacks where a malicious entity can learn private information about the participants' training data from the captured gradients. Differential Privacy is used to obtain theoretically sound privacy guarantees against such inference attacks by noising the exchanged update vectors. However, the added noise is proportional to the model size which can be very large with modern neural networks. This can result in poor model quality. In this paper, compressive sensing is used to reduce the model size and hence increase model quality without sacrificing privacy. We show experimentally, using 2 datasets, that our privacy-preserving proposal can reduce the communication costs by up to 95% with only a negligible performance penalty compared to traditional non-private federated learning schemes.

</details>

<details>

<summary>2020-11-11 16:21:06 - Energy Concealment based Compressive Sensing Encryption for Perfect Secrecy for IoT</summary>

- *Gajraj Kuldeep, Qi Zhang*

- `2011.05880v1` - [abs](http://arxiv.org/abs/2011.05880v1) - [pdf](http://arxiv.org/pdf/2011.05880v1)

> Recent study has shown that compressive sensing (CS) based computationally secure scheme using Gaussian or Binomial sensing matrix in resource-constrained IoT devices is vulnerable to ciphertext-only attack. Although the CS-based perfectly secure scheme has no such vulnerabilities, the practical realization of the perfectly secure scheme is challenging, because it requires an additional secure channel to transmit the measurement norm. In this paper, we devise a practical realization of a perfectly secure scheme by concealing energy in which the requirement of an additional secure channel is removed. Since the generation of Gaussian sensing matrices is not feasible in resource-constrained IoT devices, approximate Gaussian sensing matrices are generated using linear feedback shift registers. We also demonstrate the implementation feasibility of the proposed perfectly secure scheme in practice without additional complexity. Furthermore, the security analysis of the proposed scheme is performed and compared with the state-of-the-art compressive sensing based energy obfuscation scheme.

</details>

<details>

<summary>2020-11-12 02:08:36 - Malicious Requests Detection with Improved Bidirectional Long Short-term Memory Neural Networks</summary>

- *Wenhao Li, Bincheng Zhang, Jiajie Zhang*

- `2010.13285v4` - [abs](http://arxiv.org/abs/2010.13285v4) - [pdf](http://arxiv.org/pdf/2010.13285v4)

> Detecting and intercepting malicious requests are one of the most widely used ways against attacks in the network security. Most existing detecting approaches, including matching blacklist characters and machine learning algorithms have all shown to be vulnerable to sophisticated attacks. To address the above issues, a more general and rigorous detection method is required. In this paper, we formulate the problem of detecting malicious requests as a temporal sequence classification problem, and propose a novel deep learning model namely Convolutional Neural Network-Bidirectional Long Short-term Memory-Convolutional Neural Network (CNN-BiLSTM-CNN). By connecting the shadow and deep feature maps of the convolutional layers, the malicious feature extracting ability is improved on more detailed functionality. Experimental results on HTTP dataset CSIC 2010 have demonstrated the effectiveness of the proposed method when compared with the state-of-the-arts.

</details>

<details>

<summary>2020-11-12 08:30:42 - Securing Password Authentication for Web-based Applications</summary>

- *Teik Guan Tan, Pawel Szalachowski, Jianying Zhou*

- `2011.06257v1` - [abs](http://arxiv.org/abs/2011.06257v1) - [pdf](http://arxiv.org/pdf/2011.06257v1)

> The use of passwords and the need to protect passwords are not going away. The majority of websites that require authentication continue to support password authentication. Even high-security applications such as Internet Banking portals, which deploy 2-factor authentication, rely on password authentication as one of the authentication factors. However phishing attacks continue to plague password-based authentication despite aggressive efforts in detection and takedown as well as comprehensive user awareness and training programs.   There is currently no foolproof mechanism even for security-conscious websites to prevent users from being directed to fraudulent websites and having their passwords phished. In this paper, we apply a threat analysis on the web password login process, and uncover a design vulnerability in the HTML<inputtype="password"> field. This vulnerability can be exploited for phishing attacks as the web authentication process is not end-to-end secured from each input password field to the web server. We identify four properties that encapsulate the requirements to stop web-based password phishing, and propose a secure protocol to be used with a new credential field that complies with the four properties. We further analyze the proposed protocol through an abuse-case evaluation, discuss various deployment issues, and also perform a test implementation to understand its data and execution overheads

</details>

<details>

<summary>2020-11-12 15:57:23 - World Trade Center responders in their own words: Predicting PTSD symptom trajectories with AI-based language analyses of interviews</summary>

- *Youngseo Son, Sean A. P. Clouston, Roman Kotov, Johannes C. Eichstaedt, Evelyn J. Bromet, Benjamin J. Luft, H Andrew Schwartz*

- `2011.06457v1` - [abs](http://arxiv.org/abs/2011.06457v1) - [pdf](http://arxiv.org/pdf/2011.06457v1)

> Background: Oral histories from 9/11 responders to the World Trade Center (WTC) attacks provide rich narratives about distress and resilience. Artificial Intelligence (AI) models promise to detect psychopathology in natural language, but they have been evaluated primarily in non-clinical settings using social media. This study sought to test the ability of AI-based language assessments to predict PTSD symptom trajectories among responders. Methods: Participants were 124 responders whose health was monitored at the Stony Brook WTC Health and Wellness Program who completed oral history interviews about their initial WTC experiences. PTSD symptom severity was measured longitudinally using the PTSD Checklist (PCL) for up to 7 years post-interview. AI-based indicators were computed for depression, anxiety, neuroticism, and extraversion along with dictionary-based measures of linguistic and interpersonal style. Linear regression and multilevel models estimated associations of AI indicators with concurrent and subsequent PTSD symptom severity (significance adjusted by false discovery rate). Results: Cross-sectionally, greater depressive language (beta=0.32; p=0.043) and first-person singular usage (beta=0.31; p=0.044) were associated with increased symptom severity. Longitudinally, anxious language predicted future worsening in PCL scores (beta=0.31; p=0.031), whereas first-person plural usage (beta=-0.37; p=0.007) and longer words usage (beta=-0.36; p=0.007) predicted improvement. Conclusions: This is the first study to demonstrate the value of AI in understanding PTSD in a vulnerable population. Future studies should extend this application to other trauma exposures and to other demographic groups, especially under-represented minorities.

</details>

<details>

<summary>2020-11-12 18:30:23 - Task-Relevant Adversarial Imitation Learning</summary>

- *Konrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarejo, David Budden, Serkan Cabi, Misha Denil, Nando de Freitas, Ziyu Wang*

- `1910.01077v2` - [abs](http://arxiv.org/abs/1910.01077v2) - [pdf](http://arxiv.org/pdf/1910.01077v2)

> We show that a critical vulnerability in adversarial imitation is the tendency of discriminator networks to learn spurious associations between visual features and expert labels. When the discriminator focuses on task-irrelevant features, it does not provide an informative reward signal, leading to poor task performance. We analyze this problem in detail and propose a solution that outperforms standard Generative Adversarial Imitation Learning (GAIL). Our proposed method, Task-Relevant Adversarial Imitation Learning (TRAIL), uses constrained discriminator optimization to learn informative rewards. In comprehensive experiments, we show that TRAIL can solve challenging robotic manipulation tasks from pixels by imitating human operators without access to any task rewards, and clearly outperforms comparable baseline imitation agents, including those trained via behaviour cloning and conventional GAIL.

</details>

<details>

<summary>2020-11-12 20:34:49 - Context-aware Stand-alone Neural Spelling Correction</summary>

- *Xiangci Li, Hairong Liu, Liang Huang*

- `2011.06642v1` - [abs](http://arxiv.org/abs/2011.06642v1) - [pdf](http://arxiv.org/pdf/2011.06642v1)

> Existing natural language processing systems are vulnerable to noisy inputs resulting from misspellings. On the contrary, humans can easily infer the corresponding correct words from their misspellings and surrounding context. Inspired by this, we address the stand-alone spelling correction problem, which only corrects the spelling of each token without additional token insertion or deletion, by utilizing both spelling information and global context representations. We present a simple yet powerful solution that jointly detects and corrects misspellings as a sequence labeling task by fine-turning a pre-trained language model. Our solution outperforms the previous state-of-the-art result by 12.8% absolute F0.5 score.

</details>

<details>

<summary>2020-11-13 22:06:48 - Neuron Shapley: Discovering the Responsible Neurons</summary>

- *Amirata Ghorbani, James Zou*

- `2002.09815v3` - [abs](http://arxiv.org/abs/2002.09815v3) - [pdf](http://arxiv.org/pdf/2002.09815v3)

> We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.

</details>

<details>

<summary>2020-11-14 01:05:21 - Adversarial symmetric GANs: bridging adversarial samples and adversarial networks</summary>

- *Faqiang Liu, Mingkun Xu, Guoqi Li, Jing Pei, Luping Shi, Rong Zhao*

- `1912.09670v5` - [abs](http://arxiv.org/abs/1912.09670v5) - [pdf](http://arxiv.org/pdf/1912.09670v5)

> Generative adversarial networks have achieved remarkable performance on various tasks but suffer from training instability. Despite many training strategies proposed to improve training stability, this issue remains as a challenge. In this paper, we investigate the training instability from the perspective of adversarial samples and reveal that adversarial training on fake samples is implemented in vanilla GANs, but adversarial training on real samples has long been overlooked. Consequently, the discriminator is extremely vulnerable to adversarial perturbation and the gradient given by the discriminator contains non-informative adversarial noises, which hinders the generator from catching the pattern of real samples. Here, we develop adversarial symmetric GANs (AS-GANs) that incorporate adversarial training of the discriminator on real samples into vanilla GANs, making adversarial training symmetrical. The discriminator is therefore more robust and provides more informative gradient with less adversarial noise, thereby stabilizing training and accelerating convergence. The effectiveness of the AS-GANs is verified on image generation on CIFAR-10 , CelebA, and LSUN with varied network architectures. Not only the training is more stabilized, but the FID scores of generated samples are consistently improved by a large margin compared to the baseline. The bridging of adversarial samples and adversarial networks provides a new approach to further develop adversarial networks.

</details>

<details>

<summary>2020-11-14 10:48:57 - Modelling Attacks in Blockchain Systems using Petri Nets</summary>

- *Md. Atik Shahriar, Faisal Haque Bappy, A. K. M. Fakhrul Hossain, Dayamoy Datta Saikat, Md Sadek Ferdous, Mohammad Jabed M. Chowdhury, Md Zakirul Alam Bhuiyan*

- `2011.07262v1` - [abs](http://arxiv.org/abs/2011.07262v1) - [pdf](http://arxiv.org/pdf/2011.07262v1)

> Blockchain technology has evolved through many changes and modifications, such as smart-contracts since its inception in 2008. The popularity of a blockchain system is due to the fact that it offers a significant security advantage over other traditional systems. However, there have been many attacks in various blockchain systems, exploiting different vulnerabilities and bugs, which caused a significant financial loss. Therefore, it is essential to understand how these attacks in blockchain occur, which vulnerabilities they exploit, and what threats they expose. Another concerning issue in this domain is the recent advancement in the quantum computing field, which imposes a significant threat to the security aspects of many existing secure systems, including blockchain, as they would invalidate many widely-used cryptographic algorithms. Thus, it is important to examine how quantum computing will affect these or other new attacks in the future. In this paper, we explore different vulnerabilities in current blockchain systems and analyse the threats that various theoretical and practical attacks in the blockchain expose. We then model those attacks using Petri nets concerning current systems and future quantum computers.

</details>

<details>

<summary>2020-11-14 19:19:03 - Mobility Map Inference from Thermal Modeling of a Building</summary>

- *Risul Islam, Andrey Lokhov, Nathan Lemons, Michalis Faloutsos*

- `2011.07372v1` - [abs](http://arxiv.org/abs/2011.07372v1) - [pdf](http://arxiv.org/pdf/2011.07372v1)

> We consider the problem of inferring the mobility map, which is the distribution of the building occupants at each timestamp, from the temperatures of the rooms. We also want to explore the effects of noise in the temperature measurement, room layout, etc. in the reconstruction of the movement of people within the building. Our proposed algorithm tackles down the aforementioned challenges leveraging a parameter learner, the modified Least Square Estimator. In the absence of a complete data set with mobility map, room and ambient temperatures, and HVAC data in the public domain, we simulate a physics-based thermal model of the rooms in a building and evaluate the performance of our inference algorithm on this simulated data. We find an upper bound of the noise standard deviation (<= 1F) in the input temperature data of our model. Within this bound, our algorithm can reconstruct the mobility map with a reasonable reconstruction error. Our work can be used in a wide range of applications, for example, ensuring the physical security of office buildings, elderly and infant monitoring, building resources management, emergency building evacuation, and vulnerability assessment of HVAC data. Our work brings together multiple research areas, Thermal Modeling and Parameter Estimation, towards achieving a common goal of inferring the distribution of people within a large office building.

</details>

<details>

<summary>2020-11-14 20:21:11 - UnMask: Adversarial Detection and Defense Through Robust Feature Alignment</summary>

- *Scott Freitas, Shang-Tse Chen, Zijie J. Wang, Duen Horng Chau*

- `2002.09576v2` - [abs](http://arxiv.org/abs/2002.09576v2) - [pdf](http://arxiv.org/pdf/2002.09576v2)

> Deep learning models are being integrated into a wide range of high-impact, security-critical systems, from self-driving cars to medical diagnosis. However, recent research has demonstrated that many of these deep learning architectures are vulnerable to adversarial attacks--highlighting the vital need for defensive techniques to detect and mitigate these attacks before they occur. To combat these adversarial attacks, we developed UnMask, an adversarial detection and defense framework based on robust feature alignment. The core idea behind UnMask is to protect these models by verifying that an image's predicted class ("bird") contains the expected robust features (e.g., beak, wings, eyes). For example, if an image is classified as "bird", but the extracted features are wheel, saddle and frame, the model may be under attack. UnMask detects such attacks and defends the model by rectifying the misclassification, re-classifying the image based on its robust features. Our extensive evaluation shows that UnMask (1) detects up to 96.75% of attacks, and (2) defends the model by correctly classifying up to 93% of adversarial images produced by the current strongest attack, Projected Gradient Descent, in the gray-box setting. UnMask provides significantly better protection than adversarial training across 8 attack vectors, averaging 31.18% higher accuracy. We open source the code repository and data with this paper: https://github.com/safreita1/unmask.

</details>

<details>

<summary>2020-11-15 01:32:58 - Dynamic backdoor attacks against federated learning</summary>

- *Anbu Huang*

- `2011.07429v1` - [abs](http://arxiv.org/abs/2011.07429v1) - [pdf](http://arxiv.org/pdf/2011.07429v1)

> Federated Learning (FL) is a new machine learning framework, which enables millions of participants to collaboratively train machine learning model without compromising data privacy and security. Due to the independence and confidentiality of each client, FL does not guarantee that all clients are honest by design, which makes it vulnerable to adversarial attack naturally. In this paper, we focus on dynamic backdoor attacks under FL setting, where the goal of the adversary is to reduce the performance of the model on targeted tasks while maintaining a good performance on the main task, current existing studies are mainly focused on static backdoor attacks, that is the poison pattern injected is unchanged, however, FL is an online learning framework, and adversarial targets can be changed dynamically by attacker, traditional algorithms require learning a new targeted task from scratch, which could be computationally expensive and require a large number of adversarial training examples, to avoid this, we bridge meta-learning and backdoor attacks under FL setting, in which case we can learn a versatile model from previous experiences, and fast adapting to new adversarial tasks with a few of examples. We evaluate our algorithm on different datasets, and demonstrate that our algorithm can achieve good results with respect to dynamic backdoor attacks. To the best of our knowledge, this is the first paper that focus on dynamic backdoor attacks research under FL setting.

</details>

<details>

<summary>2020-11-16 10:16:05 - Adversarially Robust Classification based on GLRT</summary>

- *Bhagyashree Puranik, Upamanyu Madhow, Ramtin Pedarsani*

- `2011.07835v1` - [abs](http://arxiv.org/abs/2011.07835v1) - [pdf](http://arxiv.org/pdf/2011.07835v1)

> Machine learning models are vulnerable to adversarial attacks that can often cause misclassification by introducing small but well designed perturbations. In this paper, we explore, in the setting of classical composite hypothesis testing, a defense strategy based on the generalized likelihood ratio test (GLRT), which jointly estimates the class of interest and the adversarial perturbation. We evaluate the GLRT approach for the special case of binary hypothesis testing in white Gaussian noise under $\ell_{\infty}$ norm-bounded adversarial perturbations, a setting for which a minimax strategy optimizing for the worst-case attack is known. We show that the GLRT approach yields performance competitive with that of the minimax approach under the worst-case attack, and observe that it yields a better robustness-accuracy trade-off under weaker attacks, depending on the values of signal components relative to the attack budget. We also observe that the GLRT defense generalizes naturally to more complex models for which optimal minimax classifiers are not known.

</details>

<details>

<summary>2020-11-16 21:42:38 - Discrete logarithm problem in some families of sandpile groups</summary>

- *Krisztián Dsupin, Szabolcs Tengely*

- `2011.08296v1` - [abs](http://arxiv.org/abs/2011.08296v1) - [pdf](http://arxiv.org/pdf/2011.08296v1)

> Biggs proposed the sandpile group of certain modified wheel graphs for cryptosystems relying on the difficulty of the discrete logarithm problem. Blackburn and independently Shokrieh showed that the discrete logarithm problem is efficiently solvable. We study Shokrieh's method in cases of graphs such that the sandpile group is not cyclic, namely the square cycle graphs and the wheel graphs. Knowing generators of the group or the form of the pseudoinverse of the Laplacian matrix makes the problem more vulnerable. We also consider the discrete logarithm problem in case of the so-called subdivided banana graphs. In certain cases the sandpile group is cyclic and a generator is known and one can solve the discrete logarithm problem without computing the pseudoinverse of the Laplacian matrix.

</details>

<details>

<summary>2020-11-16 22:45:03 - Don't Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities</summary>

- *Carla Pérez-Almendros, Luis Espinosa-Anke, Steven Schockaert*

- `2011.08320v1` - [abs](http://arxiv.org/abs/2011.08320v1) - [pdf](http://arxiv.org/pdf/2011.08320v1)

> In this paper, we introduce a new annotated dataset which is aimed at supporting the development of NLP models to identify and categorize language that is patronizing or condescending towards vulnerable communities (e.g. refugees, homeless people, poor families). While the prevalence of such language in the general media has long been shown to have harmful effects, it differs from other types of harmful language, in that it is generally used unconsciously and with good intentions. We furthermore believe that the often subtle nature of patronizing and condescending language (PCL) presents an interesting technical challenge for the NLP community. Our analysis of the proposed dataset shows that identifying PCL is hard for standard NLP models, with language models such as BERT achieving the best results.

</details>

<details>

<summary>2020-11-17 04:56:02 - Weak Links in Authentication Chains: A Large-scale Analysis of Email Sender Spoofing Attacks</summary>

- *Kaiwen Shen, Chuhan Wang, Minglei Guo, Xiaofeng Zheng, Chaoyi Lu, Baojun Liu, Yuxuan Zhao, Shuang Hao, Haixin Duan, Qingfeng Pan, Min Yang*

- `2011.08420v1` - [abs](http://arxiv.org/abs/2011.08420v1) - [pdf](http://arxiv.org/pdf/2011.08420v1)

> As a fundamental communicative service, email is playing an important role in both individual and corporate communications, which also makes it one of the most frequently attack vectors. An email's authenticity is based on an authentication chain involving multiple protocols, roles and services, the inconsistency among which creates security threats. Thus, it depends on the weakest link of the chain, as any failed part can break the whole chain-based defense. This paper systematically analyzes the transmission of an email and identifies a series of new attacks capable of bypassing SPF, DKIM, DMARC and user-interface protections. In particular, by conducting a "cocktail" joint attack, more realistic emails can be forged to penetrate the celebrated email services, such as Gmail and Outlook. We conduct a large-scale experiment on 30 popular email services and 23 email clients, and find that all of them are vulnerable to certain types of new attacks. We have duly reported the identified vulnerabilities to the related email service providers, and received positive responses from 11 of them, including Gmail, Yahoo, iCloud and Alibaba. Furthermore, we propose key mitigating measures to defend against the new attacks. Therefore, this work is of great value for identifying email spoofing attacks and improving the email ecosystem's overall security.

</details>

<details>

<summary>2020-11-17 12:57:38 - The Vulnerability of the Neural Networks Against Adversarial Examples in Deep Learning Algorithms</summary>

- *Rui Zhao*

- `2011.05976v2` - [abs](http://arxiv.org/abs/2011.05976v2) - [pdf](http://arxiv.org/pdf/2011.05976v2)

> With further development in the fields of computer vision, network security, natural language processing and so on so forth, deep learning technology gradually exposed certain security risks. The existing deep learning algorithms cannot effectively describe the essential characteristics of data, making the algorithm unable to give the correct result in the face of malicious input. Based on current security threats faced by deep learning, this paper introduces the problem of adversarial examples in deep learning, sorts out the existing attack and defense methods of the black box and white box, and classifies them. It briefly describes the application of some adversarial examples in different scenarios in recent years, compares several defense technologies of adversarial examples, and finally summarizes the problems in this research field and prospects for its future development. This paper introduces the common white box attack methods in detail, and further compares the similarities and differences between the attack of the black and white box. Correspondingly, the author also introduces the defense methods, and analyzes the performance of these methods against the black and white box attack.

</details>

<details>

<summary>2020-11-17 16:16:18 - Bootstrap Aggregation for Point-based Generalized Membership Inference Attacks</summary>

- *Daniel L. Felps, Amelia D. Schwickerath, Joyce D. Williams, Trung N. Vuong, Alan Briggs, Matthew Hunt, Evan Sakmar, David D. Saranchak, Tyler Shumaker*

- `2011.08738v1` - [abs](http://arxiv.org/abs/2011.08738v1) - [pdf](http://arxiv.org/pdf/2011.08738v1)

> An efficient scheme is introduced that extends the generalized membership inference attack to every point in a model's training data set. Our approach leverages data partitioning to create variable sized training sets for the reference models. We then train an attack model for every single training example for a reference model configuration based upon output for each individual point. This allows us to quantify the membership inference attack vulnerability of each training data point. Using this approach, we discovered that smaller amounts of reference model training data led to a stronger attack. Furthermore, the reference models do not need to be of the same architecture as the target model, providing additional attack efficiencies. The attack may also be performed by an adversary even when they do not have the complete original data set.

</details>

<details>

<summary>2020-11-18 18:13:57 - Assessment of System-Level Cyber Attack Vulnerability for Connected and Autonomous Vehicles Using Bayesian Networks</summary>

- *Gurcan Comert, Mashrur Chowdhury, David M. Nicol*

- `2011.09436v1` - [abs](http://arxiv.org/abs/2011.09436v1) - [pdf](http://arxiv.org/pdf/2011.09436v1)

> This study presents a methodology to quantify vulnerability of cyber attacks and their impacts based on probabilistic graphical models for intelligent transportation systems under connected and autonomous vehicles framework. Cyber attack vulnerabilities from various types and their impacts are calculated for intelligent signals and cooperative adaptive cruise control (CACC) applications based on the selected performance measures. Numerical examples are given that show impact of vulnerabilities in terms of average intersection queue lengths, number of stops, average speed, and delays. At a signalized network with and without redundant systems, vulnerability can increase average queues and delays by $3\%$ and $15\%$ and $4\%$ and $17\%$, respectively. For CACC application, impact levels reach to $50\%$ delay difference on average when low amount of speed information is perturbed. When significantly different speed characteristics are inserted by an attacker, delay difference increases beyond $100\%$ of normal traffic conditions.

</details>

<details>

<summary>2020-11-19 04:16:05 - Self-Gradient Networks</summary>

- *Hossein Aboutalebi, Mohammad Javad Shafiee Alexander Wong*

- `2011.09364v2` - [abs](http://arxiv.org/abs/2011.09364v2) - [pdf](http://arxiv.org/pdf/2011.09364v2)

> The incredible effectiveness of adversarial attacks on fooling deep neural networks poses a tremendous hurdle in the widespread adoption of deep learning in safety and security-critical domains. While adversarial defense mechanisms have been proposed since the discovery of the adversarial vulnerability issue of deep neural networks, there is a long path to fully understand and address this issue. In this study, we hypothesize that part of the reason for the incredible effectiveness of adversarial attacks is their ability to implicitly tap into and exploit the gradient flow of a deep neural network. This innate ability to exploit gradient flow makes defending against such attacks quite challenging. Motivated by this hypothesis we argue that if a deep neural network architecture can explicitly tap into its own gradient flow during the training, it can boost its defense capability significantly. Inspired by this fact, we introduce the concept of self-gradient networks, a novel deep neural network architecture designed to be more robust against adversarial perturbations. Gradient flow information is leveraged within self-gradient networks to achieve greater perturbation stability beyond what can be achieved in the standard training process. We conduct a theoretical analysis to gain better insights into the behaviour of the proposed self-gradient networks to illustrate the efficacy of leverage this additional gradient flow information. The proposed self-gradient network architecture enables much more efficient and effective adversarial training, leading to faster convergence towards an adversarially robust solution by at least 10X. Experimental results demonstrate the effectiveness of self-gradient networks when compared with state-of-the-art adversarial learning strategies, with 10% improvement on the CIFAR10 dataset under PGD and CW adversarial perturbations.

</details>

<details>

<summary>2020-11-19 13:46:23 - Effective, Efficient and Robust Neural Architecture Search</summary>

- *Zhixiong Yue, Baijiong Lin, Xiaonan Huang, Yu Zhang*

- `2011.09820v1` - [abs](http://arxiv.org/abs/2011.09820v1) - [pdf](http://arxiv.org/pdf/2011.09820v1)

> Recent advances in adversarial attacks show the vulnerability of deep neural networks searched by Neural Architecture Search (NAS). Although NAS methods can find network architectures with the state-of-the-art performance, the adversarial robustness and resource constraint are often ignored in NAS. To solve this problem, we propose an Effective, Efficient, and Robust Neural Architecture Search (E2RNAS) method to search a neural network architecture by taking the performance, robustness, and resource constraint into consideration. The objective function of the proposed E2RNAS method is formulated as a bi-level multi-objective optimization problem with the upper-level problem as a multi-objective optimization problem, which is different from existing NAS methods. To solve the proposed objective function, we integrate the multiple-gradient descent algorithm, a widely studied gradient-based multi-objective optimization algorithm, with the bi-level optimization. Experiments on benchmark datasets show that the proposed E2RNAS method can find adversarially robust architectures with optimized model size and comparable classification accuracy.

</details>

<details>

<summary>2020-11-19 13:56:58 - Multi-Task Adversarial Attack</summary>

- *Pengxin Guo, Yuancheng Xu, Baijiong Lin, Yu Zhang*

- `2011.09824v1` - [abs](http://arxiv.org/abs/2011.09824v1) - [pdf](http://arxiv.org/pdf/2011.09824v1)

> Deep neural networks have achieved impressive performance in various areas, but they are shown to be vulnerable to adversarial attacks. Previous works on adversarial attacks mainly focused on the single-task setting. However, in real applications, it is often desirable to attack several models for different tasks simultaneously. To this end, we propose Multi-Task adversarial Attack (MTA), a unified framework that can craft adversarial examples for multiple tasks efficiently by leveraging shared knowledge among tasks, which helps enable large-scale applications of adversarial attacks on real-world systems. More specifically, MTA uses a generator for adversarial perturbations which consists of a shared encoder for all tasks and multiple task-specific decoders. Thanks to the shared encoder, MTA reduces the storage cost and speeds up the inference when attacking multiple tasks simultaneously. Moreover, the proposed framework can be used to generate per-instance and universal perturbations for targeted and non-targeted attacks. Experimental results on the Office-31 and NYUv2 datasets demonstrate that MTA can improve the quality of attacks when compared with its single-task counterpart.

</details>

<details>

<summary>2020-11-20 17:23:49 - COVID-19 and Mental Health/Substance Use Disorders on Reddit: A Longitudinal Study</summary>

- *Amanuel Alambo, Swati Padhee, Tanvi Banerjee, Krishnaprasad Thirunarayan*

- `2011.10518v1` - [abs](http://arxiv.org/abs/2011.10518v1) - [pdf](http://arxiv.org/pdf/2011.10518v1)

> COVID-19 pandemic has adversely and disproportionately impacted people suffering from mental health issues and substance use problems. This has been exacerbated by social isolation during the pandemic and the social stigma associated with mental health and substance use disorders, making people reluctant to share their struggles and seek help. Due to the anonymity and privacy they provide, social media emerged as a convenient medium for people to share their experiences about their day to day struggles. Reddit is a well-recognized social media platform that provides focused and structured forums called subreddits, that users subscribe to and discuss their experiences with others. Temporal assessment of the topical correlation between social media postings about mental health/substance use and postings about Coronavirus is crucial to better understand public sentiment on the pandemic and its evolving impact, especially related to vulnerable populations. In this study, we conduct a longitudinal topical analysis of postings between subreddits r/depression, r/Anxiety, r/SuicideWatch, and r/Coronavirus, and postings between subreddits r/opiates, r/OpiatesRecovery, r/addiction, and r/Coronavirus from January 2020 - October 2020. Our results show a high topical correlation between postings in r/depression and r/Coronavirus in September 2020. Further, the topical correlation between postings on substance use disorders and Coronavirus fluctuates, showing the highest correlation in August 2020. By monitoring these trends from platforms such as Reddit, epidemiologists, and mental health professionals can gain insights into the challenges faced by communities for targeted interventions.

</details>

<details>

<summary>2020-11-20 19:36:47 - Simultaneously forecasting global geomagnetic activity using Recurrent Networks</summary>

- *Charles Topliff, Morris Cohen, William Bristow*

- `2010.06487v2` - [abs](http://arxiv.org/abs/2010.06487v2) - [pdf](http://arxiv.org/pdf/2010.06487v2)

> Many systems used by society are extremely vulnerable to space weather events such as solar flares and geomagnetic storms which could potentially cause catastrophic damage. In recent years, many works have emerged to provide early warning to such systems by forecasting these events through some proxy, but these approaches have largely focused on a specific phenomenon. We present a sequence-to-sequence learning approach to the problem of forecasting global space weather conditions at an hourly resolution. This approach improves upon other work in this field by simultaneously forecasting several key proxies for geomagnetic activity up to 6 hours in advance. We demonstrate an improvement over the best currently known predictor of geomagnetic storms, and an improvement over a persistence baseline several hours in advance.

</details>

<details>

<summary>2020-11-21 02:56:27 - Internet Security Awareness of Filipinos: A Survey Paper</summary>

- *C. D. Omorog, R. P. Medina*

- `2012.03669v1` - [abs](http://arxiv.org/abs/2012.03669v1) - [pdf](http://arxiv.org/pdf/2012.03669v1)

> Purpose. This paper examines the Internet security perception of Filipinos to establish a need and sense of urgency on the part of the government to create a culture of cybersecurity for every Filipino. Method. A quantitative survey was conducted through traditional, online, and phone interviews among 252 respondents using a two-page questionnaire that covers basic demographic information and two key elements (1) Internet usage and (2) security practices. Results. Based on findings, there is a sharp increase in Internet users for the last three years (50%), and most access to the Internet through mobile (94.4%). Although at home is the most frequent location for Internet access (94.4%), a good percentage still use free WiFi access points available in malls (22.2%), restaurants (11.1%), and other public areas (38.9%) doing Internet services (email and downloading) that are vulnerable to cyberattacks. The study also revealed that although respondents may have good knowledge of Internet security software, proper implementation is very limited. Conclusion. Filipinos are susceptible to cyberattacks, particularly to phishing and malware attacks. Also, the majority of the respondents' Internet security perception is derivative: they practice online measures but with a limited understanding of the purpose. Therefore proper education, through training and awareness, is an effective approach to remedy the situation. Recommendations. The Philippine government must now take actions and tap industries to educate Filipinos about Internet security before any negative consequences happen in the future. Research Implications. The information collected sets a clear picture of the importance of cybersecurity awareness from a regional to a global perspective.

</details>

<details>

<summary>2020-11-21 04:54:13 - A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models</summary>

- *Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu Luo, Alex Liu, Ting Wang*

- `1911.01559v3` - [abs](http://arxiv.org/abs/1911.01559v3) - [pdf](http://arxiv.org/pdf/1911.01559v3)

> Despite their tremendous success in a range of domains, deep learning systems are inherently susceptible to two types of manipulations: adversarial inputs -- maliciously crafted samples that deceive target deep neural network (DNN) models, and poisoned models -- adversely forged DNNs that misbehave on pre-defined inputs. While prior work has intensively studied the two attack vectors in parallel, there is still a lack of understanding about their fundamental connections: what are the dynamic interactions between the two attack vectors? what are the implications of such interactions for optimizing existing attacks? what are the potential countermeasures against the enhanced attacks? Answering these key questions is crucial for assessing and mitigating the holistic vulnerabilities of DNNs deployed in realistic settings.   Here we take a solid step towards this goal by conducting the first systematic study of the two attack vectors within a unified framework. Specifically, (i) we develop a new attack model that jointly optimizes adversarial inputs and poisoned models; (ii) with both analytical and empirical evidence, we reveal that there exist intriguing "mutual reinforcement" effects between the two attack vectors -- leveraging one vector significantly amplifies the effectiveness of the other; (iii) we demonstrate that such effects enable a large design spectrum for the adversary to enhance the existing attacks that exploit both vectors (e.g., backdoor attacks), such as maximizing the attack evasiveness with respect to various detection methods; (iv) finally, we discuss potential countermeasures against such optimized attacks and their technical challenges, pointing to several promising research directions.

</details>

<details>

<summary>2020-11-21 14:06:59 - Spatially Correlated Patterns in Adversarial Images</summary>

- *Nandish Chattopadhyay, Lionell Yip En Zhi, Bryan Tan Bing Xing, Anupam Chattopadhyay*

- `2011.10794v1` - [abs](http://arxiv.org/abs/2011.10794v1) - [pdf](http://arxiv.org/pdf/2011.10794v1)

> Adversarial attacks have proved to be the major impediment in the progress on research towards reliable machine learning solutions. Carefully crafted perturbations, imperceptible to human vision, can be added to images to force misclassification by an otherwise high performing neural network. To have a better understanding of the key contributors of such structured attacks, we searched for and studied spatially co-located patterns in the distribution of pixels in the input space. In this paper, we propose a framework for segregating and isolating regions within an input image which are particularly critical towards either classification (during inference), or adversarial vulnerability or both. We assert that during inference, the trained model looks at a specific region in the image, which we call Region of Importance (RoI); and the attacker looks at a region to alter/modify, which we call Region of Attack (RoA). The success of this approach could also be used to design a post-hoc adversarial defence method, as illustrated by our observations. This uses the notion of blocking out (we call neutralizing) that region of the image which is highly vulnerable to adversarial attacks but is not important for the task of classification. We establish the theoretical setup for formalising the process of segregation, isolation and neutralization and substantiate it through empirical analysis on standard benchmarking datasets. The findings strongly indicate that mapping features into the input space preserves the significant patterns typically observed in the feature-space while adding major interpretability and therefore simplifies potential defensive mechanisms.

</details>

<details>

<summary>2020-11-22 06:01:22 - Who is in Control? Practical Physical Layer Attack and Defense for mmWave based Sensing in Autonomous Vehicles</summary>

- *Zhi Sun, Sarankumar Balakrishnan, Lu Su, Arupjyoti Bhuyan, Pu Wang, Chunming Qiao*

- `2011.10947v1` - [abs](http://arxiv.org/abs/2011.10947v1) - [pdf](http://arxiv.org/pdf/2011.10947v1)

> With the wide bandwidths in millimeter wave (mmWave) frequency band that results in unprecedented accuracy, mmWave sensing has become vital for many applications, especially in autonomous vehicles (AVs). In addition, mmWave sensing has superior reliability compared to other sensing counterparts such as camera and LiDAR, which is essential for safety-critical driving. Therefore, it is critical to understand the security vulnerabilities and improve the security and reliability of mmWave sensing in AVs. To this end, we perform the end-to-end security analysis of a mmWave-based sensing system in AVs, by designing and implementing practical physical layer attack and defense strategies in a state-of-the-art mmWave testbed and an AV testbed in real-world settings. Various strategies are developed to take control of the victim AV by spoofing its mmWave sensing module, including adding fake obstacles at arbitrary locations and faking the locations of existing obstacles. Five real-world attack scenarios are constructed to spoof the victim AV and force it to make dangerous driving decisions leading to a fatal crash. Field experiments are conducted to study the impact of the various attack scenarios using a Lincoln MKZ-based AV testbed, which validate that the attacker can indeed assume control of the victim AV to compromise its security and safety. To defend the attacks, we design and implement a challenge-response authentication scheme and a RF fingerprinting scheme to reliably detect aforementioned spoofing attacks.

</details>

<details>

<summary>2020-11-23 15:31:12 - Generative Adversarial Simulator</summary>

- *Jonathan Raiman*

- `2011.11472v1` - [abs](http://arxiv.org/abs/2011.11472v1) - [pdf](http://arxiv.org/pdf/2011.11472v1)

> Knowledge distillation between machine learning models has opened many new avenues for parameter count reduction, performance improvements, or amortizing training time when changing architectures between the teacher and student network. In the case of reinforcement learning, this technique has also been applied to distill teacher policies to students. Until now, policy distillation required access to a simulator or real world trajectories.   In this paper we introduce a simulator-free approach to knowledge distillation in the context of reinforcement learning. A key challenge is having the student learn the multiplicity of cases that correspond to a given action. While prior work has shown that data-free knowledge distillation is possible with supervised learning models by generating synthetic examples, these approaches to are vulnerable to only producing a single prototype example for each class. We propose an extension to explicitly handle multiple observations per output class that seeks to find as many exemplars as possible for a given output class by reinitializing our data generator and making use of an adversarial loss.   To the best of our knowledge, this is the first demonstration of simulator-free knowledge distillation between a teacher and a student policy. This new approach improves over the state of the art on data-free learning of student networks on benchmark datasets (MNIST, Fashion-MNIST, CIFAR-10), and we also demonstrate that it specifically tackles issues with multiple input modes. We also identify open problems when distilling agents trained in high dimensional environments such as Pong, Breakout, or Seaquest.

</details>

<details>

<summary>2020-11-24 15:25:29 - Pain Intensity Assessment in Sickle Cell Disease patients using Vital Signs during Hospital Visits</summary>

- *Swati Padhee, Amanuel Alambo, Tanvi Banerjee, Arvind Subramaniam, Daniel M. Abrams, Gary K. Nave Jr., Nirmish Shah*

- `2012.01126v1` - [abs](http://arxiv.org/abs/2012.01126v1) - [pdf](http://arxiv.org/pdf/2012.01126v1)

> Pain in sickle cell disease (SCD) is often associated with increased morbidity, mortality, and high healthcare costs. The standard method for predicting the absence, presence, and intensity of pain has long been self-report. However, medical providers struggle to manage patients based on subjective pain reports correctly and pain medications often lead to further difficulties in patient communication as they may cause sedation and sleepiness. Recent studies have shown that objective physiological measures can predict subjective self-reported pain scores for inpatient visits using machine learning (ML) techniques. In this study, we evaluate the generalizability of ML techniques to data collected from 50 patients over an extended period across three types of hospital visits (i.e., inpatient, outpatient and outpatient evaluation). We compare five classification algorithms for various pain intensity levels at both intra-individual (within each patient) and inter-individual (between patients) level. While all the tested classifiers perform much better than chance, a Decision Tree (DT) model performs best at predicting pain on an 11-point severity scale (from 0-10) with an accuracy of 0.728 at an inter-individual level and 0.653 at an intra-individual level. The accuracy of DT significantly improves to 0.941 on a 2-point rating scale (i.e., no/mild pain: 0-5, severe pain: 6-10) at an intra-individual level. Our experimental results demonstrate that ML techniques can provide an objective and quantitative evaluation of pain intensity levels for all three types of hospital visits.

</details>

<details>

<summary>2020-11-24 21:23:09 - ConAML: Constrained Adversarial Machine Learning for Cyber-Physical Systems</summary>

- *Jiangnan Li, Yingyuan Yang, Jinyuan Stella Sun, Kevin Tomsovic, Hairong Qi*

- `2003.05631v3` - [abs](http://arxiv.org/abs/2003.05631v3) - [pdf](http://arxiv.org/pdf/2003.05631v3)

> Recent research demonstrated that the superficially well-trained machine learning (ML) models are highly vulnerable to adversarial examples. As ML techniques are becoming a popular solution for cyber-physical systems (CPSs) applications in research literatures, the security of these applications is of concern. However, current studies on adversarial machine learning (AML) mainly focus on pure cyberspace domains. The risks the adversarial examples can bring to the CPS applications have not been well investigated. In particular, due to the distributed property of data sources and the inherent physical constraints imposed by CPSs, the widely-used threat models and the state-of-the-art AML algorithms in previous cyberspace research become infeasible.   We study the potential vulnerabilities of ML applied in CPSs by proposing Constrained Adversarial Machine Learning (ConAML), which generates adversarial examples that satisfy the intrinsic constraints of the physical systems. We first summarize the difference between AML in CPSs and AML in existing cyberspace systems and propose a general threat model for ConAML. We then design a best-effort search algorithm to iteratively generate adversarial examples with linear physical constraints. We evaluate our algorithms with simulations of two typical CPSs, the power grids and the water treatment system. The results show that our ConAML algorithms can effectively generate adversarial examples which significantly decrease the performance of the ML models even under practical constraints.

</details>

<details>

<summary>2020-11-25 01:13:39 - EI-MTD:Moving Target Defense for Edge Intelligence against Adversarial Attacks</summary>

- *Yaguan Qian, Qiqi Shao, Jiamin Wang, Xiang Lin, Yankai Guo, Zhaoquan Gu, Bin Wang, Chunming Wu*

- `2009.10537v3` - [abs](http://arxiv.org/abs/2009.10537v3) - [pdf](http://arxiv.org/pdf/2009.10537v3)

> With the boom of edge intelligence, its vulnerability to adversarial attacks becomes an urgent problem. The so-called adversarial example can fool a deep learning model on the edge node to misclassify. Due to the property of transferability, the adversary can easily make a black-box attack using a local substitute model. Nevertheless, the limitation of resource of edge nodes cannot afford a complicated defense mechanism as doing on the cloud data center. To overcome the challenge, we propose a dynamic defense mechanism, namely EI-MTD. It first obtains robust member models with small size through differential knowledge distillation from a complicated teacher model on the cloud data center. Then, a dynamic scheduling policy based on a Bayesian Stackelberg game is applied to the choice of a target model for service. This dynamic defense can prohibit the adversary from selecting an optimal substitute model for black-box attacks. Our experimental result shows that this dynamic scheduling can effectively protect edge intelligence against adversarial attacks under the black-box setting.

</details>

<details>

<summary>2020-11-25 10:23:42 - Distributed Additive Encryption and Quantization for Privacy Preserving Federated Deep Learning</summary>

- *Hangyu Zhu, Rui Wang, Yaochu Jin, Kaitai Liang, Jianting Ning*

- `2011.12623v1` - [abs](http://arxiv.org/abs/2011.12623v1) - [pdf](http://arxiv.org/pdf/2011.12623v1)

> Homomorphic encryption is a very useful gradient protection technique used in privacy preserving federated learning. However, existing encrypted federated learning systems need a trusted third party to generate and distribute key pairs to connected participants, making them unsuited for federated learning and vulnerable to security risks. Moreover, encrypting all model parameters is computationally intensive, especially for large machine learning models such as deep neural networks. In order to mitigate these issues, we develop a practical, computationally efficient encryption based protocol for federated deep learning, where the key pairs are collaboratively generated without the help of a third party. By quantization of the model parameters on the clients and an approximated aggregation on the server, the proposed method avoids encryption and decryption of the entire model. In addition, a threshold based secret sharing technique is designed so that no one can hold the global private key for decryption, while aggregated ciphertexts can be successfully decrypted by a threshold number of clients even if some clients are offline. Our experimental results confirm that the proposed method significantly reduces the communication costs and computational complexity compared to existing encrypted federated learning without compromising the performance and security.

</details>

<details>

<summary>2020-11-25 18:14:30 - Assessing the Quality of Gridded Population Data for Quantifying the Population Living in Deprived Communities</summary>

- *Agatha C. H. de Mattos, Gavin McArdle, Michela Bertolotto*

- `2011.12923v1` - [abs](http://arxiv.org/abs/2011.12923v1) - [pdf](http://arxiv.org/pdf/2011.12923v1)

> Over a billion people live in slums in settlements that are often located in ecologically sensitive areas and hence highly vulnerable. This is a problem in many parts of the world, but it is more prominent in low-income countries, where in 2014 on average 65% of the urban population lived in slums. As a result, building resilient communities requires quantifying the population living in these deprived areas and improving their living conditions. However, most of the data about slums comes from census data, which is only available at aggregate levels and often excludes these settlements. Consequently, researchers have looked at alternative approaches. These approaches, however, commonly rely on expensive high-resolution satellite imagery and field-surveys, which hinders their large-scale applicability. In this paper, we investigate a cost-effective methodology to estimate the slum population by assessing the quality of gridded population data. We evaluate the accuracy of the WorldPOP and LandScan population layers against ground-truth data composed of 1,703 georeferenced polygons that were mapped as deprived areas and which had their population surveyed during the 2010 Brazilian census. While the LandScan data did not produce satisfactory results for most polygons, the WorldPOP estimates were less than 20% off for 67% of the polygons and the overall error for the totality of the studied area was only -5.9%. This small error margin demonstrates that population layers with a resolution of at least a 100m, such as WorldPOP's, can be useful tools to estimate the population living in slums.

</details>

<details>

<summary>2020-11-25 19:42:32 - LEADER: Low Overhead Rank Attack Detection for Securing RPL based IoT</summary>

- *Somnath Karmakar, Jayasree Sengupta, Sipra Das Bit*

- `2011.12996v1` - [abs](http://arxiv.org/abs/2011.12996v1) - [pdf](http://arxiv.org/pdf/2011.12996v1)

> In recent times researchers have found several security vulnerabilities in the Routing Protocol for Low power and Lossy network (RPL), amongst which rank attack is a predominant one causing detrimental effects on the network by creating a fake topology. To address this concern, we propose a low-overhead rank attack detection scheme for non-storing mode of RPL used in IoT to deal with both increased and decreased rank attacks. Accordingly, we have modified the RPL Destination Oriented Directed Acyclic Graph (DODAG) formation algorithm to detect rank attacks during topology formation and maintenance. The distributed module of the algorithm runs in all the participating nodes whereas the centralized module runs in the sink. Unlike many existing schemes, instead of sending additional control message, we make the scheme low-overhead by simply modifying the DAO control message. Additionally, a lightweight Message Authentication Code (HMAC-LOCHA) is used to verify the integrity and authenticity of the control messages exchanged between nodes and the sink. The correctness of the proposed scheme is established through a concrete proof using multiple test case scenarios. Finally, the performance of the proposed scheme is evaluated both theoretically and through simulation in Contiki-based Cooja simulator. Theoretical evaluation proves the energy efficiency of the scheme. Simulation results show that our scheme outperforms over a state-of-the-art rank attack detection scheme in terms of detection accuracy, false positive or negative rate and energy consumption while also keeping acceptable network performance such as improved detection latency and at par packet delivery ratio.

</details>

<details>

<summary>2020-11-26 16:45:11 - Introducing Network Coding to RPL: The Chained Secure Mode (CSM)</summary>

- *Ahmed Raoof, Chung-Horng Lung, Ashraf Matrawy*

- `2006.00310v3` - [abs](http://arxiv.org/abs/2006.00310v3) - [pdf](http://arxiv.org/pdf/2006.00310v3)

> The current standard of Routing Protocol for Low Power and Lossy Networks (RPL) incorporates three modes of security: the Unsecured Mode (UM), Preinstalled Secure Mode (PSM), and the Authenticated Secure Mode (ASM). While the PSM and ASM are intended to protect against external routing attacks and some replay attacks (through an optional replay protection mechanism), recent research showed that RPL in PSM is still vulnerable to many routing attacks, both internal and external. In this paper, we propose a novel secure mode for RPL, the Chained Secure Mode (CSM), based on the concept of intraflow Network Coding. The main goal of CSM is to enhance RPL resilience against replay attacks, with the ability to mitigate some of them. The security and performance of a proof-of-concept prototype of CSM were evaluated and compared against RPL in UM and PSM (with and without the optional replay protection) in the presence of Neighbor attack as an example. It showed that CSM has better performance and more enhanced security compared to both the UM and PSM with the replay protection. On the other hand, it showed a need for a proper recovery mechanism for the case of losing a control message.

</details>

<details>

<summary>2020-11-26 17:08:06 - Exposing the Robustness and Vulnerability of Hybrid 8T-6T SRAM Memory Architectures to Adversarial Attacks in Deep Neural Networks</summary>

- *Abhishek Moitra, Priyadarshini Panda*

- `2011.13392v1` - [abs](http://arxiv.org/abs/2011.13392v1) - [pdf](http://arxiv.org/pdf/2011.13392v1)

> Deep Learning is able to solve a plethora of once impossible problems. However, they are vulnerable to input adversarial attacks preventing them from being autonomously deployed in critical applications. Several algorithm-centered works have discussed methods to cause adversarial attacks and improve adversarial robustness of a Deep Neural Network (DNN). In this work, we elicit the advantages and vulnerabilities of hybrid 6T-8T memories to improve the adversarial robustness and cause adversarial attacks on DNNs. We show that bit-error noise in hybrid memories due to erroneous 6T-SRAM cells have deterministic behaviour based on the hybrid memory configurations (V_DD, 8T-6T ratio). This controlled noise (surgical noise) can be strategically introduced into specific DNN layers to improve the adversarial accuracy of DNNs. At the same time, surgical noise can be carefully injected into the DNN parameters stored in hybrid memory to cause adversarial attacks. To improve the adversarial robustness of DNNs using surgical noise, we propose a methodology to select appropriate DNN layers and their corresponding hybrid memory configurations to introduce the required surgical noise. Using this, we achieve 2-8% higher adversarial accuracy without re-training against white-box attacks like FGSM, than the baseline models (with no surgical noise introduced). To demonstrate adversarial attacks using surgical noise, we design a novel, white-box attack on DNN parameters stored in hybrid memory banks that causes the DNN inference accuracy to drop by more than 60% with over 90% confidence value. We support our claims with experiments, performed using benchmark datasets-CIFAR10 and CIFAR100 on VGG19 and ResNet18 networks.

</details>

<details>

<summary>2020-11-26 17:40:03 - Cyber-Risks in Paper Voting</summary>

- *David M. Sommer, Moritz Schneider, Jannik Gut, Srdjan Capkun*

- `1906.07532v3` - [abs](http://arxiv.org/abs/1906.07532v3) - [pdf](http://arxiv.org/pdf/1906.07532v3)

> Paper ballot voting with its fully-reviewable paper-trail is usually considered as more secure than their e-voting counterparts, given the large number of recent incidents. In this work, we explore the security of paper voting and show that paper voting, as it is implemented today, is surprisingly vulnerable to cyber-attacks. In particular, the aggregation methods of preliminary voting results of various countries rely on insecure communication channels like telephone, fax or non-secure e-mail. Furthermore, regulations typically do not mandate the use of secure channels for the transmission of preliminary results. We illustrate that preliminary results, despite their temporary nature, may have a severe impact on real-world decisions during the 3 to 30 days window until the final results are declared. An attacker exploiting this discrepancy can, e.g., benefit from stock market manipulation or call into question the legitimacy of the elections. This work investigates the cyber-risks in paper voting in a systematic manner by reviewing procedures in several countries (Estonia, France, Germany, the United Kingdom, and the United States of America) and through a comprehensive case-study of Switzerland. We examine the transmission systems currently in use through inquires from election officials. Moreover, we illustrate the feasibility of attacks by analyzing the frequent historical discrepancies between preliminary and final results. Considering our results and recent reports about easily modifiable preliminary results in Germany and the Netherlands, we conjecture similar weaknesses in other countries as well.

</details>

<details>

<summary>2020-11-26 19:19:48 - Attacks on Lightweight Hardware-Based Security Primitives</summary>

- *Jack Edmonds, Fatemeh Tehranipoor*

- `2011.13450v1` - [abs](http://arxiv.org/abs/2011.13450v1) - [pdf](http://arxiv.org/pdf/2011.13450v1)

> In today's digital age, the ease of data collection, transfer, and storage continue to shape modern society and the ways we interact with our world. The advantages are numerous, but there is also an increased risk of information unintentionally falling into the wrong hands. Finding methods of protecting sensitive information at the hardware level is of utmost importance, and in this paper, we aim to provide a survey on recent developments in attacks on lightweight hardware-based security primitives (LHSPs) designed to do just that. Specifically, we provide an analysis of the attack resilience of these proposed LHSPs in an attempt to bring awareness to any vulnerabilities that may exist. We do this in the hope that it will encourage the continued development of attack countermeasures as well as completely new methods of data protection in order to prevent the discussed methods of attack from remaining viable in the future. The types of LHSPs discussed include physical unclonable functions (PUFs) and true random number generators (TRNGs), with a primary emphasis placed on PUFs.

</details>

<details>

<summary>2020-11-27 02:24:43 - Robust Attacks on Deep Learning Face Recognition in the Physical World</summary>

- *Meng Shen, Hao Yu, Liehuang Zhu, Ke Xu, Qi Li, Xiaojiang Du*

- `2011.13526v1` - [abs](http://arxiv.org/abs/2011.13526v1) - [pdf](http://arxiv.org/pdf/2011.13526v1)

> Deep neural networks (DNNs) have been increasingly used in face recognition (FR) systems. Recent studies, however, show that DNNs are vulnerable to adversarial examples, which can potentially mislead the FR systems using DNNs in the physical world. Existing attacks on these systems either generate perturbations working merely in the digital world, or rely on customized equipments to generate perturbations and are not robust in varying physical environments. In this paper, we propose FaceAdv, a physical-world attack that crafts adversarial stickers to deceive FR systems. It mainly consists of a sticker generator and a transformer, where the former can craft several stickers with different shapes and the latter transformer aims to digitally attach stickers to human faces and provide feedbacks to the generator to improve the effectiveness of stickers. We conduct extensive experiments to evaluate the effectiveness of FaceAdv on attacking 3 typical FR systems (i.e., ArcFace, CosFace and FaceNet). The results show that compared with a state-of-the-art attack, FaceAdv can significantly improve success rate of both dodging and impersonating attacks. We also conduct comprehensive evaluations to demonstrate the robustness of FaceAdv.

</details>

<details>

<summary>2020-11-27 05:24:53 - Interpretable Poverty Mapping using Social Media Data, Satellite Images, and Geospatial Information</summary>

- *Chiara Ledesma, Oshean Lee Garonita, Lorenzo Jaime Flores, Isabelle Tingzon, Danielle Dalisay*

- `2011.13563v1` - [abs](http://arxiv.org/abs/2011.13563v1) - [pdf](http://arxiv.org/pdf/2011.13563v1)

> Access to accurate, granular, and up-to-date poverty data is essential for humanitarian organizations to identify vulnerable areas for poverty alleviation efforts. Recent works have shown success in combining computer vision and satellite imagery for poverty estimation; however, the cost of acquiring high-resolution images coupled with black box models can be a barrier to adoption for many development organizations. In this study, we present a interpretable and cost-efficient approach to poverty estimation using machine learning and readily accessible data sources including social media data, low-resolution satellite images, and volunteered geographic information. Using our method, we achieve an $R^2$ of 0.66 for wealth estimation in the Philippines, compared to 0.63 using satellite imagery. Finally, we use feature importance analysis to identify the highest contributing features both globally and locally to help decision makers gain deeper insights into poverty.

</details>

<details>

<summary>2020-11-27 12:25:18 - Stay Connected, Leave no Trace: Enhancing Security and Privacy in WiFi via Obfuscating Radiometric Fingerprints</summary>

- *Luis F. Abanto-Leon, Andreas Baeuml, Gek Hong, Sim, Matthias Hollick, Arash Asadi*

- `2011.12644v2` - [abs](http://arxiv.org/abs/2011.12644v2) - [pdf](http://arxiv.org/pdf/2011.12644v2)

> The intrinsic hardware imperfection of WiFi chipsets manifests itself in the transmitted signal, leading to a unique radiometric fingerprint. This fingerprint can be used as an additional means of authentication to enhance security. In fact, recent works propose practical fingerprinting solutions that can be readily implemented in commercial-off-the-shelf devices. In this paper, we prove analytically and experimentally that these solutions are highly vulnerable to impersonation attacks. We also demonstrate that such a unique device-based signature can be abused to violate privacy by tracking the user device, and, as of today, users do not have any means to prevent such privacy attacks other than turning off the device.   We propose RF-Veil, a radiometric fingerprinting solution that not only is robust against impersonation attacks but also protects user privacy by obfuscating the radiometric fingerprint of the transmitter for non-legitimate receivers. Specifically, we introduce a randomized pattern of phase errors to the transmitted signal such that only the intended receiver can extract the original fingerprint of the transmitter. In a series of experiments and analyses, we expose the vulnerability of adopting naive randomization to statistical attacks and introduce countermeasures. Finally, we show the efficacy of RF-Veil experimentally in protecting user privacy and enhancing security. More importantly, our proposed solution allows communicating with other devices, which do not employ RF-Veil.

</details>

<details>

<summary>2020-11-27 21:04:07 - Black Loans Matter: Distributionally Robust Fairness for Fighting Subgroup Discrimination</summary>

- *Mark Weber, Mikhail Yurochkin, Sherif Botros, Vanio Markov*

- `2012.01193v1` - [abs](http://arxiv.org/abs/2012.01193v1) - [pdf](http://arxiv.org/pdf/2012.01193v1)

> Algorithmic fairness in lending today relies on group fairness metrics for monitoring statistical parity across protected groups. This approach is vulnerable to subgroup discrimination by proxy, carrying significant risks of legal and reputational damage for lenders and blatantly unfair outcomes for borrowers. Practical challenges arise from the many possible combinations and subsets of protected groups. We motivate this problem against the backdrop of historical and residual racism in the United States polluting all available training data and raising public sensitivity to algorithimic bias. We review the current regulatory compliance protocols for fairness in lending and discuss their limitations relative to the contributions state-of-the-art fairness methods may afford. We propose a solution for addressing subgroup discrimination, while adhering to existing group fairness requirements, from recent developments in individual fairness methods and corresponding fair metric learning algorithms.

</details>

<details>

<summary>2020-11-28 00:08:45 - Voting based ensemble improves robustness of defensive models</summary>

- *Devvrit, Minhao Cheng, Cho-Jui Hsieh, Inderjit Dhillon*

- `2011.14031v1` - [abs](http://arxiv.org/abs/2011.14031v1) - [pdf](http://arxiv.org/pdf/2011.14031v1)

> Developing robust models against adversarial perturbations has been an active area of research and many algorithms have been proposed to train individual robust models. Taking these pretrained robust models, we aim to study whether it is possible to create an ensemble to further improve robustness. Several previous attempts tackled this problem by ensembling the soft-label prediction and have been proved vulnerable based on the latest attack methods. In this paper, we show that if the robust training loss is diverse enough, a simple hard-label based voting ensemble can boost the robust error over each individual model. Furthermore, given a pool of robust models, we develop a principled way to select which models to ensemble. Finally, to verify the improved robustness, we conduct extensive experiments to study how to attack a voting-based ensemble and develop several new white-box attacks. On CIFAR-10 dataset, by ensembling several state-of-the-art pre-trained defense models, our method can achieve a 59.8% robust accuracy, outperforming all the existing defensive models without using additional data.

</details>

<details>

<summary>2020-11-28 04:37:29 - Rewrite to Reinforce: Rewriting the Binary to Apply Countermeasures against Fault Injection</summary>

- *Pantea Kiaei, Cees-Bart Breunesse, Mohsen Ahmadi, Patrick Schaumont, Jasper van Woudenberg*

- `2011.14067v1` - [abs](http://arxiv.org/abs/2011.14067v1) - [pdf](http://arxiv.org/pdf/2011.14067v1)

> Fault injection attacks can cause errors in software for malicious purposes. Oftentimes, vulnerable points of a program are detected after its development. It is therefore critical for the user of the program to be able to apply last-minute security assurance to the executable file without having access to the source code. In this work, we explore two methodologies based on binary rewriting that aid in injecting countermeasures in the binary file. The first approach injects countermeasures by reassembling the disassembly whereas the second approach leverages a full translation to a high-level IR and lowering that back to the target architecture.

</details>

<details>

<summary>2020-11-28 06:05:53 - Query-Free Adversarial Transfer via Undertrained Surrogates</summary>

- *Chris Miller, Soroush Vosoughi*

- `2007.00806v2` - [abs](http://arxiv.org/abs/2007.00806v2) - [pdf](http://arxiv.org/pdf/2007.00806v2)

> Deep neural networks are vulnerable to adversarial examples -- minor perturbations added to a model's input which cause the model to output an incorrect prediction. We introduce a new method for improving the efficacy of adversarial attacks in a black-box setting by undertraining the surrogate model which the attacks are generated on. Using two datasets and five model architectures, we show that this method transfers well across architectures and outperforms state-of-the-art methods by a wide margin. We interpret the effectiveness of our approach as a function of reduced surrogate model loss function curvature and increased universal gradient characteristics, and show that our approach reduces the presence of local loss maxima which hinder transferability. Our results suggest that finding strong single surrogate models is a highly effective and simple method for generating transferable adversarial attacks, and that this method represents a valuable route for future study in this field.

</details>

<details>

<summary>2020-11-29 13:19:53 - A Targeted Universal Attack on Graph Convolutional Network</summary>

- *Jiazhu Dai, Weifeng Zhu, Xiangfeng Luo*

- `2011.14365v1` - [abs](http://arxiv.org/abs/2011.14365v1) - [pdf](http://arxiv.org/pdf/2011.14365v1)

> Graph-structured data exist in numerous applications in real life. As a state-of-the-art graph neural network, the graph convolutional network (GCN) plays an important role in processing graph-structured data. However, a recent study reported that GCNs are also vulnerable to adversarial attacks, which means that GCN models may suffer malicious attacks with unnoticeable modifications of the data. Among all the adversarial attacks on GCNs, there is a special kind of attack method called the universal adversarial attack, which generates a perturbation that can be applied to any sample and causes GCN models to output incorrect results. Although universal adversarial attacks in computer vision have been extensively researched, there are few research works on universal adversarial attacks on graph structured data. In this paper, we propose a targeted universal adversarial attack against GCNs. Our method employs a few nodes as the attack nodes. The attack capability of the attack nodes is enhanced through a small number of fake nodes connected to them. During an attack, any victim node will be misclassified by the GCN as the attack node class as long as it is linked to them. The experiments on three popular datasets show that the average attack success rate of the proposed attack on any victim node in the graph reaches 83% when using only 3 attack nodes and 6 fake nodes. We hope that our work will make the community aware of the threat of this type of attack and raise the attention given to its future defense.

</details>

<details>

<summary>2020-11-30 08:35:16 - Learning Explainable Interventions to Mitigate HIV Transmission in Sex Workers Across Five States in India</summary>

- *Raghav Awasthi, Prachi Patel, Vineet Joshi, Shama Karkal, Tavpritesh Sethi*

- `2012.01930v1` - [abs](http://arxiv.org/abs/2012.01930v1) - [pdf](http://arxiv.org/pdf/2012.01930v1)

> Female sex workers(FSWs) are one of the most vulnerable and stigmatized groups in society. As a result, they often suffer from a lack of quality access to care. Grassroot organizations engaged in improving health services are often faced with the challenge of improving the effectiveness of interventions due to complex influences. This work combines structure learning, discriminative modeling, and grass-root level expertise of designing interventions across five different Indian states to discover the influence of non-obvious factors for improving safe-sex practices in FSWs. A bootstrapped, ensemble-averaged Bayesian Network structure was learned to quantify the factors that could maximize condom usage as revealed from the model. A discriminative model was then constructed using XgBoost and random forest in order to predict condom use behavior The best model achieved 83% sensitivity, 99% specificity, and 99% area under the precision-recall curve for the prediction. Both generative and discriminative modeling approaches revealed that financial literacy training was the primary influence and predictor of condom use in FSWs. These insights have led to a currently ongoing field trial for assessing the real-world utility of this approach. Our work highlights the potential of explainable models for transparent discovery and prioritization of anti-HIV interventions in female sex workers in a resource-limited setting.

</details>

<details>

<summary>2020-11-30 20:09:42 - Enhance CNN Robustness Against Noises for Classification of 12-Lead ECG with Variable Length</summary>

- *Linhai Ma, Liang Liang*

- `2008.03609v4` - [abs](http://arxiv.org/abs/2008.03609v4) - [pdf](http://arxiv.org/pdf/2008.03609v4)

> Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor the condition of the cardiovascular system. Deep neural networks (DNNs), have been developed in many research labs for automatic interpretation of ECG signals to identify potential abnormalities in patient hearts. Studies have shown that given a sufficiently large amount of data, the classification accuracy of DNNs could reach human-expert cardiologist level. However, despite of the excellent performance in classification accuracy, it has been shown that DNNs are highly vulnerable to adversarial noises which are subtle changes in input of a DNN and lead to a wrong class-label prediction with a high confidence. Thus, it is challenging and essential to improve robustness of DNNs against adversarial noises for ECG signal classification, a life-critical application. In this work, we designed a CNN for classification of 12-lead ECG signals with variable length, and we applied three defense methods to improve robustness of this CNN for this classification task. The ECG data in this study is very challenging because the sample size is limited, and the length of each ECG recording varies in a large range. The evaluation results show that our customized CNN reached satisfying F1 score and average accuracy, comparable to the top-6 entries in the CPSC2018 ECG classification challenge, and the defense methods enhanced robustness of our CNN against adversarial noises and white noises, with a minimal reduction in accuracy on clean data.

</details>


## 2020-12

<details>

<summary>2020-12-02 12:18:27 - Blockchain Assisted Decentralized Federated Learning (BLADE-FL) with Lazy Clients</summary>

- *Jun Li, Yumeng Shao, Ming Ding, Chuan Ma, Kang Wei, Zhu Han, H. Vincent Poor*

- `2012.02044v1` - [abs](http://arxiv.org/abs/2012.02044v1) - [pdf](http://arxiv.org/pdf/2012.02044v1)

> Federated learning (FL), as a distributed machine learning approach, has drawn a great amount of attention in recent years. FL shows an inherent advantage in privacy preservation, since users' raw data are processed locally. However, it relies on a centralized server to perform model aggregation. Therefore, FL is vulnerable to server malfunctions and external attacks. In this paper, we propose a novel framework by integrating blockchain into FL, namely, blockchain assisted decentralized federated learning (BLADE-FL), to enhance the security of FL. The proposed BLADE-FL has a good performance in terms of privacy preservation, tamper resistance, and effective cooperation of learning. However, it gives rise to a new problem of training deficiency, caused by lazy clients who plagiarize others' trained models and add artificial noises to conceal their cheating behaviors. To be specific, we first develop a convergence bound of the loss function with the presence of lazy clients and prove that it is convex with respect to the total number of generated blocks $K$. Then, we solve the convex problem by optimizing $K$ to minimize the loss function. Furthermore, we discover the relationship between the optimal $K$, the number of lazy clients, and the power of artificial noises used by lazy clients. We conduct extensive experiments to evaluate the performance of the proposed framework using the MNIST and Fashion-MNIST datasets. Our analytical results are shown to be consistent with the experimental results. In addition, the derived optimal $K$ achieves the minimum value of loss function, and in turn the optimal accuracy performance.

</details>

<details>

<summary>2020-12-03 04:56:06 - FenceBox: A Platform for Defeating Adversarial Examples with Data Augmentation Techniques</summary>

- *Han Qiu, Yi Zeng, Tianwei Zhang, Yong Jiang, Meikang Qiu*

- `2012.01701v1` - [abs](http://arxiv.org/abs/2012.01701v1) - [pdf](http://arxiv.org/pdf/2012.01701v1)

> It is extensively studied that Deep Neural Networks (DNNs) are vulnerable to Adversarial Examples (AEs). With more and more advanced adversarial attack methods have been developed, a quantity of corresponding defense solutions were designed to enhance the robustness of DNN models. It has become a popularity to leverage data augmentation techniques to preprocess input samples before inference to remove adversarial perturbations. By obfuscating the gradients of DNN models, these approaches can defeat a considerable number of conventional attacks. Unfortunately, advanced gradient-based attack techniques (e.g., BPDA and EOT) were introduced to invalidate these preprocessing effects.   In this paper, we present FenceBox, a comprehensive framework to defeat various kinds of adversarial attacks. FenceBox is equipped with 15 data augmentation methods from three different categories. We comprehensively evaluated that these methods can effectively mitigate various adversarial attacks. FenceBox also provides APIs for users to easily deploy the defense over their models in different modes: they can either select an arbitrary preprocessing method, or a combination of functions for a better robustness guarantee, even under advanced adversarial attacks. We open-source FenceBox, and expect it can be used as a standard toolkit to facilitate the research of adversarial attacks and defenses.

</details>

<details>

<summary>2020-12-03 14:21:38 - Can I Take Your Subdomain? Exploring Related-Domain Attacks in the Modern Web</summary>

- *Marco Squarcina, Mauro Tempesta, Lorenzo Veronese, Stefano Calzavara, Matteo Maffei*

- `2012.01946v1` - [abs](http://arxiv.org/abs/2012.01946v1) - [pdf](http://arxiv.org/pdf/2012.01946v1)

> Related-domain attackers control a sibling domain of their target web application, e.g., as the result of a subdomain takeover. Despite their additional power over traditional web attackers, related-domain attackers received only limited attention by the research community. In this paper we define and quantify for the first time the threats that related-domain attackers pose to web application security. In particular, we first clarify the capabilities that related-domain attackers can acquire through different attack vectors, showing that different instances of the related-domain attacker concept are worth attention. We then study how these capabilities can be abused to compromise web application security by focusing on different angles, including: cookies, CSP, CORS, postMessage and domain relaxation. By building on this framework, we report on a large-scale security measurement on the top 50k domains from the Tranco list that led to the discovery of vulnerabilities in 887 sites, where we quantified the threats posed by related-domain attackers to popular web applications.

</details>

<details>

<summary>2020-12-04 03:00:58 - An Empirical Study on the Relation between Network Interpretability and Adversarial Robustness</summary>

- *Adam Noack, Isaac Ahern, Dejing Dou, Boyang Li*

- `1912.03430v6` - [abs](http://arxiv.org/abs/1912.03430v6) - [pdf](http://arxiv.org/pdf/1912.03430v6)

> Deep neural networks (DNNs) have had many successes, but they suffer from two major issues: (1) a vulnerability to adversarial examples and (2) a tendency to elude human interpretation. Interestingly, recent empirical and theoretical evidence suggests these two seemingly disparate issues are actually connected. In particular, robust models tend to provide more interpretable gradients than non-robust models. However, whether this relationship works in the opposite direction remains obscure. With this paper, we seek empirical answers to the following question: can models acquire adversarial robustness when they are trained to have interpretable gradients? We introduce a theoretically inspired technique called Interpretation Regularization (IR), which encourages a model's gradients to (1) match the direction of interpretable target salience maps and (2) have small magnitude. To assess model performance and tease apart factors that contribute to adversarial robustness, we conduct extensive experiments on MNIST and CIFAR-10 with both $\ell_2$ and $\ell_\infty$ attacks. We demonstrate that training the networks to have interpretable gradients improves their robustness to adversarial perturbations. Applying the network interpretation technique SmoothGrad yields additional performance gains, especially in cross-norm attacks and under heavy perturbations. The results indicate that the interpretability of the model gradients is a crucial factor for adversarial robustness. Code for the experiments can be found at https://github.com/a1noack/interp_regularization.

</details>

<details>

<summary>2020-12-04 08:12:38 - Towards Natural Robustness Against Adversarial Examples</summary>

- *Haoyu Chu, Shikui Wei, Yao Zhao*

- `2012.02452v1` - [abs](http://arxiv.org/abs/2012.02452v1) - [pdf](http://arxiv.org/pdf/2012.02452v1)

> Recent studies have shown that deep neural networks are vulnerable to adversarial examples, but most of the methods proposed to defense adversarial examples cannot solve this problem fundamentally. In this paper, we theoretically prove that there is an upper bound for neural networks with identity mappings to constrain the error caused by adversarial noises. However, in actual computations, this kind of neural network no longer holds any upper bound and is therefore susceptible to adversarial examples. Following similar procedures, we explain why adversarial examples can fool other deep neural networks with skip connections. Furthermore, we demonstrate that a new family of deep neural networks called Neural ODEs (Chen et al., 2018) holds a weaker upper bound. This weaker upper bound prevents the amount of change in the result from being too large. Thus, Neural ODEs have natural robustness against adversarial examples. We evaluate the performance of Neural ODEs compared with ResNet under three white-box adversarial attacks (FGSM, PGD, DI2-FGSM) and one black-box adversarial attack (Boundary Attack). Finally, we show that the natural robustness of Neural ODEs is even better than the robustness of neural networks that are trained with adversarial training methods, such as TRADES and YOPO.

</details>

<details>

<summary>2020-12-04 12:30:36 - Automating Seccomp Filter Generation for Linux Applications</summary>

- *Claudio Canella, Mario Werner, Daniel Gruss, Michael Schwarz*

- `2012.02554v1` - [abs](http://arxiv.org/abs/2012.02554v1) - [pdf](http://arxiv.org/pdf/2012.02554v1)

> Software vulnerabilities in applications undermine the security of applications. By blocking unused functionality, the impact of potential exploits can be reduced. While seccomp provides a solution for filtering syscalls, it requires manual implementation of filter rules for each individual application. Recent work has investigated automated approaches for detecting and installing the necessary filter rules. However, as we show, these approaches make assumptions that are not necessary or require overly time-consuming analysis.   In this paper, we propose Chestnut, an automated approach for generating strict syscall filters for Linux userspace applications with lower requirements and limitations. Chestnut comprises two phases, with the first phase consisting of two static components, i.e., a compiler and a binary analyzer, that extract the used syscalls during compilation or in an analysis of the binary. The compiler-based approach of Chestnut is up to factor 73 faster than previous approaches without affecting the accuracy adversely. On the binary analysis level, we demonstrate that the requirement of position-independent binaries of related work is not needed, enlarging the set of applications for which Chestnut is usable. In an optional second phase, Chestnut provides a dynamic refinement tool that allows restricting the set of allowed syscalls further. We demonstrate that Chestnut on average blocks 302 syscalls (86.5%) via the compiler and 288 (82.5%) using the binary-level analysis on a set of 18 widely used applications. We found that Chestnut blocks the dangerous exec syscall in 50% and 77.7% of the tested applications using the compiler- and binary-based approach, respectively. For the tested applications, Chestnut prevents exploitation of more than 62% of the 175 CVEs that target the kernel via syscalls. Finally, we perform a 6 month long-term study of a sandboxed Nginx server.

</details>

<details>

<summary>2020-12-04 16:58:09 - Efficient Sealable Protection Keys for RISC-V</summary>

- *Leila Delshadtehrani, Sadullah Canakci, Manuel Egele, Ajay Joshi*

- `2012.02715v1` - [abs](http://arxiv.org/abs/2012.02715v1) - [pdf](http://arxiv.org/pdf/2012.02715v1)

> With the continuous increase in the number of software-based attacks, there has been a growing effort towards isolating sensitive data and trusted software components from untrusted third-party components. A hardware-assisted intra-process isolation mechanism enables software developers to partition a process into isolated components and in turn secure sensitive data from untrusted components. However, most of the existing hardware-assisted intra-process isolation mechanisms in modern processors, such as ARM and IBM Power, rely on costly kernel operations for switching between trusted and untrusted domains. Recently, Intel introduced a new hardware feature for intra-process memory isolation, called Memory Protection Keys (MPK), which enables a user-space process to switch the domains in an efficient way. While the efficiency of Intel MPK enables developers to leverage it for common use cases such as Code-Pointer Integrity, the limited number of unique domains (16) prohibits its use in cases such as OpenSSL where a large number of domains are required. Moreover, Intel MPK suffers from the protection key use-after-free vulnerability. To address these shortcomings, in this paper, we propose an efficient intra-process isolation technique for the RISC-V open ISA, called SealPK, which supports up to 1024 unique domains. SealPK prevents the protection key use-after-free problem by leveraging a lazy de-allocation approach. To further strengthen SealPK, we devise three novel sealing features to protect the allocated domains, their associated pages, and their permissions from modifications or tampering by an attacker. To demonstrate the feasibility of our design, we implement SealPK on a RISC-V Rocket processor, provide the OS support for it, and prototype our design on an FPGA. We demonstrate the efficiency of SealPK by leveraging it to implement an isolated shadow stack on our FPGA prototype.

</details>

<details>

<summary>2020-12-05 21:39:09 - Injecting Reliable Radio Frequency Fingerprints Using Metasurface for The Internet of Things</summary>

- *Sekhar Rajendran, Zhi Sun, Feng Lin, Kui Ren*

- `2006.06895v3` - [abs](http://arxiv.org/abs/2006.06895v3) - [pdf](http://arxiv.org/pdf/2006.06895v3)

> In Internet of Things, where billions of devices with limited resources are communicating with each other, security has become a major stumbling block affecting the progress of this technology. Existing authentication schemes-based on digital signatures have overhead costs associated with them in terms of computation time, battery power, bandwidth, memory, and related hardware costs. Radio frequency fingerprint (RFF), utilizing the unique device-based information, can be a promising solution for IoT. However, traditional RFFs have become obsolete because of low reliability and reduced user capability. Our proposed solution, Metasurface RF-Fingerprinting Injection (MeRFFI), is to inject a carefully-designed radio frequency fingerprint into the wireless physical layer that can increase the security of a stationary IoT device with minimal overhead. The injection of fingerprint is implemented using a low cost metasurface developed and fabricated in our lab, which is designed to make small but detectable perturbations in the specific frequency band in which the IoT devices are communicating. We have conducted comprehensive system evaluations including distance, orientation, multiple channels where the feasibility, effectiveness, and reliability of these fingerprints are validated. The proposed MeRFFI system can be easily integrated into the existing authentication schemes. The security vulnerabilities are analyzed for some of the most threatening wireless physical layer-based attacks.

</details>

<details>

<summary>2020-12-06 02:26:36 - ML-based Flood Forecasting: Advances in Scale, Accuracy and Reach</summary>

- *Sella Nevo, Gal Elidan, Avinatan Hassidim, Guy Shalev, Oren Gilon, Grey Nearing, Yossi Matias*

- `2012.00671v2` - [abs](http://arxiv.org/abs/2012.00671v2) - [pdf](http://arxiv.org/pdf/2012.00671v2)

> Floods are among the most common and deadly natural disasters in the world, and flood warning systems have been shown to be effective in reducing harm. Yet the majority of the world's vulnerable population does not have access to reliable and actionable warning systems, due to core challenges in scalability, computational costs, and data availability. In this paper we present two components of flood forecasting systems which were developed over the past year, providing access to these critical systems to 75 million people who didn't have this access before.

</details>

<details>

<summary>2020-12-07 01:14:19 - Black-box Model Inversion Attribute Inference Attacks on Classification Models</summary>

- *Shagufta Mehnaz, Ninghui Li, Elisa Bertino*

- `2012.03404v1` - [abs](http://arxiv.org/abs/2012.03404v1) - [pdf](http://arxiv.org/pdf/2012.03404v1)

> Increasing use of ML technologies in privacy-sensitive domains such as medical diagnoses, lifestyle predictions, and business decisions highlights the need to better understand if these ML technologies are introducing leakages of sensitive and proprietary training data. In this paper, we focus on one kind of model inversion attacks, where the adversary knows non-sensitive attributes about instances in the training data and aims to infer the value of a sensitive attribute unknown to the adversary, using oracle access to the target classification model. We devise two novel model inversion attribute inference attacks -- confidence modeling-based attack and confidence score-based attack, and also extend our attack to the case where some of the other (non-sensitive) attributes are unknown to the adversary. Furthermore, while previous work uses accuracy as the metric to evaluate the effectiveness of attribute inference attacks, we find that accuracy is not informative when the sensitive attribute distribution is unbalanced. We identify two metrics that are better for evaluating attribute inference attacks, namely G-mean and Matthews correlation coefficient (MCC). We evaluate our attacks on two types of machine learning models, decision tree and deep neural network, trained with two real datasets. Experimental results show that our newly proposed attacks significantly outperform the state-of-the-art attacks. Moreover, we empirically show that specific groups in the training dataset (grouped by attributes, e.g., gender, race) could be more vulnerable to model inversion attacks. We also demonstrate that our attacks' performances are not impacted significantly when some of the other (non-sensitive) attributes are also unknown to the adversary.

</details>

<details>

<summary>2020-12-07 08:13:39 - PiPoMonitor: Mitigating Cross-core Cache Attacks Using the Auto-Cuckoo Filter</summary>

- *Fengkai Yuan, Kai Wang, Rui Hou, Xiaoxin Li, Peinan Li, Lutan Zhao, Jiameng Ying, Amro Awad, Dan Meng*

- `2012.01046v2` - [abs](http://arxiv.org/abs/2012.01046v2) - [pdf](http://arxiv.org/pdf/2012.01046v2)

> Cache side channel attacks obtain victim cache line access footprint to infer security-critical information. Among them, cross-core attacks exploiting the shared last level cache are more threatening as their simplicity to set up and high capacity. Stateful approaches of detection-based mitigation observe precise cache behaviors and protect specific cache lines that are suspected of being attacked. However, their recording structures incur large storage overhead and are vulnerable to reverse engineering attacks. Exploring the intrinsic non-determinate layout of a traditional Cuckoo filter, this paper proposes a space efficient Auto-Cuckoo filter to record access footprints, which succeed to decrease storage overhead and resist reverse engineering attacks at the same time. With Auto-Cuckoo filter, we propose PiPoMonitor to detect \textit{Ping-Pong patterns} and prefetch specific cache line to interfere with adversaries' cache probes. Security analysis shows the PiPoMonitor can effectively mitigate cross-core attacks and the Auto-Cuckoo filter is immune to reverse engineering attacks. Evaluation results indicate PiPoMonitor has negligible impact on performance and the storage overhead is only 0.37$\%$, an order of magnitude lower than previous stateful approaches.

</details>

<details>

<summary>2020-12-07 08:13:50 - Defense for Black-box Attacks on Anti-spoofing Models by Self-Supervised Learning</summary>

- *Haibin Wu, Andy T. Liu, Hung-yi Lee*

- `2006.03214v3` - [abs](http://arxiv.org/abs/2006.03214v3) - [pdf](http://arxiv.org/pdf/2006.03214v3)

> High-performance anti-spoofing models for automatic speaker verification (ASV), have been widely used to protect ASV by identifying and filtering spoofing audio that is deliberately generated by text-to-speech, voice conversion, audio replay, etc. However, it has been shown that high-performance anti-spoofing models are vulnerable to adversarial attacks. Adversarial attacks, that are indistinguishable from original data but result in the incorrect predictions, are dangerous for anti-spoofing models and not in dispute we should detect them at any cost. To explore this issue, we proposed to employ Mockingjay, a self-supervised learning based model, to protect anti-spoofing models against adversarial attacks in the black-box scenario. Self-supervised learning models are effective in improving downstream task performance like phone classification or ASR. However, their effect in defense for adversarial attacks has not been explored yet. In this work, we explore the robustness of self-supervised learned high-level representations by using them in the defense against adversarial attacks. A layerwise noise to signal ratio (LNSR) is proposed to quantize and measure the effectiveness of deep models in countering adversarial noise. Experimental results on the ASVspoof 2019 dataset demonstrate that high-level representations extracted by Mockingjay can prevent the transferability of adversarial examples, and successfully counter black-box attacks.

</details>

<details>

<summary>2020-12-07 08:40:56 - Backpropagating Linearly Improves Transferability of Adversarial Examples</summary>

- *Yiwen Guo, Qizhang Li, Hao Chen*

- `2012.03528v1` - [abs](http://arxiv.org/abs/2012.03528v1) - [pdf](http://arxiv.org/pdf/2012.03528v1)

> The vulnerability of deep neural networks (DNNs) to adversarial examples has drawn great attention from the community. In this paper, we study the transferability of such examples, which lays the foundation of many black-box attacks on DNNs. We revisit a not so new but definitely noteworthy hypothesis of Goodfellow et al.'s and disclose that the transferability can be enhanced by improving the linearity of DNNs in an appropriate manner. We introduce linear backpropagation (LinBP), a method that performs backpropagation in a more linear fashion using off-the-shelf attacks that exploit gradients. More specifically, it calculates forward as normal but backpropagates loss as if some nonlinear activations are not encountered in the forward pass. Experimental results demonstrate that this simple yet effective method obviously outperforms current state-of-the-arts in crafting transferable adversarial examples on CIFAR-10 and ImageNet, leading to more effective attacks on a variety of DNNs.

</details>

<details>

<summary>2020-12-07 12:10:36 - Automatically Eliminating Speculative Leaks from Cryptographic Code with Blade</summary>

- *Marco Vassena, Craig Disselkoen, Klaus V. Gleissenthall, Sunjay Cauligi, Rami Gökhan Kici, Ranjit Jhala, Dean Tullsen, Deian Stefan*

- `2005.00294v3` - [abs](http://arxiv.org/abs/2005.00294v3) - [pdf](http://arxiv.org/pdf/2005.00294v3)

> We introduce BLADE, a new approach to automatically and efficiently eliminate speculative leaks from cryptographic code. BLADE is built on the insight that to stop leaks via speculation, it suffices to $\textit{cut}$ the dataflow from expressions that speculatively introduce secrets ($\textit{sources}$) to those that leak them through the cache ($\textit{sinks}$), rather than prohibit speculation altogether. We formalize this insight in a $\textit{static type system}$ that (1) types each expression as either $\textit{transient}$, i.e., possibly containing speculative secrets or as being $\textit{stable}$, and (2) prohibits speculative leaks by requiring that all $\textit{sink}$ expressions are stable. BLADE relies on a new new abstract primitive, $\textbf{protect}$, to halt speculation at fine granularity. We formalize and implement $\textbf{protect}$ using existing architectural mechanisms, and show how BLADE's type system can automatically synthesize a $\textit{minimal}$ number of $\textbf{protect}$s to provably eliminate speculative leaks. We implement BLADE in the Cranelift WebAssembly compiler and evaluate our approach by repairing several verified, yet vulnerable WebAssembly implementations of cryptographic primitives. We find that Blade can fix existing programs that leak via speculation $\textit{automatically}$, without user intervention, and $\textit{efficiently}$ even when using fences to implement $\textbf{protect}$.

</details>

<details>

<summary>2020-12-07 15:57:12 - Vulnerability Forecasting: In theory and practice</summary>

- *Éireann Leverett, Matilda Rhode, Adam Wedgbury*

- `2012.03814v1` - [abs](http://arxiv.org/abs/2012.03814v1) - [pdf](http://arxiv.org/pdf/2012.03814v1)

> Why wait for zero-days when you could predict them in advance? It is possible to predict the volume of CVEs released in the NVD as much as a year in advance. This can be done within 3 percent of the actual value, and different predictive algorithms perform well at different lookahead values. It is also possible to estimate the proportions of that total volumn belonging to specific vendors, software, CVSS scores, or vulnerability types. Strategic patch management should become much easier, with this uncertainty reduction.

</details>

<details>

<summary>2020-12-07 21:19:24 - Dragonblood is Still Leaking: Practical Cache-based Side-Channel in the Wild</summary>

- *Daniel De Almeida Braga, Pierre-Alain Fouque, Mohamed Sabt*

- `2012.02745v2` - [abs](http://arxiv.org/abs/2012.02745v2) - [pdf](http://arxiv.org/pdf/2012.02745v2)

> Recently, the Dragonblood attacks have attracted new interests on the security of WPA-3 implementation and in particular on the Dragonfly code deployed on many open-source libraries. One attack concerns the protection of users passwords during authentication. In the Password Authentication Key Exchange (PAKE) protocol called Dragonfly, the secret, namely the password, is mapped to an elliptic curve point. This operation is sensitive, as it involves the secret password, and therefore its resistance against side-channel attacks is of utmost importance. Following the initial disclosure of Dragonblood, we notice that this particular attack has been partially patched by only a few implementations.   In this work, we show that the patches implemented after the disclosure of Dragonblood are insufficient. We took advantage of state-of-the-art techniques to extend the original attack, demonstrating that we are able to recover the password with only a third of the measurements needed in Dragonblood attack. We mainly apply our attack on two open-source projects: iwd (iNet Wireless Daemon) and FreeRADIUS, in order underline the practicability of our attack. Indeed, the iwd package, written by Intel, is already deployed in the Arch Linux distribution, which is well-known among security experts, and aims to offer an alternative to wpa\_supplicant. As for FreeRADIUS, it is widely deployed and well-maintained upstream open-source project. We publish a full Proof of Concept of our attack, and actively participated in the process of patching the vulnerable code. Here, in a backward compatibility perspective, we advise the use of a branch-free implementation as a mitigation technique, as what was used in hostapd, due to its quite simplicity and its negligible incurred overhead.

</details>

<details>

<summary>2020-12-08 11:03:17 - Reinforcement Based Learning on Classification Task Could Yield Better Generalization and Adversarial Accuracy</summary>

- *Shashi Kant Gupta*

- `2012.04353v1` - [abs](http://arxiv.org/abs/2012.04353v1) - [pdf](http://arxiv.org/pdf/2012.04353v1)

> Deep Learning has become interestingly popular in computer vision, mostly attaining near or above human-level performance in various vision tasks. But recent work has also demonstrated that these deep neural networks are very vulnerable to adversarial examples (adversarial examples - inputs to a model which are naturally similar to original data but fools the model in classifying it into a wrong class). Humans are very robust against such perturbations; one possible reason could be that humans do not learn to classify based on an error between "target label" and "predicted label" but possibly due to reinforcements that they receive on their predictions. In this work, we proposed a novel method to train deep learning models on an image classification task. We used a reward-based optimization function, similar to the vanilla policy gradient method used in reinforcement learning, to train our model instead of conventional cross-entropy loss. An empirical evaluation on the cifar10 dataset showed that our method learns a more robust classifier than the same model architecture trained using cross-entropy loss function (on adversarial training). At the same time, our method shows a better generalization with the difference in test accuracy and train accuracy $< 2\%$ for most of the time compared to the cross-entropy one, whose difference most of the time remains $> 2\%$.

</details>

<details>

<summary>2020-12-08 14:11:32 - Towards Communication-efficient and Attack-Resistant Federated Edge Learning for Industrial Internet of Things</summary>

- *Yi Liu, Ruihui Zhao, Jiawen Kang, Abdulsalam Yassine, Dusit Niyato, Jialiang Peng*

- `2012.04436v1` - [abs](http://arxiv.org/abs/2012.04436v1) - [pdf](http://arxiv.org/pdf/2012.04436v1)

> Federated Edge Learning (FEL) allows edge nodes to train a global deep learning model collaboratively for edge computing in the Industrial Internet of Things (IIoT), which significantly promotes the development of Industrial 4.0. However, FEL faces two critical challenges: communication overhead and data privacy. FEL suffers from expensive communication overhead when training large-scale multi-node models. Furthermore, due to the vulnerability of FEL to gradient leakage and label-flipping attacks, the training process of the global model is easily compromised by adversaries. To address these challenges, we propose a communication-efficient and privacy-enhanced asynchronous FEL framework for edge computing in IIoT. First, we introduce an asynchronous model update scheme to reduce the computation time that edge nodes wait for global model aggregation. Second, we propose an asynchronous local differential privacy mechanism, which improves communication efficiency and mitigates gradient leakage attacks by adding well-designed noise to the gradients of edge nodes. Third, we design a cloud-side malicious node detection mechanism to detect malicious nodes by testing the local model quality. Such a mechanism can avoid malicious nodes participating in training to mitigate label-flipping attacks. Extensive experimental studies on two real-world datasets demonstrate that the proposed framework can not only improve communication efficiency but also mitigate malicious attacks while its accuracy is comparable to traditional FEL frameworks.

</details>

<details>

<summary>2020-12-08 16:02:53 - Deterministic Random Number Generator Attack against the Kirchhoff-Law-Johnson-Noise Secure Key Exchange Protocol</summary>

- *Christiana Chamon, Shahriar Ferdous, Laszlo Kish*

- `2012.02848v2` - [abs](http://arxiv.org/abs/2012.02848v2) - [pdf](http://arxiv.org/pdf/2012.02848v2)

> This paper demonstrates the vulnerability of the Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchanger to compromised random number generator(s) even if these random numbers are used solely to generate the noises emulating the Johnson noise of Alice's and Bob's resistors. The attacks shown are deterministic in the sense that Eve's knowledge of Alice's and/or Bob's random numbers is basically deterministic. Moreover, no statistical evaluation is needed, except for rarely occurring events of negligible, random waiting time and verification time. We explore two situations. In the first case, Eve knows both Alice's and Bob's random noises. We show that, in this situation, Eve can quickly crack the secure key bit by using Ohm's Law. In the other situation, Eve knows only Bob's random noise. Then Eve first can learn Bob's resistance value by using Ohm's Law. Therefore, she will have the same knowledge as Bob, thus at the end of the bit exchange period, she will know Alice's bit.

</details>

<details>

<summary>2020-12-08 19:04:31 - On the Privacy and Integrity Risks of Contact-Tracing Applications</summary>

- *Jianwei Huang, Vinod Yegneswaran, Phillip Porras, Guofei Gu*

- `2012.03283v2` - [abs](http://arxiv.org/abs/2012.03283v2) - [pdf](http://arxiv.org/pdf/2012.03283v2)

> Smartphone-based contact-tracing applications are at the epicenter of the global fight against the Covid-19 pandemic. While governments and healthcare agencies are eager to mandate the deployment of such applications en-masse, they face increasing scrutiny from the popular press, security companies, and human rights watch agencies that fear the exploitation of these technologies as surveillance tools. Finding the optimal balance between community safety and privacy has been a challenge, and strategies to address these concerns have varied among countries. This paper describes two important attacks that affect a broad swath of contact-tracing applications. The first, referred to as contact-isolation attack, is a user-privacy attack that can be used to identify potentially infected patients in your neighborhood. The second is a contact-pollution attack that affects the integrity of contact tracing applications by causing them to produce a high volume of false-positive alerts. We developed prototype implementations and evaluated both attacks in the context of the DP-3T application framework, but these vulnerabilities affect a much broader class of applications. We found that both attacks are feasible and realizable with a minimal attacker work factor. We further conducted an impact assessment of these attacks by using a simulation study and measurements from the SafeGraph database. Our results indicate that attacks launched from a modest number (on the order of 10,000) of monitoring points can effectively decloak between 5-40\% of infected users in a major metropolis, such as Houston.

</details>

<details>

<summary>2020-12-08 20:42:12 - Provable Defense against Privacy Leakage in Federated Learning from Representation Perspective</summary>

- *Jingwei Sun, Ang Li, Binghui Wang, Huanrui Yang, Hai Li, Yiran Chen*

- `2012.06043v1` - [abs](http://arxiv.org/abs/2012.06043v1) - [pdf](http://arxiv.org/pdf/2012.06043v1)

> Federated learning (FL) is a popular distributed learning framework that can reduce privacy risks by not explicitly sharing private data. However, recent works demonstrated that sharing model updates makes FL vulnerable to inference attacks. In this work, we show our key observation that the data representation leakage from gradients is the essential cause of privacy leakage in FL. We also provide an analysis of this observation to explain how the data presentation is leaked. Based on this observation, we propose a defense against model inversion attack in FL. The key idea of our defense is learning to perturb data representation such that the quality of the reconstructed data is severely degraded, while FL performance is maintained. In addition, we derive certified robustness guarantee to FL and convergence guarantee to FedAvg, after applying our defense. To evaluate our defense, we conduct experiments on MNIST and CIFAR10 for defending against the DLG attack and GS attack. Without sacrificing accuracy, the results demonstrate that our proposed defense can increase the mean squared error between the reconstructed data and the raw data by as much as more than 160X for both DLG attack and GS attack, compared with baseline defense methods. The privacy of the FL system is significantly improved.

</details>

<details>

<summary>2020-12-08 20:51:43 - A Deep Marginal-Contrastive Defense against Adversarial Attacks on 1D Models</summary>

- *Mohammed Hassanin, Nour Moustafa, Murat Tahtali*

- `2012.04734v1` - [abs](http://arxiv.org/abs/2012.04734v1) - [pdf](http://arxiv.org/pdf/2012.04734v1)

> Deep learning algorithms have been recently targeted by attackers due to their vulnerability. Several research studies have been conducted to address this issue and build more robust deep learning models. Non-continuous deep models are still not robust against adversarial, where most of the recent studies have focused on developing attack techniques to evade the learning process of the models. One of the main reasons behind the vulnerability of such models is that a learning classifier is unable to slightly predict perturbed samples. To address this issue, we propose a novel objective/loss function, the so-called marginal contrastive, which enforces the features to lie under a specified margin to facilitate their prediction using deep convolutional networks (i.e., Char-CNN). Extensive experiments have been conducted on continuous cases (e.g., UNSW NB15 dataset) and discrete ones (i.e, eight-large-scale datasets [32]) to prove the effectiveness of the proposed method. The results revealed that the regularization of the learning process based on the proposed loss function can improve the performance of Char-CNN.

</details>

<details>

<summary>2020-12-08 21:25:44 - Mitigating the Impact of Adversarial Attacks in Very Deep Networks</summary>

- *Mohammed Hassanin, Ibrahim Radwan, Nour Moustafa, Murat Tahtali, Neeraj Kumar*

- `2012.04750v1` - [abs](http://arxiv.org/abs/2012.04750v1) - [pdf](http://arxiv.org/pdf/2012.04750v1)

> Deep Neural Network (DNN) models have vulnerabilities related to security concerns, with attackers usually employing complex hacking techniques to expose their structures. Data poisoning-enabled perturbation attacks are complex adversarial ones that inject false data into models. They negatively impact the learning process, with no benefit to deeper networks, as they degrade a model's accuracy and convergence rates. In this paper, we propose an attack-agnostic-based defense method for mitigating their influence. In it, a Defensive Feature Layer (DFL) is integrated with a well-known DNN architecture which assists in neutralizing the effects of illegitimate perturbation samples in the feature space. To boost the robustness and trustworthiness of this method for correctly classifying attacked input samples, we regularize the hidden space of a trained model with a discriminative loss function called Polarized Contrastive Loss (PCL). It improves discrimination among samples in different classes and maintains the resemblance of those in the same class. Also, we integrate a DFL and PCL in a compact model for defending against data poisoning attacks. This method is trained and tested using the CIFAR-10 and MNIST datasets with data poisoning-enabled perturbation attacks, with the experimental results revealing its excellent performance compared with those of recent peer techniques.

</details>

<details>

<summary>2020-12-08 21:59:49 - Fine-Grained Network Analysis for Modern Software Ecosystems</summary>

- *Paolo Boldi, Georgios Gousios*

- `2012.04760v1` - [abs](http://arxiv.org/abs/2012.04760v1) - [pdf](http://arxiv.org/pdf/2012.04760v1)

> Modern software development is increasingly dependent on components, libraries and frameworks coming from third-party vendors or open-source suppliers and made available through a number of platforms (or forges). This way of writing software puts an emphasis on reuse and on composition, commoditizing the services which modern applications require. On the other hand, bugs and vulnerabilities in a single library living in one such ecosystem can affect, directly or by transitivity, a huge number of other libraries and applications. Currently, only product-level information on library dependencies is used to contain this kind of danger, but this knowledge often reveals itself too imprecise to lead to effective (and possibly automated) handling policies. We will discuss how fine-grained function-level dependencies can greatly improve reliability and reduce the impact of vulnerabilities on the whole software ecosystem.

</details>

<details>

<summary>2020-12-09 05:21:05 - Predicting Individual Substance Abuse Vulnerability using Machine Learning Techniques</summary>

- *Uwaise Ibna Islam, Iqbal H. Sarker, Enamul Haque, Mohammed Moshiul Hoque*

- `2101.03184v1` - [abs](http://arxiv.org/abs/2101.03184v1) - [pdf](http://arxiv.org/pdf/2101.03184v1)

> Substance abuse is the unrestrained and detrimental use of psychoactive chemical substances, unauthorized drugs, and alcohol. Continuous use of these substances can ultimately lead a human to disastrous consequences. As patients display a high rate of relapse, prevention at an early stage can be an effective restraint. We therefore propose a binary classifier to identify any individual's present vulnerability towards substance abuse by analyzing subjects' socio-economic environment. We have collected data by a questionnaire which is created after carefully assessing the commonly involved factors behind substance abuse. Pearson's chi-squared test of independence is used to identify key feature variables influencing substance abuse. Later we build the predictive classifiers using machine learning classification algorithms on those variables. Logistic regression classifier trained with 18 features can predict individual vulnerability with the best accuracy.

</details>

<details>

<summary>2020-12-09 09:38:21 - Word-level Textual Adversarial Attacking as Combinatorial Optimization</summary>

- *Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun*

- `1910.12196v4` - [abs](http://arxiv.org/abs/1910.12196v4) - [pdf](http://arxiv.org/pdf/1910.12196v4)

> Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.

</details>

<details>

<summary>2020-12-09 14:14:59 - Why Charles Can Pen-test: an Evolutionary Approach to Vulnerability Testing</summary>

- *Gabriele Costa, Andrea Valenza*

- `2011.13213v2` - [abs](http://arxiv.org/abs/2011.13213v2) - [pdf](http://arxiv.org/pdf/2011.13213v2)

> Discovering vulnerabilities in applications of real-world complexity is a daunting task: a vulnerability may affect a single line of code, and yet it compromises the security of the entire application. Even worse, vulnerabilities may manifest only in exceptional circumstances that do not occur in the normal operation of the application. It is widely recognized that state-of-the-art penetration testing tools play a crucial role, and are routinely used, to dig up vulnerabilities. Yet penetration testing is still primarily a human-driven activity, and its effectiveness still depends on the skills and ingenuity of the security analyst driving the tool. In this paper, we propose a technique for the automatic discovery of vulnerabilities in event-based systems, such as web and mobile applications. Our approach is based on a collaborative, co-evolutionary and contract-driven search strategy that iteratively (i) executes a pool of test cases, (ii) identifies the most promising ones, and (iii) generates new test cases from them. The approach makes a synergistic combination of evolutionary algorithms where several "species" contribute to solving the problem: one species, the test species, evolves to find the target test case, i.e., the set of instruction whose execution lead to the vulnerable statement, whereas the other species, called contract species, evolve to select the parameters for the procedure calls needed to trigger the vulnerability. To assess the effectiveness of our approach, we implemented a working prototype and ran it against both a case study and a benchmark web application. The experimental results confirm that our tool automatically discovers and executes a number of injection flaw attacks that are out of reach for state-of-the-art web scanners.

</details>

<details>

<summary>2020-12-09 18:56:31 - Systematic Evaluation of Privacy Risks of Machine Learning Models</summary>

- *Liwei Song, Prateek Mittal*

- `2003.10595v2` - [abs](http://arxiv.org/abs/2003.10595v2) - [pdf](http://arxiv.org/pdf/2003.10595v2)

> Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported.   Next, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to identify samples with high privacy risks and perform attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score and demonstrate that the distribution of privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risks, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.

</details>

<details>

<summary>2020-12-09 21:09:03 - Securing Deep Spiking Neural Networks against Adversarial Attacks through Inherent Structural Parameters</summary>

- *Rida El-Allami, Alberto Marchisio, Muhammad Shafique, Ihsen Alouani*

- `2012.05321v1` - [abs](http://arxiv.org/abs/2012.05321v1) - [pdf](http://arxiv.org/pdf/2012.05321v1)

> Deep Learning (DL) algorithms have gained popularity owing to their practical problem-solving capacity. However, they suffer from a serious integrity threat, i.e., their vulnerability to adversarial attacks. In the quest for DL trustworthiness, recent works claimed the inherent robustness of Spiking Neural Networks (SNNs) to these attacks, without considering the variability in their structural spiking parameters. This paper explores the security enhancement of SNNs through internal structural parameters. Specifically, we investigate the SNNs robustness to adversarial attacks with different values of the neuron's firing voltage thresholds and time window boundaries. We thoroughly study SNNs security under different adversarial attacks in the strong white-box setting, with different noise budgets and under variable spiking parameters. Our results show a significant impact of the structural parameters on the SNNs' security, and promising sweet spots can be reached to design trustworthy SNNs with 85% higher robustness than a traditional non-spiking DL system. To the best of our knowledge, this is the first work that investigates the impact of structural parameters on SNNs robustness to adversarial attacks. The proposed contributions and the experimental framework is available online to the community for reproducible research.

</details>

<details>

<summary>2020-12-09 22:10:17 - Vulnerability Analysis of Face Morphing Attacks from Landmarks and Generative Adversarial Networks</summary>

- *Eklavya Sarkar, Pavel Korshunov, Laurent Colbois, Sébastien Marcel*

- `2012.05344v1` - [abs](http://arxiv.org/abs/2012.05344v1) - [pdf](http://arxiv.org/pdf/2012.05344v1)

> Morphing attacks is a threat to biometric systems where the biometric reference in an identity document can be altered. This form of attack presents an important issue in applications relying on identity documents such as border security or access control. Research in face morphing attack detection is developing rapidly, however very few datasets with several forms of attacks are publicly available. This paper bridges this gap by providing a new dataset with four different types of morphing attacks, based on OpenCV, FaceMorpher, WebMorph and a generative adversarial network (StyleGAN), generated with original face images from three public face datasets. We also conduct extensive experiments to assess the vulnerability of the state-of-the-art face recognition systems, notably FaceNet, VGG-Face, and ArcFace. The experiments demonstrate that VGG-Face, while being less accurate face recognition system compared to FaceNet, is also less vulnerable to morphing attacks. Also, we observed that na\"ive morphs generated with a StyleGAN do not pose a significant threat.

</details>

<details>

<summary>2020-12-10 06:02:51 - Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption</summary>

- *Xu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan Luo, Liangyou Li*

- `2006.05620v2` - [abs](http://arxiv.org/abs/2006.05620v2) - [pdf](http://arxiv.org/pdf/2006.05620v2)

> We argue that the vulnerability of model parameters is of crucial value to the study of model robustness and generalization but little research has been devoted to understanding this matter. In this work, we propose an indicator to measure the robustness of neural network parameters by exploiting their vulnerability via parameter corruption. The proposed indicator describes the maximum loss variation in the non-trivial worst-case scenario under parameter corruption. For practical purposes, we give a gradient-based estimation, which is far more effective than random corruption trials that can hardly induce the worst accuracy degradation. Equipped with theoretical support and empirical validation, we are able to systematically investigate the robustness of different model parameters and reveal vulnerability of deep neural networks that has been rarely paid attention to before. Moreover, we can enhance the models accordingly with the proposed adversarial corruption-resistant training, which not only improves the parameter robustness but also translates into accuracy elevation.

</details>

<details>

<summary>2020-12-10 09:30:31 - Integration of Security Modules in Software Development Lifecycle Phases</summary>

- *Isaac Chin Eian, Lim Ka Yong, Majesty Yeap Xiao Li, Noor Affan Bin Noor Hasmaddi, Fatima-tuz-Zahra*

- `2012.05540v1` - [abs](http://arxiv.org/abs/2012.05540v1) - [pdf](http://arxiv.org/pdf/2012.05540v1)

> Information protection is becoming a focal point for designing, creating and implementing software applications within highly integrated technology environments. The use of a safe coding technique in the software development process is required by many industrial IT security standards and policies. Despite current cyber protection measures and best practices, vulnerabilities still remain strong and become a huge threat to every developed software. It is crucial to understand the position of secure software development for security management, which is affected by causes such as human security-related factors. Although developers are often held accountable for security vulnerabilities, in reality, many problems often grow from a lack of organizational support during development tasks to handle security. While abstract safe coding guidelines are generally recognized, there are limited low-level secure coding guidelines for various programming languages. A good technique is required to standardize these guidelines for software developers. The goal of this paper is to address this gap by providing software designers and developers with direction by identifying a set of secure software development guidelines. Additionally, an overview of criteria for selection of safe coding guidelines is performed along with investigation of appropriate awareness methods for secure coding.

</details>

<details>

<summary>2020-12-10 11:19:11 - Probabilistic Jacobian-based Saliency Maps Attacks</summary>

- *Théo Combey, António Loison, Maxime Faucher, Hatem Hajri*

- `2007.06032v4` - [abs](http://arxiv.org/abs/2007.06032v4) - [pdf](http://arxiv.org/pdf/2007.06032v4)

> Neural network classifiers (NNCs) are known to be vulnerable to malicious adversarial perturbations of inputs including those modifying a small fraction of the input features named sparse or $L_0$ attacks. Effective and fast $L_0$ attacks, such as the widely used Jacobian-based Saliency Map Attack (JSMA) are practical to fool NNCs but also to improve their robustness. In this paper, we show that penalising saliency maps of JSMA by the output probabilities and the input features of the NNC allows to obtain more powerful attack algorithms that better take into account each input's characteristics. This leads us to introduce improved versions of JSMA, named Weighted JSMA (WJSMA) and Taylor JSMA (TJSMA), and demonstrate through a variety of white-box and black-box experiments on three different datasets (MNIST, CIFAR-10 and GTSRB), that they are both significantly faster and more efficient than the original targeted and non-targeted versions of JSMA. Experiments also demonstrate, in some cases, very competitive results of our attacks in comparison with the Carlini-Wagner (CW) $L_0$ attack, while remaining, like JSMA, significantly faster (WJSMA and TJSMA are more than 50 times faster than CW $L_0$ on CIFAR-10). Therefore, our new attacks provide good trade-offs between JSMA and CW for $L_0$ real-time adversarial testing on datasets such as the ones previously cited. Codes are publicly available through the link https://github.com/probabilistic-jsmas/probabilistic-jsmas.

</details>

<details>

<summary>2020-12-10 23:40:23 - Robustness and Transferability of Universal Attacks on Compressed Models</summary>

- *Alberto G. Matachana, Kenneth T. Co, Luis Muñoz-González, David Martinez, Emil C. Lupu*

- `2012.06024v1` - [abs](http://arxiv.org/abs/2012.06024v1) - [pdf](http://arxiv.org/pdf/2012.06024v1)

> Neural network compression methods like pruning and quantization are very effective at efficiently deploying Deep Neural Networks (DNNs) on edge devices. However, DNNs remain vulnerable to adversarial examples-inconspicuous inputs that are specifically designed to fool these models. In particular, Universal Adversarial Perturbations (UAPs), are a powerful class of adversarial attacks which create adversarial perturbations that can generalize across a large set of inputs. In this work, we analyze the effect of various compression techniques to UAP attacks, including different forms of pruning and quantization. We test the robustness of compressed models to white-box and transfer attacks, comparing them with their uncompressed counterparts on CIFAR-10 and SVHN datasets. Our evaluations reveal clear differences between pruning methods, including Soft Filter and Post-training Pruning. We observe that UAP transfer attacks between pruned and full models are limited, suggesting that the systemic vulnerabilities across these models are different. This finding has practical implications as using different compression techniques can blunt the effectiveness of black-box transfer attacks. We show that, in some scenarios, quantization can produce gradient-masking, giving a false sense of security. Finally, our results suggest that conclusions about the robustness of compressed models to UAP attacks is application dependent, observing different phenomena in the two datasets used in our experiments.

</details>

<details>

<summary>2020-12-11 00:50:09 - Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable</summary>

- *Odest Chadwicke Jenkins, Daniel Lopresti, Melanie Mitchell*

- `2012.06058v1` - [abs](http://arxiv.org/abs/2012.06058v1) - [pdf](http://arxiv.org/pdf/2012.06058v1)

> The history of AI has included several "waves" of ideas. The first wave, from the mid-1950s to the 1980s, focused on logic and symbolic hand-encoded representations of knowledge, the foundations of so-called "expert systems". The second wave, starting in the 1990s, focused on statistics and machine learning, in which, instead of hand-programming rules for behavior, programmers constructed "statistical learning algorithms" that could be trained on large datasets. In the most recent wave research in AI has largely focused on deep (i.e., many-layered) neural networks, which are loosely inspired by the brain and trained by "deep learning" methods. However, while deep neural networks have led to many successes and new capabilities in computer vision, speech recognition, language processing, game-playing, and robotics, their potential for broad application remains limited by several factors.   A concerning limitation is that even the most successful of today's AI systems suffer from brittleness-they can fail in unexpected ways when faced with situations that differ sufficiently from ones they have been trained on. This lack of robustness also appears in the vulnerability of AI systems to adversarial attacks, in which an adversary can subtly manipulate data in a way to guarantee a specific wrong answer or action from an AI system. AI systems also can absorb biases-based on gender, race, or other factors-from their training data and further magnify these biases in their subsequent decision-making. Taken together, these various limitations have prevented AI systems such as automatic medical diagnosis or autonomous vehicles from being sufficiently trustworthy for wide deployment. The massive proliferation of AI across society will require radically new ideas to yield technology that will not sacrifice our productivity, our quality of life, or our values.

</details>

<details>

<summary>2020-12-11 04:03:15 - I-GCN: Robust Graph Convolutional Network via Influence Mechanism</summary>

- *Haoxi Zhan, Xiaobing Pei*

- `2012.06110v1` - [abs](http://arxiv.org/abs/2012.06110v1) - [pdf](http://arxiv.org/pdf/2012.06110v1)

> Deep learning models for graphs, especially Graph Convolutional Networks (GCNs), have achieved remarkable performance in the task of semi-supervised node classification. However, recent studies show that GCNs suffer from adversarial perturbations. Such vulnerability to adversarial attacks significantly decreases the stability of GCNs when being applied to security-critical applications. Defense methods such as preprocessing, attention mechanism and adversarial training have been discussed by various studies. While being able to achieve desirable performance when the perturbation rates are low, such methods are still vulnerable to high perturbation rates. Meanwhile, some defending algorithms perform poorly when the node features are not visible. Therefore, in this paper, we propose a novel mechanism called influence mechanism, which is able to enhance the robustness of the GCNs significantly. The influence mechanism divides the effect of each node into two parts: introverted influence which tries to maintain its own features and extroverted influence which exerts influences on other nodes. Utilizing the influence mechanism, we propose the Influence GCN (I-GCN) model. Extensive experiments show that our proposed model is able to achieve higher accuracy rates than state-of-the-art methods when defending against non-targeted attacks.

</details>

<details>

<summary>2020-12-11 16:17:17 - Defending Against Neural Fake News</summary>

- *Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi*

- `1905.12616v3` - [abs](http://arxiv.org/abs/1905.12616v3) - [pdf](http://arxiv.org/pdf/1905.12616v3)

> Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.   Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.   Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.

</details>

<details>

<summary>2020-12-12 17:21:00 - Adversarial Attacks and Defenses on Graphs: A Review, A Tool and Empirical Studies</summary>

- *Wei Jin, Yaxin Li, Han Xu, Yiqi Wang, Shuiwang Ji, Charu Aggarwal, Jiliang Tang*

- `2003.00653v3` - [abs](http://arxiv.org/abs/2003.00653v3) - [pdf](http://arxiv.org/pdf/2003.00653v3)

> Deep neural networks (DNNs) have achieved significant performance in various tasks. However, recent studies have shown that DNNs can be easily fooled by small perturbation on the input, called adversarial attacks. As the extensions of DNNs to graphs, Graph Neural Networks (GNNs) have been demonstrated to inherit this vulnerability. Adversary can mislead GNNs to give wrong predictions by modifying the graph structure such as manipulating a few edges. This vulnerability has arisen tremendous concerns for adapting GNNs in safety-critical applications and has attracted increasing research attention in recent years. Thus, it is necessary and timely to provide a comprehensive overview of existing graph adversarial attacks and the countermeasures. In this survey, we categorize existing attacks and defenses, and review the corresponding state-of-the-art methods. Furthermore, we have developed a repository with representative algorithms (https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph). The repository enables us to conduct empirical studies to deepen our understandings on attacks and defenses on graphs.

</details>

<details>

<summary>2020-12-12 17:54:01 - Normalized Label Distribution: Towards Learning Calibrated, Adaptable and Efficient Activation Maps</summary>

- *Utkarsh Uppal, Bharat Giddwani*

- `2012.06876v1` - [abs](http://arxiv.org/abs/2012.06876v1) - [pdf](http://arxiv.org/pdf/2012.06876v1)

> The vulnerability of models to data aberrations and adversarial attacks influences their ability to demarcate distinct class boundaries efficiently. The network's confidence and uncertainty play a pivotal role in weight adjustments and the extent of acknowledging such attacks. In this paper, we address the trade-off between the accuracy and calibration potential of a classification network. We study the significance of ground-truth distribution changes on the performance and generalizability of various state-of-the-art networks and compare the proposed method's response to unanticipated attacks. Furthermore, we demonstrate the role of label-smoothing regularization and normalization in yielding better generalizability and calibrated probability distribution by proposing normalized soft labels to enhance the calibration of feature maps. Subsequently, we substantiate our inference by translating conventional convolutions to padding based partial convolution to establish the tangible impact of corrections in reinforcing the performance and convergence rate. We graphically elucidate the implication of such variations with the critical purpose of corroborating the reliability and reproducibility for multiple datasets.

</details>

<details>

<summary>2020-12-13 11:07:30 - Threat Detection and Investigation with System-level Provenance Graphs: A Survey</summary>

- *Zhenyuan Li, Qi Alfred Chen, Runqing Yang, Yan Chen*

- `2006.01722v3` - [abs](http://arxiv.org/abs/2006.01722v3) - [pdf](http://arxiv.org/pdf/2006.01722v3)

> With the development of information technology, the border of the cyberspace gets much broader, exposing more and more vulnerabilities to attackers. Traditional mitigation-based defence strategies are challenging to cope with the current complicated situation. Security practitioners urgently need better tools to describe and modelling attacks for defence.   The provenance graph seems like an ideal method for threat modelling with powerful semantic expression ability and attacks historic correlation ability. In this paper, we firstly introduce the basic concepts about system-level provenance graph and proposed typical system architecture for provenance graph-based threat detection and investigation. A comprehensive provenance graph-based threat detection system can be divided into three modules, namely, "data collection module", "data management module", and "threat detection modules". Each module contains several components and involves many research problem. We systematically analyzed the algorithms and design details involved. By comparison, we give the strategy of technology selection. Moreover, we pointed out the shortcomings of the existing work for future improvement.

</details>

<details>

<summary>2020-12-14 16:40:53 - Improving Adversarial Robustness via Probabilistically Compact Loss with Logit Constraints</summary>

- *Xin Li, Xiangrui Li, Deng Pan, Dongxiao Zhu*

- `2012.07688v1` - [abs](http://arxiv.org/abs/2012.07688v1) - [pdf](http://arxiv.org/pdf/2012.07688v1)

> Convolutional neural networks (CNNs) have achieved state-of-the-art performance on various tasks in computer vision. However, recent studies demonstrate that these models are vulnerable to carefully crafted adversarial samples and suffer from a significant performance drop when predicting them. Many methods have been proposed to improve adversarial robustness (e.g., adversarial training and new loss functions to learn adversarially robust feature representations). Here we offer a unique insight into the predictive behavior of CNNs that they tend to misclassify adversarial samples into the most probable false classes. This inspires us to propose a new Probabilistically Compact (PC) loss with logit constraints which can be used as a drop-in replacement for cross-entropy (CE) loss to improve CNN's adversarial robustness. Specifically, PC loss enlarges the probability gaps between true class and false classes meanwhile the logit constraints prevent the gaps from being melted by a small perturbation. We extensively compare our method with the state-of-the-art using large scale datasets under both white-box and black-box attacks to demonstrate its effectiveness. The source codes are available from the following url: https://github.com/xinli0928/PC-LC.

</details>

<details>

<summary>2020-12-14 16:53:07 - MPro: Combining Static and Symbolic Analysis for Scalable Testing of Smart Contract</summary>

- *William Zhang, Sebastian Banescu, Leonardo Passos, Steven Stewart, Vijay Ganesh*

- `1911.00570v3` - [abs](http://arxiv.org/abs/1911.00570v3) - [pdf](http://arxiv.org/pdf/1911.00570v3)

> Smart contracts are executable programs that enable the building of a programmable trust mechanism between multiple entities without the need of a trusted third-party. Researchers have developed several security scanners in the past couple of years. However, many of these analyzers either do not scale well, or if they do, produce many false positives. This issue is exacerbated when bugs are triggered only after a series of interactions with the functions of the contract-under-test. A depth-n vulnerability, refers to a vulnerability that requires invoking a specific sequence of n functions to trigger. Depth-n vulnerabilities are time-consuming to detect by existing automated analyzers, because of the combinatorial explosion of sequences of functions that could be executed on smart contracts.   In this paper, we present a technique to analyze depth-n vulnerabilities in an efficient and scalable way by combining symbolic execution and data dependency analysis. A significant advantage of combining symbolic with static analysis is that it scales much better than symbolic alone and does not have the problem of false positive that static analysis tools typically have. We have implemented our technique in a tool called MPro, a scalable and automated smart contract analyzer based on the existing symbolic analysis tool Mythril-Classic and the static analysis tool Slither. We analyzed 100 randomly chosen smart contracts on MPro and our evaluation shows that MPro is about n-times faster than Mythril-Classic for detecting depth-n vulnerabilities, while preserving all the detection capabilities of Mythril-Classic.

</details>

<details>

<summary>2020-12-14 20:00:40 - When Physical Unclonable Function Meets Biometrics</summary>

- *Kavya Dayananda, Nima Karimian*

- `2012.07916v1` - [abs](http://arxiv.org/abs/2012.07916v1) - [pdf](http://arxiv.org/pdf/2012.07916v1)

> As the Covid-19 pandemic grips the world, healthcare systems are being reshaped, where the e-health concepts become more likely to be accepted. Wearable devices often carry sensitive information from users which are exposed to security and privacy risks. Moreover, users have always had the concern of being counterfeited between the fabrication process and vendors' storage. Hence, not only securing personal data is becoming a crucial obligation, but also device verification is another challenge. To address biometrics authentication and physically unclonable functions (PUFs) need to be put in place to mitigate the security and privacy of the users. Among biometrics modalities, Electrocardiogram (ECG) based biometric has become popular as it can authenticate patients and monitor the patient's vital signs. However, researchers have recently started to study the vulnerabilities of the ECG biometric systems and tried to address the issues of spoofing. Moreover, most of the wearable is enabled with CPU and memories. Thus, volatile memory-based (NVM) PUF can be easily placed in the device to avoid counterfeit. However, many research challenged the unclonability characteristics of PUFs. Thus, a careful study on these attacks should be sufficient to address the need. In this paper, our aim is to provide a comprehensive study on the state-of-the-art developments papers based on biometrics enabled hardware security.

</details>

<details>

<summary>2020-12-14 21:19:37 - Holes in the Geofence: Privacy Vulnerabilities in "Smart" DNS Services</summary>

- *Rahel A. Fainchtein, Adam J. Aviv, Micah Sherr, Stephen Ribaudo, Armaan Khullar*

- `2012.07944v1` - [abs](http://arxiv.org/abs/2012.07944v1) - [pdf](http://arxiv.org/pdf/2012.07944v1)

> Smart DNS (SDNS) services advertise access to "geofenced" content (typically, video streaming sites such as Netflix or Hulu) that is normally inaccessible unless the client is within a prescribed geographic region. SDNS is simple to use and involves no software installation. Instead, it requires only that users modify their DNS settings to point to an SDNS resolver. The SDNS resolver "smartly" identifies geofenced domains and, in lieu of their proper DNS resolutions, returns IP addresses of proxy servers located within the geofence. These servers then transparently proxy traffic between the users and their intended destinations, allowing for the bypass of these geographic restrictions.   This paper presents the first academic study of SDNS services. We identify a number of serious and pervasive privacy vulnerabilities that expose information about the users of these systems. These include architectural weaknesses that enable content providers to identify which requesting clients use SDNS. Worse, we identify flaws in the design of some SDNS services that allow {\em any} arbitrary third party to enumerate these services' users (by IP address), even if said users are currently offline. We present mitigation strategies to these attacks that have been adopted by at least one SDNS provider in response to our findings.

</details>

<details>

<summary>2020-12-14 22:54:53 - Binary Black-box Evasion Attacks Against Deep Learning-based Static Malware Detectors with Adversarial Byte-Level Language Model</summary>

- *Mohammadreza Ebrahimi, Ning Zhang, James Hu, Muhammad Taqi Raza, Hsinchun Chen*

- `2012.07994v1` - [abs](http://arxiv.org/abs/2012.07994v1) - [pdf](http://arxiv.org/pdf/2012.07994v1)

> Anti-malware engines are the first line of defense against malicious software. While widely used, feature engineering-based anti-malware engines are vulnerable to unseen (zero-day) attacks. Recently, deep learning-based static anti-malware detectors have achieved success in identifying unseen attacks without requiring feature engineering and dynamic analysis. However, these detectors are susceptible to malware variants with slight perturbations, known as adversarial examples. Generating effective adversarial examples is useful to reveal the vulnerabilities of such systems. Current methods for launching such attacks require accessing either the specifications of the targeted anti-malware model, the confidence score of the anti-malware response, or dynamic malware analysis, which are either unrealistic or expensive. We propose MalRNN, a novel deep learning-based approach to automatically generate evasive malware variants without any of these restrictions. Our approach features an adversarial example generation process, which learns a language model via a generative sequence-to-sequence recurrent neural network to augment malware binaries. MalRNN effectively evades three recent deep learning-based malware detectors and outperforms current benchmark methods. Findings from applying our MalRNN on a real dataset with eight malware categories are discussed.

</details>

<details>

<summary>2020-12-15 02:58:16 - A Quantitative Study of Security Bug Fixes of GitHub Repositories</summary>

- *Daito Nakano, Mingyang Yin, Ryosuke Sato, Abram Hindle, Yasutaka Kamei, Naoyasu Ubayashi*

- `2012.08053v1` - [abs](http://arxiv.org/abs/2012.08053v1) - [pdf](http://arxiv.org/pdf/2012.08053v1)

> Software is prone to bugs and failures. Security bugs are those that expose or share privileged information and access in violation of the software's requirements. Given the seriousness of security bugs, there are centralized mechanisms for supporting and tracking these bugs across multiple products, one such mechanism is the Common Vulnerabilities and Exposures (CVE) ID description. When a bug gets a CVE, it is referenced by its CVE ID. Thus we explore thousands of Free/Libre Open Source Software (FLOSS) projects, on Github, to determine if developers reference or discuss CVEs in their code, commits, and issues. CVEs will often refer to 3rd party software dependencies of a project and thus the bug will not be in the actual product itself. We study how many of these references are intentional CVE references, and how many are relevant bugs within the projects themselves. We investigate how the bugs that reference CVEs are fixed and how long it takes to fix these bugs. The results of our manual classification for 250 bug reports show that 88 (35%), 32 (13%), and 130 (52%) are classified into "Version Update", "Fixing Code", and "Discussion". To understand how long it takes to fix those bugs, we compare two periods, Reporting Period, a period between the disclosure date of vulnerability information in CVE repositories and the creation date of the bug report in a project, and Fixing Period, a period between the creation date of the bug report and the fixing date of the bug report. We find that 44% of bug reports that are classified into "Version Update" or "Fixing Code" have longer Reporting Period than Fixing Period. This suggests that those who submit CVEs should notify affected projects more directly.

</details>

<details>

<summary>2020-12-15 05:19:54 - FAWA: Fast Adversarial Watermark Attack on Optical Character Recognition (OCR) Systems</summary>

- *Lu Chen, Jiao Sun, Wei Xu*

- `2012.08096v1` - [abs](http://arxiv.org/abs/2012.08096v1) - [pdf](http://arxiv.org/pdf/2012.08096v1)

> Deep neural networks (DNNs) significantly improved the accuracy of optical character recognition (OCR) and inspired many important applications. Unfortunately, OCRs also inherit the vulnerabilities of DNNs under adversarial examples. Different from colorful vanilla images, text images usually have clear backgrounds. Adversarial examples generated by most existing adversarial attacks are unnatural and pollute the background severely. To address this issue, we propose the Fast Adversarial Watermark Attack (FAWA) against sequence-based OCR models in the white-box manner. By disguising the perturbations as watermarks, we can make the resulting adversarial images appear natural to human eyes and achieve a perfect attack success rate. FAWA works with either gradient-based or optimization-based perturbation generation. In both letter-level and word-level attacks, our experiments show that in addition to natural appearance, FAWA achieves a 100% attack success rate with 60% less perturbations and 78% fewer iterations on average. In addition, we further extend FAWA to support full-color watermarks, other languages, and even the OCR accuracy-enhancing mechanism.

</details>

<details>

<summary>2020-12-15 17:05:25 - Efficient Oblivious Database Joins</summary>

- *Simeon Krastnikov, Florian Kerschbaum, Douglas Stebila*

- `2003.09481v3` - [abs](http://arxiv.org/abs/2003.09481v3) - [pdf](http://arxiv.org/pdf/2003.09481v3)

> A major algorithmic challenge in designing applications intended for secure remote execution is ensuring that they are oblivious to their inputs, in the sense that their memory access patterns do not leak sensitive information to the server. This problem is particularly relevant to cloud databases that wish to allow queries over the client's encrypted data. One of the major obstacles to such a goal is the join operator, which is non-trivial to implement obliviously without resorting to generic but inefficient solutions like Oblivious RAM (ORAM).   We present an oblivious algorithm for equi-joins which (up to a logarithmic factor) matches the optimal $O(n\log n)$ complexity of the standard non-secure sort-merge join (on inputs producing $O(n)$ outputs). We do not use use expensive primitives like ORAM or rely on unrealistic hardware or security assumptions. Our approach, which is based on sorting networks and novel provably-oblivious constructions, is conceptually simple, easily verifiable, and very efficient in practice. Its data-independent algorithmic structure makes it secure in various different settings for remote computation, even in those that are known to be vulnerable to certain side-channel attacks (such as Intel SGX) or with strict requirements for low circuit complexity (like secure multiparty computation). We confirm that our approach is easily realizable through a compact implementation which matches our expectations for performance and is shown, both formally and empirically, to possess the desired security characteristics.

</details>

<details>

<summary>2020-12-15 19:12:23 - Intrusion detection in computer systems by using artificial neural networks with Deep Learning approaches</summary>

- *Sergio Hidalgo-Espinoza, Kevin Chamorro-Cupueran, Oscar Chang-Tortolero*

- `2012.08559v1` - [abs](http://arxiv.org/abs/2012.08559v1) - [pdf](http://arxiv.org/pdf/2012.08559v1)

> Intrusion detection into computer networks has become one of the most important issues in cybersecurity. Attackers keep on researching and coding to discover new vulnerabilities to penetrate information security system. In consequence computer systems must be daily upgraded using up-to-date techniques to keep hackers at bay. This paper focuses on the design and implementation of an intrusion detection system based on Deep Learning architectures. As a first step, a shallow network is trained with labelled log-in [into a computer network] data taken from the Dataset CICIDS2017. The internal behaviour of this network is carefully tracked and tuned by using plotting and exploring codes until it reaches a functional peak in intrusion prediction accuracy. As a second step, an autoencoder, trained with big unlabelled data, is used as a middle processor which feeds compressed information and abstract representation to the original shallow network. It is proven that the resultant deep architecture has a better performance than any version of the shallow network alone. The resultant functional code scripts, written in MATLAB, represent a re-trainable system which has been proved using real data, producing good precision and fast response.

</details>

<details>

<summary>2020-12-16 04:37:28 - An adaptive algorithm for embedding information into compressed JPEG images using the QIM method</summary>

- *Anna Melman, Pavel Petrov, Alexander Shelupanov*

- `2012.08742v1` - [abs](http://arxiv.org/abs/2012.08742v1) - [pdf](http://arxiv.org/pdf/2012.08742v1)

> The widespread use of JPEG images makes them good covers for secret messages storing and transmitting. This paper proposes a new algorithm for embedding information in JPEG images based on the steganographic QIM method. The main problem of such embedding is the vulnerability to statistical steganalysis. To solve this problem, it is proposed to use a variable quantization step, which is adaptively selected for each block of the JPEG cover image. Experimental results show that the proposed approach successfully increases the security of embedding.

</details>

<details>

<summary>2020-12-16 09:19:34 - Investigating the Ecosystem of Offensive Information Security Tools</summary>

- *Simon D Duque Anton, Daniel Fraunholz, Daniel Schneider*

- `2012.08811v1` - [abs](http://arxiv.org/abs/2012.08811v1) - [pdf](http://arxiv.org/pdf/2012.08811v1)

> The internet landscape is growing and at the same time becoming more heterogeneous. Services are performed via computers and networks, critical data is stored digitally. This enables freedom for the user, and flexibility for operators. Data is easier to manage and distribute. However, every device connected to a network is potentially susceptible to cyber attacks. Security solutions, such as antivirus software or firewalls, are widely established. However, certain types of attacks cannot be prevented with defensive measures alone. Offensive security describes the practice of security professionals using methods and tools of cyber criminals. This allows them to find vulnerabilities before they become the point of entry in a real attack. Furthermore, following the methods of cyber criminals enables security professionals to adapt to a criminal's point of view and potentially discover attack angles formerly ignored. As cyber criminals often employ freely available security tools, having knowledge about these provides additional insight for professionals. This work categorises and compares tools regarding metrics concerning maintainability, usability and technical details. Generally, several well-established tools are available for the first phases, while phases after the initial breach lack a variety of tools.

</details>

<details>

<summary>2020-12-16 10:17:53 - A Hybrid Graph Neural Network Approach for Detecting PHP Vulnerabilities</summary>

- *Rishi Rabheru, Hazim Hanif, Sergio Maffeis*

- `2012.08835v1` - [abs](http://arxiv.org/abs/2012.08835v1) - [pdf](http://arxiv.org/pdf/2012.08835v1)

> This paper presents DeepTective, a deep learning approach to detect vulnerabilities in PHP source code. Our approach implements a novel hybrid technique that combines Gated Recurrent Units and Graph Convolutional Networks to detect SQLi, XSS and OSCI vulnerabilities leveraging both syntactic and semantic information. We evaluate DeepTective and compare it to the state of the art on an established synthetic dataset and on a novel real-world dataset collected from GitHub. Experimental results show that DeepTective achieves near perfect classification on the synthetic dataset, and an F1 score of 88.12% on the realistic dataset, outperforming related approaches. We validate DeepTective in the wild by discovering 4 novel vulnerabilities in established WordPress plugins.

</details>

<details>

<summary>2020-12-16 16:15:58 - Backdoor Attacks on Federated Meta-Learning</summary>

- *Chien-Lun Chen, Leana Golubchik, Marco Paolieri*

- `2006.07026v2` - [abs](http://arxiv.org/abs/2006.07026v2) - [pdf](http://arxiv.org/pdf/2006.07026v2)

> Federated learning allows multiple users to collaboratively train a shared classification model while preserving data privacy. This approach, where model updates are aggregated by a central server, was shown to be vulnerable to poisoning backdoor attacks: a malicious user can alter the shared model to arbitrarily classify specific inputs from a given class. In this paper, we analyze the effects of backdoor attacks on federated meta-learning, where users train a model that can be adapted to different sets of output classes using only a few examples. While the ability to adapt could, in principle, make federated learning frameworks more robust to backdoor attacks (when new training examples are benign), we find that even 1-shot~attacks can be very successful and persist after additional training. To address these vulnerabilities, we propose a defense mechanism inspired by matching networks, where the class of an input is predicted from the similarity of its features with a support set of labeled examples. By removing the decision logic from the model shared with the federation, success and persistence of backdoor attacks are greatly reduced.

</details>

<details>

<summary>2020-12-16 16:34:11 - Optimized Random Forest Model for Botnet Detection Based on DNS Queries</summary>

- *Abdallah Moubayed, MohammadNoor Injadat, Abdallah Shami*

- `2012.11326v1` - [abs](http://arxiv.org/abs/2012.11326v1) - [pdf](http://arxiv.org/pdf/2012.11326v1)

> The Domain Name System (DNS) protocol plays a major role in today's Internet as it translates between website names and corresponding IP addresses. However, due to the lack of processes for data integrity and origin authentication, the DNS protocol has several security vulnerabilities. This often leads to a variety of cyber-attacks, including botnet network attacks. One promising solution to detect DNS-based botnet attacks is adopting machine learning (ML) based solutions. To that end, this paper proposes a novel optimized ML-based framework to detect botnets based on their corresponding DNS queries. More specifically, the framework consists of using information gain as a feature selection method and genetic algorithm (GA) as a hyper-parameter optimization model to tune the parameters of a random forest (RF) classifier. The proposed framework is evaluated using a state-of-the-art TI-2016 DNS dataset. Experimental results show that the proposed optimized framework reduced the feature set size by up to 60%. Moreover, it achieved a high detection accuracy, precision, recall, and F-score compared to the default classifier. This highlights the effectiveness and robustness of the proposed framework in detecting botnet attacks.

</details>

<details>

<summary>2020-12-16 16:39:55 - Detecting Botnet Attacks in IoT Environments: An Optimized Machine Learning Approach</summary>

- *MohammadNoor Injadat, Abdallah Moubayed, Abdallah Shami*

- `2012.11325v1` - [abs](http://arxiv.org/abs/2012.11325v1) - [pdf](http://arxiv.org/pdf/2012.11325v1)

> The increased reliance on the Internet and the corresponding surge in connectivity demand has led to a significant growth in Internet-of-Things (IoT) devices. The continued deployment of IoT devices has in turn led to an increase in network attacks due to the larger number of potential attack surfaces as illustrated by the recent reports that IoT malware attacks increased by 215.7% from 10.3 million in 2017 to 32.7 million in 2018. This illustrates the increased vulnerability and susceptibility of IoT devices and networks. Therefore, there is a need for proper effective and efficient attack detection and mitigation techniques in such environments. Machine learning (ML) has emerged as one potential solution due to the abundance of data generated and available for IoT devices and networks. Hence, they have significant potential to be adopted for intrusion detection for IoT environments. To that end, this paper proposes an optimized ML-based framework consisting of a combination of Bayesian optimization Gaussian Process (BO-GP) algorithm and decision tree (DT) classification model to detect attacks on IoT devices in an effective and efficient manner. The performance of the proposed framework is evaluated using the Bot-IoT-2018 dataset. Experimental results show that the proposed optimized framework has a high detection accuracy, precision, recall, and F-score, highlighting its effectiveness and robustness for the detection of botnet attacks in IoT environments.

</details>

<details>

<summary>2020-12-16 21:38:56 - Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without Catastrophic Forgetting</summary>

- *Craig Atkinson, Brendan McCane, Lech Szymanski, Anthony Robins*

- `1812.02464v6` - [abs](http://arxiv.org/abs/1812.02464v6) - [pdf](http://arxiv.org/pdf/1812.02464v6)

> Neural networks can achieve excellent results in a wide variety of applications. However, when they attempt to sequentially learn, they tend to learn the new task while catastrophically forgetting previous ones. We propose a model that overcomes catastrophic forgetting in sequential reinforcement learning by combining ideas from continual learning in both the image classification domain and the reinforcement learning domain. This model features a dual memory system which separates continual learning from reinforcement learning and a pseudo-rehearsal system that "recalls" items representative of previous tasks via a deep generative network. Our model sequentially learns Atari 2600 games without demonstrating catastrophic forgetting and continues to perform above human level on all three games. This result is achieved without: demanding additional storage requirements as the number of tasks increases, storing raw data or revisiting past tasks. In comparison, previous state-of-the-art solutions are substantially more vulnerable to forgetting on these complex deep reinforcement learning tasks.

</details>

<details>

<summary>2020-12-17 07:19:03 - Adversarial Defense via Local Flatness Regularization</summary>

- *Jia Xu, Yiming Li, Yong Jiang, Shu-Tao Xia*

- `1910.12165v4` - [abs](http://arxiv.org/abs/1910.12165v4) - [pdf](http://arxiv.org/pdf/1910.12165v4)

> Adversarial defense is a popular and important research area. Due to its intrinsic mechanism, one of the most straightforward and effective ways of defending attacks is to analyze the property of loss surface in the input space. In this paper, we define the local flatness of the loss surface as the maximum value of the chosen norm of the gradient regarding to the input within a neighborhood centered on the benign sample, and discuss the relationship between the local flatness and adversarial vulnerability. Based on the analysis, we propose a novel defense approach via regularizing the local flatness, dubbed local flatness regularization (LFR). We also demonstrate the effectiveness of the proposed method from other perspectives, such as human visual mechanism, and analyze the relationship between LFR and other related methods theoretically. Experiments are conducted to verify our theory and demonstrate the superiority of the proposed method.

</details>

<details>

<summary>2020-12-17 22:40:04 - Understanding The Top 10 OWASP Vulnerabilities</summary>

- *Matthew Bach-Nutman*

- `2012.09960v1` - [abs](http://arxiv.org/abs/2012.09960v1) - [pdf](http://arxiv.org/pdf/2012.09960v1)

> Understanding the common vulnerabilities in web applications help businesses be better prepared in protecting their data against such attacks. With the knowledge gained from research users and developers can be better equipped to deal with the most common attacks and form solutions to prevent future attacks against their web applications. Vulnerabilities exist in many forms within modern web applications which can be easily mitigated with investment of time and research.

</details>

<details>

<summary>2020-12-18 00:31:01 - Effectiveness of SCADA System Security Used Within Critical Infrastructure</summary>

- *Joshua Taylor*

- `2012.11375v1` - [abs](http://arxiv.org/abs/2012.11375v1) - [pdf](http://arxiv.org/pdf/2012.11375v1)

> Since the 1960s Supervisory Control and Data Acquisition (SCADA) systems have been used within industry. Referred to as critical infrastructure (CI), key installations such as power stations, water treatment and energy grids are controlled using SCADA. Existing literature reveals inherent security risks to CI and suggests this stems from the rise of interconnected networks, leading to the hypothesis that the rise of interconnectivity between corporate networks and SCADA system networks pose security risks to CI. The results from studies into previous global attacks involving SCADA and CI, with focus on two highly serious incidents in Iran and Ukraine, reveal that although interconnectivity is a major factor, isolated CIs are still highly vulnerable to attack due to risks within the SCADA controllers and protocols.

</details>

<details>

<summary>2020-12-18 13:50:17 - AdvExpander: Generating Natural Language Adversarial Examples by Expanding Text</summary>

- *Zhihong Shao, Zitao Liu, Jiyong Zhang, Zhongqin Wu, Minlie Huang*

- `2012.10235v1` - [abs](http://arxiv.org/abs/2012.10235v1) - [pdf](http://arxiv.org/pdf/2012.10235v1)

> Adversarial examples are vital to expose the vulnerability of machine learning models. Despite the success of the most popular substitution-based methods which substitutes some characters or words in the original examples, only substitution is insufficient to uncover all robustness issues of models. In this paper, we present AdvExpander, a method that crafts new adversarial examples by expanding text, which is complementary to previous substitution-based methods. We first utilize linguistic rules to determine which constituents to expand and what types of modifiers to expand with. We then expand each constituent by inserting an adversarial modifier searched from a CVAE-based generative model which is pre-trained on a large scale corpus. To search adversarial modifiers, we directly search adversarial latent codes in the latent space without tuning the pre-trained parameters. To ensure that our adversarial examples are label-preserving for text matching, we also constrain the modifications with a heuristic rule. Experiments on three classification tasks verify the effectiveness of AdvExpander and the validity of our adversarial examples. AdvExpander crafts a new type of adversarial examples by text expansion, thereby promising to reveal new robustness issues.

</details>

<details>

<summary>2020-12-19 13:12:08 - Blockchain-Based Approach for Securing Spectrum Trading in Multibeam Satellite Systems</summary>

- *Feng Li, Kwok-Yan Lam, Min Jia, Jun Zhao, Xiuhua Li, Li Wang*

- `2012.10681v1` - [abs](http://arxiv.org/abs/2012.10681v1) - [pdf](http://arxiv.org/pdf/2012.10681v1)

> This paper presents a blockchain-based approach for securing spectrum sharing in multi-beam satellite systems. Satellite spectrum is a scarce resource that requires highly efficient management schemes for optimized sharing by network users. However, spectrum sharing is vulnerable to attacks by malicious protocol participants. In order to ensure efficient spectrum management in the face of dishonest satellite users or cyber attackers, it is important for spectrum sharing mechanism to provide transparency and traceability of the trading process so as to enable the system to detect, and hence eliminate, unauthorized access by malicious users. We address these requirements by proposing the use of blockchain which, apart from its ability to provide transparency and traceability, ensures an immutable means for keeping track of user trading reputation. Besides, in order to address the practical constraints of heterogeneous user nodes, we also propose the use of edge computing to support users with limited computing power. In this paper, we propose a blockchain-based spectrum trading framework and, based on which, a multibeam satellite spectrum sharing algorithm for interference pricing and heterogeneous spectrum demands is devised to improve the efficiency of satellite spectrum. By leveraging on the system characteristics of blockchain, a dynamic spectrum sharing mechanism with traceability, openness and transparency for whole trading process is presented. Numerical results are also provided to evaluate the system benefits and spectrum pricing of the proposed mechanism.

</details>

<details>

<summary>2020-12-21 09:41:05 - Edge Computing in Transportation: Security Issues and Challenges</summary>

- *Nikheel Soni, Reza Malekian, Arnav Thakur*

- `2012.11206v1` - [abs](http://arxiv.org/abs/2012.11206v1) - [pdf](http://arxiv.org/pdf/2012.11206v1)

> As the amount of data that needs to be processed in real-time due to recent application developments increase, the need for a new computing paradigm is required. Edge computing resolves this issue by offloading computing resources required by intelligent transportation systems such as the Internet of Vehicles from the cloud closer to the end devices to improve performance however, it is susceptible to security issues that make the transportation systems vulnerable to attackers. In addition to this, there are security issues in transportation technologies that impact the edge computing paradigm as well. This paper presents some of the main security issues and challenges that are present in edge computing, which are Distributed Denial of Service attacks, side channel attacks, malware injection attacks and authentication and authorization attacks, how these impact intelligent transportation systems and research being done to help realize and mitigate these issues.

</details>

<details>

<summary>2020-12-21 10:07:20 - FuSeBMC: A White-Box Fuzzer for Finding Security Vulnerabilities in C Programs</summary>

- *Kaled M. Alshmrany, Rafael S. Menezes, Mikhail R. Gadelha, Lucas C. Cordeiro*

- `2012.11223v1` - [abs](http://arxiv.org/abs/2012.11223v1) - [pdf](http://arxiv.org/pdf/2012.11223v1)

> We describe and evaluate a novel white-box fuzzer for C programs named FuSeBMC, which combines fuzzing and symbolic execution, and applies Bounded Model Checking (BMC) to find security vulnerabilities in C programs. FuSeBMC explores and analyzes C programs (1) to find execution paths that lead to property violations and (2) to incrementally inject labels to guide the fuzzer and the BMC engine to produce test-cases for code coverage. FuSeBMC successfully participates in Test-Comp'21 and achieves first place in the Cover-Error category and second place in the Overall category.

</details>

<details>

<summary>2020-12-21 13:30:25 - Characterizing the Evasion Attackability of Multi-label Classifiers</summary>

- *Zhuo Yang, Yufei Han, Xiangliang Zhang*

- `2012.09427v2` - [abs](http://arxiv.org/abs/2012.09427v2) - [pdf](http://arxiv.org/pdf/2012.09427v2)

> Evasion attack in multi-label learning systems is an interesting, widely witnessed, yet rarely explored research topic. Characterizing the crucial factors determining the attackability of the multi-label adversarial threat is the key to interpret the origin of the adversarial vulnerability and to understand how to mitigate it. Our study is inspired by the theory of adversarial risk bound. We associate the attackability of a targeted multi-label classifier with the regularity of the classifier and the training data distribution. Beyond the theoretical attackability analysis, we further propose an efficient empirical attackability estimator via greedy label space exploration. It provides provably computational efficiency and approximation accuracy. Substantial experimental results on real-world datasets validate the unveiled attackability factors and the effectiveness of the proposed empirical attackability indicator

</details>

<details>

<summary>2020-12-21 18:21:09 - Privacy Interpretation of Behavioural-based Anomaly Detection Approaches</summary>

- *Muhammad Imran Khan, Simon Foley, Barry O'Sullivan*

- `2012.11541v1` - [abs](http://arxiv.org/abs/2012.11541v1) - [pdf](http://arxiv.org/pdf/2012.11541v1)

> This paper proposes the notion of 'Privacy-Anomaly Detection' and considers the question of whether behavioural-based anomaly detection approaches can have a privacy semantic interpretation and whether the detected anomalies can be related to the conventional (formal) definitions of privacy semantics such as k-anonymity. The idea is to learn the user's past querying behaviour in terms of privacy and then identifying deviations from past behaviour in order to detect privacy violations. Privacy attacks, violations of formal privacy definition, based on a sequence of SQL queries (query correlations) are also considered in the paper and it is shown that interactive querying settings are vulnerable to privacy attacks based on query sequences. Investigation on whether these types of privacy attacks can potentially manifest themselves as anomalies, specifically as privacy-anomalies was carried out. It is shown that in this paper that behavioural-based anomaly detection approaches have the potential to detect privacy attacks based on query sequences (violation of formal privacy definition) as privacy-anomalies.

</details>

<details>

<summary>2020-12-21 23:35:47 - A Neuro-Inspired Autoencoding Defense Against Adversarial Perturbations</summary>

- *Can Bakiskan, Metehan Cekic, Ahmet Dundar Sezer, Upamanyu Madhow*

- `2011.10867v2` - [abs](http://arxiv.org/abs/2011.10867v2) - [pdf](http://arxiv.org/pdf/2011.10867v2)

> Deep Neural Networks (DNNs) are vulnerable to adversarial attacks: carefully constructed perturbations to an image can seriously impair classification accuracy, while being imperceptible to humans. While there has been a significant amount of research on defending against such attacks, most defenses based on systematic design principles have been defeated by appropriately modified attacks. For a fixed set of data, the most effective current defense is to train the network using adversarially perturbed examples. In this paper, we investigate a radically different, neuro-inspired defense mechanism, starting from the observation that human vision is virtually unaffected by adversarial examples designed for machines. We aim to reject L^inf bounded adversarial perturbations before they reach a classifier DNN, using an encoder with characteristics commonly observed in biological vision: sparse overcomplete representations, randomness due to synaptic noise, and drastic nonlinearities. Encoder training is unsupervised, using standard dictionary learning. A CNN-based decoder restores the size of the encoder output to that of the original image, enabling the use of a standard CNN for classification. Our nominal design is to train the decoder and classifier together in standard supervised fashion, but we also consider unsupervised decoder training based on a regression objective (as in a conventional autoencoder) with separate supervised training of the classifier. Unlike adversarial training, all training is based on clean images.   Our experiments on the CIFAR-10 show performance competitive with state-of-the-art defenses based on adversarial training, and point to the promise of neuro-inspired techniques for the design of robust neural networks. In addition, we provide results for a subset of the Imagenet dataset to verify that our approach scales to larger images.

</details>

<details>

<summary>2020-12-23 01:17:53 - Training Robust Deep Neural Networks via Adversarial Noise Propagation</summary>

- *Aishan Liu, Xianglong Liu, Chongzhi Zhang, Hang Yu, Qiang Liu, Dacheng Tao*

- `1909.09034v2` - [abs](http://arxiv.org/abs/1909.09034v2) - [pdf](http://arxiv.org/pdf/1909.09034v2)

> In practice, deep neural networks have been found to be vulnerable to various types of noise, such as adversarial examples and corruption. Various adversarial defense methods have accordingly been developed to improve adversarial robustness for deep models. However, simply training on data mixed with adversarial examples, most of these models still fail to defend against the generalized types of noise. Motivated by the fact that hidden layers play a highly important role in maintaining a robust model, this paper proposes a simple yet powerful training algorithm, named \emph{Adversarial Noise Propagation} (ANP), which injects noise into the hidden layers in a layer-wise manner. ANP can be implemented efficiently by exploiting the nature of the backward-forward training style. Through thorough investigations, we determine that different hidden layers make different contributions to model robustness and clean accuracy, while shallow layers are comparatively more critical than deep layers. Moreover, our framework can be easily combined with other adversarial training methods to further improve model robustness by exploiting the potential of hidden layers. Extensive experiments on MNIST, CIFAR-10, CIFAR-10-C, CIFAR-10-P, and ImageNet demonstrate that ANP enables the strong robustness for deep models against both adversarial and corrupted ones, and also significantly outperforms various adversarial defense methods.

</details>

<details>

<summary>2020-12-23 05:27:53 - Relationship between manifold smoothness and adversarial vulnerability in deep learning with local errors</summary>

- *Zijian Jiang, Jianwen Zhou, Haiping Huang*

- `2007.02047v2` - [abs](http://arxiv.org/abs/2007.02047v2) - [pdf](http://arxiv.org/pdf/2007.02047v2)

> Artificial neural networks can achieve impressive performances, and even outperform humans in some specific tasks. Nevertheless, unlike biological brains, the artificial neural networks suffer from tiny perturbations in sensory input, under various kinds of adversarial attacks. It is therefore necessary to study the origin of the adversarial vulnerability. Here, we establish a fundamental relationship between geometry of hidden representations (manifold perspective) and the generalization capability of the deep networks. For this purpose, we choose a deep neural network trained by local errors, and then analyze emergent properties of trained networks through the manifold dimensionality, manifold smoothness, and the generalization capability. To explore effects of adversarial examples, we consider independent Gaussian noise attacks and fast-gradient-sign-method (FGSM) attacks. Our study reveals that a high generalization accuracy requires a relatively fast power-law decay of the eigen-spectrum of hidden representations. Under Gaussian attacks, the relationship between generalization accuracy and power-law exponent is monotonic, while a non-monotonic behavior is observed for FGSM attacks. Our empirical study provides a route towards a final mechanistic interpretation of adversarial vulnerability under adversarial attacks.

</details>

<details>

<summary>2020-12-23 15:53:14 - On the Feasibility of Load-Changing Attacks in Power Systems during the COVID-19 Pandemic</summary>

- *Juan Ospina, XiaoRui Liu, Charalambos Konstantinou, Yury Dvorkin*

- `2011.09982v2` - [abs](http://arxiv.org/abs/2011.09982v2) - [pdf](http://arxiv.org/pdf/2011.09982v2)

> The electric power grid is a complex cyberphysical energy system (CPES) in which information and communication technologies (ICT) are integrated into the operations and services of the power grid infrastructure. The growing number of Internet-of-things (IoT) high-wattage appliances, such as air conditioners and electric vehicles, being connected to the power grid, together with the high dependence of ICT and control interfaces, make CPES vulnerable to high-impact, low-probability load-changing cyberattacks. Moreover, the side-effects of the COVID-19 pandemic demonstrate a modification of electricity consumption patterns with utilities experiencing significant net-load and peak reductions. These unusual sustained low load demand conditions could be leveraged by adversaries to cause frequency instabilities in CPES by compromising hundreds of thousands of IoT-connected high-wattage loads. This paper presents a feasibility study of the impacts of load-changing attacks on CPES during the low loading conditions caused by the lockdown measures implemented during the COVID-19 pandemic. The load demand reductions caused by the lockdown measures are analyzed using dynamic mode decomposition (DMD), focusing on the March-to-July 2020 period and the New York region as the most impacted time period and location in terms of load reduction due to the lockdowns being in full execution. Our feasibility study evaluates load-changing attack scenarios using real load consumption data from the New York Independent System Operator (NYISO) and shows that an attacker with sufficient knowledge and resources could be capable of producing frequency stability problems, with frequency excursions going up to 60.5 Hz and 63.4 Hz, when no mitigation measures are taken.

</details>

<details>

<summary>2020-12-23 16:19:12 - PQFabric: A Permissioned Blockchain Secure from Both Classical and Quantum Attacks</summary>

- *Amelia Holcomb, Geovandro C. C. F. Pereira, Bhargav Das, Michele Mosca*

- `2010.06571v3` - [abs](http://arxiv.org/abs/2010.06571v3) - [pdf](http://arxiv.org/pdf/2010.06571v3)

> Hyperledger Fabric is a prominent and flexible solution for building permissioned distributed ledger platforms. Access control and identity management relies on a Membership Service Provider (MSP) whose cryptographic interface only handles standard PKI methods for authentication: RSA and ECDSA classical signatures. Also, MSP-issued credentials may use only one signature scheme, tying the credential-related functions to classical single-signature primitives. RSA and ECDSA are vulnerable to quantum attacks, with an ongoing post-quantum standardization process to identify quantum-safe drop-in replacements. In this paper, we propose a redesign of Fabric's credential-management procedures and related specifications in order to incorporate hybrid digital signatures, protecting against both classical and quantum attacks using one classical and one quantum-safe signature. We create PQFabric, an implementation of Fabric with hybrid signatures that integrates with the Open Quantum Safe (OQS) library. Our implementation offers complete crypto-agility, with the ability to perform live migration to a hybrid quantum-safe blockchain and select any existing OQS signature algorithm for each node. We perform comparative benchmarks of PQFabric with each of the NIST candidates and alternates, revealing that long public keys and signatures lead to an increase in hashing time that is sometimes comparable to the time spent signing or verifying messages itself. This is a new and potentially significant issue in the migration of blockchains to post-quantum signatures.

</details>

<details>

<summary>2020-12-24 05:07:29 - Implementation of Security Features in Software Development Phases</summary>

- *Ariessa Davaindran Lingham, Nelson Tang Kwong Kin, Chen Wan Jing, Chong Heng Loong, Fatima-tuz-Zahra*

- `2012.13108v1` - [abs](http://arxiv.org/abs/2012.13108v1) - [pdf](http://arxiv.org/pdf/2012.13108v1)

> Security holds an important role in a software. Most people are not aware of the significance of security in software system and tend to assume that they will be fine without security in their software systems. However, the lack of security features causes to expose all the vulnerabilities possible to the public. This provides opportunities for the attackers to perform dangerous activities to the vulnerable insecure systems. This is the reason why many organizations are reported for being victims of system security attacks. In order to achieve the security requirement, developers must take time to study so that they truly understand the consequences and importance of security. Hence, this paper is written to discuss how secure software development can be performed. To reach the goal of this paper, relevant researches have been reviewed. Multiple case study papers have been studied to find out the answers to how the vulnerabilities are identified, how to eliminate them, when to implement security features, why do we implement them. Finally, the paper is concluded with final remarks on implementation of security features during software development process. It is expected that this paper will be a contribution towards the aforementioned software security domain which is often ignored during practical application.

</details>

<details>

<summary>2020-12-24 05:17:21 - Exploring Adversarial Examples via Invertible Neural Networks</summary>

- *Ruqi Bai, Saurabh Bagchi, David I. Inouye*

- `2012.13111v1` - [abs](http://arxiv.org/abs/2012.13111v1) - [pdf](http://arxiv.org/pdf/2012.13111v1)

> Adversarial examples (AEs) are images that can mislead deep neural network (DNN) classifiers via introducing slight perturbations into original images. This security vulnerability has led to vast research in recent years because it can introduce real-world threats into systems that rely on neural networks. Yet, a deep understanding of the characteristics of adversarial examples has remained elusive. We propose a new way of achieving such understanding through a recent development, namely, invertible neural models with Lipschitz continuous mapping functions from the input to the output. With the ability to invert any latent representation back to its corresponding input image, we can investigate adversarial examples at a deeper level and disentangle the adversarial example's latent representation. Given this new perspective, we propose a fast latent space adversarial example generation method that could accelerate adversarial training. Moreover, this new perspective could contribute to new ways of adversarial example detection.

</details>

<details>

<summary>2020-12-24 12:53:38 - SoK: Lending Pools in Decentralized Finance</summary>

- *Massimo Bartoletti, James Hsin-yu Chiang, Alberto Lluch-Lafuente*

- `2012.13230v1` - [abs](http://arxiv.org/abs/2012.13230v1) - [pdf](http://arxiv.org/pdf/2012.13230v1)

> Lending pools are decentralized applications which allow mutually untrusted users to lend and borrow crypto-assets. These applications feature complex, highly parametric incentive mechanisms to equilibrate the loan market. This complexity makes the behaviour of lending pools difficult to understand and to predict: indeed, ineffective incentives and attacks could potentially lead to emergent unwanted behaviours. Reasoning about lending pools is made even harder by the lack of executable models of their behaviour: to precisely understand how users interact with lending pools, eventually one has to inspect their implementations, where the incentive mechanisms are intertwined with low-level implementation details. Further, the variety of existing implementations makes it difficult to distill the common aspects of lending pools. We systematize the existing knowledge about lending pools, leveraging a new formal model of interactions with users, which reflects the archetypal features of mainstream implementations. This enables us to prove some general properties of lending pools, such as the correct handling of funds, and to precisely describe vulnerabilities and attacks. We also discuss the role of lending pools in the broader context of decentralized finance.

</details>

<details>

<summary>2020-12-24 13:55:13 - Designing a Socio-Technical Business Process for Analyzing Information Quality Requirements: Experience Report</summary>

- *Mohamad Gharib*

- `2012.13254v1` - [abs](http://arxiv.org/abs/2012.13254v1) - [pdf](http://arxiv.org/pdf/2012.13254v1)

> Although many BPs involve critical activities that demand high-quality information for their successful enactment, most available BP approaches focus mainly on control-flow, and either ignore the Information Quality (IQ) perspective or consider it as a mere technical issue, instead of a social and organizational one. This leaves a BP subject to different types of social and organizational IQ vulnerabilities that may negatively impact, or even abort the BP enactment. To tackle this problem, a Socio-Technical BP (STBP), namely a Workflow-net with Actors (WFA-net) has been developed. WFA-net allows for capturing IQ requirements in their social and organizational context. This paper reports on the experience gained, findings and lessons learned while developing the WFA-net.

</details>

<details>

<summary>2020-12-24 17:48:45 - Blockchain Technology: Methodology, Application and Security Issues</summary>

- *AKM Bahalul Haque, Mahbubur Rahman*

- `2012.13366v1` - [abs](http://arxiv.org/abs/2012.13366v1) - [pdf](http://arxiv.org/pdf/2012.13366v1)

> Blockchain technology is an interlinked systematic chain of blocks that contains transaction history and other user data. It works under the principle of decentralized distributed digital ledger. This technology enables cryptographically secure and anonymous financial transactions among the user nodes of the network enabling the transactions to be validated and approved by all the users in a transparent environment. It is a revolutionary technology that earned its emerging popularity through the usage of digital cryptocurrencies. Even though Blockchain holds a promising scope of development in the online transaction system, it is prone to several security and vulnerability issues. In this paper, blockchain methodology, its applications, and security issues are discussed which might shed some light on blockchain enthusiasts and researchers.

</details>

<details>

<summary>2020-12-24 23:43:29 - Security of Connected and Automated Vehicles</summary>

- *Mashrur Chowdhury, Mhafuzul Islam, Zadid Khan*

- `2012.13464v1` - [abs](http://arxiv.org/abs/2012.13464v1) - [pdf](http://arxiv.org/pdf/2012.13464v1)

> The transportation system is rapidly evolving with new connected and automated vehicle (CAV) technologies that integrate CAVs with other vehicles and roadside infrastructure in a cyberphysical system (CPS). Through connectivity, CAVs affect their environments and vice versa, increasing the size of the cyberattack surface and the risk of exploitation of security vulnerabilities by malicious actors. Thus, greater understanding of potential CAV-CPS cyberattacks and of ways to prevent them is a high priority. In this article we describe CAV-CPS cyberattack surfaces and security vulnerabilities, and outline potential cyberattack detection and mitigation strategies. We examine emerging technologies - artificial intelligence, software-defined networks, network function virtualization, edge computing, information-centric and virtual dispersive networking, fifth generation (5G) cellular networks, blockchain technology, and quantum and postquantum cryptography - as potential solutions aiding in securing CAVs and transportation infrastructure against existing and future cyberattacks.

</details>

<details>

<summary>2020-12-25 05:32:11 - Fuzzing with Fast Failure Feedback</summary>

- *Rahul Gopinath, Bachir Bendrissou, Björn Mathis, Andreas Zeller*

- `2012.13516v1` - [abs](http://arxiv.org/abs/2012.13516v1) - [pdf](http://arxiv.org/pdf/2012.13516v1)

> Fuzzing -- testing programs with random inputs -- has become the prime technique to detect bugs and vulnerabilities in programs. To generate inputs that cover new functionality, fuzzers require execution feedback from the program -- for instance, the coverage obtained by previous inputs, or the conditions that need to be resolved to cover new branches. If such execution feedback is not available, though, fuzzing can only rely on chance, which is ineffective. In this paper, we introduce a novel fuzzing technique that relies on failure feedback only -- that is, information on whether an input is valid or not, and if not, where the error occurred. Our bFuzzer tool enumerates byte after byte of the input space and tests the program until it finds valid prefixes, and continues exploration from these prefixes. Since no instrumentation or execution feedback is required, bFuzzer is language agnostic and the required tests execute very quickly. We evaluate our technique on five subjects, and show that bFuzzer is effective and efficient even in comparison to its white-box counterpart.

</details>

<details>

<summary>2020-12-25 16:51:30 - DNS Typo-squatting Domain Detection: A Data Analytics & Machine Learning Based Approach</summary>

- *Abdallah Moubayed, MohammadNoor Injadat, Abdallah Shami, Hanan Lutfiyya*

- `2012.13604v1` - [abs](http://arxiv.org/abs/2012.13604v1) - [pdf](http://arxiv.org/pdf/2012.13604v1)

> Domain Name System (DNS) is a crucial component of current IP-based networks as it is the standard mechanism for name to IP resolution. However, due to its lack of data integrity and origin authentication processes, it is vulnerable to a variety of attacks. One such attack is Typosquatting. Detecting this attack is particularly important as it can be a threat to corporate secrets and can be used to steal information or commit fraud. In this paper, a machine learning-based approach is proposed to tackle the typosquatting vulnerability. To that end, exploratory data analytics is first used to better understand the trends observed in eight domain name-based extracted features. Furthermore, a majority voting-based ensemble learning classifier built using five classification algorithms is proposed that can detect suspicious domains with high accuracy. Moreover, the observed trends are validated by studying the same features in an unlabeled dataset using K-means clustering algorithm and through applying the developed ensemble learning classifier. Results show that legitimate domains have a smaller domain name length and fewer unique characters. Moreover, the developed ensemble learning classifier performs better in terms of accuracy, precision, and F-score. Furthermore, it is shown that similar trends are observed when clustering is used. However, the number of domains identified as potentially suspicious is high. Hence, the ensemble learning classifier is applied with results showing that the number of domains identified as potentially suspicious is reduced by almost a factor of five while still maintaining the same trends in terms of features' statistics.

</details>

<details>

<summary>2020-12-26 16:54:12 - Multimodal Safety-Critical Scenarios Generation for Decision-Making Algorithms Evaluation</summary>

- *Wenhao Ding, Baiming Chen, Bo Li, Kim Ji Eun, Ding Zhao*

- `2009.08311v3` - [abs](http://arxiv.org/abs/2009.08311v3) - [pdf](http://arxiv.org/pdf/2009.08311v3)

> Existing neural network-based autonomous systems are shown to be vulnerable against adversarial attacks, therefore sophisticated evaluation on their robustness is of great importance. However, evaluating the robustness only under the worst-case scenarios based on known attacks is not comprehensive, not to mention that some of them even rarely occur in the real world. In addition, the distribution of safety-critical data is usually multimodal, while most traditional attacks and evaluation methods focus on a single modality. To solve the above challenges, we propose a flow-based multimodal safety-critical scenario generator for evaluating decisionmaking algorithms. The proposed generative model is optimized with weighted likelihood maximization and a gradient-based sampling procedure is integrated to improve the sampling efficiency. The safety-critical scenarios are generated by querying the task algorithms and the log-likelihood of the generated scenarios is in proportion to the risk level. Experiments on a self-driving task demonstrate our advantages in terms of testing efficiency and multimodal modeling capability. We evaluate six Reinforcement Learning algorithms with our generated traffic scenarios and provide empirical conclusions about their robustness.

</details>

<details>

<summary>2020-12-27 13:15:19 - Pain Assessment based on fNIRS using Bidirectional LSTMs</summary>

- *Raul Fernandez Rojas, Julio Romero, Jehu Lopez-Aparicio, Keng-Liang Ou*

- `2012.13231v2` - [abs](http://arxiv.org/abs/2012.13231v2) - [pdf](http://arxiv.org/pdf/2012.13231v2)

> Assessing pain in patients unable to speak (also called non-verbal patients) is extremely complicated and often is done by clinical judgement. However, this method is not reliable since patients vital signs can fluctuate significantly due to other underlying medical conditions. No objective diagnosis test exists to date that can assist medical practitioners in the diagnosis of pain. In this study we propose the use of functional near-infrared spectroscopy (fNIRS) and deep learning for the assessment of human pain. The aim of this study is to explore the use deep learning to automatically learn features from fNIRS raw data to reduce the level of subjectivity and domain knowledge required in the design of hand-crafted features. Four deep learning models were evaluated, multilayer perceptron (MLP), forward and backward long short-term memory net-works (LSTM), and bidirectional LSTM. The results showed that the Bi-LSTM model achieved the highest accuracy (90.6%)and faster than the other three models. These results advance knowledge in pain assessment using neuroimaging as a method of diagnosis and represent a step closer to developing a physiologically based diagnosis of human pain that will benefit vulnerable populations who cannot self-report pain.

</details>

<details>

<summary>2020-12-28 20:57:48 - A Survey on Vulnerabilities of Ethereum Smart Contracts</summary>

- *Zulfiqar Ali Khan, Akbar Siami Namin*

- `2012.14481v1` - [abs](http://arxiv.org/abs/2012.14481v1) - [pdf](http://arxiv.org/pdf/2012.14481v1)

> Smart contract (SC) is an extension of BlockChain technology. Ethereum BlockChain was the first to incorporate SC and thus started a new era of crypto-currencies and electronic transactions. Solidity helps to program the SCs. Still, soon after Solidity's emergence in 2014, Solidity-based SCs suffered many attacks that deprived the SC account holders of their precious funds. The main reason for these attacks was the presence of vulnerabilities in SC. This paper discusses SC vulnerabilities and classifies them according to the domain knowledge of the faulty operations. This classification is a source of reminding developers and software engineers that for SC's safety, each SC requires proper testing with effective tools to catch those classes' vulnerabilities.

</details>

<details>

<summary>2020-12-30 09:41:19 - Explainability Matters: Backdoor Attacks on Medical Imaging</summary>

- *Munachiso Nwadike, Takumi Miyawaki, Esha Sarkar, Michail Maniatakos, Farah Shamout*

- `2101.00008v1` - [abs](http://arxiv.org/abs/2101.00008v1) - [pdf](http://arxiv.org/pdf/2101.00008v1)

> Deep neural networks have been shown to be vulnerable to backdoor attacks, which could be easily introduced to the training set prior to model training. Recent work has focused on investigating backdoor attacks on natural images or toy datasets. Consequently, the exact impact of backdoors is not yet fully understood in complex real-world applications, such as in medical imaging where misdiagnosis can be very costly. In this paper, we explore the impact of backdoor attacks on a multi-label disease classification task using chest radiography, with the assumption that the attacker can manipulate the training dataset to execute the attack. Extensive evaluation of a state-of-the-art architecture demonstrates that by introducing images with few-pixel perturbations into the training set, an attacker can execute the backdoor successfully without having to be involved with the training procedure. A simple 3$\times$3 pixel trigger can achieve up to 1.00 Area Under the Receiver Operating Characteristic (AUROC) curve on the set of infected images. In the set of clean images, the backdoored neural network could still achieve up to 0.85 AUROC, highlighting the stealthiness of the attack. As the use of deep learning based diagnostic systems proliferates in clinical practice, we also show how explainability is indispensable in this context, as it can identify spatially localized backdoors in inference time.

</details>

<details>

<summary>2020-12-30 11:24:44 - Stack-based Buffer Overflow Detection using Recurrent Neural Networks</summary>

- *William Arild Dahl, Laszlo Erdodi, Fabio Massimo Zennaro*

- `2012.15116v1` - [abs](http://arxiv.org/abs/2012.15116v1) - [pdf](http://arxiv.org/pdf/2012.15116v1)

> Detecting vulnerabilities in software is a critical challenge in the development and deployment of applications. One of the most known and dangerous vulnerabilities is stack-based buffer overflows, which may allow potential attackers to execute malicious code. In this paper we consider the use of modern machine learning models, specifically recurrent neural networks, to detect stack-based buffer overflow vulnerabilities in the assembly code of a program. Since assembly code is a generic and common representation, focusing on this language allows us to potentially consider programs written in several different programming languages. Moreover, we subscribe to the hypothesis that code may be treated as natural language, and thus we process assembly code using standard architectures commonly employed in natural language processing. We perform a set of experiments aimed at confirming the validity of the natural language hypothesis and the feasibility of using recurrent neural networks for detecting vulnerabilities. Our results show that our architecture is able to capture subtle stack-based buffer overflow vulnerabilities that strongly depend on the context, thus suggesting that this approach may be extended to real-world setting, as well as to other forms of vulnerability detection.

</details>

<details>

<summary>2020-12-31 03:12:46 - Heterogeneous recovery from large scale power failures</summary>

- *Amir Hossein Afsharinejad, Chuanyi Ji, Robert Wilcox*

- `2012.15420v1` - [abs](http://arxiv.org/abs/2012.15420v1) - [pdf](http://arxiv.org/pdf/2012.15420v1)

> Large-scale power failures are induced by nearly all natural disasters from hurricanes to wild fires. A fundamental problem is whether and how recovery guided by government policies is able to meet the challenge of a wide range of disruptions. Prior research on this problem is scant due to lack of sharing large-scale granular data at the operational energy grid, stigma of revealing limitations of services, and complex recovery coupled with policies and customers. As such, both quantification and firsthand information are lacking on capabilities and fundamental limitation of energy services in response to extreme events. Furthermore, government policies that guide recovery are often sidelined by prior study. This work studies the fundamental problem through the lens of recovery guided by two commonly adopted policies. We develop data analysis on unsupervised learning from non-stationary data. The data span failure events, from moderate to extreme, at the operational distribution grid during the past nine years in two service regions at the state of New York and Massachusetts. We show that under the prioritization policy favoring large failures, recovery exhibits a surprising scaling property which counteracts failure scaling on the infrastructure vulnerability. However, heterogeneous recovery widens with the severity of failure events: large failures that cannot be prioritized increase customer interruption time by 47 folds. And, prolonged small failures dominate the entire temporal evolution of recovery.

</details>

<details>

<summary>2020-12-31 13:45:15 - Garfield: System Support for Byzantine Machine Learning</summary>

- *Rachid Guerraoui, Arsany Guirguis, Jérémy Max Plassmann, Anton Alexandre Ragot, Sébastien Rouault*

- `2010.05888v2` - [abs](http://arxiv.org/abs/2010.05888v2) - [pdf](http://arxiv.org/pdf/2010.05888v2)

> We present Garfield, a library to transparently make machine learning (ML) applications, initially built with popular (but fragile) frameworks, e.g., TensorFlow and PyTorch, Byzantine-resilient. Garfield relies on a novel object-oriented design, reducing the coding effort, and addressing the vulnerability of the shared-graph architecture followed by classical ML frameworks. Garfield encompasses various communication patterns and supports computations on CPUs and GPUs, allowing addressing the general question of the very practical cost of Byzantine resilience in SGD-based ML applications. We report on the usage of Garfield on three main ML architectures: (a) a single server with multiple workers, (b) several servers and workers, and (c) peer-to-peer settings. Using Garfield, we highlight several interesting facts about the cost of Byzantine resilience. In particular, (a) Byzantine resilience, unlike crash resilience, induces an accuracy loss, (b) the throughput overhead comes more from communication than from robust aggregation, and (c) tolerating Byzantine servers costs more than tolerating Byzantine workers.

</details>

