# 2019

## TOC

- [2019-01](#2019-01)
- [2019-02](#2019-02)
- [2019-03](#2019-03)
- [2019-04](#2019-04)
- [2019-05](#2019-05)
- [2019-06](#2019-06)
- [2019-07](#2019-07)
- [2019-08](#2019-08)
- [2019-09](#2019-09)
- [2019-10](#2019-10)
- [2019-11](#2019-11)
- [2019-12](#2019-12)

## 2019-01

<details>

<summary>2019-01-02 22:13:02 - Multi-Label Adversarial Perturbations</summary>

- *Qingquan Song, Haifeng Jin, Xiao Huang, Xia Hu*

- `1901.00546v1` - [abs](http://arxiv.org/abs/1901.00546v1) - [pdf](http://arxiv.org/pdf/1901.00546v1)

> Adversarial examples are delicately perturbed inputs, which aim to mislead machine learning models towards incorrect outputs. While most of the existing work focuses on generating adversarial perturbations in multi-class classification problems, many real-world applications fall into the multi-label setting in which one instance could be associated with more than one label. For example, a spammer may generate adversarial spams with malicious advertising while maintaining the other labels such as topic labels unchanged. To analyze the vulnerability and robustness of multi-label learning models, we investigate the generation of multi-label adversarial perturbations. This is a challenging task due to the uncertain number of positive labels associated with one instance, as well as the fact that multiple labels are usually not mutually exclusive with each other. To bridge this gap, in this paper, we propose a general attacking framework targeting on multi-label classification problem and conduct a premier analysis on the perturbations for deep neural networks. Leveraging the ranking relationships among labels, we further design a ranking-based framework to attack multi-label ranking algorithms. We specify the connection between the two proposed frameworks and separately design two specific methods grounded on each of them to generate targeted multi-label perturbations. Experiments on real-world multi-label image classification and ranking problems demonstrate the effectiveness of our proposed frameworks and provide insights of the vulnerability of multi-label deep learning models under diverse targeted attacking strategies. Several interesting findings including an unpolished defensive strategy, which could potentially enhance the interpretability and robustness of multi-label deep learning models, are further presented and discussed at the end.

</details>

<details>

<summary>2019-01-03 02:39:02 - Quantum Differential Cryptanalysis</summary>

- *Qing Zhou, Songfeng Lu, Zhigang Zhang, Jie Sun*

- `1811.09931v2` - [abs](http://arxiv.org/abs/1811.09931v2) - [pdf](http://arxiv.org/pdf/1811.09931v2)

> In this paper, we propose a quantum version of the differential cryptanalysis which offers a quadratic speedup over the existing classical one and show the quantum circuit implementing it. The quantum differential cryptanalysis is based on the quantum minimum/maximum-finding algorithm, where the values to be compared and filtered are obtained by calling the quantum counting algorithm. Any cipher which is vulnerable to the classical differential cryptanalysis based on counting procedures can be cracked more quickly under this quantum differential attack.

</details>

<details>

<summary>2019-01-03 12:29:59 - How Wrong Am I? - Studying Adversarial Examples and their Impact on Uncertainty in Gaussian Process Machine Learning Models</summary>

- *Kathrin Grosse, David Pfaff, Michael Thomas Smith, Michael Backes*

- `1711.06598v4` - [abs](http://arxiv.org/abs/1711.06598v4) - [pdf](http://arxiv.org/pdf/1711.06598v4)

> Machine learning models are vulnerable to Adversarial Examples: minor perturbations to input samples intended to deliberately cause misclassification. Current defenses against adversarial examples, especially for Deep Neural Networks (DNN), are primarily derived from empirical developments, and their security guarantees are often only justified retroactively. Many defenses therefore rely on hidden assumptions that are subsequently subverted by increasingly elaborate attacks. This is not surprising: deep learning notoriously lacks a comprehensive mathematical framework to provide meaningful guarantees.   In this paper, we leverage Gaussian Processes to investigate adversarial examples in the framework of Bayesian inference. Across different models and datasets, we find deviating levels of uncertainty reflect the perturbation introduced to benign samples by state-of-the-art attacks, including novel white-box attacks on Gaussian Processes. Our experiments demonstrate that even unoptimized uncertainty thresholds already reject adversarial examples in many scenarios.   Comment: Thresholds can be broken in a modified attack, which was done in arXiv:1812.02606 (The limitations of model uncertainty in adversarial settings).

</details>

<details>

<summary>2019-01-04 12:38:33 - Towards Automated Network Mitigation Analysis (extended)</summary>

- *Patrick Speicher, Marcel Steinmetz, Jörg Hoffmann, Michael Backes, Robert Künnemann*

- `1705.05088v2` - [abs](http://arxiv.org/abs/1705.05088v2) - [pdf](http://arxiv.org/pdf/1705.05088v2)

> Penetration testing is a well-established practical concept for the identification of potentially exploitable security weaknesses and an important component of a security audit. Providing a holistic security assessment for networks consisting of several hundreds hosts is hardly feasible though without some sort of mechanization. Mitigation, prioritizing counter-measures subject to a given budget, currently lacks a solid theoretical understanding and is hence more art than science. In this work, we propose the first approach for conducting comprehensive what-if analyses in order to reason about mitigation in a conceptually well-founded manner. To evaluate and compare mitigation strategies, we use simulated penetration testing, i.e., automated attack-finding, based on a network model to which a subset of a given set of mitigation actions, e.g., changes to the network topology, system updates, configuration changes etc. is applied. Using Stackelberg planning, we determine optimal combinations that minimize the maximal attacker success (similar to a Stackelberg game), and thus provide a well-founded basis for a holistic mitigation strategy. We show that these Stackelberg planning models can largely be derived from network scan, public vulnerability databases and manual inspection with various degrees of automation and detail, and we simulate mitigation analysis on networks of different size and vulnerability.

</details>

<details>

<summary>2019-01-04 14:40:06 - V-Fuzz: Vulnerability-Oriented Evolutionary Fuzzing</summary>

- *Yuwei Li, Shouling Ji, Chenyang Lv, Yuan Chen, Jianhai Chen, Qinchen Gu, Chunming Wu*

- `1901.01142v1` - [abs](http://arxiv.org/abs/1901.01142v1) - [pdf](http://arxiv.org/pdf/1901.01142v1)

> Fuzzing is a technique of finding bugs by executing a software recurrently with a large number of abnormal inputs. Most of the existing fuzzers consider all parts of a software equally, and pay too much attention on how to improve the code coverage. It is inefficient as the vulnerable code only takes a tiny fraction of the entire code. In this paper, we design and implement a vulnerability-oriented evolutionary fuzzing prototype named V-Fuzz, which aims to find bugs efficiently and quickly in a limited time. V-Fuzz consists of two main components: a neural network-based vulnerability prediction model and a vulnerability-oriented evolutionary fuzzer. Given a binary program to V-Fuzz, the vulnerability prediction model will give a prior estimation on which parts of the software are more likely to be vulnerable. Then, the fuzzer leverages an evolutionary algorithm to generate inputs which tend to arrive at the vulnerable locations, guided by the vulnerability prediction result. Experimental results demonstrate that V-Fuzz can find bugs more efficiently than state-of-the-art fuzzers. Moreover, V-Fuzz has discovered 10 CVEs, and 3 of them are newly discovered. We reported the new CVEs, and they have been confirmed and fixed.

</details>

<details>

<summary>2019-01-04 22:08:39 - How Reliable is the Crowdsourced Knowledge of Security Implementation?</summary>

- *Mengsu Chen, Felix Fischer, Na Meng, Xiaoyin Wang, Jens Grossklags*

- `1901.01327v1` - [abs](http://arxiv.org/abs/1901.01327v1) - [pdf](http://arxiv.org/pdf/1901.01327v1)

> Stack Overflow (SO) is the most popular online Q&A site for developers to share their expertise in solving programming issues. Given multiple answers to certain questions, developers may take the accepted answer, the answer from a person with high reputation, or the one frequently suggested. However, researchers recently observed exploitable security vulnerabilities in popular SO answers. This observation inspires us to explore the following questions: How much can we trust the security implementation suggestions on SO? If suggested answers are vulnerable, can developers rely on the community's dynamics to infer the vulnerability and identify a secure counterpart?   To answer these highly important questions, we conducted a study on SO posts by contrasting secure and insecure advices with the community-given content evaluation. We investigated whether SO incentive mechanism is effective in improving security properties of distributed code examples. Moreover, we also traced duplicated answers to assess whether the community behavior facilitates propagation of secure and insecure code suggestions. We compiled 953 different groups of similar security-related code examples and labeled their security, identifying 785 secure answer posts and 644 insecure ones. Compared with secure suggestions, insecure ones had higher view counts (36,508 vs. 18,713), received a higher score (14 vs. 5), and had significantly more duplicates (3.8 vs. 3.0) on average. 34% of the posts provided by highly reputable so-called trusted users were insecure.   Our findings show that there are lots of insecure snippets on SO, while the community-given feedback does not allow differentiating secure from insecure choices. Moreover, the reputation mechanism fails in indicating trustworthy users with respect to security questions, ultimately leaving other users wandering around alone in a software security minefield.

</details>

<details>

<summary>2019-01-05 11:56:13 - Fake News Detection via NLP is Vulnerable to Adversarial Attacks</summary>

- *Zhixuan Zhou, Huankang Guan, Meghana Moorthy Bhat, Justin Hsu*

- `1901.09657v1` - [abs](http://arxiv.org/abs/1901.09657v1) - [pdf](http://arxiv.org/pdf/1901.09657v1)

> News plays a significant role in shaping people's beliefs and opinions. Fake news has always been a problem, which wasn't exposed to the mass public until the past election cycle for the 45th President of the United States. While quite a few detection methods have been proposed to combat fake news since 2015, they focus mainly on linguistic aspects of an article without any fact checking. In this paper, we argue that these models have the potential to misclassify fact-tampering fake news as well as under-written real news. Through experiments on Fakebox, a state-of-the-art fake news detector, we show that fact tampering attacks can be effective. To address these weaknesses, we argue that fact checking should be adopted in conjunction with linguistic characteristics analysis, so as to truly separate fake news from real news. A crowdsourced knowledge graph is proposed as a straw man solution to collecting timely facts about news events.

</details>

<details>

<summary>2019-01-09 01:27:18 - Risk analysis beyond vulnerability and resilience - characterizing the defensibility of critical systems</summary>

- *Vicki Bier, Alexander Gutfraind*

- `1901.07313v1` - [abs](http://arxiv.org/abs/1901.07313v1) - [pdf](http://arxiv.org/pdf/1901.07313v1)

> A common problem in risk analysis is to characterize the overall security of a system of valuable assets (e.g., government buildings or communication hubs), and to suggest measures to mitigate any hazards or security threats. Currently, analysts typically rely on a combination of indices, such as resilience, robustness, redundancy, security, and vulnerability. However, these indices are not by themselves sufficient as a guide to action; for example, while it is possible to develop policies to decrease vulnerability, such policies may not always be cost-effective. Motivated by this gap, we propose a new index, defensibility. A system is considered defensible to the extent that a modest investment can significantly reduce the damage from an attack or disruption. To compare systems whose performance is not readily commensurable (e.g., the electrical grid vs. the water-distribution network, both of which are critical, but which provide distinct types of services), we defined defensibility as a dimensionless index. After defining defensibility quantitatively, we illustrate how the defensibility of a system depends on factors such as the defender and attacker asset valuations, the nature of the threat (whether intelligent and adaptive, or random), and the levels of attack and defense strengths and provide analytical results that support the observations arising from the above illustrations. Overall, we argue that the defensibility of a system is an important dimension to consider when evaluating potential defensive investments, and that it can be applied in a variety of different contexts.

</details>

<details>

<summary>2019-01-10 06:53:48 - Image Transformation can make Neural Networks more robust against Adversarial Examples</summary>

- *Dang Duy Thang, Toshihiro Matsui*

- `1901.03037v1` - [abs](http://arxiv.org/abs/1901.03037v1) - [pdf](http://arxiv.org/pdf/1901.03037v1)

> Neural networks are being applied in many tasks related to IoT with encouraging results. For example, neural networks can precisely detect human, objects and animal via surveillance camera for security purpose. However, neural networks have been recently found vulnerable to well-designed input samples that called adversarial examples. Such issue causes neural networks to misclassify adversarial examples that are imperceptible to humans. We found giving a rotation to an adversarial example image can defeat the effect of adversarial examples. Using MNIST number images as the original images, we first generated adversarial examples to neural network recognizer, which was completely fooled by the forged examples. Then we rotated the adversarial image and gave them to the recognizer to find the recognizer to regain the correct recognition. Thus, we empirically confirmed rotation to images can protect pattern recognizer based on neural networks from adversarial example attacks.

</details>

<details>

<summary>2019-01-10 09:21:23 - Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples</summary>

- *Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, Joseph Keshet*

- `1802.04528v3` - [abs](http://arxiv.org/abs/1802.04528v3) - [pdf](http://arxiv.org/pdf/1802.04528v3)

> In recent years, deep learning has shown performance breakthroughs in many applications, such as image detection, image segmentation, pose estimation, and speech recognition. However, this comes with a major concern: deep networks have been found to be vulnerable to adversarial examples. Adversarial examples are slightly modified inputs that are intentionally designed to cause a misclassification by the model. In the domains of images and speech, the modifications are so small that they are not seen or heard by humans, but nevertheless greatly affect the classification of the model.   Deep learning models have been successfully applied to malware detection. In this domain, generating adversarial examples is not straightforward, as small modifications to the bytes of the file could lead to significant changes in its functionality and validity. We introduce a novel loss function for generating adversarial examples specifically tailored for discrete input sets, such as executable bytes. We modify malicious binaries so that they would be detected as benign, while preserving their original functionality, by injecting a small sequence of bytes (payload) in the binary file. We applied this approach to an end-to-end convolutional deep learning malware detection model and show a high rate of detection evasion. Moreover, we show that our generated payload is robust enough to be transferable within different locations of the same file and across different files, and that its entropy is low and similar to that of benign data sections.

</details>

<details>

<summary>2019-01-11 00:02:04 - Software-Defined Radio GNSS Instrumentation for Spoofing Mitigation: A Review and a Case Study</summary>

- *Erick Schmidt, Zach A. Ruble, David Akopian, Daniel J. Pack*

- `1901.03434v1` - [abs](http://arxiv.org/abs/1901.03434v1) - [pdf](http://arxiv.org/pdf/1901.03434v1)

> Recently, several global navigation satellite systems (GNSS) emerged following the transformative technology impact of the first GNSS: US Global Positioning System (GPS). The power level of GNSS signals as measured at the earths surface is below the noise floor and is consequently vulnerable against interference. Spoofers are smart GNSS-like interferers, which mislead the receivers into generating false position and time information. While many spoofing mitigation techniques exist, spoofers are continually evolving, producing a cycle of new spoofing attacks and counter-measures against them. Thus, upgradability of receivers becomes an important advantage for maintaining their immunity against spoofing. Software-defined radio (SDR) implementations of a GPS receiver address such flexibility but are challenged by demanding computational requirements of both GNSS signal processing and spoofing mitigation. Therefore, this paper reviews reported SDRs in the context of instrumentation capabilities for both conventional and spoofing mitigation modes. This separation is necessitated by significantly increased computational loads when in spoofing domain. This is demonstrated by a case study budget analysis.

</details>

<details>

<summary>2019-01-11 14:53:42 - ACMiner: Extraction and Analysis of Authorization Checks in Android's Middleware</summary>

- *Sigmund Albert Gorski III, Benjamin Andow, Adwait Nadkarni, Sunil Manandhar, William Enck, Eric Bodden, Alexandre Bartel*

- `1901.03603v1` - [abs](http://arxiv.org/abs/1901.03603v1) - [pdf](http://arxiv.org/pdf/1901.03603v1)

> Billions of users rely on the security of the Android platform to protect phones, tablets, and many different types of consumer electronics. While Android's permission model is well studied, the enforcement of the protection policy has received relatively little attention. Much of this enforcement is spread across system services, taking the form of hard-coded checks within their implementations. In this paper, we propose Authorization Check Miner (ACMiner), a framework for evaluating the correctness of Android's access control enforcement through consistency analysis of authorization checks. ACMiner combines program and text analysis techniques to generate a rich set of authorization checks, mines the corresponding protection policy for each service entry point, and uses association rule mining at a service granularity to identify inconsistencies that may correspond to vulnerabilities. We used ACMiner to study the AOSP version of Android 7.1.1 to identify 28 vulnerabilities relating to missing authorization checks. In doing so, we demonstrate ACMiner's ability to help domain experts process thousands of authorization checks scattered across millions of lines of code.

</details>

<details>

<summary>2019-01-11 15:42:59 - ADef: an Iterative Algorithm to Construct Adversarial Deformations</summary>

- *Rima Alaifari, Giovanni S. Alberti, Tandri Gauksson*

- `1804.07729v3` - [abs](http://arxiv.org/abs/1804.07729v3) - [pdf](http://arxiv.org/pdf/1804.07729v3)

> While deep neural networks have proven to be a powerful tool for many recognition and classification tasks, their stability properties are still not well understood. In the past, image classifiers have been shown to be vulnerable to so-called adversarial attacks, which are created by additively perturbing the correctly classified image. In this paper, we propose the ADef algorithm to construct a different kind of adversarial attack created by iteratively applying small deformations to the image, found through a gradient descent step. We demonstrate our results on MNIST with convolutional neural networks and on ImageNet with Inception-v3 and ResNet-101.

</details>

<details>

<summary>2019-01-11 18:28:38 - Don't Wait to be Breached! Creating Asymmetric Uncertainty of Cloud Applications via Moving Target Defenses</summary>

- *Kennedy A. Torkura, Christoph Meinel, Nane Kratzke*

- `1901.04319v1` - [abs](http://arxiv.org/abs/1901.04319v1) - [pdf](http://arxiv.org/pdf/1901.04319v1)

> Cloud applications expose - besides service endpoints - also potential or actual vulnerabilities. Therefore, cloud security engineering efforts focus on hardening the fortress walls but seldom assume that attacks may be successful. At least against zero-day exploits, this approach is often toothless. Other than most security approaches and comparable to biological systems we accept that defensive "walls" can be breached at several layers. Instead of hardening the "fortress" walls we propose to make use of an (additional) active and adaptive defense system to attack potential intruders - an immune system that is inspired by the concept of a moving target defense. This "immune system" works on two layers. On the infrastructure layer, virtual machines are continuously regenerated (cell regeneration) to wipe out even undetected intruders. On the application level, the vertical and horizontal attack surface is continuously modified to circumvent successful replays of formerly scripted attacks. Our evaluations with two common cloud-native reference applications in popular cloud service infrastructures (Amazon Web Services, Google Compute Engine, Azure and OpenStack) show that it is technically possible to limit the time of attackers acting undetected down to minutes. Further, more than 98% of an attack surface can be changed automatically and minimized which makes it hard for intruders to replay formerly successful scripted attacks. So, even if intruders get a foothold in the system, it is hard for them to maintain it.

</details>

<details>

<summary>2019-01-12 21:05:12 - Threats, Protection and Attribution of Cyber Attacks on Critical Infrastructures</summary>

- *Leandros Maglaras, Mohamed Amine Ferrag, Abdelouahid Derhab, Mithun Mukherjee, Helge Janicke, Stylianos Rallis*

- `1901.03899v1` - [abs](http://arxiv.org/abs/1901.03899v1) - [pdf](http://arxiv.org/pdf/1901.03899v1)

> As Critical National Infrastructures are becoming more vulnerable to cyber attacks, their protection becomes a significant issue for any organization as well as a nation. Moreover, the ability to attribute is a vital element of avoiding impunity in cyberspace. In this article, we present main threats to critical infrastructures along with protective measures that one nation can take, and which are classified according to legal, technical, organizational, capacity building, and cooperation aspects. Finally we provide an overview of current methods and practices regarding cyber attribution and cyber peace keeping

</details>

<details>

<summary>2019-01-14 18:06:10 - Peel the onion: Recognition of Android apps behind the Tor Network</summary>

- *Emanuele Petagna, Giuseppe Laurenza, Claudio Ciccotelli, Leonardo Querzoni*

- `1901.04434v1` - [abs](http://arxiv.org/abs/1901.04434v1) - [pdf](http://arxiv.org/pdf/1901.04434v1)

> In this work we show that Tor is vulnerable to app deanonymization attacks on Android devices through network traffic analysis. For this purpose, we describe a general methodology for performing an attack that allows to deanonymize the apps running on a target smartphone using Tor, which is the victim of the attack. Then, we discuss a Proof-of-Concept, implementing the methodology, that shows how the attack can be performed in practice and allows to assess the deanonymization accuracy that it is possible to achieve. While attacks against Tor anonymity have been already gained considerable attention in the context of website fingerprinting in desktop environments, to the best of our knowledge this is the first work that highlights Tor vulnerability to apps deanonymization attacks on Android devices. In our experiments we achieved an accuracy of 97%.

</details>

<details>

<summary>2019-01-15 07:21:44 - The Limitations of Adversarial Training and the Blind-Spot Attack</summary>

- *Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S. Dhillon, Cho-Jui Hsieh*

- `1901.04684v1` - [abs](http://arxiv.org/abs/1901.04684v1) - [pdf](http://arxiv.org/pdf/1901.04684v1)

> The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural networks (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the "blind-spot attack", where the input images reside in "blind-spots" (low density regions) of the empirical distribution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Wong & Kolter, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.

</details>

<details>

<summary>2019-01-16 13:19:11 - Defending via strategic ML selection</summary>

- *Eitan Farchi, Onn Shehory, Guy Barash*

- `1904.00737v1` - [abs](http://arxiv.org/abs/1904.00737v1) - [pdf](http://arxiv.org/pdf/1904.00737v1)

> The results of a learning process depend on the input data. There are cases in which an adversary can strategically tamper with the input data to affect the outcome of the learning process. While some datasets are difficult to attack, many others are susceptible to manipulation. A resourceful attacker can tamper with large portions of the dataset and affect them. An attacker can additionally strategically focus on a preferred subset of the attributes in the dataset to maximize the effectiveness of the attack and minimize the resources allocated to data manipulation. In light of this vulnerability, we introduce a solution according to which the defender implements an array of learners, and their activation is performed strategically. The defender computes the (game theoretic) strategy space and accordingly applies a dominant strategy where possible, and a Nash-stable strategy otherwise. In this paper we provide the details of this approach. We analyze Nash equilibrium in such a strategic learning environment, and demonstrate our solution by specific examples.

</details>

<details>

<summary>2019-01-17 17:19:24 - RTL-PSC: Automated Power Side-Channel Leakage Assessment at Register-Transfer Level</summary>

- *Miao, He, Jungmin Park, Adib Nahiyan, Apostol Vassilev, Yier Jin, Mark Tehranipoor*

- `1901.05909v1` - [abs](http://arxiv.org/abs/1901.05909v1) - [pdf](http://arxiv.org/pdf/1901.05909v1)

> Power side-channel attacks (SCAs) have become a major concern to the security community due to their non-invasive feature, low-cost, and effectiveness in extracting secret information from hardware implementation of cryto algorithms. Therefore, it is imperative to evaluate if the hardware is vulnerable to SCAs during its design and validation stages. Currently, however, there is little-known effort in evaluating the vulnerability of a hardware to SCAs at early design stage. In this paper, we propose, for the first time, an automated framework, named RTL-PSC, for power side-channel leakage assessment of hardware crypto designs at register-transfer level (RTL) with built-in evaluation metrics. RTL-PSC first estimates power profile of a hardware design using functional simulation at RTL. Then it utilizes the evaluation metrics, comprising of KL divergence metric and the success rate (SR) metric based on maximum likelihood estimation to perform power side-channel leakage (PSC) vulnerability assessment at RTL. We analyze Galois-Field (GF) and Look-up Table (LUT) based AES designs using RTL-PSC and validate its effectiveness and accuracy through both gate-level simulation and FPGA results. RTL-PSC is also capable of identifying blocks inside the design that contribute the most to the PSC vulnerability which can be used for efficient countermeasure implementation.

</details>

<details>

<summary>2019-01-18 08:20:58 - Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing</summary>

- *Jingyi Wang, Guoliang Dong, Jun Sun, Xinyu Wang, Peixin Zhang*

- `1812.05793v2` - [abs](http://arxiv.org/abs/1812.05793v2) - [pdf](http://arxiv.org/pdf/1812.05793v2)

> Deep neural networks (DNN) have been shown to be useful in a wide range of applications. However, they are also known to be vulnerable to adversarial samples. By transforming a normal sample with some carefully crafted human imperceptible perturbations, even highly accurate DNN make wrong decisions. Multiple defense mechanisms have been proposed which aim to hinder the generation of such adversarial samples. However, a recent work show that most of them are ineffective. In this work, we propose an alternative approach to detect adversarial samples at runtime. Our main observation is that adversarial samples are much more sensitive than normal samples if we impose random mutations on the DNN. We thus first propose a measure of `sensitivity' and show empirically that normal samples and adversarial samples have distinguishable sensitivity. We then integrate statistical hypothesis testing and model mutation testing to check whether an input sample is likely to be normal or adversarial at runtime by measuring its sensitivity. We evaluated our approach on the MNIST and CIFAR10 datasets. The results show that our approach detects adversarial samples generated by state-of-the-art attacking methods efficiently and accurately.

</details>

<details>

<summary>2019-01-18 09:36:14 - Robust Watermarking of Neural Network with Exponential Weighting</summary>

- *Ryota Namba, Jun Sakuma*

- `1901.06151v1` - [abs](http://arxiv.org/abs/1901.06151v1) - [pdf](http://arxiv.org/pdf/1901.06151v1)

> Deep learning has been achieving top performance in many tasks. Since training of a deep learning model requires a great deal of cost, we need to treat neural network models as valuable intellectual properties. One concern in such a situation is that some malicious user might redistribute the model or provide a prediction service using the model without permission. One promising solution is digital watermarking, to embed a mechanism into the model so that the owner of the model can verify the ownership of the model externally. In this study, we present a novel attack method against watermark, query modification, and demonstrate that all of the existing watermark methods are vulnerable to either of query modification or existing attack method (model modification). To overcome this vulnerability, we present a novel watermarking method, exponential weighting. We experimentally show that our watermarking method achieves high verification performance of watermark even under a malicious attempt of unauthorized service providers, such as model modification and query modification, without sacrificing the predictive performance of the neural network model.

</details>

<details>

<summary>2019-01-18 18:49:23 - Smart-Lock Security Re-engineered using Cryptography and Steganography</summary>

- *Chaitanya Bapat, Ganesh Baleri, Shivani Inamdar, Anant V Nimkar*

- `1901.06381v1` - [abs](http://arxiv.org/abs/1901.06381v1) - [pdf](http://arxiv.org/pdf/1901.06381v1)

> After the rise of E-commerce, social media and messenger bots, rapid developments have been made in the field of connecting things, gadgets, and devices, i.e, the Internet of Things (IoT). In the fast-paced lifestyle, it is very difficult to maintain multiple keys for traditional mechanical locks. Electromagnetic smart locks are a possible solution to this problem. To connect a smart lock with a key, Bluetooth Low Energy (BLE) protocol can be used. BLE protocol is vulnerable to Man-in-the-Middle (MITM) attack. Ensuring security over BLE is an ongoing challenge. This paper aims to analyze the MITM vulnerability of BLE and develop a possible solution for designing smart-locks with an increased level of security. The observation shows that the combination of Image Steganography and Cryptography helps to overcome the vulnerabilities of BLE protocol.

</details>

<details>

<summary>2019-01-20 21:32:46 - Ring Oscillator and its application as Physical Unclonable Function (PUF) for Password Management</summary>

- *Alireza Shamsoshoara*

- `1901.06733v1` - [abs](http://arxiv.org/abs/1901.06733v1) - [pdf](http://arxiv.org/pdf/1901.06733v1)

> Mobile and embedded devices are becoming inevitable parts of our daily routine. Similar to other electronic devices such as read access memory (RAM) and storage, mobile devices require to authenticate and to be authenticated in a secure way. Usually, this can be accomplished by servers which possess private information for all devices. Since these devices are inherently mobile and operating in untrusted environments, they are prone to be accessed by untrustworthy users. Physical unclonable function (PUF) is a unique physical feature of a semiconductor device such as a microprocessor that can be generated from physical conditions such as supply voltage, temperature, etc. Ring oscillators are the principal parts of PUFs that are synthesized on a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC). Password manager systems are used in order to keep a database of usernames and password for clients in order for registration and authentication. This table plays a crucial role in authentication in many systems. Hence, security is one of the important features of these systems. Normally, in these tables only usernames and passwords are stored; however, they are vulnerable to many attacks. So, the first method of enhancing the security is using the hash instead of the original data, but, advanced hackers can break these hash data again. Hence, the method that we are going to use in this project is utilizing PUF to store the challenge of each user instead of saving the hash of passwords.

</details>

<details>

<summary>2019-01-21 02:08:40 - VeriSolid: Correct-by-Design Smart Contracts for Ethereum</summary>

- *Anastasia Mavridou, Aron Laszka, Emmanouela Stachtiari, Abhishek Dubey*

- `1901.01292v2` - [abs](http://arxiv.org/abs/1901.01292v2) - [pdf](http://arxiv.org/pdf/1901.01292v2)

> The adoption of blockchain based distributed ledgers is growing fast due to their ability to provide reliability, integrity, and auditability without trusted entities. One of the key capabilities of these emerging platforms is the ability to create self-enforcing smart contracts. However, the development of smart contracts has proven to be error-prone in practice, and as a result, contracts deployed on public platforms are often riddled with security vulnerabilities. This issue is exacerbated by the design of these platforms, which forbids updating contract code and rolling back malicious transactions. In light of this, it is crucial to ensure that a smart contract is secure before deploying it and trusting it with significant amounts of cryptocurrency. To this end, we introduce the VeriSolid framework for the formal verification of contracts that are specified using a transition-system based model with rigorous operational semantics. Our model-based approach allows developers to reason about and verify contract behavior at a high level of abstraction. VeriSolid allows the generation of Solidity code from the verified models, which enables the correct-by-design development of smart contracts.

</details>

<details>

<summary>2019-01-22 02:14:16 - Sensitivity Analysis of Deep Neural Networks</summary>

- *Hai Shu, Hongtu Zhu*

- `1901.07152v1` - [abs](http://arxiv.org/abs/1901.07152v1) - [pdf](http://arxiv.org/pdf/1901.07152v1)

> Deep neural networks (DNNs) have achieved superior performance in various prediction tasks, but can be very vulnerable to adversarial examples or perturbations. Therefore, it is crucial to measure the sensitivity of DNNs to various forms of perturbations in real applications. We introduce a novel perturbation manifold and its associated influence measure to quantify the effects of various perturbations on DNN classifiers. Such perturbations include various external and internal perturbations to input samples and network parameters. The proposed measure is motivated by information geometry and provides desirable invariance properties. We demonstrate that our influence measure is useful for four model building tasks: detecting potential 'outliers', analyzing the sensitivity of model architectures, comparing network sensitivity between training and test sets, and locating vulnerable areas. Experiments show reasonably good performance of the proposed measure for the popular DNN models ResNet50 and DenseNet121 on CIFAR10 and MNIST datasets.

</details>

<details>

<summary>2019-01-22 12:49:18 - Securing Manufacturing Intelligence for the Industrial Internet of Things</summary>

- *Hussain Al-Aqrabi, Richard Hill, Phil Lane, Hamza Aagela*

- `1901.07284v1` - [abs](http://arxiv.org/abs/1901.07284v1) - [pdf](http://arxiv.org/pdf/1901.07284v1)

> Widespread interest in the emerging area of predictive analytics is driving industries such as manufacturing to explore new approaches to the collection and management of data provided from Industrial Internet of Things (IIoT) devices. Often, analytics processing for Business Intelligence (BI) is an intensive task, and it also presents both an opportunity for competitive advantage as well as a security vulnerability in terms of the potential for losing Intellectual Property (IP). This article explores two approaches to securing BI in the manufacturing domain. Simulation results indicate that a Unified Threat Management (UTM) model is simpler to maintain and has less potential vulnerabilities than a distributed security model. Conversely, a distributed model of security out-performs the UTM model and offers more scope for the use of existing hardware resources. In conclusion, a hybrid security model is proposed where security controls are segregated into a multi-cloud architecture.

</details>

<details>

<summary>2019-01-23 12:21:47 - Superion: Grammar-Aware Greybox Fuzzing</summary>

- *Junjie Wang, Bihuan Chen, Lei Wei, Yang Liu*

- `1812.01197v3` - [abs](http://arxiv.org/abs/1812.01197v3) - [pdf](http://arxiv.org/pdf/1812.01197v3)

> In recent years, coverage-based greybox fuzzing has proven itself to be one of the most effective techniques for finding security bugs in practice. Particularly, American Fuzzy Lop (AFL for short) is deemed to be a great success in fuzzing relatively simple test inputs. Unfortunately, when it meets structured test inputs such as XML and JavaScript, those grammar-blind trimming and mutation strategies in AFL hinder the effectiveness and efficiency.   To this end, we propose a grammar-aware coverage-based greybox fuzzing approach to fuzz programs that process structured inputs. Given the grammar (which is often publicly available) of test inputs, we introduce a grammar-aware trimming strategy to trim test inputs at the tree level using the abstract syntax trees (ASTs) of parsed test inputs. Further, we introduce two grammar-aware mutation strategies (i.e., enhanced dictionary-based mutation and tree-based mutation). Specifically, tree-based mutation works via replacing subtrees using the ASTs of parsed test inputs. Equipped with grammar-awareness, our approach can carry the fuzzing exploration into width and depth.   We implemented our approach as an extension to AFL, named Superion; and evaluated the effectiveness of Superion on real-life large-scale programs (a XML engine libplist and three JavaScript engines WebKit, Jerryscript and ChakraCore). Our results have demonstrated that Superion can improve the code coverage (i.e., 16.7% and 8.8% in line and function coverage) and bug-finding capability (i.e., 31 new bugs, among which we discovered 21 new vulnerabilities with 16 CVEs assigned and 3.2K USD bug bounty rewards received) over AFL and jsfunfuzz. We also demonstrated the effectiveness of our grammar-aware trimming and mutation.

</details>

<details>

<summary>2019-01-24 05:39:09 - Forward-Secure Group Signatures from Lattices</summary>

- *San Ling, Khoa Nguyen, Huaxiong Wang, Yanhong Xu*

- `1801.08323v2` - [abs](http://arxiv.org/abs/1801.08323v2) - [pdf](http://arxiv.org/pdf/1801.08323v2)

> Group signature is a fundamental cryptographic primitive, aiming to protect anonymity and ensure accountability of users. It allows group members to anonymously sign messages on behalf of the whole group, while incorporating a tracing mechanism to identify the signer of any suspected signature. Most of the existing group signature schemes, however, do not guarantee security once secret keys are exposed. To reduce potential damages caused by key exposure attacks, Song (ACMCCS 2001) put forward the concept of forward-secure group signature (FSGS), which prevents attackers from forging group signatures pertaining to past time periods even if a secret group signing key is revealed at the current time period. For the time being, however, all known secure FSGS schemes are based on number-theoretic assumptions, and are vulnerable against quantum computers.   In this work, we construct the first lattice-based FSGS scheme. Our scheme is proven secure under the Short Integer Solution and Learning With Errors assumptions. At the heart of our construction is a scalable lattice-based key evolving mechanism, allowing users to periodically update their secret keys and to efficiently prove in zero-knowledge that key evolution process is done correctly. To realize this essential building block, we first employ the Bonsai tree structure by Cash et al. (EUROCRYPT 2010) to handle the key evolution process, and then develop Langlois et al.'s construction (PKC 2014) to design its supporting zero-knowledge protocol.

</details>

<details>

<summary>2019-01-24 08:04:43 - Explaining Vulnerabilities of Deep Learning to Adversarial Malware Binaries</summary>

- *Luca Demetrio, Battista Biggio, Giovanni Lagorio, Fabio Roli, Alessandro Armando*

- `1901.03583v2` - [abs](http://arxiv.org/abs/1901.03583v2) - [pdf](http://arxiv.org/pdf/1901.03583v2)

> Recent work has shown that deep-learning algorithms for malware detection are also susceptible to adversarial examples, i.e., carefully-crafted perturbations to input malware that enable misleading classification. Although this has questioned their suitability for this task, it is not yet clear why such algorithms are easily fooled also in this particular application domain. In this work, we take a first step to tackle this issue by leveraging explainable machine-learning algorithms developed to interpret the black-box decisions of deep neural networks. In particular, we use an explainable technique known as feature attribution to identify the most influential input features contributing to each decision, and adapt it to provide meaningful explanations to the classification of malware binaries. In this case, we find that a recently-proposed convolutional neural network does not learn any meaningful characteristic for malware detection from the data and text sections of executable files, but rather tends to learn to discriminate between benign and malware samples based on the characteristics found in the file header. Based on this finding, we propose a novel attack algorithm that generates adversarial malware binaries by only changing few tens of bytes in the file header. With respect to the other state-of-the-art attack algorithms, our attack does not require injecting any padding bytes at the end of the file, and it is much more efficient, as it requires manipulating much fewer bytes.

</details>

<details>

<summary>2019-01-24 11:29:51 - Cross-Entropy Loss and Low-Rank Features Have Responsibility for Adversarial Examples</summary>

- *Kamil Nar, Orhan Ocal, S. Shankar Sastry, Kannan Ramchandran*

- `1901.08360v1` - [abs](http://arxiv.org/abs/1901.08360v1) - [pdf](http://arxiv.org/pdf/1901.08360v1)

> State-of-the-art neural networks are vulnerable to adversarial examples; they can easily misclassify inputs that are imperceptibly different than their training and test data. In this work, we establish that the use of cross-entropy loss function and the low-rank features of the training data have responsibility for the existence of these inputs. Based on this observation, we suggest that addressing adversarial examples requires rethinking the use of cross-entropy loss function and looking for an alternative that is more suited for minimization with low-rank features. In this direction, we present a training scheme called differential training, which uses a loss function defined on the differences between the features of points from opposite classes. We show that differential training can ensure a large margin between the decision boundary of the neural network and the points in the training dataset. This larger margin increases the amount of perturbation needed to flip the prediction of the classifier and makes it harder to find an adversarial example with small perturbations. We test differential training on a binary classification task with CIFAR-10 dataset and demonstrate that it radically reduces the ratio of images for which an adversarial example could be found -- not only in the training dataset, but in the test dataset as well.

</details>

<details>

<summary>2019-01-24 14:13:58 - Intrinsic Geometric Vulnerability of High-Dimensional Artificial Intelligence</summary>

- *Luca Bortolussi, Guido Sanguinetti*

- `1811.03571v2` - [abs](http://arxiv.org/abs/1811.03571v2) - [pdf](http://arxiv.org/pdf/1811.03571v2)

> The success of modern Artificial Intelligence (AI) technologies depends critically on the ability to learn non-linear functional dependencies from large, high dimensional data sets. Despite recent high-profile successes, empirical evidence indicates that the high predictive performance is often paired with low robustness, making AI systems potentially vulnerable to adversarial attacks. In this report, we provide a simple intuitive argument suggesting that high performance and vulnerability are intrinsically coupled, and largely dependent on the geometry of typical, high-dimensional data sets. Our work highlights a major potential pitfall of modern AI systems, and suggests practical research directions to ameliorate the problem.

</details>

<details>

<summary>2019-01-24 16:14:40 - OAuthGuard: Protecting User Security and Privacy with OAuth 2.0 and OpenID Connect</summary>

- *Wanpeng Li, Chris J Mitchell, Thomas Chen*

- `1901.08960v1` - [abs](http://arxiv.org/abs/1901.08960v1) - [pdf](http://arxiv.org/pdf/1901.08960v1)

> Millions of users routinely use Google to log in to websites supporting OAuth 2.0 or OpenID Connect; the security of OAuth 2.0 and OpenID Connect is therefore of critical importance. As revealed in previous studies, in practice RPs often implement OAuth 2.0 incorrectly, and so many real-world OAuth 2.0 and OpenID Connect systems are vulnerable to attack. However, users of such flawed systems are typically unaware of these issues, and so are at risk of attacks which could result in unauthorised access to the victim user's account at an RP. In order to address this threat, we have developed OAuthGuard, an OAuth 2.0 and OpenID Connect vulnerability scanner and protector, that works with RPs using Google OAuth 2.0 and OpenID Connect services. It protects user security and privacy even when RPs do not implement OAuth 2.0 or OpenID Connect correctly. We used OAuthGuard to survey the 1000 top-ranked websites supporting Google sign-in for the possible presence of five OAuth 2.0 or OpenID Connect security and privacy vulnerabilities, of which one has not previously been described in the literature. Of the 137 sites in our study that employ Google Sign-in, 69 were found to suffer from at least one serious vulnerability. OAuthGuard was able to protect user security and privacy for 56 of these 69 RPs, and for the other 13 was able to warn users that they were using an insecure implementation.

</details>

<details>

<summary>2019-01-25 22:12:39 - On Learning Meaningful Code Changes via Neural Machine Translation</summary>

- *Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, Denys Poshyvanyk*

- `1901.09102v1` - [abs](http://arxiv.org/abs/1901.09102v1) - [pdf](http://arxiv.org/pdf/1901.09102v1)

> Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.

</details>

<details>

<summary>2019-01-25 22:56:10 - Generative Adversarial Networks for Black-Box API Attacks with Limited Training Data</summary>

- *Yi Shi, Yalin E. Sagduyu, Kemal Davaslioglu, Jason H. Li*

- `1901.09113v1` - [abs](http://arxiv.org/abs/1901.09113v1) - [pdf](http://arxiv.org/pdf/1901.09113v1)

> As online systems based on machine learning are offered to public or paid subscribers via application programming interfaces (APIs), they become vulnerable to frequent exploits and attacks. This paper studies adversarial machine learning in the practical case when there are rate limitations on API calls. The adversary launches an exploratory (inference) attack by querying the API of an online machine learning system (in particular, a classifier) with input data samples, collecting returned labels to build up the training data, and training an adversarial classifier that is functionally equivalent and statistically close to the target classifier. The exploratory attack with limited training data is shown to fail to reliably infer the target classifier of a real text classifier API that is available online to the public. In return, a generative adversarial network (GAN) based on deep learning is built to generate synthetic training data from a limited number of real training data samples, thereby extending the training data and improving the performance of the inferred classifier. The exploratory attack provides the basis to launch the causative attack (that aims to poison the training process) and evasion attack (that aims to fool the classifier into making wrong decisions) by selecting training and test data samples, respectively, based on the confidence scores obtained from the inferred classifier. These stealth attacks with small footprint (using a small number of API calls) make adversarial machine learning practical under the realistic case with limited training data available to the adversary.

</details>

<details>

<summary>2019-01-26 10:58:37 - What's in a Downgrade? A Taxonomy of Downgrade Attacks in the TLS Protocol and Application Protocols Using TLS</summary>

- *Eman Salem Alashwali, Kasper Rasmussen*

- `1809.05681v2` - [abs](http://arxiv.org/abs/1809.05681v2) - [pdf](http://arxiv.org/pdf/1809.05681v2)

> A number of important real-world protocols including the Transport Layer Security (TLS) protocol have the ability to negotiate various security-related choices such as the protocol version and the cryptographic algorithms to be used in a particular session. Furthermore, some insecure application-layer protocols such as the Simple Mail Transfer Protocol (SMTP) negotiate the use of TLS itself on top of the application protocol to secure the communication channel. These protocols are often vulnerable to a class of attacks known as downgrade attacks which targets this negotiation mechanism. In this paper we create the first taxonomy of TLS downgrade attacks. Our taxonomy classifies possible attacks with respect to four different vectors: the protocol element that is targeted, the type of vulnerability that enables the attack, the attack method, and the level of damage that the attack causes. We base our taxonomy on a thorough analysis of fifteen notable published attacks. Our taxonomy highlights clear and concrete aspects that many downgrade attacks have in common, and allows for a common language, classification, and comparison of downgrade attacks. We demonstrate the application of our taxonomy by classifying the surveyed attacks.

</details>

<details>

<summary>2019-01-28 05:17:09 - Diffie-Hellman in the Air: A Link Layer Approach for In-Band Wireless Pairing</summary>

- *Wenlong Shen, Yu Cheng, Bo Yin, Kecheng Liu, Xianghui Cao*

- `1901.09520v1` - [abs](http://arxiv.org/abs/1901.09520v1) - [pdf](http://arxiv.org/pdf/1901.09520v1)

> Key establishment is one fundamental issue in wireless security. The widely used Diffie-Hellman key exchange is vulnerable to the man-in-the-middle attack. This paper presents a novel in-band solution for defending the man-in-the-middle attack during the key establishment process for wireless devices. Our solution is based on the insight that an attacker inevitably affects the link layer behavior of the wireless channel, and this behavior change introduced by the attacker can be detected by the legitimate users. Specifically, we propose a key exchange protocol and its corresponding channel access mechanism for the protocol message transmission, in which the Diffie-Hellman parameter is transmitted multiple times in a row without being interrupted by other data transmission on the same wireless channel. The proposed key exchange protocol forces the MITM attacker to cause multiple packet collisions consecutively at the receiver side, which can then be monitored by the proposed detection algorithm. The performance of the proposed solution is validated through both theoretical analysis and simulation: the proposed solution is secure against the MITM attack and can achieve an arbitrarily low false positive ratio. This proposed link layer solution works completely in-band, and can be easily implemented on off-the-shelf wireless devices without the requirement of any special hardware.

</details>

<details>

<summary>2019-01-28 20:12:15 - Improving Adversarial Robustness of Ensembles with Diversity Training</summary>

- *Sanjay Kariyappa, Moinuddin K. Qureshi*

- `1901.09981v1` - [abs](http://arxiv.org/abs/1901.09981v1) - [pdf](http://arxiv.org/pdf/1901.09981v1)

> Deep Neural Networks are vulnerable to adversarial attacks even in settings where the attacker has no direct access to the model being attacked. Such attacks usually rely on the principle of transferability, whereby an attack crafted on a surrogate model tends to transfer to the target model. We show that an ensemble of models with misaligned loss gradients can provide an effective defense against transfer-based attacks. Our key insight is that an adversarial example is less likely to fool multiple models in the ensemble if their loss functions do not increase in a correlated fashion. To this end, we propose Diversity Training, a novel method to train an ensemble of models with uncorrelated loss functions. We show that our method significantly improves the adversarial robustness of ensembles and can also be combined with existing methods to create a stronger defense.

</details>

<details>

<summary>2019-01-29 01:29:32 - Beware of the App! On the Vulnerability Surface of Smart Devices through their Companion Apps</summary>

- *Davino Mauro Junior, Luis Melo, Harvey Lu, Marcelo d'Amorim, Atul Prakash*

- `1901.10062v1` - [abs](http://arxiv.org/abs/1901.10062v1) - [pdf](http://arxiv.org/pdf/1901.10062v1)

> Internet of Things (IoT) devices are becoming increasingly important. These devices are often resource-limited, hindering rigorous enforcement of security policies. Assessing the vulnerability of IoT devices is an important problem, but analyzing their firmware is difficult for a variety of reasons, including requiring the purchase of devices. This paper finds that analyzing companion apps to these devices for clues to security vulnerabilities can be an effective strategy. Compared to device hardware and firmware, these apps are easy to download and analyze. A key finding of this study is that the communication between an IoT device and its app is often not properly encrypted and authenticated and these issues enable the construction of exploits to remotely control the devices. To confirm the vulnerabilities found, we created exploits against five popular IoT devices from Amazon by using a combination of static and dynamic analyses. We also did a larger study, finding that analyzing 96 popular IoT devices only required analyzing 32 companion apps. Among the conservative findings, 50% of the apps corresponding to 38% of the devices did not use proper encryption techniques to secure device to companion app communication. Finally, we discuss defense strategies that developers can adapt to address the lessons from our work.

</details>

<details>

<summary>2019-01-30 21:30:18 - RED-Attack: Resource Efficient Decision based Attack for Machine Learning</summary>

- *Faiq Khalid, Hassan Ali, Muhammad Abdullah Hanif, Semeen Rehman, Rehan Ahmed, Muhammad Shafique*

- `1901.10258v2` - [abs](http://arxiv.org/abs/1901.10258v2) - [pdf](http://arxiv.org/pdf/1901.10258v2)

> Due to data dependency and model leakage properties, Deep Neural Networks (DNNs) exhibit several security vulnerabilities. Several security attacks exploited them but most of them require the output probability vector. These attacks can be mitigated by concealing the output probability vector. To address this limitation, decision-based attacks have been proposed which can estimate the model but they require several thousand queries to generate a single untargeted attack image. However, in real-time attacks, resources and attack time are very crucial parameters. Therefore, in resource-constrained systems, e.g., autonomous vehicles where an untargeted attack can have a catastrophic effect, these attacks may not work efficiently. To address this limitation, we propose a resource efficient decision-based methodology which generates the imperceptible attack, i.e., the RED-Attack, for a given black-box model. The proposed methodology follows two main steps to generate the imperceptible attack, i.e., classification boundary estimation and adversarial noise optimization. Firstly, we propose a half-interval search-based algorithm for estimating a sample on the classification boundary using a target image and a randomly selected image from another class. Secondly, we propose an optimization algorithm which first, introduces a small perturbation in some randomly selected pixels of the estimated sample. Then to ensure imperceptibility, it optimizes the distance between the perturbed and target samples. For illustration, we evaluate it for CFAR-10 and German Traffic Sign Recognition (GTSR) using state-of-the-art networks.

</details>

<details>

<summary>2019-01-31 09:23:57 - Conversational Networks for Automatic Online Moderation</summary>

- *Etienne Papegnies, Vincent Labatut, Richard Dufour, Georges Linares*

- `1901.11281v1` - [abs](http://arxiv.org/abs/1901.11281v1) - [pdf](http://arxiv.org/pdf/1901.11281v1)

> Moderation of user-generated content in an online community is a challenge that has great socio-economical ramifications. However, the costs incurred by delegating this work to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language-dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French Massively Multiplayer Online Game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an F-measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining most of the performance (82.65).

</details>

<details>

<summary>2019-01-31 12:49:48 - Formal methods and software engineering for DL. Security, safety and productivity for DL systems development</summary>

- *Gaetan J. D. R. Hains, Arvid Jakobsson, Youry Khmelevsky*

- `1901.11334v1` - [abs](http://arxiv.org/abs/1901.11334v1) - [pdf](http://arxiv.org/pdf/1901.11334v1)

> Deep Learning (DL) techniques are now widespread and being integrated into many important systems. Their classification and recognition abilities ensure their relevance for multiple application domains. As machine-learning that relies on training instead of algorithm programming, they offer a high degree of productivity. But they can be vulnerable to attacks and the verification of their correctness is only just emerging as a scientific and engineering possibility. This paper is a major update of a previously-published survey, attempting to cover all recent publications in this area. It also covers an even more recent trend, namely the design of domain-specific languages for producing and training neural nets.

</details>


## 2019-02

<details>

<summary>2019-02-01 18:05:24 - Towards Demystifying Membership Inference Attacks</summary>

- *Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, Wenqi Wei*

- `1807.09173v2` - [abs](http://arxiv.org/abs/1807.09173v2) - [pdf](http://arxiv.org/pdf/1807.09173v2)

> Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. In providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and corresponding attack models are largely transferable. Though different model types display different vulnerabilities to membership inference, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning exposes vulnerabilities to membership inference risks when the adversary is a participant. We also discuss countermeasure and mitigation strategies.

</details>

<details>

<summary>2019-02-04 06:03:22 - Adversarial Attacks Against Medical Deep Learning Systems</summary>

- *Samuel G. Finlayson, Hyung Won Chung, Isaac S. Kohane, Andrew L. Beam*

- `1804.05296v3` - [abs](http://arxiv.org/abs/1804.05296v3) - [pdf](http://arxiv.org/pdf/1804.05296v3)

> The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.

</details>

<details>

<summary>2019-02-08 06:06:01 - Understanding the One-Pixel Attack: Propagation Maps and Locality Analysis</summary>

- *Danilo Vasconcellos Vargas, Jiawei Su*

- `1902.02947v1` - [abs](http://arxiv.org/abs/1902.02947v1) - [pdf](http://arxiv.org/pdf/1902.02947v1)

> Deep neural networks were shown to be vulnerable to single pixel modifications. However, the reason behind such phenomena has never been elucidated. Here, we propose Propagation Maps which show the influence of the perturbation in each layer of the network. Propagation Maps reveal that even in extremely deep networks such as Resnet, modification in one pixel easily propagates until the last layer. In fact, this initial local perturbation is also shown to spread becoming a global one and reaching absolute difference values that are close to the maximum value of the original feature maps in a given layer. Moreover, we do a locality analysis in which we demonstrate that nearby pixels of the perturbed one in the one-pixel attack tend to share the same vulnerability, revealing that the main vulnerability lies in neither neurons nor pixels but receptive fields. Hopefully, the analysis conducted in this work together with a new technique called propagation maps shall shed light into the inner workings of other adversarial samples and be the basis of new defense systems to come.

</details>

<details>

<summary>2019-02-08 11:05:59 - Taxonomy driven indicator scoring in MISP threat intelligence platforms</summary>

- *Sami Mokaddem, Gerard Wagener, Alexandre Dulaunoy, Andras Iklody*

- `1902.03914v1` - [abs](http://arxiv.org/abs/1902.03914v1) - [pdf](http://arxiv.org/pdf/1902.03914v1)

> IT security community is recently facing a change of trend from closed to open working groups and from restrictive information to full information disclosure and sharing. One major feature for this trend change is the number of incidents and various Indicators of compromise (IoC) that appear on a daily base, which can only be faced and solved in a collaborative way. Sharing information is key to stay on top of the threats.   To cover the needs of having a medium for information sharing, different initiatives were taken such as the Open Source Threat Intelligence and Sharing Platform called MISP. At current state, this sharing and collection platform has become far more than a malware information sharing platform. It includes all kind of IoCs, malware and vulnerabilities, but also financial threat or fraud information. Hence, the volume of information is increasing and evolving.   In this paper we present implemented distributed data interaction methods for MISP followed by a generic scoring model for decaying information that is shared within MISP communities. As the MISP community members do not have the same objectives, use cases and implementations of the scoring model are discussed. A commonly encountered use case in practice is the detection of indicators of compromise in operational networks.

</details>

<details>

<summary>2019-02-08 20:38:10 - A Light-Weight Authentication Scheme for Air Force Internet of Things</summary>

- *Xi Hang Cao, Xiaojiang Du, E. Paul Ratazzi*

- `1902.03282v1` - [abs](http://arxiv.org/abs/1902.03282v1) - [pdf](http://arxiv.org/pdf/1902.03282v1)

> Internet of Things (IoT) is ubiquitous because of its broad applications and the advance in communication technologies. The capabilities of IoT also enable its important role in homeland security and tactical missions, including Reconnaissance, Intelligence, Surveillance, and Target Acquisition (RISTA). IoT security becomes the most critical issue before its extensive use in military operations. While the majority of research focuses on smart IoT devices, treatments for legacy dumb network-ready devices are lacking; moreover, IoT devices deployed in a hostile environment are often required to be dumb due to the strict hardware constraints, making them highly vulnerable to cyber attacks. To mitigate the problem, we propose a light-weight authentication scheme for dumb IoT devices, in a case study of the UAV-sensor collaborative RISTA missions. Our scheme utilizes the covert channels in the physical layer for authentications and does not request conventional key deployments, key generations which may cause security risks and large overhead that a dumb sensor cannot afford. Our scheme operates on the physical layer, and thus it is highly portable and generalizable to most commercial and military communication protocols. We demonstrate the viability of our scheme by building a prototype system and conducting experiments to emulate the behaviors of UAVs and sensors in real scenarios.

</details>

<details>

<summary>2019-02-08 23:59:23 - Systematization of Vulnerability Discovery Knowledge: Review Protocol</summary>

- *Nuthan Munaiah, Andrew Meneely*

- `1902.03331v1` - [abs](http://arxiv.org/abs/1902.03331v1) - [pdf](http://arxiv.org/pdf/1902.03331v1)

> In this report, we describe the review protocol that will guide the systematic review of the literature in metrics-based discovery of vulnerabilities. The protocol have been developed in adherence with the guidelines for performing Systematic Literature Reviews in Software Engineering prescribed by Kitchenham and Charters.

</details>

<details>

<summary>2019-02-09 03:40:49 - The Adversarial Attack and Detection under the Fisher Information Metric</summary>

- *Chenxiao Zhao, P. Thomas Fletcher, Mixue Yu, Yaxin Peng, Guixu Zhang, Chaomin Shen*

- `1810.03806v2` - [abs](http://arxiv.org/abs/1810.03806v2) - [pdf](http://arxiv.org/pdf/1810.03806v2)

> Many deep learning models are vulnerable to the adversarial attack, i.e., imperceptible but intentionally-designed perturbations to the input can cause incorrect output of the networks. In this paper, using information geometry, we provide a reasonable explanation for the vulnerability of deep learning models. By considering the data space as a non-linear space with the Fisher information metric induced from a neural network, we first propose an adversarial attack algorithm termed one-step spectral attack (OSSA). The method is described by a constrained quadratic form of the Fisher information matrix, where the optimal adversarial perturbation is given by the first eigenvector, and the model vulnerability is reflected by the eigenvalues. The larger an eigenvalue is, the more vulnerable the model is to be attacked by the corresponding eigenvector. Taking advantage of the property, we also propose an adversarial detection method with the eigenvalues serving as characteristics. Both our attack and detection algorithms are numerically optimized to work efficiently on large datasets. Our evaluations show superior performance compared with other methods, implying that the Fisher information is a promising approach to investigate the adversarial attacks and defenses.

</details>

<details>

<summary>2019-02-11 03:34:08 - Blockchain based Privacy-Preserving Software Updates with Proof-of-Delivery for Internet of Things</summary>

- *Yanqi Zhao, Yiming Liu, Yong Yu, Yannan Li*

- `1902.03712v1` - [abs](http://arxiv.org/abs/1902.03712v1) - [pdf](http://arxiv.org/pdf/1902.03712v1)

> A large number of IoT devices are connected via the Internet. However, most of these IoT devices are generally not perfect-by-design even have security weaknesses or vulnerabilities. Thus, it is essential to update these IoT devices securely, patching their vulnerabilities and protecting the safety of the involved users. Existing studies deliver secure and reliable updates based on blockchain network which serves as the transmission network. However, these approaches could compromise users privacy when updating the IoT devices.   In this paper, we propose a new blockchain based privacy-preserving software updates protocol, which delivers secure and reliable updates with an incentive mechanism, as well protects the privacy of involved users. The vendor delivers the updates and it makes a commitment by using a smart contract to provide financial incentive to the transmission nodes who deliver the updates to the IoT devices. A transmission node gets financial incentive by providing a proof-of-delivery. The transmission node uses double authentication preventing signature (DAPS) to carry out the fair exchange to obtain the proof-of-delivery. Specifically, the transmission node exchanges an attribute-based signature from a IoT device by using DAPS. Then, it uses the attribute-based signature as a proof-of-delivery to receive financial incentives. Generally, the IoT device has to execute complex computation for an attribute-based signature (ABS). It is intolerable for resource limited devices. We propose a concrete outsourced attribute-based signature (OABS) scheme to resist the weakness. Then, we prove the security of the proposed OABS and the protocol as well. Finally, we implement smart contract in Solidity to demonstrate the validity of the proposed protocol.

</details>

<details>

<summary>2019-02-12 04:56:07 - Adversarial Samples on Android Malware Detection Systems for IoT Systems</summary>

- *Xiaolei Liu, Xiaojiang Du, Xiaosong Zhang, Qingxin Zhu, Mohsen Guizani*

- `1902.04238v1` - [abs](http://arxiv.org/abs/1902.04238v1) - [pdf](http://arxiv.org/pdf/1902.04238v1)

> Many IoT(Internet of Things) systems run Android systems or Android-like systems. With the continuous development of machine learning algorithms, the learning-based Android malware detection system for IoT devices has gradually increased. However, these learning-based detection models are often vulnerable to adversarial samples. An automated testing framework is needed to help these learning-based malware detection systems for IoT devices perform security analysis. The current methods of generating adversarial samples mostly require training parameters of models and most of the methods are aimed at image data. To solve this problem, we propose a \textbf{t}esting framework for \textbf{l}earning-based \textbf{A}ndroid \textbf{m}alware \textbf{d}etection systems(TLAMD) for IoT Devices. The key challenge is how to construct a suitable fitness function to generate an effective adversarial sample without affecting the features of the application. By introducing genetic algorithms and some technical improvements, our test framework can generate adversarial samples for the IoT Android Application with a success rate of nearly 100\% and can perform black-box testing on the system.

</details>

<details>

<summary>2019-02-12 10:22:31 - Mind the Mining</summary>

- *Guy Goren, Alexander Spiegelman*

- `1902.03899v2` - [abs](http://arxiv.org/abs/1902.03899v2) - [pdf](http://arxiv.org/pdf/1902.03899v2)

> In this paper we revisit the mining strategies in proof of work based cryptocurrencies and propose two strategies, we call smart and smarter mining, that in many cases strictly dominate honest mining. In contrast to other known attacks, like selfish mining, which induce zero-sum games among the miners, the strategies proposed in this paper increase miners' profit by reducing their variable costs (i.e., electricity). Moreover, the proposed strategies are viable for much smaller miners than previously known attacks, and surprisingly, an attack performed by one miner is profitable for all other miners as well.   While saving electricity power is very encouraging for the environment, it is less so for the coin's security. The smart/smarter strategies expose the coin to under 50\% attacks and this vulnerability might only grow when new miners join the coin as a response to the increase in profit margins induced by these strategies.

</details>

<details>

<summary>2019-02-12 12:21:07 - Task Agnostic Continual Learning Using Online Variational Bayes</summary>

- *Chen Zeno, Itay Golan, Elad Hoffer, Daniel Soudry*

- `1803.10123v3` - [abs](http://arxiv.org/abs/1803.10123v3) - [pdf](http://arxiv.org/pdf/1803.10123v3)

> Catastrophic forgetting is the notorious vulnerability of neural networks to the change of the data distribution while learning. This phenomenon has long been considered a major obstacle for allowing the use of learning agents in realistic continual learning settings. A large body of continual learning research assumes that task boundaries are known during training. However, research for scenarios in which task boundaries are unknown during training has been lacking. In this paper we present, for the first time, a method for preventing catastrophic forgetting (BGD) for scenarios with task boundaries that are unknown during training --- task-agnostic continual learning. Code of our algorithm is available at https://github.com/igolan/bgd.

</details>

<details>

<summary>2019-02-12 13:55:54 - Real Time Lateral Movement Detection based on Evidence Reasoning Network for Edge Computing Environment</summary>

- *Zhihong Tian, Wei Shi, Yuhang Wang, Chunsheng Zhu, Xiaojiang Du, Shen Su, Yanbin Sun, Nadra Guizani*

- `1902.04387v1` - [abs](http://arxiv.org/abs/1902.04387v1) - [pdf](http://arxiv.org/pdf/1902.04387v1)

> Edge computing is providing higher class intelligent service and computing capabilities at the edge of the network. The aim is to ease the backhaul impacts and offer an improved user experience, however, the edge artificial intelligence exacerbates the security of the cloud computing environment due to the dissociation of data, access control and service stages. In order to prevent users from using the edge-cloud computing environment to carry out lateral movement attacks, we proposed a method named CloudSEC meaning real time lateral movement detection based on evidence reasoning network for the edge-cloud environment. The concept of vulnerability correlation is introduced. Based on the vulnerability knowledge and environmental information of the network system, the evidence reasoning network is constructed, and the lateral movement reasoning ability provided by the evidence reasoning network is used. CloudSEC realizes the reconfiguration of the efficient real-time attack process. The experiment shows that the results are complete and credible.

</details>

<details>

<summary>2019-02-12 16:24:59 - Contribution of Herv{é} Suaudeau for the mission of the French Senate Law Commission on electronic voting on 25 January 2018</summary>

- *Hervé Suaudeau*

- `1901.07308v2` - [abs](http://arxiv.org/abs/1901.07308v2) - [pdf](http://arxiv.org/pdf/1901.07308v2)

> This contribution investigate the compatibility between the use of dematerialised voting and Article 3 of the french Constitution, which stipulates that 'voting must always be universal, equal and secret'. The IT risk management of electronic voting is managed in a rational way like any other risk management areas. We are therefore prepared to accept the use of a non dematerialised technique to cover a significant, well-identified and unavoidable risk. However, research has shown that a verifiable dematerialized vote intrinsically loses its anonymity. In addition, a study of the history of vulnerabilities shows that no terminal in recent years has been able to guarantee the anonymity of its connection and that personal voting data has always been potentially exposed. Electronic voting systems are therefore, to the best of our knowledge, unable to provide the required constitutional guarantee.

</details>

<details>

<summary>2019-02-13 02:23:40 - Towards a Better Indicator for Cache Timing Channels</summary>

- *Fan Yao, Hongyu Fang, Milos Doroslovacki, Guru Venkataramani*

- `1902.04711v1` - [abs](http://arxiv.org/abs/1902.04711v1) - [pdf](http://arxiv.org/pdf/1902.04711v1)

> Recent studies highlighting the vulnerability of computer architecture to information leakage attacks have been a cause of significant concern. Among the various classes of microarchitectural attacks, cache timing channels are especially worrisome since they have the potential to compromise users' private data at high bit rates. Prior works have demonstrated the use of cache miss patterns to detect these attacks. We find that cache miss traces can be easily spoofed and thus they may not be able to identify smarter adversaries. In this work, we show that \emph{cache occupancy}, which records the number of cache blocks owned by a specific process, can be leveraged as a stronger indicator for the presence of cache timing channels. We observe that the modulation of cache access latency in timing channels can be recognized through analyzing pairwise cache occupancy patterns. Our experimental results show that cache occupancy patterns cannot be easily obfuscated even by advanced adversaries that successfully evade cache miss-based detection.

</details>

<details>

<summary>2019-02-13 10:31:46 - Vulnerability Prediction Based on Weighted Software Network for Secure Software Building</summary>

- *Shengjun Wei, Hao Zhong, Chun Shan, Lin Ye, Xiaojiang Du, Mohsen Guizani*

- `1902.04844v1` - [abs](http://arxiv.org/abs/1902.04844v1) - [pdf](http://arxiv.org/pdf/1902.04844v1)

> To build a secure communications software, Vulnerability Prediction Models (VPMs) are used to predict vulnerable software modules in the software system before software security testing. At present many software security metrics have been proposed to design a VPM. In this paper, we predict vulnerable classes in a software system by establishing the system's weighted software network. The metrics are obtained from the nodes' attributes in the weighted software network. We design and implement a crawler tool to collect all public security vulnerabilities in Mozilla Firefox. Based on these data, the prediction model is trained and tested. The results show that the VPM based on weighted software network has a good performance in accuracy, precision, and recall. Compared to other studies, it shows that the performance of prediction has been improved greatly in Pr and Re.

</details>

<details>

<summary>2019-02-14 01:20:52 - Spectre is here to stay: An analysis of side-channels and speculative execution</summary>

- *Ross Mcilroy, Jaroslav Sevcik, Tobias Tebbi, Ben L. Titzer, Toon Verwaest*

- `1902.05178v1` - [abs](http://arxiv.org/abs/1902.05178v1) - [pdf](http://arxiv.org/pdf/1902.05178v1)

> The recent discovery of the Spectre and Meltdown attacks represents a watershed moment not just for the field of Computer Security, but also of Programming Languages. This paper explores speculative side-channel attacks and their implications for programming languages. These attacks leak information through micro-architectural side-channels which we show are not mere bugs, but in fact lie at the foundation of optimization. We identify three open problems, (1) finding side-channels, (2) understanding speculative vulnerabilities, and (3) mitigating them. For (1) we introduce a mathematical meta-model that clarifies the source of side-channels in simulations and CPUs. For (2) we introduce an architectural model with speculative semantics to study recently-discovered vulnerabilities. For (3) we explore and evaluate software mitigations and prove one correct for this model. Our analysis is informed by extensive offensive research and defensive implementation work for V8, the production JavaScript virtual machine in Chrome. Straightforward extensions to model real hardware suggest these vulnerabilities present formidable challenges for effective, efficient mitigation. As a result of our work, we now believe that speculative vulnerabilities on today's hardware defeat all language-enforced confidentiality with no known comprehensive software mitigations, as we have discovered that untrusted code can construct a universal read gadget to read all memory in the same address space through side-channels. In the face of this reality, we have shifted the security model of the Chrome web browser and V8 to process isolation.

</details>

<details>

<summary>2019-02-14 01:31:38 - Crossfire Attack Detection using Deep Learning in Software Defined ITS Networks</summary>

- *Akash Raj Narayanadoss, Tram Truong-Huu, Purnima Murali Mohan, Mohan Gurusamy*

- `1812.03639v2` - [abs](http://arxiv.org/abs/1812.03639v2) - [pdf](http://arxiv.org/pdf/1812.03639v2)

> Recent developments in intelligent transport systems (ITS) based on smart mobility significantly improves safety and security over roads and highways. ITS networks are comprised of the Internet-connected vehicles (mobile nodes), roadside units (RSU), cellular base stations and conventional core network routers to create a complete data transmission platform that provides real-time traffic information and enable prediction of future traffic conditions. However, the heterogeneity and complexity of the underlying ITS networks raise new challenges in intrusion prevention of mobile network nodes and detection of security attacks due to such highly vulnerable mobile nodes. In this paper, we consider a new type of security attack referred to as crossfire attack, which involves a large number of compromised nodes that generate low-intensity traffic in a temporally coordinated fashion such that target links or hosts (victims) are disconnected from the rest of the network. Detection of such attacks is challenging since the attacking traffic flows are indistinguishable from the legitimate flows. With the support of software-defined networking that enables dynamic network monitoring and traffic characteristic extraction, we develop a machine learning model that can learn the temporal correlation among traffic flows traversing in the ITS network, thus differentiating legitimate flows from coordinated attacking flows. We use different deep learning algorithms to train the model and study the performance using Mininet-WiFi emulation platform. The results show that our approach achieves a detection accuracy of at least 80%.

</details>

<details>

<summary>2019-02-14 03:18:02 - Injecting Software Vulnerabilities with Voltage Glitching</summary>

- *Yifan Lu*

- `1903.08102v1` - [abs](http://arxiv.org/abs/1903.08102v1) - [pdf](http://arxiv.org/pdf/1903.08102v1)

> We show how voltage glitching can cause timing violations in CMOS behavior. Then we attack a real, security hardened, consumer device to gain code execution and dump the secure boot ROM.

</details>

<details>

<summary>2019-02-14 15:53:22 - Generating Adversarial Examples with Adversarial Networks</summary>

- *Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, Dawn Song*

- `1801.02610v5` - [abs](http://arxiv.org/abs/1801.02610v5) - [pdf](http://arxiv.org/pdf/1801.02610v5)

> Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs. Such adversarial examples can mislead DNNs to produce adversary-selected results. Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses. We apply AdvGAN in both semi-whitebox and black-box attack settings. In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks. In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly. Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks. Our attack has placed the first with 92.76% accuracy on a public MNIST black-box attack challenge.

</details>

<details>

<summary>2019-02-14 20:11:12 - Can Intelligent Hyperparameter Selection Improve Resistance to Adversarial Examples?</summary>

- *Cody Burkard, Brent Lagesse*

- `1902.05586v1` - [abs](http://arxiv.org/abs/1902.05586v1) - [pdf](http://arxiv.org/pdf/1902.05586v1)

> Convolutional Neural Networks and Deep Learning classification systems in general have been shown to be vulnerable to attack by specially crafted data samples that appear to belong to one class but are instead classified as another, commonly known as adversarial examples. A variety of attack strategies have been proposed to craft these samples; however, there is no standard model that is used to compare the success of each type of attack. Furthermore, there is no literature currently available that evaluates how common hyperparameters and optimization strategies may impact a model's ability to resist these samples. This research bridges that lack of awareness and provides a means for the selection of training and model parameters in future research on evasion attacks against convolutional neural networks. The findings of this work indicate that the selection of model hyperparameters does impact the ability of a model to resist attack, although they alone cannot prevent the existence of adversarial examples.

</details>

<details>

<summary>2019-02-15 05:26:38 - ForestFirewalls: Getting Firewall Configuration Right in Critical Networks (Technical Report)</summary>

- *Dinesha Ranathunga, Matthew Roughan, Paul Tune, Phil Kernick, Nick Falkner*

- `1902.05689v1` - [abs](http://arxiv.org/abs/1902.05689v1) - [pdf](http://arxiv.org/pdf/1902.05689v1)

> Firewall configuration is critical, yet often conducted manually with inevitable errors, leaving networks vulnerable to cyber attack [40]. The impact of misconfigured firewalls can be catastrophic in Supervisory Control and Data Acquisition (SCADA) networks. These networks control the distributed assets of industrial systems such as power generation and water distribution systems. Automation can make designing firewall configurations less tedious and their deployment more reliable. In this paper, we propose ForestFirewalls, a high-level approach to configuring SCADA firewalls. Our goals are three-fold. We aim to: first, decouple implementation details from security policy design by abstracting the former; second, simplify policy design; and third, provide automated checks, pre and post-deployment, to guarantee configuration accuracy. We achieve these goals by automating the implementation of a policy to a network and by auto-validating each stage of the configuration process. We test our approach on a real SCADA network to demonstrate its effectiveness.

</details>

<details>

<summary>2019-02-16 00:55:43 - A Study of the Feasibility of Co-located App Attacks against BLE and a Large-Scale Analysis of the Current Application-Layer Security Landscape</summary>

- *Pallavi Sivakumaran, Jorge Blasco*

- `1808.03778v3` - [abs](http://arxiv.org/abs/1808.03778v3) - [pdf](http://arxiv.org/pdf/1808.03778v3)

> Bluetooth Low Energy (BLE) is a fast-growing wireless technology with a large number of potential use cases, particularly in the IoT domain. Increasingly, these use cases require the storage of sensitive user data or critical device controls on the BLE device, as well as the access of this data by an augmentative mobile application. Uncontrolled access to such data could violate user privacy, cause a device to malfunction, or even endanger lives. The BLE standard provides security mechanisms such as pairing and bonding to protect sensitive data such that only authenticated devices can access it. In this paper we show how unauthorized co-located Android applications can access pairing-protected BLE data, without the user's knowledge. We discuss mitigation strategies in terms of the various stakeholders involved in this ecosystem, and argue that at present, the only possible option for securing BLE data is for BLE developers to implement remedial measures in the form of application-layer security between the BLE device and the Android application. We introduce BLECryptracer, a tool for identifying the presence of such application-layer security, and present the results of a large-scale static analysis over 18,900+ BLE-enabled Android applications. Our findings indicate that over 45% of these applications do not implement measures to protect BLE data, and that cryptography is sometimes applied incorrectly in those that do. This implies that a potentially large number of corresponding BLE peripheral devices are vulnerable to unauthorized data access.

</details>

<details>

<summary>2019-02-16 08:37:32 - Precise Attack Synthesis for Smart Contracts</summary>

- *Yu Feng, Emina Torlak, Rastislav Bodik*

- `1902.06067v1` - [abs](http://arxiv.org/abs/1902.06067v1) - [pdf](http://arxiv.org/pdf/1902.06067v1)

> Smart contracts are programs running on top of blockchain platforms. They interact with each other through well-defined interfaces to perform financial transactions in a distributed system with no trusted third parties. But these interfaces also provide a favorable setting for attackers, who can exploit security vulnerabilities in smart contracts to achieve financial gain.   This paper presents SmartScopy, a system for automatic synthesis of adversarial contracts that identify and exploit vulnerabilities in a victim smart contract. Our tool explores the space of \emph{attack programs} based on the Application Binary Interface (ABI) specification of a victim smart contract in the Ethereum ecosystem. To make the synthesis tractable, we introduce \emph{summary-based symbolic evaluation}, which significantly reduces the number of instructions that our synthesizer needs to evaluate symbolically, without compromising the precision of the vulnerability query. Building on the summary-based symbolic evaluation, SmartScopy further introduces a novel approach for partitioning the synthesis search space for parallel exploration, as well as a lightweight deduction technique that can prune infeasible candidates earlier. We encoded common vulnerabilities of smart contracts in our query language, and evaluated SmartScopy on the entire data set from etherscan with $>$25K smart contracts. Our experiments demonstrate the benefits of summary-based symbolic evaluation and show that SmartScopy outperforms two state-of-the-art smart contracts analyzers, Oyente and Contractfuzz, in terms of running time, precision, and soundness. Furthermore, running on recent popular smart contracts, SmartScopy uncovers 20 vulnerable smart contracts that contain the recent BatchOverflow vulnerability and cannot be precisely detected by existing tools.

</details>

<details>

<summary>2019-02-17 21:28:22 - Let the Cloud Watch Over Your IoT File Systems</summary>

- *Liwei Guo, Yiying Zhang, Felix Xiaozhu Lin*

- `1902.06327v1` - [abs](http://arxiv.org/abs/1902.06327v1) - [pdf](http://arxiv.org/pdf/1902.06327v1)

> Smart devices produce security-sensitive data and keep them in on-device storage for persistence. The current storage stack on smart devices, however, offers weak security guarantees: not only because the stack depends on a vulnerable commodity OS, but also because smart device deployment is known weak on security measures.   To safeguard such data on smart devices, we present a novel storage stack architecture that i) protects file data in a trusted execution environment (TEE); ii) outsources file system logic and metadata out of TEE; iii) running a metadata-only file system replica in the cloud for continuously verifying the on-device file system behaviors. To realize the architecture, we build Overwatch, aTrustZone-based storage stack. Overwatch addresses unique challenges including discerning metadata at fine grains, hiding network delays, and coping with cloud disconnection. On a suite of three real-world applications, Overwatch shows moderate security overheads.

</details>

<details>

<summary>2019-02-17 22:12:01 - Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer</summary>

- *Hsueh-Ti Derek Liu, Michael Tao, Chun-Liang Li, Derek Nowrouzezahrai, Alec Jacobson*

- `1808.02651v2` - [abs](http://arxiv.org/abs/1808.02651v2) - [pdf](http://arxiv.org/pdf/1808.02651v2)

> Many machine learning image classifiers are vulnerable to adversarial attacks, inputs with perturbations designed to intentionally trigger misclassification. Current adversarial methods directly alter pixel colors and evaluate against pixel norm-balls: pixel perturbations smaller than a specified magnitude, according to a measurement norm. This evaluation, however, has limited practical utility since perturbations in the pixel space do not correspond to underlying real-world phenomena of image formation that lead to them and has no security motivation attached. Pixels in natural images are measurements of light that has interacted with the geometry of a physical scene. As such, we propose the direct perturbation of physical parameters that underly image formation: lighting and geometry. As such, we propose a novel evaluation measure, parametric norm-balls, by directly perturbing physical parameters that underly image formation. One enabling contribution we present is a physically-based differentiable renderer that allows us to propagate pixel gradients to the parametric space of lighting and geometry. Our approach enables physically-based adversarial attacks, and our differentiable renderer leverages models from the interactive rendering literature to balance the performance and accuracy trade-offs necessary for a memory-efficient and scalable adversarial data augmentation workflow.

</details>

<details>

<summary>2019-02-18 04:39:10 - Evaluating Robustness of Neural Networks with Mixed Integer Programming</summary>

- *Vincent Tjeng, Kai Xiao, Russ Tedrake*

- `1711.07356v3` - [abs](http://arxiv.org/abs/1711.07356v3) - [pdf](http://arxiv.org/pdf/1711.07356v3)

> Neural networks have demonstrated considerable success on a wide variety of real-world problems. However, networks trained only to optimize for training accuracy can often be fooled by adversarial examples - slightly perturbed inputs that are misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional networks with an order of magnitude more ReLUs than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded $l_\infty$ norm $\epsilon=0.1$: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness (to perturbations with bounded norm) for the remainder. Across all robust training procedures and network architectures considered, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.

</details>

<details>

<summary>2019-02-18 05:52:14 - AuxBlocks: Defense Adversarial Example via Auxiliary Blocks</summary>

- *Yueyao Yu, Pengfei Yu, Wenye Li*

- `1902.06415v1` - [abs](http://arxiv.org/abs/1902.06415v1) - [pdf](http://arxiv.org/pdf/1902.06415v1)

> Deep learning models are vulnerable to adversarial examples, which poses an indisputable threat to their applications. However, recent studies observe gradient-masking defenses are self-deceiving methods if an attacker can realize this defense. In this paper, we propose a new defense method based on appending information. We introduce the Aux Block model to produce extra outputs as a self-ensemble algorithm and analytically investigate the robustness mechanism of Aux Block. We have empirically studied the efficiency of our method against adversarial examples in two types of white-box attacks, and found that even in the full white-box attack where an adversary can craft malicious examples from defense models, our method has a more robust performance of about 54.6% precision on Cifar10 dataset and 38.7% precision on Mini-Imagenet dataset. Another advantage of our method is that it is able to maintain the prediction accuracy of the classification model on clean images, and thereby exhibits its high potential in practical applications

</details>

<details>

<summary>2019-02-18 13:53:00 - Binary Debloating for Security via Demand Driven Loading</summary>

- *Girish Mururu, Chris Porter, Prithayan Barua, Santosh Pande*

- `1902.06570v1` - [abs](http://arxiv.org/abs/1902.06570v1) - [pdf](http://arxiv.org/pdf/1902.06570v1)

> Modern software systems heavily use C/C++ based libraries. Because of the weak memory model of C/C++, libraries may suffer from vulnerabilities which can expose the applications to potential attacks. For example, a very large number of return oriented programming gadgets exist in glibc that allow stitching together semantically valid but malicious Turing-complete programs. In spite of significant advances in attack detection and mitigation, full defense is unrealistic against an ever-growing set of possibilities for generating such malicious programs.   In this work, we create a defense mechanism by debloating libraries to reduce the dynamic functions linked so that the possibilities of constructing malicious programs diminishes significantly. The key idea is to locate each library call site within an application, and in each case to load only the set of library functions that will be used at that call site. This approach of demand-driven loading relies on an input-aware oracle that predicts a near-exact set of library functions needed at a given call site during the execution. The predicted functions are loaded just in time, and the complete call chain (of function bodies) inside the library is purged after returning from the library call back into the application. We present a decision-tree based predictor, which acts as an oracle, and an optimized runtime system, which works directly with library binaries like GNU libc and libstdc++. We show that on average, the proposed scheme cuts the exposed code surface of libraries by 97.2%, reduces ROP gadgets present in linked libraries by 97.9%, achieves a prediction accuracy in most cases of at least 97%, and adds a small runtime overhead of 18% on all libraries (16% for glibc, 2% for others) across all benchmarks of SPEC 2006, suggesting this scheme is practical.

</details>

<details>

<summary>2019-02-18 18:39:33 - DoubleEcho: Mitigating Context-Manipulation Attacks in Copresence Verification</summary>

- *Hien Thi Thu Truong, Juhani Toivonen, Thien Duc Nguyen, Claudio Soriente, Sasu Tarkoma, N. Asokan*

- `1803.07211v3` - [abs](http://arxiv.org/abs/1803.07211v3) - [pdf](http://arxiv.org/pdf/1803.07211v3)

> Copresence verification based on context can improve usability and strengthen security of many authentication and access control systems. By sensing and comparing their surroundings, two or more devices can tell whether they are copresent and use this information to make access control decisions. To the best of our knowledge, all context-based copresence verification mechanisms to date are susceptible to context-manipulation attacks. In such attacks, a distributed adversary replicates the same context at the (different) locations of the victim devices, and induces them to believe that they are copresent. In this paper we propose DoubleEcho, a context-based copresence verification technique that leverages acoustic Room Impulse Response (RIR) to mitigate context-manipulation attacks. In DoubleEcho, one device emits a wide-band audible chirp and all participating devices record reflections of the chirp from the surrounding environment. Since RIR is, by its very nature, dependent on the physical surroundings, it constitutes a unique location signature that is hard for an adversary to replicate. We evaluate DoubleEcho by collecting RIR data with various mobile devices and in a range of different locations. We show that DoubleEcho mitigates context-manipulation attacks whereas all other approaches to date are entirely vulnerable to such attacks. DoubleEcho detects copresence (or lack thereof) in roughly 2 seconds and works on commodity devices.

</details>

<details>

<summary>2019-02-18 21:32:32 - Detecting Standard Violation Errors in Smart Contracts</summary>

- *Ao Li, Fan Long*

- `1812.07702v2` - [abs](http://arxiv.org/abs/1812.07702v2) - [pdf](http://arxiv.org/pdf/1812.07702v2)

> We present SOLAR, a new analysis tool for automatically detecting standard violation errors in Ethereum smart contracts.Given the Ethereum Virtual Machine (EVM) bytecode of a smart contract and a user specified constraint or invariant derived from a technical standard such as ERC-20,SOLAR symbolically executes the contract, explores all possible execution paths, and checks whether it is possible to initiate a sequence of malicious transactions to violate the specified constraint or invariant. Our experimental results highlight the effectiveness of SOLAR in finding new errors in smart con-tracts. Out of the evaluated 779 ERC-20 and 310 ERC-721smart contracts, SOLAR found 255 standard violation errors in 197 vulnerable contracts with only three false positives.237 out of the 255 errors are zero-day errors that are not re-ported before. Our results sound the alarm on the prevalence of standard violation errors in critical smart contracts that manipulate publicly traded digital assets

</details>

<details>

<summary>2019-02-20 09:46:02 - Identification of Bugs and Vulnerabilities in TLS Implementation for Windows Operating System Using State Machine Learning</summary>

- *Tarun Yadav, Koustav Sadhukhan*

- `1902.07471v1` - [abs](http://arxiv.org/abs/1902.07471v1) - [pdf](http://arxiv.org/pdf/1902.07471v1)

> TLS protocol is an essential part of secure Internet communication. In past, many attacks have been identified on the protocol. Most of these attacks are due to flaws in protocol implementation. The flaws are due to improper design and implementation of program logic by programmers. One of the widely used implementation of TLS is SChannel which is used in Windows operating system since its inception. We have used protocol state fuzzing to identify vulnerable and undesired state transitions in the state machine of the protocol for various versions of SChannel. The client as well as server components have been analyzed thoroughly using this technique and various flaws have been discovered in the implementation. Exploitation of these flaws under specific circumstances may lead to serious attacks which could disrupt secure communication. In this paper, we analyze state machine models of TLS protocol implementation of SChannel library and describe weaknesses and design flaws in these models, found using protocol state fuzzing.

</details>

<details>

<summary>2019-02-22 02:03:17 - On the Sensitivity of Adversarial Robustness to Input Data Distributions</summary>

- *Gavin Weiguang Ding, Kry Yik Chau Lui, Xiaomeng Jin, Luyu Wang, Ruitong Huang*

- `1902.08336v1` - [abs](http://arxiv.org/abs/1902.08336v1) - [pdf](http://arxiv.org/pdf/1902.08336v1)

> Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve comparable clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon.

</details>

<details>

<summary>2019-02-22 08:16:59 - Physical Adversarial Attacks Against End-to-End Autoencoder Communication Systems</summary>

- *Meysam Sadeghi, Erik G. Larsson*

- `1902.08391v1` - [abs](http://arxiv.org/abs/1902.08391v1) - [pdf](http://arxiv.org/pdf/1902.08391v1)

> We show that end-to-end learning of communication systems through deep neural network (DNN) autoencoders can be extremely vulnerable to physical adversarial attacks. Specifically, we elaborate how an attacker can craft effective physical black-box adversarial attacks. Due to the openness (broadcast nature) of the wireless channel, an adversary transmitter can increase the block-error-rate of a communication system by orders of magnitude by transmitting a well-designed perturbation signal over the channel. We reveal that the adversarial attacks are more destructive than jamming attacks. We also show that classical coding schemes are more robust than autoencoders against both adversarial and jamming attacks. The codes are available at [1].

</details>

<details>

<summary>2019-02-22 22:06:06 - RAMHU: A New Robust Lightweight Scheme for Mutual Users Authentication in Healthcare Applications</summary>

- *Mishall Al-Zubaidie, Zhongwei Zhang, Ji Zhang*

- `1902.08686v1` - [abs](http://arxiv.org/abs/1902.08686v1) - [pdf](http://arxiv.org/pdf/1902.08686v1)

> Providing a mechanism to authenticate users in healthcare applications is an essential security requirement to prevent both external and internal attackers from penetrating patients' identities and revealing their health data. Many schemes have been developed to provide authentication mechanisms to ensure that only legitimate users are authorized to connect, but these schemes still suffer from vulnerable security. Various attacks expose patients' data for malicious tampering or destruction. Transferring health-related data and information between users and the health centre makes them exposed to penetration by adversaries as they may move through an insecure channel. In addition, previous mechanisms have suffered from the poor protection of users' authentication information. To ensure the protection of patients' information and data, we propose a scheme that authenticates users based on the information of both the device and the legitimate user. In this paper, we propose a Robust Authentication Model for Healthcare Users (RAMHU) that provides mutual authentication between server and clients. This model utilizes an Elliptic Curve Integrated Encryption Scheme (ECIES) and PHOTON to achieve strong security and a good overall performance. RAMHU relies on multi pseudonyms, physical address, and one-time password mechanisms to authenticate legitimate users. Moreover, extensive informal and formal security analysis with the automated validation of Internet security protocols and applications (AVISPA) tool demonstrates that our model offers a high level of security in repelling a wide variety of possible attacks.

</details>

<details>

<summary>2019-02-24 04:41:07 - Fishy Cyber Attack Detection in Industrial Control Systems</summary>

- *Manikanta Reddy Dornala*

- `1812.03409v2` - [abs](http://arxiv.org/abs/1812.03409v2) - [pdf](http://arxiv.org/pdf/1812.03409v2)

> Cyber attacks have become serious threats to Industrial Control systems as well. It becomes important to develop a serious threat defense system against such vulnerabilities. For such process control systems, safety should also be assured apart from security. As unearthing vulnerabilities and patching them is not a feasible solution, these critical infrastructures need safeguards to prevent accidents, both natural and artificial, that could potentially be hazardous. Morita proposed an effective Zone division, capable of evaluating remote and concealed attacks on the system, coupled with Principal Component Analysis. But the need to analyze the node that has been compromised and stopping any further damages, requires an automated technique. Illustrating the basic ideas we'll simulate a simple Water plant. We propose a new automated approach based on Long Short Term Memory networks capable of detecting attacks and pin point the location of the breach.

</details>

<details>

<summary>2019-02-24 09:11:04 - MaskDGA: A Black-box Evasion Technique Against DGA Classifiers and Adversarial Defenses</summary>

- *Lior Sidi, Asaf Nadler, Asaf Shabtai*

- `1902.08909v1` - [abs](http://arxiv.org/abs/1902.08909v1) - [pdf](http://arxiv.org/pdf/1902.08909v1)

> Domain generation algorithms (DGAs) are commonly used by botnets to generate domain names through which bots can establish a resilient communication channel with their command and control servers. Recent publications presented deep learning, character-level classifiers that are able to detect algorithmically generated domain (AGD) names with high accuracy, and correspondingly, significantly reduce the effectiveness of DGAs for botnet communication. In this paper we present MaskDGA, a practical adversarial learning technique that adds perturbation to the character-level representation of algorithmically generated domain names in order to evade DGA classifiers, without the attacker having any knowledge about the DGA classifier's architecture and parameters. MaskDGA was evaluated using the DMD-2018 dataset of AGD names and four recently published DGA classifiers, in which the average F1-score of the classifiers degrades from 0.977 to 0.495 when applying the evasion technique. An additional evaluation was conducted using the same classifiers but with adversarial defenses implemented: adversarial re-training and distillation. The results of this evaluation show that MaskDGA can be used for improving the robustness of the character-level DGA classifiers against adversarial attacks, but that ideally DGA classifiers should incorporate additional features alongside character-level features that are demonstrated in this study to be vulnerable to adversarial attacks.

</details>

<details>

<summary>2019-02-26 11:22:57 - DifFuzz: Differential Fuzzing for Side-Channel Analysis</summary>

- *Shirin Nilizadeh, Yannic Noller, Corina S. Pasareanu*

- `1811.07005v2` - [abs](http://arxiv.org/abs/1811.07005v2) - [pdf](http://arxiv.org/pdf/1811.07005v2)

> Side-channel attacks allow an adversary to uncover secret program data by observing the behavior of a program with respect to a resource, such as execution time, consumed memory or response size. Side-channel vulnerabilities are difficult to reason about as they involve analyzing the correlations between resource usage over multiple program paths. We present DifFuzz, a fuzzing-based approach for detecting side-channel vulnerabilities related to time and space. DifFuzz automatically detects these vulnerabilities by analyzing two versions of the program and using resource-guided heuristics to find inputs that maximize the difference in resource consumption between secret-dependent paths. The methodology of DifFuzz is general and can be applied to programs written in any language. For this paper, we present an implementation that targets analysis of Java programs, and uses and extends the Kelinci and AFL fuzzers. We evaluate DifFuzz on a large number of Java programs and demonstrate that it can reveal unknown side-channel vulnerabilities in popular applications. We also show that DifFuzz compares favorably against Blazer and Themis, two state-of-the-art analysis tools for finding side-channels in Java programs.

</details>

<details>

<summary>2019-02-26 12:49:32 - An Abstract View on the De-anonymization Process</summary>

- *Alexandros Bampoulidis, Mihai Lupu*

- `1902.09897v1` - [abs](http://arxiv.org/abs/1902.09897v1) - [pdf](http://arxiv.org/pdf/1902.09897v1)

> Over the recent years, the availability of datasets containing personal, but anonymized information has been continuously increasing. Extensive research has revealed that such datasets are vulnerable to privacy breaches: being able to reveal sensitive information about individuals through deanonymization methods. Here, we provide a taxonomy of the research in de-anonymization.

</details>

<details>

<summary>2019-02-27 04:49:57 - Disentangled Deep Autoencoding Regularization for Robust Image Classification</summary>

- *Zhenyu Duan, Martin Renqiang Min, Li Erran Li, Mingbo Cai, Yi Xu, Bingbing Ni*

- `1902.11134v1` - [abs](http://arxiv.org/abs/1902.11134v1) - [pdf](http://arxiv.org/pdf/1902.11134v1)

> In spite of achieving revolutionary successes in machine learning, deep convolutional neural networks have been recently found to be vulnerable to adversarial attacks and difficult to generalize to novel test images with reasonably large geometric transformations. Inspired by a recent neuroscience discovery revealing that primate brain employs disentangled shape and appearance representations for object recognition, we propose a general disentangled deep autoencoding regularization framework that can be easily applied to any deep embedding based classification model for improving the robustness of deep neural networks. Our framework effectively learns disentangled appearance code and geometric code for robust image classification, which is the first disentangling based method defending against adversarial attacks and complementary to standard defense methods. Extensive experiments on several benchmark datasets show that, our proposed regularization framework leveraging disentangled embedding significantly outperforms traditional unregularized convolutional neural networks for image classification on robustness against adversarial attacks and generalization to novel test data.

</details>

<details>

<summary>2019-02-27 06:44:46 - Energy efficient mining on a quantum-enabled blockchain using light</summary>

- *Adam J Bennet, Shakib Daryanoosh*

- `1902.09520v2` - [abs](http://arxiv.org/abs/1902.09520v2) - [pdf](http://arxiv.org/pdf/1902.09520v2)

> We outline a quantum-enabled blockchain architecture based on a consortium of quantum servers. The network is hybridised, utilising digital systems for sharing and processing classical information combined with a fibre--optic infrastructure and quantum devices for transmitting and processing quantum information. We deliver an energy efficient interactive mining protocol enacted between clients and servers which uses quantum information encoded in light and removes the need for trust in network infrastructure. Instead, clients on the network need only trust the transparent network code, and that their devices adhere to the rules of quantum physics. To demonstrate the energy efficiency of the mining protocol, we elaborate upon the results of two previous experiments (one performed over 1km of optical fibre) as applied to this work. Finally, we address some key vulnerabilities, explore open questions, and observe forward--compatibility with the quantum internet and quantum computing technologies.

</details>

<details>

<summary>2019-02-28 01:26:02 - Markov Game Modeling of Moving Target Defense for Strategic Detection of Threats in Cloud Networks</summary>

- *Ankur Chowdhary, Sailik Sengupta, Dijiang Huang, Subbarao Kambhampati*

- `1812.09660v2` - [abs](http://arxiv.org/abs/1812.09660v2) - [pdf](http://arxiv.org/pdf/1812.09660v2)

> The processing and storage of critical data in large-scale cloud networks necessitate the need for scalable security solutions. It has been shown that deploying all possible security measures incurs a cost on performance by using up valuable computing and networking resources which are the primary selling points for cloud service providers. Thus, there has been a recent interest in developing Moving Target Defense (MTD) mechanisms that helps one optimize the joint objective of maximizing security while ensuring that the impact on performance is minimized. Often, these techniques model the problem of multi-stage attacks by stealthy adversaries as a single-step attack detection game using graph connectivity measures as a heuristic to measure performance, thereby (1) losing out on valuable information that is inherently present in graph-theoretic models designed for large cloud networks, and (2) coming up with certain strategies that have asymmetric impacts on performance. In this work, we leverage knowledge in attack graphs of a cloud network in formulating a zero-sum Markov Game and use the Common Vulnerability Scoring System (CVSS) to come up with meaningful utility values for this game. Then, we show that the optimal strategy of placing detecting mechanisms against an adversary is equivalent to computing the mixed Min-max Equilibrium of the Markov Game. We compare the gains obtained by using our method to other techniques presently used in cloud network security, thereby showing its effectiveness. Finally, we highlight how the method was used for a small real-world cloud system.

</details>

<details>

<summary>2019-02-28 11:14:45 - Towards Understanding Adversarial Examples Systematically: Exploring Data Size, Task and Model Factors</summary>

- *Ke Sun, Zhanxing Zhu, Zhouchen Lin*

- `1902.11019v1` - [abs](http://arxiv.org/abs/1902.11019v1) - [pdf](http://arxiv.org/pdf/1902.11019v1)

> Most previous works usually explained adversarial examples from several specific perspectives, lacking relatively integral comprehension about this problem. In this paper, we present a systematic study on adversarial examples from three aspects: the amount of training data, task-dependent and model-specific factors. Particularly, we show that adversarial generalization (i.e. test accuracy on adversarial examples) for standard training requires more data than standard generalization (i.e. test accuracy on clean examples); and uncover the global relationship between generalization and robustness with respect to the data size especially when data is augmented by generative models. This reveals the trade-off correlation between standard generalization and robustness in limited training data regime and their consistency when data size is large enough. Furthermore, we explore how different task-dependent and model-specific factors influence the vulnerability of deep neural networks by extensive empirical analysis. Relevant recommendations on defense against adversarial attacks are provided as well. Our results outline a potential path towards the luminous and systematic understanding of adversarial examples.

</details>

<details>

<summary>2019-02-28 11:53:38 - Enhancing the Robustness of Deep Neural Networks by Boundary Conditional GAN</summary>

- *Ke Sun, Zhanxing Zhu, Zhouchen Lin*

- `1902.11029v1` - [abs](http://arxiv.org/abs/1902.11029v1) - [pdf](http://arxiv.org/pdf/1902.11029v1)

> Deep neural networks have been widely deployed in various machine learning tasks. However, recent works have demonstrated that they are vulnerable to adversarial examples: carefully crafted small perturbations to cause misclassification by the network. In this work, we propose a novel defense mechanism called Boundary Conditional GAN to enhance the robustness of deep neural networks against adversarial examples. Boundary Conditional GAN, a modified version of Conditional GAN, can generate boundary samples with true labels near the decision boundary of a pre-trained classifier. These boundary samples are fed to the pre-trained classifier as data augmentation to make the decision boundary more robust. We empirically show that the model improved by our approach consistently defenses against various types of adversarial attacks successfully. Further quantitative investigations about the improvement of robustness and visualization of decision boundaries are also provided to justify the effectiveness of our strategy. This new defense mechanism that uses boundary samples to enhance the robustness of networks opens up a new way to defense adversarial attacks consistently.

</details>

<details>

<summary>2019-02-28 20:35:22 - Cyber-physical risks of hacked Internet-connected vehicles</summary>

- *Skanda Vivek, David Yanni, Peter J. Yunker, Jesse L. Silverberg*

- `1903.00059v1` - [abs](http://arxiv.org/abs/1903.00059v1) - [pdf](http://arxiv.org/pdf/1903.00059v1)

> The integration of automotive technology with Internet-connectivity promises to both dramatically improve transportation, while simultaneously introducing the potential for new unknown risks. Internet-connected vehicles are like digital data because they can be targeted for malicious hacking. Unlike digital data, however, Internet-connected vehicles are cyber-physical systems that physically interact with each other and their environment. As such, the extension of cybersecurity concerns into the cyber-physical domain introduces new possibilities for self-organized phenomena in traffic flow. Here, we study a scenario envisioned by cybersecurity experts leading to a large number of Internet-connected vehicles being suddenly and simultaneously disabled. We investigate post-hack traffic using agent-based simulations, and discover the critical relevance of percolation for probabilistically predicting the outcomes on a multi-lane road in the immediate aftermath of a vehicle-targeted cyber attack. We develop an analytic percolation-based model to rapidly assess road conditions given the density of disabled vehicles and apply it to study the street network of Manhattan (NY, USA) revealing the city's vulnerability to this particular cyber-physical attack.

</details>


## 2019-03

<details>

<summary>2019-03-01 04:17:32 - TrojDRL: Trojan Attacks on Deep Reinforcement Learning Agents</summary>

- *Panagiota Kiourti, Kacper Wardega, Susmit Jha, Wenchao Li*

- `1903.06638v1` - [abs](http://arxiv.org/abs/1903.06638v1) - [pdf](http://arxiv.org/pdf/1903.06638v1)

> Recent work has identified that classification models implemented as neural networks are vulnerable to data-poisoning and Trojan attacks at training time. In this work, we show that these training-time vulnerabilities extend to deep reinforcement learning (DRL) agents and can be exploited by an adversary with access to the training process. In particular, we focus on Trojan attacks that augment the function of reinforcement learning policies with hidden behaviors. We demonstrate that such attacks can be implemented through minuscule data poisoning (as little as 0.025% of the training data) and in-band reward modification that does not affect the reward on normal inputs. The policies learned with our proposed attack approach perform imperceptibly similar to benign policies but deteriorate drastically when the Trojan is triggered in both targeted and untargeted settings. Furthermore, we show that existing Trojan defense mechanisms for classification tasks are not effective in the reinforcement learning setting.

</details>

<details>

<summary>2019-03-01 14:29:01 - Detecting Target-Area Link-Flooding DDoS Attacks using Traffic Analysis and Supervised Learning</summary>

- *Mostafa Rezazad, Matthias R. Brust, Mohammad Akbari, Pascal Bouvry, Ngai-Man Cheung*

- `1903.01550v1` - [abs](http://arxiv.org/abs/1903.01550v1) - [pdf](http://arxiv.org/pdf/1903.01550v1)

> A novel class of extreme link-flooding DDoS (Distributed Denial of Service) attacks is designed to cut off entire geographical areas such as cities and even countries from the Internet by simultaneously targeting a selected set of network links. The Crossfire attack is a target-area link-flooding attack, which is orchestrated in three complex phases. The attack uses a massively distributed large-scale botnet to generate low-rate benign traffic aiming to congest selected network links, so-called target links. The adoption of benign traffic, while simultaneously targeting multiple network links, makes detecting the Crossfire attack a serious challenge. In this paper, we present analytical and emulated results showing hitherto unidentified vulnerabilities in the execution of the attack, such as a correlation between coordination of the botnet traffic and the quality of the attack, and a correlation between the attack distribution and detectability of the attack. Additionally, we identified a warm-up period due to the bot synchronization. For attack detection, we report results of using two supervised machine learning approaches: Support Vector Machine (SVM) and Random Forest (RF) for classification of network traffic to normal and abnormal traffic, i.e, attack traffic. These machine learning models have been trained in various scenarios using the link volume as the main feature set.

</details>

<details>

<summary>2019-03-01 16:21:34 - Evaluating Adversarial Evasion Attacks in the Context of Wireless Communications</summary>

- *Bryse Flowers, R. Michael Buehrer, William C. Headley*

- `1903.01563v1` - [abs](http://arxiv.org/abs/1903.01563v1) - [pdf](http://arxiv.org/pdf/1903.01563v1)

> Recent advancements in radio frequency machine learning (RFML) have demonstrated the use of raw in-phase and quadrature (IQ) samples for multiple spectrum sensing tasks. Yet, deep learning techniques have been shown, in other applications, to be vulnerable to adversarial machine learning (ML) techniques, which seek to craft small perturbations that are added to the input to cause a misclassification. The current work differentiates the threats that adversarial ML poses to RFML systems based on where the attack is executed from: direct access to classifier input, synchronously transmitted over the air (OTA), or asynchronously transmitted from a separate device. Additionally, the current work develops a methodology for evaluating adversarial success in the context of wireless communications, where the primary metric of interest is bit error rate and not human perception, as is the case in image recognition. The methodology is demonstrated using the well known Fast Gradient Sign Method to evaluate the vulnerabilities of raw IQ based Automatic Modulation Classification and concludes RFML is vulnerable to adversarial examples, even in OTA attacks. However, RFML domain specific receiver effects, which would be encountered in an OTA attack, can present significant impairments to adversarial evasion.

</details>

<details>

<summary>2019-03-01 19:06:11 - On the Security of Cryptographic Protocols Using the Little Theorem of Witness Functions</summary>

- *Jaouhar Fattahi, Mohamed Mejri, Emil Pricop*

- `1903.00499v1` - [abs](http://arxiv.org/abs/1903.00499v1) - [pdf](http://arxiv.org/pdf/1903.00499v1)

> In this paper, we show how practical the little theorem of witness functions is in detecting security flaws in some category of cryptographic protocols. We convey a formal analysis of the Needham-Schroeder symmetric-key protocol in the theory of witness functions. We show how it helps to teach about a security vulnerability in a given step of this protocol where the value of security of a particular sensitive ticket in a sent message unexpectedly plummets compared with its value when received. This vulnerability may be exploited by an intruder to mount a replay attack as described by Denning and Sacco.

</details>

<details>

<summary>2019-03-02 00:38:38 - PuVAE: A Variational Autoencoder to Purify Adversarial Examples</summary>

- *Uiwon Hwang, Jaewoo Park, Hyemi Jang, Sungroh Yoon, Nam Ik Cho*

- `1903.00585v1` - [abs](http://arxiv.org/abs/1903.00585v1) - [pdf](http://arxiv.org/pdf/1903.00585v1)

> Deep neural networks are widely used and exhibit excellent performance in many areas. However, they are vulnerable to adversarial attacks that compromise the network at the inference time by applying elaborately designed perturbation to input data. Although several defense methods have been proposed to address specific attacks, other attack methods can circumvent these defense mechanisms. Therefore, we propose Purifying Variational Autoencoder (PuVAE), a method to purify adversarial examples. The proposed method eliminates an adversarial perturbation by projecting an adversarial example on the manifold of each class, and determines the closest projection as a purified sample. We experimentally illustrate the robustness of PuVAE against various attack methods without any prior knowledge. In our experiments, the proposed method exhibits performances competitive with state-of-the-art defense methods, and the inference time is approximately 130 times faster than that of Defense-GAN that is the state-of-the art purifier model.

</details>

<details>

<summary>2019-03-02 14:27:57 - Safe Artificial General Intelligence via Distributed Ledger Technology</summary>

- *Kristen W. Carlson*

- `1902.03689v2` - [abs](http://arxiv.org/abs/1902.03689v2) - [pdf](http://arxiv.org/pdf/1902.03689v2)

> Background. Expert observers and artificial intelligence (AI) progression metrics indicate AI will exceed human intelligence within a few decades. Whether general AI that exceeds human capabilities (AGI) will be the single greatest boon in history or a disaster is unknown. No proofs exist that AGI will benefit humans or that AGI will not harm or eliminate humans.   Objective. I propose a set of logically distinct conceptual components that are necessary and sufficient to 1) ensure that most known AGI scenarios will not harm humanity and 2) robustly align AGI values and goals with human values.   Methods. By systematically addressing each pathway category to malevolent AI we can induce the methods/axioms required to redress the category.   Results and Discussion. Distributed ledger technology (DLT, blockchain) is integral to this proposal, e.g. to reduce the probability of hacking, provide an audit trail to detect and correct errors or identify components causing vulnerability or failure and replace them or shut them down remotely and/or automatically, and to separate and balance key AGI components via decentralized apps (dApps). Smart contracts based on DLT are necessary to address evolution of AI that will be too fast for human monitoring and intervention.   The proposed axioms. 1) Access to technology by market license. 2) Transparent ethics embodied in DLT. 3) Morality encrypted via DLT. 4) Behavior control structure with values (ethics) at roots. 5) Individual bar-code identification of all critical components. 6) Configuration Item (from business continuity/disaster recovery planning). 7) Identity verification secured via DLT. 8) Smart automated contracts based on DLT. 9) Decentralized applications - AI software code modules encrypted via DLT. 10) Audit trail of component usage stored via DLT. 11) Social ostracism (denial of societal resources) augmented by DLT petitions.

</details>

<details>

<summary>2019-03-02 22:49:20 - The Historical Perspective of Botnet tools</summary>

- *Maxwell Scale Uwadia Osagie, Osatohanmwen Enagbonma, Amanda Iriagbonse Inyang*

- `1904.00948v1` - [abs](http://arxiv.org/abs/1904.00948v1) - [pdf](http://arxiv.org/pdf/1904.00948v1)

> Bot as it is popularly called is an inherent attributes of botnet tool. Botnet is a group of malicious tools acting as an entity. Furthermore, history has it that the aim of what gave rise to botnet was the idea to simplify the method of message exchange within networking platform. However, this has led to several botnet tools ravaging the server environments in recent times. The working principle of these botnet tools is to get client systems that are vulnerable and thereafter, steal valuable credentials. This work is part of a comprehensive research work into botnet detection mechanism but, on this paper it primarily look at how botnet as threat tool began, the trend since inception and as well as few approaches that have been used to curb it.

</details>

<details>

<summary>2019-03-04 21:15:50 - Survey on Vehicular Ad Hoc Networks and Its Access Technologies Security Vulnerabilities and Countermeasures</summary>

- *Kaveh Bakhsh Kelarestaghi, Mahsa Foruhandeh, Kevin Heaslip, Ryan Gerdes*

- `1903.01541v1` - [abs](http://arxiv.org/abs/1903.01541v1) - [pdf](http://arxiv.org/pdf/1903.01541v1)

> In this study, we attempt to add to the literature of Connected and Automated Vehicle (CAV) security by incorporating the security vulnerabilities and countermeasures of the Vehicular Ad hoc Networks (VANETs) and their access technologies. Compounding VANETs and modern vehicles will allow adversaries to gain access to the in-vehicle networks and take control of vehicles remotely to use them as a target or a foothold. Extensive attention has been given to the security breaches in VANETs and in-vehicle networks in literature but there is a gap in literature to assess the security vulnerabilities associated with VANETs access technologies. That is, in this paper we contribute to the CAV security literature in threefold. First, we synthesize the current literature in order to investigate security attacks and countermeasures on VANETs as an ad hoc network. Second, we survey security challenges that emerge from application of different VANETs access technologies. To augment this discussion, we investigate security solutions to thwart adversaries to compromise the access technologies. Third, we provide a detailed comparison of different access technologies performance, security challenges and propound heterogeneous technologies to achieve the highest security and best performance in VANETs. These access technologies extend from DSRC, Satellite Radio, and Bluetooth to VLC and 5G. The outcome of this study is of critical importance, because of two main reasons: (1) independent studies on security of VANETs on different strata need to come together and to be covered from a whole end-to-end system perspective, (2) adversaries taking control of the VANETs entities will compromise the safety, privacy, and security of the road users and will be followed by legal exposures, as well as data, time and monetary losses.

</details>

<details>

<summary>2019-03-05 20:10:11 - Risk Assessment of Autonomous Vehicles Using Bayesian Defense Graphs</summary>

- *Ali Behfarnia, Ali Eslami*

- `1903.02034v1` - [abs](http://arxiv.org/abs/1903.02034v1) - [pdf](http://arxiv.org/pdf/1903.02034v1)

> Recent developments have made autonomous vehicles (AVs) closer to hitting our roads. However, their security is still a major concern among drivers as well as manufacturers. Although some work has been done to identify threats and possible solutions, a theoretical framework is needed to measure the security of AVs. In this paper, a simple security model based on defense graphs is proposed to quantitatively assess the likelihood of threats on components of an AV in the presence of available countermeasures. A Bayesian network (BN) analysis is then applied to obtain the associated security risk. In a case study, the model and the analysis are studied for GPS spoofing attacks to demonstrate the effectiveness of the proposed approach for a highly vulnerable component.

</details>

<details>

<summary>2019-03-05 20:56:05 - A Data-Driven Approach for Predicting Vegetation-Related Outages in Power Distribution Systems</summary>

- *Milad Doostan, Reza Sohrabi, Badrul Chowdhury*

- `1807.06180v2` - [abs](http://arxiv.org/abs/1807.06180v2) - [pdf](http://arxiv.org/pdf/1807.06180v2)

> This paper presents a novel data-driven approach for predicting the number of vegetation-related outages that occur in power distribution systems on a monthly basis. In order to develop an approach that is able to successfully fulfill this objective, there are two main challenges that ought to be addressed. The first challenge is to define the extent of the target area. An unsupervised machine learning approach is proposed to overcome this difficulty. The second challenge is to correctly identify the main causes of vegetation-related outages and to thoroughly investigate their nature. In this paper, these outages are categorized into two main groups: growth-related and weather-related outages, and two types of models, namely time series and non-linear machine learning regression models are proposed to conduct the prediction tasks, respectively. Moreover, various features that can explain the variability in vegetation-related outages are engineered and employed. Actual outage data, obtained from a major utility in the U.S., in addition to different types of weather and geographical data are utilized to build the proposed approach. Finally, by utilizing various time series models and machine learning methods, a comprehensive case study is carried out to demonstrate how the proposed approach can be used to successfully predict the number of vegetation-related outages and to help decision-makers to detect vulnerable zones in their systems.

</details>

<details>

<summary>2019-03-06 12:59:01 - Improving SIEM for Critical SCADA Water Infrastructures Using Machine Learning</summary>

- *Hanan Hindy, David Brosset, Ethan Bayne, Amar Seeam, Xavier Bellekens*

- `1904.05724v1` - [abs](http://arxiv.org/abs/1904.05724v1) - [pdf](http://arxiv.org/pdf/1904.05724v1)

> Network Control Systems (NAC) have been used in many industrial processes. They aim to reduce the human factor burden and efficiently handle the complex process and communication of those systems. Supervisory control and data acquisition (SCADA) systems are used in industrial, infrastructure and facility processes (e.g. manufacturing, fabrication, oil and water pipelines, building ventilation, etc.) Like other Internet of Things (IoT) implementations, SCADA systems are vulnerable to cyber-attacks, therefore, a robust anomaly detection is a major requirement. However, having an accurate anomaly detection system is not an easy task, due to the difficulty to differentiate between cyber-attacks and system internal failures (e.g. hardware failures). In this paper, we present a model that detects anomaly events in a water system controlled by SCADA. Six Machine Learning techniques have been used in building and evaluating the model. The model classifies different anomaly events including hardware failures (e.g. sensor failures), sabotage and cyber-attacks (e.g. DoS and Spoofing). Unlike other detection systems, our proposed work focuses on notifying the operator when an anomaly occurs with a probability of the event occurring. This additional information helps in accelerating the mitigation process. The model is trained and tested using a real-world dataset.

</details>

<details>

<summary>2019-03-06 18:57:35 - A Survey on Modality Characteristics, Performance Evaluation Metrics, and Security for Traditional and Wearable Biometric Systems</summary>

- *Aditya Sundararajan, Arif I. Sarwat, Alexander Pons*

- `1903.02548v1` - [abs](http://arxiv.org/abs/1903.02548v1) - [pdf](http://arxiv.org/pdf/1903.02548v1)

> Biometric research is directed increasingly towards Wearable Biometric Systems (WBS) for user authentication and identification. However, prior to engaging in WBS research, how their operational dynamics and design considerations differ from those of Traditional Biometric Systems (TBS) must be understood. While the current literature is cognizant of those differences, there is no effective work that summarizes the factors where TBS and WBS differ, namely, their modality characteristics, performance, security and privacy. To bridge the gap, this paper accordingly reviews and compares the key characteristics of modalities, contrasts the metrics used to evaluate system performance, and highlights the divergence in critical vulnerabilities, attacks and defenses for TBS and WBS. It further discusses how these factors affect the design considerations for WBS, the open challenges and future directions of research in these areas. In doing so, the paper provides a big-picture overview of the important avenues of challenges and potential solutions that researchers entering the field should be aware of. Hence, this survey aims to be a starting point for researchers in comprehending the fundamental differences between TBS and WBS before understanding the core challenges associated with WBS and its design.

</details>

<details>

<summary>2019-03-06 19:48:52 - Distributed Byzantine Tolerant Stochastic Gradient Descent in the Era of Big Data</summary>

- *Richeng Jin, Xiaofan He, Huaiyu Dai*

- `1902.10336v3` - [abs](http://arxiv.org/abs/1902.10336v3) - [pdf](http://arxiv.org/pdf/1902.10336v3)

> The recent advances in sensor technologies and smart devices enable the collaborative collection of a sheer volume of data from multiple information sources. As a promising tool to efficiently extract useful information from such big data, machine learning has been pushed to the forefront and seen great success in a wide range of relevant areas such as computer vision, health care, and financial market analysis. To accommodate the large volume of data, there is a surge of interest in the design of distributed machine learning, among which stochastic gradient descent (SGD) is one of the mostly adopted methods. Nonetheless, distributed machine learning methods may be vulnerable to Byzantine attack, in which the adversary can deliberately share falsified information to disrupt the intended machine learning procedures. Therefore, two asynchronous Byzantine tolerant SGD algorithms are proposed in this work, in which the honest collaborative workers are assumed to store the model parameters derived from their own local data and use them as the ground truth. The proposed algorithms can deal with an arbitrary number of Byzantine attackers and are provably convergent. Simulation results based on a real-world dataset are presented to verify the theoretical results and demonstrate the effectiveness of the proposed algorithms.

</details>

<details>

<summary>2019-03-06 20:21:11 - Attack Graph Obfuscation</summary>

- *Rami Puzis, Hadar Polad, Bracha Shapira*

- `1903.02601v1` - [abs](http://arxiv.org/abs/1903.02601v1) - [pdf](http://arxiv.org/pdf/1903.02601v1)

> Before executing an attack, adversaries usually explore the victim's network in an attempt to infer the network topology and identify vulnerabilities in the victim's servers and personal computers. Falsifying the information collected by the adversary post penetration may significantly slower lateral movement and increase the amount of noise generated within the victim's network. We investigate the effect of fake vulnerabilities within a real enterprise network on the attacker performance. We use the attack graphs to model the path of an attacker making its way towards a target in a given network. We use combinatorial optimization in order to find the optimal assignments of fake vulnerabilities. We demonstrate the feasibility of our deception-based defense by presenting results of experiments with a large scale real network. We show that adding fake vulnerabilities forces the adversary to invest a significant amount of effort, in terms of time and exploitability cost.

</details>

<details>

<summary>2019-03-08 14:47:45 - A Study on Smart Online Frame Forging Attacks against Video Surveillance System</summary>

- *Deeraj Nagothu, Jacob Schwell, Yu Chen, Erik Blasch, Sencun Zhu*

- `1903.03473v1` - [abs](http://arxiv.org/abs/1903.03473v1) - [pdf](http://arxiv.org/pdf/1903.03473v1)

> Video Surveillance Systems (VSS) have become an essential infrastructural element of smart cities by increasing public safety and countering criminal activities. A VSS is normally deployed in a secure network to prevent access from unauthorized personnel. Compared to traditional systems that continuously record video regardless of the actions in the frame, a smart VSS has the capability of capturing video data upon motion detection or object detection, and then extracts essential information and send to users. This increasing design complexity of the surveillance system, however, also introduces new security vulnerabilities. In this work, a smart, real-time frame duplication attack is investigated. We show the feasibility of forging the video streams in real-time as the camera's surroundings change. The generated frames are compared constantly and instantly to identify changes in the pixel values that could represent motion detection or changes in light intensities outdoors. An attacker (intruder) can remotely trigger the replay of some previously duplicated video streams manually or automatically, via a special quick response (QR) code or when the face of an intruder appears in the camera field of view. A detection technique is proposed by leveraging the real-time electrical network frequency (ENF) reference database to match with the power grid frequency.

</details>

<details>

<summary>2019-03-09 08:18:34 - SAFECHAIN: Securing Trigger-Action Programming from Attack Chains (Extended Technical Report)</summary>

- *Kai-Hsiang Hsu, Yu-Hsi Chiang, Hsu-Chun Hsiao*

- `1903.03760v1` - [abs](http://arxiv.org/abs/1903.03760v1) - [pdf](http://arxiv.org/pdf/1903.03760v1)

> The proliferation of Internet of Things (IoT) is reshaping our lifestyle. With IoT sensors and devices communicating with each other via the Internet, people can customize automation rules to meet their needs. Unless carefully defined, however, such rules can easily become points of security failure as the number of devices and complexity of rules increase. Device owners may end up unintentionally providing access or revealing private information to unauthorized entities due to complex chain reactions among devices. Prior work on trigger-action programming either focuses on conflict resolution or usability issues, or fails to accurately and efficiently detect such attack chains. This paper explores security vulnerabilities when users have the freedom to customize automation rules using trigger-action programming. We define two broad classes of attack--privilege escalation and privacy leakage--and present a practical model-checking-based system called SAFECHAIN that detects hidden attack chains exploiting the combination of rules. Built upon existing model-checking techniques, SAFECHAIN identifies attack chains by modeling the IoT ecosystem as a Finite State Machine. To improve practicability, SAFECHAIN avoids the need to accurately model an environment by frequently re-checking the automation rules given the current states, and employs rule-aware optimizations to further reduce overhead. Our comparative analysis shows that SAFECHAIN can efficiently and accurately identify attack chains, and our prototype implementation of SAFECHAIN can verify 100 rules in less than one second with no false positives.

</details>

<details>

<summary>2019-03-09 13:36:30 - Quantifying Dynamic Leakage: Complexity Analysis and Model Counting-based Calculation</summary>

- *Bao Trung Chu, Kenji Hashimoto, Hiroyuki Seki*

- `1903.03802v1` - [abs](http://arxiv.org/abs/1903.03802v1) - [pdf](http://arxiv.org/pdf/1903.03802v1)

> A program is non-interferent if it leaks no secret information to an observable output. However, non-interference is too strict in many practical cases and quantitative information flow (QIF) has been proposed and studied in depth. Originally, QIF is defined as the average of leakage amount of secret information over all executions of a program. However, a vulnerable program that has executions leaking the whole secret but has the small average leakage could be considered as secure. This counter-intuition raises a need for a new definition of information leakage of a particular run, i.e., dynamic leakage. As discussed in [5], entropy-based definitions do not work well for quantifying information leakage dynamically; Belief-based definition on the other hand is appropriate for deterministic programs, however, it is not appropriate for probabilistic ones. In this paper, we propose new simple notions of dynamic leakage based on entropy which are compatible with existing QIF definitions for deterministic programs, and yet reasonable for probabilistic programs in the sense of [5]. We also investigated the complexity of computing the proposed dynamic leakage for three classes of Boolean programs. We also implemented a tool for QIF calculation using model counting tools for Boolean formulae. Experimental results on popular benchmarks of QIF research show the flexibility of our framework. Finally, we discuss the improvement of performance and scalability of the proposed method as well as an extension to more general cases.

</details>

<details>

<summary>2019-03-11 20:45:33 - BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain</summary>

- *Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg*

- `1708.06733v2` - [abs](http://arxiv.org/abs/1708.06733v2) - [pdf](http://arxiv.org/pdf/1708.06733v2)

> Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \emph{BadNet}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of {25}\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.

</details>

<details>

<summary>2019-03-12 16:56:17 - Detection of LDDoS Attacks Based on TCP Connection Parameters</summary>

- *Michael Siracusano, Stavros Shiaeles, Bogdan Ghita*

- `1904.01508v1` - [abs](http://arxiv.org/abs/1904.01508v1) - [pdf](http://arxiv.org/pdf/1904.01508v1)

> Low-rate application layer distributed denial of service (LDDoS) attacks are both powerful and stealthy. They force vulnerable webservers to open all available connections to the adversary, denying resources to real users. Mitigation advice focuses on solutions that potentially degrade quality of service for legitimate connections. Furthermore, without accurate detection mechanisms, distributed attacks can bypass these defences. A methodology for detection of LDDoS attacks, based on characteristics of malicious TCP flows, is proposed within this paper. Research will be conducted using combinations of two datasets: one generated from a simulated network, the other from the publically available CIC DoS dataset. Both contain the attacks slowread, slowheaders and slowbody, alongside legitimate web browsing. TCP flow features are extracted from all connections. Experimentation was carried out using six supervised AI algorithms to categorise attack from legitimate flows. Decision trees and k-NN accurately classified up to 99.99% of flows, with exceptionally low false positive and false negative rates, demonstrating the potential of AI in LDDoS detection.

</details>

<details>

<summary>2019-03-12 19:13:12 - Simple Physical Adversarial Examples against End-to-End Autonomous Driving Models</summary>

- *Adith Boloor, Xin He, Christopher Gill, Yevgeniy Vorobeychik, Xuan Zhang*

- `1903.05157v1` - [abs](http://arxiv.org/abs/1903.05157v1) - [pdf](http://arxiv.org/pdf/1903.05157v1)

> Recent advances in machine learning, especially techniques such as deep neural networks, are promoting a range of high-stakes applications, including autonomous driving, which often relies on deep learning for perception. While deep learning for perception has been shown to be vulnerable to a host of subtle adversarial manipulations of images, end-to-end demonstrations of successful attacks, which manipulate the physical environment and result in physical consequences, are scarce. Moreover, attacks typically involve carefully constructed adversarial examples at the level of pixels. We demonstrate the first end-to-end attacks on autonomous driving in simulation, using simple physically realizable attacks: the painting of black lines on the road. These attacks target deep neural network models for end-to-end autonomous driving control. A systematic investigation shows that such attacks are surprisingly easy to engineer, and we describe scenarios (e.g., right turns) in which they are highly effective, and others that are less vulnerable (e.g., driving straight). Further, we use network deconvolution to demonstrate that the attacks succeed by inducing activation patterns similar to entirely different scenarios used in training.

</details>

<details>

<summary>2019-03-13 08:14:51 - A Demand-Side Viewpoint to Software Vulnerabilities in WordPress Plugins</summary>

- *Jukka Ruohonen*

- `1812.05293v3` - [abs](http://arxiv.org/abs/1812.05293v3) - [pdf](http://arxiv.org/pdf/1812.05293v3)

> WordPress has long been the most popular content management system (CMS). This CMS powers millions and millions of websites. Although WordPress has had a particularly bad track record in terms of security, in recent years many of the well-known security risks have transmuted from the core WordPress to the numerous plugins and themes written for the CMS. Given this background, the paper analyzes known software vulnerabilities discovered from WordPress plugins. A demand-side viewpoint was used to motivate the analysis; the basic hypothesis is that plugins with large installation bases have been affected by multiple vulnerabilities. As the hypothesis also holds according to the empirical results, the paper contributes to the recent discussion about common security folklore. A few general insights are also provided about the relation between software vulnerabilities and software maintenance.

</details>

<details>

<summary>2019-03-14 02:33:23 - Preventing the attempts of abusing cheap-hosting Web-servers for monetization attacks</summary>

- *Van-Linh Nguyen, Po-Ching Lin, Ren-Hung Hwang*

- `1903.05470v2` - [abs](http://arxiv.org/abs/1903.05470v2) - [pdf](http://arxiv.org/pdf/1903.05470v2)

> Over the past decades, the web is always one of the most popular targets of hackers. Today, along with the popular usage of open sources such as Wordpress and Joomla, the explosion of the vulnerabilities in such frameworks causes the websites using them to face numerous security threats. Unfortunately, many clients and small companies may not be aware of these serious security threats and call a rescuer only when the website is hacked, compromised, or blocked by the search engines. In this paper, we present an effective counter against such threats, including monetization attempts in the less valuable targets such as small websites.

</details>

<details>

<summary>2019-03-18 07:33:35 - Generating Adversarial Examples With Conditional Generative Adversarial Net</summary>

- *Ping Yu, Kaitao Song, Jianfeng Lu*

- `1903.07282v1` - [abs](http://arxiv.org/abs/1903.07282v1) - [pdf](http://arxiv.org/pdf/1903.07282v1)

> Recently, deep neural networks have significant progress and successful application in various fields, but they are found vulnerable to attack instances, e.g., adversarial examples. State-of-art attack methods can generate attack images by adding small perturbation to the source image. These attack images can fool the classifier but have little impact to human. Therefore, such attack instances are difficult to generate by searching the feature space. How to design an effective and robust generating method has become a spotlight. Inspired by adversarial examples, we propose two novel generative models to produce adaptive attack instances directly, in which conditional generative adversarial network is adopted and distinctive strategy is designed for training. Compared with the common method, such as Fast Gradient Sign Method, our models can reduce the generating cost and improve robustness and has about one fifth running time for producing attack instance.

</details>

<details>

<summary>2019-03-18 20:10:13 - Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems</summary>

- *Hadi Abdullah, Washington Garcia, Christian Peeters, Patrick Traynor, Kevin R. B. Butler, Joseph Wilson*

- `1904.05734v1` - [abs](http://arxiv.org/abs/1904.05734v1) - [pdf](http://arxiv.org/pdf/1904.05734v1)

> Voice Processing Systems (VPSes), now widely deployed, have been made significantly more accurate through the application of recent advances in machine learning. However, adversarial machine learning has similarly advanced and has been used to demonstrate that VPSes are vulnerable to the injection of hidden commands - audio obscured by noise that is correctly recognized by a VPS but not by human beings. Such attacks, though, are often highly dependent on white-box knowledge of a specific machine learning model and limited to specific microphones and speakers, making their use across different acoustic hardware platforms (and thus their practicality) limited. In this paper, we break these dependencies and make hidden command attacks more practical through model-agnostic (blackbox) attacks, which exploit knowledge of the signal processing algorithms commonly used by VPSes to generate the data fed into machine learning systems. Specifically, we exploit the fact that multiple source audio samples have similar feature vectors when transformed by acoustic feature extraction algorithms (e.g., FFTs). We develop four classes of perturbations that create unintelligible audio and test them against 12 machine learning models, including 7 proprietary models (e.g., Google Speech API, Bing Speech API, IBM Speech API, Azure Speaker API, etc), and demonstrate successful attacks against all targets. Moreover, we successfully use our maliciously generated audio samples in multiple hardware configurations, demonstrating effectiveness across both models and real systems. In so doing, we demonstrate that domain-specific knowledge of audio signal processing represents a practical means of generating successful hidden voice command attacks.

</details>

<details>

<summary>2019-03-19 10:33:34 - A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software</summary>

- *Serena E. Ponta, Henrik Plate, Antonino Sabetta, Michele Bezzi, Cédric Dangremont*

- `1902.02595v3` - [abs](http://arxiv.org/abs/1902.02595v3) - [pdf](http://arxiv.org/pdf/1902.02595v3)

> Advancing our understanding of software vulnerabilities, automating their identification, the analysis of their impact, and ultimately their mitigation is necessary to enable the development of software that is more secure. While operating a vulnerability assessment tool that we developed and that is currently used by hundreds of development units at SAP, we manually collected and curated a dataset of vulnerabilities of open-source software and the commits fixing them. The data was obtained both from the National Vulnerability Database (NVD) and from project-specific Web resources that we monitor on a continuous basis. From that data, we extracted a dataset that maps 624 publicly disclosed vulnerabilities affecting 205 distinct open-source Java projects, used in SAP products or internal tools, onto the 1282 commits that fix them. Out of 624 vulnerabilities, 29 do not have a CVE identifier at all and 46, which do have a CVE identifier assigned by a numbering authority, are not available in the NVD yet. The dataset is released under an open-source license, together with supporting scripts that allow researchers to automatically retrieve the actual content of the commits from the corresponding repositories and to augment the attributes available for each instance. Also, these scripts allow to complement the dataset with additional instances that are not security fixes (which is useful, for example, in machine learning applications). Our dataset has been successfully used to train classifiers that could automatically identify security-relevant commits in code repositories. The release of this dataset and the supporting code as open-source will allow future research to be based on data of industrial relevance; also, it represents a concrete step towards making the maintenance of this dataset a shared effort involving open-source communities, academia, and the industry.

</details>

<details>

<summary>2019-03-20 22:48:49 - RoPAD: Robust Presentation Attack Detection through Unsupervised Adversarial Invariance</summary>

- *Ayush Jaiswal, Shuai Xia, Iacopo Masi, Wael AbdAlmageed*

- `1903.03691v2` - [abs](http://arxiv.org/abs/1903.03691v2) - [pdf](http://arxiv.org/pdf/1903.03691v2)

> For enterprise, personal and societal applications, there is now an increasing demand for automated authentication of identity from images using computer vision. However, current authentication technologies are still vulnerable to presentation attacks. We present RoPAD, an end-to-end deep learning model for presentation attack detection that employs unsupervised adversarial invariance to ignore visual distractors in images for increased robustness and reduced overfitting. Experiments show that the proposed framework exhibits state-of-the-art performance on presentation attack detection on several benchmark datasets.

</details>

<details>

<summary>2019-03-24 20:20:46 - Highly Accelerated Multishot EPI through Synergistic Machine Learning and Joint Reconstruction</summary>

- *Berkin Bilgic, Itthi Chatnuntawech, Mary Kate Manhard, Qiyuan Tian, Congyu Liao, Stephen F. Cauley, Susie Y. Huang, Jonathan R. Polimeni, Lawrence L. Wald, Kawin Setsompop*

- `1808.02814v3` - [abs](http://arxiv.org/abs/1808.02814v3) - [pdf](http://arxiv.org/pdf/1808.02814v3)

> Purpose: To introduce a combined machine learning (ML) and physics-based image reconstruction framework that enables navigator-free, highly accelerated multishot echo planar imaging (msEPI), and demonstrate its application in high-resolution structural and diffusion imaging.   Methods: Singleshot EPI is an efficient encoding technique, but does not lend itself well to high-resolution imaging due to severe distortion artifacts and blurring. While msEPI can mitigate these artifacts, high-quality msEPI has been elusive because of phase mismatch arising from shot-to-shot variations which preclude the combination of the multiple-shot data into a single image. We employ deep learning to obtain an interim image with minimal artifacts, which permits estimation of image phase variations due to shot-to-shot changes. These variations are then included in a Joint Virtual Coil Sensitivity Encoding (JVC-SENSE) reconstruction to utilize data from all shots and improve upon the ML solution.   Results: Our combined ML + physics approach enabled Rinplane x MultiBand (MB) = 8x2-fold acceleration using 2 EPI-shots for multi-echo imaging, so that whole-brain T2 and T2* parameter maps could be derived from an 8.3 sec acquisition at 1x1x3mm3 resolution. This has also allowed high-resolution diffusion imaging with high geometric fidelity using 5-shots at Rinplane x MB = 9x2-fold acceleration. To make these possible, we extended the state-of-the-art MUSSELS reconstruction technique to Simultaneous MultiSlice (SMS) encoding and used it as an input to our ML network.   Conclusion: Combination of ML and JVC-SENSE enabled navigator-free msEPI at higher accelerations than previously possible while using fewer shots, with reduced vulnerability to poor generalizability and poor acceptance of end-to-end ML approaches.

</details>

<details>

<summary>2019-03-25 15:23:24 - Understanding Childhood Vulnerability in The City of Surrey</summary>

- *Cody Griffith, Varoon Mathur, Catherine Lin, Kevin Zhu*

- `1903.09639v1` - [abs](http://arxiv.org/abs/1903.09639v1) - [pdf](http://arxiv.org/pdf/1903.09639v1)

> Understanding the community conditions that best support universal access and improved childhood outcomes allows ultimately to improve decision-making in the areas of planning and investment across the early stages of childhood development. Here we describe two different data-driven approaches to visualizing the lived experiences of children throughout the City of Surrey, combining data derived from both public and private sources. In one approach, we find specifically that the Early Development Instrument measuring childhood vulnerabilities across varying domains can be used to cluster neighborhoods, and that census variables can help explain similarities between neighborhoods within these clusters. In our second approach, we use program registration data from the City of Surrey's Community and Recreation Services Division. We also find a critical age of entry and exit for each program related to early childhood development and beyond, and find that certain neighborhoods and recreational programs have larger retention rates than others. This report details the journey of using data to tell the story of these neighborhoods, and provides a lens to which community initiatives can be strategically crafted through their use.

</details>

<details>

<summary>2019-03-25 17:29:52 - Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness</summary>

- *Jörn-Henrik Jacobsen, Jens Behrmannn, Nicholas Carlini, Florian Tramèr, Nicolas Papernot*

- `1903.10484v1` - [abs](http://arxiv.org/abs/1903.10484v1) - [pdf](http://arxiv.org/pdf/1903.10484v1)

> Adversarial examples are malicious inputs crafted to cause a model to misclassify them. Their most common instantiation, "perturbation-based" adversarial examples introduce changes to the input that leave its true label unchanged, yet result in a different model prediction. Conversely, "invariance-based" adversarial examples insert changes to the input that leave the model's prediction unaffected despite the underlying input's label having changed.   In this paper, we demonstrate that robustness to perturbation-based adversarial examples is not only insufficient for general robustness, but worse, it can also increase vulnerability of the model to invariance-based adversarial examples. In addition to analytical constructions, we empirically study vision classifiers with state-of-the-art robustness to perturbation-based adversaries constrained by an $\ell_p$ norm. We mount attacks that exploit excessive model invariance in directions relevant to the task, which are able to find adversarial examples within the $\ell_p$ ball. In fact, we find that classifiers trained to be $\ell_p$-norm robust are more vulnerable to invariance-based adversarial examples than their undefended counterparts.   Excessive invariance is not limited to models trained to be robust to perturbation-based $\ell_p$-norm adversaries. In fact, we argue that the term adversarial example is used to capture a series of model limitations, some of which may not have been discovered yet. Accordingly, we call for a set of precise definitions that taxonomize and address each of these shortcomings in learning.

</details>

<details>

<summary>2019-03-25 20:28:19 - RowHammer and Beyond</summary>

- *Onur Mutlu*

- `1903.11056v1` - [abs](http://arxiv.org/abs/1903.11056v1) - [pdf](http://arxiv.org/pdf/1903.11056v1)

> We will discuss the RowHammer problem in DRAM, which is a prime (and likely the first) example of how a circuit-level failure mechanism in Dynamic Random Access Memory (DRAM) can cause a practical and widespread system security vulnerability. RowHammer is the phenomenon that repeatedly accessing a row in a modern DRAM chip predictably causes errors in physically-adjacent rows. It is caused by a hardware failure mechanism called read disturb errors. Building on our initial fundamental work that appeared at ISCA 2014, Google Project Zero demonstrated that this hardware phenomenon can be exploited by user-level programs to gain kernel privileges. Many other recent works demonstrated other attacks exploiting RowHammer, including remote takeover of a server vulnerable to RowHammer. We will analyze the root causes of the problem and examine solution directions. We will also discuss what other problems may be lurking in DRAM and other types of memories, e.g., NAND flash and Phase Change Memory, which can potentially threaten the foundations of reliable and secure systems, as the memory technologies scale to higher densities.

</details>

<details>

<summary>2019-03-26 10:04:37 - Blockchain Solutions for Forensic Evidence Preservation in IoT Environments</summary>

- *Sotirios Brotsis, Nicholas Kolokotronis, Konstantinos Limniotis, Stavros Shiaeles, Dimitris Kavallieros, Emanuele Bellini, Clement Pavue*

- `1903.10770v1` - [abs](http://arxiv.org/abs/1903.10770v1) - [pdf](http://arxiv.org/pdf/1903.10770v1)

> The technological evolution brought by the Internet of things (IoT) comes with new forms of cyber-attacks exploiting the complexity and heterogeneity of IoT networks, as well as, the existence of many vulnerabilities in IoT devices. The detection of compromised devices, as well as the collection and preservation of evidence regarding alleged malicious behavior in IoT networks emerge as a areas of high priority. This paper presents a blockchain-based solution, which is designed for the smart home domain, dealing with the collection and preservation of digital forensic evidence. The system utilizes a private forensic evidence database, where the captured evidence is stored, along with a permissioned blockchain that allows providing security services like integrity, authentication, and non-repudiation, so that the evidence can be used in a court of law. The blockchain stores evidences' metadata, which are critical for providing the aforementioned services, and interacts via smart contracts with the different entities involved in an investigation process, including Internet service providers, law enforcement agencies and prosecutors. A high-level architecture of the blockchain-based solution is presented that allows tackling the unique challenges posed by the need for digitally handling forensic evidence collected from IoT networks.

</details>

<details>

<summary>2019-03-26 12:43:09 - Musical Tempo and Key Estimation using Convolutional Neural Networks with Directional Filters</summary>

- *Hendrik Schreiber, Meinard Müller*

- `1903.10839v1` - [abs](http://arxiv.org/abs/1903.10839v1) - [pdf](http://arxiv.org/pdf/1903.10839v1)

> In this article we explore how the different semantics of spectrograms' time and frequency axes can be exploited for musical tempo and key estimation using Convolutional Neural Networks (CNN). By addressing both tasks with the same network architectures ranging from shallow, domain-specific approaches to deep variants with directional filters, we show that axis-aligned architectures perform similarly well as common VGG-style networks developed for computer vision, while being less vulnerable to confounding factors and requiring fewer model parameters.

</details>

<details>

<summary>2019-03-27 04:25:57 - On Attribution of Recurrent Neural Network Predictions via Additive Decomposition</summary>

- *Mengnan Du, Ninghao Liu, Fan Yang, Shuiwang Ji, Xia Hu*

- `1903.11245v1` - [abs](http://arxiv.org/abs/1903.11245v1) - [pdf](http://arxiv.org/pdf/1903.11245v1)

> RNN models have achieved the state-of-the-art performance in a wide range of text mining tasks. However, these models are often regarded as black-boxes and are criticized due to the lack of interpretability. In this paper, we enhance the interpretability of RNNs by providing interpretable rationales for RNN predictions. Nevertheless, interpreting RNNs is a challenging problem. Firstly, unlike existing methods that rely on local approximation, we aim to provide rationales that are more faithful to the decision making process of RNN models. Secondly, a flexible interpretation method should be able to assign contribution scores to text segments of varying lengths, instead of only to individual words. To tackle these challenges, we propose a novel attribution method, called REAT, to provide interpretations to RNN predictions. REAT decomposes the final prediction of a RNN into additive contribution of each word in the input text. This additive decomposition enables REAT to further obtain phrase-level attribution scores. In addition, REAT is generally applicable to various RNN architectures, including GRU, LSTM and their bidirectional versions. Experimental results demonstrate the faithfulness and interpretability of the proposed attribution method. Comprehensive analysis shows that our attribution method could unveil the useful linguistic knowledge captured by RNNs. Some analysis further demonstrates our method could be utilized as a debugging tool to examine the vulnerability and failure reasons of RNNs, which may lead to several promising future directions to promote generalization ability of RNNs.

</details>

<details>

<summary>2019-03-27 05:57:10 - CryptoGuard: High Precision Detection of Cryptographic Vulnerabilities in Massive-sized Java Projects</summary>

- *Sazzadur Rahaman, Ya Xiao, Sharmin Afrose, Fahad Shaon, Ke Tian, Miles Frantz, Danfeng, Yao, Murat Kantarcioglu*

- `1806.06881v5` - [abs](http://arxiv.org/abs/1806.06881v5) - [pdf](http://arxiv.org/pdf/1806.06881v5)

> Cryptographic API misuses, such as exposed secrets, predictable random numbers, and vulnerable certificate verification, seriously threaten software security. The vision of automatically screening cryptographic API calls in massive-sized (e.g., millions of LoC) Java programs is not new. However, hindered by the practical difficulty of reducing false positives without compromising analysis quality, this goal has not been accomplished. State-of-the-art crypto API screening solutions are not designed to operate on a large scale.   Our technical innovation is a set of fast and highly accurate slicing algorithms. Our algorithms refine program slices by identifying language-specific irrelevant elements. The refinements reduce false alerts by 76% to 80% in our experiments. Running our tool, CrytoGuard, on 46 high-impact large-scale Apache projects and 6,181 Android apps generate many security insights. Our findings helped multiple popular Apache projects to harden their code, including Spark, Ranger, and Ofbiz. We also have made substantial progress towards the science of analysis in this space, including: i) manually analyzing 1,295 Apache alerts and confirming 1,277 true positives (98.61% precision), ii) creating a benchmark with 38-unit basic cases and 74-unit advanced cases, iii) performing an in-depth comparison with leading solutions including CrySL, SpotBugs, and Coverity. We are in the process of integrating CryptoGuard with the Software Assurance Marketplace (SWAMP).

</details>

<details>

<summary>2019-03-28 16:38:09 - Extending Signature-based Intrusion Detection Systems WithBayesian Abductive Reasoning</summary>

- *Ashwinkumar Ganesan, Pooja Parameshwarappa, Akshay Peshave, Zhiyuan Chen, Tim Oates*

- `1903.12101v1` - [abs](http://arxiv.org/abs/1903.12101v1) - [pdf](http://arxiv.org/pdf/1903.12101v1)

> Evolving cybersecurity threats are a persistent challenge for systemadministrators and security experts as new malwares are continu-ally released. Attackers may look for vulnerabilities in commercialproducts or execute sophisticated reconnaissance campaigns tounderstand a targets network and gather information on securityproducts like firewalls and intrusion detection / prevention systems(network or host-based). Many new attacks tend to be modificationsof existing ones. In such a scenario, rule-based systems fail to detectthe attack, even though there are minor differences in conditions /attributes between rules to identify the new and existing attack. Todetect these differences the IDS must be able to isolate the subset ofconditions that are true and predict the likely conditions (differentfrom the original) that must be observed. In this paper, we proposeaprobabilistic abductive reasoningapproach that augments an exist-ing rule-based IDS (snort [29]) to detect these evolved attacks by (a)Predicting rule conditions that are likely to occur (based on existingrules) and (b) able to generate new snort rules when provided withseed rule (i.e. a starting rule) to reduce the burden on experts toconstantly update them. We demonstrate the effectiveness of theapproach by generating new rules from the snort 2012 rules set andtesting it on the MACCDC 2012 dataset [6].

</details>

<details>

<summary>2019-03-30 09:48:01 - NEWSTRADCOIN: A Blockchain Based Privacy Preserving Secure NEWS Trading Network</summary>

- *Anik Islam, Md. Fazlul Kader, Md Mofijul Islam], Soo Young Shin*

- `1904.00184v1` - [abs](http://arxiv.org/abs/1904.00184v1) - [pdf](http://arxiv.org/pdf/1904.00184v1)

> In order to stay up to date with world issues and cutting-edge technol-ogies, the newspaper plays a crucial role. However, collecting news is not a very easy task. Currently, news publishers are collecting news from their correspond-ents through social networks, email, phone call, fax etc. and sometimes they buy news from the agencies. However, the existing news sharing networks may not provide security for data integrity and any third party may obstruct the regular flow of news sharing. Moreover, the existing news schemes are very vulnerable in case of disclosing the identity. Therefore, a universal platform is needed in the era of globalization where anyone can share and trade news from anywhere in the world securely, without the interference of third-party, and without disclosing the identity of an individual. Recently, blockchain has gained popularity because of its security mechanism over data, identity, etc. Blockchain enables a distrib-uted way of managing transactions where each participant of the network holds the same copy of the transactions. Therefore, with the help of pseudonymity, fault-tolerance, immutability and the distributed structure of blockchain, a scheme (termed as NEWSTRADCOIN) is presented in this paper in which not only news can be shared securely but also anyone can earn money by selling news. The proposed NEWSTRADCOIN can provide a universal platform where publishers can directly obtain news from news-gatherers in a secure way by main-taining data integrity, without experiencing the interference of a third-party, and without disclosing the identity of the news gatherer and publishers.

</details>

<details>

<summary>2019-03-31 06:27:08 - On the Vulnerability of CNN Classifiers in EEG-Based BCIs</summary>

- *Xiao Zhang, Dongrui Wu*

- `1904.01002v1` - [abs](http://arxiv.org/abs/1904.01002v1) - [pdf](http://arxiv.org/pdf/1904.01002v1)

> Deep learning has been successfully used in numerous applications because of its outstanding performance and the ability to avoid manual feature engineering. One such application is electroencephalogram (EEG) based brain-computer interface (BCI), where multiple convolutional neural network (CNN) models have been proposed for EEG classification. However, it has been found that deep learning models can be easily fooled with adversarial examples, which are normal examples with small deliberate perturbations. This paper proposes an unsupervised fast gradient sign method (UFGSM) to attack three popular CNN classifiers in BCIs, and demonstrates its effectiveness. We also verify the transferability of adversarial examples in BCIs, which means we can perform attacks even without knowing the architecture and parameters of the target models, or the datasets they were trained on. To our knowledge, this is the first study on the vulnerability of CNN classifiers in EEG-based BCIs, and hopefully will trigger more attention on the security of BCI systems.

</details>


## 2019-04

<details>

<summary>2019-04-01 10:27:33 - Defending against adversarial attacks by randomized diversification</summary>

- *Olga Taran, Shideh Rezaeifar, Taras Holotyak, Slava Voloshynovskiy*

- `1904.00689v1` - [abs](http://arxiv.org/abs/1904.00689v1) - [pdf](http://arxiv.org/pdf/1904.00689v1)

> The vulnerability of machine learning systems to adversarial attacks questions their usage in many applications. In this paper, we propose a randomized diversification as a defense strategy. We introduce a multi-channel architecture in a gray-box scenario, which assumes that the architecture of the classifier and the training data set are known to the attacker. The attacker does not only have access to a secret key and to the internal states of the system at the test time. The defender processes an input in multiple channels. Each channel introduces its own randomization in a special transform domain based on a secret key shared between the training and testing stages. Such a transform based randomization with a shared key preserves the gradients in key-defined sub-spaces for the defender but it prevents gradient back propagation and the creation of various bypass systems for the attacker. An additional benefit of multi-channel randomization is the aggregation that fuses soft-outputs from all channels, thus increasing the reliability of the final score. The sharing of a secret key creates an information advantage to the defender. Experimental evaluation demonstrates an increased robustness of the proposed method to a number of known state-of-the-art attacks.

</details>

<details>

<summary>2019-04-01 15:51:12 - Robustness of 3D Deep Learning in an Adversarial Setting</summary>

- *Matthew Wicker, Marta Kwiatkowska*

- `1904.00923v1` - [abs](http://arxiv.org/abs/1904.00923v1) - [pdf](http://arxiv.org/pdf/1904.00923v1)

> Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difficult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classification accuracy after the occlusion of at most 6.5% of the occupied input space.

</details>

<details>

<summary>2019-04-01 16:29:46 - Does the hiding mechanism for Stack Overflow comments work well? No!</summary>

- *Haoxiang Zhang, Shaowei Wang, Tse-Hsun Peter Chen, Ahmed E. Hassan*

- `1904.00946v1` - [abs](http://arxiv.org/abs/1904.00946v1) - [pdf](http://arxiv.org/pdf/1904.00946v1)

> Stack Overflow has accumulated millions of answers. Informative comments can strengthen their associated answers (e.g., providing additional information). Currently, Stack Overflow hides comments that are ranked beyond the top 5. Stack Overflow aims to display more informative comments (i.e., the ones with higher scores) and hide less informative ones using this mechanism. As a result, 4.4 million comments are hidden under their answer threads. Therefore, it is very important to understand how well the current comment hiding mechanism works. In this study, we investigate whether the mechanism can effectively deliver informative comments while hiding uninformative comments. We find that: 1) Hidden comments are as informative as displayed comments; more than half of the comments (both hidden and displayed) are informative (e.g., providing alternative answers, or pointing out flaws in their associated answers). 2) The current comment hiding mechanism tends to rank and hide comments based on their creation time instead of their score in most cases due to the large amount of tie-scored comments (e.g., 87% of the comments have 0-score). 3) In 97.3% of answers that have hidden comments, at least one comment is hidden while there is another comment with the same score is displayed (i.e., we refer to such cases as unfairly hidden comments). Among such unfairly hidden comments, the longest unfairly hidden comment is more likely to be informative than the shortest unfairly displayed comments. Our findings suggest that Stack Overflow should consider adjusting their current comment hiding mechanism, e.g., displaying longer unfairly hidden comments to replace shorter unfairly displayed comments. We also recommend that users examine all comments, in case they would miss informative details such as software obsolescence, code error reports, or notices of security vulnerability in hidden comments.

</details>

<details>

<summary>2019-04-02 20:27:15 - Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction</summary>

- *Karl Stratos*

- `1804.07849v4` - [abs](http://arxiv.org/abs/1804.07849v4) - [pdf](http://arxiv.org/pdf/1804.07849v4)

> We address part-of-speech (POS) induction by maximizing the mutual information between the induced label and its context. We focus on two training objectives that are amenable to stochastic gradient descent (SGD): a novel generalization of the classical Brown clustering objective and a recently proposed variational lower bound. While both objectives are subject to noise in gradient updates, we show through analysis and experiments that the variational lower bound is robust whereas the generalized Brown objective is vulnerable. We obtain competitive performance on a multitude of datasets and languages with a simple architecture that encodes morphology and context.

</details>

<details>

<summary>2019-04-03 04:19:18 - An Automated Security Analysis Framework and Implementation for Cloud</summary>

- *Hootan Alavizadeh, Hooman Alavizadeh, Dong Seong Kim, Julian Jang-Jaccard, Masood Niazi Torshiz*

- `1904.01758v1` - [abs](http://arxiv.org/abs/1904.01758v1) - [pdf](http://arxiv.org/pdf/1904.01758v1)

> Cloud service providers offer their customers with on-demand and cost-effective services, scalable computing, and network infrastructures. Enterprises migrate their services to the cloud to utilize the benefit of cloud computing such as eliminating the capital expense of their computing need. There are security vulnerabilities and threats in the cloud. Many researches have been proposed to analyze the cloud security using Graphical Security Models (GSMs) and security metrics. In addition, it has been widely researched in finding appropriate defensive strategies for the security of the cloud. Moving Target Defense (MTD) techniques can utilize the cloud elasticity features to change the attack surface and confuse attackers. Most of the previous work incorporating MTDs into the GSMs are theoretical and the performance was evaluated based on the simulation. In this paper, we realized the previous framework and designed, implemented and tested a cloud security assessment tool in a real cloud platform named UniteCloud. Our security solution can (1) monitor cloud computing in real-time, (2) automate the security modeling and analysis and visualize the GSMs using a Graphical User Interface via a web application, and (3) deploy three MTD techniques including Diversity, Redundancy, and Shuffle on the real cloud infrastructure. We analyzed the automation process using the APIs and showed the practicality and feasibility of automation of deploying all the three MTD techniques on the UniteCloud.

</details>

<details>

<summary>2019-04-03 05:55:49 - Universal Rules for Fooling Deep Neural Networks based Text Classification</summary>

- *Di Li, Danilo Vasconcellos Vargas, Sakurai Kouichi*

- `1901.07132v2` - [abs](http://arxiv.org/abs/1901.07132v2) - [pdf](http://arxiv.org/pdf/1901.07132v2)

> Recently, deep learning based natural language processing techniques are being extensively used to deal with spam mail, censorship evaluation in social networks, among others. However, there is only a couple of works evaluating the vulnerabilities of such deep neural networks. Here, we go beyond attacks to investigate, for the first time, universal rules, i.e., rules that are sample agnostic and therefore could turn any text sample in an adversarial one. In fact, the universal rules do not use any information from the method itself (no information from the method, gradient information or training dataset information is used), making them black-box universal attacks. In other words, the universal rules are sample and method agnostic. By proposing a coevolutionary optimization algorithm we show that it is possible to create universal rules that can automatically craft imperceptible adversarial samples (only less than five perturbations which are close to misspelling are inserted in the text sample). A comparison with a random search algorithm further justifies the strength of the method. Thus, universal rules for fooling networks are here shown to exist. Hopefully, the results from this work will impact the development of yet more sample and model agnostic attacks as well as their defenses, culminating in perhaps a new age for artificial intelligence.

</details>

<details>

<summary>2019-04-04 08:31:15 - White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks</summary>

- *Yotam Gil, Yoav Chai, Or Gorodissky, Jonathan Berant*

- `1904.02405v1` - [abs](http://arxiv.org/abs/1904.02405v1) - [pdf](http://arxiv.org/pdf/1904.02405v1)

> Adversarial examples are important for understanding the behavior of neural models, and can improve their robustness through adversarial training. Recent work in natural language processing generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (Ebrahimi et al., 2018). In this work, we show that the knowledge implicit in the optimization procedure can be distilled into another more efficient neural network. We train a model to emulate the behavior of a white-box attack and show that it generalizes well across examples. Moreover, it reduces adversarial example generation time by 19x-39x. We also show that our approach transfers to a black-box setting, by attacking The Google Perspective API and exposing its vulnerability. Our attack flips the API-predicted label in 42\% of the generated examples, while humans maintain high-accuracy in predicting the gold label.

</details>

<details>

<summary>2019-04-05 02:58:34 - Approximation Algorithms for Graph Burning</summary>

- *Anthony Bonato, Shahin Kamali*

- `1811.04449v2` - [abs](http://arxiv.org/abs/1811.04449v2) - [pdf](http://arxiv.org/pdf/1811.04449v2)

> Numerous approaches study the vulnerability of networks against social contagion. Graph burning studies how fast a contagion, modeled as a set of fires, spreads in a graph. The burning process takes place in synchronous, discrete rounds. In each round, a fire breaks out at a vertex, and the fire spreads to all vertices that are adjacent to a burning vertex. The selection of vertices where fires start defines a schedule that indicates the number of rounds required to burn all vertices. Given a graph, the objective of an algorithm is to find a schedule that minimizes the number of rounds to burn graph. Finding the optimal schedule is known to be NP-hard, and the problem remains NP-hard when the graph is a tree or a set of disjoint paths. The only known algorithm is an approximation algorithm for disjoint paths, which has an approximation ratio of 1.5.

</details>

<details>

<summary>2019-04-05 06:15:51 - Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks</summary>

- *Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu*

- `1904.02884v1` - [abs](http://arxiv.org/abs/1904.02884v1) - [pdf](http://arxiv.org/pdf/1904.02884v1)

> Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.

</details>

<details>

<summary>2019-04-05 14:16:58 - Efficient attack countermeasure selection accounting for recovery and action costs</summary>

- *Jukka Soikkeli, Luis Muñoz-González, Emil C. Lupu*

- `1904.03082v1` - [abs](http://arxiv.org/abs/1904.03082v1) - [pdf](http://arxiv.org/pdf/1904.03082v1)

> The losses arising from a system being hit by cyber attacks can be staggeringly high, but defending against such attacks can also be costly. This work proposes an attack countermeasure selection approach based on cost impact analysis that takes into account the impacts of actions by both the attacker and the defender. We consider a networked system providing services whose provision depends on other components in the network. We model the costs and losses to service availability from compromises and defensive actions to the components, and show that while containment of the attack can be an effective defensive strategy, it can be more cost-efficient to allow parts of the attack to continue further whilst focusing on recovering services to a functional state. Based on this insight, we build a countermeasure selection method that chooses the most cost-effective action based on its impact on expected losses and costs over a given time horizon. Our method is evaluated using simulations in synthetic graphs representing network dependencies and vulnerabilities, and found to perform well in comparison to alternatives.

</details>

<details>

<summary>2019-04-06 16:34:34 - Exploring the Attack Surface of Blockchain: A Systematic Overview</summary>

- *Muhammad Saad, Jeffrey Spaulding, Laurent Njilla, Charles Kamhoua, Sachin Shetty, DaeHun Nyang, Aziz Mohaisen*

- `1904.03487v1` - [abs](http://arxiv.org/abs/1904.03487v1) - [pdf](http://arxiv.org/pdf/1904.03487v1)

> In this paper, we systematically explore the attack surface of the Blockchain technology, with an emphasis on public Blockchains. Towards this goal, we attribute attack viability in the attack surface to 1) the Blockchain cryptographic constructs, 2) the distributed architecture of the systems using Blockchain, and 3) the Blockchain application context. To each of those contributing factors, we outline several attacks, including selfish mining, the 51% attack, Domain Name System (DNS) attacks, distributed denial-of-service (DDoS) attacks, consensus delay (due to selfish behavior or distributed denial-of-service attacks), Blockchain forks, orphaned and stale blocks, block ingestion, wallet thefts, smart contract attacks, and privacy attacks. We also explore the causal relationships between these attacks to demonstrate how various attack vectors are connected to one another. A secondary contribution of this work is outlining effective defense measures taken by the Blockchain technology or proposed by researchers to mitigate the effects of these attacks and patch associated vulnerabilities

</details>

<details>

<summary>2019-04-07 21:20:53 - Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search</summary>

- *Adnan Siraj Rakin, Zhezhi He, Deliang Fan*

- `1903.12269v2` - [abs](http://arxiv.org/abs/1903.12269v2) - [pdf](http://arxiv.org/pdf/1903.12269v2)

> Several important security issues of Deep Neural Network (DNN) have been raised recently associated with different applications and components. The most widely investigated security concern of DNN is from its malicious input, a.k.a adversarial example. Nevertheless, the security challenge of DNN's parameters is not well explored yet. In this work, we are the first to propose a novel DNN weight attack methodology called Bit-Flip Attack (BFA) which can crush a neural network through maliciously flipping extremely small amount of bits within its weight storage memory system (i.e., DRAM). The bit-flip operations could be conducted through well-known Row-Hammer attack, while our main contribution is to develop an algorithm to identify the most vulnerable bits of DNN weight parameters (stored in memory as binary bits), that could maximize the accuracy degradation with a minimum number of bit-flips. Our proposed BFA utilizes a Progressive Bit Search (PBS) method which combines gradient ranking and progressive search to identify the most vulnerable bit to be flipped. With the aid of PBS, we can successfully attack a ResNet-18 fully malfunction (i.e., top-1 accuracy degrade from 69.8% to 0.1%) only through 13 bit-flips out of 93 million bits, while randomly flipping 100 bits merely degrades the accuracy by less than 1%.

</details>

<details>

<summary>2019-04-08 01:13:43 - The Art, Science, and Engineering of Fuzzing: A Survey</summary>

- *Valentin J. M. Manes, HyungSeok Han, Choongwoo Han, Sang Kil Cha, Manuel Egele, Edward J. Schwartz, Maverick Woo*

- `1812.00140v4` - [abs](http://arxiv.org/abs/1812.00140v4) - [pdf](http://arxiv.org/pdf/1812.00140v4)

> Among the many software vulnerability discovery techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.

</details>

<details>

<summary>2019-04-08 02:13:19 - EVMFuzz: Differential Fuzz Testing of Ethereum Virtual Machine</summary>

- *Ying Fu, Meng Ren, Fuchen Ma, Yu Jiang, Heyuan Shi, Jiaguang Sun*

- `1903.08483v2` - [abs](http://arxiv.org/abs/1903.08483v2) - [pdf](http://arxiv.org/pdf/1903.08483v2)

> Ethereum Virtual Machine (EVM) is the run-time environment for smart contracts and its vulnerabilities may lead to serious problems to the Ethereum ecology. With lots of techniques being developed for the validation of smart contracts, the security problems of EVM have not been well-studied. In this paper, we propose EVMFuzz, aiming to detect vulnerabilities of EVMs with differential fuzz testing. The core idea of EVMFuzz is to continuously generate seed contracts for different EVMs' execution, so as to find as many inconsistencies among execution results as possible, eventually discover vulnerabilities with output cross-referencing. First, we present the evaluation metric for the internal inconsistency indicator, such as the opcode sequence executed and gas used. Then, we construct seed contracts via a set of predefined mutators and employ dynamic priority scheduling algorithm to guide seed contracts selection and maximize the inconsistency. Finally, we leverage different EVMs as crossreferencing oracles to avoid manual checking of the execution output. For evaluation, we conducted large-scale mutation on 36,295 real-world smart contracts and generated 253,153 smart contracts. Among them, 66.2% showed differential performance, including 1,596 variant contracts triggered inconsistent output among EVMs. Accompanied by manual root cause analysis, we found 5 previously unknown security bugs in four widely used EVMs, and all had been included in Common Vulnerabilities and Exposures (CVE) database.

</details>

<details>

<summary>2019-04-08 11:36:46 - Towards Motion Invariant Authentication for On-Body IoT Devices</summary>

- *Yong Huang, Mengnian Xu, Wei Wang, Hao Wang, Tao Jiang, Qian Zhang*

- `1904.03968v1` - [abs](http://arxiv.org/abs/1904.03968v1) - [pdf](http://arxiv.org/pdf/1904.03968v1)

> As the rapid proliferation of on-body Internet of Things (IoT) devices, their security vulnerabilities have raised serious privacy and safety issues. Traditional efforts to secure these devices against impersonation attacks mainly rely on either dedicated sensors or specified user motions, impeding their wide-scale adoption. This paper transcends these limitations with a general security solution by leveraging ubiquitous wireless chips available in IoT devices. Particularly, representative time and frequency features are first extracted from received signal strengths (RSSs) to characterize radio propagation profiles. Then, an adversarial multi-player network is developed to recognize underlying radio propagation patterns and facilitate on-body device authentication. We prove that at equilibrium, our adversarial model can extract all information about propagation patterns and eliminate any irrelevant information caused by motion variances. We build a prototype of our system using universal software radio peripheral (USRP) devices and conduct extensive experiments with both static and dynamic body motions in typical indoor and outdoor environments. The experimental results show that our system achieves an average authentication accuracy of 90.4%, with a high area under the receiver operating characteristic curve (AUROC) of 0.958 and better generalization performance in comparison with the conventional non-adversarial-based approach.

</details>

<details>

<summary>2019-04-09 02:45:35 - Efficient Decision-based Black-box Adversarial Attacks on Face Recognition</summary>

- *Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, Jun Zhu*

- `1904.04433v1` - [abs](http://arxiv.org/abs/1904.04433v1) - [pdf](http://arxiv.org/pdf/1904.04433v1)

> Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometries of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.

</details>

<details>

<summary>2019-04-09 14:44:04 - Secure Biometric-based Remote Authentication Protocol using Chebyshev Polynomials and Fuzzy Extractor</summary>

- *Thi Ai Thao Nguyen, Tran Khanh Dang, Quynh Chi Truong, Dinh Thanh Nguyen*

- `1904.04710v1` - [abs](http://arxiv.org/abs/1904.04710v1) - [pdf](http://arxiv.org/pdf/1904.04710v1)

> In this paper, we have proposed a multi factor biometric-based remote authentication protocol. Our proposal overcomes the vulnerabilities of some previous works. At the same time, the protocol also obtains a low false accept rate (FAR) and false reject rate (FRR).

</details>

<details>

<summary>2019-04-11 00:04:22 - Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey</summary>

- *Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, Chenliang Li*

- `1901.06796v3` - [abs](http://arxiv.org/abs/1901.06796v3) - [pdf](http://arxiv.org/pdf/1901.06796v3)

> With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs were vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking DNNs for textual applications emerges in recent years. However, existing perturbation methods for images cannotbe directly applied to texts as text data is discrete. In this article, we review research works that address this difference and generatetextual adversarial examples on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive way andcover all the related information to make the article self-contained. Finally, drawing on the reviewed literature, we provide further discussions and suggestions on this topic.

</details>

<details>

<summary>2019-04-11 17:15:11 - Deployment Optimization of IoT Devices through Attack Graph Analysis</summary>

- *Noga Agmon, Asaf Shabtai, Rami Puzis*

- `1904.05853v1` - [abs](http://arxiv.org/abs/1904.05853v1) - [pdf](http://arxiv.org/pdf/1904.05853v1)

> The Internet of things (IoT) has become an integral part of our life at both work and home. However, these IoT devices are prone to vulnerability exploits due to their low cost, low resources, the diversity of vendors, and proprietary firmware. Moreover, short range communication protocols (e.g., Bluetooth or ZigBee) open additional opportunities for the lateral movement of an attacker within an organization. Thus, the type and location of IoT devices may significantly change the level of network security of the organizational network. In this paper, we quantify the level of network security based on an augmented attack graph analysis that accounts for the physical location of IoT devices and their communication capabilities. We use the depth-first branch and bound (DFBnB) heuristic search algorithm to solve two optimization problems: Full Deployment with Minimal Risk (FDMR) and Maximal Utility without Risk Deterioration (MURD). An admissible heuristic is proposed to accelerate the search. The proposed method is evaluated using a real network with simulated deployment of IoT devices. The results demonstrate (1) the contribution of the augmented attack graphs to quantifying the impact of IoT devices deployed within the organization on security, and (2) the effectiveness of the optimized IoT deployment.

</details>

<details>

<summary>2019-04-13 12:44:59 - Flint for Safer Smart Contracts</summary>

- *Franklin Schrans, Daniel Hails, Alexander Harkness, Sophia Drossopoulou, Susan Eisenbach*

- `1904.06534v1` - [abs](http://arxiv.org/abs/1904.06534v1) - [pdf](http://arxiv.org/pdf/1904.06534v1)

> The Ethereum blockchain platform supports the execution of decentralised applications or smart contracts. These typically hold and transfer digital currency to other parties on the platform; however, they have been subject to numerous attacks due to the unintentional introduction of bugs. Over a billion dollars worth of currency has been stolen since its release in July 2015. As smart contracts cannot be updated after deployment, it is imperative that the programming language supports the development of robust contracts.   We propose Flint, a new statically-typed programming language specifically designed for writing robust smart contracts. Flint's features enforce the writing of safe and predictable code. To encourage good practices, we introduce protection blocks. Protection blocks restrict who can run code and when (using typestate) it can be executed. To prevent vulnerabilities relating to the unintentional loss of currency, Flint Asset traits provide safe atomic operations, ensuring the state of contracts is always consistent. Writes to state are restricted, simplifying reasoning about smart contracts.

</details>

<details>

<summary>2019-04-13 20:51:26 - Towards Vulnerability Analysis of Voice-Driven Interfaces and Countermeasures for Replay</summary>

- *Khalid Mahmood Malik, Hafiz Malik, Roland Baumann*

- `1904.06591v1` - [abs](http://arxiv.org/abs/1904.06591v1) - [pdf](http://arxiv.org/pdf/1904.06591v1)

> Fake audio detection is expected to become an important research area in the field of smart speakers such as Google Home, Amazon Echo and chatbots developed for these platforms. This paper presents replay attack vulnerability of voice-driven interfaces and proposes a countermeasure to detect replay attack on these platforms. This paper presents a novel framework to model replay attack distortion, and then use a non-learning-based method for replay attack detection on smart speakers. The reply attack distortion is modeled as a higher-order nonlinearity in the replay attack audio. Higher-order spectral analysis (HOSA) is used to capture characteristics distortions in the replay audio. Effectiveness of the proposed countermeasure scheme is evaluated on original speech as well as corresponding replayed recordings. The replay attack recordings are successfully injected into the Google Home device via Amazon Alexa using the drop-in conferencing feature.

</details>

<details>

<summary>2019-04-15 07:48:21 - Towards Realistic Battery-DoS Protection of Implantable Medical Devices</summary>

- *Muhammad Ali Siddiqi, Christos Strydis*

- `1904.06893v1` - [abs](http://arxiv.org/abs/1904.06893v1) - [pdf](http://arxiv.org/pdf/1904.06893v1)

> Modern Implantable Medical Devices (IMDs) feature wireless connectivity, which makes them vulnerable to security attacks. Particular to IMDs is the battery Denial-of-Service attack whereby attackers aim to fully deplete the battery by occupying the IMD with continuous authentication requests. Zero-Power Defense (ZPD) based on energy harvesting is known to be an excellent protection against these attacks. This paper establishes essential design specifications for employing ZPD techniques in IMDs, offers a critical review of ZPD techniques found in literature and, subsequently, gives crucial recommendations for developing comprehensive ZPD solutions.

</details>

<details>

<summary>2019-04-15 15:55:06 - RF-Trojan: Leaking Kernel Data Using Register File Trojan</summary>

- *Mohammad Nasim Imtiaz Khan, Asmit De, Swaroop Ghosh*

- `1904.07144v1` - [abs](http://arxiv.org/abs/1904.07144v1) - [pdf](http://arxiv.org/pdf/1904.07144v1)

> Register Files (RFs) are the most frequently accessed memories in a microprocessor for fast and efficient computation and control logic. Segment registers and control registers are especially critical for maintaining the CPU mode of execution that determinesthe access privileges. In this work, we explore the vulnerabilities in RF and propose a class of hardware Trojans which can inject faults during read or retention mode. The Trojan trigger is activated if one pre-selected address of L1 data-cache is hammered for certain number of times. The trigger evades post-silicon test since the required number of hammering to trigger is significantly high even under process and temperature variation. Once activated, the trigger can deliver payloads to cause Bitcell Corruption (BC) and inject read error by Read Port (RP) and Local Bitline (LBL). We model the Trojan in GEM5 architectural simulator performing a privilege escalation. We propose countermeasures such as read verification leveraging multiport feature, securing control and segment registers by hashing and L1 address obfuscation.

</details>

<details>

<summary>2019-04-15 18:33:28 - ct-fuzz: Fuzzing for Timing Leaks</summary>

- *Shaobo He, Michael Emmi, Gabriela Ciocarlie*

- `1904.07280v1` - [abs](http://arxiv.org/abs/1904.07280v1) - [pdf](http://arxiv.org/pdf/1904.07280v1)

> Testing-based methodologies like fuzzing are able to analyze complex software which is not amenable to traditional formal approaches like verification, model checking, and abstract interpretation. Despite enormous success at exposing countless security vulnerabilities in many popular software projects, applications of testing-based approaches have mainly targeted checking traditional safety properties like memory safety. While unquestionably important, this class of properties does not precisely characterize other important security aspects such as information leakage, e.g., through side channels. In this work we extend testing-based software analysis methodologies to two-safety properties, which enables the precise discovery of information leaks in complex software. In particular, we present the ct-fuzz tool, which lends coverage-guided greybox fuzzers the ability to detect two-safety property violations. Our approach is capable of exposing violations to any two-safety property expressed as equality between two program traces. Empirically, we demonstrate that ct-fuzz swiftly reveals timing leaks in popular cryptographic implementations.

</details>

<details>

<summary>2019-04-16 04:21:03 - On the Impact of Perceived Vulnerability in the Adoption of Information Systems Security Innovations</summary>

- *Mumtaz Abdul Hameed, Nalin Asanka Gamagedara Arachchilage*

- `1904.08229v1` - [abs](http://arxiv.org/abs/1904.08229v1) - [pdf](http://arxiv.org/pdf/1904.08229v1)

> A number of determinants predict the adoption of Information Systems (IS) security innovations. Amongst, perceived vulnerability of IS security threats has been examined in a number of past explorations. In this research, we examined the processes pursued in analysing the relationship between perceived vulnerability of IS security threats and the adoption of IS security innovations. The study uses Systematic Literature Review (SLR) method to evaluate the practice involved in examining perceived vulnerability on IS security innovation adoption. The SLR findings revealed the appropriateness of the existing empirical investigations of the relationship between perceived vulnerability of IS security threats on IS security innovation adoption. Furthermore, the SLR results confirmed that individuals who perceives vulnerable to an IS security threat are more likely to engage in the adoption an IS security innovation. In addition, the study validates the past studies on the relationship between perceived vulnerability and IS security innovation adoption.

</details>

<details>

<summary>2019-04-16 16:55:54 - Malware Evasion Attack and Defense</summary>

- *Yonghong Huang, Utkarsh Verma, Celeste Fralick, Gabriel Infante-Lopez, Brajesh Kumarz, Carl Woodward*

- `1904.05747v2` - [abs](http://arxiv.org/abs/1904.05747v2) - [pdf](http://arxiv.org/pdf/1904.05747v2)

> Machine learning (ML) classifiers are vulnerable to adversarial examples. An adversarial example is an input sample which is slightly modified to induce misclassification in an ML classifier. In this work, we investigate white-box and grey-box evasion attacks to an ML-based malware detector and conduct performance evaluations in a real-world setting. We compare the defense approaches in mitigating the attacks. We propose a framework for deploying grey-box and black-box attacks to malware detection systems.

</details>

<details>

<summary>2019-04-17 11:09:28 - Re: What's Up Johnny? -- Covert Content Attacks on Email End-to-End Encryption</summary>

- *Jens Müller, Marcus Brinkmann, Damian Poddebniak, Sebastian Schinzel, Jörg Schwenk*

- `1904.07550v2` - [abs](http://arxiv.org/abs/1904.07550v2) - [pdf](http://arxiv.org/pdf/1904.07550v2)

> We show practical attacks against OpenPGP and S/MIME encryption and digital signatures in the context of email. Instead of targeting the underlying cryptographic primitives, our attacks abuse legitimate features of the MIME standard and HTML, as supported by email clients, to deceive the user regarding the actual message content. We demonstrate how the attacker can unknowingly abuse the user as a decryption oracle by replying to an unsuspicious looking email. Using this technique, the plaintext of hundreds of encrypted emails can be leaked at once. Furthermore, we show how users could be tricked into signing arbitrary text by replying to emails containing CSS conditional rules. An evaluation shows that 17 out of 19 OpenPGP-capable email clients, as well as 21 out of 22 clients supporting S/MIME, are vulnerable to at least one attack. We provide different countermeasures and discuss their advantages and disadvantages.

</details>

<details>

<summary>2019-04-17 18:23:24 - Defensive Quantization: When Efficiency Meets Robustness</summary>

- *Ji Lin, Chuang Gan, Song Han*

- `1904.08444v1` - [abs](http://arxiv.org/abs/1904.08444v1) - [pdf](http://arxiv.org/pdf/1904.08444v1)

> Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack.

</details>

<details>

<summary>2019-04-17 22:41:14 - Mobility profiles and calendars for food security and livelihoods analysis</summary>

- *Pedro J. Zufiria, David Pastor-Escuredo, Luis Ubeda Medina, Miguel A. Hernandez Medina, Iker Barriales Valbuena, Alfredo J. Morales, Wilfred Nkwambi, John Quinn, Paula Hidalgo Sanchis, Miguel Luengo-Oroz*

- `1904.08525v1` - [abs](http://arxiv.org/abs/1904.08525v1) - [pdf](http://arxiv.org/pdf/1904.08525v1)

> Social vulnerability is defined as the capacity of individuals and social groups to respond to any external stress placed on their livelihoods and wellbeing. Mobility and migrations are relevant when assessing vulnerability since the movements of a population reflect on their livelihoods, coping strategies and social safety nets. Although in general migration characterization is complex and open to controversy, changes in mobility patterns for vulnerable population groups are likely to indicate a change in livelihoods or coping strategies. These changes can also indicate that the population groups may be exposed to new shocks; hence, monitoring of changes in mobility patterns can be a powerful early warning mechanism.

</details>

<details>

<summary>2019-04-18 13:26:46 - The current state of affairs in 5G security and the main remaining security challenges</summary>

- *Roger Piqueras Jover*

- `1904.08394v2` - [abs](http://arxiv.org/abs/1904.08394v2) - [pdf](http://arxiv.org/pdf/1904.08394v2)

> The first release of the 5G protocol specifications, 3rd Generation Partnership Project (3GPP) Release 15, were published in December 2017 and the first 5G protocol security specifications in March 2018. As one of the technology cornerstones for Vehicle-to-Vehicle (V2X), Vehicle-to-Everything (V2E) systems and other critical systems, 5G defines some strict communication goals, such as massive device connectivity, sub-10ms latency and ultra high bit-rate. Likewise, given the firm security requirements of certain critical applications expected to be deployed on this new cellular communications standard, 5G defines important security goals. As such, 5G networks are intended to address known protocol vulnerabilities present in both legacy GSM (Global System for Mobile Communications) networks as well as current LTE (Long Term Evolution) mobile systems. This manuscript presents a summary and analysis of the current state of affairs in 5G protocol security, discussing the main areas that should still be improved further before 5G systems go live. Although the 5G security standard documents were released just a year ago, there is a number of research papers detailing security vulnerabilities, which are summarized in this manuscript as well.

</details>

<details>

<summary>2019-04-18 13:54:20 - Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects</summary>

- *Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, Anh Nguyen*

- `1811.11553v3` - [abs](http://arxiv.org/abs/1811.11553v3) - [pdf](http://arxiv.org/pdf/1811.11553v3)

> Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, non-adversarial ones, which are common in real-world settings. In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models. That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image. Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97% of their pose space. In addition, DNNs are highly sensitive to slight pose perturbations. Importantly, adversarial poses transfer across models and datasets. We find that 99.9% and 99.4% of the poses misclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image classifiers trained on the same ImageNet dataset, respectively, and 75.5% transfer to the YOLOv3 object detector trained on MS COCO.

</details>

<details>

<summary>2019-04-19 04:31:54 - Disguised-Nets: Image Disguising for Privacy-preserving Outsourced Deep Learning</summary>

- *Sagar Sharma, Keke Chen*

- `1902.01878v2` - [abs](http://arxiv.org/abs/1902.01878v2) - [pdf](http://arxiv.org/pdf/1902.01878v2)

> Deep learning model developers often use cloud GPU resources to experiment with large data and models that need expensive setups. However, this practice raises privacy concerns. Adversaries may be interested in: 1) personally identifiable information or objects encoded in the training images, and 2) the models trained with sensitive data to launch model-based attacks. Learning deep neural networks (DNN) from encrypted data is still impractical due to the large training data and the expensive learning process. A few recent studies have tried to provide efficient, practical solutions to protect data privacy in outsourced deep-learning. However, we find out that they are vulnerable under certain attacks. In this paper, we specifically identify two types of unique attacks on outsourced deep-learning: 1) the visual re-identification attack on the training data, and 2) the class membership attack on the learned models, which can break existing privacy-preserving solutions. We develop an image disguising approach to address these attacks and design a suite of methods to evaluate the levels of attack resilience for a privacy-preserving solution for outsourced deep learning. The experimental results show that our image-disguising mechanisms can provide a high level of protection against the two attacks while still generating high-quality DNN models for image classification.

</details>

<details>

<summary>2019-04-20 22:43:22 - Shoulder Surfing: From An Experimental Study to a Comparative Framework</summary>

- *Leon Bošnjak, Boštjan Brumen*

- `1902.02501v2` - [abs](http://arxiv.org/abs/1902.02501v2) - [pdf](http://arxiv.org/pdf/1902.02501v2)

> Shoulder surfing is an attack vector widely recognized as a real threat - enough to warrant researchers dedicating a considerable effort toward designing novel authentication methods to be shoulder surfing resistant. Despite a multitude of proposed solutions over the years, few have employed empirical evaluations and comparisons between different methods, and our understanding of the shoulder surfing phenomenon remains limited. Barring the challenges in experimental design, the reason for that can be primarily attributed to the lack of objective and comparable vulnerability measures. In this paper, we develop an ensemble of vulnerability metrics, a first endeavour toward a comprehensive assessment of a given method's susceptibility to observational attacks. In the largest on-site shoulder surfing experiment (n = 274) to date, we verify the model on four conceptually different authentication methods in two observation scenarios. On the example of a novel hybrid authentication method based on associations, we explore the effect of input type on the adversary's effectiveness. We provide first empirical evidence that graphical passwords are easier to observe; however, that does not necessarily mean that the observed information will allow the attacker to guess the victim's password easier. An in-depth analysis of individual metrics within the clusters offers insight into many additional aspects of the shoulder surfing attack not explored before. Our comparative framework makes an advancement in evaluation of shoulder surfing and furthers our understanding of observational attacks. The results have important implications for future shoulder surfing studies and the field of Password Security as a whole.

</details>

<details>

<summary>2019-04-20 23:57:09 - EOP: An Encryption-Obfuscation Solution for Protecting PCBs Against Tampering and Reverse Engineering</summary>

- *Zimu Guo, Xiaolin Xu, Mark M. Tehranipoor, Domenic Forte*

- `1904.09516v1` - [abs](http://arxiv.org/abs/1904.09516v1) - [pdf](http://arxiv.org/pdf/1904.09516v1)

> PCBs are the core components for the devices ranging from the consumer electronics to military applications. Due to the accessibility of the PCBs, they are vulnerable to the attacks such as probing, eavesdropping, and reverse engineering. In this paper, a solution named EOP is proposed to migrate these threats. EOP encrypts the inter-chip communications with the stream cipher. The encryption and decryption are driven by the dedicated clock modules. These modules guarantee the stream cipher is correctly synchronized and free from tampering. Additionally, EOP also incorporates the PCB-level obfuscation for protection against reverse engineering. EOP is designated to be accomplished by utilizing the COTS components. For the validation, EOP is implemented in a Zynq SoC based system. Both the normal operation and tampering detection performance are verified. The results show that EOP can deliver the data from one chip to another without any errors. It is proved to be sensitive to any active tampering attacks.

</details>

<details>

<summary>2019-04-22 05:21:51 - RowHammer: A Retrospective</summary>

- *Onur Mutlu, Jeremie S. Kim*

- `1904.09724v1` - [abs](http://arxiv.org/abs/1904.09724v1) - [pdf](http://arxiv.org/pdf/1904.09724v1)

> This retrospective paper describes the RowHammer problem in Dynamic Random Access Memory (DRAM), which was initially introduced by Kim et al. at the ISCA 2014 conference~\cite{rowhammer-isca2014}. RowHammer is a prime (and perhaps the first) example of how a circuit-level failure mechanism can cause a practical and widespread system security vulnerability. It is the phenomenon that repeatedly accessing a row in a modern DRAM chip causes bit flips in physically-adjacent rows at consistently predictable bit locations. RowHammer is caused by a hardware failure mechanism called {\em DRAM disturbance errors}, which is a manifestation of circuit-level cell-to-cell interference in a scaled memory technology.   Researchers from Google Project Zero demonstrated in 2015 that this hardware failure mechanism can be effectively exploited by user-level programs to gain kernel privileges on real systems. Many other follow-up works demonstrated other practical attacks exploiting RowHammer. In this article, we comprehensively survey the scientific literature on RowHammer-based attacks as well as mitigation techniques to prevent RowHammer. We also discuss what other related vulnerabilities may be lurking in DRAM and other types of memories, e.g., NAND flash memory or Phase Change Memory, that can potentially threaten the foundations of secure systems, as the memory technologies scale to higher densities. We conclude by describing and advocating a principled approach to memory reliability and security research that can enable us to better anticipate and prevent such vulnerabilities.

</details>

<details>

<summary>2019-04-22 13:30:34 - Real-time Intent Prediction of Pedestrians for Autonomous Ground Vehicles via Spatio-Temporal DenseNet</summary>

- *Khaled Saleh, Mohammed Hossny, Saeid Nahavandi*

- `1904.09862v1` - [abs](http://arxiv.org/abs/1904.09862v1) - [pdf](http://arxiv.org/pdf/1904.09862v1)

> Understanding the behaviors and intentions of humans are one of the main challenges autonomous ground vehicles still faced with. More specifically, when it comes to complex environments such as urban traffic scenes, inferring the intentions and actions of vulnerable road users such as pedestrians become even harder. In this paper, we address the problem of intent action prediction of pedestrians in urban traffic environments using only image sequences from a monocular RGB camera. We propose a real-time framework that can accurately detect, track and predict the intended actions of pedestrians based on a tracking-by-detection technique in conjunction with a novel spatio-temporal DenseNet model. We trained and evaluated our framework based on real data collected from urban traffic environments. Our framework has shown resilient and competitive results in comparison to other baseline approaches. Overall, we achieved an average precision score of 84.76% with a real-time performance at 20 FPS.

</details>

<details>

<summary>2019-04-22 17:51:28 - Detecting ADS-B Spoofing Attacks using Deep Neural Networks</summary>

- *Xuhang Ying, Joanna Mazer, Giuseppe Bernieri, Mauro Conti, Linda Bushnell, Radha Poovendran*

- `1904.09969v1` - [abs](http://arxiv.org/abs/1904.09969v1) - [pdf](http://arxiv.org/pdf/1904.09969v1)

> The Automatic Dependent Surveillance-Broadcast (ADS-B) system is a key component of the Next Generation Air Transportation System (NextGen) that manages the increasingly congested airspace. It provides accurate aircraft localization and efficient air traffic management and also improves the safety of billions of current and future passengers. While the benefits of ADS-B are well known, the lack of basic security measures like encryption and authentication introduces various exploitable security vulnerabilities. One practical threat is the ADS-B spoofing attack that targets the ADS-B ground station, in which the ground-based or aircraft-based attacker manipulates the International Civil Aviation Organization (ICAO) address (a unique identifier for each aircraft) in the ADS-B messages to fake the appearance of non-existent aircraft or masquerade as a trusted aircraft. As a result, this attack can confuse the pilots or the air traffic control personnel and cause dangerous maneuvers. In this paper, we introduce SODA - a two-stage Deep Neural Network (DNN)-based spoofing detector for ADS-B that consists of a message classifier and an aircraft classifier. It allows a ground station to examine each incoming message based on the PHY-layer features (e.g., IQ samples and phases) and flag suspicious messages. Our experimental results show that SODA detects ground-based spoofing attacks with a probability of 99.34%, while having a very small false alarm rate (i.e., 0.43%). It outperforms other machine learning techniques such as XGBoost, Logistic Regression, and Support Vector Machine. It further identifies individual aircraft with an average F-score of 96.68% and an accuracy of 96.66%, with a significant improvement over the state-of-the-art detector.

</details>

<details>

<summary>2019-04-23 16:44:32 - DPatch: An Adversarial Patch Attack on Object Detectors</summary>

- *Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai Li, Yiran Chen*

- `1806.02299v4` - [abs](http://arxiv.org/abs/1806.02299v4) - [pdf](http://arxiv.org/pdf/1806.02299v4)

> Object detectors have emerged as an indispensable module in modern computer vision systems. In this work, we propose DPatch -- a black-box adversarial-patch-based attack towards mainstream object detectors (i.e. Faster R-CNN and YOLO). Unlike the original adversarial patch that only manipulates image-level classifier, our DPatch simultaneously attacks the bounding box regression and object classification so as to disable their predictions. Compared to prior works, DPatch has several appealing properties: (1) DPatch can perform both untargeted and targeted effective attacks, degrading the mAP of Faster R-CNN and YOLO from 75.10% and 65.7% down to below 1%, respectively. (2) DPatch is small in size and its attacking effect is location-independent, making it very practical to implement real-world attacks. (3) DPatch demonstrates great transferability among different detectors as well as training datasets. For example, DPatch that is trained on Faster R-CNN can effectively attack YOLO, and vice versa. Extensive evaluations imply that DPatch can perform effective attacks under black-box setup, i.e., even without the knowledge of the attacked network's architectures and parameters. Successful realization of DPatch also illustrates the intrinsic vulnerability of the modern detector architectures to such patch-based adversarial attacks.

</details>

<details>

<summary>2019-04-24 03:16:32 - Security Analysis of Near-Field Communication (NFC) Payments</summary>

- *Dennis Giese, Kevin Liu, Michael Sun, Tahin Syed, Linda Zhang*

- `1904.10623v1` - [abs](http://arxiv.org/abs/1904.10623v1) - [pdf](http://arxiv.org/pdf/1904.10623v1)

> Near-Field Communication (NFC) is a modern technology for short range communication with a variety of applications ranging from physical access control to contactless payments. These applications are often heralded as being more secure, as they require close physical proximity and do not involve Wi-Fi or mobile networks. However, these systems are still vulnerable to security attacks at the time of transaction, as they require little to no additional authentication from the user's end. In this paper, we propose a method to attack mobile-based NFC payment methods and make payments at locations far away from where the attack occurs. We evaluate our methods on our personal Apple and Google Pay accounts and demonstrate two successful attacks on these NFC payment systems.

</details>

<details>

<summary>2019-04-24 10:22:50 - Impersonating LoRaWAN gateways using Semtech Packet Forwarder</summary>

- *Lukas Simon Laufenberg*

- `1904.10728v1` - [abs](http://arxiv.org/abs/1904.10728v1) - [pdf](http://arxiv.org/pdf/1904.10728v1)

> Low Power Wide Area Network (LPWAN) technologies like the Long Range Wide Area Network (LoRaWAN) standard provide the foundation of applications realizing communication and intelligent interaction between almost any kind of object. These applications are commonly called Smart Cities and the Internet of Things (IoT). Offering the potential of great benefits for mankind, these applications can also present a significant risk, especially when their security is compromised. This paper's work analyzes the possibility of two particular scenarios of impersonating a LoRaWAN gateway combining existing attacks. Impersonated gateways are of use when exploiting vulnerabilities already shown by other researchers. We give a basic overview about LoRaWAN, the Semtech Packet Forwarder protocol, attacks needed to perform the impersonation, and assumptions made. We explain our attack and propose countermeasures to increase the security of LoRaWAN networks. We show a gateway impersonation is possible in particular circumstances but can be detected and prevented.

</details>

<details>

<summary>2019-04-24 21:41:04 - How You Act Tells a Lot: Privacy-Leakage Attack on Deep Reinforcement Learning</summary>

- *Xinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, Dawn Song*

- `1904.11082v1` - [abs](http://arxiv.org/abs/1904.11082v1) - [pdf](http://arxiv.org/pdf/1904.11082v1)

> Machine learning has been widely applied to various applications, some of which involve training with privacy-sensitive data. A modest number of data breaches have been studied, including credit card information in natural language data and identities from face dataset. However, most of these studies focus on supervised learning models. As deep reinforcement learning (DRL) has been deployed in a number of real-world systems, such as indoor robot navigation, whether trained DRL policies can leak private information requires in-depth study. To explore such privacy breaches in general, we mainly propose two methods: environment dynamics search via genetic algorithm and candidate inference based on shadow policies. We conduct extensive experiments to demonstrate such privacy vulnerabilities in DRL under various settings. We leverage the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The proposed algorithm can correctly infer most of the floor plans and reaches an average recovery rate of 95.83% using policy gradient trained agents. In addition, we are able to recover the robot configuration in continuous control environments and an autonomous driving simulator with high accuracy. To the best of our knowledge, this is the first work to investigate privacy leakage in DRL settings and we show that DRL-based agents do potentially leak privacy-sensitive information from the trained policies.

</details>

<details>

<summary>2019-04-25 16:13:58 - $d$-MABE: Distributed Multilevel Attribute-Based EMR Management and Applications</summary>

- *Ehab Zaghloul, Tongtong Li, Matt Mutka, Jian Ren*

- `1904.11432v1` - [abs](http://arxiv.org/abs/1904.11432v1) - [pdf](http://arxiv.org/pdf/1904.11432v1)

> Current systems used by medical institutions for the management and transfer of Electronic Medical Records (EMR) can be vulnerable to security and privacy threats. In addition, these centralized systems often lack interoperability and give patients limited or no access to their own EMRs. In this paper, we propose a novel distributed data sharing scheme that applies the security benefits of blockchain to handle these concerns. With blockchain, we incorporate smart contracts and a distributed storage system to alleviate the dependence on the record-generating institutions to manage and share patient records. To preserve privacy of patient records, we implement our smart contracts as a method to allow patients to verify attributes prior to granting access rights. Our proposed scheme also facilitates selective sharing of medical records among staff members that belong to different levels of a hierarchical institution. We provide extensive security, privacy, and evaluation analyses to show that our proposed scheme is both efficient and practical.

</details>

<details>

<summary>2019-04-26 12:21:18 - Adversarial Attacks on Deep Neural Networks for Time Series Classification</summary>

- *Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, Pierre-Alain Muller*

- `1903.07054v2` - [abs](http://arxiv.org/abs/1903.07054v2) - [pdf](http://arxiv.org/pdf/1903.07054v2)

> Time Series Classification (TSC) problems are encountered in many real life data mining tasks ranging from medicine and security to human activity recognition and food safety. With the recent success of deep neural networks in various domains such as computer vision and natural language processing, researchers started adopting these techniques for solving time series data mining problems. However, to the best of our knowledge, no previous work has considered the vulnerability of deep learning models to adversarial time series examples, which could potentially make them unreliable in situations where the decision taken by the classifier is crucial such as in medicine and security. For computer vision problems, such attacks have been shown to be very easy to perform by altering the image and adding an imperceptible amount of noise to trick the network into wrongly classifying the input image. Following this line of work, we propose to leverage existing adversarial attack mechanisms to add a special noise to the input time series in order to decrease the network's confidence when classifying instances at test time. Our results reveal that current state-of-the-art deep learning time series classifiers are vulnerable to adversarial attacks which can have major consequences in multiple domains such as food safety and quality assurance.

</details>

<details>

<summary>2019-04-26 15:50:24 - Risk Assessment of Cyber Attacks on Telemetry Enabled Cardiac Implantable Electronic Devices (CIED)</summary>

- *Ngamboé Mikaela, Berthier Paul, Ammari Nader, Dyrda Katia, Fernandez José*

- `1904.11908v1` - [abs](http://arxiv.org/abs/1904.11908v1) - [pdf](http://arxiv.org/pdf/1904.11908v1)

> Cardiac Implantable Electronic Devices (CIED) are fast becoming a fundamental tool of advanced medical technology and a key instrument in saving lives. Despite their importance, previous studies have shown that CIED are not completely secure against cyber attacks and especially those who are exploiting their Radio Frequency (RF) communication interfaces. Furthermore, the telemetry capabilities and IP connectivity of the external devices interacting with the CIED are creating other entry points that may be used by attackers. In this paper, we carry out a realistic risk analysis of such attacks. This analysis is composed of three parts. First, an actor-based analysis to determine the impact of the attacks. Second, a scenario-based analysis to determine the probability of occurrence of each threat. Finally, a combined analysis to determine which attack outcomes (i.e. attack goals) are riskiest and to identify the vulnerabilities that constitute the highest overall risk exposure. The conducted study showed that the vulnerabilities associated with the RF interface of CIED represent an acceptable risk. In contrast, the network and internet connectivity of external devices represent an important potential risk. The previously described findings suggest that the highest risk is associated with external systems and not the CIED itself.

</details>

<details>

<summary>2019-04-26 18:12:56 - Knowing When to Stop: Evaluation and Verification of Conformity to Output-size Specifications</summary>

- *Chenglong Wang, Rudy Bunel, Krishnamurthy Dvijotham, Po-Sen Huang, Edward Grefenstette, Pushmeet Kohli*

- `1904.12004v1` - [abs](http://arxiv.org/abs/1904.12004v1) - [pdf](http://arxiv.org/pdf/1904.12004v1)

> Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in real world applications. While the ability of these neural architectures to produce variable-length outputs makes them extremely effective for problems like Machine Translation and Image Captioning, it also leaves them vulnerable to failures of the form where the model produces outputs of undesirable length. This behavior can have severe consequences such as usage of increased computation and induce faults in downstream modules that expect outputs of a certain length. Motivated by the need to have a better understanding of the failures of these models, this paper proposes and studies the novel output-size modulation problem and makes two key technical contributions. First, to evaluate model robustness, we develop an easy-to-compute differentiable proxy objective that can be used with gradient-based algorithms to find output-lengthening inputs. Second and more importantly, we develop a verification approach that can formally verify whether a network always produces outputs within a certain length. Experimental results on Machine Translation and Image Captioning show that our output-lengthening approach can produce outputs that are 50 times longer than the input, while our verification approach can, given a model and input domain, prove that the output length is below a certain size.

</details>

<details>

<summary>2019-04-27 21:36:29 - Credential Masquerading and OpenSSL Spy: Exploring ROS 2 using DDS security</summary>

- *Vincenzo DiLuoffo, William R. Michalson, Berk Sunar*

- `1904.09179v2` - [abs](http://arxiv.org/abs/1904.09179v2) - [pdf](http://arxiv.org/pdf/1904.09179v2)

> The trend toward autonomous robot deployments is on an upward growth curve. These robots are undertaking new tasks and are being integrated into society. Examples of this trend are autonomous vehicles, humanoids, and eldercare. The movement from factory floors to streets and homes has also increased the number of vulnerabilities that adversaries can utilize. To improve security, Robot Operating System (ROS) 2 has standardized on using Data Distributed Services (DDS) as the messaging layer, which supports a security standard for protecting messages between parties with access control enforcement. DDS security is dependent on the OpenSSL and a security configuration file that specifies sensitive data location. DSS Security assumes that the underlining Operating System (OS) is secure and that the dependencies are consistent, but ongoing integrity checks are not performed. This paper looks at two vulnerabilities that we exploit using an OpenSSL spy process and a security property file manipulation. An overview of each exploit is provided with an evaluation of mitigation technologies that may be employed in client computers, servers, and other areas. Since, ROS 2 and DDS run in user space, these processes are prone to vulnerabilities. We provide recommendations about mitigation technology, as currently autonomous platforms are being deployed without safe-guards for on or off-line threats. The Trust Platform Module (TPM) is new to robotic systems, but the standard usage model does not provide risk mitigation above the OS layer for the types of attacks we discuss.

</details>

<details>

<summary>2019-04-28 02:10:30 - Inference of Tampered Smart Meters with Validations from Feeder-Level Power Injections</summary>

- *Yachen Tang, Chee-Wooi Ten, Kevin P. Schneider*

- `1904.13208v1` - [abs](http://arxiv.org/abs/1904.13208v1) - [pdf](http://arxiv.org/pdf/1904.13208v1)

> Tampering of metering infrastructure of an electrical distribution system can significantly cause customers' billing discrepancy. The large-scale deployment of smart meters may potentially be tampered by malware by propagating their agents to other IP-based meters. Such a possibility is to pivot through the physical perimeters of a smart meter. While this framework may help utilities to accurately energy consumption information on the regular basis, it is challenging to identify malicious meters when there is a large number of users that are exploited to vulnerability and kWh information being altered. This paper presents a reconfiguration switching scheme based on graph theory incorporating the concept of distributed generators to accelerate the anomaly localization process within an electrical distribution network. First, a data form transformation from a visualized grid topology to a graph with vertices and edges is presented. A conversion from the graph representation to machine recognized matrix representation is then performed. The connection of the grid topology is illustrated as an adjacency or incidence matrix for the following analysis. A switching procedure to change elements in the topological matrix is used to detect and localize the tampered node or cluster. The procedure has to meet the electrical and the temporary closed-loop operational constraints. The customer-level anomaly detection is then performed in accordance with probability derived from smart meter anomalies.

</details>

<details>

<summary>2019-04-29 00:56:48 - Algorithm Diversity for Resilient Systems</summary>

- *Scott D. Stoller, Yanhong A. Liu*

- `1904.12409v1` - [abs](http://arxiv.org/abs/1904.12409v1) - [pdf](http://arxiv.org/pdf/1904.12409v1)

> Diversity can significantly increase the resilience of systems, by reducing the prevalence of shared vulnerabilities and making vulnerabilities harder to exploit. Work on software diversity for security typically creates variants of a program using low-level code transformations. This paper is the first to study algorithm diversity for resilience. We first describe how a method based on high-level invariants and systematic incrementalization can be used to create algorithm variants. Executing multiple variants in parallel and comparing their outputs provides greater resilience than executing one variant. To prevent different parallel schedules from causing variants' behaviors to diverge, we present a synchronized execution algorithm for DistAlgo, an extension of Python for high-level, precise, executable specifications of distributed algorithms. We propose static and dynamic metrics for measuring diversity. An experimental evaluation of algorithm diversity combined with implementation-level diversity for several sequential algorithms and distributed algorithms shows the benefits of algorithm diversity.

</details>

<details>

<summary>2019-04-29 14:55:54 - Chaotic Compilation for Encrypted Computing: Obfuscation but Not in Name</summary>

- *Peter T. Breuer*

- `1904.09429v2` - [abs](http://arxiv.org/abs/1904.09429v2) - [pdf](http://arxiv.org/pdf/1904.09429v2)

> An `obfuscation' for encrypted computing is quantified exactly here, leading to an argument that security against polynomial-time attacks has been achieved for user data via the deliberately `chaotic' compilation required for security properties in that environment. Encrypted computing is the emerging science and technology of processors that take encrypted inputs to encrypted outputs via encrypted intermediate values (at nearly conventional speeds). The aim is to make user data in general-purpose computing secure against the operator and operating system as potential adversaries. A stumbling block has always been that memory addresses are data and good encryption means the encrypted value varies randomly, and that makes hitting any target in memory problematic without address decryption, yet decryption anywhere on the memory path would open up many easily exploitable vulnerabilities. This paper `solves (chaotic) compilation' for processors without address decryption, covering all of ANSI C while satisfying the required security properties and opening up the field for the standard software tool-chain and infrastructure. That produces the argument referred to above, which may also hold without encryption.

</details>


## 2019-05

<details>

<summary>2019-05-01 04:15:47 - Dropping Pixels for Adversarial Robustness</summary>

- *Hossein Hosseini, Sreeram Kannan, Radha Poovendran*

- `1905.00180v1` - [abs](http://arxiv.org/abs/1905.00180v1) - [pdf](http://arxiv.org/pdf/1905.00180v1)

> Deep neural networks are vulnerable against adversarial examples. In this paper, we propose to train and test the networks with randomly subsampled images with high drop rates. We show that this approach significantly improves robustness against adversarial examples in all cases of bounded L0, L2 and L_inf perturbations, while reducing the standard accuracy by a small value. We argue that subsampling pixels can be thought to provide a set of robust features for the input image and, thus, improves robustness without performing adversarial training.

</details>

<details>

<summary>2019-05-01 05:18:53 - POBA-GA: Perturbation Optimized Black-Box Adversarial Attacks via Genetic Algorithm</summary>

- *Jinyin Chen, Mengmeng Su, Shijing Shen, Hui Xiong, Haibin Zheng*

- `1906.03181v1` - [abs](http://arxiv.org/abs/1906.03181v1) - [pdf](http://arxiv.org/pdf/1906.03181v1)

> Most deep learning models are easily vulnerable to adversarial attacks. Various adversarial attacks are designed to evaluate the robustness of models and develop defense model. Currently, adversarial attacks are brought up to attack their own target model with their own evaluation metrics. And most of the black-box adversarial attack algorithms cannot achieve the expected success rate compared with white-box attacks. In this paper, comprehensive evaluation metrics are brought up for different adversarial attack methods. A novel perturbation optimized black-box adversarial attack based on genetic algorithm (POBA-GA) is proposed for achieving white-box comparable attack performances. Approximate optimal adversarial examples are evolved through evolutionary operations including initialization, selection, crossover and mutation. Fitness function is specifically designed to evaluate the example individual in both aspects of attack ability and perturbation control. Population diversity strategy is brought up in evolutionary process to promise the approximate optimal perturbations obtained. Comprehensive experiments are carried out to testify POBA-GA's performances. Both simulation and application results prove that our method is better than current state-of-art black-box attack methods in aspects of attack capability and perturbation control.

</details>

<details>

<summary>2019-05-01 11:35:13 - Characterizing Code Clones in the Ethereum Smart Contract Ecosystem</summary>

- *Ningyu He, Lei Wu, Haoyu Wang, Yao Guo, Xuxian Jiang*

- `1905.00272v1` - [abs](http://arxiv.org/abs/1905.00272v1) - [pdf](http://arxiv.org/pdf/1905.00272v1)

> In this paper, we present the first large-scale and systematic study to characterize the code reuse practice in the Ethereum smart contract ecosystem. We first performed a detailed similarity comparison study on a dataset of 10 million contracts we had harvested, and then we further conducted a qualitative analysis to characterize the diversity of the ecosystem, understand the correlation between code reuse and vulnerabilities, and detect the plagiarist DApps. Our analysis revealed that over 96% of the contracts had duplicates, while a large number of them were similar, which suggests that the ecosystem is highly homogeneous. Our results also suggested that roughly 9.7% of the similar contract pairs have exactly the same vulnerabilities, which we assume were introduced by code clones. In addition, we identified 41 DApps clusters, involving 73 plagiarized DApps which had caused huge financial loss to the original creators, accounting for 1/3 of the original market volume.

</details>

<details>

<summary>2019-05-01 17:30:58 - Detecting Adversarial Examples through Nonlinear Dimensionality Reduction</summary>

- *Francesco Crecchi, Davide Bacciu, Battista Biggio*

- `1904.13094v2` - [abs](http://arxiv.org/abs/1904.13094v2) - [pdf](http://arxiv.org/pdf/1904.13094v2)

> Deep neural networks are vulnerable to adversarial examples, i.e., carefully-perturbed inputs aimed to mislead classification. This work proposes a detection method based on combining non-linear dimensionality reduction and density estimation techniques. Our empirical findings show that the proposed approach is able to effectively detect adversarial examples crafted by non-adaptive attackers, i.e., not specifically tuned to bypass the detection method. Given our promising results, we plan to extend our analysis to adaptive attackers in future work.

</details>

<details>

<summary>2019-05-02 09:14:37 - InternalBlue - Bluetooth Binary Patching and Experimentation Framework</summary>

- *Dennis Mantz, Jiska Classen, Matthias Schulz, Matthias Hollick*

- `1905.00631v1` - [abs](http://arxiv.org/abs/1905.00631v1) - [pdf](http://arxiv.org/pdf/1905.00631v1)

> Bluetooth is one of the most established technologies for short range digital wireless data transmission. With the advent of wearables and the Internet of Things (IoT), Bluetooth has again gained importance, which makes security research and protocol optimizations imperative. Surprisingly, there is a lack of openly available tools and experimental platforms to scrutinize Bluetooth. In particular, system aspects and close to hardware protocol layers are mostly uncovered.   We reverse engineer multiple Broadcom Bluetooth chipsets that are widespread in off-the-shelf devices. Thus, we offer deep insights into the internal architecture of a popular commercial family of Bluetooth controllers used in smartphones, wearables, and IoT platforms. Reverse engineered functions can then be altered with our InternalBlue Python framework---outperforming evaluation kits, which are limited to documented and vendor-defined functions. The modified Bluetooth stack remains fully functional and high-performance. Hence, it provides a portable low-cost research platform.   InternalBlue is a versatile framework and we demonstrate its abilities by implementing tests and demos for known Bluetooth vulnerabilities. Moreover, we discover a novel critical security issue affecting a large selection of Broadcom chipsets that allows executing code within the attacked Bluetooth firmware. We further show how to use our framework to fix bugs in chipsets out of vendor support and how to add new security features to Bluetooth firmware.

</details>

<details>

<summary>2019-05-02 13:12:13 - A multi-agent system approach in evaluating human spatio-temporal vulnerability to seismic risk using social attachment</summary>

- *Julius Bañgate, Julie Dugdale, Elise Beck, Carole Adam*

- `1905.01365v1` - [abs](http://arxiv.org/abs/1905.01365v1) - [pdf](http://arxiv.org/pdf/1905.01365v1)

> Social attachment theory states that individuals seek the proximity of attachment figures (e.g. family members, friends, colleagues, familiar places or objects) when faced with threat. During disasters, this means that family members may seek each other before evacuating, gather personal property before heading to familiar exits and places, or follow groups/crowds, etc. This hard-wired human tendency should be considered in the assessment of risk and the creation of disaster management plans. Doing so may result in more realistic evacuation procedures and may minimise the number of casualties and injuries. In this context, a dynamic spatio-temporal analysis of seismic risk is presented using SOLACE, a multi-agent model of pedestrian behaviour based on social attachment theory implemented using the Belief-Desire-Intention approach. The model focuses on the influence of human, social, physical and temporal factors on successful evacuation. Human factors considered include perception and mobility defined by age. Social factors are defined by attachment bonds, social groups, population distribution, and cultural norms. Physical factors refer to the location of the epicentre of the earthquake, spatial distribution/layout and attributes of environmental objects such as buildings, roads, barriers (cars), placement of safe areas, evacuation routes, and the resulting debris/damage from the earthquake. Experiments tested the influence of time of the day, presence of disabled persons and earthquake intensity. Initial results show that factors that influence arrivals in safe areas include (a) human factors (age, disability, speed), (b) pre-evacuation behaviours, (c) perception distance (social attachment, time of day), (d) social interaction during evacuation, and (e) physical and spatial aspects, such as limitations imposed by debris (damage), and the distance to safe areas. To validate the results, scenarios will be designed with stakeholders, who will also take part in the definition of a serious game. The recommendation of this research is that both social and physical aspects should be considered when defining vulnerability in the analysis of risk.

</details>

<details>

<summary>2019-05-02 15:12:18 - Bug Searching in Smart Contract</summary>

- *Xiaotao Feng, Qin Wang, Xiaogang Zhu, Sheng Wen*

- `1905.00799v1` - [abs](http://arxiv.org/abs/1905.00799v1) - [pdf](http://arxiv.org/pdf/1905.00799v1)

> With the frantic development of smart contracts on the Ethereum platform, its market value has also climbed. In 2016, people were shocked by the loss of nearly $50 million in cryptocurrencies from the DAO reentrancy attack. Due to the tremendous amount of money flowing in smart contracts, its security has attracted much attention of researchers. In this paper, we investigated several common smart contract vulnerabilities and analyzed their possible scenarios and how they may be exploited. Furthermore, we survey the smart contract vulnerability detection tools for the Ethereum platform in recent years. We found that these tools have similar prototypes in software vulnerability detection technology. Moreover, for the features of public distribution systems such as Ethereum, we present the new challenges that these software vulnerability detection technologies face.

</details>

<details>

<summary>2019-05-03 03:36:49 - HADES-IoT: A Practical Host-Based Anomaly Detection System for IoT Devices (Extended Version)</summary>

- *Dominik Breitenbacher, Ivan Homoliak, Yan Lin Aung, Nils Ole Tippenhauer, Yuval Elovici*

- `1905.01027v1` - [abs](http://arxiv.org/abs/1905.01027v1) - [pdf](http://arxiv.org/pdf/1905.01027v1)

> Internet of Things (IoT) devices have become ubiquitous and are spread across many application domains including the industry, transportation, healthcare, and households. However, the proliferation of the IoT devices has raised the concerns about their security, especially when observing that many manufacturers focus only on the core functionality of their products due to short time to market and low-cost pressures, while neglecting security aspects. Moreover, it does not exist any established or standardized method for measuring and ensuring the security of IoT devices. Consequently, vulnerabilities are left untreated, allowing attackers to exploit IoT devices for various purposes, such as compromising privacy, recruiting devices into a botnet, or misusing devices to perform cryptocurrency mining.   In this paper, we present a practical Host-based Anomaly DEtection System for IoT (HADES-IoT) that represents the last line of defense. HADES-IoT has proactive detection capabilities, provides tamper-proof resistance, and it can be deployed on a wide range of Linux-based IoT devices. The main advantage of HADES-IoT is its low performance overhead, which makes it suitable for the IoT domain, where state-of-the-art approaches cannot be applied due to their high-performance demands. We deployed HADES-IoT on seven IoT devices to evaluate its effectiveness and performance overhead. Our experiments show that HADES-IoT achieved 100% effectiveness in the detection of current IoT malware such as VPNFilter and IoTReaper; while on average, requiring only 5.5% of available memory and causing only a low CPU load.

</details>

<details>

<summary>2019-05-03 08:38:03 - RDV: An Alternative To Proof-of-Work And A Real Decentralized Consensus For Blockchain</summary>

- *Siamak Solat*

- `1707.05091v5` - [abs](http://arxiv.org/abs/1707.05091v5) - [pdf](http://arxiv.org/pdf/1707.05091v5)

> A blockchain is a decentralized ledger where all transactions are recorded. For having a reliable blockchain and double-spending prevention, we need a decentralized consensus and agreement on a blockchain. Bitcoin uses proof-of-work (PoW). It is a cryptographic puzzle that is difficult to solve but easy to verify. However, because of significant latency of proof-of-work for transactions confirmation, this consensus mechanism is vulnerable against double-spending. On the other hand, PoW consumes a significant amount of energy that by growing the network, it becomes a major problematic of this consensus mechanism. In this paper, we introduce an alternative to PoW, because of all its major problems and security issues that may lead to collapsing decentralization of the blockchain, while a full decentralized system is the main purpose of using blockchain technology. The approach we introduce is based on a distributed voting process and called "RDV: Register, Deposit, Vote". Since in RDV algorithm, there is no mining process, so it is appropriate for low-level energy devices and Internet of Things (IoT).

</details>

<details>

<summary>2019-05-03 08:55:15 - Proving Erasure</summary>

- *Xavier Coiteux-Roy, Stefan Wolf*

- `1902.06656v2` - [abs](http://arxiv.org/abs/1902.06656v2) - [pdf](http://arxiv.org/pdf/1902.06656v2)

> It seems impossible to certify that a remote hosting service does not leak its users' data --- or does quantum mechanics make it possible? We investigate if a server hosting data can information-theoretically prove its definite deletion using a "BB84-like" protocol. To do so, we first rigorously introduce an alternative to privacy by encryption: privacy delegation. We then apply this novel concept to provable deletion and remote data storage. For both tasks, we present a protocol, sketch its partial security, and display its vulnerability to eavesdropping attacks targeting only a few bits.

</details>

<details>

<summary>2019-05-03 17:17:29 - Analyzing the Perceived Severity of Cybersecurity Threats Reported on Social Media</summary>

- *Shi Zong, Alan Ritter, Graham Mueller, Evan Wright*

- `1902.10680v3` - [abs](http://arxiv.org/abs/1902.10680v3) - [pdf](http://arxiv.org/pdf/1902.10680v3)

> Breaking cybersecurity events are shared across a range of websites, including security blogs (FireEye, Kaspersky, etc.), in addition to social media platforms such as Facebook and Twitter. In this paper, we investigate methods to analyze the severity of cybersecurity threats based on the language that is used to describe them online. A corpus of 6,000 tweets describing software vulnerabilities is annotated with authors' opinions toward their severity. We show that our corpus supports the development of automatic classifiers with high precision for this task. Furthermore, we demonstrate the value of analyzing users' opinions about the severity of threats reported online as an early indicator of important software vulnerabilities. We present a simple, yet effective method for linking software vulnerabilities reported in tweets to Common Vulnerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD). Using our predicted severity scores, we show that it is possible to achieve a Precision@50 of 0.86 when forecasting high severity vulnerabilities, significantly outperforming a baseline that is based on tweet volume. Finally we show how reports of severe vulnerabilities online are predictive of real-world exploits.

</details>

<details>

<summary>2019-05-04 01:28:22 - A Feature-Oriented Corpus for Understanding, Evaluating and Improving Fuzz Testing</summary>

- *Xiaogang Zhu, Xiaotao Feng, Tengyun Jiao, Sheng Wen, Yang Xiang, Seyit Camtepe, Jingling Xue*

- `1905.01405v1` - [abs](http://arxiv.org/abs/1905.01405v1) - [pdf](http://arxiv.org/pdf/1905.01405v1)

> Fuzzing is a promising technique for detecting security vulnerabilities. Newly developed fuzzers are typically evaluated in terms of the number of bugs found on vulnerable programs/binaries. However,existing corpora usually do not capture the features that prevent fuzzers from finding bugs, leading to ambiguous conclusions on the pros and cons of the fuzzers evaluated. A typical example is that Driller detects more bugs than AFL, but its evaluation cannot establish if the advancement of Driller stems from the concolic execution or not, since, for example, its ability in resolving a dataset`s magic values is unclear. In this paper, we propose to address the above problem by generating corpora based on search-hampering features. As a proof-of-concept, we have designed FEData, a prototype corpus that currently focuses on four search-hampering features to generate vulnerable programs for fuzz testing. Unlike existing corpora that can only answer "how", FEData can also further answer "why" by exposing (or understanding) the reasons for the identified weaknesses in a fuzzer. The "why" information serves as the key to the improvement of fuzzers.

</details>

<details>

<summary>2019-05-04 04:52:48 - EnFuzz: Ensemble Fuzzing with Seed Synchronization among Diverse Fuzzers</summary>

- *Yuanliang Chen, Yu Jiang, Fuchen Ma, Jie Liang, Mingzhe Wang, Chijin Zhou, Zhuo Su, Xun Jiao*

- `1807.00182v2` - [abs](http://arxiv.org/abs/1807.00182v2) - [pdf](http://arxiv.org/pdf/1807.00182v2)

> Fuzzing is widely used for software vulnerability detection. There are various kinds of fuzzers with different fuzzing strategies, and most of them perform well on their targets. However, in industry practice and empirical study, the performance and generalization ability of those well-designed fuzzing strategies are challenged by the complexity and diversity of real-world applications.   In this paper, inspired by the idea of ensemble learning, we first propose an ensemble fuzzing approach EnFuzz, that integrates multiple fuzzing strategies to obtain better performance and generalization ability than that of any constituent fuzzer alone. First, we define the diversity of the base fuzzers and choose those most recent and well-designed fuzzers as base fuzzers. Then, EnFuzz ensembles those base fuzzers with seed synchronization and result integration mechanisms. For evaluation, we implement EnFuzz , a prototype basing on four strong open-source fuzzers (AFL, AFLFast, AFLGo, FairFuzz), and test them on Google's fuzzing test suite, which consists of widely used real-world applications. The 24-hour experiment indicates that, with the same resources usage, these four base fuzzers perform variously on different applications, while EnFuzz shows better generalization ability and always outperforms others in terms of path coverage, branch coverage and crash discovery. Even compared with the best cases of AFL, AFLFast, AFLGo and FairFuzz, EnFuzz discovers 26.8%, 117%, 38.8% and 39.5% more unique crashes, executes 9.16%, 39.2%, 19.9% and 20.0% more paths and covers 5.96%, 12.0%, 21.4% and 11.1% more branches respectively.

</details>

<details>

<summary>2019-05-04 20:45:09 - MAVSec: Securing the MAVLink Protocol for Ardupilot/PX4 Unmanned Aerial Systems</summary>

- *Azza Allouch, Omar Cheikhrouhou, Anis Koubaa, Mohamed Khalgui, Tarek Abbes*

- `1905.00265v2` - [abs](http://arxiv.org/abs/1905.00265v2) - [pdf](http://arxiv.org/pdf/1905.00265v2)

> The MAVLink is a lightweight communication protocol between Unmanned Aerial Vehicles (UAVs) and ground control stations (GCSs). It defines a set of bi-directional messages exchanged between a UAV (aka drone) and a ground station. The messages carry out information about the UAV's states and control commands sent from the ground station. However, the MAVLink protocol is not secure and has several vulnerabilities to different attacks that result in critical threats and safety concerns. Very few studies provided solutions to this problem. In this paper, we discuss the security vulnerabilities of the MAVLink protocol and propose MAVSec, a security-integrated mechanism for MAVLink that leverages the use of encryption algorithms to ensure the protection of exchanged MAVLink messages between UAVs and GCSs. To validate MAVSec, we implemented it in Ardupilot and evaluated the performance of different encryption algorithms (i.e. AES-CBC, AES-CTR, RC4, and ChaCha20) in terms of memory usage and CPU consumption. The experimental results show that ChaCha20 has a better performance and is more efficient than other encryption algorithms. Integrating ChaCha20 into MAVLink can guarantee its messages confidentiality, without affecting its performance, while occupying less memory and CPU consumption, thus, preserving memory and saving the battery for the resource-constrained drone.

</details>

<details>

<summary>2019-05-06 15:36:50 - Domain Attentive Fusion for End-to-end Dialect Identification with Unknown Target Domain</summary>

- *Suwon Shon, Ahmed Ali, James Glass*

- `1812.01501v2` - [abs](http://arxiv.org/abs/1812.01501v2) - [pdf](http://arxiv.org/pdf/1812.01501v2)

> End-to-end deep learning language or dialect identification systems operate on the spectrogram or other acoustic feature and directly generate identification scores for each class. An important issue for end-to-end systems is to have some knowledge of the application domain, because the system can be vulnerable to use cases that were not seen in the training phase; such a scenario is often referred to as a domain mismatched condition. In general, we assume that there is enough variation in the training dataset to expose the system to multiple domains. In this work, we study how to best make use a training dataset in order to have maximum effectiveness on unknown target domains. Our goal is to process the input without any knowledge of the target domain while preserving robust performance on other domains as well. To accomplish this objective, we propose a domain attentive fusion approach for end-to-end dialect/language identification systems. To help with experimentation, we collect a dataset from three different domains, and create experimental protocols for a domain mismatched condition. The results of our proposed approach, which were tested on a variety of broadcast and YouTube data, shows significant performance gain compared to traditional approaches, even without any prior target domain information.

</details>

<details>

<summary>2019-05-07 09:10:48 - Representation of White- and Black-Box Adversarial Examples in Deep Neural Networks and Humans: A Functional Magnetic Resonance Imaging Study</summary>

- *Chihye Han, Wonjun Yoon, Gihyun Kwon, Seungkyu Nam, Daeshik Kim*

- `1905.02422v1` - [abs](http://arxiv.org/abs/1905.02422v1) - [pdf](http://arxiv.org/pdf/1905.02422v1)

> The recent success of brain-inspired deep neural networks (DNNs) in solving complex, high-level visual tasks has led to rising expectations for their potential to match the human visual system. However, DNNs exhibit idiosyncrasies that suggest their visual representation and processing might be substantially different from human vision. One limitation of DNNs is that they are vulnerable to adversarial examples, input images on which subtle, carefully designed noises are added to fool a machine classifier. The robustness of the human visual system against adversarial examples is potentially of great importance as it could uncover a key mechanistic feature that machine vision is yet to incorporate. In this study, we compare the visual representations of white- and black-box adversarial examples in DNNs and humans by leveraging functional magnetic resonance imaging (fMRI). We find a small but significant difference in representation patterns for different (i.e. white- versus black- box) types of adversarial examples for both humans and DNNs. However, human performance on categorical judgment is not degraded by noise regardless of the type unlike DNN. These results suggest that adversarial examples may be differentially represented in the human visual system, but unable to affect the perceptual experience.

</details>

<details>

<summary>2019-05-07 09:34:03 - Negative Update Intervals in Deep Multi-Agent Reinforcement Learning</summary>

- *Gregory Palmer, Rahul Savani, Karl Tuyls*

- `1809.05096v3` - [abs](http://arxiv.org/abs/1809.05096v3) - [pdf](http://arxiv.org/pdf/1809.05096v3)

> In Multi-Agent Reinforcement Learning (MA-RL), independent cooperative learners must overcome a number of pathologies to learn optimal joint policies. Addressing one pathology often leaves approaches vulnerable towards others. For instance, hysteretic Q-learning addresses miscoordination while leaving agents vulnerable towards misleading stochastic rewards. Other methods, such as leniency, have proven more robust when dealing with multiple pathologies simultaneously. However, leniency has predominately been studied within the context of strategic form games (bimatrix games) and fully observable Markov games consisting of a small number of probabilistic state transitions. This raises the question of whether these findings scale to more complex domains. For this purpose we implement a temporally extend version of the Climb Game, within which agents must overcome multiple pathologies simultaneously, including relative overgeneralisation, stochasticity, the alter-exploration and moving target problems, while learning from a large observation space. We find that existing lenient and hysteretic approaches fail to consistently learn near optimal joint-policies in this environment. To address these pathologies we introduce Negative Update Intervals-DDQN (NUI-DDQN), a Deep MA-RL algorithm which discards episodes yielding cumulative rewards outside the range of expanding intervals. NUI-DDQN consistently gravitates towards optimal joint-policies in our environment, overcoming the outlined pathologies.

</details>

<details>

<summary>2019-05-07 22:20:58 - Built-in Vulnerabilities to Imperceptible Adversarial Perturbations</summary>

- *Thomas Tanay, Jerone T. A. Andrews, Lewis D. Griffin*

- `1806.07409v2` - [abs](http://arxiv.org/abs/1806.07409v2) - [pdf](http://arxiv.org/pdf/1806.07409v2)

> Designing models that are robust to small adversarial perturbations of their inputs has proven remarkably difficult. In this work we show that the reverse problem---making models more vulnerable---is surprisingly easy. After presenting some proofs of concept on MNIST, we introduce a generic tilting attack that injects vulnerabilities into the linear layers of pre-trained networks by increasing their sensitivity to components of low variance in the training data without affecting their performance on test data. We illustrate this attack on a multilayer perceptron trained on SVHN and use it to design a stand-alone adversarial module which we call a steganogram decoder. Finally, we show on CIFAR-10 that a poisoning attack with a poisoning rate as low as 0.1% can induce vulnerabilities to chosen imperceptible backdoor signals in state-of-the-art networks. Beyond their practical implications, these different results shed new light on the nature of the adversarial example phenomenon.

</details>

<details>

<summary>2019-05-08 07:27:04 - Robust Federated Training via Collaborative Machine Teaching using Trusted Instances</summary>

- *Yufei Han, Xiangliang Zhang*

- `1905.02941v1` - [abs](http://arxiv.org/abs/1905.02941v1) - [pdf](http://arxiv.org/pdf/1905.02941v1)

> Federated learning performs distributed model training using local data hosted by agents. It shares only model parameter updates for iterative aggregation at the server. Although it is privacy-preserving by design, federated learning is vulnerable to noise corruption of local agents, as demonstrated in the previous study on adversarial data poisoning threat against federated learning systems. Even a single noise-corrupted agent can bias the model training. In our work, we propose a collaborative and privacy-preserving machine teaching paradigm with multiple distributed teachers, to improve robustness of the federated training process against local data corruption. We assume that each local agent (teacher) have the resources to verify a small portions of trusted instances, which may not by itself be adequate for learning. In the proposed collaborative machine teaching method, these trusted instances guide the distributed agents to jointly select a compact while informative training subset from data hosted by their own. Simultaneously, the agents learn to add changes of limited magnitudes into the selected data instances, in order to improve the testing performances of the federally trained model despite of the training data corruption. Experiments on toy and real data demonstrate that our approach can identify training set bugs effectively and suggest appropriate changes to the labels. Our algorithm is a step toward trustworthy machine learning.

</details>

<details>

<summary>2019-05-08 20:56:47 - Enhancing Cross-task Transferability of Adversarial Examples with Dispersion Reduction</summary>

- *Yunhan Jia, Yantao Lu, Senem Velipasalar, Zhenyu Zhong, Tao Wei*

- `1905.03333v1` - [abs](http://arxiv.org/abs/1905.03333v1) - [pdf](http://arxiv.org/pdf/1905.03333v1)

> Neural networks are known to be vulnerable to carefully crafted adversarial examples, and these malicious samples often transfer, i.e., they maintain their effectiveness even against other models. With great efforts delved into the transferability of adversarial examples, surprisingly, less attention has been paid to its impact on real-world deep learning deployment. In this paper, we investigate the transferability of adversarial examples across a wide range of real-world computer vision tasks, including image classification, explicit content detection, optical character recognition (OCR), and object detection. It represents the cybercriminal's situation where an ensemble of different detection mechanisms need to be evaded all at once. We propose practical attack that overcomes existing attacks' limitation of requiring task-specific loss functions by targeting on the `dispersion' of internal feature map. We report evaluation on four different computer vision tasks provided by Google Cloud Vision APIs to show how our approach outperforms existing attacks by degrading performance of multiple CV tasks by a large margin with only modest perturbations.

</details>

<details>

<summary>2019-05-09 10:24:39 - Mitigating Deep Learning Vulnerabilities from Adversarial Examples Attack in the Cybersecurity Domain</summary>

- *Chris Einar San Agustin*

- `1905.03517v1` - [abs](http://arxiv.org/abs/1905.03517v1) - [pdf](http://arxiv.org/pdf/1905.03517v1)

> Deep learning models are known to solve classification and regression problems by employing a number of epoch and training samples on a large dataset with optimal accuracy. However, that doesn't mean they are attack-proof or unexposed to vulnerabilities. Newly deployed systems particularly on a public environment (i.e public networks) are vulnerable to attacks from various entities. Moreover, published research on deep learning systems (Goodfellow et al., 2014) have determined a significant number of attacks points and a wide array of attack surface that has evidence of exploitation from adversarial examples. Successful exploit on these systems could lead to critical real world repercussions. For instance, (1) an adversarial attack on a self-driving car running a deep reinforcement learning system yields a direct misclassification on humans causing untoward accidents.(2) a self-driving vehicle misreading a red light signal may cause the car to crash to another car (3) misclassification of a pedestrian lane as an intersection lane that could lead to car crashes. This is just the tip of the iceberg, computer vision deployment are not entirely focused on self-driving cars but on many other areas as well - that would have definitive impact on the real-world. These vulnerabilities must be mitigated at an early stage of development. It is imperative to develop and implement baseline security standards at a global level prior to real-world deployment.

</details>

<details>

<summary>2019-05-09 12:28:20 - TRIDEnT: Building Decentralized Incentives for Collaborative Security</summary>

- *Nikolaos Alexopoulos, Emmanouil Vasilomanolakis, Stephane Le Roux, Steven Rowe, Max Mühlhäuser*

- `1905.03571v1` - [abs](http://arxiv.org/abs/1905.03571v1) - [pdf](http://arxiv.org/pdf/1905.03571v1)

> Sophisticated mass attacks, especially when exploiting zero-day vulnerabilities, have the potential to cause destructive damage to organizations and critical infrastructure. To timely detect and contain such attacks, collaboration among the defenders is critical. By correlating real-time detection information (alerts) from multiple sources (collaborative intrusion detection), defenders can detect attacks and take the appropriate defensive measures in time. However, although the technical tools to facilitate collaboration exist, real-world adoption of such collaborative security mechanisms is still underwhelming. This is largely due to a lack of trust and participation incentives for companies and organizations. This paper proposes TRIDEnT, a novel collaborative platform that aims to enable and incentivize parties to exchange network alert data, thus increasing their overall detection capabilities. TRIDEnT allows parties that may be in a competitive relationship, to selectively advertise, sell and acquire security alerts in the form of (near) real-time peer-to-peer streams. To validate the basic principles behind TRIDEnT, we present an intuitive game-theoretic model of alert sharing, that is of independent interest, and show that collaboration is bound to take place infinitely often. Furthermore, to demonstrate the feasibility of our approach, we instantiate our design in a decentralized manner using Ethereum smart contracts and provide a fully functional prototype.

</details>

<details>

<summary>2019-05-10 02:51:35 - Hardware/Software Co-monitoring</summary>

- *Li Lei, Kai Cong, Zhenkun Yang, Bo Chen, Fei Xie*

- `1905.03915v1` - [abs](http://arxiv.org/abs/1905.03915v1) - [pdf](http://arxiv.org/pdf/1905.03915v1)

> Hardware/Software (HW/SW) interfaces, mostly implemented as devices and device drivers, are pervasive in various computer systems. Nowadays HW/SW interfaces typically undergo intensive testing and validation before release, but they are still unreliable and insecure when deployed together with computer systems to end users. Escaped logic bugs, hardware transient failures, and malicious exploits are prevalent in HW/SW interactions, making the entire system vulnerable and unstable.   We present HW/SW co-monitoring, a runtime co-verification approach to detecting failures and malicious exploits in device/driver interactions. Our approach utilizes a formal device model (FDM), a transaction-level model derived from the device specification, to shadow the real device execution. Based on the co-execution of the device and FDM, HW/SW co-monitoring carries out two-tier runtime checking: (1) device checking checks if the device behaviors conform to the FDM behaviors; (2) property checking detects invalid driver commands issued to the device by verifying system properties against driver/device interactions. We have applied HW/SW co-monitoring to five widely-used devices and their Linux drivers, discovering 9 real bugs and vulnerabilities while introducing modest runtime overhead. The results demonstrate the major potential of HW/SW co-monitoring in improving system reliability and security.

</details>

<details>

<summary>2019-05-10 13:45:21 - On the Connection Between Adversarial Robustness and Saliency Map Interpretability</summary>

- *Christian Etmann, Sebastian Lunz, Peter Maass, Carola-Bibiane Schönlieb*

- `1905.04172v1` - [abs](http://arxiv.org/abs/1905.04172v1) - [pdf](http://arxiv.org/pdf/1905.04172v1)

> Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows,so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.

</details>

<details>

<summary>2019-05-10 14:12:46 - DÏoT: A Federated Self-learning Anomaly Detection System for IoT</summary>

- *Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni, N. Asokan, Ahmad-Reza Sadeghi*

- `1804.07474v5` - [abs](http://arxiv.org/abs/1804.07474v5) - [pdf](http://arxiv.org/pdf/1804.07474v5)

> IoT devices are increasingly deployed in daily life. Many of these devices are, however, vulnerable due to insecure design, implementation, and configuration. As a result, many networks already have vulnerable IoT devices that are easy to compromise. This has led to a new category of malware specifically targeting IoT devices. However, existing intrusion detection techniques are not effective in detecting compromised IoT devices given the massive scale of the problem in terms of the number of different types of devices and manufacturers involved. In this paper, we present D\"IoT, an autonomous self-learning distributed system for detecting compromised IoT devices effectively. In contrast to prior work, D\"IoT uses a novel self-learning approach to classify devices into device types and build normal communication profiles for each of these that can subsequently be used to detect anomalous deviations in communication patterns. D\"IoT utilizes a federated learning approach for aggregating behavior profiles efficiently. To the best of our knowledge, it is the first system to employ a federated learning approach to anomaly-detection-based intrusion detection. Consequently, D\"IoT can cope with emerging new and unknown attacks. We systematically and extensively evaluated more than 30 off-the-shelf IoT devices over a long term and show that D\"IoT is highly effective (95.6% detection rate) and fast (~257 ms) at detecting devices compromised by, for instance, the infamous Mirai malware. D\"IoT reported no false alarms when evaluated in a real-world smart home deployment setting.

</details>

<details>

<summary>2019-05-10 20:26:51 - Adversarial Defense Framework for Graph Neural Network</summary>

- *Shen Wang, Zhengzhang Chen, Jingchao Ni, Xiao Yu, Zhichun Li, Haifeng Chen, Philip S. Yu*

- `1905.03679v2` - [abs](http://arxiv.org/abs/1905.03679v2) - [pdf](http://arxiv.org/pdf/1905.03679v2)

> Graph neural network (GNN), as a powerful representation learning model on graph data, attracts much attention across various disciplines. However, recent studies show that GNN is vulnerable to adversarial attacks. How to make GNN more robust? What are the key vulnerabilities in GNN? How to address the vulnerabilities and defense GNN against the adversarial attacks? In this paper, we propose DefNet, an effective adversarial defense framework for GNNs. In particular, we first investigate the latent vulnerabilities in every layer of GNNs and propose corresponding strategies including dual-stage aggregation and bottleneck perceptron. Then, to cope with the scarcity of training data, we propose an adversarial contrastive learning method to train the GNN in a conditional GAN manner by leveraging the high-level graph representation. Extensive experiments on three public datasets demonstrate the effectiveness of DefNet in improving the robustness of popular GNN variants, such as Graph Convolutional Network and GraphSAGE, under various types of adversarial attacks.

</details>

<details>

<summary>2019-05-11 00:06:43 - On the Compositionality of Dynamic Leakage and Its Application to the Quantification Problem</summary>

- *Bao Trung Chu, Kenji Hashimoto, Hiroyuki Seki*

- `1905.04409v1` - [abs](http://arxiv.org/abs/1905.04409v1) - [pdf](http://arxiv.org/pdf/1905.04409v1)

> Quantitative information flow (QIF) is traditionally defined as the expected value of information leakage over all feasible program runs and it fails to identify vulnerable programs where only limited number of runs leak large amount of information. As discussed in Bielova (2016), a good notion for dynamic leakage and an efficient way of computing the leakage are needed. To address this problem, the authors have already proposed two notions for dynamic leakage and a method of quantifying dynamic leakage based on model counting. Inspired by the work of Kawamoto et. al. (2017), this paper proposes two efficient methods for computing dynamic leakage, a compositional method along with the sequential structure of a program and a parallel computation based on the value domain decomposition. For the former, we also investigate both exact and approximated calculations. From the perspective of implementation, we utilize binary decision diagrams (BDDs) and deterministic decomposable negation normal forms (d-DNNFs) to represent Boolean formulas in model counting. Finally, we show experimental results on several examples.

</details>

<details>

<summary>2019-05-11 03:17:21 - HSTS Preloading is Ineffective as a Long-Term, Wide-Scale MITM-Prevention Solution: Results from Analyzing the 2013 - 2017 HSTS Preload List</summary>

- *JV Roig, Eunice Grace Gatdula*

- `1905.04436v1` - [abs](http://arxiv.org/abs/1905.04436v1) - [pdf](http://arxiv.org/pdf/1905.04436v1)

> HSTS (HTTP Strict Transport Security) serves to protect websites from certain attacks by allowing web servers to inform browsers that only secure HTTPS connections should be used. However, this still leaves the initial connection unsecured and vulnerable to man-in-the-middle attacks. The HSTS preload list, now supported by most major browsers, is an attempt to close this initial vulnerability. In this study, the researchers analyzed the HSTS preload list to see the status of its deployment and industry acceptance as of December 2017. The findings here show a bleak picture: adoption of the HSTS Preload List seem to be practically nil for essential industries like Finance, and a significant percentage of entries are test sites or nonfunctional.

</details>

<details>

<summary>2019-05-11 04:07:21 - ECG Identification under Exercise and Rest Situations via Various Learning Methods</summary>

- *Zihan Wang, Yaoguang Li, Wei Cui*

- `1905.04442v1` - [abs](http://arxiv.org/abs/1905.04442v1) - [pdf](http://arxiv.org/pdf/1905.04442v1)

> As the advancement of information security, human recognition as its core technology, has absorbed an increasing amount of attention in the past few years. A myriad of biometric features including fingerprint, face, iris, have been applied to security systems, which are occasionally considered vulnerable to forgery and spoofing attacks. Due to the difficulty of being fabricated, electrocardiogram (ECG) has attracted much attention. Though many works have shown the excellent human identification provided by ECG, most current ECG human identification (ECGID) researches only focus on rest situation. In this manuscript, we overcome the oversimplification of previous researches and evaluate the performance under both exercise and rest situations, especially the influence of exercise on ECGID. By applying various existing learning methods to our ECG dataset, we find that current methods which can well support the identification of individuals under rests, do not suffice to present satisfying ECGID performance under exercise situations, therefore exposing the deficiency of existing ECG identification methods.

</details>

<details>

<summary>2019-05-11 08:22:32 - Moving Target Defense for Deep Visual Sensing against Adversarial Examples</summary>

- *Qun Song, Zhenyu Yan, Rui Tan*

- `1905.13148v1` - [abs](http://arxiv.org/abs/1905.13148v1) - [pdf](http://arxiv.org/pdf/1905.13148v1)

> Deep learning based visual sensing has achieved attractive accuracy but is shown vulnerable to adversarial example attacks. Specifically, once the attackers obtain the deep model, they can construct adversarial examples to mislead the model to yield wrong classification results. Deployable adversarial examples such as small stickers pasted on the road signs and lanes have been shown effective in misleading advanced driver-assistance systems. Many existing countermeasures against adversarial examples build their security on the attackers' ignorance of the defense mechanisms. Thus, they fall short of following Kerckhoffs's principle and can be subverted once the attackers know the details of the defense. This paper applies the strategy of moving target defense (MTD) to generate multiple new deep models after system deployment, that will collaboratively detect and thwart adversarial examples. Our MTD design is based on the adversarial examples' minor transferability to models differing from the one (e.g., the factory-designed model) used for attack construction. The post-deployment quasi-secret deep models significantly increase the bar for the attackers to construct effective adversarial examples. We also apply the technique of serial data fusion with early stopping to reduce the inference time by a factor of up to 5 while maintaining the sensing and defense performance. Extensive evaluation based on three datasets including a road sign image database and a GPU-equipped Jetson embedded computing board shows the effectiveness of our approach.

</details>

<details>

<summary>2019-05-12 03:14:33 - On Cyber Risk Management of Blockchain Networks: A Game Theoretic Approach</summary>

- *Shaohan Feng, Wenbo Wang, Zehui Xiong, Dusit Niyato, Ping Wang, Shaun Shuxun Wang*

- `1804.10412v2` - [abs](http://arxiv.org/abs/1804.10412v2) - [pdf](http://arxiv.org/pdf/1804.10412v2)

> Open-access blockchains based on proof-of-work protocols have gained tremendous popularity for their capabilities of providing decentralized tamper-proof ledgers and platforms for data-driven autonomous organization. Nevertheless, the proof-of-work based consensus protocols are vulnerable to cyber-attacks such as double-spending. In this paper, we propose a novel approach of cyber risk management for blockchain-based service. In particular, we adopt the cyber-insurance as an economic tool for neutralizing cyber risks due to attacks in blockchain networks. We consider a blockchain service market, which is composed of the infrastructure provider, the blockchain provider, the cyber-insurer, and the users. The blockchain provider purchases from the infrastructure provider, e.g., a cloud, the computing resources to maintain the blockchain consensus, and then offers blockchain services to the users. The blockchain provider strategizes its investment in the infrastructure and the service price charged to the users, in order to improve the security of the blockchain and thus optimize its profit. Meanwhile, the blockchain provider also purchases a cyber-insurance from the cyber-insurer to protect itself from the potential damage due to the attacks. In return, the cyber-insurer adjusts the insurance premium according to the perceived risk level of the blockchain service. Based on the assumption of rationality for the market entities, we model the interaction among the blockchain provider, the users, and the cyber-insurer as a two-level Stackelberg game. Namely, the blockchain provider and the cyber-insurer lead to set their pricing/investment strategies, and then the users follow to determine their demand of the blockchain service. Specifically, we consider the scenario of double-spending attacks and provide a series of analytical results about the Stackelberg equilibrium in the market game.

</details>

<details>

<summary>2019-05-12 22:15:33 - Graph Matching Networks for Learning the Similarity of Graph Structured Objects</summary>

- *Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, Pushmeet Kohli*

- `1904.12787v2` - [abs](http://arxiv.org/abs/1904.12787v2) - [pdf](http://arxiv.org/pdf/1904.12787v2)

> This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions. First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning. Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism. We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.

</details>

<details>

<summary>2019-05-13 19:18:25 - Features and Operation of an Autonomous Agent for Cyber Defense</summary>

- *Michael J. De Lucia, Allison Newcomb, Alexander Kott*

- `1905.05253v1` - [abs](http://arxiv.org/abs/1905.05253v1) - [pdf](http://arxiv.org/pdf/1905.05253v1)

> An ever increasing number of battlefield devices that are capable of collecting, processing, storing, and communicating information are rapidly becoming interconnected. The staggering number of connected devices on the battlefield greatly increases the possibility that an adversary could find ways to exploit hardware or software vulnerabilities, degrading or denying Warfighters the assured and secure use of those devices. Autonomous software agents will become necessities to manage, defend, and react to cyber threats in the future battlespace. The number of connected devices increases disproportionately to the number of cyber experts that could be available within an operational environment. In this paper, an autonomous agent capability and a scenario of how it could operate are proposed. The goal of developing such capability is to increase the security posture of the Internet of Battlefield Things and meet the challenges of an increasingly complex battlefield. This paper describes an illustrative scenario in a notional use case and discusses the challenges associated with such autonomous agents. We conclude by offering ideas for potential research into developing autonomous agents suitable for cyber defense in a battlefield environment.

</details>

<details>

<summary>2019-05-14 08:39:09 - Robustification of deep net classifiers by key based diversified aggregation with pre-filtering</summary>

- *Olga Taran, Shideh Rezaeifar, Taras Holotyak, Slava Voloshynovskiy*

- `1905.05454v1` - [abs](http://arxiv.org/abs/1905.05454v1) - [pdf](http://arxiv.org/pdf/1905.05454v1)

> In this paper, we address a problem of machine learning system vulnerability to adversarial attacks. We propose and investigate a Key based Diversified Aggregation (KDA) mechanism as a defense strategy. The KDA assumes that the attacker (i) knows the architecture of classifier and the used defense strategy, (ii) has an access to the training data set but (iii) does not know the secret key. The robustness of the system is achieved by a specially designed key based randomization. The proposed randomization prevents the gradients' back propagation or the creating of a "bypass" system. The randomization is performed simultaneously in several channels and a multi-channel aggregation stabilizes the results of randomization by aggregating soft outputs from each classifier in multi-channel system. The performed experimental evaluation demonstrates a high robustness and universality of the KDA against the most efficient gradient based attacks like those proposed by N. Carlini and D. Wagner and the non-gradient based sparse adversarial perturbations like OnePixel attacks.

</details>

<details>

<summary>2019-05-15 00:13:18 - Threats on Logic Locking: A Decade Later</summary>

- *Kimia Zamiri Azar, Hadi Mardani Kamali, Houman Homayoun, Avesta Sasan*

- `1905.05896v1` - [abs](http://arxiv.org/abs/1905.05896v1) - [pdf](http://arxiv.org/pdf/1905.05896v1)

> To reduce the cost of ICs and to meet the market's demand, a considerable portion of manufacturing supply chain, including silicon fabrication, packaging and testing may be pushed offshore. Utilizing a global IC manufacturing supply chain, and inclusion of non-trusted parties in the supply chain has raised concerns over security and trust related challenges including those of overproduction, counterfeiting, IP piracy, and Hardware Trojans to name a few. To reduce the risk of IC manufacturing in an untrusted and globally distributed supply chain, the researchers have proposed various locking and obfuscation mechanisms for hiding the functionality of the ICs during the manufacturing, that requires the activation of the IP after fabrication using the key value(s) that is only known to the IP/IC owner. At the same time, many such proposed obfuscation and locking mechanisms are broken with attacks that exploit the inherent vulnerabilities in such solutions. The past decade of research in this area, has resulted in many such defense and attack solutions. In this paper, we review a decade of research on hardware obfuscation from an attacker perspective, elaborate on attack and defense lessons learned, and discuss future directions that could be exploited for building stronger defenses.

</details>

<details>

<summary>2019-05-15 06:54:55 - Neverland: Lightweight Hardware Extensions for Enforcing Operating System Integrity</summary>

- *Salessawi Ferede Yitbarek, Todd Austin*

- `1905.05975v1` - [abs](http://arxiv.org/abs/1905.05975v1) - [pdf](http://arxiv.org/pdf/1905.05975v1)

> The security of applications hinges on the trustworthiness of the operating system, as applications rely on the OS to protect code and data. As a result, multiple protections for safeguarding the integrity of kernel code and data are being continuously proposed and deployed. These existing protections, however, are far from ideal as they either provide partial protection, or require complex and high overhead hardware and software stacks.   In this work, we present Neverland: a low-overhead, hardware-assisted, memory protection scheme that safeguards the operating system from rootkits and kernel-mode malware. Once the system is done booting, Neverland's hardware takes away the operating system's ability to overwrite certain configuration registers, as well as portions of its own physical address space that contain kernel code and security-critical data. Furthermore, it prohibits the CPU from fetching privileged code from any memory region lying outside the physical addresses assigned to the OS kernel and drivers (regardless of virtual page permissions). This combination of protections makes it extremely hard for an attacker to tamper with the kernel or introduce new privileged code into the system -- even in the presence of kernel vulnerabilities. Our evaluations show that the extra hardware required to support these protections incurs minimal silicon and energy overheads. Neverland enables operating systems to reduce their attack surface without having to rely on complex integrity monitoring software or hardware.

</details>

<details>

<summary>2019-05-15 10:14:12 - Towards a Security-Aware Benchmarking Framework for Function-as-a-Service</summary>

- *Roland Pellegrini, Igor Ivkic, Markus Tauber*

- `1905.07228v1` - [abs](http://arxiv.org/abs/1905.07228v1) - [pdf](http://arxiv.org/pdf/1905.07228v1)

> In a world, where complexity increases on a daily basis the Function-as-a-Service (FaaS) cloud model seams to take countermeasures. In comparison to other cloud models, the fast evolving FaaS increasingly abstracts the underlying infrastructure and refocuses on the application logic. This trend brings huge benefits in application and performance but comes with difficulties for benchmarking cloud applications. In this position paper, we present an initial investigation of benchmarking FaaS in close to reality production systems. Furthermore, we outline the architectural design including the necessary benchmarking metrics. We also discuss the possibility of using the proposed framework for identifying security vulnerabilities.

</details>

<details>

<summary>2019-05-15 14:49:47 - LASER: Lightweight And SEcure Remote keyless entry protocol (Extended version)</summary>

- *Vanesa Daza, Xavier Salleras*

- `1905.05694v2` - [abs](http://arxiv.org/abs/1905.05694v2) - [pdf](http://arxiv.org/pdf/1905.05694v2)

> Since Remote Keyless Entry (RKE) systems started to be widely used, several vulnerabilities in their protocols have been found. Attacks such as jamming-and-replay attacks and relay attacks are still effective against most recent RKE systems, even when many secure schemes have been designed. Although they are interesting from a theoretical point of view, the complexity of these solutions is excessive to implement them into a fob. This paper presents a lightweight and general solution based on a one message protocol, which guarantees the integrity and validity of the authentication in RKE systems, protecting the communication against the well-known jamming-and-replay and relay attacks, without using complex cryptographic schemes. Moreover, we also adapt our protocol for passive RKE (PRKE) systems. Our solution also includes a novel frequency-hopping-based approach which mitigates deny-of-service attacks. Finally, a prototype has been implemented using non-expensive hardware. Obtained results assure scalability, effectiveness and robustness.

</details>

<details>

<summary>2019-05-15 16:45:16 - Harvey: A Greybox Fuzzer for Smart Contracts</summary>

- *Valentin Wüstholz, Maria Christakis*

- `1905.06944v1` - [abs](http://arxiv.org/abs/1905.06944v1) - [pdf](http://arxiv.org/pdf/1905.06944v1)

> We present Harvey, an industrial greybox fuzzer for smart contracts, which are programs managing accounts on a blockchain. Greybox fuzzing is a lightweight test-generation approach that effectively detects bugs and security vulnerabilities. However, greybox fuzzers randomly mutate program inputs to exercise new paths; this makes it challenging to cover code that is guarded by narrow checks, which are satisfied by no more than a few input values. Moreover, most real-world smart contracts transition through many different states during their lifetime, e.g., for every bid in an auction. To explore these states and thereby detect deep vulnerabilities, a greybox fuzzer would need to generate sequences of contract transactions, e.g., by creating bids from multiple users, while at the same time keeping the search space and test suite tractable. In this experience paper, we explain how Harvey alleviates both challenges with two key fuzzing techniques and distill the main lessons learned. First, Harvey extends standard greybox fuzzing with a method for predicting new inputs that are more likely to cover new paths or reveal vulnerabilities in smart contracts. Second, it fuzzes transaction sequences in a targeted and demand-driven way. We have evaluated our approach on 27 real-world contracts. Our experiments show that the underlying techniques significantly increase Harvey's effectiveness in achieving high coverage and detecting vulnerabilities, in most cases orders-of-magnitude faster; they also reveal new insights about contract code.

</details>

<details>

<summary>2019-05-16 14:14:41 - Improving ICS Cyber Resilience through Optimal Diversification of Network Resources</summary>

- *Tingting Li, Cheng Feng, Chris Hankin*

- `1811.00142v2` - [abs](http://arxiv.org/abs/1811.00142v2) - [pdf](http://arxiv.org/pdf/1811.00142v2)

> Network diversity has been widely recognized as an effective defense strategy to mitigate the spread of malware. Optimally diversifying network resources can improve the resilience of a network against malware propagation. This work proposes an efficient method to compute such an optimal deployment, in the context of upgrading a legacy Industrial Control System with modern IT infrastructure. Our approach can tolerate various constraints when searching for an optimal diversification, such as outdated products and strict configuration policies. We explicitly measure the vulnerability similarity of products based on the CVE/NVD, to estimate the infection rate of malware between products. A Stuxnet-inspired case demonstrates our optimal diversification in practice, particularly when constrained by various requirements. We then measure the improved resilience of the diversified network in terms of a well-defined diversity metric and Mean-time-to-compromise (MTTC), to verify the effectiveness of our approach. We further evaluate three factors affecting the performance of the optimization, such as the network structure, the variety of products and constraints. Finally, we show the competitive scalability of our approach in finding optimal solutions within a couple of seconds to minutes for networks of large scales (up to 10,000 hosts) and high densities (up to 240,000 edges).

</details>

<details>

<summary>2019-05-16 16:01:24 - Better Security Bug Report Classification via Hyperparameter Optimization</summary>

- *Rui Shu, Tianpei Xia, Laurie Williams, Tim Menzies*

- `1905.06872v1` - [abs](http://arxiv.org/abs/1905.06872v1) - [pdf](http://arxiv.org/pdf/1905.06872v1)

> When security bugs are detected, they should be (a)~discussed privately by security software engineers; and (b)~not mentioned to the general public until security patches are available. Software engineers usually report bugs to bug tracking system, and label them as security bug reports (SBRs) or not-security bug reports (NSBRs), while SBRs have a higher priority to be fixed before exploited by attackers than NSBRs. Yet suspected security bug reports are often publicly disclosed because the mislabelling issues ( i.e., mislabel security bug reports as not-security bug report). The goal of this paper is to aid software developers to better classify bug reports that identify security vulnerabilities as security bug reports through parameter tuning of learners and data pre-processor. Previous work has applied text analytics and machine learning learners to classify which reported bugs are security related. We improve on that work, as shown by our analysis of five open source projects. We apply hyperparameter optimization to (a)~the control parameters of a learner; and (b)~the data pre-processing methods that handle the case where the target class is a small fraction of all the data. We show that optimizing the pre-processor is more useful than optimizing the learners. We also show that improvements gained from our approach can be very large. For example, using the same data sets as recently analyzed by our baseline approach, we show that adjusting the data pre-processing results in improvements to classification recall of 35% to 65% (median to max) with moderate increment of false positive rate.

</details>

<details>

<summary>2019-05-16 17:29:08 - Fooling Computer Vision into Inferring the Wrong Body Mass Index</summary>

- *Owen Levin, Zihang Meng, Vikas Singh, Xiaojin Zhu*

- `1905.06916v1` - [abs](http://arxiv.org/abs/1905.06916v1) - [pdf](http://arxiv.org/pdf/1905.06916v1)

> Recently it's been shown that neural networks can use images of human faces to accurately predict Body Mass Index (BMI), a widely used health indicator. In this paper we demonstrate that a neural network performing BMI inference is indeed vulnerable to test-time adversarial attacks. This extends test-time adversarial attacks from classification tasks to regression. The application we highlight is BMI inference in the insurance industry, where such adversarial attacks imply a danger of insurance fraud.

</details>

<details>

<summary>2019-05-17 17:35:04 - POPQORN: Quantifying Robustness of Recurrent Neural Networks</summary>

- *Ching-Yun Ko, Zhaoyang Lyu, Tsui-Wei Weng, Luca Daniel, Ngai Wong, Dahua Lin*

- `1905.07387v1` - [abs](http://arxiv.org/abs/1905.07387v1) - [pdf](http://arxiv.org/pdf/1905.07387v1)

> The vulnerability to adversarial attacks has been a critical issue for deep neural networks. Addressing this issue requires a reliable way to evaluate the robustness of a network. Recently, several methods have been developed to compute $\textit{robustness quantification}$ for neural networks, namely, certified lower bounds of the minimum adversarial perturbation. Such methods, however, were devised for feed-forward networks, e.g. multi-layer perceptron or convolutional networks. It remains an open problem to quantify robustness for recurrent networks, especially LSTM and GRU. For such networks, there exist additional challenges in computing the robustness quantification, such as handling the inputs at multiple steps and the interaction between gates and states. In this work, we propose $\textit{POPQORN}$ ($\textbf{P}$ropagated-$\textbf{o}$ut$\textbf{p}$ut $\textbf{Q}$uantified R$\textbf{o}$bustness for $\textbf{RN}$Ns), a general algorithm to quantify robustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its effectiveness on different network architectures and show that the robustness quantification on individual steps can lead to new insights.

</details>

<details>

<summary>2019-05-17 20:21:47 - Privacy and Security Risks of "Not-a-Virus" Bundled Adware: The Wajam Case</summary>

- *Xavier de Carné de Carnavalet, Mohammad Mannan*

- `1905.05224v2` - [abs](http://arxiv.org/abs/1905.05224v2) - [pdf](http://arxiv.org/pdf/1905.05224v2)

> Comprehensive case studies on malicious code mostly focus on botnets and worms (recently revived with IoT devices), prominent pieces of malware or Advanced Persistent Threats, exploit kits, and ransomware. However, adware seldom receives such attention. Previous studies on "unwanted" Windows applications, including adware, favored breadth of analysis, uncovering ties between different actors and distribution methods. In this paper, we demonstrate the capabilities, privacy and security risks, and prevalence of a particularly successful and active adware business: Wajam, by tracking its evolution over nearly six years. We first study its multi-layer antivirus evasion capabilities, a combination of known and newly adapted techniques, that ensure low detection rates of its daily variants, along with prominent features, e.g., traffic interception and browser process injection. Then, we look at the privacy and security implications for infected users, including plaintext leaks of browser histories and keyword searches on highly popular websites, along with arbitrary content injection on HTTPS webpages and remote code execution vulnerabilities. Finally, we study Wajam's prevalence through the popularity of its domains. Once considered as seriously as spyware, adware is now merely called "not-a-virus", "optional" or "unwanted" although its negative impact is growing. We emphasize that the adware problem has been overlooked for too long, which can reach (or even surplus) the complexity and impact of regular malware, and pose both privacy and security risks to users, more so than many well-known and thoroughly-analyzed malware families.

</details>

<details>

<summary>2019-05-18 17:42:42 - CSAI: Open-Source Cellular Radio Access Network Security Analysis Instrument</summary>

- *Thomas Byrd, Vuk Marojevic, Roger Piqueras Jover*

- `1905.07617v1` - [abs](http://arxiv.org/abs/1905.07617v1) - [pdf](http://arxiv.org/pdf/1905.07617v1)

> This paper presents our methodology and toolbox that allows analyzing the radio access network security of laboratory and commercial 4G and future 5G cellular networks. We leverage a free open-source software suite that implements the LTE UE and eNB enabling real-time signaling using software radio peripherals. We modify the UE software processing stack to act as an LTE packet collection and examination tool. This is possible because of the openness of the 3GPP specifications. Hence, we are able to receive and decode LTE downlink messages for the purpose of analyzing potential security problems of the standard. This paper shows how to rapidly prototype LTE tools and build a software-defined radio access network (RAN) analysis instrument for research and education. Using CSAI, the Cellular RAN Security Analysis Instrument, a researcher can analyze broadcast and paging messages of cellular networks. CSAI is also able to test networks to aid in the identification of vulnerabilities and verify functionality post-remediation. Additionally, we found that it can crash an eNB which motivates equivalent analyses of commercial network equipment and its robustness against denial of service attacks.

</details>

<details>

<summary>2019-05-19 02:17:13 - The Maestro Attack: Orchestrating Malicious Flows with BGP</summary>

- *Tyler McDaniel, Jared M. Smith, Max Schuchard*

- `1905.07673v1` - [abs](http://arxiv.org/abs/1905.07673v1) - [pdf](http://arxiv.org/pdf/1905.07673v1)

> We present the Maestro attack, a novel Link Flooding Attack (LFA) that leverages control-plane traffic engineering techniques to concentrate botnet-sourced Distributed Denial of Service flows on transit links. Executed from a compromised or malicious Autonomous System (AS), Maestro advertises specific-prefix routes poisoned for selected ASes to collapse inbound traffic paths onto a single target link. A greedy heuristic fed by publicly available AS relationship data iteratively builds the set of ASes to poison. Given a compromised BGP speaker with advantageous positioning relative to the target link in the Internet topology, an adversary can expect to enhance total flow density by more than 30%. For a large botnet (e.g., Mirai), that translates to augmenting a DDoS by more than a million additional infected hosts. Interestingly, the size of the adversary-controlled AS plays little role in this amplification effect. Devastating attacks on core links can be executed by small, resource-limited ASes. To understand the scope of the attack, we evaluate widespread Internet link vulnerability across several metrics, including BGP betweenness and botnet flow density. We then assess where an adversary must be positioned to execute the attack most successfully. Finally, we present effective mitigations for network operators seeking to insulate themselves from this attack.

</details>

<details>

<summary>2019-05-19 11:43:21 - Progressive Feature Alignment for Unsupervised Domain Adaptation</summary>

- *Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, Junzhou Huang*

- `1811.08585v2` - [abs](http://arxiv.org/abs/1811.08585v2) - [pdf](http://arxiv.org/pdf/1811.08585v2)

> Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich source domain to a fully-unlabeled target domain. To tackle this task, recent approaches resort to discriminative domain transfer in virtue of pseudo-labels to enforce the class-level distribution alignment across the source and target domains. These methods, however, are vulnerable to the error accumulation and thus incapable of preserving cross-domain category consistency, as the pseudo-labeling accuracy is not guaranteed explicitly. In this paper, we propose the Progressive Feature Alignment Network (PFAN) to align the discriminative features across domains progressively and effectively, via exploiting the intra-class variation in the target domain. To be specific, we first develop an Easy-to-Hard Transfer Strategy (EHTS) and an Adaptive Prototype Alignment (APA) step to train our model iteratively and alternatively. Moreover, upon observing that a good domain adaptation usually requires a non-saturated source classifier, we consider a simple yet efficient way to retard the convergence speed of the source classification loss by further involving a temperature variate into the soft-max function. The extensive experimental results reveal that the proposed PFAN exceeds the state-of-the-art performance on three UDA datasets.

</details>

<details>

<summary>2019-05-20 12:33:04 - Safety vs. Security: Attacking Avionic Systems with Humans in the Loop</summary>

- *Matthew Smith, Martin Strohmeier, Jon Harman, Vincent Lenders, Ivan Martinovic*

- `1905.08039v1` - [abs](http://arxiv.org/abs/1905.08039v1) - [pdf](http://arxiv.org/pdf/1905.08039v1)

> Many wireless communications systems found in aircraft lack standard security mechanisms, leaving them fundamentally vulnerable to attack. With affordable software-defined radios available, a novel threat has emerged, allowing a wide range of attackers to easily interfere with wireless avionic systems. Whilst these vulnerabilities are known, concrete attacks that exploit them are still novel and not yet well understood. This is true in particular with regards to their kinetic impact on the handling of the attacked aircraft and consequently its safety.   To investigate this, we invited 30 Airbus A320 type-rated pilots to fly simulator scenarios in which they were subjected to attacks on their avionics. We implement and analyse novel wireless attacks on three safety-related systems: Traffic Collision Avoidance System (TCAS), Ground Proximity Warning System (GPWS) and the Instrument Landing System (ILS).   We found that all three analysed attack scenarios created significant control impact and cost of disruption through turnarounds, avoidance manoeuvres, and diversions. They further increased workload, distrust in the affected system, and in 38% of cases caused the attacked safety system to be switched off entirely. All pilots felt the scenarios were useful, with 93.3% feeling that simulator training for wireless attacks could be valuable.

</details>

<details>

<summary>2019-05-20 20:02:53 - Towards Neural Decompilation</summary>

- *Omer Katz, Yuval Olshaker, Yoav Goldberg, Eran Yahav*

- `1905.08325v1` - [abs](http://arxiv.org/abs/1905.08325v1) - [pdf](http://arxiv.org/pdf/1905.08325v1)

> We address the problem of automatic decompilation, converting a program in low-level representation back to a higher-level human-readable programming language. The problem of decompilation is extremely important for security researchers. Finding vulnerabilities and understanding how malware operates is much easier when done over source code.   The importance of decompilation has motivated the construction of hand-crafted rule-based decompilers. Such decompilers have been designed by experts to detect specific control-flow structures and idioms in low-level code and lift them to source level. The cost of supporting additional languages or new language features in these models is very high.   We present a novel approach to decompilation based on neural machine translation. The main idea is to automatically learn a decompiler from a given compiler. Given a compiler from a source language S to a target language T , our approach automatically trains a decompiler that can translate (decompile) T back to S . We used our framework to decompile both LLVM IR and x86 assembly to C code with high success rates. Using our LLVM and x86 instantiations, we were able to successfully decompile over 97% and 88% of our benchmarks respectively.

</details>

<details>

<summary>2019-05-22 14:34:32 - Differential Privacy for Power Grid Obfuscation</summary>

- *Ferdinando Fioretto, Terrence W. K. Mak, Pascal Van Hentenryck*

- `1901.06949v2` - [abs](http://arxiv.org/abs/1901.06949v2) - [pdf](http://arxiv.org/pdf/1901.06949v2)

> The availability of high-fidelity energy networks brings significant value to academic and commercial research. However, such releases also raise fundamental concerns related to privacy and security as they can reveal sensitive commercial information and expose system vulnerabilities. This paper investigates how to release power networks where the parameters of transmission lines and transformers are obfuscated. It does so by using the framework of Differential Privacy (DP), that provides strong privacy guarantees and has attracted significant attention in recent years. Unfortunately, simple DP mechanisms often result in AC-infeasible networks. To address these concerns, this paper presents a novel differential privacy mechanism that guarantees AC-feasibility and largely preserves the fidelity of the obfuscated network. Experimental results also show that the obfuscation significantly reduces the potential damage of an attacker exploiting the release of the dataset.

</details>

<details>

<summary>2019-05-22 19:29:45 - Optimal Secure Multi-Layer IoT Network Design</summary>

- *Juntao Chen, Corinne Touati, Quanyan Zhu*

- `1707.07046v3` - [abs](http://arxiv.org/abs/1707.07046v3) - [pdf](http://arxiv.org/pdf/1707.07046v3)

> With the remarkable growth of the Internet and communication technologies over the past few decades, Internet of Things (IoTs) is enabling the ubiquitous connectivity of heterogeneous physical devices with software, sensors, and actuators. IoT networks are naturally two-layer with the cloud and cellular networks coexisting with the underlaid device-to-device (D2D) communications. The connectivity of IoTs plays an important role in information dissemination for mission-critical and civilian applications. However, IoT communication networks are vulnerable to cyber attacks including the denial-of-service (DoS) and jamming attacks, resulting in link removals in IoT network. In this work, we develop a heterogeneous IoT network design framework in which a network designer can add links to provide additional communication paths between two nodes or secure links against attacks by investing resources. By anticipating the strategic cyber attacks, we characterize the optimal design of secure IoT network by first providing a lower bound on the number of links a secure network requires for a given budget of protected links, and then developing a method to construct networks that satisfy the heterogeneous network design specifications. Therefore, each layer of the designed heterogeneous IoT network is resistant to a predefined level of malicious attacks with minimum resources. Finally, we provide case studies on the Internet of Battlefield Things (IoBT) to corroborate and illustrate our obtained results.

</details>

<details>

<summary>2019-05-22 20:16:25 - Hey Google, What Exactly Do Your Security Patches Tell Us? A Large-Scale Empirical Study on Android Patched Vulnerabilities</summary>

- *Sadegh Farhang, Mehmet Bahadir Kirdan, Aron Laszka, Jens Grossklags*

- `1905.09352v1` - [abs](http://arxiv.org/abs/1905.09352v1) - [pdf](http://arxiv.org/pdf/1905.09352v1)

> In this paper, we perform a comprehensive study of 2,470 patched Android vulnerabilities that we collect from different data sources such as Android security bulletins, CVEDetails, Qualcomm Code Aurora, AOSP Git repository, and Linux Patchwork. In our data analysis, we focus on determining the affected layers, OS versions, severity levels, and common weakness enumerations (CWE) associated with the patched vulnerabilities. Further, we assess the timeline of each vulnerability, including discovery and patch dates. We find that (i) even though the number of patched vulnerabilities changes considerably from month to month, the relative number of patched vulnerabilities for each severity level remains stable over time, (ii) there is a significant delay in patching vulnerabilities that originate from the Linux community or concern Qualcomm components, even though Linux and Qualcomm provide and release their own patches earlier, (iii) different AOSP versions receive security updates for different periods of time, (iv) for 94% of patched Android vulnerabilities, the date of disclosure in public datasets is not before the patch release date, (v) there exist some inconsistencies among public vulnerability data sources, e.g., some CVE IDs are listed in Android Security bulletins with detailed information, but in CVEDetails they are listed as unknown, (vi) many patched vulnerabilities for newer Android versions likely also affect older versions that do not receive security patches due to end-of-life.

</details>

<details>

<summary>2019-05-23 08:57:45 - MemoryRanger Prevents Hijacking FILE_OBJECT Structures in Windows Kernel</summary>

- *Igor Korkin*

- `1905.09543v1` - [abs](http://arxiv.org/abs/1905.09543v1) - [pdf](http://arxiv.org/pdf/1905.09543v1)

> Windows OS kernel memory is one of the main targets of cyber-attacks. By launching such attacks, hackers are succeeding in process privilege escalation and tampering with users data by accessing kernel mode memory. This paper considers a new example of such an attack, which results in access to the files opened in an exclusive mode. Windows built-in security features prevent such legal access, but attackers can circumvent them by patching dynamically allocated objects. The research shows that the Windows 10, version 1809 x64 is vulnerable to this attack. The paper provides an example of using MemoryRanger, a hypervisor-based solution to prevent such attack by running kernel-mode drivers in isolated kernel memory enclaves.

</details>

<details>

<summary>2019-05-24 12:52:46 - CapsAttacks: Robust and Imperceptible Adversarial Attacks on Capsule Networks</summary>

- *Alberto Marchisio, Giorgio Nanfa, Faiq Khalid, Muhammad Abdullah Hanif, Maurizio Martina, Muhammad Shafique*

- `1901.09878v2` - [abs](http://arxiv.org/abs/1901.09878v2) - [pdf](http://arxiv.org/pdf/1901.09878v2)

> Capsule Networks preserve the hierarchical spatial relationships between objects, and thereby bears a potential to surpass the performance of traditional Convolutional Neural Networks (CNNs) in performing tasks like image classification. A large body of work has explored adversarial examples for CNNs, but their effectiveness on Capsule Networks has not yet been well studied. In our work, we perform an analysis to study the vulnerabilities in Capsule Networks to adversarial attacks. These perturbations, added to the test inputs, are small and imperceptible to humans, but can fool the network to mispredict. We propose a greedy algorithm to automatically generate targeted imperceptible adversarial examples in a black-box attack scenario. We show that this kind of attacks, when applied to the German Traffic Sign Recognition Benchmark (GTSRB), mislead Capsule Networks. Moreover, we apply the same kind of adversarial attacks to a 5-layer CNN and a 9-layer CNN, and analyze the outcome, compared to the Capsule Networks to study differences in their behavior.

</details>

<details>

<summary>2019-05-24 13:39:20 - PAC it up: Towards Pointer Integrity using ARM Pointer Authentication</summary>

- *Hans Liljestrand, Thomas Nyman, Kui Wang, Carlos Chinea Perez, Jan-Erik Ekberg, N. Asokan*

- `1811.09189v4` - [abs](http://arxiv.org/abs/1811.09189v4) - [pdf](http://arxiv.org/pdf/1811.09189v4)

> Run-time attacks against programs written in memory-unsafe programming languages (e.g., C and C++) remain a prominent threat against computer systems. The prevalence of techniques like return-oriented programming (ROP) in attacking real-world systems has prompted major processor manufacturers to design hardware-based countermeasures against specific classes of run-time attacks. An example is the recently added support for pointer authentication (PA) in the ARMv8-A processor architecture, commonly used in devices like smartphones. PA is a low-cost technique to authenticate pointers so as to resist memory vulnerabilities. It has been shown to enable practical protection against memory vulnerabilities that corrupt return addresses or function pointers. However, so far, PA has received very little attention as a general purpose protection mechanism to harden software against various classes of memory attacks. In this paper, we use PA to build novel defenses against various classes of run-time attacks, including the first PA-based mechanism for data pointer integrity. We present PARTS, an instrumentation framework that integrates our PA-based defenses into the LLVM compiler and the GNU/Linux operating system and show, via systematic evaluation, that PARTS provides better protection than current solutions at a reasonable performance overhead

</details>

<details>

<summary>2019-05-25 02:03:25 - PTrix: Efficient Hardware-Assisted Fuzzing for COTS Binary</summary>

- *Yaohui Chen, Dongliang Mu, Jun Xu, Zhichuang Sun, Wenbo Shen, Xinyu Xing, Long Lu, Bing Mao*

- `1905.10499v1` - [abs](http://arxiv.org/abs/1905.10499v1) - [pdf](http://arxiv.org/pdf/1905.10499v1)

> Despite its effectiveness in uncovering software defects, American Fuzzy Lop (AFL), one of the best grey-box fuzzers, is inefficient when fuzz-testing source-unavailable programs. AFL's binary-only fuzzing mode, QEMU-AFL, is typically 2-5X slower than its source-available fuzzing mode. The slowdown is largely caused by the heavy dynamic instrumentation. Recent fuzzing techniques use Intel Processor Tracing (PT), a light-weight tracing feature supported by recent Intel CPUs, to remove the need of dynamic instrumentation. However, we found that these PT-based fuzzing techniques are even slower than QEMU-AFL when fuzzing real-world programs, making them less effective than QEMU-AFL. This poor performance is caused by the slow extraction of code coverage information from highly compressed PT traces.   In this work, we present the design and implementation of PTrix, which fully unleashes the benefits of PT for fuzzing via three novel techniques. First, PTrix introduces a scheme to highly parallel the processing of PT trace and target program execution. Second, it directly takes decoded PT trace as feedback for fuzzing, avoiding the expensive reconstruction of code coverage information. Third, PTrix maintains the new feedback with stronger feedback than edge-based code coverage, which helps reach new code space and defects that AFL may not. We evaluated PTrix by comparing its performance with the state-of-the-art fuzzers. Our results show that, given the same amount of time, PTrix achieves a significantly higher fuzzing speed and reaches into code regions missed by the other fuzzers. In addition, PTrix identifies 35 new vulnerabilities in a set of previously well-fuzzed binaries, showing its ability to complement existing fuzzers.

</details>

<details>

<summary>2019-05-25 23:24:15 - Adversarial Distillation for Ordered Top-k Attacks</summary>

- *Zekun Zhang, Tianfu Wu*

- `1905.10695v1` - [abs](http://arxiv.org/abs/1905.10695v1) - [pdf](http://arxiv.org/pdf/1905.10695v1)

> Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, especially white-box targeted attacks. One scheme of learning attacks is to design a proper adversarial objective function that leads to the imperceptible perturbation for any test image (e.g., the Carlini-Wagner (C&W) method). Most methods address targeted attacks in the Top-1 manner. In this paper, we propose to learn ordered Top-k attacks (k>= 1) for image classification tasks, that is to enforce the Top-k predicted labels of an adversarial example to be the k (randomly) selected and ordered labels (the ground-truth label is exclusive). To this end, we present an adversarial distillation framework: First, we compute an adversarial probability distribution for any given ordered Top-k targeted labels with respect to the ground-truth of a test image. Then, we learn adversarial examples by minimizing the Kullback-Leibler (KL) divergence together with the perturbation energy penalty, similar in spirit to the network distillation method. We explore how to leverage label semantic similarities in computing the targeted distributions, leading to knowledge-oriented attacks. In experiments, we thoroughly test Top-1 and Top-5 attacks in the ImageNet-1000 validation dataset using two popular DNNs trained with clean ImageNet-1000 train dataset, ResNet-50 and DenseNet-121. For both models, our proposed adversarial distillation approach outperforms the C&W method in the Top-1 setting, as well as other baseline methods. Our approach shows significant improvement in the Top-5 setting against a strong modified C&W method.

</details>

<details>

<summary>2019-05-26 04:57:55 - Purifying Adversarial Perturbation with Adversarially Trained Auto-encoders</summary>

- *Hebi Li, Qi Xiao, Shixin Tian, Jin Tian*

- `1905.10729v1` - [abs](http://arxiv.org/abs/1905.10729v1) - [pdf](http://arxiv.org/pdf/1905.10729v1)

> Machine learning models are vulnerable to adversarial examples. Iterative adversarial training has shown promising results against strong white-box attacks. However, adversarial training is very expensive, and every time a model needs to be protected, such expensive training scheme needs to be performed. In this paper, we propose to apply iterative adversarial training scheme to an external auto-encoder, which once trained can be used to protect other models directly. We empirically show that our model outperforms other purifying-based methods against white-box attacks, and transfers well to directly protect other base models with different architectures.

</details>

<details>

<summary>2019-05-27 13:26:58 - Cryptanalysis of the SLAP Authentication Protocol</summary>

- *Ciampi Giovanni*

- `1906.03228v1` - [abs](http://arxiv.org/abs/1906.03228v1) - [pdf](http://arxiv.org/pdf/1906.03228v1)

> RFID (Radio Frequency Identification) is a powerful technology that, due to its numerous advantages, is supposed to replace the various identification systems such as barcodes or magnetic stripes in a short time. There are three devices involved in an RFID protocol: a Reader, a Tag, and a back-end Database. One of the key factors for the RFID technology is that, in order to be used on large scale, the price of the Tags has to be cheap: it cannot be expensive because who is supposed to use it would need a great amount of them, furthermore, Tags must have very small dimensions. The low-cost nature of such devices implies the impossibility to use standard cryptographic protocols on them, furthermore, Ultra-Lightweight Tags even lack the necessary computational power to generate random numbers. Many experts are trying to build secure protocols that involve just simple bitwise logical operations for these Tags, but, unfortunately, each of these protocols turned out to be vulnerable to some serious attack after short time from publication. The need for a secure RFID authentication protocol today seems more urgent than ever, because this technology is already used for security purposes, such as electronic passports. The challenge to build a secure protocol for Ultra-Lightweight RFID systems is so hard that in several years no one has been able to build one. In this thesis we analyze in great detail a recently proposed protocol for Ultra-Lightweight RFID systems called SLAP, with the aim of finding new vulnerabilities. SLAP has already been violated (along with a set of similar protocols) by Safkhani and Bagheri, that have recently published a de-synchronization attack. At the end of our analysis, we will propose an impersonification attack to the protocol, along with a fix for our attack and some considerations on the attacks proposed on this kind of protocols.

</details>

<details>

<summary>2019-05-27 13:52:16 - Automated Ground Truth Estimation of Vulnerable Road Users in Automotive Radar Data Using GNSS</summary>

- *Nicolas Scheiner, Nils Appenrodt, Jürgen Dickmann, Bernhard Sick*

- `1905.11219v1` - [abs](http://arxiv.org/abs/1905.11219v1) - [pdf](http://arxiv.org/pdf/1905.11219v1)

> Annotating automotive radar data is a difficult task. This article presents an automated way of acquiring data labels which uses a highly accurate and portable global navigation satellite system (GNSS). The proposed system is discussed besides a revision of other label acquisitions techniques and a problem description of manual data annotation. The article concludes with a systematic comparison of conventional hand labeling and automatic data acquisition. The results show clear advantages of the proposed method without a relevant loss in labeling accuracy. Minor changes can be observed in the measured radar data, but the so introduced bias of the GNSS reference is clearly outweighed by the indisputable time savings. Beside data annotation, the proposed system can also provide a ground truth for validating object tracking or other automated driving system applications.

</details>

<details>

<summary>2019-05-27 16:16:11 - Adversarial Attacks on Node Embeddings via Graph Poisoning</summary>

- *Aleksandar Bojchevski, Stephan Günnemann*

- `1809.01093v3` - [abs](http://arxiv.org/abs/1809.01093v3) - [pdf](http://arxiv.org/pdf/1809.01093v3)

> The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.

</details>

<details>

<summary>2019-05-27 16:33:41 - SparseFool: a few pixels make a big difference</summary>

- *Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard*

- `1811.02248v4` - [abs](http://arxiv.org/abs/1811.02248v4) - [pdf](http://arxiv.org/pdf/1811.02248v4)

> Deep Neural Networks have achieved extraordinary results on image classification tasks, but have been shown to be vulnerable to attacks with carefully crafted perturbations of the input data. Although most attacks usually change values of many image's pixels, it has been shown that deep networks are also vulnerable to sparse alterations of the input. However, no computationally efficient method has been proposed to compute sparse perturbations. In this paper, we exploit the low mean curvature of the decision boundary, and propose SparseFool, a geometry inspired sparse attack that controls the sparsity of the perturbations. Extensive evaluations show that our approach computes sparse perturbations very fast, and scales efficiently to high dimensional data. We further analyze the transferability and the visual effects of the perturbations, and show the existence of shared semantic information across the images and the networks. Finally, we show that adversarial training can only slightly improve the robustness against sparse additive perturbations computed with SparseFool.

</details>

<details>

<summary>2019-05-27 21:42:56 - Open Platforms for Artificial Intelligence for Social Good: Common Patterns as a Pathway to True Impact</summary>

- *Kush R. Varshney, Aleksandra Mojsilovic*

- `1905.11519v1` - [abs](http://arxiv.org/abs/1905.11519v1) - [pdf](http://arxiv.org/pdf/1905.11519v1)

> The AI for social good movement has now reached a state in which a large number of one-off demonstrations have illustrated that partnerships of AI practitioners and social change organizations are possible and can address problems faced in sustainable development. In this paper, we discuss how moving from demonstrations to true impact on humanity will require a different course of action, namely open platforms containing foundational AI capabilities to support common needs of multiple organizations working in similar topical areas. We lend credence to this proposal by describing three example patterns of social good problems and their AI-based solutions: natural language processing for making sense of international development reports, causal inference for providing guidance to vulnerable individuals, and discrimination-aware classification for supporting unbiased allocation decisions. We argue that the development of such platforms will be possible through convenings of social change organizations, AI companies, and grantmaking foundations.

</details>

<details>

<summary>2019-05-28 09:13:56 - Privacy Vulnerabilities of Dataset Anonymization Techniques</summary>

- *Eyal Nussbaum, Michael Segal*

- `1905.11694v1` - [abs](http://arxiv.org/abs/1905.11694v1) - [pdf](http://arxiv.org/pdf/1905.11694v1)

> Vast amounts of information of all types are collected daily about people by governments, corporations and individuals. The information is collected when users register to or use on-line applications, receive health related services, use their mobile phones, utilize search engines, or perform common daily activities. As a result, there is an enormous quantity of privately-owned records that describe individuals' finances, interests, activities, and demographics. These records often include sensitive data and may violate the privacy of the users if published. The common approach to safeguarding user information, or data in general, is to limit access to the storage (usually a database) by using and authentication and authorization protocol. This way, only users with legitimate permissions can access the user data. In many cases though, the publication of user data for statistical analysis and research can be extremely beneficial for both academic and commercial uses, such as statistical research and recommendation systems. To maintain user privacy when such a publication occurs many databases employ anonymization techniques, either on the query results or the data itself. In this paper we examine variants of 2 such techniques, "data perturbation" and "query-set-size control" and discuss their vulnerabilities. Data perturbation deals with changing the values of records in the dataset while maintaining a level of accuracy over the resulting queries. We focus on a relatively new data perturbation method called NeNDS to show a possible partial knowledge attack on its privacy. The query-set-size control allows publication of a query result dependent on having a minimum set size, k, of records satisfying the query parameters. We show some query types relying on this method may still be used to extract hidden information, and prove others maintain privacy even when using multiple queries.

</details>

<details>

<summary>2019-05-28 10:01:24 - Improving the Robustness of Deep Neural Networks via Adversarial Training with Triplet Loss</summary>

- *Pengcheng Li, Jinfeng Yi, Bowen Zhou, Lijun Zhang*

- `1905.11713v1` - [abs](http://arxiv.org/abs/1905.11713v1) - [pdf](http://arxiv.org/pdf/1905.11713v1)

> Recent studies have highlighted that deep neural networks (DNNs) are vulnerable to adversarial examples. In this paper, we improve the robustness of DNNs by utilizing techniques of Distance Metric Learning. Specifically, we incorporate Triplet Loss, one of the most popular Distance Metric Learning methods, into the framework of adversarial training. Our proposed algorithm, Adversarial Training with Triplet Loss (AT$^2$L), substitutes the adversarial example against the current model for the anchor of triplet loss to effectively smooth the classification boundary. Furthermore, we propose an ensemble version of AT$^2$L, which aggregates different attack methods and model structures for better defense effects. Our empirical studies verify that the proposed approach can significantly improve the robustness of DNNs without sacrificing accuracy. Finally, we demonstrate that our specially designed triplet loss can also be used as a regularization term to enhance other defense methods.

</details>

<details>

<summary>2019-05-28 16:05:35 - Monetary Stabilization in Cryptocurrencies - Design Approaches and Open Questions</summary>

- *Ingolf G. A. Pernice, Sebastian Henningsen, Roman Proskalovich, Martin Florian, Hermann Elendner, Björn Scheuermann*

- `1905.11905v1` - [abs](http://arxiv.org/abs/1905.11905v1) - [pdf](http://arxiv.org/pdf/1905.11905v1)

> The price volatility of cryptocurrencies is often cited as a major hindrance to their wide-scale adoption. Consequently, during the last two years, multiple so called stablecoins have surfaced---cryptocurrencies focused on maintaining stable exchange rates. In this paper, we systematically explore and analyze the stablecoin landscape. Based on a survey of 24 specific stablecoin projects, we go beyond individual coins for extracting general concepts and approaches. We combine our findings with learnings from classical monetary policy, resulting in a comprehensive taxonomy of cryptocurrency stabilization. We use our taxonomy to highlight the current state of development from different perspectives and show blank spots. For instance, while over 91% of projects promote 1-to-1 stabilization targets to external assets, monetary policy literature suggests that the smoothing of short term volatility is often a more sustainable alternative. Our taxonomy bridges computer science and economics, fostering the transfer of expertise. For example, we find that 38% of the reviewed projects use a combination of exchange rate targeting and specific stabilization techniques that can render them vulnerable to speculative economic attacks - an avoidable design flaw.

</details>

<details>

<summary>2019-05-28 17:47:33 - ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation</summary>

- *Yuzhe Yang, Guo Zhang, Dina Katabi, Zhi Xu*

- `1905.11971v1` - [abs](http://arxiv.org/abs/1905.11971v1) - [pdf](http://arxiv.org/pdf/1905.11971v1)

> Deep neural networks are vulnerable to adversarial attacks. The literature is rich with algorithms that can easily craft successful adversarial examples. In contrast, the performance of defense techniques still lags behind. This paper proposes ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are preprocessed using two steps: first pixels are randomly dropped from the image; then, the image is reconstructed using ME. We show that this process destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image. Since humans typically rely on such global structures in classifying images, the process makes the network mode compatible with human perception. We conduct comprehensive experiments on prevailing benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. Comparing ME-Net with state-of-the-art defense mechanisms shows that ME-Net consistently outperforms prior techniques, improving robustness against both black-box and white-box attacks.

</details>

<details>

<summary>2019-05-28 18:56:44 - Fault Sneaking Attack: a Stealthy Framework for Misleading Deep Neural Networks</summary>

- *Pu Zhao, Siyue Wang, Cheng Gongye, Yanzhi Wang, Yunsi Fei, Xue Lin*

- `1905.12032v1` - [abs](http://arxiv.org/abs/1905.12032v1) - [pdf](http://arxiv.org/pdf/1905.12032v1)

> Despite the great achievements of deep neural networks (DNNs), the vulnerability of state-of-the-art DNNs raises security concerns of DNNs in many application domains requiring high reliability.We propose the fault sneaking attack on DNNs, where the adversary aims to misclassify certain input images into any target labels by modifying the DNN parameters. We apply ADMM (alternating direction method of multipliers) for solving the optimization problem of the fault sneaking attack with two constraints: 1) the classification of the other images should be unchanged and 2) the parameter modifications should be minimized. Specifically, the first constraint requires us not only to inject designated faults (misclassifications), but also to hide the faults for stealthy or sneaking considerations by maintaining model accuracy. The second constraint requires us to minimize the parameter modifications (using L0 norm to measure the number of modifications and L2 norm to measure the magnitude of modifications). Comprehensive experimental evaluation demonstrates that the proposed framework can inject multiple sneaking faults without losing the overall test accuracy performance.

</details>

<details>

<summary>2019-05-29 05:49:57 - Improving Adversarial Robustness via Promoting Ensemble Diversity</summary>

- *Tianyu Pang, Kun Xu, Chao Du, Ning Chen, Jun Zhu*

- `1901.08846v3` - [abs](http://arxiv.org/abs/1901.08846v3) - [pdf](http://arxiv.org/pdf/1901.08846v3)

> Though deep neural networks have achieved significant progress on various tasks, often enhanced by model ensemble, existing high-performance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then constructing a straightforward ensemble, e.g., by directly averaging the outputs, which ignores the interaction among networks. This paper presents a new method that explores the interaction among individual networks to improve robustness for ensemble models. Technically, we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal predictions of individual members, and present an adaptive diversity promoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examples difficult to transfer among individual members. Our method is computationally efficient and compatible with the defense methods acting on individual networks. Empirical results on various datasets verify that our method can improve adversarial robustness while maintaining state-of-the-art accuracy on normal examples.

</details>

<details>

<summary>2019-05-29 07:10:04 - Cognitive Triaging of Phishing Attacks</summary>

- *Amber van der Heijden, Luca Allodi*

- `1905.02162v2` - [abs](http://arxiv.org/abs/1905.02162v2) - [pdf](http://arxiv.org/pdf/1905.02162v2)

> In this paper we employ quantitative measurements of cognitive vulnerability triggers in phishing emails to predict the degree of success of an attack. To achieve this we rely on the cognitive psychology literature and develop an automated and fully quantitative method based on machine learning and econometrics to construct a triaging mechanism built around the cognitive features of a phishing email; we showcase our approach relying on data from the anti-phishing division of a large financial organization in Europe. Our evaluation shows empirically that an effective triaging mechanism for phishing success can be put in place by response teams to effectively prioritize remediation efforts (e.g. domain takedowns), by first acting on those attacks that are more likely to collect high response rates from potential victims.

</details>

<details>

<summary>2019-05-29 15:17:39 - Certified Robustness to Adversarial Examples with Differential Privacy</summary>

- *Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Suman Jana*

- `1802.03471v4` - [abs](http://arxiv.org/abs/1802.03471v4) - [pdf](http://arxiv.org/pdf/1802.03471v4)

> Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks, but they either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google's Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired formalism, that provides a rigorous, generic, and flexible foundation for defense.

</details>

<details>

<summary>2019-05-29 17:10:04 - ATTACK2VEC: Leveraging Temporal Word Embeddings to Understand the Evolution of Cyberattacks</summary>

- *Yun Shen, Gianluca Stringhini*

- `1905.12590v1` - [abs](http://arxiv.org/abs/1905.12590v1) - [pdf](http://arxiv.org/pdf/1905.12590v1)

> Despite the fact that cyberattacks are constantly growing in complexity, the research community still lacks effective tools to easily monitor and understand them. In particular, there is a need for techniques that are able to not only track how prominently certain malicious actions, such as the exploitation of specific vulnerabilities, are exploited in the wild, but also (and more importantly) how these malicious actions factor in as attack steps in more complex cyberattacks. In this paper we present ATTACK2VEC, a system that uses temporal word embeddings to model how attack steps are exploited in the wild, and track how they evolve. We test ATTACK2VEC on a dataset of billions of security events collected from the customers of a commercial Intrusion Prevention System over a period of two years, and show that our approach is effective in monitoring the emergence of new attack strategies in the wild and in flagging which attack steps are often used together by attackers (e.g., vulnerabilities that are frequently exploited together). ATTACK2VEC provides a useful tool for researchers and practitioners to better understand cyberattacks and their evolution, and use this knowledge to improve situational awareness and develop proactive defenses.

</details>

<details>

<summary>2019-05-29 19:52:52 - The Art of The Scam: Demystifying Honeypots in Ethereum Smart Contracts</summary>

- *Christof Ferreira Torres, Mathis Steichen, Radu State*

- `1902.06976v2` - [abs](http://arxiv.org/abs/1902.06976v2) - [pdf](http://arxiv.org/pdf/1902.06976v2)

> Modern blockchains, such as Ethereum, enable the execution of so-called smart contracts - programs that are executed across a decentralised network of nodes. As smart contracts become more popular and carry more value, they become more of an interesting target for attackers. In the past few years, several smart contracts have been exploited by attackers. However, a new trend towards a more proactive approach seems to be on the rise, where attackers do not search for vulnerable contracts anymore. Instead, they try to lure their victims into traps by deploying seemingly vulnerable contracts that contain hidden traps. This new type of contracts is commonly referred to as honeypots. In this paper, we present the first systematic analysis of honeypot smart contracts, by investigating their prevalence, behaviour and impact on the Ethereum blockchain. We develop a taxonomy of honeypot techniques and use this to build HoneyBadger - a tool that employs symbolic execution and well defined heuristics to expose honeypots. We perform a large-scale analysis on more than 2 million smart contracts and show that our tool not only achieves high precision, but is also highly efficient. We identify 690 honeypot smart contracts as well as 240 victims in the wild, with an accumulated profit of more than $90,000 for the honeypot creators. Our manual validation shows that 87% of the reported contracts are indeed honeypots.

</details>

<details>

<summary>2019-05-29 20:02:55 - Fallout: Reading Kernel Writes From User Space</summary>

- *Marina Minkin, Daniel Moghimi, Moritz Lipp, Michael Schwarz, Jo Van Bulck, Daniel Genkin, Daniel Gruss, Frank Piessens, Berk Sunar, Yuval Yarom*

- `1905.12701v1` - [abs](http://arxiv.org/abs/1905.12701v1) - [pdf](http://arxiv.org/pdf/1905.12701v1)

> Recently, out-of-order execution, an important performance optimization in modern high-end processors, has been revealed to pose a significant security threat, allowing information leaks across security domains. In particular, the Meltdown attack leaks information from the operating system kernel to user space, completely eroding the security of the system. To address this and similar attacks, without incurring the performance costs of software countermeasures, Intel includes hardware-based defenses in its recent Coffee Lake R processors.   In this work, we show that the recent hardware defenses are not sufficient. Specifically, we present Fallout, a new transient execution attack that leaks information from a previously unexplored microarchitectural component called the store buffer. We show how unprivileged user processes can exploit Fallout to reconstruct privileged information recently written by the kernel. We further show how Fallout can be used to bypass kernel address space randomization. Finally, we identify and explore microcode assists as a hitherto ignored cause of transient execution.   Fallout affects all processor generations we have tested. However, we notice a worrying regression, where the newer Coffee Lake R processors are more vulnerable to Fallout than older generations.

</details>

<details>

<summary>2019-05-29 22:08:08 - Batch Normalization is a Cause of Adversarial Vulnerability</summary>

- *Angus Galloway, Anna Golubeva, Thomas Tanay, Medhat Moussa, Graham W. Taylor*

- `1905.02161v2` - [abs](http://arxiv.org/abs/1905.02161v2) - [pdf](http://arxiv.org/pdf/1905.02161v2)

> Batch normalization (batch norm) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and noise by double-digit percentages, as we show on five standard datasets. Furthermore, substituting weight decay for batch norm is sufficient to nullify the relationship between adversarial vulnerability and the input dimension. Our work is consistent with a mean-field analysis that found that batch norm causes exploding gradients.

</details>

<details>

<summary>2019-05-30 00:34:50 - Bandlimiting Neural Networks Against Adversarial Attacks</summary>

- *Yuping Lin, Kasra Ahmadi K. A., Hui Jiang*

- `1905.12797v1` - [abs](http://arxiv.org/abs/1905.12797v1) - [pdf](http://arxiv.org/pdf/1905.12797v1)

> In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep ReLU neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the ImageNet dataset have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including FGSM, PGD, DeepFool and C&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over 95% of the adversarial samples generated by these methods without introducing any significant performance degradation (less than 1%) on the original clean images.

</details>

<details>

<summary>2019-05-30 11:11:39 - Mapping Informal Settlements in Developing Countries using Machine Learning and Low Resolution Multi-spectral Data</summary>

- *Bradley Gram-Hansen, Patrick Helber, Indhu Varatharajan, Faiza Azam, Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski*

- `1901.00861v3` - [abs](http://arxiv.org/abs/1901.00861v3) - [pdf](http://arxiv.org/pdf/1901.00861v3)

> Informal settlements are home to the most socially and economically vulnerable people on the planet. In order to deliver effective economic and social aid, non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), require detailed maps of the locations of informal settlements. However, data regarding informal and formal settlements is primarily unavailable and if available is often incomplete. This is due, in part, to the cost and complexity of gathering data on a large scale. To address these challenges, we, in this work, provide three contributions. 1) A brand new machine learning data-set, purposely developed for informal settlement detection. 2) We show that it is possible to detect informal settlements using freely available low-resolution (LR) data, in contrast to previous studies that use very-high resolution (VHR) satellite and aerial imagery, something that is cost-prohibitive for NGOs. 3) We demonstrate two effective classification schemes on our curated data set, one that is cost-efficient for NGOs and another that is cost-prohibitive for NGOs, but has additional utility. We integrate these schemes into a semi-automated pipeline that converts either a LR or VHR satellite image into a binary map that encodes the locations of informal settlements.

</details>

<details>

<summary>2019-05-30 12:19:50 - Generating Material Maps to Map Informal Settlements</summary>

- *Patrick Helber, Bradley Gram-Hansen, Indhu Varatharajan, Faiza Azam, Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski*

- `1812.00786v2` - [abs](http://arxiv.org/abs/1812.00786v2) - [pdf](http://arxiv.org/pdf/1812.00786v2)

> Detecting and mapping informal settlements encompasses several of the United Nations sustainable development goals. This is because informal settlements are home to the most socially and economically vulnerable people on the planet. Thus, understanding where these settlements are is of paramount importance to both government and non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), who can use this information to deliver effective social and economic aid. We propose a method that detects and maps the locations of informal settlements using only freely available, Sentinel-2 low-resolution satellite spectral data and socio-economic data. This is in contrast to previous studies that only use costly very-high resolution (VHR) satellite and aerial imagery. We show how we can detect informal settlements by combining both domain knowledge and machine learning techniques, to build a classifier that looks for known roofing materials used in informal settlements. Please find additional material at https://frontierdevelopmentlab.github.io/informal-settlements/.

</details>

<details>

<summary>2019-05-30 13:02:33 - CharBot: A Simple and Effective Method for Evading DGA Classifiers</summary>

- *Jonathan Peck, Claire Nie, Raaghavi Sivaguru, Charles Grumer, Femi Olumofin, Bin Yu, Anderson Nascimento, Martine De Cock*

- `1905.01078v2` - [abs](http://arxiv.org/abs/1905.01078v2) - [pdf](http://arxiv.org/pdf/1905.01078v2)

> Domain generation algorithms (DGAs) are commonly leveraged by malware to create lists of domain names which can be used for command and control (C&C) purposes. Approaches based on machine learning have recently been developed to automatically detect generated domain names in real-time. In this work, we present a novel DGA called CharBot which is capable of producing large numbers of unregistered domain names that are not detected by state-of-the-art classifiers for real-time detection of DGAs, including the recently published methods FANCI (a random forest based on human-engineered features) and LSTM.MI (a deep learning approach). CharBot is very simple, effective and requires no knowledge of the targeted DGA classifiers. We show that retraining the classifiers on CharBot samples is not a viable defense strategy. We believe these findings show that DGA classifiers are inherently vulnerable to adversarial attacks if they rely only on the domain name string to make a decision. Designing a robust DGA classifier may, therefore, necessitate the use of additional information besides the domain name alone. To the best of our knowledge, CharBot is the simplest and most efficient black-box adversarial attack against DGA classifiers proposed to date.

</details>

<details>

<summary>2019-05-31 06:09:16 - Privacy-Preserving Detection of IoT Devices Connected Behind a NAT in a Smart Home Setup</summary>

- *Yair Meidan, Vinay Sachidananda, Yuval Elovici, Asaf Shabtai*

- `1905.13430v1` - [abs](http://arxiv.org/abs/1905.13430v1) - [pdf](http://arxiv.org/pdf/1905.13430v1)

> Today, telecommunication service providers (telcos) are exposed to cyber-attacks executed by compromised IoT devices connected to their customers' networks. Such attacks might have severe effects not only on the target of attacks but also on the telcos themselves. To mitigate those risks we propose a machine learning based method that can detect devices of specific vulnerable IoT models connected behind a domestic NAT, thereby identifying home networks that pose a risk to the telco's infrastructure and availability of services. As part of the effort to preserve the domestic customers' privacy, our method relies on NetFlow data solely, refraining from inspecting the payload. To promote future research in this domain we share our novel dataset, collected in our lab from numerous and various commercial IoT devices.

</details>

<details>

<summary>2019-05-31 09:26:00 - Security analysis of a self-embedding fragile image watermark scheme</summary>

- *Xinhui Gong, Feng Yu, Xiaohong Zhao, Shihong Wang*

- `1812.11735v2` - [abs](http://arxiv.org/abs/1812.11735v2) - [pdf](http://arxiv.org/pdf/1812.11735v2)

> Recently, a self-embedding fragile watermark scheme based on reference-bits interleaving and adaptive selection of embedding mode was proposed. Reference bits are derived from the scrambled MSB bits of a cover image, and then are combined with authentication bits to form the watermark bits for LSB embedding. We find this algorithm has a feature of block independence of embedding watermark such that it is vulnerable to a collage attack. In addition, because the generation of authentication bits via hash function operations is not related to secret keys, we analyze this algorithm by a multiple stego-image attack. We find that the cost of obtaining all the permutation relations of $l\cdot b^2$ watermark bits of each block (i.e., equivalent permutation keys) is about $(l\cdot b^2)!$ for the embedding mode $(m, l)$, where $m$ MSB layers of a cover image are used for generating reference bits and $l$ LSB layers for embedding watermark, and $b\times b$ is the size of image block. The simulation results and the statistical results demonstrate our analysis is effective.

</details>

<details>

<summary>2019-05-31 10:01:03 - Misbinding Attacks on Secure Device Pairing and Bootstrapping</summary>

- *Mohit Sethi, Aleksi Peltonen, Tuomas Aura*

- `1902.07550v2` - [abs](http://arxiv.org/abs/1902.07550v2) - [pdf](http://arxiv.org/pdf/1902.07550v2)

> In identity misbinding attacks against authenticated key-exchange protocols, a legitimate but compromised participant manipulates the honest parties so that the victim becomes unknowingly associated with a third party. These attacks are well known, and resistance to misbinding is considered a critical requirement for security protocols on the Internet. In the context of device pairing, on the other hand, the attack has received little attention outside the trusted-computing community. This paper points out that most device pairing protocols are vulnerable to misbinding. Device pairing protocols are characterized by lack of a-priory information, such as identifiers and cryptographic roots of trust, about the other endpoint. Therefore, the devices in pairing protocols need to be identified by the user's physical access to them. As case studies for demonstrating the misbinding vulnerability, we use Bluetooth and a protocol that registers new IoT devices to authentication servers on wireless networks. We have implemented the attacks. We also show how the attacks can be found in formal models of the protocols with carefully formulated correspondence assertions. The formal analysis yields a new type of double misbinding attack. While pairing protocols have been extensively modelled and analyzed, misbinding seems to be an aspect that has not previously received sufficient attention. Finally, we discuss potential ways to mitigate the threat and its significance to security of pairing protocols.

</details>


## 2019-06

<details>

<summary>2019-06-01 00:36:16 - On the Effectiveness of Low Frequency Perturbations</summary>

- *Yash Sharma, Gavin Weiguang Ding, Marcus Brubaker*

- `1903.00073v2` - [abs](http://arxiv.org/abs/1903.00073v2) - [pdf](http://arxiv.org/pdf/1903.00073v2)

> Carefully crafted, often imperceptible, adversarial perturbations have been shown to cause state-of-the-art models to yield extremely inaccurate outputs, rendering them unsuitable for safety-critical application domains. In addition, recent work has shown that constraining the attack space to a low frequency regime is particularly effective. Yet, it remains unclear whether this is due to generally constraining the attack search space or specifically removing high frequency components from consideration. By systematically controlling the frequency components of the perturbation, evaluating against the top-placing defense submissions in the NeurIPS 2017 competition, we empirically show that performance improvements in both the white-box and black-box transfer settings are yielded only when low frequency components are preserved. In fact, the defended models based on adversarial training are roughly as vulnerable to low frequency perturbations as undefended models, suggesting that the purported robustness of state-of-the-art ImageNet defenses is reliant upon adversarial perturbations being high frequency in nature. We do find that under $\ell_\infty$ $\epsilon=16/255$, the competition distortion bound, low frequency perturbations are indeed perceptible. This questions the use of the $\ell_\infty$-norm, in particular, as a distortion metric, and, in turn, suggests that explicitly considering the frequency space is promising for learning robust models which better align with human perception.

</details>

<details>

<summary>2019-06-01 11:26:22 - Perceptual Evaluation of Adversarial Attacks for CNN-based Image Classification</summary>

- *Sid Ahmed Fezza, Yassine Bakhti, Wassim Hamidouche, Olivier Déforges*

- `1906.00204v1` - [abs](http://arxiv.org/abs/1906.00204v1) - [pdf](http://arxiv.org/pdf/1906.00204v1)

> Deep neural networks (DNNs) have recently achieved state-of-the-art performance and provide significant progress in many machine learning tasks, such as image classification, speech processing, natural language processing, etc. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. For instance, in the image classification domain, adding small imperceptible perturbations to the input image is sufficient to fool the DNN and to cause misclassification. The perturbed image, called \textit{adversarial example}, should be visually as close as possible to the original image. However, all the works proposed in the literature for generating adversarial examples have used the $L_{p}$ norms ($L_{0}$, $L_{2}$ and $L_{\infty}$) as distance metrics to quantify the similarity between the original image and the adversarial example. Nonetheless, the $L_{p}$ norms do not correlate with human judgment, making them not suitable to reliably assess the perceptual similarity/fidelity of adversarial examples. In this paper, we present a database for visual fidelity assessment of adversarial examples. We describe the creation of the database and evaluate the performance of fifteen state-of-the-art full-reference (FR) image fidelity assessment metrics that could substitute $L_{p}$ norms. The database as well as subjective scores are publicly available to help designing new metrics for adversarial examples and to facilitate future research works.

</details>

<details>

<summary>2019-06-01 17:12:24 - Improving Transferability of Adversarial Examples with Input Diversity</summary>

- *Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, Alan Yuille*

- `1803.06978v4` - [abs](http://arxiv.org/abs/1803.06978v4) - [pdf](http://arxiv.org/pdf/1803.06978v4)

> Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples --- crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. By evaluating our method against top defense solutions and official baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at https://github.com/cihangxie/DI-2-FGSM.

</details>

<details>

<summary>2019-06-03 04:56:53 - Reliable Smart Road Signs</summary>

- *Muhammed O. Sayin, Chung-Wei Lin, Eunsuk Kang, Shinichi Shiraishi, Tamer Basar*

- `1901.10622v2` - [abs](http://arxiv.org/abs/1901.10622v2) - [pdf](http://arxiv.org/pdf/1901.10622v2)

> In this paper, we propose a game theoretical adversarial intervention detection mechanism for reliable smart road signs. A future trend in intelligent transportation systems is ``smart road signs" that incorporate smart codes (e.g., visible at infrared) on their surface to provide more detailed information to smart vehicles. Such smart codes make road sign classification problem aligned with communication settings more than conventional classification. This enables us to integrate well-established results in communication theory, e.g., error-correction methods, into road sign classification problem. Recently, vision-based road sign classification algorithms have been shown to be vulnerable against (even) small scale adversarial interventions that are imperceptible for humans. On the other hand, smart codes constructed via error-correction methods can lead to robustness against small scale intelligent or random perturbations on them. In the recognition of smart road signs, however, humans are out of the loop since they cannot see or interpret them. Therefore, there is no equivalent concept of imperceptible perturbations in order to achieve a comparable performance with humans. Robustness against small scale perturbations would not be sufficient since the attacker can attack more aggressively without such a constraint. Under a game theoretical solution concept, we seek to ensure certain measure of guarantees against even the worst case (intelligent) attackers that can perturb the signal even at large scale. We provide a randomized detection strategy based on the distance between the decoder output and the received input, i.e., error rate. Finally, we examine the performance of the proposed scheme over various scenarios.

</details>

<details>

<summary>2019-06-03 05:07:58 - Deep Learning for Signal Authentication and Security in Massive Internet of Things Systems</summary>

- *Aidin Ferdowsi, Walid Saad*

- `1803.00916v2` - [abs](http://arxiv.org/abs/1803.00916v2) - [pdf](http://arxiv.org/pdf/1803.00916v2)

> Secure signal authentication is arguably one of the most challenging problems in the Internet of Things (IoT) environment, due to the large-scale nature of the system and its susceptibility to man-in-the-middle and eavesdropping attacks. In this paper, a novel deep learning method is proposed for dynamic authentication of IoT signals to detect cyber attacks. The proposed learning framework, based on a long short-term memory (LSTM) structure, enables the IoT devices (IoTDs) to extract a set of stochastic features from their generated signal and dynamically watermark these features into the signal. This method enables the cloud, which collects signals from the IoT devices, to effectively authenticate the reliability of the signals. Moreover, in massive IoT scenarios, since the cloud cannot authenticate all the IoTDs simultaneously due to computational limitations, a game-theoretic framework is proposed to improve the cloud's decision making process by predicting vulnerable IoTDs. The mixed-strategy Nash equilibrium (MSNE) for this game is derived and the uniqueness of the expected utility at the equilibrium is proven. In the massive IoT system, due to a large set of available actions for the cloud, it is shown that analytically deriving the MSNE is challenging and, thus, a learning algorithm proposed that converges to the MSNE. Moreover, in order to cope with the incomplete information case in which the cloud cannot access the state of the unauthenticated IoTDs, a deep reinforcement learning algorithm is proposed to dynamically predict the state of unauthenticated IoTDs and allow the cloud to decide on which IoTDs to authenticate. Simulation results show that, with an attack detection delay of under 1 second the messages can be transmitted from IoT devices with an almost 100% reliability.

</details>

<details>

<summary>2019-06-03 09:57:24 - Automated Ground Truth Estimation For Automotive Radar Tracking Applications With Portable GNSS And IMU Devices</summary>

- *Nicolas Scheiner, Stefan Haag, Nils Appenrodt, Bharanidhar Duraisamy, Jürgen Dickmann, Martin Fritzsche, Bernhard Sick*

- `1905.11987v2` - [abs](http://arxiv.org/abs/1905.11987v2) - [pdf](http://arxiv.org/pdf/1905.11987v2)

> Baseline generation for tracking applications is a difficult task when working with real world radar data. Data sparsity usually only allows an indirect way of estimating the original tracks as most objects' centers are not represented in the data. This article proposes an automated way of acquiring reference trajectories by using a highly accurate hand-held global navigation satellite system (GNSS). An embedded inertial measurement unit (IMU) is used for estimating orientation and motion behavior. This article contains two major contributions. A method for associating radar data to vulnerable road user (VRU) tracks is described. It is evaluated how accurate the system performs under different GNSS reception conditions and how carrying a reference system alters radar measurements. Second, the system is used to track pedestrians and cyclists over many measurement cycles in order to generate object centered occupancy grid maps. The reference system allows to much more precisely generate real world radar data distributions of VRUs than compared to conventional methods. Hereby, an important step towards radar-based VRU tracking is accomplished.

</details>

<details>

<summary>2019-06-03 10:03:31 - The Adversarial Machine Learning Conundrum: Can The Insecurity of ML Become The Achilles' Heel of Cognitive Networks?</summary>

- *Muhammad Usama, Junaid Qadir, Ala Al-Fuqaha, Mounir Hamdi*

- `1906.00679v1` - [abs](http://arxiv.org/abs/1906.00679v1) - [pdf](http://arxiv.org/pdf/1906.00679v1)

> The holy grail of networking is to create \textit{cognitive networks} that organize, manage, and drive themselves. Such a vision now seems attainable thanks in large part to the progress in the field of machine learning (ML), which has now already disrupted a number of industries and revolutionized practically all fields of research. But are the ML models foolproof and robust to security attacks to be in charge of managing the network? Unfortunately, many modern ML models are easily misled by simple and easily-crafted adversarial perturbations, which does not bode well for the future of ML-based cognitive networks unless ML vulnerabilities for the cognitive networking environment are identified, addressed, and fixed. The purpose of this article is to highlight the problem of insecure ML and to sensitize the readers to the danger of adversarial ML by showing how an easily-crafted adversarial ML example can compromise the operations of the cognitive self-driving network. In this paper, we demonstrate adversarial attacks on two simple yet representative cognitive networking applications (namely, intrusion detection and network traffic classification). We also provide some guidelines to design secure ML models for cognitive networks that are robust to adversarial attacks on the ML pipeline of cognitive networks.

</details>

<details>

<summary>2019-06-03 10:50:42 - Adversarial Risk Bounds for Neural Networks through Sparsity based Compression</summary>

- *Emilio Rafael Balda, Arash Behboodi, Niklas Koep, Rudolf Mathar*

- `1906.00698v1` - [abs](http://arxiv.org/abs/1906.00698v1) - [pdf](http://arxiv.org/pdf/1906.00698v1)

> Neural networks have been shown to be vulnerable against minor adversarial perturbations of their inputs, especially for high dimensional data under $\ell_\infty$ attacks. To combat this problem, techniques like adversarial training have been employed to obtain models which are robust on the training set. However, the robustness of such models against adversarial perturbations may not generalize to unseen data. To study how robustness generalizes, recent works assume that the inputs have bounded $\ell_2$-norm in order to bound the adversarial risk for $\ell_\infty$ attacks with no explicit dimension dependence. In this work we focus on $\ell_\infty$ attacks on $\ell_\infty$ bounded inputs and prove margin-based bounds. Specifically, we use a compression based approach that relies on efficiently compressing the set of tunable parameters without distorting the adversarial risk. To achieve this, we apply the concept of effective sparsity and effective joint sparsity on the weight matrices of neural networks. This leads to bounds with no explicit dependence on the input dimension, neither on the number of classes. Our results show that neural networks with approximately sparse weight matrices not only enjoy enhanced robustness, but also better generalization.

</details>

<details>

<summary>2019-06-03 15:20:47 - SmartSeed: Smart Seed Generation for Efficient Fuzzing</summary>

- *Chenyang Lyu, Shouling Ji, Yuwei Li, Junfeng Zhou, Jianhai Chen, Jing Chen*

- `1807.02606v3` - [abs](http://arxiv.org/abs/1807.02606v3) - [pdf](http://arxiv.org/pdf/1807.02606v3)

> Fuzzing is an automated application vulnerability detection method. For genetic algorithm-based fuzzing, it can mutate the seed files provided by users to obtain a number of inputs, which are then used to test the objective application in order to trigger potential crashes. As shown in existing literature, the seed file selection is crucial for the efficiency of fuzzing. However, current seed selection strategies do not seem to be better than randomly picking seed files. Therefore, in this paper, we propose a novel and generic system, named SmartSeed, to generate seed files towards efficient fuzzing. Specifically, SmartSeed is designed based on a machine learning model to learn and generate high-value binary seeds. We evaluate SmartSeed along with American Fuzzy Lop (AFL) on 12 open-source applications with the input formats of mp3, bmp or flv. We also combine SmartSeed with different fuzzing tools to examine its compatibility. From extensive experiments, we find that SmartSeed has the following advantages: First, it only requires tens of seconds to generate sufficient high-value seeds. Second, it can generate seeds with multiple kinds of input formats and significantly improves the fuzzing performance for most applications with the same input format. Third, SmartSeed is compatible to different fuzzing tools. In total, our system discovers more than twice unique crashes and 5,040 extra unique paths than the existing best seed selection strategy for the evaluated 12 applications. From the crashes found by SmartSeed, we discover 16 new vulnerabilities and have received their CVE IDs.

</details>

<details>

<summary>2019-06-03 15:47:47 - Secure Distributed On-Device Learning Networks With Byzantine Adversaries</summary>

- *Yanjie Dong, Julian Cheng, Md. Jahangir Hossain, Victor C. M. Leung*

- `1906.00887v1` - [abs](http://arxiv.org/abs/1906.00887v1) - [pdf](http://arxiv.org/pdf/1906.00887v1)

> The privacy concern exists when the central server has the copies of datasets. Hence, there is a paradigm shift for the learning networks to change from centralized in-cloud learning to distributed \mbox{on-device} learning. Benefit from the parallel computing, the on-device learning networks have a lower bandwidth requirement than the in-cloud learning networks. Moreover, the on-device learning networks also have several desirable characteristics such as privacy preserving and flexibility. However, the \mbox{on-device} learning networks are vulnerable to the malfunctioning terminals across the networks. The worst-case malfunctioning terminals are the Byzantine adversaries, that can perform arbitrary harmful operations to compromise the learned model based on the full knowledge of the networks. Hence, the design of secure learning algorithms becomes an emerging topic in the on-device learning networks with Byzantine adversaries. In this article, we present a comprehensive overview of the prevalent secure learning algorithms for the two promising on-device learning networks: Federated-Learning networks and decentralized-learning networks. We also review several future research directions in the \mbox{Federated-Learning} and decentralized-learning networks.

</details>

<details>

<summary>2019-06-03 18:28:09 - Terminal Brain Damage: Exposing the Graceless Degradation in Deep Neural Networks Under Hardware Fault Attacks</summary>

- *Sanghyun Hong, Pietro Frigo, Yiğitcan Kaya, Cristiano Giuffrida, Tudor Dumitraş*

- `1906.01017v1` - [abs](http://arxiv.org/abs/1906.01017v1) - [pdf](http://arxiv.org/pdf/1906.01017v1)

> Deep neural networks (DNNs) have been shown to tolerate "brain damage": cumulative changes to the network's parameters (e.g., pruning, numerical perturbations) typically result in a graceful degradation of classification accuracy. However, the limits of this natural resilience are not well understood in the presence of small adversarial changes to the DNN parameters' underlying memory representation, such as bit-flips that may be induced by hardware fault attacks. We study the effects of bitwise corruptions on 19 DNN models---six architectures on three image classification tasks---and we show that most models have at least one parameter that, after a specific bit-flip in their bitwise representation, causes an accuracy loss of over 90%. We employ simple heuristics to efficiently identify the parameters likely to be vulnerable. We estimate that 40-50% of the parameters in a model might lead to an accuracy drop greater than 10% when individually subjected to such single-bit perturbations. To demonstrate how an adversary could take advantage of this vulnerability, we study the impact of an exemplary hardware fault attack, Rowhammer, on DNNs. Specifically, we show that a Rowhammer enabled attacker co-located in the same physical machine can inflict significant accuracy drops (up to 99%) even with single bit-flip corruptions and no knowledge of the model. Our results expose the limits of DNNs' resilience against parameter perturbations induced by real-world fault attacks. We conclude by discussing possible mitigations and future research directions towards fault attack-resilient DNNs.

</details>

<details>

<summary>2019-06-03 22:43:54 - RL-Based Method for Benchmarking the Adversarial Resilience and Robustness of Deep Reinforcement Learning Policies</summary>

- *Vahid Behzadan, William Hsu*

- `1906.01110v1` - [abs](http://arxiv.org/abs/1906.01110v1) - [pdf](http://arxiv.org/pdf/1906.01110v1)

> This paper investigates the resilience and robustness of Deep Reinforcement Learning (DRL) policies to adversarial perturbations in the state space. We first present an approach for the disentanglement of vulnerabilities caused by representation learning of DRL agents from those that stem from the sensitivity of the DRL policies to distributional shifts in state transitions. Building on this approach, we propose two RL-based techniques for quantitative benchmarking of adversarial resilience and robustness in DRL policies against perturbations of state transitions. We demonstrate the feasibility of our proposals through experimental evaluation of resilience and robustness in DQN, A2C, and PPO2 policies trained in the Cartpole environment.

</details>

<details>

<summary>2019-06-03 23:38:33 - Adversarial Exploitation of Policy Imitation</summary>

- *Vahid Behzadan, William Hsu*

- `1906.01121v1` - [abs](http://arxiv.org/abs/1906.01121v1) - [pdf](http://arxiv.org/pdf/1906.01121v1)

> This paper investigates a class of attacks targeting the confidentiality aspect of security in Deep Reinforcement Learning (DRL) policies. Recent research have established the vulnerability of supervised machine learning models (e.g., classifiers) to model extraction attacks. Such attacks leverage the loosely-restricted ability of the attacker to iteratively query the model for labels, thereby allowing for the forging of a labeled dataset which can be used to train a replica of the original model. In this work, we demonstrate the feasibility of exploiting imitation learning techniques in launching model extraction attacks on DRL agents. Furthermore, we develop proof-of-concept attacks that leverage such techniques for black-box attacks against the integrity of DRL policies. We also present a discussion on potential solution concepts for mitigation techniques.

</details>

<details>

<summary>2019-06-04 09:06:40 - Need for Critical Cyber Defence, Security Strategy and Privacy Policy in Bangladesh - Hype or Reality?</summary>

- *AKM Bahalul Haque*

- `1906.01285v1` - [abs](http://arxiv.org/abs/1906.01285v1) - [pdf](http://arxiv.org/pdf/1906.01285v1)

> Cyber security is one of the burning issues in modern world. Increased IT infrastructure has given rise to enormous chances of security breach. Bangladesh being a relatively new member of cyber security arena has its own demand and appeal. Digitalization is happening in Bangladesh for last few years at an appreciable rate. People are being connected to the worldwide web community with their smart devices. These devices have their own vulnerability issues as well as the data shared over the internet has a very good chances of getting breached. Common vulnerability issues like infecting the device with malware, Trojan, virus are on the rise. Moreover, a lack of proper cyber security policy and strategy might make the existing situation at the vulnerable edge of tipping point. Hence the upcoming new infrastructures will be at a greater risk if the issues are not dealt with at an early age. In this paper common vulnerability issues including their recent attacks on cyber space of Bangladesh, cyber security strategy and need for data privacy policy is discussed and analysed briefly.

</details>

<details>

<summary>2019-06-04 14:35:32 - What do AI algorithms actually learn? - On false structures in deep learning</summary>

- *Laura Thesing, Vegard Antun, Anders C. Hansen*

- `1906.01478v1` - [abs](http://arxiv.org/abs/1906.01478v1) - [pdf](http://arxiv.org/pdf/1906.01478v1)

> There are two big unsolved mathematical questions in artificial intelligence (AI): (1) Why is deep learning so successful in classification problems and (2) why are neural nets based on deep learning at the same time universally unstable, where the instabilities make the networks vulnerable to adversarial attacks. We present a solution to these questions that can be summed up in two words; false structures. Indeed, deep learning does not learn the original structures that humans use when recognising images (cats have whiskers, paws, fur, pointy ears, etc), but rather different false structures that correlate with the original structure and hence yield the success. However, the false structure, unlike the original structure, is unstable. The false structure is simpler than the original structure, hence easier to learn with less data and the numerical algorithm used in the training will more easily converge to the neural network that captures the false structure. We formally define the concept of false structures and formulate the solution as a conjecture. Given that trained neural networks always are computed with approximations, this conjecture can only be established through a combination of theoretical and computational results similar to how one establishes a postulate in theoretical physics (e.g. the speed of light is constant). Establishing the conjecture fully will require a vast research program characterising the false structures. We provide the foundations for such a program establishing the existence of the false structures in practice. Finally, we discuss the far reaching consequences the existence of the false structures has on state-of-the-art AI and Smale's 18th problem.

</details>

<details>

<summary>2019-06-04 14:57:19 - A backdoor attack against LSTM-based text classification systems</summary>

- *Jiazhu Dai, Chuanshuai Chen*

- `1905.12457v2` - [abs](http://arxiv.org/abs/1905.12457v2) - [pdf](http://arxiv.org/pdf/1905.12457v2)

> With the widespread use of deep learning system in many applications, the adversary has strong incentive to explore vulnerabilities of deep neural networks and manipulate them. Backdoor attacks against deep neural networks have been reported to be a new type of threat. In this attack, the adversary will inject backdoors into the model and then cause the misbehavior of the model through inputs including backdoor triggers. Existed research mainly focuses on backdoor attacks in image classification based on CNN, little attention has been paid to the backdoor attacks in RNN. In this paper, we implement a backdoor attack in text classification based on LSTM by data poisoning. When the backdoor is injected, the model will misclassify any text samples that contains a specific trigger sentence into the target category determined by the adversary. The existence of the backdoor trigger is stealthy and the backdoor injected has little impact on the performance of the model. We consider the backdoor attack in black-box setting where the adversary has no knowledge of model structures or training algorithms except for small amount of training data. We verify the attack through sentiment analysis on the dataset of IMDB movie reviews. The experimental results indicate that our attack can achieve around 95% success rate with 1% poisoning rate.

</details>

<details>

<summary>2019-06-05 12:04:45 - Inspection Guidelines to Identify Security Design Flaws</summary>

- *Katja Tuma, Danial Hosseini, Kyriakos Malamas, Riccardo Scandariato*

- `1906.01961v1` - [abs](http://arxiv.org/abs/1906.01961v1) - [pdf](http://arxiv.org/pdf/1906.01961v1)

> Recent trends in the software development practices (Agile, DevOps, CI) have shortened the development life-cycle causing the need for efficient security-by-design approaches. In this context, software architectures are analyzed for potential vulnerabilities and design flaws. Yet, design flaws are often documented with natural language and require a manual analysis, which is inefficient. Besides low-level vulnerability databases (e.g., CWE, CAPEC) there is little systematized knowledge on security design flaws. The purpose of this work is to provide a catalog of security design flaws and to empirically evaluate the inspection guidelines for detecting security design flaws. To this aim, we present a catalog of 19 security design flaws and conduct empirical studies with master and doctoral students. This paper contributes with: (i) a catalog of security design flaws, (ii) an empirical evaluation of the inspection guidelines with master students, and (iii) a replicated evaluation with doctoral students. We also account for the shortcomings of the inspection guidelines and make suggestions for their improvement with respect to the generalization of guidelines, catalog re-organization, and format of documentation. We record similar precision, recall, and productivity in both empirical studies and discuss the potential for automating the security design flaw detection.

</details>

<details>

<summary>2019-06-05 18:23:31 - Updating the Wassenaar Debate Once Again: Surveillance, Intrusion Software, and Ambiguity</summary>

- *Jukka Ruohonen, Kai Kimppa*

- `1906.02235v1` - [abs](http://arxiv.org/abs/1906.02235v1) - [pdf](http://arxiv.org/pdf/1906.02235v1)

> This paper analyzes a recent debate on regulating cyber weapons through multilateral export controls. The background relates to the amending of the international Wassenaar Arrangement with offensive cyber security technologies known as intrusion software. Implicitly, such software is related to previously unregulated software vulnerabilities and exploits, which also make the ongoing debate particularly relevant. By placing the debate into a historical context, the paper reveals interesting historical parallels, elaborates the political background, and underlines many ambiguity problems related to rigorous definitions for cyber weapons. Many difficult problems remaining for framing offensive security tools with multilateral export controls are also pointed out.

</details>

<details>

<summary>2019-06-05 19:55:00 - Investigation of Cyber Attacks on a Water Distribution System</summary>

- *Sridhar Adepu, Venkata Reddy Palleti, Gyanendra Mishra, Aditya Mathur*

- `1906.02279v1` - [abs](http://arxiv.org/abs/1906.02279v1) - [pdf](http://arxiv.org/pdf/1906.02279v1)

> A Cyber Physical System (CPS) consists of cyber components for computation and communication, and physical components such as sensors and actuators for process control. These components are networked and interact in a feedback loop. CPS are found in critical infrastructure such as water distribution, power grid, and mass transportation. Often these systems are vulnerable to attacks as the cyber components such as Supervisory Control and Data Acquisition workstations, Human Machine Interface and Programmable Logic Controllers are potential targets for attackers. In this work, we report a study to investigate the impact of cyber attacks on a water distribution (WADI) system. Attacks were designed to meet attacker objectives and launched on WADI using a specially designed tool. This tool enables the launch of single and multi-point attacks where the latter are designed to specifically hide one or more attacks. The outcome of the experiments led to a better understanding of attack propagation and behavior of WADI in response to the attacks as well as to the design of an attack detection mechanism for water distribution system.

</details>

<details>

<summary>2019-06-05 20:52:44 - StreamBox-TZ: Secure Stream Analytics at the Edge with TrustZone</summary>

- *Heejin Park, Shuang Zhai, Long Lu, Felix Xiaozhu Lin*

- `1808.05078v2` - [abs](http://arxiv.org/abs/1808.05078v2) - [pdf](http://arxiv.org/pdf/1808.05078v2)

> While it is compelling to process large streams of IoT data on the cloud edge, doing so exposes the data to a sophisticated, vulnerable software stack on the edge and hence security threats. To this end, we advocate isolating the data and its computations in a trusted execution environment (TEE) on the edge, shielding them from the remaining edge software stack which we deem untrusted. This approach faces two major challenges: (1) executing high-throughput, low-delay stream analytics in a single TEE, which is constrained by a low trusted computing base (TCB) and limited physical memory; (2) verifying execution of stream analytics as the execution involves untrusted software components on the edge. In response, we present StreamBox-TZ (SBT), a stream analytics engine for an edge platform that offers strong data security, verifiable results, and good performance. SBT contributes a data plane designed and optimized for a TEE based on ARM TrustZone. It supports continuous remote attestation for analytics correctness and result freshness while incurring low overhead. SBT only adds 42.5 KB executable to the TCB (16% of the entire TCB). On an octa core ARMv8 platform, it delivers the state-of-the-art performance by processing input events up to 140 MB/sec (12M events/sec) with sub-second delay. The overhead incurred by SBT's security mechanism is less than 25%.

</details>

<details>

<summary>2019-06-06 07:02:04 - Robust Neural Machine Translation with Doubly Adversarial Inputs</summary>

- *Yong Cheng, Lu Jiang, Wolfgang Macherey*

- `1906.02443v1` - [abs](http://arxiv.org/abs/1906.02443v1) - [pdf](http://arxiv.org/pdf/1906.02443v1)

> Neural machine translation (NMT) often suffers from the vulnerability to noisy perturbations in the input. We propose an approach to improving the robustness of NMT models, which consists of two parts: (1) attack the translation model with adversarial source examples; (2) defend the translation model with adversarial target inputs to improve its robustness against the adversarial source inputs.For the generation of adversarial inputs, we propose a gradient-based method to craft adversarial examples informed by the translation loss over the clean inputs.Experimental results on Chinese-English and English-German translation tasks demonstrate that our approach achieves significant improvements ($2.8$ and $1.6$ BLEU points) over Transformer on standard clean benchmarks as well as exhibiting higher robustness on noisy data.

</details>

<details>

<summary>2019-06-06 09:31:14 - Understanding Adversarial Behavior of DNNs by Disentangling Non-Robust and Robust Components in Performance Metric</summary>

- *Yujun Shi, Benben Liao, Guangyong Chen, Yun Liu, Ming-Ming Cheng, Jiashi Feng*

- `1906.02494v1` - [abs](http://arxiv.org/abs/1906.02494v1) - [pdf](http://arxiv.org/pdf/1906.02494v1)

> The vulnerability to slight input perturbations is a worrying yet intriguing property of deep neural networks (DNNs). Despite many previous works studying the reason behind such adversarial behavior, the relationship between the generalization performance and adversarial behavior of DNNs is still little understood. In this work, we reveal such relation by introducing a metric characterizing the generalization performance of a DNN. The metric can be disentangled into an information-theoretic non-robust component, responsible for adversarial behavior, and a robust component. Then, we show by experiments that current DNNs rely heavily on optimizing the non-robust component in achieving decent performance. We also demonstrate that current state-of-the-art adversarial training algorithms indeed try to robustify the DNNs by preventing them from using the non-robust component to distinguish samples from different categories. Also, based on our findings, we take a step forward and point out the possible direction for achieving decent standard performance and adversarial robustness simultaneously. We believe that our theory could further inspire the community to make more interesting discoveries about the relationship between standard generalization and adversarial generalization of deep learning models.

</details>

<details>

<summary>2019-06-07 05:41:56 - Smart Contract Design Meets State Machine Synthesis: Case Studies</summary>

- *Dmitrii Suvorov, Vladimir Ulyantsev*

- `1906.02906v1` - [abs](http://arxiv.org/abs/1906.02906v1) - [pdf](http://arxiv.org/pdf/1906.02906v1)

> Modern blockchain systems support creation of smart contracts -- stateful programs hosted and executed on a blockchain. Smart contracts hold and transfer significant amounts of digital currency which makes them an attractive target for security attacks. It has been shown that many contracts deployed to public ledgers contain security vulnerabilities. Moreover, the design of blockchain systems does not allow the code of the smart contract to be changed after it has been deployed to the system. Therefore, it is important to guarantee the correctness of smart contracts prior to their deployment.   Formal verification is widely used to check smart contracts for correctness with respect to given specification. In this work we consider program synthesis techniques in which the specification is used to generate correct-by-construction programs. We focus on one of the special cases of program synthesis where programs are modeled with finite state machines (FSMs). We show how FSM synthesis can be applied to the problem of automatic smart contract generation. Several case studies of smart contracts are outlined: crowdfunding platform, blinded auction and a license contract. For each case study we specify the corresponding smart contract with a set of formulas in linear temporal logic (LTL) and use this specification together with test scenarios to synthesize a FSM model for that contract. These models are later used to generate executable Solidity code which can be directly used in a blockchain system.

</details>

<details>

<summary>2019-06-07 06:42:04 - Towards Safer Smart Contracts: A Sequence Learning Approach to Detecting Security Threats</summary>

- *Wesley Joon-Wie Tann, Xing Jie Han, Sourav Sen Gupta, Yew-Soon Ong*

- `1811.06632v3` - [abs](http://arxiv.org/abs/1811.06632v3) - [pdf](http://arxiv.org/pdf/1811.06632v3)

> Symbolic analysis of security exploits in smart contracts has demonstrated to be valuable for analyzing predefined vulnerability properties. While some symbolic tools perform complex analysis steps, they require a predetermined invocation depth to search vulnerable execution paths, and the search time increases with depth. The number of contracts on blockchains like Ethereum has increased 176 fold since December 2015. If these symbolic tools fail to analyze the increasingly large number of contracts in time, entire classes of exploits could cause irrevocable damage. In this paper, we aim to have safer smart contracts against emerging threats. We propose the approach of sequential learning of smart contract weaknesses using machine learning---long-short term memory (LSTM)---that allows us to be able to detect new attack trends relatively quickly, leading to safer smart contracts. Our experimental studies on 620,000 smart contracts prove that our model can easily scale to analyze a massive amount of contracts; that is, the LSTM maintains near constant analysis time as contracts increase in complexity. In addition, our approach achieves $99\%$ test accuracy and correctly analyzes contracts that were false positive (FP) errors made by a symbolic tool.

</details>

<details>

<summary>2019-06-07 09:49:11 - Small World with High Risks: A Study of Security Threats in the npm Ecosystem</summary>

- *Markus Zimmermann, Cristian-Alexandru Staicu, Cam Tenny, Michael Pradel*

- `1902.09217v2` - [abs](http://arxiv.org/abs/1902.09217v2) - [pdf](http://arxiv.org/pdf/1902.09217v2)

> The popularity of JavaScript has lead to a large ecosystem of third-party packages available via the npm software package registry. The open nature of npm has boosted its growth, providing over 800,000 free and reusable software packages. Unfortunately, this open nature also causes security risks, as evidenced by recent incidents of single packages that broke or attacked software running on millions of computers. This paper studies security risks for users of npm by systematically analyzing dependencies between packages, the maintainers responsible for these packages, and publicly reported security issues. Studying the potential for running vulnerable or malicious code due to third-party dependencies, we find that individual packages could impact large parts of the entire ecosystem. Moreover, a very small number of maintainer accounts could be used to inject malicious code into the majority of all packages, a problem that has been increasing over time. Studying the potential for accidentally using vulnerable code, we find that lack of maintenance causes many packages to depend on vulnerable code, even years after a vulnerability has become public. Our results provide evidence that npm suffers from single points of failure and that unmaintained packages threaten large code bases. We discuss several mitigation techniques, such as trusted maintainers and total first-party security, and analyze their potential effectiveness.

</details>

<details>

<summary>2019-06-07 10:52:15 - Reconstruction and Membership Inference Attacks against Generative Models</summary>

- *Benjamin Hilprecht, Martin Härterich, Daniel Bernau*

- `1906.03006v1` - [abs](http://arxiv.org/abs/1906.03006v1) - [pdf](http://arxiv.org/pdf/1906.03006v1)

> We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.

</details>

<details>

<summary>2019-06-07 15:32:43 - Convolutional Neural Network for Intrusion Detection System In Cyber Physical Systems</summary>

- *Gael Kamdem De Teyou, Junior Ziazet*

- `1905.03168v2` - [abs](http://arxiv.org/abs/1905.03168v2) - [pdf](http://arxiv.org/pdf/1905.03168v2)

> The extensive use of Information and Communication Technology in critical infrastructures such as Industrial Control Systems make them vulnerable to cyber-attacks. One particular class of cyber-attacks is advanced persistent threats where highly skilled attackers can steal user authentication information's and move in the network from host to host until a valuable target is reached. The detection of the attacker should occur as soon as possible in order to take appropriate response, otherwise the attacker will have enough time to reach sensitive assets. When facing intelligent threats, intelligent solutions have to be designed. Therefore, in this paper, we take advantage of recent progress in deep learning to build a convolutional neural networks that can detect intrusions in cyber physical system. The Intrusion Detection System is applied on the NSL-KDD dataset and the performances of the proposed approach are presented and compared with the state of art. Results show the effectiveness of the techniques.

</details>

<details>

<summary>2019-06-08 22:25:52 - An Empirical Evaluation of Adversarial Robustness under Transfer Learning</summary>

- *Todor Davchev, Timos Korres, Stathi Fotiadis, Nick Antonopoulos, Subramanian Ramamoorthy*

- `1905.02675v4` - [abs](http://arxiv.org/abs/1905.02675v4) - [pdf](http://arxiv.org/pdf/1905.02675v4)

> In this work, we evaluate adversarial robustness in the context of transfer learning from a source trained on CIFAR 100 to a target network trained on CIFAR 10. Specifically, we study the effects of using robust optimisation in the source and target networks. This allows us to identify transfer learning strategies under which adversarial defences are successfully retained, in addition to revealing potential vulnerabilities. We study the extent to which features learnt by a fast gradient sign method (FGSM) and its iterative alternative (PGD) can preserve their defence properties against black and white-box attacks under three different transfer learning strategies. We find that using PGD examples during training on the source task leads to more general robust features that are easier to transfer. Furthermore, under successful transfer, it achieves 5.2% more accuracy against white-box PGD attacks than suitable baselines. Overall, our empirical evaluations give insights on how well adversarial robustness under transfer learning can generalise.

</details>

<details>

<summary>2019-06-09 10:22:52 - On the Vulnerability of Capsule Networks to Adversarial Attacks</summary>

- *Felix Michels, Tobias Uelwer, Eric Upschulte, Stefan Harmeling*

- `1906.03612v1` - [abs](http://arxiv.org/abs/1906.03612v1) - [pdf](http://arxiv.org/pdf/1906.03612v1)

> This paper extensively evaluates the vulnerability of capsule networks to different adversarial attacks. Recent work suggests that these architectures are more robust towards adversarial attacks than other neural networks. However, our experiments show that capsule networks can be fooled as easily as convolutional neural networks.

</details>

<details>

<summary>2019-06-10 00:51:44 - Improved Adversarial Robustness via Logit Regularization Methods</summary>

- *Cecilia Summers, Michael J. Dinneen*

- `1906.03749v1` - [abs](http://arxiv.org/abs/1906.03749v1) - [pdf](http://arxiv.org/pdf/1906.03749v1)

> While great progress has been made at making neural networks effective across a wide range of visual tasks, most models are surprisingly vulnerable. This frailness takes the form of small, carefully chosen perturbations of their input, known as adversarial examples, which represent a security threat for learned vision models in the wild -- a threat which should be responsibly defended against in safety-critical applications of computer vision. In this paper, we advocate for and experimentally investigate the use of a family of logit regularization techniques as an adversarial defense, which can be used in conjunction with other methods for creating adversarial robustness at little to no marginal cost. We also demonstrate that much of the effectiveness of one recent adversarial defense mechanism can in fact be attributed to logit regularization, and show how to improve its defense against both white-box and black-box attacks, in the process creating a stronger black-box attack against PGD-based models. We validate our methods on three datasets and include results on both gradient-free attacks and strong gradient-based iterative attacks with as many as 1,000 steps.

</details>

<details>

<summary>2019-06-10 10:11:49 - Malware Detection with LSTM using Opcode Language</summary>

- *Renjie Lu*

- `1906.04593v1` - [abs](http://arxiv.org/abs/1906.04593v1) - [pdf](http://arxiv.org/pdf/1906.04593v1)

> Nowadays, with the booming development of Internet and software industry, more and more malware variants are designed to perform various malicious activities. Traditional signature-based detection methods can not detect variants of malware. In addition, most behavior-based methods require a secure and isolated environment to perform malware detection, which is vulnerable to be contaminated. In this paper, similar to natural language processing, we propose a novel and efficient approach to perform static malware analysis, which can automatically learn the opcode sequence patterns of malware. We propose modeling malware as a language and assess the feasibility of this approach. First, We use the disassembly tool IDA Pro to obtain opcode sequence of malware. Then the word embedding technique is used to learn the feature vector representation of opcode. Finally, we propose a two-stage LSTM model for malware detection, which use two LSTM layers and one mean-pooling layer to obtain the feature representations of opcode sequences of malwares. We perform experiments on the dataset that includes 969 malware and 123 benign files. In terms of malware detection and malware classification, the evaluation results show our proposed method can achieve average AUC of 0.99 and average AUC of 0.987 in best case, respectively.

</details>

<details>

<summary>2019-06-11 01:32:05 - Secure Software-Defined Networking Based on Blockchain</summary>

- *Weng Jiasi, Weng Jian, Liu Jia-Nan, Zhang Yue*

- `1906.04342v1` - [abs](http://arxiv.org/abs/1906.04342v1) - [pdf](http://arxiv.org/pdf/1906.04342v1)

> Software-Defined Networking (SDN) separates the network control plane and data plane, which provides a network-wide view with centralized control (in the control plane) and programmable network configuration for data plane injected by SDN applications (in the application plane). With these features, a number of drawbacks of the traditional network architectures such as static configuration, non-scalability and low efficiency can be effectively avoided. However, SDN also brings with it some new security challenges, such as single-point failure of the control plane, malicious flows from applications, exposed network-wide resources and a vulnerable channel between the control plane and the data plane. In this paper, we design a monolithic security mechanism for SDN based on Blockchain. Our mechanism decentralizes the control plane to overcome single-point failure while maintaining a network-wide view. The mechanism also guarantees the authenticity, traceability, and accountability of application flows, and hence secures the programmable configuration. Moreover, the mechanism provides a fine-grained access control of network-wide resources and a secure controller-switch channel to further protect resources and communication in SDN.

</details>

<details>

<summary>2019-06-11 07:58:19 - Sharing of vulnerability information among companies -- a survey of Swedish companies</summary>

- *Thomas Olsson, Martin Hell, Martin Höst, Ulrik Franke, Markus Borg*

- `1906.04424v1` - [abs](http://arxiv.org/abs/1906.04424v1) - [pdf](http://arxiv.org/pdf/1906.04424v1)

> Software products are rarely developed from scratch and vulnerabilities in such products might reside in parts that are either open source software or provided by another organization. Hence, the total cybersecurity of a product often depends on cooperation, explicit or implicit, between several organizations. We study the attitudes and practices of companies in software ecosystems towards sharing vulnerability information. Furthermore, we compare these practices to contemporary cybersecurity recommendations. This is performed through a questionnaire-based qualitative survey. The questionnaire is divided into two parts: the providers' perspective and the acquirers' perspective. The results show that companies are willing to share information with each other regarding vulnerabilities. Sharing is not considered to be harmful neither to the cybersecurity nor their business, even though a majority of the respondents consider vulnerability information sensitive. However, the companies, despite being open to sharing, are less inclined to proactively sharing vulnerability information. Furthermore, the providers do not perceive that there is a large interest in vulnerability information from their customers. Hence, the companies' overall attitude to sharing vulnerability information is passive but open. In contrast, contemporary cybersecurity guidelines recommend active disclosure and sharing among actors in an ecosystem.

</details>

<details>

<summary>2019-06-11 08:55:52 - Robust Decision Trees Against Adversarial Examples</summary>

- *Hongge Chen, Huan Zhang, Duane Boning, Cho-Jui Hsieh*

- `1902.10660v2` - [abs](http://arxiv.org/abs/1902.10660v2) - [pdf](http://arxiv.org/pdf/1902.10660v2)

> Although adversarial examples and model robustness have been extensively studied in the context of linear models and neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examples is still limited. In this paper, we show that tree based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees. At its core, our method aims to optimize the performance under the worst-case perturbation of input features, which leads to a max-min saddle point problem. Incorporating this saddle point objective into the decision tree building procedure is non-trivial due to the discrete nature of trees --- a naive approach to finding the best split according to this saddle point objective will take exponential time. To make our approach practical and scalable, we propose efficient tree building algorithms by approximating the inner minimizer in this saddle point problem, and present efficient implementations for classical information gain based trees as well as state-of-the-art tree boosting models such as XGBoost. Experimental results on real world datasets demonstrate that the proposed algorithms can substantially improve the robustness of tree-based models against adversarial examples.

</details>

<details>

<summary>2019-06-11 11:53:45 - EASYFLOW: Keep Ethereum Away From Overflow</summary>

- *Jianbo Gao, Han Liu, Chao Liu, Qingshan Li, Zhi Guan, Zhong Chen*

- `1811.03814v2` - [abs](http://arxiv.org/abs/1811.03814v2) - [pdf](http://arxiv.org/pdf/1811.03814v2)

> While Ethereum smart contracts enabled a wide range of blockchain applications, they are extremely vulnerable to different forms of security attacks. Due to the fact that transactions to smart contracts commonly involve cryptocurrency transfer, any successful attacks can lead to money loss or even financial disorder. In this paper, we focus on the overflow attacks in Ethereum , mainly because they widely rooted in many smart contracts and comparatively easy to exploit. We have developed EASYFLOW , an overflow detector at Ethereum Virtual Machine level. The key insight behind EASYFLOW is a taint analysis based tracking technique to analyze the propagation of involved taints. Specifically, EASYFLOW can not only divide smart contracts into safe contracts, manifested overflows, well-protected overflows and potential overflows, but also automatically generate transactions to trigger potential overflows. In our preliminary evaluation, EASYFLOW managed to find potentially vulnerable Ethereum contracts with little runtime overhead.

</details>

<details>

<summary>2019-06-12 21:50:45 - Metrics Towards Measuring Cyber Agility</summary>

- *Jose David Mireles, Eric Ficke, Jin-Hee Cho, Patrick Hurley, Shouhuai Xu*

- `1906.05395v1` - [abs](http://arxiv.org/abs/1906.05395v1) - [pdf](http://arxiv.org/pdf/1906.05395v1)

> In cyberspace, evolutionary strategies are commonly used by both attackers and defenders. For example, an attacker's strategy often changes over the course of time, as new vulnerabilities are discovered and/or mitigated. Similarly, a defender's strategy changes over time. These changes may or may not be in direct response to a change in the opponent's strategy. In any case, it is important to have a set of quantitative metrics to characterize and understand the effectiveness of attackers' and defenders' evolutionary strategies, which reflect their {\em cyber agility}. Despite its clear importance, few systematic metrics have been developed to quantify the cyber agility of attackers and defenders. In this paper, we propose the first metric framework for measuring cyber agility in terms of the effectiveness of the dynamic evolution of cyber attacks and defenses. The proposed framework is generic and applicable to transform any relevant, quantitative, and/or conventional static security metrics (e.g., false positives and false negatives) into dynamic metrics to capture dynamics of system behaviors. In order to validate the usefulness of the proposed framework, we conduct case studies on measuring the evolution of cyber attacks and defenses using two real-world datasets. We discuss the limitations of the current work and identify future research directions.

</details>

<details>

<summary>2019-06-13 08:11:55 - A Security Case Study for Blockchain Games</summary>

- *Tian Min, Wei Cai*

- `1906.05538v1` - [abs](http://arxiv.org/abs/1906.05538v1) - [pdf](http://arxiv.org/pdf/1906.05538v1)

> Blockchain gaming is an emerging entertainment paradigm. However, blockchain games are still suffering from security issues, due to the immature blockchain technologies and its unsophisticated developers. In this work, we analyzed the blockchain game architecture and reveal the possible penetration methods of cracking. We scanned more than 600 commercial blockchain games to summarize a security overview from the perspective of the web server and smart contract, respectively. We also conducted three case studies for blockchain games to show detailed vulnerability detection.

</details>

<details>

<summary>2019-06-13 10:01:26 - Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks</summary>

- *Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea, Cristina Nita-Rotaru, Fabio Roli*

- `1809.02861v4` - [abs](http://arxiv.org/abs/1809.02861v4) - [pdf](http://arxiv.org/pdf/1809.02861v4)

> Transferability captures the ability of an attack against a machine-learning model to be effective against a different, potentially unknown, model. Empirical evidence for transferability has been shown in previous work, but the underlying reasons why an attack transfers or not are not yet well understood. In this paper, we present a comprehensive analysis aimed to investigate the transferability of both test-time evasion and training-time poisoning attacks. We provide a unifying optimization framework for evasion and poisoning attacks, and a formal definition of transferability of such attacks. We highlight two main factors contributing to attack transferability: the intrinsic adversarial vulnerability of the target model, and the complexity of the surrogate model used to optimize the attack. Based on these insights, we define three metrics that impact an attack's transferability. Interestingly, our results derived from theoretical analysis hold for both evasion and poisoning attacks, and are confirmed experimentally using a wide range of linear and non-linear classifiers and datasets.

</details>

<details>

<summary>2019-06-13 10:56:47 - A Computationally Efficient Method for Defending Adversarial Deep Learning Attacks</summary>

- *Rajeev Sahay, Rehana Mahfuz, Aly El Gamal*

- `1906.05599v1` - [abs](http://arxiv.org/abs/1906.05599v1) - [pdf](http://arxiv.org/pdf/1906.05599v1)

> The reliance on deep learning algorithms has grown significantly in recent years. Yet, these models are highly vulnerable to adversarial attacks, which introduce visually imperceptible perturbations into testing data to induce misclassifications. The literature has proposed several methods to combat such adversarial attacks, but each method either fails at high perturbation values, requires excessive computing power, or both. This letter proposes a computationally efficient method for defending the Fast Gradient Sign (FGS) adversarial attack by simultaneously denoising and compressing data. Specifically, our proposed defense relies on training a fully connected multi-layer Denoising Autoencoder (DAE) and using its encoder as a defense against the adversarial attack. Our results show that using this dimensionality reduction scheme is not only highly effective in mitigating the effect of the FGS attack in multiple threat models, but it also provides a 2.43x speedup in comparison to defense strategies providing similar robustness against the same attack.

</details>

<details>

<summary>2019-06-14 04:01:44 - U2Fi: A Provisioning Scheme of IoT Devices with Universal Cryptographic Tokens</summary>

- *Wang Kang*

- `1906.06009v1` - [abs](http://arxiv.org/abs/1906.06009v1) - [pdf](http://arxiv.org/pdf/1906.06009v1)

> Provisioning is the starting point of the whole life-cycle of IoT devices. The traditional provisioning methods of IoT devices are facing several issues, either about user experience or privacy harvesting. Moreover, IoT devices are vulnerable to different levels of attacks due to limited resources and long online duration. In this paper, we proposed U2Fi, a novel provisioning scheme for IoT devices. We provide a solution to make the U2F device that has been trusted by the cloud in the distribution process, via WiFi or its side channel, to provision the new IoT device. Further, subsequent device settings modification, setting update, and owner transfer can also be performed by using a U2F device that has been trusted to improve security and provide a better user experience. This could provide helpful user friendliness to some valuable new application scenarios in IoT, such as smart hotel. Users could migrate the whole authentication of smart devices into a new site by simply inserting the universal cryptographic token into the secure gateway and authorizing by pressing the user-presence button on the token. Besides, the relevant unbinding process could also be done with a single cryptographic operation signed by the cryptographic token.

</details>

<details>

<summary>2019-06-14 23:46:00 - Detecting Adversarial Examples via Neural Fingerprinting</summary>

- *Sumanth Dathathri, Stephan Zheng, Tianwei Yin, Richard M. Murray, Yisong Yue*

- `1803.03870v3` - [abs](http://arxiv.org/abs/1803.03870v3) - [pdf](http://arxiv.org/pdf/1803.03870v3)

> Deep neural networks are vulnerable to adversarial examples, which dramatically alter model output using small input changes. We propose Neural Fingerprinting, a simple, yet effective method to detect adversarial examples by verifying whether model behavior is consistent with a set of secret fingerprints, inspired by the use of biometric and cryptographic signatures. The benefits of our method are that 1) it is fast, 2) it is prohibitively expensive for an attacker to reverse-engineer which fingerprints were used, and 3) it does not assume knowledge of the adversary. In this work, we pose a formal framework to analyze fingerprints under various threat models, and characterize Neural Fingerprinting for linear models. For complex neural networks, we empirically demonstrate that Neural Fingerprinting significantly improves on state-of-the-art detection mechanisms by detecting the strongest known adversarial attacks with 98-100% AUC-ROC scores on the MNIST, CIFAR-10 and MiniImagenet (20 classes) datasets. In particular, the detection accuracy of Neural Fingerprinting generalizes well to unseen test-data under various black- and whitebox threat models, and is robust over a wide range of hyperparameters and choices of fingerprints.

</details>

<details>

<summary>2019-06-15 01:26:56 - Robust or Private? Adversarial Training Makes Models More Vulnerable to Privacy Attacks</summary>

- *Felipe A. Mejia, Paul Gamble, Zigfried Hampel-Arias, Michael Lomnitz, Nina Lopatina, Lucas Tindall, Maria Alejandra Barrios*

- `1906.06449v1` - [abs](http://arxiv.org/abs/1906.06449v1) - [pdf](http://arxiv.org/pdf/1906.06449v1)

> Adversarial training was introduced as a way to improve the robustness of deep learning models to adversarial attacks. This training method improves robustness against adversarial attacks, but increases the models vulnerability to privacy attacks. In this work we demonstrate how model inversion attacks, extracting training data directly from the model, previously thought to be intractable become feasible when attacking a robustly trained model. The input space for a traditionally trained model is dominated by adversarial examples - data points that strongly activate a certain class but lack semantic meaning - this makes it difficult to successfully conduct model inversion attacks. We demonstrate this effect using the CIFAR-10 dataset under three different model inversion attacks, a vanilla gradient descent method, gradient based method at different scales, and a generative adversarial network base attacks.

</details>

<details>

<summary>2019-06-15 05:48:29 - Physical Integrity Attack Detection of Surveillance Camera with Deep Learning Based Video Frame Interpolation</summary>

- *Jonathan Pan*

- `1906.06475v1` - [abs](http://arxiv.org/abs/1906.06475v1) - [pdf](http://arxiv.org/pdf/1906.06475v1)

> Surveillance cameras, which is a form of Cyber Physical System, are deployed extensively to provide visual surveillance monitoring of activities of interest or anomalies. However, these cameras are at risks of physical security attacks against their physical attributes or configuration like tampering of their recording coverage, camera positions or recording configurations like focus and zoom factors. Such adversarial alteration of physical configuration could also be invoked through cyber security attacks against the camera's software vulnerabilities to administratively change the camera's physical configuration settings. When such Cyber Physical attacks occur, they affect the integrity of the targeted cameras that would in turn render these cameras ineffective in fulfilling the intended security functions. There is a significant measure of research work in detection mechanisms of cyber-attacks against these Cyber Physical devices, however it is understudied area with such mechanisms against integrity attacks on physical configuration. This research proposes the use of the novel use of deep learning algorithms to detect such physical attacks originating from cyber or physical spaces. Additionally, we proposed the novel use of deep learning-based video frame interpolation for such detection that has comparatively better performance to other anomaly detectors in spatiotemporal environments.

</details>

<details>

<summary>2019-06-16 20:55:06 - First-order Adversarial Vulnerability of Neural Networks and Input Dimension</summary>

- *Carl-Johann Simon-Gabriel, Yann Ollivier, Léon Bottou, Bernhard Schölkopf, David Lopez-Paz*

- `1802.01421v4` - [abs](http://arxiv.org/abs/1802.01421v4) - [pdf](http://arxiv.org/pdf/1802.01421v4)

> Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the $\ell_1$-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.

</details>

<details>

<summary>2019-06-17 02:51:58 - A Secure Contained Testbed for Analyzing IoT Botnets</summary>

- *Ayush Kumar, Teng Joon Lim*

- `1906.07175v1` - [abs](http://arxiv.org/abs/1906.07175v1) - [pdf](http://arxiv.org/pdf/1906.07175v1)

> Many security issues have come to the fore with the increasingly widespread adoption of Internet-of-Things (IoT) devices. The Mirai attack on Dyn DNS service, in which vulnerable IoT devices such as IP cameras, DVRs and routers were infected and used to propagate large-scale DDoS attacks, is one of the more prominent recent examples. IoT botnets, consisting of hundreds-of-thousands of bots, are currently present ``in-the-wild'' at least and are only expected to grow in the future, with the potential to cause significant network downtimes and financial losses to network companies. We propose, therefore, to build testbeds for evaluating IoT botnets and design suitable mitigation techniques against them. A DETERlab-based IoT botnet testbed is presented in this work. The testbed is built in a secure contained environment and includes ancillary services such as DHCP, DNS as well as botnet infrastructure including CnC and scanListen/loading servers. Developing an IoT botnet testbed presented us with some unique challenges which are different from those encountered in non-IoT botnet testbeds and we highlight them in this paper. Further, we point out the important features of our testbed and illustrate some of its capabilities through experimental results.

</details>

<details>

<summary>2019-06-17 15:06:47 - The Attack Generator: A Systematic Approach Towards Constructing Adversarial Attacks</summary>

- *Felix Assion, Peter Schlicht, Florens Greßner, Wiebke Günther, Fabian Hüger, Nico Schmidt, Umair Rasheed*

- `1906.07077v1` - [abs](http://arxiv.org/abs/1906.07077v1) - [pdf](http://arxiv.org/pdf/1906.07077v1)

> Most state-of-the-art machine learning (ML) classification systems are vulnerable to adversarial perturbations. As a consequence, adversarial robustness poses a significant challenge for the deployment of ML-based systems in safety- and security-critical environments like autonomous driving, disease detection or unmanned aerial vehicles. In the past years we have seen an impressive amount of publications presenting more and more new adversarial attacks. However, the attack research seems to be rather unstructured and new attacks often appear to be random selections from the unlimited set of possible adversarial attacks. With this publication, we present a structured analysis of the adversarial attack creation process. By detecting different building blocks of adversarial attacks, we outline the road to new sets of adversarial attacks. We call this the "attack generator". In the pursuit of this objective, we summarize and extend existing adversarial perturbation taxonomies. The resulting taxonomy is then linked to the application context of computer vision systems for autonomous vehicles, i.e. semantic segmentation and object detection. Finally, in order to prove the usefulness of the attack generator, we investigate existing semantic segmentation attacks with respect to the detected defining components of adversarial attacks.

</details>

<details>

<summary>2019-06-17 19:50:47 - The Little Phone That Could Ch-Ch-Chroot</summary>

- *Jack Whitter-Jones, Mathew Evans*

- `1906.07242v1` - [abs](http://arxiv.org/abs/1906.07242v1) - [pdf](http://arxiv.org/pdf/1906.07242v1)

> Security testing has been a career path that many are beginning to take. In doing so, security testing can hit the realms of many different types of engagements, ranging from web, infrastructure and social engineering. With the risk of sabotage, corporate espionage it has been seen that many organisations are beginning to develop a tactical capability. In doing so, the term 'Red Team' has been coined to market such engagements. Red Teaming is the method of having almost free reign towards a target. By doing such an engagement a target will be able to fully understand the breadth of vulnerabilities facing their organisation. However, Red Teaming can be an expensive and resource intensive task. Through this paper, it is discussed that it is possible to make a covert disposable phone to help aide Red Teamer's with the reconnaissance phase without drawing attention to themselves within a day to day task.

</details>

<details>

<summary>2019-06-18 01:10:15 - SAVIOR: Towards Bug-Driven Hybrid Testing</summary>

- *Yaohui Chen, Peng Li, Jun Xu, Shengjian Guo, Rundong Zhou, Yulong Zhang, Taowei, Long Lu*

- `1906.07327v1` - [abs](http://arxiv.org/abs/1906.07327v1) - [pdf](http://arxiv.org/pdf/1906.07327v1)

> Hybrid testing combines fuzz testing and concolic execution. It leverages fuzz testing to test easy-to-reach code regions and uses concolic execution to explore code blocks guarded by complex branch conditions. However, its code coverage-centric design is inefficient in vulnerability detection. First, it blindly selects seeds for concolic execution and aims to explore new code continuously. However, as statistics show, a large portion of the explored code is often bug-free. Therefore, giving equal attention to every part of the code during hybrid testing is a non-optimal strategy. It slows down the detection of real vulnerabilities by over 43%. Second, classic hybrid testing quickly moves on after reaching a chunk of code, rather than examining the hidden defects inside. It may frequently miss subtle vulnerabilities despite that it has already explored the vulnerable code paths. We propose SAVIOR, a new hybrid testing framework pioneering a bug-driven principle. Unlike the existing hybrid testing tools, SAVIOR prioritizes the concolic execution of the seeds that are likely to uncover more vulnerabilities. Moreover, SAVIOR verifies all vulnerable program locations along the executing program path. By modeling faulty situations using SMT constraints, SAVIOR reasons the feasibility of vulnerabilities and generates concrete test cases as proofs. Our evaluation shows that the bug-driven approach outperforms mainstream automated testing techniques, including state-of-the-art hybrid testing systems driven by code coverage. On average, SAVIOR detects vulnerabilities 43.4% faster than DRILLER and 44.3% faster than QSYM, leading to the discovery of 88 and 76 more uniquebugs,respectively.Accordingtotheevaluationon11 well fuzzed benchmark programs, within the first 24 hours, SAVIOR triggers 481 UBSAN violations, among which 243 are real bugs.

</details>

<details>

<summary>2019-06-19 01:27:28 - Towards robust audio spoofing detection: a detailed comparison of traditional and learned features</summary>

- *Balamurali BT, Kin Wah Edward Lin, Simon Lui, Jer-Ming Chen, Dorien Herremans*

- `1905.12439v2` - [abs](http://arxiv.org/abs/1905.12439v2) - [pdf](http://arxiv.org/pdf/1905.12439v2)

> Automatic speaker verification, like every other biometric system, is vulnerable to spoofing attacks. Using only a few minutes of recorded voice of a genuine client of a speaker verification system, attackers can develop a variety of spoofing attacks that might trick such systems. Detecting these attacks using the audio cues present in the recordings is an important challenge. Most existing spoofing detection systems depend on knowing the used spoofing technique. With this research, we aim at overcoming this limitation, by examining robust audio features, both traditional and those learned through an autoencoder, that are generalizable over different types of replay spoofing. Furthermore, we provide a detailed account of all the steps necessary in setting up state-of-the-art audio feature detection, pre-, and postprocessing, such that the (non-audio expert) machine learning researcher can implement such systems. Finally, we evaluate the performance of our robust replay speaker detection system with a wide variety and different combinations of both extracted and machine learned audio features on the `out in the wild' ASVspoof 2017 dataset. This dataset contains a variety of new spoofing configurations. Since our focus is on examining which features will ensure robustness, we base our system on a traditional Gaussian Mixture Model-Universal Background Model. We then systematically investigate the relative contribution of each feature set. The fused models, based on both the known audio features and the machine learned features respectively, have a comparable performance with an Equal Error Rate (EER) of 12. The final best performing model, which obtains an EER of 10.8, is a hybrid model that contains both known and machine learned features, thus revealing the importance of incorporating both types of features when developing a robust spoofing prediction model.

</details>

<details>

<summary>2019-06-19 05:16:57 - Global Adversarial Attacks for Assessing Deep Learning Robustness</summary>

- *Hanbin Hu, Mit Shah, Jianhua Z. Huang, Peng Li*

- `1906.07920v1` - [abs](http://arxiv.org/abs/1906.07920v1) - [pdf](http://arxiv.org/pdf/1906.07920v1)

> It has been shown that deep neural networks (DNNs) may be vulnerable to adversarial attacks, raising the concern on their robustness particularly for safety-critical applications. Recognizing the local nature and limitations of existing adversarial attacks, we present a new type of global adversarial attacks for assessing global DNN robustness. More specifically, we propose a novel concept of global adversarial example pairs in which each pair of two examples are close to each other but have different class labels predicted by the DNN. We further propose two families of global attack methods and show that our methods are able to generate diverse and intriguing adversarial example pairs at locations far from the training or testing data. Moreover, we demonstrate that DNNs hardened using the strong projected gradient descent (PGD) based (local) adversarial training are vulnerable to the proposed global adversarial example pairs, suggesting that global robustness must be considered while training robust deep learning networks.

</details>

<details>

<summary>2019-06-19 05:31:17 - VizADS-B: Analyzing Sequences of ADS-B Images Using Explainable Convolutional LSTM Encoder-Decoder to Detect Cyber Attacks</summary>

- *Sefi Akerman, Edan Habler, Asaf Shabtai*

- `1906.07921v1` - [abs](http://arxiv.org/abs/1906.07921v1) - [pdf](http://arxiv.org/pdf/1906.07921v1)

> The purpose of the automatic dependent surveillance broadcast (ADS-B) technology is to serve as a replacement for the current radar-based, air traffic control systems. Despite the considerable time and resources devoted to designing and developing the system, the ADS-B is well known for its lack of security mechanisms. Attempts to address these security vulnerabilities have been made in previous studies by modifying the protocol's current architecture or by using additional hardware components. These solutions, however, are considered impractical because of 1) the complex regulatory process involving avionic systems, 2) the high costs of using hardware components, and 3) the fact that the ADS-B system itself is already deployed in most aircraft and ground stations around the world. In this paper, we propose VizADS-B, an alternative software-based security solution for detecting anomalous ADS-B messages, which does not require any alteration of the current ADS-B architecture or the addition of sensors. According to the proposed method, the information obtained from all aircraft within a specific geographical area is aggregated and represented as a stream of images. Then, a convolutional LSTM encoder-decoder model is used for analyzing and detecting anomalies in the sequences of images. In addition, we propose an explainability technique, designed specifically for convolutional LSTM encoder-decoder models, which is used for providing operative information to the pilot as a visual indicator of a detected anomaly, thus allowing the pilot to make relevant decisions. We evaluated our proposed method on five datasets by injecting and subsequently identifying five different attacks. Our experiments demonstrate that most of the attacks can be detected based on spatio-temporal anomaly detection approach.

</details>

<details>

<summary>2019-06-20 03:08:34 - Cryptanalysis of Khatoon et al.'s ECC-based Authentication Protocol for Healthcare Systems</summary>

- *Mahdi Nikooghadam, Haleh Amintoosi*

- `1906.08424v1` - [abs](http://arxiv.org/abs/1906.08424v1) - [pdf](http://arxiv.org/pdf/1906.08424v1)

> Telecare medical information systems are gaining rapid popularity in terms of providing the delivery of online health-related services such as online remote health profile access for patients and doctors. Due to being installed entirely on Internet, these systems are exposed to various security and privacy threats. Hence, establishing a secure key agreement and authentication process between the patients and the medical servers is an important challenge. Recently, Khatoon et.al proposed an ECC-based unlink-able authentication and key agreement method for healthcare related application in smart city. In this article, we provide a descriptive analysis on their proposed scheme and prove that Khatoon et al.'s scheme is vulnerable to known-session-specific temporary information attack and is not able to provide perfect forward secrecy.

</details>

<details>

<summary>2019-06-20 07:45:02 - Privacy-Enhancing Fall Detection from Remote Sensor Data Using Multi-Party Computation</summary>

- *Pradip Mainali, Carlton Shepherd*

- `1904.09896v2` - [abs](http://arxiv.org/abs/1904.09896v2) - [pdf](http://arxiv.org/pdf/1904.09896v2)

> Motion-based fall detection systems are concerned with detecting falls from vulnerable users, which is typically performed by classifying measurements from a body-worn inertial measurement unit (IMU) using machine learning. Such systems, however, necessitate the collection of high-resolution measurements that may violate users' privacy, such as revealing their gait, activities of daily living (ADLs), and relative position using dead reckoning. In this paper, we investigate the application of multi-party computation (MPC) to IMU-based fall detection for protecting device measurement confidentiality. Our system is evaluated in a cloud-based setting that precludes parties from learning the underlying data using multiple, disparate cloud instances deployed in three geographical configurations. Using a publicly-available dataset, we demonstrate that MPC-based fall detection from IMU measurements is practical while achieving state-of-the-art error rates. In the best case, our system executes in 365.2 milliseconds, which falls well within the required time window for on-device data acquisition (750ms).

</details>

<details>

<summary>2019-06-20 08:58:22 - Multiple-Identity Image Attacks Against Face-based Identity Verification</summary>

- *Jerone T. A. Andrews, Thomas Tanay, Lewis D. Griffin*

- `1906.08507v1` - [abs](http://arxiv.org/abs/1906.08507v1) - [pdf](http://arxiv.org/pdf/1906.08507v1)

> Facial verification systems are vulnerable to poisoning attacks that make use of multiple-identity images (MIIs)---face images stored in a database that resemble multiple persons, such that novel images of any of the constituent persons are verified as matching the identity of the MII. Research on this mode of attack has focused on defence by detection, with no explanation as to why the vulnerability exists. New quantitative results are presented that support an explanation in terms of the geometry of the representations spaces used by the verification systems. In the spherical geometry of those spaces, the angular distance distributions of matching and non-matching pairs of face representations are only modestly separated, approximately centred at 90 and 40-60 degrees, respectively. This is sufficient for open-set verification on normal data but provides an opportunity for MII attacks. Our analysis considers ideal MII algorithms, demonstrating that, if realisable, they would deliver faces roughly 45 degrees from their constituent faces, thus classed as matching them. We study the performance of three methods for MII generation---gallery search, image space morphing, and representation space inversion---and show that the latter two realise the ideal well enough to produce effective attacks, while the former could succeed but only with an implausibly large gallery to search. Gallery search and inversion MIIs depend on having access to a facial comparator, for optimisation, but our results show that these attacks can still be effective when attacking disparate comparators, thus securing a deployed comparator is an insufficient defence.

</details>

<details>

<summary>2019-06-20 17:44:20 - Adversarial attacks on Copyright Detection Systems</summary>

- *Parsa Saadatpanah, Ali Shafahi, Tom Goldstein*

- `1906.07153v2` - [abs](http://arxiv.org/abs/1906.07153v2) - [pdf](http://arxiv.org/pdf/1906.07153v2)

> It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. We discuss a range of copyright detection systems, and why they are particularly vulnerable to attacks. These vulnerabilities are especially apparent for neural network based systems. As a proof of concept, we describe a well-known music identification method, and implement this system in the form of a neural net. We then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube's Content ID system. Our goal is to raise awareness of the threats posed by adversarial examples in this space, and to highlight the importance of hardening copyright detection systems to attacks.

</details>

<details>

<summary>2019-06-21 09:24:32 - Adversarial Example Decomposition</summary>

- *Horace He, Aaron Lou, Qingxuan Jiang, Isay Katsman, Serge Belongie, Ser-Nam Lim*

- `1812.01198v2` - [abs](http://arxiv.org/abs/1812.01198v2) - [pdf](http://arxiv.org/pdf/1812.01198v2)

> Research has shown that widely used deep neural networks are vulnerable to carefully crafted adversarial perturbations. Moreover, these adversarial perturbations often transfer across models. We hypothesize that adversarial weakness is composed of three sources of bias: architecture, dataset, and random initialization. We show that one can decompose adversarial examples into an architecture-dependent component, data-dependent component, and noise-dependent component and that these components behave intuitively. For example, noise-dependent components transfer poorly to all other models, while architecture-dependent components transfer better to retrained models with the same architecture. In addition, we demonstrate that these components can be recombined to improve transferability without sacrificing efficacy on the original model.

</details>

<details>

<summary>2019-06-21 13:16:00 - A Novel Fuzzy Search Approach over Encrypted Data with Improved Accuracy and Efficiency</summary>

- *Jinkun Cao, Jinhao Zhu, Liwei Lin, Zhengui Xue, Ruhui Ma, Haibing Guan*

- `1904.12111v2` - [abs](http://arxiv.org/abs/1904.12111v2) - [pdf](http://arxiv.org/pdf/1904.12111v2)

> As cloud computing becomes prevalent in recent years, more and more enterprises and individuals outsource their data to cloud servers. To avoid privacy leaks, outsourced data usually is encrypted before being sent to cloud servers, which disables traditional search schemes for plain text. To meet both end of security and searchability, search-supported encryption is proposed. However, many previous schemes suffer severe vulnerability when typos and semantic diversity exist in query requests. To overcome such flaw, higher error-tolerance is always expected for search-supported encryption design, sometimes defined as 'fuzzy search'. In this paper, we propose a new scheme of multi-keyword fuzzy search over encrypted and outsourced data. Our approach introduces a new mechanism to map a natural language expression into a word-vector space. Compared with previous approaches, our design shows higher robustness when multiple kinds of typos are involved. Besides, our approach is enhanced with novel data structures to improve search efficiency. These two innovations can work well for both accuracy and efficiency. Moreover, these designs will not hurt the fundamental security. Experiments on a real-world dataset demonstrate the effectiveness of our proposed approach, which outperforms currently popular approaches focusing on similar tasks.

</details>

<details>

<summary>2019-06-21 13:38:35 - Categorizing Wireheading in Partially Embedded Agents</summary>

- *Arushi Majha, Sayan Sarkar, Davide Zagami*

- `1906.09136v1` - [abs](http://arxiv.org/abs/1906.09136v1) - [pdf](http://arxiv.org/pdf/1906.09136v1)

> $\textit{Embedded agents}$ are not explicitly separated from their environment, lacking clear I/O channels. Such agents can reason about and modify their internal parts, which they are incentivized to shortcut or $\textit{wirehead}$ in order to achieve the maximal reward. In this paper, we provide a taxonomy of ways by which wireheading can occur, followed by a definition of wirehead-vulnerable agents. Starting from the fully dualistic universal agent AIXI, we introduce a spectrum of partially embedded agents and identify wireheading opportunities that such agents can exploit, experimentally demonstrating the results with the GRL simulation platform AIXIjs. We contextualize wireheading in the broader class of all misalignment problems - where the goals of the agent conflict with the goals of the human designer - and conjecture that the only other possible type of misalignment is specification gaming. Motivated by this taxonomy, we define wirehead-vulnerable agents as embedded agents that choose to behave differently from fully dualistic agents lacking access to their internal parts.

</details>

<details>

<summary>2019-06-22 07:49:38 - Real-Time Adversarial Attacks</summary>

- *Yuan Gong, Boyang Li, Christian Poellabauer, Yiyu Shi*

- `1905.13399v2` - [abs](http://arxiv.org/abs/1905.13399v2) - [pdf](http://arxiv.org/pdf/1905.13399v2)

> In recent years, many efforts have demonstrated that modern machine learning algorithms are vulnerable to adversarial attacks, where small, but carefully crafted, perturbations on the input can make them fail. While these attack methods are very effective, they only focus on scenarios where the target model takes static input, i.e., an attacker can observe the entire original sample and then add a perturbation at any point of the sample. These attack approaches are not applicable to situations where the target model takes streaming input, i.e., an attacker is only able to observe past data points and add perturbations to the remaining (unobserved) data points of the input. In this paper, we propose a real-time adversarial attack scheme for machine learning models with streaming inputs.

</details>

<details>

<summary>2019-06-23 03:32:11 - Experimental Security Analysis of Controller Software in SDNs: A Review</summary>

- *Tiago V. Ortiz, Bruno Kimura, Jó Ueyama, Valério Rosset*

- `1906.09546v1` - [abs](http://arxiv.org/abs/1906.09546v1) - [pdf](http://arxiv.org/pdf/1906.09546v1)

> The software defined networking paradigm relies on the programmability of the network to automatically perform management and reconfiguration tasks. The result of adopting this programmability feature is twofold: first by designing new solutions and, second, by concurrently making room for the exploitation of new security threats. As a malfunction in the controller software may lead to a collapse of the network, assessing the security of solutions before their deployment, is a major concern in SDNs. In light of this, we have conducted a comprehensive review of the literature on the experimental security analysis of the control plane in SDNs, with an emphasis on vulnerabilities of the controller software. Additionally, we have introduced a taxonomy of the techniques found in the literature with regard to the experimental security analysis of SDN controller software. Furthermore, a comparative study has been carried out of existing experimental approaches considering the security requirements defined by the Open Network Foundation (ONF). As a result, we highlighted that there is a need for a standardization of the methodologies employed for automated security analysis, that can meet the appropriate requirements, and support the development of reliable and secure software for SDNs.

</details>

<details>

<summary>2019-06-23 19:59:42 - On Quantum Chosen-Ciphertext Attacks and Learning with Errors</summary>

- *Gorjan Alagic, Stacey Jeffery, Maris Ozols, Alexander Poremba*

- `1808.09655v2` - [abs](http://arxiv.org/abs/1808.09655v2) - [pdf](http://arxiv.org/pdf/1808.09655v2)

> Large-scale quantum computing is a significant threat to classical public-key cryptography. In strong "quantum access" security models, numerous symmetric-key cryptosystems are also vulnerable. We consider classical encryption in a model which grants the adversary quantum oracle access to encryption and decryption, but where the latter is restricted to non-adaptive (i.e., pre-challenge) queries only. We define this model formally using appropriate notions of ciphertext indistinguishability and semantic security (which are equivalent by standard arguments) and call it QCCA1 in analogy to the classical CCA1 security model. Using a bound on quantum random-access codes, we show that the standard PRF- and PRP-based encryption schemes are QCCA1-secure when instantiated with quantum-secure primitives.   We then revisit standard IND-CPA-secure Learning with Errors (LWE) encryption and show that leaking just one quantum decryption query (and no other queries or leakage of any kind) allows the adversary to recover the full secret key with constant success probability. In the classical setting, by contrast, recovering the key uses a linear number of decryption queries, and this is optimal. The algorithm at the core of our attack is a (large-modulus version of) the well-known Bernstein-Vazirani algorithm. We emphasize that our results should *not* be interpreted as a weakness of these cryptosystems in their stated security setting (i.e., post-quantum chosen-plaintext secrecy). Rather, our results mean that, if these cryptosystems are exposed to chosen-ciphertext attacks (e.g., as a result of deployment in an inappropriate real-world setting) then quantum attacks are even more devastating than classical ones.

</details>

<details>

<summary>2019-06-24 04:18:38 - EDIMA: Early Detection of IoT Malware Network Activity Using Machine Learning Techniques</summary>

- *Ayush Kumar, Teng Joon Lim*

- `1906.09715v1` - [abs](http://arxiv.org/abs/1906.09715v1) - [pdf](http://arxiv.org/pdf/1906.09715v1)

> The widespread adoption of Internet of Things has led to many security issues. Post the Mirai-based DDoS attack in 2016 which compromised IoT devices, a host of new malware using Mirai's leaked source code and targeting IoT devices have cropped up, e.g. Satori, Reaper, Amnesia, Masuta etc. These malware exploit software vulnerabilities to infect IoT devices instead of open TELNET ports (like Mirai) making them more difficult to block using existing solutions such as firewalls. In this research, we present EDIMA, a distributed modular solution which can be used towards the detection of IoT malware network activity in large-scale networks (e.g. ISP, enterprise networks) during the scanning/infecting phase rather than during an attack. EDIMA employs machine learning algorithms for edge devices' traffic classification, a packet traffic feature vector database, a policy module and an optional packet sub-sampling module. We evaluate the classification performance of EDIMA through testbed experiments and present the results obtained.

</details>

<details>

<summary>2019-06-24 08:45:19 - Extending Attack Graphs to Represent Cyber-Attacks in Communication Protocols and Modern IT Networks</summary>

- *Orly Stan, Ron Bitton, Michal Ezrets, Moran Dadon, Masaki Inokuchi, Yoshinobu Ohta, Yoshiyuki Yamada, Tomohiko Yagyu, Yuval Elovici, Asaf Shabtai*

- `1906.09786v1` - [abs](http://arxiv.org/abs/1906.09786v1) - [pdf](http://arxiv.org/pdf/1906.09786v1)

> An attack graph is a method used to enumerate the possible paths that an attacker can execute in the organization network. MulVAL is a known open-source framework used to automatically generate attack graphs. MulVAL's default modeling has two main shortcomings. First, it lacks the representation of network protocol vulnerabilities, and thus it cannot be used to model common network attacks such as ARP poisoning, DNS spoofing, and SYN flooding. Second, it does not support advanced types of communication such as wireless and bus communication, and thus it cannot be used to model cyber-attacks on networks that include IoT devices or industrial components. In this paper, we present an extended network security model for MulVAL that: (1) considers the physical network topology, (2) supports short-range communication protocols (e.g., Bluetooth), (3) models vulnerabilities in the design of network protocols, and (4) models specific industrial communication architectures. Using the proposed extensions, we were able to model multiple attack techniques including: spoofing, man-in-the-middle, and denial of service, as well as attacks on advanced types of communication. We demonstrate the proposed model on a testbed implementing a simplified network architecture comprised of both IT and industrial components.

</details>

<details>

<summary>2019-06-25 00:21:05 - CAPnet: A Defense Against Cache Accounting Attacks on Content Distribution Networks</summary>

- *Ghada Almashaqbeh, Kevin Kelley, Allison Bishop, Justin Cappos*

- `1906.10272v1` - [abs](http://arxiv.org/abs/1906.10272v1) - [pdf](http://arxiv.org/pdf/1906.10272v1)

> Peer-assisted content distribution networks(CDNs) have emerged to improve performance and reduce deployment costs of traditional, infrastructure-based content delivery networks. This is done by employing peer-to-peer data transfers to supplement the resources of the network infrastructure. However, these hybrid systems are vulnerable to accounting attacks in which the peers, or caches, collude with clients in order to report that content was transferred when it was not. This is a particular issue in systems that incentivize cache participation, because malicious caches may collect rewards from the content publishers operating the CDN without doing any useful work.   In this paper, we introduce CAPnet, the first technique that lets untrusted caches join a peer-assisted CDN while providing a bound on the effectiveness of accounting attacks. At its heart is a lightweight cache accountability puzzle that clients must solve before caches are given credit. This puzzle requires colocating the data a client has requested, so its solution confirms that the content (or at least an amount of data within a pre-configured bound) has actually been retrieved. We analyze the security and overhead of our scheme in realistic scenarios. The results show that a modest client machine using a single core can solve puzzles at a rate sufficient to simultaneously watch dozens of 1080p videos. The technique is designed to be even more scalable on the server side. In our experiments, one core of a single low-end machine is able to generate puzzles for 4.26 Tbps of bandwidth - enabling 870,000 clients to concurrently view the same 1080p video. This demonstrates that our scheme can ensure cache accountability without degrading system productivity.

</details>

<details>

<summary>2019-06-25 07:41:13 - EVulHunter: Detecting Fake Transfer Vulnerabilities for EOSIO's Smart Contracts at Webassembly-level</summary>

- *Lijin Quan, Lei Wu, Haoyu Wang*

- `1906.10362v1` - [abs](http://arxiv.org/abs/1906.10362v1) - [pdf](http://arxiv.org/pdf/1906.10362v1)

> As one of the representative Delegated Proof-of-Stake (DPoS) blockchain platforms, EOSIO's ecosystem grows rapidly in recent years. A number of vulnerabilities and corresponding attacks of EOSIO's smart contracts have been discovered and observed in the wild, which caused a large amount of financial damages. However, the majority of EOSIO's smart contracts are not open-sourced. As a result, the WebAssembly code may become the only available object to be analyzed in most cases. Unfortunately, current tools are web-application oriented and cannot be applied to EOSIO WebAssembly code directly, which makes it more difficult to detect vulnerabilities from those smart contracts. In this paper, we propose \toolname, a static analysis tool that can be used to detect vulnerabilities from EOSIO WASM code automatically. We focus on one particular type of vulnerabilities named \textit{fake-transfer}, and the exploitation of such vulnerabilities has led to millions of dollars in damages. To the best of our knowledge, it is the first attempt to build an automatic tool to detect vulnerabilities of EOSIO's smart contracts. The experimental results demonstrate that our tool is able to detect fake transfer vulnerabilities quickly and precisely. EVulHunter is available on GitHub\footnote{Tool and benchmarks: https://github.com/EVulHunter/EVulHunter} and YouTube\footnote{Demo video: https://youtu.be/5SJ0ZJKVZvw}.

</details>

<details>

<summary>2019-06-25 18:25:20 - Uncertainty-aware Model-based Policy Optimization</summary>

- *Tung-Long Vuong, Kenneth Tran*

- `1906.10717v1` - [abs](http://arxiv.org/abs/1906.10717v1) - [pdf](http://arxiv.org/pdf/1906.10717v1)

> Model-based reinforcement learning has the potential to be more sample efficient than model-free approaches. However, existing model-based methods are vulnerable to model bias, which leads to poor generalization and asymptotic performance compared to model-free counterparts. In addition, they are typically based on the model predictive control (MPC) framework, which not only is computationally inefficient at decision time but also does not enable policy transfer due to the lack of an explicit policy representation. In this paper, we propose a novel uncertainty-aware model-based policy optimization framework which solves those issues. In this framework, the agent simultaneously learns an uncertainty-aware dynamics model and optimizes the policy according to these learned models. In the optimization step, the policy gradient is computed by automatic differentiation through the models. With respect to sample efficiency alone, our approach shows promising results on challenging continuous control benchmarks with competitive asymptotic performance and significantly lower sample complexity than state-of-the-art baselines.

</details>

<details>

<summary>2019-06-25 19:38:57 - Harnessing the Vulnerability of Latent Layers in Adversarially Trained Models</summary>

- *Mayank Singh, Abhishek Sinha, Nupur Kumari, Harshitha Machiraju, Balaji Krishnamurthy, Vineeth N Balasubramanian*

- `1905.05186v2` - [abs](http://arxiv.org/abs/1905.05186v2) - [pdf](http://arxiv.org/pdf/1905.05186v2)

> Neural networks are vulnerable to adversarial attacks -- small visually imperceptible crafted noise which when added to the input drastically changes the output. The most effective method of defending against these adversarial attacks is to use the methodology of adversarial training. We analyze the adversarially trained robust models to study their vulnerability against adversarial attacks at the level of the latent layers. Our analysis reveals that contrary to the input layer which is robust to adversarial attack, the latent layer of these robust models are highly susceptible to adversarial perturbations of small magnitude. Leveraging this information, we introduce a new technique Latent Adversarial Training (LAT) which comprises of fine-tuning the adversarially trained models to ensure the robustness at the feature layers. We also propose Latent Attack (LA), a novel algorithm for construction of adversarial examples. LAT results in minor improvement in test accuracy and leads to a state-of-the-art adversarial accuracy against the universal first-order adversarial PGD attack which is shown for the MNIST, CIFAR-10, CIFAR-100 datasets.

</details>

<details>

<summary>2019-06-26 09:56:32 - Heuristic Approach Towards Countermeasure Selection using Attack Graphs</summary>

- *Orly Stan, Ron Bitton, Michal Ezrets, Moran Dadon, Masaki Inokuchi, Yoshinobu Ohta, Tomohiko Yagyu, Yuval Elovici, Asaf Shabtai*

- `1906.10943v1` - [abs](http://arxiv.org/abs/1906.10943v1) - [pdf](http://arxiv.org/pdf/1906.10943v1)

> Selecting the optimal set of countermeasures is a challenging task that involves various considerations and tradeoffs such as prioritizing the risks to mitigate and costs. The vast majority of studies for selecting a countermeasure deployment are based on a limited risk assessment procedure that utilizes the common vulnerability scoring system (CVSS). Such a risk assessment procedure does not necessarily consider the prerequisites and exploitability of a specific asset, cannot distinguish insider from outsider threat actor, and does not express the consequences of exploiting a vulnerability as well as the attacker's lateral movements. Other studies applied a more extensive risk assessment procedure that relies on manual work and repeated assessment. These solutions however, do not consider the network topology and do not specify the optimal position for deploying the countermeasures, and therefore are less practical. In this paper we suggest a heuristic search approach for selecting the optimal countermeasure deployment under a given budget limitation. The proposed method expresses the risk of the system using an extended attack graph modeling, which considers the prerequisites and consequences of exploiting a vulnerability, examines the attacker's potential lateral movements, and express the physical network topology as well as vulnerabilities in network protocols. In addition, unlike previous studies which utilizes attack graph for countermeasure planning, the proposed methods does not require re-generating the attack graph at each stage of the procedure, which is computationally heavy, and therefore it provides a more accurate and practical countermeasure deployment planning process.

</details>

<details>

<summary>2019-06-26 13:43:57 - Security Update Labels: Establishing Economic Incentives for Security Patching of IoT Consumer Products</summary>

- *Philipp Morgner, Christoph Mai, Nicole Koschate-Fischer, Felix Freiling, Zinaida Benenson*

- `1906.11094v1` - [abs](http://arxiv.org/abs/1906.11094v1) - [pdf](http://arxiv.org/pdf/1906.11094v1)

> With the expansion of the Internet of Things (IoT), the number of security incidents due to insecure and misconfigured IoT devices is increasing. Especially on the consumer market, manufacturers focus on new features and early releases at the expense of a comprehensive security strategy. Hence, experts have started calling for regulation of the IoT consumer market, while policymakers are seeking for suitable regulatory approaches. We investigate how manufacturers can be incentivized to increase sustainable security efforts for IoT products. We propose mandatory security update labels that inform consumers during buying decisions about the willingness of the manufacturer to provide security updates in the future. Mandatory means that the labels explicitly state when security updates are not guaranteed. We conducted a user study with more than 1,400 participants to assess the importance of security update labels for the consumer choice by means of a conjoint analysis. The results show that the availability of security updates (until which date the updates are guaranteed) accounts for 8% to 35% impact on overall consumers' choice, depending on the perceived security risk of the product category. For products with a high perceived security risk, this availability is twice as important as other high-ranked product attributes. Moreover, provisioning time for security updates (how quickly the product will be patched after a vulnerability is discovered) additionally accounts for 7% to 25% impact on consumers' choices. The proposed labels are intuitively understood by consumers, do not require product assessments by third parties before release, and have a potential to incentivize manufacturers to provide sustainable security support.

</details>

<details>

<summary>2019-06-26 15:14:24 - Discovering and Understanding the Security Hazards in the Interactions between IoT Devices, Mobile Apps, and Clouds on Smart Home Platforms</summary>

- *Wei Zhou, Yan Jia, Yao Yao, Lipeng Zhu, Le Guan, Yuhang Mao, Peng Liu, Yuqing Zhang*

- `1811.03241v2` - [abs](http://arxiv.org/abs/1811.03241v2) - [pdf](http://arxiv.org/pdf/1811.03241v2)

> A smart home connects tens of home devices to the Internet, where an IoT cloud runs various home automation applications. While bringing unprecedented convenience and accessibility, it also introduces various security hazards to users. Prior research studied smart home security from several aspects. However, we found that the complexity of the interactions among the participating entities (i.e., devices, IoT clouds, and mobile apps) has not yet been systematically investigated. In this work, we conducted an in-depth analysis of five widely-used smart home platforms. Combining firmware analysis, network traffic interception, and blackbox testing, we reverse-engineered the details of the interactions among the participating entities. Based on the details, we inferred three legitimate state transition diagrams for the three entities, respectively. Using these state machines as a reference model, we identified a set of unexpected state transitions. To confirm and trigger the unexpected state transitions, we implemented a set of phantom devices to mimic a real device. By instructing the phantom devices to intervene in the normal entity-entity interactions, we have discovered several new vulnerabilities and a spectrum of attacks against real-world smart home platforms.

</details>

<details>

<summary>2019-06-27 08:53:59 - An Empirical Study of Information Flows in Real-World JavaScript</summary>

- *Cristian-Alexandru Staicu, Daniel Schoepe, Musard Balliu, Michael Pradel, Andrei Sabelfeld*

- `1906.11507v1` - [abs](http://arxiv.org/abs/1906.11507v1) - [pdf](http://arxiv.org/pdf/1906.11507v1)

> Information flow analysis prevents secret or untrusted data from flowing into public or trusted sinks. Existing mechanisms cover a wide array of options, ranging from lightweight taint analysis to heavyweight information flow control that also considers implicit flows. Dynamic analysis, which is particularly popular for languages such as JavaScript, faces the question whether to invest in analyzing flows caused by not executing a particular branch, so-called hidden implicit flows. This paper addresses the questions how common different kinds of flows are in real-world programs, how important these flows are to enforce security policies, and how costly it is to consider these flows. We address these questions in an empirical study that analyzes 56 real-world JavaScript programs that suffer from various security problems, such as code injection vulnerabilities, denial of service vulnerabilities, memory leaks, and privacy leaks. The study is based on a state-of-the-art dynamic information flow analysis and a formalization of its core. We find that implicit flows are expensive to track in terms of permissiveness, label creep, and runtime overhead. We find a lightweight taint analysis to be sufficient for most of the studied security problems, while for some privacy-related code, observable tracking is sometimes required. In contrast, we do not find any evidence that tracking hidden implicit flows reveals otherwise missed security problems. Our results help security analysts and analysis designers to understand the cost-benefit tradeoffs of information flow analysis and provide empirical evidence that analyzing implicit flows in a cost-effective way is a relevant problem.

</details>

<details>

<summary>2019-06-27 16:41:18 - A Sweet Recipe for Consolidated Vulnerabilities: Attacking a Live Website by Harnessing a Killer Combination of Vulnerabilities</summary>

- *Mazharul Islam, MD. Nazmuddoha Ansary, Novia Nurain, Salauddin Parvez Shams, A. B. M. Alim Al Islam*

- `1906.11782v1` - [abs](http://arxiv.org/abs/1906.11782v1) - [pdf](http://arxiv.org/pdf/1906.11782v1)

> The recent emergence of new vulnerabilities is an epoch-making problem in the complex world of website security. Most of the websites are failing to keep updating to tackle their websites from these new vulnerabilities leaving without realizing the weakness of the websites. As a result, when cyber-criminals scour such vulnerable old version websites, the scanner will represent a set of vulnerabilities. Once found, these vulnerabilities are then exploited to steal data, distribute malicious content, or inject defacement and spam content into the vulnerable websites. Furthermore, a combination of different vulnerabilities is able to cause more damages than anticipation. Therefore, in this paper, we endeavor to find connections among various vulnerabilities such as cross-site scripting, local file inclusion, remote file inclusion, buffer overflow CSRF, etc. To do so, we develop a Finite State Machine (FSM) attacking model, which analyzes a set of vulnerabilities towards the road to finding connections. We demonstrate the efficacy of our model by applying it to the set of vulnerabilities found on two live websites.

</details>

<details>

<summary>2019-06-28 03:29:38 - A Neural-based Program Decompiler</summary>

- *Cheng Fu, Huili Chen, Haolan Liu, Xinyun Chen, Yuandong Tian, Farinaz Koushanfar, Jishen Zhao*

- `1906.12029v1` - [abs](http://arxiv.org/abs/1906.12029v1) - [pdf](http://arxiv.org/pdf/1906.12029v1)

> Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda, the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior performance compared to baseline approaches. We assess Coda's performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82\% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0\% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70\% program accuracy.

</details>

<details>

<summary>2019-06-28 04:40:10 - Analyzing GDPR Compliance Through the Lens of Privacy Policy</summary>

- *Jayashree Mohan, Melissa Wasserman, Vijay Chidambaram*

- `1906.12038v1` - [abs](http://arxiv.org/abs/1906.12038v1) - [pdf](http://arxiv.org/pdf/1906.12038v1)

> With the arrival of the European Union's General Data Protection Regulation (GDPR), several companies are making significant changes to their systems to achieve compliance. The changes range from modifying privacy policies to redesigning systems which process personal data. This work analyzes the privacy policies of large-scaled cloud services which seek to be GDPR compliant. The privacy policy is the main medium of information dissemination between the data controller and the users. We show that many services that claim compliance today do not have clear and concise privacy policies. We identify several points in the privacy policies which potentially indicate non-compliance; we term these GDPR vulnerabilities. We identify GDPR vulnerabilities in ten cloud services. Based on our analysis, we propose seven best practices for crafting GDPR privacy policies.

</details>

<details>

<summary>2019-06-28 06:22:54 - Black-box Adversarial Attacks on Video Recognition Models</summary>

- *Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey, Yu-Gang Jiang*

- `1904.05181v2` - [abs](http://arxiv.org/abs/1904.05181v2) - [pdf](http://arxiv.org/pdf/1904.05181v2)

> Deep neural networks (DNNs) are known for their vulnerability to adversarial examples. These are examples that have undergone small, carefully crafted perturbations, and which can easily fool a DNN into making misclassifications at test time. Thus far, the field of adversarial research has mainly focused on image models, under either a white-box setting, where an adversary has full access to model parameters, or a black-box setting where an adversary can only query the target model for probabilities or labels. Whilst several white-box attacks have been proposed for video models, black-box video attacks are still unexplored. To close this gap, we propose the first black-box video attack framework, called V-BAD. V-BAD utilizes tentative perturbations transferred from image models, and partition-based rectifications found by the NES on partitions (patches) of tentative perturbations, to obtain good adversarial gradient estimates with fewer queries to the target model. V-BAD is equivalent to estimating the projection of an adversarial gradient on a selected subspace. Using three benchmark video datasets, we demonstrate that V-BAD can craft both untargeted and targeted attacks to fool two state-of-the-art deep video recognition models. For the targeted attack, it achieves $>$93\% success rate using only an average of $3.4 \sim 8.4 \times 10^4$ queries, a similar number of queries to state-of-the-art black-box image attacks. This is despite the fact that videos often have two orders of magnitude higher dimensionality than static images. We believe that V-BAD is a promising new tool to evaluate and improve the robustness of video recognition models to black-box adversarial attacks.

</details>

<details>

<summary>2019-06-29 13:06:13 - Incidents Are Meant for Learning, Not Repeating: Sharing Knowledge About Security Incidents in Cyber-Physical Systems</summary>

- *Faeq Alrimawi, Liliana Pasquale, Deepak Mehta, Nobukazu Yoshioka, Bashar Nuseibeh*

- `1907.00199v1` - [abs](http://arxiv.org/abs/1907.00199v1) - [pdf](http://arxiv.org/pdf/1907.00199v1)

> Cyber-physical systems (CPSs) are part of most critical infrastructures such as industrial automation and transportation systems. Thus, security incidents targeting CPSs can have disruptive consequences to assets and people. As prior incidents tend to re-occur, sharing knowledge about these incidents can help organizations be more prepared to prevent, mitigate or investigate future incidents. This paper proposes a novel approach to enable representation and sharing of knowledge about CPS incidents across different organizations. To support sharing, we represent incident knowledge (incident patterns) capturing incident characteristics that can manifest again, such as incident activities or vulnerabilities exploited by offenders. Incident patterns are a more abstract representation of specific incident instances and, thus, are general enough to be applicable to various systems - different than the one in which the incident occurred. They can also avoid disclosing potentially sensitive information about an organization's assets and resources. We provide an automated technique to extract an incident pattern from a specific incident instance. To understand how an incident pattern can manifest again in other cyber-physical systems, we also provide an automated technique to instantiate incident patterns to specific systems. We demonstrate the feasibility of our approach in the application domain of smart buildings. We evaluate correctness, scalability, and performance using two substantive scenarios inspired by real-world systems and incidents.

</details>

<details>

<summary>2019-06-29 15:25:53 - Causal Inference Under Interference And Network Uncertainty</summary>

- *Rohit Bhattacharya, Daniel Malinsky, Ilya Shpitser*

- `1907.00221v1` - [abs](http://arxiv.org/abs/1907.00221v1) - [pdf](http://arxiv.org/pdf/1907.00221v1)

> Classical causal and statistical inference methods typically assume the observed data consists of independent realizations. However, in many applications this assumption is inappropriate due to a network of dependences between units in the data. Methods for estimating causal effects have been developed in the setting where the structure of dependence between units is known exactly, but in practice there is often substantial uncertainty about the precise network structure. This is true, for example, in trial data drawn from vulnerable communities where social ties are difficult to query directly. In this paper we combine techniques from the structure learning and interference literatures in causal inference, proposing a general method for estimating causal effects under data dependence when the structure of this dependence is not known a priori. We demonstrate the utility of our method on synthetic datasets which exhibit network dependence.

</details>

<details>

<summary>2019-06-29 19:43:54 - MeshAdv: Adversarial Meshes for Visual Recognition</summary>

- *Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, Mingyan Liu*

- `1810.05206v2` - [abs](http://arxiv.org/abs/1810.05206v2) - [pdf](http://arxiv.org/pdf/1810.05206v2)

> Highly expressive models such as deep neural networks (DNNs) have been widely applied to various applications. However, recent studies show that DNNs are vulnerable to adversarial examples, which are carefully crafted inputs aiming to mislead the predictions. Currently, the majority of these studies have focused on perturbation added to image pixels, while such manipulation is not physically realistic. Some works have tried to overcome this limitation by attaching printable 2D patches or painting patterns onto surfaces, but can be potentially defended because 3D shape features are intact. In this paper, we propose meshAdv to generate "adversarial 3D meshes" from objects that have rich shape features but minimal textural variation. To manipulate the shape or texture of the objects, we make use of a differentiable renderer to compute accurate shading on the shape and propagate the gradient. Extensive experiments show that the generated 3D meshes are effective in attacking both classifiers and object detectors. We evaluate the attack under different viewpoints. In addition, we design a pipeline to perform black-box attack on a photorealistic renderer with unknown rendering parameters.

</details>

<details>

<summary>2019-06-30 06:36:46 - Prediction of Small Molecule Kinase Inhibitors for Chemotherapy Using Deep Learning</summary>

- *Niranjan Balachandar, Christine Liu, Winston Wang*

- `1907.00329v1` - [abs](http://arxiv.org/abs/1907.00329v1) - [pdf](http://arxiv.org/pdf/1907.00329v1)

> The current state of cancer therapeutics has been moving away from one-size-fits-all cytotoxic chemotherapy, and towards a more individualized and specific approach involving the targeting of each tumor's genetic vulnerabilities. Different tumors, even of the same type, may be more reliant on certain cellular pathways more than others. With modern advancements in our understanding of cancer genome sequencing, these pathways can be discovered. Investigating each of the millions of possible small molecule inhibitors for each kinase in vitro, however, would be extremely expensive and time consuming. This project focuses on predicting the inhibition activity of small molecules targeting 8 different kinases using multiple deep learning models. We trained fingerprint-based MLPs and simplified molecular-input line-entry specification (SMILES)-based recurrent neural networks (RNNs) and molecular graph convolutional networks (GCNs) to accurately predict inhibitory activity targeting these 8 kinases.

</details>

<details>

<summary>2019-06-30 21:54:26 - ("Oops! Had the silly thing in reverse")---Optical injection attacks in through LED status indicators</summary>

- *Joe Loughry*

- `1907.00479v1` - [abs](http://arxiv.org/abs/1907.00479v1) - [pdf](http://arxiv.org/pdf/1907.00479v1)

> It is possible to attack a computer remotely through the front panel LEDs. Following on previous results that showed information leakage at optical wavelengths, now it seems practicable to inject information into a system as well. It is shown to be definitely feasible under realistic conditions (by infosec standards) of target system compromise; experimental results suggest it further may be possible, through a slightly different mechanism, even under high security conditions that put extremely difficult constraints on the attacker. The problem is of recent origin; it could not have occurred before a confluence of unrelated technological developments made it possible. Arduino-type microcontrollers are involved; this is an Internet of Things (IoT) vulnerability. Unlike some previous findings, the vulnerability here is moderate---at present---because it takes the infosec form of a classical covert channel. However, the architecture of several popular families of microcontrollers suggests that a Rowhammer-like directed energy optical attack that requires no malware might be possible. Phase I experiments yielded surprising and encouraging results; a covert channel is definitely practicable without exotic hardware, bandwidth approaching a Mbit/s, and the majority of discrete LEDs tested were found to be reversible on GPIO pins. Phase II experiments, not yet funded, will try to open the door remotely.

</details>


## 2019-07

<details>

<summary>2019-07-01 00:32:03 - GenAttack: Practical Black-box Attacks with Gradient-Free Optimization</summary>

- *Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Huan Zhang, Cho-Jui Hsieh, Mani Srivastava*

- `1805.11090v3` - [abs](http://arxiv.org/abs/1805.11090v3) - [pdf](http://arxiv.org/pdf/1805.11090v3)

> Deep neural networks are vulnerable to adversarial examples, even in the black-box setting, where the attacker is restricted solely to query access. Existing black-box approaches to generating adversarial examples typically require a significant number of queries, either for training a substitute network or performing gradient estimation. We introduce GenAttack, a gradient-free optimization technique that uses genetic algorithms for synthesizing adversarial examples in the black-box setting. Our experiments on different datasets (MNIST, CIFAR-10, and ImageNet) show that GenAttack can successfully generate visually imperceptible adversarial examples against state-of-the-art image recognition models with orders of magnitude fewer queries than previous approaches. Against MNIST and CIFAR-10 models, GenAttack required roughly 2,126 and 2,568 times fewer queries respectively, than ZOO, the prior state-of-the-art black-box attack. In order to scale up the attack to large-scale high-dimensional ImageNet models, we perform a series of optimizations that further improve the query efficiency of our attack leading to 237 times fewer queries against the Inception-v3 model than ZOO. Furthermore, we show that GenAttack can successfully attack some state-of-the-art ImageNet defenses, including ensemble adversarial training and non-differentiable or randomized input transformations. Our results suggest that evolutionary algorithms open up a promising area of research into effective black-box attacks.

</details>

<details>

<summary>2019-07-01 04:10:31 - Why Botnets Work: Distributed Brute-Force Attacks Need No Synchronization</summary>

- *Salman Salamatian, Wasim Huleihel, Ahmad Beirami, Asaf Cohen, Muriel Médard*

- `1805.11666v2` - [abs](http://arxiv.org/abs/1805.11666v2) - [pdf](http://arxiv.org/pdf/1805.11666v2)

> In September 2017, McAffee Labs quarterly report estimated that brute force attacks represent 20\% of total network attacks, making them the most prevalent type of attack ex-aequo with browser based vulnerabilities. These attacks have sometimes catastrophic consequences, and understanding their fundamental limits may play an important role in the risk assessment of password-secured systems, and in the design of better security protocols. While some solutions exist to prevent online brute-force attacks that arise from one single IP address, attacks performed by botnets are more challenging. In this paper, we analyze these distributed attacks by using a simplified model. Our aim is to understand the impact of distribution and asynchronization on the overall computational effort necessary to breach a system. Our result is based on Guesswork, a measure of the number of queries (guesses) required of an adversary before a correct sequence, such as a password, is found in an optimal attack. Guesswork is a direct surrogate for time and computational effort of guessing a sequence from a set of sequences with associated likelihoods. We model the lack of synchronization by a worst-case optimization in which the queries made by multiple adversarial agents are received in the worst possible order for the adversary, resulting in a min-max formulation. We show that, even without synchronization, and for sequences of growing length, the asymptotic optimal performance is achievable by using randomized guesses drawn from an appropriate distribution. Therefore, randomization is key for distributed asynchronous attacks. In other words, asynchronous guessers can asymptotically perform brute-force attacks as efficiently as synchronized guessers.

</details>

<details>

<summary>2019-07-02 04:42:17 - ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems</summary>

- *Yuan Gong, Jian Yang, Jacob Huber, Mitchell MacKnight, Christian Poellabauer*

- `1904.03365v2` - [abs](http://arxiv.org/abs/1904.03365v2) - [pdf](http://arxiv.org/pdf/1904.03365v2)

> This paper introduces a new database of voice recordings with the goal of supporting research on vulnerabilities and protection of voice-controlled systems (VCSs). In contrast to prior efforts, the proposed database contains both genuine voice commands and replayed recordings of such commands, collected in realistic VCSs usage scenarios and using modern voice assistant development kits. Specifically, the database contains recordings from four systems (each with a different microphone array) in a variety of environmental conditions with different forms of background noise and relative positions between speaker and device. To the best of our knowledge, this is the first publicly available database that has been specifically designed for the protection of state-of-the-art voice-controlled systems against various replay attacks in various conditions and environments.

</details>

<details>

<summary>2019-07-02 11:09:04 - Neural Network Verification for the Masses (of AI graduates)</summary>

- *Ekaterina Komendantskaya, Rob Stewart, Kirsy Duncan, Daniel Kienitz, Pierre Le Hen, Pascal Bacchus*

- `1907.01297v1` - [abs](http://arxiv.org/abs/1907.01297v1) - [pdf](http://arxiv.org/pdf/1907.01297v1)

> Rapid development of AI applications has stimulated demand for, and has given rise to, the rapidly growing number and diversity of AI MSc degrees. AI and Robotics research communities, industries and students are becoming increasingly aware of the problems caused by unsafe or insecure AI applications. Among them, perhaps the most famous example is vulnerability of deep neural networks to ``adversarial attacks''. Owing to wide-spread use of neural networks in all areas of AI, this problem is seen as particularly acute and pervasive.   Despite of the growing number of research papers about safety and security vulnerabilities of AI applications, there is a noticeable shortage of accessible tools, methods and teaching materials for incorporating verification into AI programs. LAIV -- the Lab for AI and Verification -- is a newly opened research lab at Heriot-Watt university that engages AI and Robotics MSc students in verification projects, as part of their MSc dissertation work. In this paper, we will report on successes and unexpected difficulties LAIV faces, many of which arise from limitations of existing programming languages used for verification. We will discuss future directions for incorporating verification into AI degrees.

</details>

<details>

<summary>2019-07-03 09:49:56 - Adversarial Black-Box Attacks on Automatic Speech Recognition Systems using Multi-Objective Evolutionary Optimization</summary>

- *Shreya Khare, Rahul Aralikatte, Senthil Mani*

- `1811.01312v2` - [abs](http://arxiv.org/abs/1811.01312v2) - [pdf](http://arxiv.org/pdf/1811.01312v2)

> Fooling deep neural networks with adversarial input have exposed a significant vulnerability in the current state-of-the-art systems in multiple domains. Both black-box and white-box approaches have been used to either replicate the model itself or to craft examples which cause the model to fail. In this work, we propose a framework which uses multi-objective evolutionary optimization to perform both targeted and un-targeted black-box attacks on Automatic Speech Recognition (ASR) systems. We apply this framework on two ASR systems: Deepspeech and Kaldi-ASR, which increases the Word Error Rates (WER) of these systems by upto 980%, indicating the potency of our approach. During both un-targeted and targeted attacks, the adversarial samples maintain a high acoustic similarity of 0.98 and 0.97 with the original audio.

</details>

<details>

<summary>2019-07-03 13:39:30 - Treant: Training Evasion-Aware Decision Trees</summary>

- *Stefano Calzavara, Claudio Lucchese, Gabriele Tolomei, Seyum Assefa Abebe, Salvatore Orlando*

- `1907.01197v2` - [abs](http://arxiv.org/abs/1907.01197v2) - [pdf](http://arxiv.org/pdf/1907.01197v2)

> Despite its success and popularity, machine learning is now recognized as vulnerable to evasion attacks, i.e., carefully crafted perturbations of test inputs designed to force prediction errors. In this paper we focus on evasion attacks against decision tree ensembles, which are among the most successful predictive models for dealing with non-perceptual problems. Even though they are powerful and interpretable, decision tree ensembles have received only limited attention by the security and machine learning communities so far, leading to a sub-optimal state of the art for adversarial learning techniques. We thus propose Treant, a novel decision tree learning algorithm that, on the basis of a formal threat model, minimizes an evasion-aware loss function at each step of the tree construction. Treant is based on two key technical ingredients: robust splitting and attack invariance, which jointly guarantee the soundness of the learning process. Experimental results on three publicly available datasets show that Treant is able to generate decision tree ensembles that are at the same time accurate and nearly insensitive to evasion attacks, outperforming state-of-the-art adversarial learning techniques.

</details>

<details>

<summary>2019-07-04 01:18:46 - CARVE: Practical Security-Focused Software Debloating Using Simple Feature Set Mappings</summary>

- *Michael D. Brown, Santosh Pande*

- `1907.02180v1` - [abs](http://arxiv.org/abs/1907.02180v1) - [pdf](http://arxiv.org/pdf/1907.02180v1)

> Software debloating is an emerging field of study aimed at improving the security and performance of software by removing excess library code and features that are not needed by the end user (called bloat). Software bloat is pervasive, and several debloating techniques have been proposed to address this problem. While these techniques are effective at reducing bloat, they are not practical for the average user, risk creating unsound programs and introducing vulnerabilities, and are not well suited for debloating complex software such as network protocol implementations. In this paper, we propose CARVE, a simple yet effective security-focused debloating technique that overcomes these limitations. CARVE employs static source code annotation to map software features source code, eliminating the need for advanced software analysis during debloating and reducing the overall level of technical sophistication required by the user. CARVE surpasses existing techniques by introducing debloating with replacement, a technique capable of preserving software interoperability and mitigating the risk of creating an unsound program or introducing a vulnerability. We evaluate CARVE in 12 debloating scenarios and demonstrate security and performance improvements that meet or exceed those of existing techniques.

</details>

<details>

<summary>2019-07-04 14:33:49 - Multimodal Uncertainty Reduction for Intention Recognition in Human-Robot Interaction</summary>

- *Susanne Trick, Dorothea Koert, Jan Peters, Constantin Rothkopf*

- `1907.02426v1` - [abs](http://arxiv.org/abs/1907.02426v1) - [pdf](http://arxiv.org/pdf/1907.02426v1)

> Assistive robots can potentially improve the quality of life and personal independence of elderly people by supporting everyday life activities. To guarantee a safe and intuitive interaction between human and robot, human intentions need to be recognized automatically. As humans communicate their intentions multimodally, the use of multiple modalities for intention recognition may not just increase the robustness against failure of individual modalities but especially reduce the uncertainty about the intention to be predicted. This is desirable as particularly in direct interaction between robots and potentially vulnerable humans a minimal uncertainty about the situation as well as knowledge about this actual uncertainty is necessary. Thus, in contrast to existing methods, in this work a new approach for multimodal intention recognition is introduced that focuses on uncertainty reduction through classifier fusion. For the four considered modalities speech, gestures, gaze directions and scene objects individual intention classifiers are trained, all of which output a probability distribution over all possible intentions. By combining these output distributions using the Bayesian method Independent Opinion Pool the uncertainty about the intention to be recognized can be decreased. The approach is evaluated in a collaborative human-robot interaction task with a 7-DoF robot arm. The results show that fused classifiers which combine multiple modalities outperform the respective individual base classifiers with respect to increased accuracy, robustness, and reduced uncertainty.

</details>

<details>

<summary>2019-07-04 16:19:33 - Security Implications Of Compiler Optimizations On Cryptography -- A Review</summary>

- *A. P. Shivarpatna Venkatesh, A. Bhat Handadi, M. Mory*

- `1907.02530v1` - [abs](http://arxiv.org/abs/1907.02530v1) - [pdf](http://arxiv.org/pdf/1907.02530v1)

> When implementing secure software, developers must ensure certain requirements, such as the erasure of secret data after its use and execution in real time. Such requirements are not explicitly captured by the C language and could potentially be violated by compiler optimizations. As a result, developers typically use indirect methods to hide their code's semantics from the compiler and avoid unwanted optimizations. However, such workarounds are not permanent solutions, as increasingly efficient compiler optimization causes code that was considered secure in the past now vulnerable. This paper is a literature review of (1) the security complications caused by compiler optimizations, (2) approaches used by developers to mitigate optimization problems, and (3) recent academic efforts towards enabling security engineers to communicate implicit security requirements to the compiler. In addition, we present a short study of six cryptographic libraries and how they approach the issue of ensuring security requirements. With this paper, we highlight the need for software developers and compiler designers to work together in order to design efficient systems for writing secure software.

</details>

<details>

<summary>2019-07-04 18:31:09 - Integration of the Static Analysis Results Interchange Format in CogniCrypt</summary>

- *Sriteja Kummita, Goran Piskachev*

- `1907.02558v1` - [abs](http://arxiv.org/abs/1907.02558v1) - [pdf](http://arxiv.org/pdf/1907.02558v1)

> Background - Software companies increasingly rely on static analysis tools to detect potential bugs and security vulnerabilities in their software products. In the past decade, more and more commercial and open-source static analysis tools have been developed and are maintained. Each tool comes with its own reporting format, preventing an easy integration of multiple analysis tools in a single interface, such as the Static Analysis Server Protocol (SASP). In 2017, a collaborative effort in industry, including Microsoft and GrammaTech, has proposed the Static Analysis Results Interchange Format (SARIF) to address this issue. SARIF is a standardized format in which static analysis warnings can be encoded, to allow the import and export of analysis reports between different tools.   Purpose - This paper explains the SARIF format through examples and presents a proof of concept of the connector that allows the static analysis tool CogniCrypt to generate and export its results in SARIF format.   Design/Approach - We conduct a cross-sectional study between the SARIF format and CogniCrypt's output format before detailing the implementation of the connector. The study aims to find the components of interest in CogniCrypt that the SARIF export module can complete.   Originality/Value - The integration of SARIF into CogniCrypt described in this paper can be reused to integrate SARIF into other static analysis tools.   Conclusion - After detailing the SARIF format, we present an initial implementation to integrate SARIF into CogniCrypt. After taking advantage of all the features provided by SARIF, CogniCrypt will be able to support SASP.

</details>

<details>

<summary>2019-07-08 06:42:59 - The Price of Interpretability</summary>

- *Dimitris Bertsimas, Arthur Delarue, Patrick Jaillet, Sebastien Martin*

- `1907.03419v1` - [abs](http://arxiv.org/abs/1907.03419v1) - [pdf](http://arxiv.org/pdf/1907.03419v1)

> When quantitative models are used to support decision-making on complex and important topics, understanding a model's ``reasoning'' can increase trust in its predictions, expose hidden biases, or reduce vulnerability to adversarial attacks. However, the concept of interpretability remains loosely defined and application-specific. In this paper, we introduce a mathematical framework in which machine learning models are constructed in a sequence of interpretable steps. We show that for a variety of models, a natural choice of interpretable steps recovers standard interpretability proxies (e.g., sparsity in linear models). We then generalize these proxies to yield a parametrized family of consistent measures of model interpretability. This formal definition allows us to quantify the ``price'' of interpretability, i.e., the tradeoff with predictive accuracy. We demonstrate practical algorithms to apply our framework on real and synthetic datasets.

</details>

<details>

<summary>2019-07-08 06:59:05 - Optimal Explanations of Linear Models</summary>

- *Dimitris Bertsimas, Arthur Delarue, Patrick Jaillet, Sebastien Martin*

- `1907.04669v1` - [abs](http://arxiv.org/abs/1907.04669v1) - [pdf](http://arxiv.org/pdf/1907.04669v1)

> When predictive models are used to support complex and important decisions, the ability to explain a model's reasoning can increase trust, expose hidden biases, and reduce vulnerability to adversarial attacks. However, attempts at interpreting models are often ad hoc and application-specific, and the concept of interpretability itself is not well-defined. We propose a general optimization framework to create explanations for linear models. Our methodology decomposes a linear model into a sequence of models of increasing complexity using coordinate updates on the coefficients. Computing this decomposition optimally is a difficult optimization problem for which we propose exact algorithms and scalable heuristics. By solving this problem, we can derive a parametrized family of interpretability metrics for linear models that generalizes typical proxies, and study the tradeoff between interpretability and predictive accuracy.

</details>

<details>

<summary>2019-07-08 20:56:27 - Annotary: A Concolic Execution System for Developing Secure Smart Contracts</summary>

- *Konrad Weiss, Julian Schütte*

- `1907.03868v1` - [abs](http://arxiv.org/abs/1907.03868v1) - [pdf](http://arxiv.org/pdf/1907.03868v1)

> Ethereum smart contracts are executable programs, deployed on a peer-to-peer network and executed in a consensus-based fashion. Their bytecode is public, immutable and once deployed to the blockchain, cannot be patched anymore. As smart contracts may hold Ether worth of several million dollars, they are attractive targets for attackers and indeed some contracts have successfully been exploited in the recent past, resulting in tremendous financial losses. The correctness of smart contracts is thus of utmost importance. While first approaches on formal verification exist, they demand users to be well-versed in formal methods which are alien to many developers and are only able to analyze individual contracts, without considering their execution environment, i.e., calls to external contracts, sequences of transaction, and values from the actual blockchain storage. In this paper, we present Annotary, a concolic execution framework to analyze smart contracts for vulnerabilities, supported by annotations which developers write directly in the Solidity source code. In contrast to existing work, Annotary supports analysis of inter-transactional, inter-contract control flows and combines symbolic execution of EVM bytecode with a resolution of concrete values from the public Ethereum blockchain. While the analysis of Annotary tends to weight precision higher than soundness, we analyze inter-transactional call chains to eliminate false positives from unreachable states that traditional symbolic execution would not be able to handle. We present the annotation and analysis concepts of Annotary, explain its implementation on top of the Laser symbolic virtual machine, and demonstrate its usage as a plugin for the Sublime Text editor.

</details>

<details>

<summary>2019-07-09 08:45:33 - Deep Pixel-wise Binary Supervision for Face Presentation Attack Detection</summary>

- *Anjith George, Sebastien Marcel*

- `1907.04047v1` - [abs](http://arxiv.org/abs/1907.04047v1) - [pdf](http://arxiv.org/pdf/1907.04047v1)

> Face recognition has evolved as a prominent biometric authentication modality. However, vulnerability to presentation attacks curtails its reliable deployment. Automatic detection of presentation attacks is essential for secure use of face recognition technology in unattended scenarios. In this work, we introduce a Convolutional Neural Network (CNN) based framework for presentation attack detection, with deep pixel-wise supervision. The framework uses only frame level information making it suitable for deployment in smart devices with minimal computational and time overhead. We demonstrate the effectiveness of the proposed approach in public datasets for both intra as well as cross-dataset experiments. The proposed approach achieves an HTER of 0% in Replay Mobile dataset and an ACER of 0.42% in Protocol-1 of OULU dataset outperforming state of the art methods.

</details>

<details>

<summary>2019-07-10 19:48:40 - Pakistan's Internet Voting Experiment</summary>

- *Hina Binte Haq, Ronan McDermott, Syed Taha Ali*

- `1907.07765v1` - [abs](http://arxiv.org/abs/1907.07765v1) - [pdf](http://arxiv.org/pdf/1907.07765v1)

> Pakistan recently conducted small-scale trials of a remote Internet voting system for overseas citizens. In this contribution, we report on the experience: we document the unique combination of sociopolitical, legal, and institutional factors motivating this exercise. We describe the system and it's reported vulnerabilities, and we also highlight new issues pertaining to materiality. If this system is deployed in the next general elections, as seems likely, this development would constitute the largest enfranchised diaspora in the world. Our goal in this paper, therefore, is to provide comprehensive insight into Pakistan's experiment with Internet voting, emphasize outstanding challenges, and identify directions for future research.

</details>

<details>

<summary>2019-07-11 17:33:03 - Computational Concentration of Measure: Optimal Bounds, Reductions, and More</summary>

- *Omid Etesami, Saeed Mahloujifar, Mohammad Mahmoody*

- `1907.05401v1` - [abs](http://arxiv.org/abs/1907.05401v1) - [pdf](http://arxiv.org/pdf/1907.05401v1)

> Product measures of dimension $n$ are known to be concentrated in Hamming distance: for any set $S$ in the product space of probability $\epsilon$, a random point in the space, with probability $1-\delta$, has a neighbor in $S$ that is different from the original point in only $O(\sqrt{n\ln(1/(\epsilon\delta))})$ coordinates. We obtain the tight computational version of this result, showing how given a random point and access to an $S$-membership oracle, we can find such a close point in polynomial time. This resolves an open question of [Mahloujifar and Mahmoody, ALT 2019]. As corollaries, we obtain polynomial-time poisoning and (in certain settings) evasion attacks against learning algorithms when the original vulnerabilities have any cryptographically non-negligible probability.   We call our algorithm MUCIO ("MUltiplicative Conditional Influence Optimizer") since proceeding through the coordinates, it decides to change each coordinate of the given point based on a multiplicative version of the influence of that coordinate, where influence is computed conditioned on previously updated coordinates.   We also define a new notion of algorithmic reduction between computational concentration of measure in different metric probability spaces. As an application, we get computational concentration of measure for high-dimensional Gaussian distributions under the $\ell_1$ metric.   We prove several extensions to the results above: (1) Our computational concentration result is also true when the Hamming distance is weighted. (2) We obtain an algorithmic version of concentration around mean, more specifically, McDiarmid's inequality. (3) Our result generalizes to discrete random processes, and this leads to new tampering algorithms for collective coin tossing protocols. (4) We prove exponential lower bounds on the average running time of non-adaptive query algorithms.

</details>

<details>

<summary>2019-07-11 17:59:13 - Adversarial Objects Against LiDAR-Based Autonomous Driving Systems</summary>

- *Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan Liu, Bo Li*

- `1907.05418v1` - [abs](http://arxiv.org/abs/1907.05418v1) - [pdf](http://arxiv.org/pdf/1907.05418v1)

> Deep neural networks (DNNs) are found to be vulnerable against adversarial examples, which are carefully crafted inputs with a small magnitude of perturbation aiming to induce arbitrarily incorrect predictions. Recent studies show that adversarial examples can pose a threat to real-world security-critical applications: a "physical adversarial Stop Sign" can be synthesized such that the autonomous driving cars will misrecognize it as others (e.g., a speed limit sign). However, these image-space adversarial examples cannot easily alter 3D scans of widely equipped LiDAR or radar on autonomous vehicles. In this paper, we reveal the potential vulnerabilities of LiDAR-based autonomous driving detection systems, by proposing an optimization based approach LiDAR-Adv to generate adversarial objects that can evade the LiDAR-based detection system under various conditions. We first show the vulnerabilities using a blackbox evolution-based algorithm, and then explore how much a strong adversary can do, using our gradient-based approach LiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo autonomous driving platform and show that such physical systems are indeed vulnerable to the proposed attacks. We also 3D-print our adversarial objects and perform physical experiments to illustrate that such vulnerability exists in the real world. Please find more visualizations and results on the anonymous website: https://sites.google.com/view/lidar-adv.

</details>

<details>

<summary>2019-07-12 12:35:10 - Generating 3D Adversarial Point Clouds</summary>

- *Chong Xiang, Charles R. Qi, Bo Li*

- `1809.07016v4` - [abs](http://arxiv.org/abs/1809.07016v4) - [pdf](http://arxiv.org/pdf/1809.07016v4)

> Deep neural networks are known to be vulnerable to adversarial examples which are carefully crafted instances to cause the models to make wrong predictions. While adversarial examples for 2D images and CNNs have been extensively studied, less attention has been paid to 3D data such as point clouds. Given many safety-critical 3D applications such as autonomous driving, it is important to study how adversarial point clouds could affect current deep 3D models. In this work, we propose several novel algorithms to craft adversarial point clouds against PointNet, a widely used deep neural network for point cloud processing. Our algorithms work in two ways: adversarial point perturbation and adversarial point generation. For point perturbation, we shift existing points negligibly. For point generation, we generate either a set of independent and scattered points or a small number (1-3) of point clusters with meaningful shapes such as balls and airplanes which could be hidden in the human psyche. In addition, we formulate six perturbation measurement metrics tailored to the attacks in point clouds and conduct extensive experiments to evaluate the proposed algorithms on the ModelNet40 3D shape classification dataset. Overall, our attack algorithms achieve a success rate higher than 99% for all targeted attacks

</details>

<details>

<summary>2019-07-12 18:47:26 - NEUZZ: Efficient Fuzzing with Neural Program Smoothing</summary>

- *Dongdong She, Kexin Pei, Dave Epstein, Junfeng Yang, Baishakhi Ray, Suman Jana*

- `1807.05620v4` - [abs](http://arxiv.org/abs/1807.05620v4) - [pdf](http://arxiv.org/pdf/1807.05620v4)

> Fuzzing has become the de facto standard technique for finding software vulnerabilities. However, even state-of-the-art fuzzers are not very efficient at finding hard-to-trigger software bugs. Most popular fuzzers use evolutionary guidance to generate inputs that can trigger different bugs. Such evolutionary algorithms, while fast and simple to implement, often get stuck in fruitless sequences of random mutations. Gradient-guided optimization presents a promising alternative to evolutionary guidance. Gradient-guided techniques have been shown to significantly outperform evolutionary algorithms at solving high-dimensional structured optimization problems in domains like machine learning by efficiently utilizing gradients or higher-order derivatives of the underlying function. However, gradient-guided approaches are not directly applicable to fuzzing as real-world program behaviors contain many discontinuities, plateaus, and ridges where the gradient-based methods often get stuck. We observe that this problem can be addressed by creating a smooth surrogate function approximating the discrete branching behavior of target program. In this paper, we propose a novel program smoothing technique using surrogate neural network models that can incrementally learn smooth approximations of a complex, real-world program's branching behaviors. We further demonstrate that such neural network models can be used together with gradient-guided input generation schemes to significantly improve the fuzzing efficiency. Our extensive evaluations demonstrate that NEUZZ significantly outperforms 10 state-of-the-art graybox fuzzers on 10 real-world programs both at finding new bugs and achieving higher edge coverage. NEUZZ found 31 unknown bugs that other fuzzers failed to find in 10 real world programs and achieved 3X more edge coverage than all of the tested graybox fuzzers for 24 hours running.

</details>

<details>

<summary>2019-07-14 11:14:14 - Automatic Repair and Type Binding of Undeclared Variables using Neural Networks</summary>

- *Venkatesh Theru Mohan, Ali Jannesari*

- `1907.06205v1` - [abs](http://arxiv.org/abs/1907.06205v1) - [pdf](http://arxiv.org/pdf/1907.06205v1)

> Deep learning had been used in program analysis for the prediction of hidden software defects using software defect datasets, security vulnerabilities using generative adversarial networks as well as identifying syntax errors by learning a trained neural machine translation on program codes. However, all these approaches either require defect datasets or bug-free source codes that are executable for training the deep learning model. Our neural network model is neither trained with any defect datasets nor bug-free programming source codes, instead it is trained using structural semantic details of Abstract Syntax Tree (AST) where each node represents a construct appearing in the source code. This model is implemented to fix one of the most common semantic errors, such as undeclared variable errors as well as infer their type information before program compilation. By this approach, the model has achieved in correctly locating and identifying 81% of the programs on prutor dataset of 1059 programs with only undeclared variable errors and also inferring their types correctly in 80% of the programs.

</details>

<details>

<summary>2019-07-15 03:22:18 - Secure Distributed Dynamic State Estimation in Wide-Area Smart Grids</summary>

- *Mehmet Necip Kurt, Yasin Yilmaz, Xiaodong Wang*

- `1902.07288v2` - [abs](http://arxiv.org/abs/1902.07288v2) - [pdf](http://arxiv.org/pdf/1902.07288v2)

> Smart grid is a large complex network with a myriad of vulnerabilities, usually operated in adversarial settings and regulated based on estimated system states. In this study, we propose a novel highly secure distributed dynamic state estimation mechanism for wide-area (multi-area) smart grids, composed of geographically separated subregions, each supervised by a local control center. We firstly propose a distributed state estimator assuming regular system operation, that achieves near-optimal performance based on the local Kalman filters and with the exchange of necessary information between local centers. To enhance the security, we further propose to (i) protect the network database and the network communication channels against attacks and data manipulations via a blockchain (BC)-based system design, where the BC operates on the peer-to-peer network of local centers, (ii) locally detect the measurement anomalies in real-time to eliminate their effects on the state estimation process, and (iii) detect misbehaving (hacked/faulty) local centers in real-time via a distributed trust management scheme over the network. We provide theoretical guarantees regarding the false alarm rates of the proposed detection schemes, where the false alarms can be easily controlled. Numerical studies illustrate that the proposed mechanism offers reliable state estimation under regular system operation, timely and accurate detection of anomalies, and good state recovery performance in case of anomalies.

</details>

<details>

<summary>2019-07-15 14:31:57 - Tracking sex: The implications of widespread sexual data leakage and tracking on porn websites</summary>

- *Elena Maris, Timothy Libert, Jennifer Henrichsen*

- `1907.06520v1` - [abs](http://arxiv.org/abs/1907.06520v1) - [pdf](http://arxiv.org/pdf/1907.06520v1)

> This paper explores tracking and privacy risks on pornography websites. Our analysis of 22,484 pornography websites indicated that 93% leak user data to a third party. Tracking on these sites is highly concentrated by a handful of major companies, which we identify. We successfully extracted privacy policies for 3,856 sites, 17% of the total. The policies were written such that one might need a two-year college education to understand them. Our content analysis of the sample's domains indicated 44.97% of them expose or suggest a specific gender/sexual identity or interest likely to be linked to the user. We identify three core implications of the quantitative results: 1) the unique/elevated risks of porn data leakage versus other types of data, 2) the particular risks/impact for vulnerable populations, and 3) the complications of providing consent for porn site users and the need for affirmative consent in these online sexual interactions.

</details>

<details>

<summary>2019-07-17 11:44:18 - An Overview of Attacks and Defences on Intelligent Connected Vehicles</summary>

- *Mahdi Dibaei, Xi Zheng, Kun Jiang, Sasa Maric, Robert Abbas, Shigang Liu, Yuexin Zhang, Yao Deng, Sheng Wen, Jun Zhang, Yang Xiang, Shui Yu*

- `1907.07455v1` - [abs](http://arxiv.org/abs/1907.07455v1) - [pdf](http://arxiv.org/pdf/1907.07455v1)

> Cyber security is one of the most significant challenges in connected vehicular systems and connected vehicles are prone to different cybersecurity attacks that endanger passengers' safety. Cyber security in intelligent connected vehicles is composed of in-vehicle security and security of inter-vehicle communications. Security of Electronic Control Units (ECUs) and the Control Area Network (CAN) bus are the most significant parts of in-vehicle security. Besides, with the development of 4G LTE and 5G remote communication technologies for vehicle-toeverything (V2X) communications, the security of inter-vehicle communications is another potential problem. After giving a short introduction to the architecture of next-generation vehicles including driverless and intelligent vehicles, this review paper identifies a few major security attacks on the intelligent connected vehicles. Based on these attacks, we provide a comprehensive survey of available defences against these attacks and classify them into four categories, i.e. cryptography, network security, software vulnerability detection, and malware detection. We also explore the future directions for preventing attacks on intelligent vehicle systems.

</details>

<details>

<summary>2019-07-17 16:34:40 - SoilingNet: Soiling Detection on Automotive Surround-View Cameras</summary>

- *Michal Uricar, Pavel Krizek, Ganesh Sistu, Senthil Yogamani*

- `1905.01492v2` - [abs](http://arxiv.org/abs/1905.01492v2) - [pdf](http://arxiv.org/pdf/1905.01492v2)

> Cameras are an essential part of sensor suite in autonomous driving. Surround-view cameras are directly exposed to external environment and are vulnerable to get soiled. Cameras have a much higher degradation in performance due to soiling compared to other sensors. Thus it is critical to accurately detect soiling on the cameras, particularly for higher levels of autonomous driving. We created a new dataset having multiple types of soiling namely opaque and transparent. It will be released publicly as part of our WoodScape dataset \cite{yogamani2019woodscape} to encourage further research. We demonstrate high accuracy using a Convolutional Neural Network (CNN) based architecture. We also show that it can be combined with the existing object detection task in a multi-task learning framework. Finally, we make use of Generative Adversarial Networks (GANs) to generate more images for data augmentation and show that it works successfully similar to the style transfer.

</details>

<details>

<summary>2019-07-18 06:34:08 - Global AI Ethics: A Review of the Social Impacts and Ethical Implications of Artificial Intelligence</summary>

- *Alexa Hagerty, Igor Rubinov*

- `1907.07892v1` - [abs](http://arxiv.org/abs/1907.07892v1) - [pdf](http://arxiv.org/pdf/1907.07892v1)

> The ethical implications and social impacts of artificial intelligence have become topics of compelling interest to industry, researchers in academia, and the public. However, current analyses of AI in a global context are biased toward perspectives held in the U.S., and limited by a lack of research, especially outside the U.S. and Western Europe.   This article summarizes the key findings of a literature review of recent social science scholarship on the social impacts of AI and related technologies in five global regions. Our team of social science researchers reviewed more than 800 academic journal articles and monographs in over a dozen languages.   Our review of the literature suggests that AI is likely to have markedly different social impacts depending on geographical setting. Likewise, perceptions and understandings of AI are likely to be profoundly shaped by local cultural and social context.   Recent research in U.S. settings demonstrates that AI-driven technologies have a pattern of entrenching social divides and exacerbating social inequality, particularly among historically-marginalized groups. Our literature review indicates that this pattern exists on a global scale, and suggests that low- and middle-income countries may be more vulnerable to the negative social impacts of AI and less likely to benefit from the attendant gains.   We call for rigorous ethnographic research to better understand the social impacts of AI around the world. Global, on-the-ground research is particularly critical to identify AI systems that may amplify social inequality in order to mitigate potential harms. Deeper understanding of the social impacts of AI in diverse social settings is a necessary precursor to the development, implementation, and monitoring of responsible and beneficial AI technologies, and forms the basis for meaningful regulation of these technologies.

</details>

<details>

<summary>2019-07-18 18:23:26 - Logical Segmentation of Source Code</summary>

- *Jacob Dormuth, Ben Gelman, Jessica Moore, David Slater*

- `1907.08615v1` - [abs](http://arxiv.org/abs/1907.08615v1) - [pdf](http://arxiv.org/pdf/1907.08615v1)

> Many software analysis methods have come to rely on machine learning approaches. Code segmentation - the process of decomposing source code into meaningful blocks - can augment these methods by featurizing code, reducing noise, and limiting the problem space. Traditionally, code segmentation has been done using syntactic cues; current approaches do not intentionally capture logical content. We develop a novel deep learning approach to generate logical code segments regardless of the language or syntactic correctness of the code. Due to the lack of logically segmented source code, we introduce a unique data set construction technique to approximate ground truth for logically segmented code. Logical code segmentation can improve tasks such as automatically commenting code, detecting software vulnerabilities, repairing bugs, labeling code functionality, and synthesizing new code.

</details>

<details>

<summary>2019-07-20 06:53:26 - Design and Field Implementation of Blockchain Based Renewable Energy Trading in Residential Communities</summary>

- *Shivam Saxena, Hany Farag, Aidan Brookson, Hjalmar Turesson, Henry M. Kim*

- `1907.12370v1` - [abs](http://arxiv.org/abs/1907.12370v1) - [pdf](http://arxiv.org/pdf/1907.12370v1)

> This paper proposes a peer to peer (P2P), blockchain based energy trading market platform for residential communities with the objective of reducing overall community peak demand and household electricity bills. Smart homes within the community place energy bids for its available distributed energy resources (DERs) for each discrete trading period during a day, and a double auction mechanism is used to clear the market and compute the market clearing price (MCP). The marketplace is implemented on a permissioned blockchain infrastructure, where bids are stored to the immutable ledger and smart contracts are used to implement the MCP calculation and award service contracts to all winning bids. Utilizing the blockchain obviates the need for a trusted, centralized auctioneer, and eliminates vulnerability to a single point of failure. Simulation results show that the platform enables a community peak demand reduction of 46%, as well as a weekly savings of 6%. The platform is also tested at a real-world Canadian microgrid using the Hyperledger Fabric blockchain framework, to show the end to end connectivity of smart home DERs to the platform.

</details>

<details>

<summary>2019-07-20 20:03:01 - Defense-in-Depth: A Recipe for Logic Locking to Prevail</summary>

- *M Tanjidur Rahman, M Sazadur Rahman, Huanyu Wang, Shahin Tajik, Waleed Khalil, Farimah Farahmandi, Domenic Forte, Navid Asadizanjani, Mark Tehranipoor*

- `1907.08863v1` - [abs](http://arxiv.org/abs/1907.08863v1) - [pdf](http://arxiv.org/pdf/1907.08863v1)

> Logic locking has emerged as a promising solution for protecting the semiconductor intellectual Property (IP) from the untrusted entities in the design and fabrication process. Logic locking hides the functionality of the IP by embedding additional key-gates in the circuit. The correct output of the chip is produced, once the correct key value is available at the input of the key-gates. The confidentiality of the key is imperative for the security of the locked IP as it stands as the lone barrier against IP infringement. Therefore, the logic locking is considered as a broken scheme once the key value is exposed. The research community has shown the vulnerability of the logic locking techniques against different classes of attacks, such as Oracle-guided and physical attacks. Although several countermeasures have already been proposed against such attacks, none of them is simultaneously impeccable against Oracle-guided, Oracle-less, and physical attacks. Under such circumstances, a defense-in-depth approach can be considered as a practical approach in addressing the vulnerabilities of logic locking. Defense-in-depth is a multilayer defense approach where several independent countermeasures are implemented in the device to provide aggregated protection against different attack vectors. Introducing such a multilayer defense model in logic locking is the major contribution of this paper. With regard to this, we first identify the core components of logic locking schemes, which need to be protected. Afterwards, we categorize the vulnerabilities of core components according to potential threats for the locking key in logic locking schemes. Furthermore, we propose several defense layers and countermeasures to protect the device from those vulnerabilities. Finally, we turn our focus to open research questions and conclude with suggestions for future research directions.

</details>

<details>

<summary>2019-07-21 11:52:36 - Open DNN Box by Power Side-Channel Attack</summary>

- *Yun Xiang, Zhuangzhi Chen, Zuohui Chen, Zebin Fang, Haiyang Hao, Jinyin Chen, Yi Liu, Zhefu Wu, Qi Xuan, Xiaoniu Yang*

- `1907.10406v1` - [abs](http://arxiv.org/abs/1907.10406v1) - [pdf](http://arxiv.org/pdf/1907.10406v1)

> Deep neural networks are becoming popular and important assets of many AI companies. However, recent studies indicate that they are also vulnerable to adversarial attacks. Adversarial attacks can be either white-box or black-box. The white-box attacks assume full knowledge of the models while the black-box ones assume none. In general, revealing more internal information can enable much more powerful and efficient attacks. However, in most real-world applications, the internal information of embedded AI devices is unavailable, i.e., they are black-box. Therefore, in this work, we propose a side-channel information based technique to reveal the internal information of black-box models. Specifically, we have made the following contributions: (1) we are the first to use side-channel information to reveal internal network architecture in embedded devices; (2) we are the first to construct models for internal parameter estimation; and (3) we validate our methods on real-world devices and applications. The experimental results show that our method can achieve 96.50\% accuracy on average. Such results suggest that we should pay strong attention to the security problem of many AI applications, and further propose corresponding defensive strategies in the future.

</details>

<details>

<summary>2019-07-24 04:22:39 - SirenAttack: Generating Adversarial Audio for End-to-End Acoustic Systems</summary>

- *Tianyu Du, Shouling Ji, Jinfeng Li, Qinchen Gu, Ting Wang, Raheem Beyah*

- `1901.07846v2` - [abs](http://arxiv.org/abs/1901.07846v2) - [pdf](http://arxiv.org/pdf/1901.07846v2)

> Despite their immense popularity, deep learning-based acoustic systems are inherently vulnerable to adversarial attacks, wherein maliciously crafted audios trigger target systems to misbehave. In this paper, we present SirenAttack, a new class of attacks to generate adversarial audios. Compared with existing attacks, SirenAttack highlights with a set of significant features: (i) versatile -- it is able to deceive a range of end-to-end acoustic systems under both white-box and black-box settings; (ii) effective -- it is able to generate adversarial audios that can be recognized as specific phrases by target acoustic systems; and (iii) stealthy -- it is able to generate adversarial audios indistinguishable from their benign counterparts to human perception. We empirically evaluate SirenAttack on a set of state-of-the-art deep learning-based acoustic systems (including speech command recognition, speaker recognition and sound event classification), with results showing the versatility, effectiveness, and stealthiness of SirenAttack. For instance, it achieves 99.45% attack success rate on the IEMOCAP dataset against the ResNet18 model, while the generated adversarial audios are also misinterpreted by multiple popular ASR platforms, including Google Cloud Speech, Microsoft Bing Voice, and IBM Speech-to-Text. We further evaluate three potential defense methods to mitigate such attacks, including adversarial training, audio downsampling, and moving average filtering, which leads to promising directions for further research.

</details>

<details>

<summary>2019-07-24 09:04:23 - Towards Adversarially Robust Object Detection</summary>

- *Haichao Zhang, Jianyu Wang*

- `1907.10310v1` - [abs](http://arxiv.org/abs/1907.10310v1) - [pdf](http://arxiv.org/pdf/1907.10310v1)

> Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We first revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO verified the effectiveness of the proposed approach.

</details>

<details>

<summary>2019-07-24 21:55:00 - Leveraging Diversity for Achieving Resilient Consensus in Sparse Networks</summary>

- *Faiq Ghawash, Waseem Abbas*

- `1907.10742v1` - [abs](http://arxiv.org/abs/1907.10742v1) - [pdf](http://arxiv.org/pdf/1907.10742v1)

> A networked system can be made resilient against adversaries and attacks if the underlying network graph is structurally robust. For instance, to achieve distributed consensus in the presence of adversaries, the underlying network graph needs to satisfy certain robustness conditions. A typical approach to making networks structurally robust is to strategically add extra links between nodes, which might be prohibitively expensive. In this paper, we propose an alternative way of improving network's robustness, that is by considering heterogeneity of nodes. Nodes in a network can be of different types and can have multiple variants. As a result, different nodes can have disjoint sets of vulnerabilities, which means that an attacker can only compromise a particular type of nodes by exploiting a particular vulnerability. We show that, by such a diversification of nodes, attacker's ability to change the underlying network structure is significantly reduced. Consequently, even a sparse network with heterogeneous nodes can exhibit the properties of a structurally robust network. Using these ideas, we propose a distributed control policy that utilizes heterogeneity in the network to achieve resilient consensus in adversarial environment. We extend the notion of $(r,s)$-robustness to incorporate the diversity of nodes and provide necessary and sufficient conditions to guarantee resilient distributed consensus in heterogeneous networks. Finally we study the properties and construction of robust graphs with heterogeneous nodes.

</details>

<details>

<summary>2019-07-25 01:10:25 - Mitigating Vulnerabilities of Voltage-based Intrusion Detection Systems in Controller Area Networks</summary>

- *Sang Uk Sagong, Radha Poovendran, Linda Bushnell*

- `1907.10783v1` - [abs](http://arxiv.org/abs/1907.10783v1) - [pdf](http://arxiv.org/pdf/1907.10783v1)

> Data for controlling a vehicle is exchanged among Electronic Control Units (ECUs) via in-vehicle network protocols such as the Controller Area Network (CAN) protocol. Since these protocols are designed for an isolated network, the protocols do not encrypt data nor authenticate messages. Intrusion Detection Systems (IDSs) are developed to secure the CAN protocol by detecting abnormal deviations in physical properties. For instance, a voltage-based IDS (VIDS) exploits voltage characteristics of each ECU to detect an intrusion. An ECU with VIDS must be connected to the CAN bus using extra wires to measure voltages of the CAN bus lines. These extra wires, however, may introduce new attack surfaces to the CAN bus if the ECU with VIDS is compromised. We investigate new vulnerabilities of VIDS and demonstrate that an adversary may damage an ECU with VIDS, block message transmission, and force an ECU to retransmit messages. In order to defend the CAN bus against these attacks, we propose two hardware-based Intrusion Response Systems (IRSs) that disconnect the compromised ECU from the CAN bus once these attacks are detected. We develop four voltage-based attacks by exploiting vulnerabilities of VIDS and evaluate the effectiveness of the proposed IRSs using a CAN bus testbed.

</details>

<details>

<summary>2019-07-25 22:17:36 - Cryptanalysis of two recently proposed ultralightweight authentication protocol for IoT</summary>

- *Masoumeh Safkhani, Nasour Bagheri*

- `1907.11322v1` - [abs](http://arxiv.org/abs/1907.11322v1) - [pdf](http://arxiv.org/pdf/1907.11322v1)

> By expanding the connection of objects to the Internet and their entry to human life, the issue of security and privacy has become important. In order to enhance security and privacy on the Internet, many security protocols have been developed. Unfortunately, the security analyzes that have been carried out on these protocols show that they are vulnerable to one or few attacks, which eliminates the use of these protocols. Therefore, the need for a security protocol on the Internet of Things (IoT) has not yet been resolved.   Recently, Khor and Sidorov cryptanalyzed the Wang et al. protocol and presented an improved version of it. In this paper, at first, we show that this protocol also does not have sufficient security and so it is not recommended to be used in any application. More precisely, we present a full secret disclosure attack against this protocol, which extracted the whole secrets of the protocol by two communication with the target tag.   In addition, Sidorv et al. recently proposed an ultralightweight mutual authentication RFID protocol for blockchain enabled supply chains, supported by formal and informal security proofs. However, we present a full secret disclosure attack against this protocol as well.

</details>

<details>

<summary>2019-07-26 20:05:19 - Understanding Adversarial Robustness: The Trade-off between Minimum and Average Margin</summary>

- *Kaiwen Wu, Yaoliang Yu*

- `1907.11780v1` - [abs](http://arxiv.org/abs/1907.11780v1) - [pdf](http://arxiv.org/pdf/1907.11780v1)

> Deep models, while being extremely versatile and accurate, are vulnerable to adversarial attacks: slight perturbations that are imperceptible to humans can completely flip the prediction of deep models. Many attack and defense mechanisms have been proposed, although a satisfying solution still largely remains elusive. In this work, we give strong evidence that during training, deep models maximize the minimum margin in order to achieve high accuracy, but at the same time decrease the \emph{average} margin hence hurting robustness. Our empirical results highlight an intrinsic trade-off between accuracy and robustness for current deep model training. To further address this issue, we propose a new regularizer to explicitly promote average margin, and we verify through extensive experiments that it does lead to better robustness. Our regularized objective remains Fisher-consistent, hence asymptotically can still recover the Bayes optimal classifier.

</details>

<details>

<summary>2019-07-27 03:53:19 - MULDEF: Multi-model-based Defense Against Adversarial Examples for Neural Networks</summary>

- *Siwakorn Srisakaokul, Yuhao Zhang, Zexuan Zhong, Wei Yang, Tao Xie, Bo Li*

- `1809.00065v3` - [abs](http://arxiv.org/abs/1809.00065v3) - [pdf](http://arxiv.org/pdf/1809.00065v3)

> Despite being popularly used in many applications, neural network models have been found to be vulnerable to adversarial examples, i.e., carefully crafted examples aiming to mislead machine learning models. Adversarial examples can pose potential risks on safety and security critical applications. However, existing defense approaches are still vulnerable to attacks, especially in a white-box attack scenario. To address this issue, we propose a new defense approach, named MulDef, based on robustness diversity. Our approach consists of (1) a general defense framework based on multiple models and (2) a technique for generating these multiple models to achieve high defense capability. In particular, given a target model, our framework includes multiple models (constructed from the target model) to form a model family. The model family is designed to achieve robustness diversity (i.e., an adversarial example successfully attacking one model cannot succeed in attacking other models in the family). At runtime, a model is randomly selected from the family to be applied on each input example. Our general framework can inspire rich future research to construct a desirable model family achieving higher robustness diversity. Our evaluation results show that MulDef (with only up to 5 models in the family) can substantially improve the target model's accuracy on adversarial examples by 22-74% in a white-box attack scenario, while maintaining similar accuracy on legitimate examples.

</details>

<details>

<summary>2019-07-27 16:02:45 - Deriving ChaCha20 Key Streams From Targeted Memory Analysis</summary>

- *Peter McLaren, William J Buchanan, Gordon Russell, Zhiyuan Tan*

- `1907.11941v1` - [abs](http://arxiv.org/abs/1907.11941v1) - [pdf](http://arxiv.org/pdf/1907.11941v1)

> There can be performance and vulnerability concerns with block ciphers, thus stream ciphers can used as an alternative. Although many symmetric key stream ciphers are fairly resistant to side-channel attacks, cryptographic artefacts may exist in memory. This paper identifies a significant vulnerability within OpenSSH and OpenSSL and which involves the discovery of cryptographic artefacts used within the ChaCha20 cipher. This can allow for the cracking of tunneled data using a single targeted memory extraction. With this, law enforcement agencies and/or malicious agents could use the vulnerability to take copies of the encryption keys used for each tunnelled connection. The user of a virtual machine would not be alerted to the capturing of the encryption key, as the method runs from an extraction of the running memory. Methods of mitigation include making cryptographic artefacts difficult to discover and limiting memory access.

</details>

<details>

<summary>2019-07-28 00:25:21 - Machine Learning for Intelligent Authentication in 5G-and-Beyond Wireless Networks</summary>

- *He Fang, Xianbin Wang, Stefano Tomasin*

- `1907.00429v2` - [abs](http://arxiv.org/abs/1907.00429v2) - [pdf](http://arxiv.org/pdf/1907.00429v2)

> The fifth generation (5G) and beyond wireless networks are critical to support diverse vertical applications by connecting heterogeneous devices and machines, which directly increase vulnerability for various spoofing attacks. Conventional cryptographic and physical layer authentication techniques are facing some challenges in complex dynamic wireless environments, including significant security overhead, low reliability, as well as difficulty in pre-designing authentication model, providing continuous protections, and learning time-varying attributes. In this article, we envision new authentication approaches based on machine learning techniques by opportunistically leveraging physical layer attributes, and introduce intelligence to authentication for more efficient security provisioning. Machine learning paradigms for intelligent authentication design are presented, namely for parametric/non-parametric and supervised/unsupervised/reinforcement learning algorithms. In a nutshell, the machine learning-based intelligent authentication approaches utilize specific features in the multi-dimensional domain for achieving cost-effective, more reliable, model-free, continuous and situation-aware device validation under unknown network conditions and unpredictable dynamics.

</details>

<details>

<summary>2019-07-28 08:53:05 - Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks</summary>

- *Aamir Mustafa, Salman Khan, Munawar Hayat, Roland Goecke, Jianbing Shen, Ling Shao*

- `1904.00887v4` - [abs](http://arxiv.org/abs/1904.00887v4) - [pdf](http://arxiv.org/pdf/1904.00887v4)

> Deep neural networks are vulnerable to adversarial attacks, which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and white-box attack scenarios and show significant gains in comparison to state-of-the art defenses.

</details>

<details>

<summary>2019-07-28 20:45:02 - Are Odds Really Odd? Bypassing Statistical Detection of Adversarial Examples</summary>

- *Hossein Hosseini, Sreeram Kannan, Radha Poovendran*

- `1907.12138v1` - [abs](http://arxiv.org/abs/1907.12138v1) - [pdf](http://arxiv.org/pdf/1907.12138v1)

> Deep learning classifiers are known to be vulnerable to adversarial examples. A recent paper presented at ICML 2019 proposed a statistical test detection method based on the observation that logits of noisy adversarial examples are biased toward the true class. The method is evaluated on CIFAR-10 dataset and is shown to achieve 99% true positive rate (TPR) at only 1% false positive rate (FPR). In this paper, we first develop a classifier-based adaptation of the statistical test method and show that it improves the detection performance. We then propose Logit Mimicry Attack method to generate adversarial examples such that their logits mimic those of benign images. We show that our attack bypasses both statistical test and classifier-based methods, reducing their TPR to less than 2:2% and 1:6%, respectively, even at 5% FPR. We finally show that a classifier-based detector that is trained with logits of mimicry adversarial examples can be evaded by an adaptive attacker that specifically targets the detector. Furthermore, even a detector that is iteratively trained to defend against adaptive attacker cannot be made robust, indicating that statistics of logits cannot be used to detect adversarial examples.

</details>

<details>

<summary>2019-07-28 21:02:22 - Characterizing and Understanding Software Developer Networks in Security Development</summary>

- *Song Wang, Nachi Nagappan*

- `1907.12141v1` - [abs](http://arxiv.org/abs/1907.12141v1) - [pdf](http://arxiv.org/pdf/1907.12141v1)

> To build secure software, developers often work together during software development and maintenance to find, fix, and prevent security vulnerabilities. Examining the nature of developer interactions during their security activities regarding security introducing and fixing activities can provide insights for improving current practices.   In this work, we conduct a large-scale empirical study to characterize and understand developers' interactions during their security activities regarding security introducing and fixing, which involves more than 16K security fixing commits and over 28K security introducing commits from nine large-scale open-source software projects. For our analysis, we first examine whether a project is a hero-centric project when assessing developers' contribution in their security activities. Then we study the interaction patterns between developers, explore how the distribution of the patterns changes over time, and study the impact of developers' interactions on the quality of projects. In addition, we also characterize the nature of developer interaction in security activities in comparison to developer interaction in non-security activities (i.e., introducing and fixing non-security bugs). Among our findings we identify that: most of the experimental projects are non hero-centric projects when evaluating developers' contribution by using their security activities; there exist common dominating interaction patterns across our experimental projects; the distribution of interaction patterns has correlation with the quality of software projects. We believe the findings from this study can help developers understand how vulnerabilitiesoriginate and fix under the interactions of software developers.

</details>

<details>

<summary>2019-07-30 06:03:57 - Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation</summary>

- *Utku Ozbulak, Arnout Van Messem, Wesley De Neve*

- `1907.13124v1` - [abs](http://arxiv.org/abs/1907.13124v1) - [pdf](http://arxiv.org/pdf/1907.13124v1)

> Deep learning models, which are increasingly being used in the field of medical image analysis, come with a major security risk, namely, their vulnerability to adversarial examples. Adversarial examples are carefully crafted samples that force machine learning models to make mistakes during testing time. These malicious samples have been shown to be highly effective in misguiding classification tasks. However, research on the influence of adversarial examples on segmentation is significantly lacking. Given that a large portion of medical imaging problems are effectively segmentation problems, we analyze the impact of adversarial examples on deep learning-based image segmentation models. Specifically, we expose the vulnerability of these models to adversarial examples by proposing the Adaptive Segmentation Mask Attack (ASMA). This novel algorithm makes it possible to craft targeted adversarial examples that come with (1) high intersection-over-union rates between the target adversarial mask and the prediction and (2) with perturbation that is, for the most part, invisible to the bare eye. We lay out experimental and visual evidence by showing results obtained for the ISIC skin lesion segmentation challenge and the problem of glaucoma optic disc segmentation. An implementation of this algorithm and additional examples can be found at https://github.com/utkuozbulak/adaptive-segmentation-mask-attack.

</details>

<details>

<summary>2019-07-31 02:11:24 - Exploiting the Inherent Limitation of L0 Adversarial Examples</summary>

- *Fei Zuo, Bokai Yang, Xiaopeng Li, Lannan Luo, Qiang Zeng*

- `1812.09638v3` - [abs](http://arxiv.org/abs/1812.09638v3) - [pdf](http://arxiv.org/pdf/1812.09638v3)

> Despite the great achievements made by neural networks on tasks such as image classification, they are brittle and vulnerable to adversarial example (AE) attacks, which are crafted by adding human-imperceptible perturbations to inputs in order that a neural-network-based classifier incorrectly labels them. In particular, L0 AEs are a category of widely discussed threats where adversaries are restricted in the number of pixels that they can corrupt. However, our observation is that, while L0 attacks modify as few pixels as possible, they tend to cause large-amplitude perturbations to the modified pixels. We consider this as an inherent limitation of L0 AEs, and thwart such attacks by both detecting and rectifying them. The main novelty of the proposed detector is that we convert the AE detection problem into a comparison problem by exploiting the inherent limitation of L0 attacks. More concretely, given an image I, it is pre-processed to obtain another image I' . A Siamese network, which is known to be effective in comparison, takes I and I' as the input pair to determine whether I is an AE. A trained Siamese network automatically and precisely captures the discrepancies between I and I' to detect L0 perturbations. In addition, we show that the pre-processing technique, inpainting, used for detection can also work as an effective defense, which has a high probability of removing the adversarial influence of L0 perturbations. Thus, our system, called AEPECKER, demonstrates not only high AE detection accuracies, but also a notable capability to correct the classification results.

</details>

<details>

<summary>2019-07-31 15:16:00 - Optimal Attacks on Reinforcement Learning Policies</summary>

- *Alessio Russo, Alexandre Proutiere*

- `1907.13548v1` - [abs](http://arxiv.org/abs/1907.13548v1) - [pdf](http://arxiv.org/pdf/1907.13548v1)

> Control policies, trained using the Deep Reinforcement Learning, have been recently shown to be vulnerable to adversarial attacks introducing even very small perturbations to the policy input. The attacks proposed so far have been designed using heuristics, and build on existing adversarial example crafting techniques used to dupe classifiers in supervised learning. In contrast, this paper investigates the problem of devising optimal attacks, depending on a well-defined attacker's objective, e.g., to minimize the main agent average reward. When the policy and the system dynamics, as well as rewards, are known to the attacker, a scenario referred to as a white-box attack, designing optimal attacks amounts to solving a Markov Decision Process. For what we call black-box attacks, where neither the policy nor the system is known, optimal attacks can be trained using Reinforcement Learning techniques. Through numerical experiments, we demonstrate the efficiency of our attacks compared to existing attacks (usually based on Gradient methods). We further quantify the potential impact of attacks and establish its connection to the smoothness of the policy under attack. Smooth policies are naturally less prone to attacks (this explains why Lipschitz policies, with respect to the state, are more resilient). Finally, we show that from the main agent perspective, the system uncertainties and the attacker can be modeled as a Partially Observable Markov Decision Process. We actually demonstrate that using Reinforcement Learning techniques tailored to POMDP (e.g. using Recurrent Neural Networks) leads to more resilient policies.

</details>


## 2019-08

<details>

<summary>2019-08-01 11:03:55 - Modeling and Analysis of Integrated Proactive Defense Mechanisms for Internet-of-Things</summary>

- *Mengmeng Ge, Jin-Hee Cho, Bilal Ishfaq, Dong Seong Kim*

- `1908.00327v1` - [abs](http://arxiv.org/abs/1908.00327v1) - [pdf](http://arxiv.org/pdf/1908.00327v1)

> As a solution to protect and defend a system against inside attacks, many intrusion detection systems (IDSs) have been developed to identify and react to them for protecting a system. However, the core idea of an IDS is a reactive mechanism in nature even though it detects intrusions which have already been in the system. Hence, the reactive mechanisms would be way behind and not effective for the actions taken by agile and smart attackers. Due to the inherent limitation of an IDS with the reactive nature, intrusion prevention systems (IPSs) have been developed to thwart potential attackers and/or mitigate the impact of the intrusions before they penetrate into the system. In this chapter, we introduce an integrated defense mechanism to achieve intrusion prevention in a software-defined Internet-of-Things (IoT) network by leveraging the technologies of cyberdeception (i.e., a decoy system) and moving target defense, namely MTD (i.e., network topology shuffling). In addition, we validate their effectiveness and efficiency based on the devised graphical security model (GSM)-based evaluation framework. To develop an adaptive, proactive intrusion prevention mechanism, we employed fitness functions based on the genetic algorithm in order to identify an optimal network topology where a network topology can be shuffled based on the detected level of the system vulnerability. Our simulation results show that GA-based shuffling schemes outperform random shuffling schemes in terms of the number of attack paths toward decoy targets. In addition, we observe that there exists a tradeoff between the system lifetime (i.e., mean time to security failure) and the defense cost introduced by the proposed MTD technique for fixed and adaptive shuffling schemes. That is, a fixed GA-based shuffling can achieve higher MTTSF with more cost while an adaptive GA-based shuffling obtains less MTTSF with less cost.

</details>

<details>

<summary>2019-08-02 00:22:20 - Exploring Challenges and Opportunities in Cybersecurity Risk and Threat Communications Related To The Medical Internet Of Things (MIoT)</summary>

- *George W. Jackson, Jr., Shawon Rahman*

- `1908.00666v1` - [abs](http://arxiv.org/abs/1908.00666v1) - [pdf](http://arxiv.org/pdf/1908.00666v1)

> As device interconnectivity and ubiquitous computing continues to proliferate healthcare, the Medical Internet of Things (MIoT), also well known as the, Internet of Medical Things (IoMT) or the Internet of Healthcare Things (IoHT), is certain to play a major role in the health, and well-being of billions of people across the globe. When it comes to issues of cybersecurity risks and threats connected to the IoT in all of its various flavors the emphasis has been on technical challenges and technical solution. However, especially in the area of healthcare there is another substantial and potentially grave challenge. It is the challenge of thoroughly and accurately communicating the nature and extent of cybersecurity risks and threats to patients who are reliant upon these interconnected healthcare technologies to improve and even preserve their lives. This case study was conducted to assess the scope and depth of cybersecurity risk and threat communications delivered to an extremely vulnerable patient population, semi-structured interviews were held with cardiac medical device specialists across the United States. This research contributes scientific data in the field of healthcare cybersecurity and assists scholars and practitioners in advancing education and research in the field of MIoT patient communications.

</details>

<details>

<summary>2019-08-02 18:03:48 - Detection and Mitigation of Attacks on Transportation Networks as a Multi-Stage Security Game</summary>

- *Aron Laszka, Waseem Abbas, Yevgeniy Vorobeychik, Xenofon Koutsoukos*

- `1808.08349v2` - [abs](http://arxiv.org/abs/1808.08349v2) - [pdf](http://arxiv.org/pdf/1808.08349v2)

> In recent years, state-of-the-art traffic-control devices have evolved from standalone hardware to networked smart devices. Smart traffic control enables operators to decrease traffic congestion and environmental impact by acquiring real-time traffic data and changing traffic signals from fixed to adaptive schedules. However, these capabilities have inadvertently exposed traffic control to a wide range of cyber-attacks, which adversaries can easily mount through wireless networks or even through the Internet. Indeed, recent studies have found that a large number of traffic signals that are deployed in practice suffer from exploitable vulnerabilities, which adversaries may use to take control of the devices. Thanks to the hardware-based failsafes that most devices employ, adversaries cannot cause traffic accidents directly by setting compromised signals to dangerous configurations. Nonetheless, an adversary could cause disastrous traffic congestion by changing the schedule of compromised traffic signals, thereby effectively crippling the transportation network. To provide theoretical foundations for the protection of transportation networks from these attacks, we introduce a game-theoretic model of launching, detecting, and mitigating attacks that tamper with traffic-signal schedules. We show that finding optimal strategies is a computationally challenging problem, and we propose efficient heuristic algorithms for finding near optimal strategies. We also introduce a Gaussian-process based anomaly detector, which can alert operators to ongoing attacks. Finally, we evaluate our algorithms and the proposed detector using numerical experiments based on the SUMO traffic simulator.

</details>

<details>

<summary>2019-08-02 20:48:26 - The Efficacy of SHIELD under Different Threat Models</summary>

- *Cory Cornelius, Nilaksh Das, Shang-Tse Chen, Li Chen, Michael E. Kounavis, Duen Horng Chau*

- `1902.00541v2` - [abs](http://arxiv.org/abs/1902.00541v2) - [pdf](http://arxiv.org/pdf/1902.00541v2)

> In this appraisal paper, we evaluate the efficacy of SHIELD, a compression-based defense framework for countering adversarial attacks on image classification models, which was published at KDD 2018. Here, we consider alternative threat models not studied in the original work, where we assume that an adaptive adversary is aware of the ensemble defense approach, the defensive pre-processing, and the architecture and weights of the models used in the ensemble. We define scenarios with varying levels of threat and empirically analyze the proposed defense by varying the degree of information available to the attacker, spanning from a full white-box attack to the gray-box threat model described in the original work. To evaluate the robustness of the defense against an adaptive attacker, we consider the targeted-attack success rate of the Projected Gradient Descent (PGD) attack, which is a strong gradient-based adversarial attack proposed in adversarial machine learning research. We also experiment with training the SHIELD ensemble from scratch, which is different from re-training using a pre-trained model as done in the original work. We find that the targeted PGD attack has a success rate of 64.3% against the original SHIELD ensemble in the full white box scenario, but this drops to 48.9% if the models used in the ensemble are trained from scratch instead of being retrained. Our experiments further reveal that an ensemble whose models are re-trained indeed have higher correlation in the cosine similarity space, and models that are trained from scratch are less vulnerable to targeted attacks in the white-box and gray-box scenarios.

</details>

<details>

<summary>2019-08-04 02:51:53 - A systematic review of fuzzing based on machine learning techniques</summary>

- *Yan Wang, Peng Jia, Luping Liu, Jiayong Liu*

- `1908.01262v1` - [abs](http://arxiv.org/abs/1908.01262v1) - [pdf](http://arxiv.org/pdf/1908.01262v1)

> Security vulnerabilities play a vital role in network security system. Fuzzing technology is widely used as a vulnerability discovery technology to reduce damage in advance. However, traditional fuzzing techniques have many challenges, such as how to mutate input seed files, how to increase code coverage, and how to effectively bypass verification. Machine learning technology has been introduced as a new method into fuzzing test to alleviate these challenges. This paper reviews the research progress of using machine learning technology for fuzzing test in recent years, analyzes how machine learning improve the fuzz process and results, and sheds light on future work in fuzzing. Firstly, this paper discusses the reasons why machine learning techniques can be used for fuzzing scenarios and identifies six different stages in which machine learning have been used. Then this paper systematically study the machine learning based fuzzing models from selection of machine learning algorithm, pre-processing methods, datasets, evaluation metrics, and hyperparameters setting. Next, this paper assesses the performance of the machine learning models based on the frequently used evaluation metrics. The results of the evaluation prove that machine learning technology has an acceptable capability of categorize predictive for fuzzing. Finally, the comparison on capability of discovering vulnerabilities between traditional fuzzing tools and machine learning based fuzzing tools is analyzed. The results depict that the introduction of machine learning technology can improve the performance of fuzzing. However, there are still some limitations, such as unbalanced training samples and difficult to extract the characteristics related to vulnerabilities.

</details>

<details>

<summary>2019-08-05 02:13:38 - ImageNet-trained deep neural network exhibits illusion-like response to the Scintillating Grid</summary>

- *Eric D. Sun, Ron Dekel*

- `1907.09019v2` - [abs](http://arxiv.org/abs/1907.09019v2) - [pdf](http://arxiv.org/pdf/1907.09019v2)

> Deep neural network (DNN) models for computer vision are now capable of human-level object recognition. Consequently, similarities in the performance and vulnerabilities of DNN and human vision are of great interest. Here we characterize the response of the VGG-19 DNN to images of the Scintillating Grid visual illusion, in which white dots are perceived to be partially black. We observed a significant deviation from the expected monotonic relation between VGG-19 representational dissimilarity and dot whiteness in the Scintillating Grid. That is, a linear increase in dot whiteness leads to a non-linear increase and then, remarkably, a decrease (non-monotonicity) in representational dissimilarity. In control images, mostly monotonic relations between representational dissimilarity and dot whiteness were observed. Furthermore, the dot whiteness level corresponding to the maximal representational dissimilarity (i.e. onset of non-monotonic dissimilarity) matched closely with that corresponding to the onset of illusion perception in human observers. As such, the non-monotonic response in the DNN is a potential model correlate for human illusion perception.

</details>

<details>

<summary>2019-08-05 05:05:29 - Automated Detection System for Adversarial Examples with High-Frequency Noises Sieve</summary>

- *Dang Duy Thang, Toshihiro Matsui*

- `1908.01469v1` - [abs](http://arxiv.org/abs/1908.01469v1) - [pdf](http://arxiv.org/pdf/1908.01469v1)

> Deep neural networks are being applied in many tasks with encouraging results, and have often reached human-level performance. However, deep neural networks are vulnerable to well-designed input samples called adversarial examples. In particular, neural networks tend to misclassify adversarial examples that are imperceptible to humans. This paper introduces a new detection system that automatically detects adversarial examples on deep neural networks. Our proposed system can mostly distinguish adversarial samples and benign images in an end-to-end manner without human intervention. We exploit the important role of the frequency domain in adversarial samples and propose a method that detects malicious samples in observations. When evaluated on two standard benchmark datasets (MNIST and ImageNet), our method achieved an out-detection rate of 99.7 - 100% in many settings.

</details>

<details>

<summary>2019-08-05 06:52:33 - The Impact of Developer Experience in Using Java Cryptography</summary>

- *Mohammadreza Hazhirpasand, Mohammad Ghafari, Stefan Krüger, Eric Bodden, Oscar Nierstrasz*

- `1908.01489v1` - [abs](http://arxiv.org/abs/1908.01489v1) - [pdf](http://arxiv.org/pdf/1908.01489v1)

> Previous research has shown that crypto APIs are hard for developers to understand and difficult for them to use. They consequently rely on unvalidated boilerplate code from online resources where security vulnerabilities are common.   We analyzed 2,324 open-source Java projects that rely on Java Cryptography Architecture (JCA) to understand how crypto APIs are used in practice, and what factors account for the performance of developers in using these APIs. We found that, in general, the experience of developers in using JCA does not correlate with their performance. In particular, none of the factors such as the number or frequency of committed lines of code, the number of JCA APIs developers use, or the number of projects they are involved in correlate with developer performance in this domain.   We call for qualitative studies to shed light on the reasons underlying the success of developers who are expert in using cryptography. Also, detailed investigation at API level is necessary to further clarify a developer obstacles in this domain.

</details>

<details>

<summary>2019-08-05 13:26:14 - Learning to Identify Security-Related Issues Using Convolutional Neural Networks</summary>

- *David N. Palacio, Daniel McCrystal, Kevin Moran, Carlos Bernal-Cárdenas, Denys Poshyvanyk, Chris Shenefiel*

- `1908.00614v2` - [abs](http://arxiv.org/abs/1908.00614v2) - [pdf](http://arxiv.org/pdf/1908.00614v2)

> Software security is becoming a high priority for both large companies and start-ups alike due to the increasing potential for harm that vulnerabilities and breaches carry with them. However, attaining robust security assurance while delivering features requires a precarious balancing act in the context of agile development practices. One path forward to help aid development teams in securing their software products is through the design and development of security-focused automation. Ergo, we present a novel approach, called SecureReqNet, for automatically identifying whether issues in software issue tracking systems describe security-related content. Our approach consists of a two-phase neural net architecture that operates purely on the natural language descriptions of issues. The first phase of our approach learns high dimensional word embeddings from hundreds of thousands of vulnerability descriptions listed in the CVE database and issue descriptions extracted from open source projects. The second phase then utilizes the semantic ontology represented by these embeddings to train a convolutional neural network capable of predicting whether a given issue is security-related. We evaluated SecureReqNet by applying it to identify security-related issues from a dataset of thousands of issues mined from popular projects on GitLab and GitHub. In addition, we also applied our approach to identify security-related requirements from a commercial software project developed by a major telecommunication company. Our preliminary results are encouraging, with SecureReqNet achieving an accuracy of 96% on open source issues and 71.6% on industrial requirements.

</details>

<details>

<summary>2019-08-05 17:34:38 - Are Free Android App Security Analysis Tools Effective in Detecting Known Vulnerabilities?</summary>

- *Venkatesh-Prasad Ranganath, Joydeep Mitra*

- `1806.09059v8` - [abs](http://arxiv.org/abs/1806.09059v8) - [pdf](http://arxiv.org/pdf/1806.09059v8)

> Increasing interest in securing the Android ecosystem has spawned numerous efforts to assist app developers in building secure apps. These efforts have resulted in tools and techniques capable of detecting vulnerabilities (and malicious behaviors) in apps. However, there has been no evaluation of the effectiveness of these tools and techniques in detecting known vulnerabilities. The absence of such evaluations puts app developers at a disadvantage when choosing security analysis tools to secure their apps. In this regard, we evaluated the effectiveness of vulnerability detection tools for Android apps. We reviewed 64 tools and empirically evaluated 14 vulnerability detection tools (incidentally along with five malicious behavior detection tools) against 42 known unique vulnerabilities captured by Ghera benchmarks, which are composed of both vulnerable and secure apps. Of the 24 observations from the evaluation, the main observation is existing vulnerability detection tools for Android apps are very limited in their ability to detect known vulnerabilities -- all of the evaluated tools together could only detect 30 of the 42 known unique vulnerabilities. More effort is required if security analysis tools are to help developers build secure apps. We hope the observations from this evaluation will help app developers choose appropriate security analysis tools and persuade tool developers and researchers to identify and address limitations in their tools and techniques. We also hope this evaluation will catalyze or spark a conversation in the software engineering and security communities to require a more rigorous and explicit evaluation of security analysis tools and techniques.

</details>

<details>

<summary>2019-08-06 02:01:40 - AppMine: Behavioral Analytics for Web Application Vulnerability Detection</summary>

- *Indranil Jana, Alina Oprea*

- `1908.01928v1` - [abs](http://arxiv.org/abs/1908.01928v1) - [pdf](http://arxiv.org/pdf/1908.01928v1)

> Web applications in widespread use have always been the target of large-scale attacks, leading to massive disruption of services and financial loss, as in the Equifax data breach. It has become common practice to deploy web application in containers like Docker for better portability and ease of deployment. We design a system called AppMine for lightweight monitoring of web applications running in Docker containers and detection of unknown web vulnerabilities. AppMine is an unsupervised learning system, trained only on legitimate workloads of web application, to detect anomalies based on either traditional models (PCA and one-class SVM), or more advanced neural-network architectures (LSTM). In our evaluation, we demonstrate that the neural network model outperforms more traditional methods on a range of web applications and recreated exploits. For instance, AppMine achieves average AUC scores as high as 0.97 for the Apache Struts application (with the CVE-2017-5638 exploit used in the Equifax breach), while the AUC scores for PCA and one-class SVM are 0.81 and 0.83, respectively.

</details>

<details>

<summary>2019-08-06 03:48:29 - Random Directional Attack for Fooling Deep Neural Networks</summary>

- *Wenjian Luo, Chenwang Wu, Nan Zhou, Li Ni*

- `1908.02658v1` - [abs](http://arxiv.org/abs/1908.02658v1) - [pdf](http://arxiv.org/pdf/1908.02658v1)

> Deep neural networks (DNNs) have been widely used in many fields such as images processing, speech recognition; however, they are vulnerable to adversarial examples, and this is a security issue worthy of attention. Because the training process of DNNs converge the loss by updating the weights along the gradient descent direction, many gradient-based methods attempt to destroy the DNN model by adding perturbations in the gradient direction. Unfortunately, as the model is nonlinear in most cases, the addition of perturbations in the gradient direction does not necessarily increase loss. Thus, we propose a random directed attack (RDA) for generating adversarial examples in this paper. Rather than limiting the gradient direction to generate an attack, RDA searches the attack direction based on hill climbing and uses multiple strategies to avoid local optima that cause attack failure. Compared with state-of-the-art gradient-based methods, the attack performance of RDA is very competitive. Moreover, RDA can attack without any internal knowledge of the model, and its performance under black-box attack is similar to that of the white-box attack in most cases, which is difficult to achieve using existing gradient-based attack methods.

</details>

<details>

<summary>2019-08-06 15:06:21 - MetaAdvDet: Towards Robust Detection of Evolving Adversarial Attacks</summary>

- *Chen Ma, Chenxu Zhao, Hailin Shi, Li Chen, Junhai Yong, Dan Zeng*

- `1908.02199v1` - [abs](http://arxiv.org/abs/1908.02199v1) - [pdf](http://arxiv.org/pdf/1908.02199v1)

> Deep neural networks (DNNs) are vulnerable to adversarial attack which is maliciously implemented by adding human-imperceptible perturbation to images and thus leads to incorrect prediction. Existing studies have proposed various methods to detect the new adversarial attacks. However, new attack methods keep evolving constantly and yield new adversarial examples to bypass the existing detectors. It needs to collect tens of thousands samples to train detectors, while the new attacks evolve much more frequently than the high-cost data collection. Thus, this situation leads the newly evolved attack samples to remain in small scales. To solve such few-shot problem with the evolving attack, we propose a meta-learning based robust detection method to detect new adversarial attacks with limited examples. Specifically, the learning consists of a double-network framework: a task-dedicated network and a master network which alternatively learn the detection capability for either seen attack or a new attack. To validate the effectiveness of our approach, we construct the benchmarks with few-shot-fashion protocols based on three conventional datasets, i.e. CIFAR-10, MNIST and Fashion-MNIST. Comprehensive experiments are conducted on them to verify the superiority of our approach with respect to the traditional adversarial attack detection methods.

</details>

<details>

<summary>2019-08-06 21:11:01 - Adversarial Explanations for Understanding Image Classification Decisions and Improved Neural Network Robustness</summary>

- *Walt Woods, Jack Chen, Christof Teuscher*

- `1906.02896v2` - [abs](http://arxiv.org/abs/1906.02896v2) - [pdf](http://arxiv.org/pdf/1906.02896v2)

> For sensitive problems, such as medical imaging or fraud detection, Neural Network (NN) adoption has been slow due to concerns about their reliability, leading to a number of algorithms for explaining their decisions. NNs have also been found vulnerable to a class of imperceptible attacks, called adversarial examples, which arbitrarily alter the output of the network. Here we demonstrate both that these attacks can invalidate prior attempts to explain the decisions of NNs, and that with very robust networks, the attacks themselves may be leveraged as explanations with greater fidelity to the model. We show that the introduction of a novel regularization technique inspired by the Lipschitz constraint, alongside other proposed improvements, greatly improves an NN's resistance to adversarial examples. On the ImageNet classification task, we demonstrate a network with an Accuracy-Robustness Area (ARA) of 0.0053, an ARA 2.4x greater than the previous state of the art. Improving the mechanisms by which NN decisions are understood is an important direction for both establishing trust in sensitive domains and learning more about the stimuli to which NNs respond.

</details>

<details>

<summary>2019-08-07 09:42:09 - Running on Fumes--Preventing Out-of-Gas Vulnerabilities in Ethereum Smart Contracts using Static Resource Analysis</summary>

- *Elvira Albert, Pablo Gordillo, Albert Rubio, Ilya Sergey*

- `1811.10403v2` - [abs](http://arxiv.org/abs/1811.10403v2) - [pdf](http://arxiv.org/pdf/1811.10403v2)

> Gas is a measurement unit of the computational effort that it will take to execute every single operation that takes part in the Ethereum blockchain platform. Each instruction executed by the Ethereum Virtual Machine (EVM) has an associated gas consumption specified by Ethereum. If a transaction exceeds the amount of gas allotted by the user (known as gas limit), an out-of-gas exception is raised. There is a wide family of contract vulnerabilities due to out-of-gas behaviours. We report on the design and implementation of GASTAP, a Gas-Aware Smart contracT Analysis Platform, which takes as input a smart contract (either in EVM, disassembled EVM, or in Solidity source code) and automatically infers sound gas upper bounds for all its public functions. Our bounds ensure that if the gas limit paid by the user is higher than our inferred gas bounds, the contract is free of out-of-gas vulnerabilities.

</details>

<details>

<summary>2019-08-07 10:49:05 - Cross-Router Covert Channels</summary>

- *Adar Ovadya, Rom Ogen, Yakov Mallah, Niv Gilboa, Yossi Oren*

- `1908.02524v1` - [abs](http://arxiv.org/abs/1908.02524v1) - [pdf](http://arxiv.org/pdf/1908.02524v1)

> Many organizations protect secure networked devices from non-secure networked devices by assigning each class of devices to a different logical network. These two logical networks, commonly called the host network and the guest network, use the same router hardware, which is designed to isolate the two networks in software.   In this work we show that logical network isolation based on host and guest networks can be overcome by the use of cross-router covert channels. Using specially-crafted network traffic, these channels make it possible to leak data between the host network and the guest network, and vice versa, through the use of the router as a shared medium. We performed a survey of routers representing multiple vendors and price points, and discovered that all of the routers we surveyed are vulnerable to at least one class of covert channel. Our attack can succeed even if the attacker has very limited permissions on the infected device, and even an iframe hosting malicious JavaScript code can be used for this purpose. We provide several metrics for the effectiveness of such channels, based on their pervasiveness, rate and covertness, and discuss possible ways of identifying and preventing these leakages.

</details>

<details>

<summary>2019-08-07 19:09:22 - Investigating Decision Boundaries of Trained Neural Networks</summary>

- *Roozbeh Yousefzadeh, Dianne P O'Leary*

- `1908.02802v1` - [abs](http://arxiv.org/abs/1908.02802v1) - [pdf](http://arxiv.org/pdf/1908.02802v1)

> Deep learning models have been the subject of study from various perspectives, for example, their training process, interpretation, generalization error, robustness to adversarial attacks, etc. A trained model is defined by its decision boundaries, and therefore, many of the studies about deep learning models speculate about the decision boundaries, and sometimes make simplifying assumptions about them. So far, finding exact points on the decision boundaries of trained deep models has been considered an intractable problem. Here, we compute exact points on the decision boundaries of these models and provide mathematical tools to investigate the surfaces that define the decision boundaries. Through numerical results, we confirm that some of the speculations about the decision boundaries are accurate, some of the computational methods can be improved, and some of the simplifying assumptions may be unreliable, for models with nonlinear activation functions. We advocate for verification of simplifying assumptions and approximation methods, wherever they are used. Finally, we demonstrate that the computational practices used for finding adversarial examples can be improved and computing the closest point on the decision boundary reveals the weakest vulnerability of a model against adversarial attack.

</details>

<details>

<summary>2019-08-08 06:33:32 - ConsiDroid: A Concolic-based Tool for Detecting SQL Injection Vulnerability in Android Apps</summary>

- *Ehsan Edalat, Babak Sadeghiyan, Fatemeh Ghassemi*

- `1811.10448v3` - [abs](http://arxiv.org/abs/1811.10448v3) - [pdf](http://arxiv.org/pdf/1811.10448v3)

> In this paper, we present a concolic execution technique for detecting SQL injection vulnerabilities in Android apps, with a new tool we called ConsiDroid. We extend the source code of apps with mocking technique, such that the execution of original source code is not affected. The extended source code can be treated as Java applications and may be executed by SPF with concolic execution. We automatically produce a DummyMain class out of static analysis such that the essential functions are called sequentially and, the events leading to vulnerable functions are triggered. We extend SPF with taint analysis in ConsiDroid. For making taint analysis possible, we introduce a new technique of symbolic mock classes in order to ease the propagation of tainted values in the code. An SQL injection vulnerability is detected through receiving a tainted value by a vulnerable function. Besides, ConsiDroid takes advantage of static analysis to adjust SPF in order to inspect only suspicious paths. To illustrate the applicability of ConsiDroid, we have inspected randomly selected 140 apps from F-Droid repository. From these apps, we found three apps vulnerable to SQL injection. To verify their vulnerability, we analyzed the apps manually based on ConsiDroid's reports by using Robolectric.

</details>

<details>

<summary>2019-08-12 18:17:16 - Efficient Passive ICS Device Discovery and Identification by MAC Address Correlation</summary>

- *Matthias Niedermaier, Thomas Hanka, Sven Plaga, Alexander von Bodisco, Dominik Merli*

- `1904.04271v2` - [abs](http://arxiv.org/abs/1904.04271v2) - [pdf](http://arxiv.org/pdf/1904.04271v2)

> Owing to a growing number of attacks, the assessment of Industrial Control Systems (ICSs) has gained in importance. An integral part of an assessment is the creation of a detailed inventory of all connected devices, enabling vulnerability evaluations. For this purpose, scans of networks are crucial. Active scanning, which generates irregular traffic, is a method to get an overview of connected and active devices. Since such additional traffic may lead to an unexpected behavior of devices, active scanning methods should be avoided in critical infrastructure networks. In such cases, passive network monitoring offers an alternative, which is often used in conjunction with complex deep-packet inspection techniques. There are very few publications on lightweight passive scanning methodologies for industrial networks. In this paper, we propose a lightweight passive network monitoring technique using an efficient Media Access Control (MAC) address-based identification of industrial devices. Based on an incomplete set of known MAC address to device associations, the presented method can guess correct device and vendor information. Proving the feasibility of the method, an implementation is also introduced and evaluated regarding its efficiency. The feasibility of predicting a specific device/vendor combination is demonstrated by having similar devices in the database. In our ICS testbed, we reached a host discovery rate of 100% at an identification rate of more than 66%, outperforming the results of existing tools.

</details>

<details>

<summary>2019-08-13 06:05:22 - A Simple and Intuitive Algorithm for Preventing Directory Traversal Attacks</summary>

- *Michael Flanders*

- `1908.04502v1` - [abs](http://arxiv.org/abs/1908.04502v1) - [pdf](http://arxiv.org/pdf/1908.04502v1)

> With web applications becoming a preferred method of presenting graphical user interfaces to users, software vulnerabilities affecting web applications are becoming more and more prevalent and devastating. Some of these vulnerabilities, such as directory traversal attacks, have varying defense mechanisms and mitigations that can be difficult to understand, analyze, and test. Gaps in the testing of these directory traversal defense mechanisms can lead to vulnerabilities that allow attackers to read sensitive data from files or even execute malicious code. This paper presents an analysis of some currently used directory traversal attack defenses and presents a new, stack-based algorithm to help prevent these attacks by safely canonicalizing user-supplied path strings. The goal of this algorithm is to be small, easy to test, cross-platform compatible, and above all, intuitive. We provide a proof of correctness and verification strategies using symbolic execution for the algorithm. We hope that the algorithm is simple and effective enough to help move developers towards a unified defense against directory traversal attacks.

</details>

<details>

<summary>2019-08-13 06:15:41 - A Survey on Ethereum Systems Security: Vulnerabilities, Attacks and Defenses</summary>

- *Huashan Chen, Marcus Pendleton, Laurent Njilla, Shouhuai Xu*

- `1908.04507v1` - [abs](http://arxiv.org/abs/1908.04507v1) - [pdf](http://arxiv.org/pdf/1908.04507v1)

> The blockchain technology is believed by many to be a game changer in many application domains, especially financial applications. While the first generation of blockchain technology (i.e., Blockchain 1.0) is almost exclusively used for cryptocurrency purposes, the second generation (i.e., Blockchain 2.0), as represented by Ethereum, is an open and decentralized platform enabling a new paradigm of computing --- Decentralized Applications (DApps) running on top of blockchains. The rich applications and semantics of DApps inevitably introduce many security vulnerabilities, which have no counterparts in pure cryptocurrency systems like Bitcoin. Since Ethereum is a new, yet complex, system, it is imperative to have a systematic and comprehensive understanding on its security from a holistic perspective, which is unavailable. To the best of our knowledge, the present survey, which can also be used as a tutorial, fills this void. In particular, we systematize three aspects of Ethereum systems security: vulnerabilities, attacks, and defenses. We draw insights into, among other things, vulnerability root causes, attack consequences, and defense capabilities, which shed light on future research directions.

</details>

<details>

<summary>2019-08-13 09:01:07 - Identifying and characterizing ZMap scans: a cryptanalytic approach</summary>

- *Johan Mazel, Rémi Strullu*

- `1908.04193v2` - [abs](http://arxiv.org/abs/1908.04193v2) - [pdf](http://arxiv.org/pdf/1908.04193v2)

> Network scanning tools play a major role in Internet security. They are used by both network security researchers and malicious actors to identify vulnerable machines exposed on the Internet. ZMap is one of the most common probing tools for high-speed Internet-wide scanning. We present novel identification methods based on the IPv4 iteration process of ZMap. These methods can be used to identify ZMap scans with a small number of addresses extracted from the scan. We conduct an experimental evaluation of these detection methods on synthetic, network telescope, and backbone traffic. We manage to identify 28.5% of the ZMap scans in real-world traffic. We then perform an in-depth characterization of these scans regarding, for example, targeted prefix and probing speed.

</details>

<details>

<summary>2019-08-13 11:58:27 - Defending Against Universal Perturbations With Shared Adversarial Training</summary>

- *Chaithanya Kumar Mummadi, Thomas Brox, Jan Hendrik Metzen*

- `1812.03705v2` - [abs](http://arxiv.org/abs/1812.03705v2) - [pdf](http://arxiv.org/pdf/1812.03705v2)

> Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of image classifiers against such adversarial perturbations, it leaves them sensitive to perturbations on a non-negligible fraction of the inputs. In this work, we show that adversarial training is more effective in preventing universal perturbations, where the same perturbation needs to fool a classifier on many inputs. Moreover, we investigate the trade-off between robustness against universal perturbations and performance on unperturbed data and propose an extension of adversarial training that handles this trade-off more gracefully. We present results for image classification and semantic segmentation to showcase that universal perturbations that fool a model hardened with adversarial training become clearly perceptible and show patterns of the target scene.

</details>

<details>

<summary>2019-08-13 20:59:33 - Exploit Prediction Scoring System (EPSS)</summary>

- *Jay Jacobs, Sasha Romanosky, Benjamin Edwards, Michael Roytman, Idris Adjerid*

- `1908.04856v1` - [abs](http://arxiv.org/abs/1908.04856v1) - [pdf](http://arxiv.org/pdf/1908.04856v1)

> Despite the massive investments in information security technologies and research over the past decades, the information security industry is still immature. In particular, the prioritization of remediation efforts within vulnerability management programs predominantly relies on a mixture of subjective expert opinion, severity scores, and incomplete data. Compounding the need for prioritization is the increase in the number of vulnerabilities the average enterprise has to remediate. This paper produces the first open, data-driven framework for assessing vulnerability threat, that is, the probability that a vulnerability will be exploited in the wild within the first twelve months after public disclosure. This scoring system has been designed to be simple enough to be implemented by practitioners without specialized tools or software, yet provides accurate estimates of exploitation. Moreover, the implementation is flexible enough that it can be updated as more, and better, data becomes available. We call this system the Exploit Prediction Scoring System, EPSS.

</details>

<details>

<summary>2019-08-14 07:49:24 - Stop the Open Data Bus, We Want to Get Off</summary>

- *Chris Culnane, A/Benjamin I. P. Rubinstein, A/Vanessa Teague*

- `1908.05004v1` - [abs](http://arxiv.org/abs/1908.05004v1) - [pdf](http://arxiv.org/pdf/1908.05004v1)

> The subject of this report is the re-identification of individuals in the Myki public transport dataset released as part of the Melbourne Datathon 2018. We demonstrate the ease with which we were able to re-identify ourselves, our co-travellers, and complete strangers; our analysis raises concerns about the nature and granularity of the data released, in particular the ability to identify vulnerable or sensitive groups.

</details>

<details>

<summary>2019-08-14 08:18:09 - Side-Channel Aware Fuzzing</summary>

- *Philip Sperl, Konstantin Böttinger*

- `1908.05012v1` - [abs](http://arxiv.org/abs/1908.05012v1) - [pdf](http://arxiv.org/pdf/1908.05012v1)

> Software testing is becoming a critical part of the development cycle of embedded devices, enabling vulnerability detection. A well-studied approach of software testing is fuzz-testing (fuzzing), during which mutated input is sent to an input-processing software while its behavior is monitored. The goal is to identify faulty states in the program, triggered by malformed inputs. Even though this technique is widely performed, fuzzing cannot be applied to embedded devices to its full extent. Due to the lack of adequately powerful I/O capabilities or an operating system the feedback needed for fuzzing cannot be acquired.   In this paper we present and evaluate a new approach to extract feedback for fuzzing on embedded devices using information the power consumption leaks. Side-channel aware fuzzing is a threefold process that is initiated by sending an input to a target device and measuring its power consumption. First, we extract features from the power traces of the target device using machine learning algorithms. Subsequently, we use the features to reconstruct the code structure of the analyzed firmware. In the final step we calculate a score for the input, which is proportional to the code coverage.   We carry out our proof of concept by fuzzing synthetic software and a light-weight AES implementation running on an ARM Cortex-M4 microcontroller. Our results show that the power side-channel carries information relevant for fuzzing.

</details>

<details>

<summary>2019-08-14 15:59:21 - Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once</summary>

- *Jiangfan Han, Xiaoyi Dong, Ruimao Zhang, Dongdong Chen, Weiming Zhang, Nenghai Yu, Ping Luo, Xiaogang Wang*

- `1908.05185v1` - [abs](http://arxiv.org/abs/1908.05185v1) - [pdf](http://arxiv.org/pdf/1908.05185v1)

> Modern deep neural networks are often vulnerable to adversarial samples. Based on the first optimization-based attacking method, many following methods are proposed to improve the attacking performance and speed. Recently, generation-based methods have received much attention since they directly use feed-forward networks to generate the adversarial samples, which avoid the time-consuming iterative attacking procedure in optimization-based and gradient-based methods. However, current generation-based methods are only able to attack one specific target (category) within one model, thus making them not applicable to real classification systems that often have hundreds/thousands of categories. In this paper, we propose the first Multi-target Adversarial Network (MAN), which can generate multi-target adversarial samples with a single model. By incorporating the specified category information into the intermediate features, it can attack any category of the target classification model during runtime. Experiments show that the proposed MAN can produce stronger attack results and also have better transferability than previous state-of-the-art methods in both multi-target attack task and single-target attack task. We further use the adversarial samples generated by our MAN to improve the robustness of the classification model. It can also achieve better classification accuracy than other methods when attacked by various methods.

</details>

<details>

<summary>2019-08-14 18:53:08 - Network Reconnaissance and Vulnerability Excavation of Secure DDS Systems</summary>

- *Ruffin White, Gianluca Caiazza, Chenxu Jiang, Xinyue Ou, Zhiyue Yang, Agostino Cortesi, Henrik Christensen*

- `1908.05310v1` - [abs](http://arxiv.org/abs/1908.05310v1) - [pdf](http://arxiv.org/pdf/1908.05310v1)

> Distribution Service (DDS) is a realtime peer-to-peer protocol that serves as a scalable middleware between distributed networked systems found in many Industrial IoT domains such as automotive, medical, energy, and defense. Since the initial ratification of the standard, specifications have introduced a Security Model and Service Plugin Interface (SPI) architecture, facilitating authenticated encryption and data centric access control while preserving interoperable data exchange. However, as Secure DDS v1.1, the default plugin specifications presently exchanges digitally signed capability lists of both participants in the clear during the crypto handshake for permission attestation; thus breaching confidentiality of the context of the connection. In this work, we present an attacker model that makes use of network reconnaissance afforded by this leaked context in conjunction with formal verification and model checking to arbitrarily reason about the underlying topology and reachability of information flow, enabling targeted attacks such as selective denial of service, adversarial partitioning of the data bus, or vulnerability excavation of vendor implementations.

</details>

<details>

<summary>2019-08-14 22:19:10 - A Hybrid Approach to Privacy-Preserving Federated Learning</summary>

- *Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, Yi Zhou*

- `1812.03224v2` - [abs](http://arxiv.org/abs/1812.03224v2) - [pdf](http://arxiv.org/pdf/1812.03224v2)

> Federated learning facilitates the collaborative training of models without the sharing of raw data. However, recent attacks demonstrate that simply maintaining data locality during training processes does not provide sufficient privacy guarantees. Rather, we need a federated learning system capable of preventing inference over both the messages exchanged during training and the final trained model while ensuring the resulting model also has acceptable predictive accuracy. Existing federated learning approaches either use secure multiparty computation (SMC) which is vulnerable to inference or differential privacy which can lead to low accuracy given a large number of parties with relatively small amounts of data each. In this paper, we present an alternative approach that utilizes both differential privacy and SMC to balance these trade-offs. Combining differential privacy with secure multiparty computation enables us to reduce the growth of noise injection as the number of parties increases without sacrificing privacy while maintaining a pre-defined rate of trust. Our system is therefore a scalable approach that protects against inference threats and produces models with high accuracy. Additionally, our system can be used to train a variety of machine learning models, which we validate with experimental results on 3 different machine learning algorithms. Our experiments demonstrate that our approach out-performs state of the art solutions.

</details>

<details>

<summary>2019-08-15 08:49:06 - Towards usable automated detection of CPU architecture and endianness for arbitrary binary files and object code sequences</summary>

- *Sami Kairajärvi, Andrei Costin, Timo Hämäläinen*

- `1908.05459v1` - [abs](http://arxiv.org/abs/1908.05459v1) - [pdf](http://arxiv.org/pdf/1908.05459v1)

> Static and dynamic binary analysis techniques are actively used to reverse engineer software's behavior and to detect its vulnerabilities, even when only the binary code is available for analysis. To avoid analysis errors due to misreading op-codes for a wrong CPU architecture, these analysis tools must precisely identify the Instruction Set Architecture (ISA) of the object code under analysis. The variety of CPU architectures that modern security and reverse engineering tools must support is ever increasing due to massive proliferation of IoT devices and the diversity of firmware and malware targeting those devices. Recent studies concluded that falsely identifying the binary code's ISA caused alone about 10\% of failures of IoT firmware analysis. The state of the art approaches to detect ISA for arbitrary object code look promising - their results demonstrate effectiveness and high-performance. However, they lack the support of publicly available datasets and toolsets, which makes the evaluation, comparison, and improvement of those techniques, datasets, and machine learning models quite challenging (if not impossible). This paper bridges multiple gaps in the field of automated and precise identification of architecture and endianness of binary files and object code. We develop from scratch the toolset and datasets that are lacking in this research space. As such, we contribute a comprehensive collection of open data, open source, and open API web-services. We also attempt experiment reconstruction and cross-validation of effectiveness, efficiency, and results of the state of the art methods. When training and testing classifiers using solely code-sections from executable binary files, all our classifiers performed equally well achieving over 98\% accuracy. The results are consistent and comparable with the current state of the art, hence supports the general validity of the algorithms

</details>

<details>

<summary>2019-08-15 09:34:43 - SIF: A Framework for Solidity Code Instrumentation and Analysis</summary>

- *Chao Peng, Sefa Akca, Ajitha Rajan*

- `1905.01659v2` - [abs](http://arxiv.org/abs/1905.01659v2) - [pdf](http://arxiv.org/pdf/1905.01659v2)

> Solidity is an object-oriented and high-level language for writing smart contracts that are used to execute, verify and enforce credible transactions on permissionless blockchains. In the last few years, analysis of smart contracts has raised considerable interest and numerous techniques have been proposed to check the presence of vulnerabilities in them. Current techniques lack traceability in source code and have widely differing work flows. There is no single unifying framework for analysis, instrumentation, optimisation and code generation of Solidity contracts.   In this paper, we present SIF, a comprehensive framework for Solidity contract analysis, query, instrumentation, and code generation. SIF provides support for Solidity contract developers and testers to build source level techniques for analysis, understanding, diagnostics, optimisations and code generation. We show feasibility and applicability of the framework by building practical tools on top of it and running them on 1838 real smart contracts deployed on the Ethereum network.

</details>

<details>

<summary>2019-08-15 19:56:17 - Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers</summary>

- *Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, Chinmay Hegde*

- `1904.08489v2` - [abs](http://arxiv.org/abs/1904.08489v2) - [pdf](http://arxiv.org/pdf/1904.08489v2)

> Deep neural networks have been shown to exhibit an intriguing vulnerability to adversarial input images corrupted with imperceptible perturbations. However, the majority of adversarial attacks assume global, fine-grained control over the image pixel space. In this paper, we consider a different setting: what happens if the adversary could only alter specific attributes of the input image? These would generate inputs that might be perceptibly different, but still natural-looking and enough to fool a classifier. We propose a novel approach to generate such `semantic' adversarial examples by optimizing a particular adversarial loss over the range-space of a parametric conditional generative model. We demonstrate implementations of our attacks on binary classifiers trained on face images, and show that such natural-looking semantic adversarial examples exist. We evaluate the effectiveness of our attack on synthetic and real data, and present detailed comparisons with existing attack methods. We supplement our empirical results with theoretical bounds that demonstrate the existence of such parametric adversarial examples.

</details>

<details>

<summary>2019-08-16 09:14:32 - Evaluating User Perception of Multi-Factor Authentication: A Systematic Review</summary>

- *Sanchari Das, Bingxing Wang, Zachary Tingle, L. Jean Camp*

- `1908.05901v1` - [abs](http://arxiv.org/abs/1908.05901v1) - [pdf](http://arxiv.org/pdf/1908.05901v1)

> Security vulnerabilities of traditional single factor authentication has become a major concern for security practitioners and researchers. To mitigate single point failures, new and technologically advanced Multi-Factor Authentication (MFA) tools have been developed as security solutions. However, the usability and adoption of such tools have raised concerns. An obvious solution can be viewed as conducting user studies to create more user-friendly MFA tools. To learn more, we performed a systematic literature review of recently published academic papers (N = 623) that primarily focused on MFA technologies. While majority of these papers (m = 300) proposed new MFA tools, only 9.1% of papers performed any user evaluation research. Our meta-analysis of user focused studies (n = 57) showed that researchers found lower adoption rate to be inevitable for MFAs, while avoidance was pervasive among mandatory use. Furthermore, we noted several reporting and methodological discrepancies in the user focused studies. We identified trends in participant recruitment that is indicative of demographic biases.

</details>

<details>

<summary>2019-08-16 09:15:56 - MFA is a Waste of Time! Understanding Negative Connotation Towards MFA Applications via User Generated Content</summary>

- *Sanchari Das, Bingxing Wang, L. Jean Camp*

- `1908.05902v1` - [abs](http://arxiv.org/abs/1908.05902v1) - [pdf](http://arxiv.org/pdf/1908.05902v1)

> Traditional single-factor authentication possesses several critical security vulnerabilities due to single-point failure feature. Multi-factor authentication (MFA), intends to enhance security by providing additional verification steps. However, in practical deployment, users often experience dissatisfaction while using MFA, which leads to non-adoption. In order to understand the current design and usability issues with MFA, we analyze aggregated user generated comments (N = 12,500) about application-based MFA tools from major distributors, such as, Amazon, Google Play, Apple App Store, and others. While some users acknowledge the security benefits of MFA, majority of them still faced problems with initial configuration, system design understanding, limited device compatibility, and risk trade-offs leading to non-adoption of MFA. Based on these results, we provide actionable recommendations in technological design, initial training, and risk communication to improve the adoption and user experience of MFA.

</details>

<details>

<summary>2019-08-16 18:32:49 - Deceptive Reinforcement Learning Under Adversarial Manipulations on Cost Signals</summary>

- *Yunhan Huang, Quanyan Zhu*

- `1906.10571v3` - [abs](http://arxiv.org/abs/1906.10571v3) - [pdf](http://arxiv.org/pdf/1906.10571v3)

> This paper studies reinforcement learning (RL) under malicious falsification on cost signals and introduces a quantitative framework of attack models to understand the vulnerabilities of RL. Focusing on $Q$-learning, we show that $Q$-learning algorithms converge under stealthy attacks and bounded falsifications on cost signals. We characterize the relation between the falsified cost and the $Q$-factors as well as the policy learned by the learning agent which provides fundamental limits for feasible offensive and defensive moves. We propose a robust region in terms of the cost within which the adversary can never achieve the targeted policy. We provide conditions on the falsified cost which can mislead the agent to learn an adversary's favored policy. A numerical case study of water reservoir control is provided to show the potential hazards of RL in learning-based control systems and corroborate the results.

</details>

<details>

<summary>2019-08-18 12:12:45 - A Novel Kalman Filter Based Shilling Attack Detection Algorithm</summary>

- *Xin Liu, Yingyuan Xiao, Xu Jiao, Wenguang Zheng, Zihao Ling*

- `1908.06968v1` - [abs](http://arxiv.org/abs/1908.06968v1) - [pdf](http://arxiv.org/pdf/1908.06968v1)

> Collaborative filtering has been widely used in recommendation systems to recommend items that users might like. However, collaborative filtering based recommendation systems are vulnerable to shilling attacks. Malicious users tend to increase or decrease the recommended frequency of target items by injecting fake profiles. In this paper, we propose a Kalman filter-based attack detection model, which statistically analyzes the difference between the actual rating and the predicted rating calculated by this model to find the potential abnormal time period. The Kalman filter filters out suspicious ratings based on the abnormal time period and identifies suspicious users based on the source of these ratings. The experimental results show that our method performs much better detection performance for the shilling attack than the traditional methods.

</details>

<details>

<summary>2019-08-18 15:41:36 - Agent-based (BDI) modeling for automation of penetration testing</summary>

- *Ge Chu, Alexei Lisitsa*

- `1908.06970v1` - [abs](http://arxiv.org/abs/1908.06970v1) - [pdf](http://arxiv.org/pdf/1908.06970v1)

> Penetration testing (or pentesting) is one of the widely used and important methodologies to assess the security of computer systems and networks. Traditional pentesting relies on the domain expert knowledge and requires considerable human effort all of which incurs a high cost. The automation can significantly improve the efficiency, availability and lower the cost of penetration testing. Existing approaches to the automation include those which map vulnerability scanner results to the corresponding exploit tools, and those addressing the pentesting as a planning problem expressed in terms of attack graphs. Due to mainly non-interactive processing, such solutions can deal effectively only with static and simple targets. In this paper, we propose an automated penetration testing approach based on the belief-desire-intention (BDI) agent model, which is central in the research on agent-based processing in that it deals interactively with dynamic, uncertain and complex environments. Penetration testing actions are defined as a series of BDI plans and the BDI reasoning cycle is used to represent the penetration testing process. The model is extensible and new plans can be added, once they have been elicited from the human experts. We report on the results of testing of proof of concept BDI-based penetration testing tool in the simulated environment.

</details>

<details>

<summary>2019-08-20 00:29:23 - Protecting Neural Networks with Hierarchical Random Switching: Towards Better Robustness-Accuracy Trade-off for Stochastic Defenses</summary>

- *Xiao Wang, Siyue Wang, Pin-Yu Chen, Yanzhi Wang, Brian Kulis, Xue Lin, Peter Chin*

- `1908.07116v1` - [abs](http://arxiv.org/abs/1908.07116v1) - [pdf](http://arxiv.org/pdf/1908.07116v1)

> Despite achieving remarkable success in various domains, recent studies have uncovered the vulnerability of deep neural networks to adversarial perturbations, creating concerns on model generalizability and new threats such as prediction-evasive misclassification or stealthy reprogramming. Among different defense proposals, stochastic network defenses such as random neuron activation pruning or random perturbation to layer inputs are shown to be promising for attack mitigation. However, one critical drawback of current defenses is that the robustness enhancement is at the cost of noticeable performance degradation on legitimate data, e.g., large drop in test accuracy. This paper is motivated by pursuing for a better trade-off between adversarial robustness and test accuracy for stochastic network defenses. We propose Defense Efficiency Score (DES), a comprehensive metric that measures the gain in unsuccessful attack attempts at the cost of drop in test accuracy of any defense. To achieve a better DES, we propose hierarchical random switching (HRS), which protects neural networks through a novel randomization scheme. A HRS-protected model contains several blocks of randomly switching channels to prevent adversaries from exploiting fixed model structures and parameters for their malicious purposes. Extensive experiments show that HRS is superior in defending against state-of-the-art white-box and adaptive adversarial misclassification attacks. We also demonstrate the effectiveness of HRS in defending adversarial reprogramming, which is the first defense against adversarial programs. Moreover, in most settings the average DES of HRS is at least 5X higher than current stochastic network defenses, validating its significantly improved robustness-accuracy trade-off.

</details>

<details>

<summary>2019-08-20 04:22:58 - MicroTEE: Designing TEE OS Based on the Microkernel Architecture</summary>

- *Dongxu Ji, Qianying Zhang, Shijun Zhao, Zhiping Shi, Yong Guan*

- `1908.07159v1` - [abs](http://arxiv.org/abs/1908.07159v1) - [pdf](http://arxiv.org/pdf/1908.07159v1)

> ARM TrustZone technology is widely used to provide Trusted Execution Environments (TEE) for mobile devices. However, most TEE OSes are implemented as monolithic kernels. In such designs, device drivers, kernel services and kernel modules all run in the kernel, which results in large size of the kernel. It is difficult to guarantee that all components of the kernel have no security vulnerabilities in the monolithic kernel architecture, such as the integer overflow vulnerability in Qualcomm QSEE TrustZone and the TZDriver vulnerability in HUAWEI Hisilicon TEE architecture. This paper presents MicroTEE, a TEE OS based on the microkernel architecture. In MicroTEE, the microkernel provides strong isolation for TEE OS's basic services, such as crypto service and platform key management service. The kernel is only responsible for providing core services such as address space management, thread management, and inter-process communication. Other fundamental services, such as crypto service and platform key management service are implemented as applications at the user layer. Crypto Services and Key Management are used to provide Trusted Applications (TAs) with sensitive information encryption, data signing, and platform attestation functions. Our design avoids the compromise of the whole TEE OS if only one kernel service is vulnerable. A monitor has also been added to perform the switch between the secure world and the normal world. Finally, we implemented a MicroTEE prototype on the Freescale i.MX6Q Sabre Lite development board and tested its performance. Evaluation results show that the performance of cryptographic operations in MicroTEE is better than it in Linux when the size of data is small.

</details>

<details>

<summary>2019-08-20 09:51:24 - Addressing Model Vulnerability to Distributional Shifts over Image Transformation Sets</summary>

- *Riccardo Volpi, Vittorio Murino*

- `1903.11900v2` - [abs](http://arxiv.org/abs/1903.11900v2) - [pdf](http://arxiv.org/pdf/1903.11900v2)

> We are concerned with the vulnerability of computer vision models to distributional shifts. We formulate a combinatorial optimization problem that allows evaluating the regions in the image space where a given model is more vulnerable, in terms of image transformations applied to the input, and face it with standard search algorithms. We further embed this idea in a training procedure, where we define new data augmentation rules according to the image transformations that the current model is most vulnerable to, over iterations. An empirical evaluation on classification and semantic segmentation problems suggests that the devised algorithm allows to train models that are more robust against content-preserving image manipulations and, in general, against distributional shifts.

</details>

<details>

<summary>2019-08-21 01:23:59 - Stronger and Faster Side-Channel Protections for CSIDH</summary>

- *Daniel Cervantes-Vázquez, Mathilde Chenu, Jesús-Javier Chi-Domínguez, Luca De Feo, Francisco Rodríguez-Henríquez, Benjamin Smith*

- `1907.08704v2` - [abs](http://arxiv.org/abs/1907.08704v2) - [pdf](http://arxiv.org/pdf/1907.08704v2)

> CSIDH is a recent quantum-resistant primitive based on the difficulty of finding isogeny paths between supersingular curves. Recently, two constant-time versions of CSIDH have been proposed: first by Meyer, Campos and Reith, and then by Onuki, Aikawa, Yamazaki and Takagi. While both offer protection against timing attacks and simple power consumption analysis, they are vulnerable to more powerful attacks such as fault injections. In this work, we identify and repair two oversights in these algorithms that compromised their constant-time character. By exploiting Edwards arithmetic and optimal addition chains, we produce the fastest constant-time version of CSIDH to date. We then consider the stronger attack scenario of fault injection, which is relevant for the security of CSIDH static keys in embedded hardware. We propose and evaluate a dummy-free CSIDH algorithm. While these CSIDH variants are slower, their performance is still within a small constant factor of less-protected variants. Finally, we discuss derandomized CSIDH algorithms.

</details>

<details>

<summary>2019-08-22 22:55:13 - SmartEmbed: A Tool for Clone and Bug Detection in Smart Contracts through Structural Code Embedding</summary>

- *Zhipeng Gao, Vinoj Jayasundara, Lingxiao Jiang, Xin Xia, David Lo, John Grundy*

- `1908.08615v1` - [abs](http://arxiv.org/abs/1908.08615v1) - [pdf](http://arxiv.org/pdf/1908.08615v1)

> Ethereum has become a widely used platform to enable secure, Blockchain-based financial and business transactions. However, a major concern in Ethereum is the security of its smart contracts. Many identified bugs and vulnerabilities in smart contracts not only present challenges to maintenance of blockchain, but also lead to serious financial loses. There is a significant need to better assist developers in checking smart contracts and ensuring their reliability.In this paper, we propose a web service tool, named SmartEmbed, which can help Solidity developers to find repetitive contract code and clone-related bugs in smart contracts. Our tool is based on code embeddings and similarity checking techniques. By comparing the similarities among the code embedding vectors for existing solidity code in the Ethereum blockchain and known bugs, we are able to efficiently identify code clones and clone-related bugs for any solidity code given by users, which can help to improve the users' confidence in the reliability of their code. In addition to the uses by individual developers, SmartEmbed can also be applied to studies of smart contracts in a large scale. When applied to more than 22K solidity contracts collected from the Ethereum blockchain, we found that the clone ratio of solidity code is close to 90\%, much higher than traditional software, and 194 clone-related bugs can be identified efficiently and accurately based on our small bug database with a precision of 96\%. SmartEmbed can be accessed at \url{http://www.smartembed.net}. A demo video of SmartEmbed is at \url{https://youtu.be/o9ylyOpYFq8}

</details>

<details>

<summary>2019-08-25 19:05:32 - Privacy Risks of Securing Machine Learning Models against Adversarial Examples</summary>

- *Liwei Song, Reza Shokri, Prateek Mittal*

- `1905.10291v3` - [abs](http://arxiv.org/abs/1905.10291v3) - [pdf](http://arxiv.org/pdf/1905.10291v3)

> The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain.   In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks.   To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks.

</details>

<details>

<summary>2019-08-26 10:27:39 - AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning</summary>

- *Florian Tramèr, Pascal Dupré, Gili Rusak, Giancarlo Pellegrino, Dan Boneh*

- `1811.03194v3` - [abs](http://arxiv.org/abs/1811.03194v3) - [pdf](http://arxiv.org/pdf/1811.03194v3)

> Perceptual ad-blocking is a novel approach that detects online advertisements based on their visual content. Compared to traditional filter lists, the use of perceptual signals is believed to be less prone to an arms race with web publishers and ad networks. We demonstrate that this may not be the case. We describe attacks on multiple perceptual ad-blocking techniques, and unveil a new arms race that likely disfavors ad-blockers. Unexpectedly, perceptual ad-blocking can also introduce new vulnerabilities that let an attacker bypass web security boundaries and mount DDoS attacks.   We first analyze the design space of perceptual ad-blockers and present a unified architecture that incorporates prior academic and commercial work. We then explore a variety of attacks on the ad-blocker's detection pipeline, that enable publishers or ad networks to evade or detect ad-blocking, and at times even abuse its high privilege level to bypass web security boundaries.   On one hand, we show that perceptual ad-blocking must visually classify rendered web content to escape an arms race centered on obfuscation of page markup. On the other, we present a concrete set of attacks on visual ad-blockers by constructing adversarial examples in a real web page context. For seven ad-detectors, we create perturbed ads, ad-disclosure logos, and native web content that misleads perceptual ad-blocking with 100% success rates. In one of our attacks, we demonstrate how a malicious user can upload adversarial content, such as a perturbed image in a Facebook post, that fools the ad-blocker into removing another users' non-ad content.   Moving beyond the Web and visual domain, we also build adversarial examples for AdblockRadio, an open source radio client that uses machine learning to detects ads in raw audio streams.

</details>

<details>

<summary>2019-08-26 19:02:39 - Slither: A Static Analysis Framework For Smart Contracts</summary>

- *Josselin Feist, Gustavo Grieco, Alex Groce*

- `1908.09878v1` - [abs](http://arxiv.org/abs/1908.09878v1) - [pdf](http://arxiv.org/pdf/1908.09878v1)

> This paper describes Slither, a static analysis framework designed to provide rich information about Ethereum smart contracts. It works by converting Solidity smart contracts into an intermediate representation called SlithIR. SlithIR uses Static Single Assignment (SSA) form and a reduced instruction set to ease implementation of analyses while preserving semantic information that would be lost in transforming Solidity to bytecode. Slither allows for the application of commonly used program analysis techniques like dataflow and taint tracking. Our framework has four main use cases: (1) automated detection of vulnerabilities, (2) automated detection of code optimization opportunities, (3) improvement of the user's understanding of the contracts, and (4) assistance with code review.   In this paper, we present an overview of Slither, detail the design of its intermediate representation, and evaluate its capabilities on real-world contracts. We show that Slither's bug detection is fast, accurate, and outperforms other static analysis tools at finding issues in Ethereum smart contracts in terms of speed, robustness, and balance of detection and false positives. We compared tools using a large dataset of smart contracts and manually reviewed results for 1000 of the most used contracts.

</details>

<details>

<summary>2019-08-27 11:36:14 - Eclipsing Ethereum Peers with False Friends</summary>

- *Sebastian Henningsen, Daniel Teunis, Martin Florian, Björn Scheuermann*

- `1908.10141v1` - [abs](http://arxiv.org/abs/1908.10141v1) - [pdf](http://arxiv.org/pdf/1908.10141v1)

> Ethereum is a decentralized Blockchain system that supports the execution of Turing-complete smart contracts. Although the security of the Ethereum ecosystem has been studied in the past, the network layer has been mostly neglected. We show that Go Ethereum (Geth), the most widely used Ethereum implementation, is vulnerable to eclipse attacks, effectively circumventing recently introduced (Geth v1.8.0) security enhancements. We responsibly disclosed the vulnerability to core Ethereum developers; the corresponding countermeasures to our attack where incorporated into the v1.9.0 release of Geth. Our false friends attack exploits the Kademlia-inspired peer discovery logic used by Geth and enables a low-resource eclipsing of long-running, remote victim nodes. An adversary only needs two hosts in distinct /24 subnets to launch the eclipse, which can then be leveraged to filter the victim's view of the Blockchain. We discuss fundamental properties of Geth's node discovery logic that enable the false friends attack, as well as proposed and implemented countermeasures.

</details>

<details>

<summary>2019-08-27 14:30:40 - Smart Street Lights and Mobile Citizen Apps for Resilient Communication in a Digital City</summary>

- *Lars Baumgärtner, Jonas Höchst, Patrick Lampe, Ragnar Mogk, Artur Sterz, Pascal Weisenburger, Mira Mezini, Bernd Freisleben*

- `1908.10233v1` - [abs](http://arxiv.org/abs/1908.10233v1) - [pdf](http://arxiv.org/pdf/1908.10233v1)

> Currently, nearly four billion people live in urban areas. Since this trend is increasing, natural disasters or terrorist attacks in such areas affect an increasing number of people. While information and communication technology is crucial for the operation of urban infrastructures and the well-being of its inhabitants, current technology is quite vulnerable to disruptions of various kinds. In future smart cities, a more resilient urban infrastructure is imperative to handle the increasing number of hazardous situations. We present a novel resilient communication approach based on smart street lights as part of the public infrastructure. It supports people in their everyday life and adapts its functionality to the challenges of emergency situations. Our approach relies on various environmental sensors and in-situ processing for automatic situation assessment, and a range of communication mechanisms (e.g., public WiFi hotspot functionality and mesh networking) for maintaining a communication network. Furthermore, resilience is not only achieved based on infrastructure deployed by a digital city's municipality, but also based on integrating citizens through software that runs on their mobile devices (e.g., smartphones and tablets). Web-based zero-installation and platform-agnostic apps can switch to device-to-device communication to continue benefiting people even during a disaster situation. Our approach, featuring a covert channel for professional responders and the zero-installation app, is evaluated through a prototype implementation based on a commercially available street light.

</details>

<details>

<summary>2019-08-28 15:07:07 - DoPa: A Comprehensive CNN Detection Methodology against Physical Adversarial Attacks</summary>

- *Zirui Xu, Fuxun Yu, Xiang Chen*

- `1905.08790v4` - [abs](http://arxiv.org/abs/1905.08790v4) - [pdf](http://arxiv.org/pdf/1905.08790v4)

> Recently, Convolutional Neural Networks (CNNs) demonstrate a considerable vulnerability to adversarial attacks, which can be easily misled by adversarial perturbations. With more aggressive methods proposed, adversarial attacks can be also applied to the physical world, causing practical issues to various CNN powered applications. To secure CNNs, adversarial attack detection is considered as the most critical approach. However, most existing works focus on superficial patterns and merely search a particular method to differentiate the adversarial inputs and natural inputs, ignoring the analysis of CNN inner vulnerability. Therefore, they can only target to specific physical adversarial attacks, lacking expected versatility to different attacks. To address this issue, we propose DoPa -- a comprehensive CNN detection methodology for various physical adversarial attacks. By interpreting the CNN's vulnerability, we find that non-semantic adversarial perturbations can activate CNN with significantly abnormal activations and even overwhelm other semantic input patterns' activations. Therefore, we add a self-verification stage to analyze the semantics of distinguished activation patterns, which improves the CNN recognition process. We apply such a detection methodology into both image and audio CNN recognition scenarios. Experiments show that DoPa can achieve an average rate of 90% success for image attack detection and 92% success for audio attack detection.   Announcement:[The original DoPa draft on arXiv was modified and submitted to a conference already, while this short abstract was submitted only for a presentation at the KDD 2019 AIoT Workshop.]

</details>

<details>

<summary>2019-08-30 08:21:57 - Some SonarQube Issues have a Significant but SmallEffect on Faults and Changes. A large-scale empirical study</summary>

- *Valentina Lenarduzzi, Nyyti Saarimäki, Davide Taibi*

- `1908.11590v1` - [abs](http://arxiv.org/abs/1908.11590v1) - [pdf](http://arxiv.org/pdf/1908.11590v1)

> Context. Companies commonly invest effort to remove technical issues believed to impact software qualities, such as removing anti-patterns or coding styles violations. Objective. Our aim is to analyze the diffuseness of Technical Debt (TD) items in software systems and to assess their impact on code changes and fault-proneness, considering also the type of TD items and their severity. Method. We conducted a case study among 33 Java projects from the Apache Software Foundation (ASF) repository. We analyzed 726 commits containing 27K faults and 12M changes. The projects violated 173 SonarQube rules generating more than 95K TD items in more than 200K classes. Results. Clean classes (classes not affected by TD items) are less change-prone than dirty ones, but the difference between the groups is small. Clean classes are slightly more change-prone than classes affected by TD items of type Code Smell or Security Vulnerability. As for fault-proneness, there is no difference between clean and dirty classes. Moreover, we found a lot of incongruities in the type and severity level assigned by SonarQube. Conclusions. Our result can be useful for practitioners to understand which TD items they should refactor and for researchers to bridge the missing gaps. They can also support companies and tool vendors in identifying TD items as accurately as possible.

</details>


## 2019-09

<details>

<summary>2019-09-03 18:12:30 - Q-MIND: Defeating Stealthy DoS Attacks in SDN with a Machine-learning based Defense Framework</summary>

- *Trung V. Phan, T M Rayhan Gias, Syed Tasnimul Islam, Truong Thu Huong, Nguyen Huu Thanh, Thomas Bauschert*

- `1907.11887v2` - [abs](http://arxiv.org/abs/1907.11887v2) - [pdf](http://arxiv.org/pdf/1907.11887v2)

> Software Defined Networking (SDN) enables flexible and scalable network control and management. However, it also introduces new vulnerabilities that can be exploited by attackers. In particular, low-rate and slow or stealthy Denial-of-Service (DoS) attacks are recently attracting attention from researchers because of their detection challenges. In this paper, we propose a novel machine learning based defense framework named Q-MIND, to effectively detect and mitigate stealthy DoS attacks in SDN-based networks. We first analyze the adversary model of stealthy DoS attacks, the related vulnerabilities in SDN-based networks and the key characteristics of stealthy DoS attacks. Next, we describe and analyze an anomaly detection system that uses a Reinforcement Learning-based approach based on Q-Learning in order to maximize its detection performance. Finally, we outline the complete Q-MIND defense framework that incorporates the optimal policy derived from the Q-Learning agent to efficiently defeat stealthy DoS attacks in SDN-based networks. An extensive comparison of the Q-MIND framework and currently existing methods shows that significant improvements in attack detection and mitigation performance are obtained by Q-MIND.

</details>

<details>

<summary>2019-09-03 20:20:45 - Adversarial Robustness of Similarity-Based Link Prediction</summary>

- *Kai Zhou, Tomasz P. Michalak, Yevgeniy Vorobeychik*

- `1909.01432v1` - [abs](http://arxiv.org/abs/1909.01432v1) - [pdf](http://arxiv.org/pdf/1909.01432v1)

> Link prediction is one of the fundamental problems in social network analysis. A common set of techniques for link prediction rely on similarity metrics which use the topology of the observed subnetwork to quantify the likelihood of unobserved links. Recently, similarity metrics for link prediction have been shown to be vulnerable to attacks whereby observations about the network are adversarially modified to hide target links. We propose a novel approach for increasing robustness of similarity-based link prediction by endowing the analyst with a restricted set of reliable queries which accurately measure the existence of queried links. The analyst aims to robustly predict a collection of possible links by optimally allocating the reliable queries. We formalize the analyst problem as a Bayesian Stackelberg game in which they first choose the reliable queries, followed by an adversary who deletes a subset of links among the remaining (unreliable) queries by the analyst. The analyst in our model is uncertain about the particular target link the adversary attempts to hide, whereas the adversary has full information about the analyst and the network. Focusing on similarity metrics using only local information, we show that the problem is NP-Hard for both players, and devise two principled and efficient approaches for solving it approximately. Extensive experiments with real and synthetic networks demonstrate the effectiveness of our approach.

</details>

<details>

<summary>2019-09-04 18:53:10 - Towards Deep Learning Models Resistant to Adversarial Attacks</summary>

- *Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu*

- `1706.06083v4` - [abs](http://arxiv.org/abs/1706.06083v4) - [pdf](http://arxiv.org/pdf/1706.06083v4)

> Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge.

</details>

<details>

<summary>2019-09-05 00:45:40 - Gathering Cyber Threat Intelligence from Twitter Using Novelty Classification</summary>

- *Ba Dung Le, Guanhua Wang, Mehwish Nasim, Ali Babar*

- `1907.01755v2` - [abs](http://arxiv.org/abs/1907.01755v2) - [pdf](http://arxiv.org/pdf/1907.01755v2)

> Preventing organizations from Cyber exploits needs timely intelligence about Cyber vulnerabilities and attacks, referred as threats. Cyber threat intelligence can be extracted from various sources including social media platforms where users publish the threat information in real time. Gathering Cyber threat intelligence from social media sites is a time consuming task for security analysts that can delay timely response to emerging Cyber threats. We propose a framework for automatically gathering Cyber threat intelligence from Twitter by using a novelty detection model. Our model learns the features of Cyber threat intelligence from the threat descriptions published in public repositories such as Common Vulnerabilities and Exposures (CVE) and classifies a new unseen tweet as either normal or anomalous to Cyber threat intelligence. We evaluate our framework using a purpose-built data set of tweets from 50 influential Cyber security related accounts over twelve months (in 2018). Our classifier achieves the F1-score of 0.643 for classifying Cyber threat tweets and outperforms several baselines including binary classification models. Our analysis of the classification results suggests that Cyber threat relevant tweets on Twitter do not often include the CVE identifier of the related threats. Hence, it would be valuable to collect these tweets and associate them with the related CVE identifier for cyber security applications.

</details>

<details>

<summary>2019-09-05 13:31:52 - FraudJudger: Real-World Data Oriented Fraud Detection on Digital Payment Platforms</summary>

- *Ruoyu Deng, Na Ruan*

- `1909.02398v1` - [abs](http://arxiv.org/abs/1909.02398v1) - [pdf](http://arxiv.org/pdf/1909.02398v1)

> Automated fraud behaviors detection on electronic payment platforms is a tough problem. Fraud users often exploit the vulnerability of payment platforms and the carelessness of users to defraud money, steal passwords, do money laundering, etc, which causes enormous losses to digital payment platforms and users. There are many challenges for fraud detection in practice. Traditional fraud detection methods require a large-scale manually labeled dataset, which is hard to obtain in reality. Manually labeled data cost tremendous human efforts. Besides, the continuous and rapid evolution of fraud users makes it hard to find new fraud patterns based on existing detection rules. In our work, we propose a real-world data oriented detection paradigm which can detect fraud users and upgrade its detection ability automatically. Based on the new paradigm, we design a novel fraud detection model, FraudJudger, to analyze users behaviors on digital payment platforms and detect fraud users with fewer labeled data in training. FraudJudger can learn the latent representations of users from unlabeled data with the help of Adversarial Autoencoder (AAE). Furthermore, FraudJudger can find new fraud patterns from unknown users by cluster analysis. Our experiment is based on a real-world electronic payment dataset. Comparing with other well-known fraud detection methods, FraudJudger can achieve better detection performance with only 10% labeled data.

</details>

<details>

<summary>2019-09-05 15:03:14 - Privado: Practical and Secure DNN Inference with Enclaves</summary>

- *Karan Grover, Shruti Tople, Shweta Shinde, Ranjita Bhagwan, Ramachandran Ramjee*

- `1810.00602v2` - [abs](http://arxiv.org/abs/1810.00602v2) - [pdf](http://arxiv.org/pdf/1810.00602v2)

> Cloud providers are extending support for trusted hardware primitives such as Intel SGX. Simultaneously, the field of deep learning is seeing enormous innovation as well as an increase in adoption. In this paper, we ask a timely question: "Can third-party cloud services use Intel SGX enclaves to provide practical, yet secure DNN Inference-as-a-service?" We first demonstrate that DNN models executing inside enclaves are vulnerable to access pattern based attacks. We show that by simply observing access patterns, an attacker can classify encrypted inputs with 97% and 71% attack accuracy for MNIST and CIFAR10 datasets on models trained to achieve 99% and 79% original accuracy respectively. This motivates the need for PRIVADO, a system we have designed for secure, easy-to-use, and performance efficient inference-as-a-service. PRIVADO is input-oblivious: it transforms any deep learning framework that is written in C/C++ to be free of input-dependent access patterns thus eliminating the leakage. PRIVADO is fully-automated and has a low TCB: with zero developer effort, given an ONNX description of a model, it generates compact and enclave-compatible code which can be deployed on an SGX cloud platform. PRIVADO incurs low performance overhead: we use PRIVADO with Torch framework and show its overhead to be 17.18% on average on 11 different contemporary neural networks.

</details>

<details>

<summary>2019-09-06 09:20:40 - Security Requirements of Commercial Drones for Public Authorities by Vulnerability Analysis of Applications</summary>

- *Daegeon Kim, Huy Kang Kim*

- `1909.02786v1` - [abs](http://arxiv.org/abs/1909.02786v1) - [pdf](http://arxiv.org/pdf/1909.02786v1)

> Due to the ability to overcome the geospatial limitations and to the possibility to converge the various information communication technologies, the application domains and the market size of drones are increasing internationally. Public authorities in South Korean are investing for the domestic drone industry and the technological advancement as a power of innovation and growth of the country. They are also increasing the utilization of drones for various purposes.   The South Korean government ensures the security of IT equipment introduced to the public authorities by enforcing policies such as security compatibility verification and CCTV security certification. Considering the increase of the needs of drones and the possible security effects to the organization operating them, the government needs to develop the security requirements during introducing drones, but there are no such requirements yet.   In this paper, we inspect the vulnerabilities of drones by analyzing the applications of commercial drones made by 4 manufacturers. We also propose the minimum security requirements to resolve the vulnerabilities. We expect our work contributes to the security improvements of drones operated in public authorities.

</details>

<details>

<summary>2019-09-06 14:14:49 - Data Driven Vulnerability Exploration for Design Phase System Analysis</summary>

- *Georgios Bakirtzis, Brandon J. Simon, Aidan G. Collins, Cody H. Fleming, Carl R. Elks*

- `1909.02923v1` - [abs](http://arxiv.org/abs/1909.02923v1) - [pdf](http://arxiv.org/pdf/1909.02923v1)

> Applying security as a lifecycle practice is becoming increasingly important to combat targeted attacks in safety-critical systems. Among others there are two significant challenges in this area: (1) the need for models that can characterize a realistic system in the absence of an implementation and (2) an automated way to associate attack vector information; that is, historical data, to such system models. We propose the cybersecurity body of knowledge (CYBOK), which takes in sufficiently characteristic models of systems and acts as a search engine for potential attack vectors. CYBOK is fundamentally an algorithmic approach to vulnerability exploration, which is a significant extension to the body of knowledge it builds upon. By using CYBOK, security analysts and system designers can work together to assess the overall security posture of systems early in their lifecycle, during major design decisions and before final product designs. Consequently, assisting in applying security earlier and throughout the systems lifecycle.

</details>

<details>

<summary>2019-09-08 10:00:44 - When Explainability Meets Adversarial Learning: Detecting Adversarial Examples using SHAP Signatures</summary>

- *Gil Fidel, Ron Bitton, Asaf Shabtai*

- `1909.03418v1` - [abs](http://arxiv.org/abs/1909.03418v1) - [pdf](http://arxiv.org/pdf/1909.03418v1)

> State-of-the-art deep neural networks (DNNs) are highly effective in solving many complex real-world problems. However, these models are vulnerable to adversarial perturbation attacks, and despite the plethora of research in this domain, to this day, adversaries still have the upper hand in the cat and mouse game of adversarial example generation methods vs. detection and prevention methods. In this research, we present a novel detection method that uses Shapley Additive Explanations (SHAP) values computed for the internal layers of a DNN classifier to discriminate between normal and adversarial inputs. We evaluate our method by building an extensive dataset of adversarial examples over the popular CIFAR-10 and MNIST datasets, and training a neural network-based detector to distinguish between normal and adversarial inputs. We evaluate our detector against adversarial examples generated by diverse state-of-the-art attacks and demonstrate its high detection accuracy and strong generalization ability to adversarial inputs generated with different attack methods.

</details>

<details>

<summary>2019-09-08 16:14:31 - Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks</summary>

- *Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, Yang Liu*

- `1909.03496v1` - [abs](http://arxiv.org/abs/1909.03496v1) - [pdf](http://arxiv.org/pdf/1909.03496v1)

> Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68\% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.

</details>

<details>

<summary>2019-09-09 03:07:10 - Vulnerabilities of Electric Vehicle Battery Packs to Cyberattacks</summary>

- *Shashank Sripad, Sekar Kulandaivel, Vikram Pande, Vyas Sekar, Venkatasubramanian Viswanathan*

- `1711.04822v3` - [abs](http://arxiv.org/abs/1711.04822v3) - [pdf](http://arxiv.org/pdf/1711.04822v3)

> Electric Vehicles (EVs), like all modern vehicles, are entirely controlled by electronic devices embedded within networks that are exposed to the threat of cyberattacks. Cyber vulnerabilities are magnified with EVs due to unique risks associated with EV battery packs. Current batteries have well-known issues with specific energy, cost and fire-related safety risks. In this study, we develop a systematic framework to assess the impact of cyberattacks on EVs. While the current focus of automotive cyberattacks is on short-term physical safety, it is crucial to consider long-term cyberattacks that aim to cause financial losses through accrued impact, especially in the context of EVs. Faulty components of battery management systems such as a compromised voltage regulator could lead to cyberattacks that can overdischarge or overcharge the battery. Overdischarge could lead to failures such as internal shorts in the timescale of minutes through cyberattacks that compromise energy-intensive EV subsystems like auxiliary components. Attacks that overcharge the pack could shorten the lifetime of a new battery pack to less than a year. Further, such attacks also pose physical safety risks via the triggering of thermal (fire) events. Attacks on auxiliary components lead to battery drain, which could be up to 20% of the state-of-charge per hour. Lastly, we develop a heuristic for the stealthiness of a cyberattack to augment traditional threat models. The methodology presented here will help in building the foundational principles of electric vehicle cybersecurity: a nascent but critical topic in the coming years.

</details>

<details>

<summary>2019-09-09 11:04:33 - Proconda -- Protected Control Data</summary>

- *Marie-Therese Walter, David Pfaff, Stefan Nürnberger, Michael Backes*

- `1909.03758v1` - [abs](http://arxiv.org/abs/1909.03758v1) - [pdf](http://arxiv.org/pdf/1909.03758v1)

> Memory corruption vulnerabilities often enable attackers to take control of a target system by overwriting control-flow relevant data (such as return addresses and function pointers), which are potentially stored in close proximity of related, typically user-controlled data on the stack. In this paper, we propose ProConDa, a general approach for protecting control-flow relevant data on the stack ProConDa leverages hardware features to enforce a strict separation between control-flow relevant and regular data of programs written in non-memory-safe languages such as C. Contrary to related approaches, ProConDa does not rely on information hiding and is therefore not susceptible to several recent attacks specifically targeting information hiding as a foundation for memory isolation. We show that ProConDa enforcement is compatible with existing software by applying a software-based prototype to industry benchmarks on an ARM CPU running Linux.

</details>

<details>

<summary>2019-09-09 20:35:23 - DaTscan SPECT Image Classification for Parkinson's Disease</summary>

- *Justin Quan, Lin Xu, Rene Xu, Tyrael Tong, Jean Su*

- `1909.04142v1` - [abs](http://arxiv.org/abs/1909.04142v1) - [pdf](http://arxiv.org/pdf/1909.04142v1)

> Parkinson's Disease (PD) is a neurodegenerative disease that currently does not have a cure. In order to facilitate disease management and reduce the speed of symptom progression, early diagnosis is essential. The current clinical, diagnostic approach is to have radiologists perform human visual analysis of the degeneration of dopaminergic neurons in the substantia nigra region of the brain. Clinically, dopamine levels are monitored through observing dopamine transporter (DaT) activity. One method of DaT activity analysis is performed with the injection of an Iodine-123 fluoropropyl (123I-FP-CIT) tracer combined with single photon emission computerized tomography (SPECT) imaging. The tracer illustrates the region of interest in the resulting DaTscan SPECT images. Human visual analysis is slow and vulnerable to subjectivity between radiologists, so the goal was to develop an introductory implementation of a deep convolutional neural network that can objectively and accurately classify DaTscan SPECT images as Parkinson's Disease or normal. This study illustrates the approach of using a deep convolutional neural network and evaluates its performance on DaTscan SPECT image classification. The data used in this study was obtained through a database provided by the Parkinson's Progression Markers Initiative (PPMI). The deep neural network in this study utilizes the InceptionV3 architecture, 1st runner up in the 2015 ImageNet Large Scale Visual Recognition Competition (ILSVRC), as a base model. A custom, binary classifier block was added on top of this base. In order to account for the small dataset size, a ten fold cross validation was implemented to evaluate the model's performance.

</details>

<details>

<summary>2019-09-10 06:17:06 - Learning to Disentangle Robust and Vulnerable Features for Adversarial Detection</summary>

- *Byunggill Joe, Sung Ju Hwang, Insik Shin*

- `1909.04311v1` - [abs](http://arxiv.org/abs/1909.04311v1) - [pdf](http://arxiv.org/pdf/1909.04311v1)

> Although deep neural networks have shown promising performances on various tasks, even achieving human-level performance on some, they are shown to be susceptible to incorrect predictions even with imperceptibly small perturbations to an input. There exists a large number of previous works which proposed to defend against such adversarial attacks either by robust inference or detection of adversarial inputs. Yet, most of them cannot effectively defend against whitebox attacks where an adversary has a knowledge of the model and defense. More importantly, they do not provide a convincing reason why the generated adversarial inputs successfully fool the target models. To address these shortcomings of the existing approaches, we hypothesize that the adversarial inputs are tied to latent features that are susceptible to adversarial perturbation, which we call vulnerable features. Then based on this intuition, we propose a minimax game formulation to disentangle the latent features of each instance into robust and vulnerable ones, using variational autoencoders with two latent spaces. We thoroughly validate our model for both blackbox and whitebox attacks on MNIST, Fashion MNIST5, and Cat & Dog datasets, whose results show that the adversarial inputs cannot bypass our detector without changing its semantics, in which case the attack has failed.

</details>

<details>

<summary>2019-09-10 10:09:38 - FDA: Feature Disruptive Attack</summary>

- *Aditya Ganeshan, B. S. Vivek, R. Venkatesh Babu*

- `1909.04385v1` - [abs](http://arxiv.org/abs/1909.04385v1) - [pdf](http://arxiv.org/pdf/1909.04385v1)

> Though Deep Neural Networks (DNN) show excellent performance across various computer vision tasks, several works show their vulnerability to adversarial samples, i.e., image samples with imperceptible noise engineered to manipulate the network's prediction. Adversarial sample generation methods range from simple to complex optimization techniques. Majority of these methods generate adversaries through optimization objectives that are tied to the pre-softmax or softmax output of the network. In this work we, (i) show the drawbacks of such attacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and New Label Old Rank (NLOR) in order to quantify the extent of damage made by an attack, and (iii) propose a new adversarial attack FDA: Feature Disruptive Attack, to address the drawbacks of existing attacks. FDA works by generating image perturbation that disrupt features at each layer of the network and causes deep-features to be highly corrupt. This allows FDA adversaries to severely reduce the performance of deep networks. We experimentally validate that FDA generates stronger adversaries than other state-of-the-art methods for image classification, even in the presence of various defense measures. More importantly, we show that FDA disrupts feature-representation based tasks even without access to the task-specific network or methodology. Code available at: https://github.com/BardOfCodes/fda

</details>

<details>

<summary>2019-09-10 16:53:23 - On the (In)security of Bluetooth Low Energy One-Way Secure Connections Only Mode</summary>

- *Yue Zhang, Jian Weng, Rajib Dey, Yier Jin, Zhiqiang Lin, Xinwen Fu*

- `1908.10497v2` - [abs](http://arxiv.org/abs/1908.10497v2) - [pdf](http://arxiv.org/pdf/1908.10497v2)

> To defeat security threats such as man-in-the-middle (MITM) attacks, Bluetooth Low Energy (BLE) 4.2 and 5.x introduce the Secure Connections Only mode, under which a BLE device accepts only secure paring protocols including Passkey Entry and Numeric Comparison from an initiator, e.g., an Android mobile. However, the BLE specification does not explicitly require the Secure Connection Only mode of the initiator. Taking the Android's BLE programming framework for example, we found that it cannot enforce secure pairing, invalidating the security protection provided by the Secure Connection Only mode. The same problem applies to Apple iOS too.   Specifically, we examine the life cycle of a BLE pairing process in Android and identify four severe design flaws. These design flaws can be exploited by attackers to perform downgrading attacks, forcing the BLE pairing protocols to run in the insecure mode without the users' awareness. To validate our findings, we selected and tested 18 popular BLE commercial products and our experimental results proved that downgrading attacks and MITM attacks were all possible to these products. All 3501 BLE apps from Androzoo are also subject to these attacks. For defense, we have designed and implemented a prototype of the Secure Connection Only mode on Android 8 through the Android Open Source Project (AOSP). We have reported the identified BLE pairing vulnerabilities to Bluetooth Special Interest Group (SIG), Google, Apple, Texas Instruments (TI) and all of them are actively addressing this issue. Google rated the reported security flaw a High Severity.

</details>

<details>

<summary>2019-09-10 22:20:32 - Effectiveness of Adversarial Examples and Defenses for Malware Classification</summary>

- *Robert Podschwadt, Hassan Takabi*

- `1909.04778v1` - [abs](http://arxiv.org/abs/1909.04778v1) - [pdf](http://arxiv.org/pdf/1909.04778v1)

> Artificial neural networks have been successfully used for many different classification tasks including malware detection and distinguishing between malicious and non-malicious programs. Although artificial neural networks perform very well on these tasks, they are also vulnerable to adversarial examples. An adversarial example is a sample that has minor modifications made to it so that the neural network misclassifies it. Many techniques have been proposed, both for crafting adversarial examples and for hardening neural networks against them. Most previous work has been done in the image domain. Some of the attacks have been adopted to work in the malware domain which typically deals with binary feature vectors. In order to better understand the space of adversarial examples in malware classification, we study different approaches of crafting adversarial examples and defense techniques in the malware domain and compare their effectiveness on multiple datasets.

</details>

<details>

<summary>2019-09-11 13:28:44 - Sparse and Imperceivable Adversarial Attacks</summary>

- *Francesco Croce, Matthias Hein*

- `1909.05040v1` - [abs](http://arxiv.org/abs/1909.05040v1) - [pdf](http://arxiv.org/pdf/1909.05040v1)

> Neural networks have been proven to be vulnerable to a variety of adversarial attacks. From a safety perspective, highly sparse adversarial attacks are particularly dangerous. On the other hand the pixelwise perturbations of sparse attacks are typically large and thus can be potentially detected. We propose a new black-box technique to craft adversarial examples aiming at minimizing $l_0$-distance to the original image. Extensive experiments show that our attack is better or competitive to the state of the art. Moreover, we can integrate additional bounds on the componentwise perturbation. Allowing pixels to change only in region of high variation and avoiding changes along axis-aligned edges makes our adversarial examples almost non-perceivable. Moreover, we adapt the Projected Gradient Descent attack to the $l_0$-norm integrating componentwise constraints. This allows us to do adversarial training to enhance the robustness of classifiers against sparse and imperceivable adversarial manipulations.

</details>

<details>

<summary>2019-09-11 15:19:36 - Byzantine-Robust Federated Machine Learning through Adaptive Model Averaging</summary>

- *Luis Muñoz-González, Kenneth T. Co, Emil C. Lupu*

- `1909.05125v1` - [abs](http://arxiv.org/abs/1909.05125v1) - [pdf](http://arxiv.org/pdf/1909.05125v1)

> Federated learning enables training collaborative machine learning models at scale with many participants whilst preserving the privacy of their datasets. Standard federated learning techniques are vulnerable to Byzantine failures, biased local datasets, and poisoning attacks. In this paper we introduce Adaptive Federated Averaging, a novel algorithm for robust federated learning that is designed to detect failures, attacks, and bad updates provided by participants in a collaborative model. We propose a Hidden Markov Model to model and learn the quality of model updates provided by each participant during training. In contrast to existing robust federated learning schemes, we propose a robust aggregation rule that detects and discards bad or malicious local model updates at each training iteration. This includes a mechanism that blocks unwanted participants, which also increases the computational and communication efficiency. Our experimental evaluation on 4 real datasets show that our algorithm is significantly more robust to faulty, noisy and malicious participants, whilst being computationally more efficient than other state-of-the-art robust federated learning methods such as Multi-KRUM and coordinate-wise median.

</details>

<details>

<summary>2019-09-12 03:50:28 - Feedback Learning for Improving the Robustness of Neural Networks</summary>

- *Chang Song, Zuoguan Wang, Hai Li*

- `1909.05443v1` - [abs](http://arxiv.org/abs/1909.05443v1) - [pdf](http://arxiv.org/pdf/1909.05443v1)

> Recent research studies revealed that neural networks are vulnerable to adversarial attacks. State-of-the-art defensive techniques add various adversarial examples in training to improve models' adversarial robustness. However, these methods are not universal and can't defend unknown or non-adversarial evasion attacks. In this paper, we analyze the model robustness in the decision space. A feedback learning method is then proposed, to understand how well a model learns and to facilitate the retraining process of remedying the defects. The evaluations according to a set of distance-based criteria show that our method can significantly improve models' accuracy and robustness against different types of evasion attacks. Moreover, we observe the existence of inter-class inequality and propose to compensate it by changing the proportions of examples generated in different classes.

</details>

<details>

<summary>2019-09-12 15:21:55 - Protecting the stack with PACed canaries</summary>

- *Hans Liljestrand, Zaheer Gauhar, Thomas Nyman, Jan-Erik Ekberg, N. Asokan*

- `1909.05747v1` - [abs](http://arxiv.org/abs/1909.05747v1) - [pdf](http://arxiv.org/pdf/1909.05747v1)

> Stack canaries remain a widely deployed defense against memory corruption attacks. Despite their practical usefulness, canaries are vulnerable to memory disclosure and brute-forcing attacks. We propose PCan, a new approach based on ARMv8.3-A pointer authentication (PA), that uses dynamically-generated canaries to mitigate these weaknesses and show that it provides more fine-grained protection with minimal performance overhead.

</details>

<details>

<summary>2019-09-12 17:29:08 - On the Hardness of Robust Classification</summary>

- *Pascale Gourdeau, Varun Kanade, Marta Kwiatkowska, James Worrell*

- `1909.05822v1` - [abs](http://arxiv.org/abs/1909.05822v1) - [pdf](http://arxiv.org/pdf/1909.05822v1)

> It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. In this paper we study the feasibility of robust learning from the perspective of computational learning theory, considering both sample and computational complexity. In particular, our definition of robust learnability requires polynomial sample complexity. We start with two negative results. We show that no non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb just a single input bit. We show moreover that the class of monotone conjunctions cannot be robustly learned under the uniform distribution against an adversary who can perturb $\omega(\log n)$ input bits. However if the adversary is restricted to perturbing $O(\log n)$ bits, then the class of monotone conjunctions can be robustly learned with respect to a general class of distributions (that includes the uniform distribution). Finally, we provide a simple proof of the computational hardness of robust learning on the boolean hypercube. Unlike previous results of this nature, our result does not rely on another computational model (e.g. the statistical query model) nor on any hardness assumption other than the existence of a hard learning problem in the PAC framework.

</details>

<details>

<summary>2019-09-14 14:30:25 - Adversarial Examples Versus Cloud-based Detectors: A Black-box Empirical Study</summary>

- *Xurong Li, Shouling Ji, Meng Han, Juntao Ji, Zhenyu Ren, Yushan Liu, Chunming Wu*

- `1901.01223v4` - [abs](http://arxiv.org/abs/1901.01223v4) - [pdf](http://arxiv.org/pdf/1901.01223v4)

> Deep learning has been broadly leveraged by major cloud providers, such as Google, AWS and Baidu, to offer various computer vision related services including image classification, object identification, illegal image detection, etc. While recent works extensively demonstrated that deep learning classification models are vulnerable to adversarial examples, cloud-based image detection models, which are more complicated than classifiers, may also have similar security concern but not get enough attention yet. In this paper, we mainly focus on the security issues of real-world cloud-based image detectors. Specifically, (1) based on effective semantic segmentation, we propose four attacks to generate semantics-aware adversarial examples via only interacting with black-box APIs; and (2) we make the first attempt to conduct an extensive empirical study of black-box attacks against real-world cloud-based image detectors. Through the comprehensive evaluations on five major cloud platforms: AWS, Azure, Google Cloud, Baidu Cloud, and Alibaba Cloud, we demonstrate that our image processing based attacks can reach a success rate of approximately 100%, and the semantic segmentation based attacks have a success rate over 90% among different detection services, such as violence, politician, and pornography detection. We also proposed several possible defense strategies for these security challenges in the real-life situation.

</details>

<details>

<summary>2019-09-14 19:23:01 - Biometric Blockchain: A Secure Solution for Intelligent Vehicle Data Sharing</summary>

- *Bing Xu, Tobechukwu Agbele, Qiang Ni, Richard Jiang*

- `1909.06369v1` - [abs](http://arxiv.org/abs/1909.06369v1) - [pdf](http://arxiv.org/pdf/1909.06369v1)

> The intelligent vehicle (IV) has become a promising technology that could revolutionize our life in smart cities sooner or later. However, it yet suffers from many security vulnerabilities. Traditional security methods are incapable to secure the IV data sharing against malicious attacks. Blockchain, as expected by both research and industry communities, has emerged as a good solution to address these issues. The major issues in IV data sharing are trust, data accuracy and reliability of data sharing in the communication channel. Blockchain technology, previously working for the cryptocurrency, has recently applied to build trust and reliability in peer-to-peer networks with similar topologies of IV data sharing. In this chapter, we present a new framework, namely biometric blockchain (BBC), for secure IV data sharing. In our new scheme, biometric information is exploited as a cue to record who is responsible in the data sharing activities, while the proposed BBC technology serves as the backbone of the IV data-sharing architecture. Hence, the proposed BBC technology provides a more reliable trust environment between the vehicles while personal identities are traceable in the proposed new scheme.

</details>

<details>

<summary>2019-09-16 04:38:13 - Exploring the Landscape of Spatial Robustness</summary>

- *Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, Aleksander Madry*

- `1712.02779v4` - [abs](http://arxiv.org/abs/1712.02779v4) - [pdf](http://arxiv.org/pdf/1712.02779v4)

> The study of adversarial robustness has so far largely focused on perturbations bound in p-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network--based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and test-time input aggregation to significantly improve robustness. Finally we find that, in contrast to the p-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study. Code available at https://github.com/MadryLab/adversarial_spatial and https://github.com/MadryLab/spatial-pytorch.

</details>

<details>

<summary>2019-09-16 07:20:28 - Towards Automated Application-Specific Software Stacks</summary>

- *Nicolai Davidsson, Andre Pawlowski, Thorsten Holz*

- `1907.01933v3` - [abs](http://arxiv.org/abs/1907.01933v3) - [pdf](http://arxiv.org/pdf/1907.01933v3)

> Software complexity has increased over the years. One common way to tackle this complexity during development is to encapsulate features into a shared library. This allows developers to reuse already implemented features instead of reimplementing them over and over again. However, not all features provided by a shared library are actually used by an application. As a result, an application using shared libraries loads unused code into memory, which an attacker can use to perform code-reuse and similar types of attacks. The same holds for applications written in a scripting language such as PHP or Ruby: The interpreter typically offers much more functionality than is actually required by the application and hence provides a larger overall attack surface.   In this paper, we tackle this problem and propose a first step towards automated application-specific software stacks. We present a compiler extension capable of removing unneeded code from shared libraries and---with the help of domain knowledge---also capable of removing unused functionalities from an interpreter's code base during the compilation process. Our evaluation against a diverse set of real-world applications, among others Nginx, Lighttpd, and the PHP interpreter, removes on average 71.3% of the code in musl-libc, a popular libc implementation. The evaluation on web applications show that a tailored PHP interpreter can mitigate entire vulnerability classes, as is the case for OpenConf. We demonstrate the applicability of our debloating approach by creating an application-specific software stack for a Wordpress web application: we tailor the libc library to the Nginx web server and PHP interpreter, whereas the PHP interpreter is tailored to the Wordpress web application. In this real-world scenario, the code of the libc is decreased by 65.1% in total, thereby reducing the available code for code-reuse attacks.

</details>

<details>

<summary>2019-09-16 17:38:14 - Identifying Research Challenges in Post Quantum Cryptography Migration and Cryptographic Agility</summary>

- *David Ott, Christopher Peikert, other workshop participants*

- `1909.07353v1` - [abs](http://arxiv.org/abs/1909.07353v1) - [pdf](http://arxiv.org/pdf/1909.07353v1)

> The implications of sufficiently large quantum computers for widely used public-key cryptography is well-documented and increasingly discussed by the security community. An April 2016 report by the National Institute of Standards and Technology (NIST), notably, calls out the need for new standards to replace cryptosystems based on integer factorization and discrete logarithm problems, which have been shown to be vulnerable to Shor's quantum algorithm for prime factorization. Specifically, widely used RSA, ECDSA, ECDH, and DSA cryptosystems will need to be replaced by post-quantum cryptography (PQC) alternatives (also known as quantum-resistant or quantum-safe cryptography). Failure to transition before sufficiently powerful quantum computers are realized will jeopardize the security of public key cryptosystems which are widely deployed within communication protocols, digital signing mechanisms, authentication frameworks, and more. To avoid this, NIST has actively led a PQC standardization effort since 2016, leveraging a large and international research community.   On January 31-February 1, 2019, the Computing Community Consortium (CCC) held a workshop in Washington, D.C. to discuss research challenges associated with PQC migration. Entitled, "Identifying Research Challenges in Post Quantum Cryptography Migration and Cryptographic Agility", participants came from three distinct yet related communities: cryptographers contributing to the NIST PQC standards effort, applied cryptographers with expertise in creating cryptographic solutions and implementing cryptography in real-world settings, and industry practitioners with expertise in deploying cryptographic standards within products and compute infrastructures. Discussion centered around two key themes: identifying constituent challenges in PQC migration and imagining a new science of "cryptographic agility".

</details>

<details>

<summary>2019-09-17 14:01:01 - Variable Record Table: A Run-time Solution for Mitigating Buffer Overflow Attack</summary>

- *Love Kumar Sah, Sheikh Ariful Islam, Srinivas Katkoori*

- `1909.07821v1` - [abs](http://arxiv.org/abs/1909.07821v1) - [pdf](http://arxiv.org/pdf/1909.07821v1)

> We present a novel approach to mitigate buffer overflow attack using Variable Record Table (VRT). Dedicated memory space is used to automatically record base and bound information of variables extracted during runtime. We instrument frame pointer and function(s) related registers to decode variable memory space in stack and heap. We have modified Simplescalar/PISA simulator to extract variables space of six (6) benchmark suites from MiBench. We have tested 290 small C programs (MIT corpus suite) having 22 different buffer overflow vulnerabilities in stack and heap. Experimental results show that our approach can detect buffer overflow attack with zero instruction overhead with the memory space requirement up to 13Kb to maintain VRT for a program with 324 variables.

</details>

<details>

<summary>2019-09-17 15:05:31 - Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model</summary>

- *Prashanth Vijayaraghavan, Deb Roy*

- `1909.07873v1` - [abs](http://arxiv.org/abs/1909.07873v1) - [pdf](http://arxiv.org/pdf/1909.07873v1)

> Recently, generating adversarial examples has become an important means of measuring robustness of a deep learning model. Adversarial examples help us identify the susceptibilities of the model and further counter those vulnerabilities by applying adversarial training techniques. In natural language domain, small perturbations in the form of misspellings or paraphrases can drastically change the semantics of the text. We propose a reinforcement learning based approach towards generating adversarial examples in black-box settings. We demonstrate that our method is able to fool well-trained models for (a) IMDB sentiment classification task and (b) AG's news corpus news categorization task with significantly high success rates. We find that the adversarial examples generated are semantics-preserving perturbations to the original text.

</details>

<details>

<summary>2019-09-17 16:51:09 - A Forensic Qualitative Analysis of Contributions to Wikipedia from Anonymity Seeking Users</summary>

- *Kaylea Champion, Nora McDonald, Stephanie Bankes, Joseph Zhang, Rachel Greenstadt, Andrea Forte, Benjamin Mako Hill*

- `1909.07929v1` - [abs](http://arxiv.org/abs/1909.07929v1) - [pdf](http://arxiv.org/pdf/1909.07929v1)

> By choice or by necessity, some contributors to commons-based peer production sites use privacy-protecting services to remain anonymous. As anonymity seekers, users of the Tor network have been cast both as ill-intentioned vandals and as vulnerable populations concerned with their privacy. In this study, we use a dataset drawn from a corpus of Tor edits to Wikipedia to uncover the character of Tor users' contributions. We build in-depth narrative descriptions of Tor users' actions and conduct a thematic analysis that places their editing activity into seven broad groups. We find that although their use of a privacy-protecting service marks them as unusual within Wikipedia, the character of many Tor users' contributions is in line with the expectations and norms of Wikipedia. However, our themes point to several important places where lack of trust promotes disorder, and to contributions where risks to contributors, service providers, and communities are unaligned.

</details>

<details>

<summary>2019-09-17 21:46:14 - Physics-guided Convolutional Neural Network (PhyCNN) for Data-driven Seismic Response Modeling</summary>

- *Ruiyang Zhang, Yang Liu, Hao Sun*

- `1909.08118v1` - [abs](http://arxiv.org/abs/1909.08118v1) - [pdf](http://arxiv.org/pdf/1909.08118v1)

> Seismic events, among many other natural hazards, reduce due functionality and exacerbate vulnerability of in-service buildings. Accurate modeling and prediction of building's response subjected to earthquakes makes possible to evaluate building performance. To this end, we leverage the recent advances in deep learning and develop a physics-guided convolutional neural network (PhyCNN) framework for data-driven seismic response modeling and serviceability assessment of buildings. The proposed PhyCNN approach is capable of accurately predicting building's seismic response in a data-driven fashion without the need of a physics-based analytical/numerical model. The basic concept is to train a deep PhyCNN model based on available seismic input-output datasets (e.g., from simulation or sensing) and physics constraints. The trained PhyCNN can then used as a surrogate model for structural seismic response prediction. Available physics (e.g., the law of dynamics) can provide constraints to the network outputs, alleviate overfitting issues, reduce the need of big training datasets, and thus improve the robustness of the trained model for more reliable prediction. The trained surrogate model is then utilized for fragility analysis given certain limit state criteria (e.g., the serviceability state). In addition, an unsupervised learning algorithm based on K-means clustering is also proposed to partition the limited number of datasets to training, validation and prediction categories, so as to maximize the use of limited datasets. The performance of the proposed approach is demonstrated through three case studies including both numerical and experimental examples. Convincing results illustrate that the proposed PhyCNN paradigm outperforms conventional pure data-based neural networks.

</details>

<details>

<summary>2019-09-18 00:09:26 - Robust Classification using Robust Feature Augmentation</summary>

- *Kevin Eykholt, Swati Gupta, Atul Prakash, Amir Rahmati, Pratik Vaishnavi, Haizhong Zheng*

- `1905.10904v3` - [abs](http://arxiv.org/abs/1905.10904v3) - [pdf](http://arxiv.org/pdf/1905.10904v3)

> Existing deep neural networks, say for image classification, have been shown to be vulnerable to adversarial images that can cause a DNN misclassification, without any perceptible change to an image. In this work, we propose shock absorbing robust features such as binarization, e.g., rounding, and group extraction, e.g., color or shape, to augment the classification pipeline, resulting in more robust classifiers. Experimentally, we show that augmenting ML models with these techniques leads to improved overall robustness on adversarial inputs as well as significant improvements in training time. On the MNIST dataset, we achieved 14x speedup in training time to obtain 90% adversarial accuracy com-pared to the state-of-the-art adversarial training method of Madry et al., as well as retained higher adversarial accuracy over a broader range of attacks. We also find robustness improvements on traffic sign classification using robust feature augmentation. Finally, we give theoretical insights for why one can expect robust feature augmentation to reduce adversarial input space

</details>

<details>

<summary>2019-09-18 01:54:10 - Interpretable Deep Learning under Fire</summary>

- *Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, Ting Wang*

- `1812.00891v3` - [abs](http://arxiv.org/abs/1812.00891v3) - [pdf](http://arxiv.org/pdf/1812.00891v3)

> Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains. A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input? The improved interpretability is believed to offer a sense of security by involving human in the decision-making process. Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far.   Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes). We show that existing \imlses are highly vulnerable to adversarial manipulations. Specifically, we present ADV^2, a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV^2 the adversary is able to arbitrarily designate an input's prediction and interpretation. Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability -- a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously. Finally, we explore potential countermeasures against ADV^2, including leveraging its low transferability and incorporating it in an adversarial training framework. Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions.

</details>

<details>

<summary>2019-09-18 12:45:34 - CASAD: CAN-Aware Stealthy-Attack Detection for In-Vehicle Networks</summary>

- *Nasser Nowdehi, Wissam Aoudi, Magnus Almgren, Tomas Olovsson*

- `1909.08407v1` - [abs](http://arxiv.org/abs/1909.08407v1) - [pdf](http://arxiv.org/pdf/1909.08407v1)

> Nowadays, vehicles have complex in-vehicle networks (IVNs) with millions of lines of code controlling almost every function in the vehicle including safety-critical functions. It has recently been shown that IVNs are becoming increasingly vulnerable to cyber-attacks capable of taking control of vehicles, thereby threatening the safety of the passengers. Several countermeasures have been proposed in the literature in response to the arising threats, however, hurdle requirements imposed by the industry is hindering their adoption in practice. In particular, detecting attacks on IVNs is challenged by strict resource constraints and utterly complex communication patterns that vary even for vehicles of the same model. In addition, existing solutions suffer from two main drawbacks. First, they depend on the underlying vehicle configuration, and second, they are incapable of detecting certain attacks of a stealthy nature. In this paper, we propose CASAD, a CAN-Aware Stealthy-Attack Detection mechanism that does not abide by the strict specifications predefined for every vehicle model and addresses key real-world deployability challenges. Our fast, lightweight, and system-agnostic approach learns the normal behavior of IVN dynamics from historical data and detects deviations by continuously monitoring IVN traffic. We demonstrate the effectiveness of CASAD by conducting various experiments on a CAN bus prototype, a 2018 Volvo XC60, and publicly available data from two real vehicles. Our approach is experimentally shown to be effective against different attack scenarios, including the prompt detection of stealthy attacks, and has considerable potential applicability to real vehicles.

</details>

<details>

<summary>2019-09-18 16:58:01 - Oracle-Supported Dynamic Exploit Generation for Smart Contracts</summary>

- *Haijun Wang, Yi Li, Shang-Wei Lin, Cyrille Artho, Lei Ma, Yang Liu*

- `1909.06605v2` - [abs](http://arxiv.org/abs/1909.06605v2) - [pdf](http://arxiv.org/pdf/1909.06605v2)

> Despite the high stakes involved in smart contracts, they are often developed in an undisciplined manner, leaving the security and reliability of blockchain transactions at risk. In this paper, we introduce ContraMaster: an oracle-supported dynamic exploit generation framework for smart contracts. Existing approaches mutate only single transactions; ContraMaster exceeds these by mutating the transaction sequences. ContraMaster uses data-flow, control-flow, and the dynamic contract state to guide its mutations. It then monitors the executions of target contract programs, and validates the results against a general-purpose semantic test oracle to discover vulnerabilities. Being a dynamic technique, it guarantees that each discovered vulnerability is a violation of the test oracle and is able to generate the attack script to exploit this vulnerability. In contrast to rule-based approaches, ContraMaster has not shown any false positives, and it easily generalizes to unknown types of vulnerabilities (e.g., logic errors). We evaluate ContraMaster on 218 vulnerable smart contracts. The experimental results confirm its practical applicability and advantages over the state-of-the-art techniques, and also reveal three new types of attacks.

</details>

<details>

<summary>2019-09-19 02:26:14 - Defending against Machine Learning based Inference Attacks via Adversarial Examples: Opportunities and Challenges</summary>

- *Jinyuan Jia, Neil Zhenqiang Gong*

- `1909.08526v2` - [abs](http://arxiv.org/abs/1909.08526v2) - [pdf](http://arxiv.org/pdf/1909.08526v2)

> As machine learning (ML) becomes more and more powerful and easily accessible, attackers increasingly leverage ML to perform automated large-scale inference attacks in various domains. In such an ML-equipped inference attack, an attacker has access to some data (called public data) of an individual, a software, or a system; and the attacker uses an ML classifier to automatically infer their private data. Inference attacks pose severe privacy and security threats to individuals and systems. Inference attacks are successful because private data are statistically correlated with public data, and ML classifiers can capture such statistical correlations. In this chapter, we discuss the opportunities and challenges of defending against ML-equipped inference attacks via adversarial examples. Our key observation is that attackers rely on ML classifiers in inference attacks. The adversarial machine learning community has demonstrated that ML classifiers have various vulnerabilities. Therefore, we can turn the vulnerabilities of ML into defenses against inference attacks. For example, ML classifiers are vulnerable to adversarial examples, which add carefully crafted noise to normal examples such that an ML classifier makes predictions for the examples as we desire. To defend against inference attacks, we can add carefully crafted noise into the public data to turn them into adversarial examples, such that attackers' classifiers make incorrect predictions for the private data. However, existing methods to construct adversarial examples are insufficient because they did not consider the unique challenges and requirements for the crafted noise at defending against inference attacks. In this chapter, we take defending against inference attacks in online social networks as an example to illustrate the opportunities and challenges.

</details>

<details>

<summary>2019-09-19 08:16:35 - Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network</summary>

- *Anjith George, Zohreh Mostaani, David Geissenbuhler, Olegs Nikisins, Andre Anjos, Sebastien Marcel*

- `1909.08848v1` - [abs](http://arxiv.org/abs/1909.08848v1) - [pdf](http://arxiv.org/pdf/1909.08848v1)

> Face recognition is a mainstream biometric authentication method. However, vulnerability to presentation attacks (a.k.a spoofing) limits its usability in unsupervised applications. Even though there are many methods available for tackling presentation attacks (PA), most of them fail to detect sophisticated attacks such as silicone masks. As the quality of presentation attack instruments improves over time, achieving reliable PA detection with visual spectra alone remains very challenging. We argue that analysis in multiple channels might help to address this issue. In this context, we propose a multi-channel Convolutional Neural Network based approach for presentation attack detection (PAD). We also introduce the new Wide Multi-Channel presentation Attack (WMCA) database for face PAD which contains a wide variety of 2D and 3D presentation attacks for both impersonation and obfuscation attacks. Data from different channels such as color, depth, near-infrared and thermal are available to advance the research in face PAD. The proposed method was compared with feature-based approaches and found to outperform the baselines achieving an ACER of 0.3% on the introduced dataset. The database and the software to reproduce the results are made available publicly.

</details>

<details>

<summary>2019-09-19 08:50:01 - Adversarial Vulnerability Bounds for Gaussian Process Classification</summary>

- *Michael Thomas Smith, Kathrin Grosse, Michael Backes, Mauricio A Alvarez*

- `1909.08864v1` - [abs](http://arxiv.org/abs/1909.08864v1) - [pdf](http://arxiv.org/pdf/1909.08864v1)

> Machine learning (ML) classification is increasingly used in safety-critical systems. Protecting ML classifiers from adversarial examples is crucial. We propose that the main threat is that of an attacker perturbing a confidently classified input to produce a confident misclassification. To protect against this we devise an adversarial bound (AB) for a Gaussian process classifier, that holds for the entire input domain, bounding the potential for any future adversarial method to cause such misclassification. This is a formal guarantee of robustness, not just an empirically derived result. We investigate how to configure the classifier to maximise the bound, including the use of a sparse approximation, leading to the method producing a practical, useful and provably robust classifier, which we test using a variety of datasets.

</details>

<details>

<summary>2019-09-19 17:21:41 - BenchPress: Analyzing Android App Vulnerability Benchmark Suites</summary>

- *Joydeep Mitra, Venkatesh-Prasad Ranganath, Aditya Narkar*

- `1903.05170v4` - [abs](http://arxiv.org/abs/1903.05170v4) - [pdf](http://arxiv.org/pdf/1903.05170v4)

> In recent years, various benchmark suites have been developed to evaluate the efficacy of Android security analysis tools. The choice of such benchmark suites used in tool evaluations is often based on the availability and popularity of suites and not on their characteristics and relevance. One of the reasons for such choices is the lack of information about the characteristics and relevance of benchmarks suites.   In this context, we empirically evaluated four Android specific benchmark suites: DroidBench, Ghera, IccBench, and UBCBench. For each benchmark suite, we identified the APIs used by the suite that were discussed on Stack Overflow in the context of Android app development and measured the usage of these APIs in a sample of 227K real world apps (coverage). We also compared each pair of benchmark suites to identify the differences between them in terms of API usage. Finally, we identified security-related APIs used in real-world apps but not in any of the above benchmark suites to assess the opportunities to extend benchmark suites (gaps).   The findings in this paper can help 1) Android security analysis tool developers choose benchmark suites that are best suited to evaluate their tools (informed by coverage and pairwise comparison) and 2) Android app vulnerability benchmark creators develop and extend benchmark suites (informed by gaps).

</details>

<details>

<summary>2019-09-19 21:08:13 - Toward Robust Image Classification</summary>

- *Basemah Alshemali, Alta Graham, Jugal Kalita*

- `1909.12927v1` - [abs](http://arxiv.org/abs/1909.12927v1) - [pdf](http://arxiv.org/pdf/1909.12927v1)

> Neural networks are frequently used for image classification, but can be vulnerable to misclassification caused by adversarial images. Attempts to make neural network image classification more robust have included variations on preprocessing (cropping, applying noise, blurring), adversarial training, and dropout randomization. In this paper, we implemented a model for adversarial detection based on a combination of two of these techniques: dropout randomization with preprocessing applied to images within a given Bayesian uncertainty. We evaluated our model on the MNIST dataset, using adversarial images generated using Fast Gradient Sign Method (FGSM), Jacobian-based Saliency Map Attack (JSMA) and Basic Iterative Method (BIM) attacks. Our model achieved an average adversarial image detection accuracy of 97%, with an average image classification accuracy, after discarding images flagged as adversarial, of 99%. Our average detection accuracy exceeded that of recent papers using similar techniques.

</details>

<details>

<summary>2019-09-20 16:26:03 - HybCache: Hybrid Side-Channel-Resilient Caches for Trusted Execution Environments</summary>

- *Ghada Dessouky, Tommaso Frassetto, Ahmad-Reza Sadeghi*

- `1909.09599v1` - [abs](http://arxiv.org/abs/1909.09599v1) - [pdf](http://arxiv.org/pdf/1909.09599v1)

> Modern multi-core processors share cache resources for maximum cache utilization and performance gains. However, this leaves the cache vulnerable to side-channel attacks, where timing differences in shared cache behavior are exploited to infer information on the victim's execution patterns, ultimately leaking private information. The root cause for these attacks is mutually distrusting processes sharing cache entries and accessing them in a deterministic manner. Various defenses against cache side-channel attacks have been proposed. However, they either degrade performance significantly, impose impractical restrictions, or can only defeat certain classes of these attacks. More importantly, they assume that side-channel-resilient caches are required for the entire execution workload and do not allow to selectively enable the mitigation only for the security-critical portion of the workload. We present a generic mechanism for a flexible and soft partitioning of set-associative caches and propose a hybrid cache architecture, called HybCache. HybCache can be configured to selectively apply side-channel-resilient cache behavior only for isolated execution domains, while providing the non-isolated execution with conventional cache behavior, capacity and performance. An isolation domain can include one or more processes, specific portions of code, or a Trusted Execution Environment. We show that, with minimal hardware modifications and kernel support, HybCache can provide side-channel-resilient cache only for isolated execution with a performance overhead of 3.5-5%, while incurring no performance overhead for the remaining execution workload. We provide a simulator-based and hardware implementation of HybCache to evaluate the performance and area overheads, and show how it mitigates typical access-based and contention-based cache attacks.

</details>

<details>

<summary>2019-09-20 17:42:22 - Manipulation Attacks in Local Differential Privacy</summary>

- *Albert Cheu, Adam Smith, Jonathan Ullman*

- `1909.09630v1` - [abs](http://arxiv.org/abs/1909.09630v1) - [pdf](http://arxiv.org/pdf/1909.09630v1)

> Local differential privacy is a widely studied restriction on distributed algorithms that collect aggregates about sensitive user data, and is now deployed in several large systems. We initiate a systematic study of a fundamental limitation of locally differentially private protocols: they are highly vulnerable to adversarial manipulation. While any algorithm can be manipulated by adversaries who lie about their inputs, we show that any non-interactive locally differentially private protocol can be manipulated to a much greater extent. Namely, when the privacy level is high or the input domain is large, an attacker who controls a small fraction of the users in the protocol can completely obscure the distribution of the users' inputs. We also show that existing protocols differ greatly in their resistance to manipulation, even when they offer the same accuracy guarantee with honest execution. Our results suggest caution when deploying local differential privacy and reinforce the importance of efficient cryptographic techniques for emulating mechanisms from central differential privacy in distributed settings.

</details>

<details>

<summary>2019-09-20 18:21:56 - SeMA: A Design Methodology for Building Secure Android Apps</summary>

- *Joydeep Mitra, Venkatesh-Prasad Ranganath*

- `1902.10056v4` - [abs](http://arxiv.org/abs/1902.10056v4) - [pdf](http://arxiv.org/pdf/1902.10056v4)

> UX (user experience) designers visually capture the UX of an app via storyboards. This method is also used in Android app development to conceptualize and design apps.   Recently, security has become an integral part of Android app UX because mobile apps are used to perform critical activities such as banking, communication, and health. Therefore, securing user information is imperative in mobile apps.   In this context, storyboarding tools offer limited capabilities to capture and reason about security requirements of an app. Consequently, security cannot be baked into the app at design time. Hence, vulnerabilities stemming from design flaws can often occur in apps. To address this concern, in this paper, we propose a storyboard based design methodology to enable the specification and verification of security properties of an Android app at design time.

</details>

<details>

<summary>2019-09-21 16:00:29 - Dynamic data fusion using multi-input models for malware classification</summary>

- *Viktor Zenkov, Jason Laska*

- `1910.02021v1` - [abs](http://arxiv.org/abs/1910.02021v1) - [pdf](http://arxiv.org/pdf/1910.02021v1)

> Criminals use malware to disrupt cyber-systems. The number of these malware-vulnerable systems is increasing quickly as common systems, such as vehicles, routers, and lightbulbs, become increasingly interconnected cyber-systems. To address the scale of this problem, analysts divide malware into classes and develop, for each class, a specialized defense. In this project we classified malware with machine learning. In particular, we used a supervised multi-class long short term memory (LSTM) model. We trained the algorithm with thousands of malware files annotated with class labels (the training set), and the algorithm learned patterns indicative of each class. We used disassembled malware files (provided by Microsoft) and separated the constituent data into parsed instructions, which look like human-readable machine code text, and raw bytes, which are hexadecimal values. We are interested in which format, text or hex, is more valuable as input for classification. To solve this, we investigated four cases: a text-only model, a hexadecimal-only model, a multi-input model using both text and hexadecimal inputs, and a model based on combining the individual results. We performed this investigation using the machine learning Python package Keras, which allows easily configurable deep learning architectures and training. We hoped to understand the trade-offs between the different formats. Due to the class imbalance in the data, we used multiple methods to compare the formats, using test accuracies, balanced accuracies (taking into account weights of classes), and an accuracy derived from tables of confusion. We found that the multi-input model, which allows learning on both input types simultaneously, resulted in the best performance. Our finding expedites malware classification research by providing researchers a suitable deep learning architecture to train a tailored version to their malware.

</details>

<details>

<summary>2019-09-22 20:58:15 - Side-Channel Hardware Trojan for Provably-Secure SCA-Protected Implementations</summary>

- *Samaneh Ghandali, Thorben Moos, Amir Moradi, Christof Paar*

- `1910.00737v1` - [abs](http://arxiv.org/abs/1910.00737v1) - [pdf](http://arxiv.org/pdf/1910.00737v1)

> Hardware Trojans have drawn the attention of academia, industry and government agencies. Effective detection mechanisms and countermeasures against such malicious designs can only be developed when there is a deep understanding of how hardware Trojans can be built in practice, in particular Trojans specifically designed to avoid detection. In this work, we present a mechanism to introduce an extremely stealthy hardware Trojan into cryptographic primitives equipped with provably-secure first-order side-channel countermeasures. Once the Trojan is triggered, the malicious design exhibits exploitable side-channel leakage, leading to successful key recovery attacks. Generally, such a Trojan requires neither addition nor removal of any logic which makes it extremely hard to detect. On ASICs, it can be inserted by subtle manipulations at the sub-transistor level and on FPGAs by changing the routing of particular signals, leading to \textbf{zero} logic overhead. The underlying concept is based on modifying a securely-masked hardware implementation in such a way that running the device at a particular clock frequency violates one of its essential properties, leading to exploitable leakage. We apply our technique to a Threshold Implementation of the PRESENT block cipher realized in two different CMOS technologies, and show that triggering the Trojan makes the ASIC prototypes vulnerable.

</details>

<details>

<summary>2019-09-23 07:18:24 - Propagated Perturbation of Adversarial Attack for well-known CNNs: Empirical Study and its Explanation</summary>

- *Jihyeun Yoon, Kyungyul Kim, Jongseong Jang*

- `1909.09263v2` - [abs](http://arxiv.org/abs/1909.09263v2) - [pdf](http://arxiv.org/pdf/1909.09263v2)

> Deep Neural Network based classifiers are known to be vulnerable to perturbations of inputs constructed by an adversarial attack to force misclassification. Most studies have focused on how to make vulnerable noise by gradient based attack methods or to defense model from adversarial attack. The use of the denoiser model is one of a well-known solution to reduce the adversarial noise although classification performance had not significantly improved. In this study, we aim to analyze the propagation of adversarial attack as an explainable AI(XAI) point of view. Specifically, we examine the trend of adversarial perturbations through the CNN architectures. To analyze the propagated perturbation, we measured normalized Euclidean Distance and cosine distance in each CNN layer between the feature map of the perturbed image passed through denoiser and the non-perturbed original image. We used five well-known CNN based classifiers and three gradient-based adversarial attacks. From the experimental results, we observed that in most cases, Euclidean Distance explosively increases in the final fully connected layer while cosine distance fluctuated and disappeared at the last layer. This means that the use of denoiser can decrease the amount of noise. However, it failed to defense accuracy degradation.

</details>

<details>

<summary>2019-09-23 12:45:37 - DR.SGX: Hardening SGX Enclaves against Cache Attacks with Data Location Randomization</summary>

- *Ferdinand Brasser, Srdjan Capkun, Alexandra Dmitrienko, Tommaso Frassetto, Kari Kostiainen, Ahmad-Reza Sadeghi*

- `1709.09917v2` - [abs](http://arxiv.org/abs/1709.09917v2) - [pdf](http://arxiv.org/pdf/1709.09917v2)

> Recent research has demonstrated that Intel's SGX is vulnerable to software-based side-channel attacks. In a common attack, the adversary monitors CPU caches to infer secret-dependent data accesses patterns. Known defenses have major limitations, as they require either error-prone developer assistance, incur extremely high runtime overhead, or prevent only specific attacks. In this paper, we propose data location randomization as a novel defense against side-channel attacks that target data access patterns. Our goal is to break the link between the memory observations by the adversary and the actual data accesses by the victim. We design and implement a compiler-based tool called DR.SGX that instruments the enclave code, permuting data locations at fine granularity. To prevent correlation of repeated memory accesses we periodically re-randomize all enclave data. Our solution requires no developer assistance and strikes the balance between side-channel protection and performance based on an adjustable security parameter.

</details>

<details>

<summary>2019-09-23 13:04:37 - Adversarial Examples: Opportunities and Challenges</summary>

- *Jiliang Zhang, Chen Li*

- `1809.04790v4` - [abs](http://arxiv.org/abs/1809.04790v4) - [pdf](http://arxiv.org/pdf/1809.04790v4)

> Deep neural networks (DNNs) have shown huge superiority over humans in image recognition, speech processing, autonomous vehicles and medical diagnosis. However, recent studies indicate that DNNs are vulnerable to adversarial examples (AEs), which are designed by attackers to fool deep learning models. Different from real examples, AEs can mislead the model to predict incorrect outputs while hardly be distinguished by human eyes, therefore threaten security-critical deep-learning applications. In recent years, the generation and defense of AEs have become a research hotspot in the field of artificial intelligence (AI) security. This article reviews the latest research progress of AEs. First, we introduce the concept, cause, characteristics and evaluation metrics of AEs, then give a survey on the state-of-the-art AE generation methods with the discussion of advantages and disadvantages. After that, we review the existing defenses and discuss their limitations. Finally, future research opportunities and challenges on AEs are prospected.

</details>

<details>

<summary>2019-09-24 17:18:16 - Trick or Heat? Manipulating Critical Temperature-Based Control Systems Using Rectification Attacks</summary>

- *Yazhou Tu, Sara Rampazzi, Bin Hao, Angel Rodriguez, Kevin Fu, Xiali Hei*

- `1904.07110v4` - [abs](http://arxiv.org/abs/1904.07110v4) - [pdf](http://arxiv.org/pdf/1904.07110v4)

> Temperature sensing and control systems are widely used in the closed-loop control of critical processes such as maintaining the thermal stability of patients, or in alarm systems for detecting temperature-related hazards. However, the security of these systems has yet to be completely explored, leaving potential attack surfaces that can be exploited to take control over critical systems.   In this paper we investigate the reliability of temperature-based control systems from a security and safety perspective. We show how unexpected consequences and safety risks can be induced by physical-level attacks on analog temperature sensing components. For instance, we demonstrate that an adversary could remotely manipulate the temperature sensor measurements of an infant incubator to cause potential safety issues, without tampering with the victim system or triggering automatic temperature alarms. This attack exploits the unintended rectification effect that can be induced in operational and instrumentation amplifiers to control the sensor output, tricking the internal control loop of the victim system to heat up or cool down. Furthermore, we show how the exploit of this hardware-level vulnerability could affect different classes of analog sensors that share similar signal conditioning processes.   Our experimental results indicate that conventional defenses commonly deployed in these systems are not sufficient to mitigate the threat, so we propose a prototype design of a low-cost anomaly detector for critical applications to ensure the integrity of temperature sensor signals.

</details>

<details>

<summary>2019-09-24 20:48:10 - Ethical Hacking for IoT Security: A First Look into Bug Bounty Programs and Responsible Disclosure</summary>

- *Aaron Yi Ding, Gianluca Limon De Jesus, Marijn Janssen*

- `1909.11166v1` - [abs](http://arxiv.org/abs/1909.11166v1) - [pdf](http://arxiv.org/pdf/1909.11166v1)

> The security of the Internet of Things (IoT) has attracted much attention due to the growing number of IoT-oriented security incidents. IoT hardware and software security vulnerabilities are exploited affecting many companies and persons. Since the causes of vulnerabilities go beyond pure technical measures, there is a pressing demand nowadays to demystify IoT "security complex" and develop practical guidelines for both companies, consumers, and regulators. In this paper, we present an initial study targeting an unexplored sphere in IoT by illuminating the potential of crowdsource ethical hacking approaches for enhancing IoT vulnerability management. We focus on Bug Bounty Programs (BBP) and Responsible Disclosure (RD), which stimulate hackers to report vulnerability in exchange for monetary rewards. We carried out a qualitative investigation supported by literature survey and expert interviews to explore how BBP and RD can facilitate the practice of identifying, classifying, prioritizing, remediating, and mitigating IoT vulnerabilities in an effective and cost-efficient manner. Besides deriving tangible guidelines for IoT stakeholders, our study also sheds light on a systematic integration path to combine BBP and RD with existing security practices (e.g., penetration test) to further boost overall IoT security.

</details>

<details>

<summary>2019-09-25 00:16:33 - The Attack of the Clones Against Proof-of-Authority</summary>

- *Parinya Ekparinya, Vincent Gramoli, Guillaume Jourjon*

- `1902.10244v3` - [abs](http://arxiv.org/abs/1902.10244v3) - [pdf](http://arxiv.org/pdf/1902.10244v3)

> In this paper, we explore vulnerabilities and countermeasures of the recently proposed blockchain consensus based on proof-of-authority. The proof-of-work blockchains, like Bitcoin and Ethereum, have been shown both theoretically and empirically vulnerable to double spending attacks. This is why Byzantine fault tolerant consensus algorithms have gained popularity in the blockchain context for their ability to tolerate a limited number t of attackers among n participants. We formalize the recently proposed proof-of-authority consensus algorithms that are Byzantine fault tolerant by describing the Aura and Clique protocols present in the two mainstream implementations of Ethereum. We then introduce the Cloning Attack and show how to apply it to double spend in each of these protocols with a single malicious node. Our results show that the Cloning Attack against Aura is always successful while the same attack against Clique is about twice as fast and succeeds in most cases.

</details>

<details>

<summary>2019-09-25 03:13:01 - Quantum Entanglement in Time for a Distributed Ledger</summary>

- *Nils Paz, Steven Silverman, John Harmon*

- `1909.11265v1` - [abs](http://arxiv.org/abs/1909.11265v1) - [pdf](http://arxiv.org/pdf/1909.11265v1)

> Distributed Ledger Technology (DLT) is a shared, synchronized and replicated data spread spatially and temporally with no centralized administration and/or storage. Each node has a complete and identical set of records. All participants contribute to building and maintaining the distributed ledger. Current DLT technologies fall into two broad categories. Those that use block-chains such as in Bitcoin or Ethereum, and newer approaches which reduce computational loads for verification. All current approaches though difficult to crack can be vulnerable to quantum algorithms using Quantum Information Technologies (QIT). This effort joins the 2 technologies, constructing a Quantum Distributed Ledger (QDL) which provides a higher level of security using QIT and a decentralized data depository using DLT. This enhanced security prevents middleman attacks with quantum computers yet retains the advantages of a decentralized ledger of data.

</details>

<details>

<summary>2019-09-25 16:23:27 - Poisoning Attacks with Generative Adversarial Nets</summary>

- *Luis Muñoz-González, Bjarne Pfitzner, Matteo Russo, Javier Carnerero-Cano, Emil C. Lupu*

- `1906.07773v2` - [abs](http://arxiv.org/abs/1906.07773v2) - [pdf](http://arxiv.org/pdf/1906.07773v2)

> Machine learning algorithms are vulnerable to poisoning attacks: An adversary can inject malicious points in the training dataset to influence the learning process and degrade the algorithm's performance. Optimal poisoning attacks have already been proposed to evaluate worst-case scenarios, modelling attacks as a bi-level optimization problem. Solving these problems is computationally demanding and has limited applicability for some models such as deep networks. In this paper we introduce a novel generative model to craft systematic poisoning attacks against machine learning classifiers generating adversarial training examples, i.e. samples that look like genuine data points but that degrade the classifier's accuracy when used for training. We propose a Generative Adversarial Net with three components: generator, discriminator, and the target classifier. This approach allows us to model naturally the detectability constrains that can be expected in realistic attacks and to identify the regions of the underlying data distribution that can be more vulnerable to data poisoning. Our experimental evaluation shows the effectiveness of our attack to compromise machine learning classifiers, including deep networks.

</details>

<details>

<summary>2019-09-25 17:33:46 - Long Short-Term Memory Neural Networks for False Information Attack Detection in Software-Defined In-Vehicle Network</summary>

- *Zadid Khan, Mashrur Chowdhury, Mhafuzul Islam, Chin-Ya Huang, Mizanur Rahman*

- `1906.10203v2` - [abs](http://arxiv.org/abs/1906.10203v2) - [pdf](http://arxiv.org/pdf/1906.10203v2)

> A modern vehicle contains many electronic control units (ECUs), which communicate with each other through the in-vehicle network to ensure vehicle safety and performance. Emerging Connected and Automated Vehicles (CAVs) will have more ECUs and coupling between them due to the vast array of additional sensors, advanced driving features and Vehicle-to-Everything (V2X) connectivity. Due to the connectivity, CAVs will be more vulnerable to remote attackers. In this study, we developed a software-defined in-vehicle Ethernet networking system that provides security against false information attacks. We then created an attack model and attack datasets for false information attacks on brake-related ECUs. After analyzing the attack dataset, we found that the features of the dataset are time-series that have sequential variation patterns. Therefore, we subsequently developed a long short term memory (LSTM) neural network based false information attack/anomaly detection model for the real-time detection of anomalies within the in-vehicle network. This attack detection model can detect false information with an accuracy, precision and recall of 95%, 95% and 87%, respectively, while satisfying the real-time communication and computational requirements.

</details>

<details>

<summary>2019-09-26 01:01:32 - GAMIN: An Adversarial Approach to Black-Box Model Inversion</summary>

- *Ulrich Aïvodji, Sébastien Gambs, Timon Ther*

- `1909.11835v1` - [abs](http://arxiv.org/abs/1909.11835v1) - [pdf](http://arxiv.org/pdf/1909.11835v1)

> Recent works have demonstrated that machine learning models are vulnerable to model inversion attacks, which lead to the exposure of sensitive information contained in their training dataset. While some model inversion attacks have been developed in the past in the black-box attack setting, in which the adversary does not have direct access to the structure of the model, few of these have been conducted so far against complex models such as deep neural networks. In this paper, we introduce GAMIN (for Generative Adversarial Model INversion), a new black-box model inversion attack framework achieving significant results even against deep models such as convolutional neural networks at a reasonable computing cost. GAMIN is based on the continuous training of a surrogate model for the target model under attack and a generator whose objective is to generate inputs resembling those used to train the target model. The attack was validated against various neural networks used as image classifiers. In particular, when attacking models trained on the MNIST dataset, GAMIN is able to extract recognizable digits for up to 60% of labels produced by the target. Attacks against skin classification models trained on the pilot parliament dataset also demonstrated the capacity to extract recognizable features from the targets.

</details>

<details>

<summary>2019-09-26 01:49:00 - P$^2$IM: Scalable and Hardware-independent Firmware Testing via Automatic Peripheral Interface Modeling (extended version)</summary>

- *Bo Feng, Alejandro Mera, Long Lu*

- `1909.06472v3` - [abs](http://arxiv.org/abs/1909.06472v3) - [pdf](http://arxiv.org/pdf/1909.06472v3)

> Dynamic testing or fuzzing of embedded firmware is severely limited by hardware-dependence and poor scalability, partly contributing to the widespread vulnerable IoT devices. We propose a software framework that continuously executes a given firmware binary while channeling inputs from an off-the-shelf fuzzer, enabling hardware-independent and scalable firmware testing. Our framework, using a novel technique called P$^2$IM, abstracts diverse peripherals and handles firmware I/O on the fly based on automatically generated models. P$^2$IM is oblivious to peripheral designs and generic to firmware implementations, and therefore, applicable to a wide range of embedded devices. We evaluated our framework using 70 sample firmware and 10 firmware from real devices, including a drone, a robot, and a PLC. It successfully executed 79% of the sample firmware without any manual assistance. We also performed a limited fuzzing test on the real firmware, which unveiled 7 unique unknown bugs.

</details>

<details>

<summary>2019-09-26 01:54:19 - New Attacks and Defenses for Randomized Caches</summary>

- *Kartik Ramkrishnan, Antonia Zhai, Stephen McCamant, Pen Chung Yew*

- `1909.12302v1` - [abs](http://arxiv.org/abs/1909.12302v1) - [pdf](http://arxiv.org/pdf/1909.12302v1)

> The last level cache is vulnerable to timing based side channel attacks because it is shared by the attacker and the victim processes even if they are located on different cores. These timing attacks evict the victim cache lines using small conflict groups(SCG), and monitor the cache to observe when the victim uses these cache lines again. A conflict group is a collection of cache lines which will evict the target cache line. Randomization is often used by defenses to prevent creation of SCGs.   We introduce new attacks to demonstrate that the current randomization schemes require an extremely high refresh rate to be secure, on average a 15\% performance overhead, and upto 50\% in the worst case. Next, we propose a new randomization strategy using an indirection table, which mitigates this issue. Addresses of cache lines are encrypted and used to lookup the indirection table entry. Each indirection table entry stores a mapping to a randomly chosen cache set. The cache line is placed into this randomly chosen set. The encryption key changes upto 50x faster than CEASER's default rate, by using evictions to trigger the re-randomization. Instead of moving cache lines, this mechanism re-randomizes one iTable entry at a time, whenever the cache lines corresponding to the iTable entry are naturally evicted. Thus, the miss rate is not much worse than the baseline.   We quantitatively show that our scheme does almost as well as a fully associative cache to defend against these attacks. We also demonstrate new attacks that target the iTable by oversubscribing its entries, and quantitatively show that our scheme is resilient against new attacks for trillions of years. We estimate low area ( < 7\%) and power overhead compared to a baseline inclusive last-level cache. Lastly, we evaluate a low performance overhead (<4%) using the SPECrate 2017 and PARSEC 3.0 benchmarks.

</details>

<details>

<summary>2019-09-26 20:46:15 - Global Roaming Trust-based Model for V2X Communications</summary>

- *Alnasser Aljawharah, Sun Hongjian*

- `1909.12381v1` - [abs](http://arxiv.org/abs/1909.12381v1) - [pdf](http://arxiv.org/pdf/1909.12381v1)

> Smart cities need to connect physical devices as a network to improve the efficiency of city operations and services. Intelligent Transportation System (ITS) is one of the key components in smart cities, due to its capability of supporting communications between vehicles to improve the driving experience. Whilst Vehicle-to-Everything (V2X) communications are essential, cyber-security poses a significant challenge in V2X communications. A V2X communication link is vulnerable to various cyber-attacks including internal and external attacks. Internal attacks cannot be detected by conventional security schemes because the compromised nodes have valid credentials. Thus, a new trust model is urgently needed to mitigate cyber-security risks. In this paper, a global roaming trust-based security model is proposed for V2X communications. Each vehicle has a global knowledge about malicious nodes in the network. In addition, various experiments are conducted with different percentage of malicious nodes to measure the performance of the proposed model. Simulation results show that the proposed model improves False Negative Rate (FNR) by 33.5% in comparison with the existing method.

</details>

<details>

<summary>2019-09-27 00:43:28 - Say What I Want: Towards the Dark Side of Neural Dialogue Models</summary>

- *Haochen Liu, Tyler Derr, Zitao Liu, Jiliang Tang*

- `1909.06044v3` - [abs](http://arxiv.org/abs/1909.06044v3) - [pdf](http://arxiv.org/pdf/1909.06044v3)

> Neural dialogue models have been widely adopted in various chatbot applications because of their good performance in simulating and generalizing human conversations. However, there exists a dark side of these models -- due to the vulnerability of neural networks, a neural dialogue model can be manipulated by users to say what they want, which brings in concerns about the security of practical chatbot services. In this work, we investigate whether we can craft inputs that lead a well-trained black-box neural dialogue model to generate targeted outputs. We formulate this as a reinforcement learning (RL) problem and train a Reverse Dialogue Generator which efficiently finds such inputs for targeted outputs. Experiments conducted on a representative neural dialogue model show that our proposed model is able to discover such desired inputs in a considerable portion of cases. Overall, our work reveals this weakness of neural dialogue models and may prompt further researches of developing corresponding solutions to avoid it.

</details>

<details>

<summary>2019-09-28 09:55:00 - Data Sanity Check for Deep Learning Systems via Learnt Assertions</summary>

- *Haochuan Lu, Huanlin Xu, Nana Liu, Yangfan Zhou, Xin Wang*

- `1909.03835v3` - [abs](http://arxiv.org/abs/1909.03835v3) - [pdf](http://arxiv.org/pdf/1909.03835v3)

> Reliability is a critical consideration to DL-based systems. But the statistical nature of DL makes it quite vulnerable to invalid inputs, i.e., those cases that are not considered in the training phase of a DL model. This paper proposes to perform data sanity check to identify invalid inputs, so as to enhance the reliability of DL-based systems. We design and implement a tool to detect behavior deviation of a DL model when processing an input case. This tool extracts the data flow footprints and conducts an assertion-based validation mechanism. The assertions are built automatically, which are specifically-tailored for DL model data flow analysis. Our experiments conducted with real-world scenarios demonstrate that such an assertion-based data sanity check mechanism is effective in identifying invalid input cases.

</details>

<details>

<summary>2019-09-28 21:58:52 - Attacking Graph Convolutional Networks via Rewiring</summary>

- *Yao Ma, Suhang Wang, Tyler Derr, Lingfei Wu, Jiliang Tang*

- `1906.03750v2` - [abs](http://arxiv.org/abs/1906.03750v2) - [pdf](http://arxiv.org/pdf/1906.03750v2)

> Graph Neural Networks (GNNs) have boosted the performance of many graph related tasks such as node classification and graph classification. Recent researches show that graph neural networks are vulnerable to adversarial attacks, which deliberately add carefully created unnoticeable perturbation to the graph structure. The perturbation is usually created by adding/deleting a few edges, which might be noticeable even when the number of edges modified is small. In this paper, we propose a graph rewiring operation which affects the graph in a less noticeable way compared to adding/deleting edges. We then use reinforcement learning to learn the attack strategy based on the proposed rewiring operation. Experiments on real world graphs demonstrate the effectiveness of the proposed framework. To understand the proposed framework, we further analyze how its generated perturbation to the graph structure affects the output of the target model.

</details>

<details>

<summary>2019-09-29 11:11:57 - The Social and Psychological Impact of Cyber-Attacks</summary>

- *Maria Bada, Jason R. C. Nurse*

- `1909.13256v1` - [abs](http://arxiv.org/abs/1909.13256v1) - [pdf](http://arxiv.org/pdf/1909.13256v1)

> Cyber-attacks have become as commonplace as the Internet itself. Each year, industry reports, media outlets and academic articles highlight this increased prevalence, spanning both the amount and variety of attacks and cybercrimes. In this article, we seek to further advance discussions on cyber threats, cognitive vulnerabilities and cyberpsychology through a critical reflection on the social and psychological aspects related to cyber-attacks. In particular, we are interested in understanding how members of the public perceive and engage with risk and how they are impacted during and after a cyber-attack has occurred. This research focuses on key cognitive issues relevant to comprehending public reactions to malicious cyber events including risk perception, protection motivation, culture, and attacker characteristics (e.g., attacker identity, target identity and scale of attack). To consider the applicability of our findings, we investigate two significant cyber-attacks over the last few years, namely the WannaCry attack of 2017 and the Lloyds Banking Group attack in the same year.

</details>

<details>

<summary>2019-09-30 04:04:11 - The Dynamics of Software Composition Analysis</summary>

- *Darius Foo, Jason Yeo, Hao Xiao, Asankhaya Sharma*

- `1909.00973v2` - [abs](http://arxiv.org/abs/1909.00973v2) - [pdf](http://arxiv.org/pdf/1909.00973v2)

> Developers today use significant amounts of open source code, surfacing the need for ways to automatically audit and upgrade library dependencies, and giving rise to the subfield of Software Composition Analysis (SCA). SCA products are concerned with three tasks: discovering dependencies, checking the reachability of vulnerable code for false positive elimination, and automated remediation. The latter two tasks rely on call graphs of application and library code to check whether vulnerability-specific sinks identified in libraries are used by applications. However, statically-constructed call graphs introduce both false positives and false negatives on real-world projects. In this paper, we develop a novel, modular means of combining call graphs derived from both static and dynamic analysis to improve the performance of false positive elimination. Our experiments indicate significant performance improvements.

</details>

<details>

<summary>2019-09-30 13:28:19 - Continuous Flow Analysis to Detect Security Problems</summary>

- *Steven P. Reiss*

- `1909.13683v1` - [abs](http://arxiv.org/abs/1909.13683v1) - [pdf](http://arxiv.org/pdf/1909.13683v1)

> We introduce a tool that supports continuous flow analysis in order to detect security problems as the user edits. The tool uses abstract interpretation over both byte codes and abstract syntax trees to trace the flow of both type annotations and system states from their sources to security problems. The flow analysis achieves a balance between performance and accuracy in order to detect security vulnerabilities within seconds, and uses incremental update to provide immediate feedback to the programmer. Resource files are used to specify the specific security constraints of an application and to tune the analysis. The system can also provide detailed information to the programmer as to why it flagged a particular problem. The tool is integrated into the Code Bubbles development environment.

</details>

<details>

<summary>2019-09-30 13:37:39 - Automated Characterization of Software Vulnerabilities</summary>

- *Danielle Gonzalez, Holly Hastings, Mehdi Mirakhorli*

- `1909.13693v1` - [abs](http://arxiv.org/abs/1909.13693v1) - [pdf](http://arxiv.org/pdf/1909.13693v1)

> Preventing vulnerability exploits is a critical software maintenance task, and software engineers often rely on Common Vulnerability and Exposure (CVEs) reports for information about vulnerable systems and libraries. These reports include descriptions, disclosure sources, and manually-populated vulnerability characteristics such as root cause from the NIST Vulnerability Description Ontology (VDO). This information needs to be complete and accurate so stakeholders of affected products can prevent and react to exploits of the reported vulnerabilities. However, characterizing each report requires significant time and expertise which can lead to inaccurate or incomplete reports. This directly impacts stakeholders ability to quickly and correctly maintain their affected systems. In this study, we demonstrate that VDO characteristics can be automatically detected from the textual descriptions included in CVE reports. We evaluated the performance of 6 classification algorithms with a dataset of 365 vulnerability descriptions, each mapped to 1 of 19 characteristics from the VDO. This work demonstrates that it is feasible to train classification techniques to accurately characterize vulnerabilities from their descriptions. All 6 classifiers evaluated produced accurate results, and the Support Vector Machine classifier was the best-performing individual classifier. Automating the vulnerability characterization process is a step towards ensuring stakeholders have the necessary data to effectively maintain their systems.

</details>

<details>

<summary>2019-09-30 14:36:55 - Exploring how Component Factors and their Uncertainty Affect Judgements of Risk in Cyber-Security</summary>

- *Zack Ellerby, Josie McCulloch, Melanie Wilson, Christian Wagner*

- `1910.00703v1` - [abs](http://arxiv.org/abs/1910.00703v1) - [pdf](http://arxiv.org/pdf/1910.00703v1)

> Subjective judgements from experts provide essential information when assessing and modelling threats in respect to cyber-physical systems. For example, the vulnerability of individual system components can be described using multiple factors, such as complexity, technological maturity, and the availability of tools to aid an attack. Such information is useful for determining attack risk, but much of it is challenging to acquire automatically and instead must be collected through expert assessments. However, most experts inherently carry some degree of uncertainty in their assessments. For example, it is impossible to be certain precisely how many tools are available to aid an attack. Traditional methods of capturing subjective judgements through choices such as \emph{high}, \emph{medium} or \emph{low} do not enable experts to quantify their uncertainty. However, it is important to measure the range of uncertainty surrounding responses in order to appropriately inform system vulnerability analysis. We use a recently introduced interval-valued response-format to capture uncertainty in experts' judgements and employ inferential statistical approaches to analyse the data. We identify key attributes that contribute to hop vulnerability in cyber-systems and demonstrate the value of capturing the uncertainty around these attributes. We find that this uncertainty is not only predictive of uncertainty in the overall vulnerability of a given system component, but also significantly informs ratings of overall component vulnerability itself. We propose that these methods and associated insights can be employed in real world situations, including vulnerability assessments of cyber-physical systems, which are becoming increasingly complex and integrated into society, making them particularly susceptible to uncertainty in assessment.

</details>

<details>

<summary>2019-09-30 18:32:03 - Interpreting Adversarial Examples by Activation Promotion and Suppression</summary>

- *Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu Zhao, Quanfu Fan, Chuang Gan, Xue Lin*

- `1904.02057v2` - [abs](http://arxiv.org/abs/1904.02057v2) - [pdf](http://arxiv.org/pdf/1904.02057v2)

> It is widely known that convolutional neural networks (CNNs) are vulnerable to adversarial examples: images with imperceptible perturbations crafted to fool classifiers. However, interpretability of these perturbations is less explored in the literature. This work aims to better understand the roles of adversarial perturbations and provide visual explanations from pixel, image and network perspectives. We show that adversaries have a promotion-suppression effect (PSE) on neurons' activations and can be primarily categorized into three types: i) suppression-dominated perturbations that mainly reduce the classification score of the true label, ii) promotion-dominated perturbations that focus on boosting the confidence of the target label, and iii) balanced perturbations that play a dual role in suppression and promotion. We also provide image-level interpretability of adversarial examples. This links PSE of pixel-level perturbations to class-specific discriminative image regions localized by class activation mapping (Zhou et al. 2016). Further, we examine the adversarial effect through network dissection (Bau et al. 2017), which offers concept-level interpretability of hidden units. We show that there exists a tight connection between the units' sensitivity to adversarial attacks and their interpretability on semantic concepts. Lastly, we provide some new insights from our interpretation to improve the adversarial robustness of networks.

</details>

<details>

<summary>2019-09-30 18:39:34 - A Privacy-Preserving, Accountable and Spam-Resilient Geo-Marketplace</summary>

- *Kien Nguyen, Gabriel Ghinita, Muhammad Naveed, Cyrus Shahabi*

- `1909.00299v3` - [abs](http://arxiv.org/abs/1909.00299v3) - [pdf](http://arxiv.org/pdf/1909.00299v3)

> Mobile devices with rich features can record videos, traffic parameters or air quality readings along user trajectories. Although such data may be valuable, users are seldom rewarded for collecting them. Emerging digital marketplaces allow owners to advertise their data to interested buyers. We focus on geo-marketplaces, where buyers search data based on geo-tags. Such marketplaces present significant challenges. First, if owners upload data with revealed geo-tags, they expose themselves to serious privacy risks. Second, owners must be accountable for advertised data, and must not be allowed to subsequently alter geo-tags. Third, such a system may be vulnerable to intensive spam activities, where dishonest owners flood the system with fake advertisements. We propose a geo-marketplace that addresses all these concerns. We employ searchable encryption, digital commitments, and blockchain to protect the location privacy of owners while at the same time incorporating accountability and spam-resilience mechanisms. We implement a prototype with two alternative designs that obtain distinct trade-offs between trust assumptions and performance. Our experiments on real location data show that one can achieve the above design goals with practical performance and reasonable financial overhead.

</details>

<details>

<summary>2019-09-30 19:34:45 - Lexical Features Are More Vulnerable, Syntactic Features Have More Predictive Power</summary>

- *Jekaterina Novikova, Aparna Balagopalan, Ksenia Shkaruta, Frank Rudzicz*

- `1910.00065v1` - [abs](http://arxiv.org/abs/1910.00065v1) - [pdf](http://arxiv.org/pdf/1910.00065v1)

> Understanding the vulnerability of linguistic features extracted from noisy text is important for both developing better health text classification models and for interpreting vulnerabilities of natural language models. In this paper, we investigate how generic language characteristics, such as syntax or the lexicon, are impacted by artificial text alterations. The vulnerability of features is analysed from two perspectives: (1) the level of feature value change, and (2) the level of change of feature predictive power as a result of text modifications. We show that lexical features are more sensitive to text modifications than syntactic ones. However, we also demonstrate that these smaller changes of syntactic features have a stronger influence on classification performance downstream, compared to the impact of changes to lexical features. Results are validated across three datasets representing different text-classification tasks, with different levels of lexical and syntactic complexity of both conversational and written language.

</details>


## 2019-10

<details>

<summary>2019-10-01 07:57:42 - Cross-Layer Strategic Ensemble Defense Against Adversarial Examples</summary>

- *Wenqi Wei, Ling Liu, Margaret Loper, Ka-Ho Chow, Emre Gursoy, Stacey Truex, Yanzhao Wu*

- `1910.01742v1` - [abs](http://arxiv.org/abs/1910.01742v1) - [pdf](http://arxiv.org/pdf/1910.01742v1)

> Deep neural network (DNN) has demonstrated its success in multiple domains. However, DNN models are inherently vulnerable to adversarial examples, which are generated by adding adversarial perturbations to benign inputs to fool the DNN model to misclassify. In this paper, we present a cross-layer strategic ensemble framework and a suite of robust defense algorithms, which are attack-independent, and capable of auto-repairing and auto-verifying the target model being attacked. Our strategic ensemble approach makes three original contributions. First, we employ input-transformation diversity to design the input-layer strategic transformation ensemble algorithms. Second, we utilize model-disagreement diversity to develop the output-layer strategic model ensemble algorithms. Finally, we create an input-output cross-layer strategic ensemble defense that strengthens the defensibility by combining diverse input transformation based model ensembles with diverse output verification model ensembles. Evaluated over 10 attacks on ImageNet dataset, we show that our strategic ensemble defense algorithms can achieve high defense success rates and are more robust with high attack prevention success rates and low benign false negative rates, compared to existing representative defense methods.

</details>

<details>

<summary>2019-10-01 10:11:50 - A Look at the Dark Side of Hardware Reverse Engineering -- A Case Study</summary>

- *Sebastian Wallat, Marc Fyrbiak, Moritz Schlögel, Christof Paar*

- `1910.01519v1` - [abs](http://arxiv.org/abs/1910.01519v1) - [pdf](http://arxiv.org/pdf/1910.01519v1)

> A massive threat to the modern and complex IC production chain is the use of untrusted off-shore foundries which are able to infringe valuable hardware design IP or to inject hardware Trojans causing severe loss of safety and security. Similarly, market dominating SRAM-based FPGAs are vulnerable to both attacks since the crucial gate-level netlist can be retrieved even in field for the majority of deployed device series. In order to perform IP infringement or Trojan injection, reverse engineering (parts of) the hardware design is necessary to understand its internal workings. Even though IP protection and obfuscation techniques exist to hinder both attacks, the security of most techniques is doubtful since realistic capabilities of reverse engineering are often neglected. The contribution of our work is twofold: first, we carefully review an IP watermarking scheme tailored to FPGAs and improve its security by using opaque predicates. In addition, we show novel reverse engineering strategies on proposed opaque predicate implementations that again enables to automatically detect and alter watermarks. Second, we demonstrate automatic injection of hardware Trojans specifically tailored for third-party cryptographic IP gate-level netlists. More precisely, we extend our understanding of adversary's capabilities by presenting how block and stream cipher implementations can be surreptitiously weakened.

</details>

<details>

<summary>2019-10-01 11:38:27 - Teaching Hardware Reverse Engineering: Educational Guidelines and Practical Insights</summary>

- *Carina Wiesen, Steffen Becker, Marc Fyrbiak, Nils Albartus, Malte Elson, Nikol Rummel, Christof Paar*

- `1910.00312v1` - [abs](http://arxiv.org/abs/1910.00312v1) - [pdf](http://arxiv.org/pdf/1910.00312v1)

> Since underlying hardware components form the basis of trust in virtually any computing system, security failures in hardware pose a devastating threat to our daily lives. Hardware reverse engineering is commonly employed by security engineers in order to identify security vulnerabilities, to detect IP violations, or to conduct very-large-scale integration (VLSI) failure analysis. Even though industry and the scientific community demand experts with expertise in hardware reverse engineering, there is a lack of educational offerings, and existing training is almost entirely unstructured and on the job. To the best of our knowledge, we have developed the first course to systematically teach students hardware reverse engineering based on insights from the fields of educational research, cognitive science, and hardware security. The contribution of our work is threefold: (1) we propose underlying educational guidelines for practice-oriented courses which teach hardware reverse engineering; (2) we develop such a lab course with a special focus on gate-level netlist reverse engineering and provide the required tools to support it; (3) we conduct an educational evaluation of our pilot course. Based on our results, we provide valuable insights on the structure and content necessary to design and teach future courses on hardware reverse engineering.

</details>

<details>

<summary>2019-10-01 11:58:27 - Attacking CNN-based anti-spoofing face authentication in the physical domain</summary>

- *Bowen Zhang, Benedetta Tondi, Mauro Barni*

- `1910.00327v1` - [abs](http://arxiv.org/abs/1910.00327v1) - [pdf](http://arxiv.org/pdf/1910.00327v1)

> In this paper, we study the vulnerability of anti-spoofing methods based on deep learning against adversarial perturbations. We first show that attacking a CNN-based anti-spoofing face authentication system turns out to be a difficult task. When a spoofed face image is attacked in the physical world, in fact, the attack has not only to remove the rebroadcast artefacts present in the image, but it has also to take into account that the attacked image will be recaptured again and then compensate for the distortions that will be re-introduced after the attack by the subsequent rebroadcast process. Subsequently, we propose a method to craft robust physical domain adversarial images against anti-spoofing CNN-based face authentication. The attack built in this way can successfully pass all the steps in the authentication chain (that is, face detection, face recognition and spoofing detection), by achieving simultaneously the following goals: i) make the spoofing detection fail; ii) let the facial region be detected as a face and iii) recognized as belonging to the victim of the attack. The effectiveness of the proposed attack is validated experimentally within a realistic setting, by considering the REPLAY-MOBILE database, and by feeding the adversarial images to a real face authentication system capturing the input images through a mobile phone camera.

</details>

<details>

<summary>2019-10-01 12:43:55 - Adaptive Generation of Unrestricted Adversarial Inputs</summary>

- *Isaac Dunn, Hadrien Pouget, Tom Melham, Daniel Kroening*

- `1905.02463v2` - [abs](http://arxiv.org/abs/1905.02463v2) - [pdf](http://arxiv.org/pdf/1905.02463v2)

> Neural networks are vulnerable to adversarially-constructed perturbations of their inputs. Most research so far has considered perturbations of a fixed magnitude under some $l_p$ norm. Although studying these attacks is valuable, there has been increasing interest in the construction of (and robustness to) unrestricted attacks, which are not constrained to a small and rather artificial subset of all possible adversarial inputs. We introduce a novel algorithm for generating such unrestricted adversarial inputs which, unlike prior work, is adaptive: it is able to tune its attacks to the classifier being targeted. It also offers a 400-2,000x speedup over the existing state of the art. We demonstrate our approach by generating unrestricted adversarial inputs that fool classifiers robust to perturbation-based attacks. We also show that, by virtue of being adaptive and unrestricted, our attack is able to defeat adversarial training against it.

</details>

<details>

<summary>2019-10-01 15:58:47 - An Analysis of Malware Trends in Enterprise Networks</summary>

- *Abbas Acar, Long Lu, A. Selcuk Uluagac, Engin Kirda*

- `1910.00508v1` - [abs](http://arxiv.org/abs/1910.00508v1) - [pdf](http://arxiv.org/pdf/1910.00508v1)

> We present an empirical and large-scale analysis of malware samples captured from two different enterprises from 2017 to early 2018. Particularly, we perform threat vector, social-engineering, vulnerability and time-series analysis on our dataset. Unlike existing malware studies, our analysis is specifically focused on the recent enterprise malware samples. First of all, based on our analysis on the combined datasets of two enterprises, our results confirm the general consensus that AV-only solutions are not enough for real-time defenses in enterprise settings because on average 40% of the malware samples, when first appeared, are not detected by most AVs on VirusTotal or not uploaded to VT at all (i.e., never seen in the wild yet). Moreover, our analysis also shows that enterprise users transfer documents more than executables and other types of files. Therefore, attackers embed malicious codes into documents to download and install the actual malicious payload instead of sending malicious payload directly or using vulnerability exploits. Moreover, we also found that financial matters (e.g., purchase orders and invoices) are still the most common subject seen in Business Email Compromise (BEC) scams that aim to trick employees. Finally, based on our analysis on the timestamps of captured malware samples, we found that 93% of the malware samples were delivered on weekdays. Our further analysis also showed that while the malware samples that require user interaction such as macro-based malware samples have been captured during the working hours of the employees, the massive malware attacks are triggered during the off-times of the employees to be able to silently spread over the networks.

</details>

<details>

<summary>2019-10-01 17:43:49 - Diminishing the Effect of Adversarial Perturbations via Refining Feature Representation</summary>

- *Nader Asadi, AmirMohammad Sarfi, Mehrdad Hosseinzadeh, Sahba Tahsini, Mahdi Eftekhari*

- `1907.01023v2` - [abs](http://arxiv.org/abs/1907.01023v2) - [pdf](http://arxiv.org/pdf/1907.01023v2)

> Deep neural networks are highly vulnerable to adversarial examples, which imposes severe security issues for these state-of-the-art models. Many defense methods have been proposed to mitigate this problem. However, a lot of them depend on modification or additional training of the target model. In this work, we analytically investigate each layer's representation of non-perturbed and perturbed images and show the effect of perturbations on each of these representations. Accordingly, a method based on whitening coloring transform is proposed in order to diminish the misrepresentation of any desirable layer caused by adversaries. Our method can be applied to any layer of any arbitrary model without the need of any modification or additional training. Due to the fact that the full whitening of the layer's representation is not easily differentiable, our proposed method is superbly robust against white-box attacks. Furthermore, we demonstrate the strength of our method against some state-of-the-art black-box attacks.

</details>

<details>

<summary>2019-10-02 08:09:27 - Machine-Learning Techniques for Detecting Attacks in SDN</summary>

- *Mahmoud Said Elsayed, Nhien-An Le-Khac, Soumyabrata Dev, Anca Delia Jurcut*

- `1910.00817v1` - [abs](http://arxiv.org/abs/1910.00817v1) - [pdf](http://arxiv.org/pdf/1910.00817v1)

> With the advent of Software Defined Networks (SDNs), there has been a rapid advancement in the area of cloud computing. It is now scalable, cheaper, and easier to manage. However, SDNs are more prone to security vulnerabilities as compared to legacy systems. Therefore, machine-learning techniques are now deployed in the SDN infrastructure for the detection of malicious traffic. In this paper, we provide a systematic benchmarking analysis of the existing machine-learning techniques for the detection of malicious traffic in SDNs. We identify the limitations in these classical machine-learning based methods, and lay the foundation for a more robust framework. Our experiments are performed on a publicly available dataset of Intrusion Detection Systems (IDSs).

</details>

<details>

<summary>2019-10-02 17:19:25 - RecordFlux: Formal Message Specification and Generation of Verifiable Binary Parsers</summary>

- *Tobias Reiher, Alexander Senier, Jeronimo Castrillon, Thorsten Strufe*

- `1910.02146v1` - [abs](http://arxiv.org/abs/1910.02146v1) - [pdf](http://arxiv.org/pdf/1910.02146v1)

> Various vulnerabilities have been found in message parsers of protocol implementations in the past. Even highly sensitive software components like TLS libraries are affected regularly. Resulting issues range from denial-of-service attacks to the extraction of sensitive information. The complexity of protocols and imprecise specifications in natural language are the core reasons for subtle bugs in implementations, which are hard to find. The lack of precise specifications impedes formal verification.   In this paper, we propose a model and a corresponding domain-specific language to formally specify message formats of existing real-world binary protocols. A unique feature of the model is the capability to define invariants, which specify relations and dependencies between message fields. Furthermore, the model allows defining the relation of messages between different protocol layers and thus ensures correct interpretation of payload data. We present a technique to derive verifiable parsers based on the model, generate efficient code for their implementation, and automatically prove the absence of runtime errors. Examples of parser specifications for Ethernet and TLS demonstrate the applicability of our approach.

</details>

<details>

<summary>2019-10-02 20:56:55 - Attacking Vision-based Perception in End-to-End Autonomous Driving Models</summary>

- *Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy Vorobeychik, Xuan Zhang*

- `1910.01907v1` - [abs](http://arxiv.org/abs/1910.01907v1) - [pdf](http://arxiv.org/pdf/1910.01907v1)

> Recent advances in machine learning, especially techniques such as deep neural networks, are enabling a range of emerging applications. One such example is autonomous driving, which often relies on deep learning for perception. However, deep learning-based perception has been shown to be vulnerable to a host of subtle adversarial manipulations of images. Nevertheless, the vast majority of such demonstrations focus on perception that is disembodied from end-to-end control. We present novel end-to-end attacks on autonomous driving in simulation, using simple physically realizable attacks: the painting of black lines on the road. These attacks target deep neural network models for end-to-end autonomous driving control. A systematic investigation shows that such attacks are easy to engineer, and we describe scenarios (e.g., right turns) in which they are highly effective. We define several objective functions that quantify the success of an attack and develop techniques based on Bayesian Optimization to efficiently traverse the search space of higher dimensional attacks. Additionally, we define a novel class of hijacking attacks, where painted lines on the road cause the driver-less car to follow a target path. Through the use of network deconvolution, we provide insights into the successful attacks, which appear to work by mimicking activations of entirely different scenarios. Our code is available at https://github.com/xz-group/AdverseDrive

</details>

<details>

<summary>2019-10-03 06:13:59 - Eradicating Attacks on the Internal Network with Internal Network Policy</summary>

- *Yehuda Afek, Anat Bremler-Barr, Alon Noy*

- `1910.00975v2` - [abs](http://arxiv.org/abs/1910.00975v2) - [pdf](http://arxiv.org/pdf/1910.00975v2)

> In this paper we present three attacks on private internal networks behind a NAT and a corresponding new protection mechanism, Internal Network Policy, to mitigate a wide range of attacks that penetrate internal networks behind a NAT. In the attack scenario, a victim is tricked to visit the attacker's website, which contains a malicious script that lets the attacker access the victim's internal network in different ways, including opening a port in the NAT or sending a sophisticated request to local devices. The first attack utilizes DNS Rebinding in a particular way, while the other two demonstrate different methods of attacking the network, based on application security vulnerabilities. Following the attacks, we provide a new browser security policy, Internal Network Policy (INP), which protects against these types of vulnerabilities and attacks. This policy is implemented in the browser just like Same Origin Policy (SOP) and prevents malicious access to internal resources by external entities.

</details>

<details>

<summary>2019-10-03 12:34:08 - Vulnerability of Face Recognition to Deep Morphing</summary>

- *Pavel Korshunov, Sébastien Marcel*

- `1910.01933v1` - [abs](http://arxiv.org/abs/1910.01933v1) - [pdf](http://arxiv.org/pdf/1910.01933v1)

> It is increasingly easy to automatically swap faces in images and video or morph two faces into one using generative adversarial networks (GANs). The high quality of the resulted deep-morph raises the question of how vulnerable the current face recognition systems are to such fake images and videos. It also calls for automated ways to detect these GAN-generated faces. In this paper, we present the publicly available dataset of the Deepfake videos with faces morphed with a GAN-based algorithm. To generate these videos, we used open source software based on GANs, and we emphasize that training and blending parameters can significantly impact the quality of the resulted videos. We show that the state of the art face recognition systems based on VGG and Facenet neural networks are vulnerable to the deep morph videos, with 85.62 and 95.00 false acceptance rates, respectively, which means methods for detecting these videos are necessary. We consider several baseline approaches for detecting deep morphs and find that the method based on visual quality metrics (often used in presentation attack detection domain) leads to the best performance with 8.97 equal error rate. Our experiments demonstrate that GAN-generated deep morph videos are challenging for both face recognition systems and existing detection methods, and the further development of deep morphing technologies will make it even more so.

</details>

<details>

<summary>2019-10-03 19:38:48 - Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics</summary>

- *Yuxin Ma, Tiankai Xie, Jundong Li, Ross Maciejewski*

- `1907.07296v4` - [abs](http://arxiv.org/abs/1907.07296v4) - [pdf](http://arxiv.org/pdf/1907.07296v4)

> Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.

</details>

<details>

<summary>2019-10-04 18:16:11 - Adversarial Examples for Cost-Sensitive Classifiers</summary>

- *Gavin S. Hartnett, Andrew J. Lohn, Alexander P. Sedlack*

- `1910.02095v1` - [abs](http://arxiv.org/abs/1910.02095v1) - [pdf](http://arxiv.org/pdf/1910.02095v1)

> Motivated by safety-critical classification problems, we investigate adversarial attacks against cost-sensitive classifiers. We use current state-of-the-art adversarially-resistant neural network classifiers [1] as the underlying models. Cost-sensitive predictions are then achieved via a final processing step in the feed-forward evaluation of the network. We evaluate the effectiveness of cost-sensitive classifiers against a variety of attacks and we introduce a new cost-sensitive attack which performs better than targeted attacks in some cases. We also explored the measures a defender can take in order to limit their vulnerability to these attacks. This attacker/defender scenario is naturally framed as a two-player zero-sum finite game which we analyze using game theory.

</details>

<details>

<summary>2019-10-04 20:00:12 - Requirements for Developing Robust Neural Networks</summary>

- *John S. Hyatt, Michael S. Lee*

- `1910.02125v1` - [abs](http://arxiv.org/abs/1910.02125v1) - [pdf](http://arxiv.org/pdf/1910.02125v1)

> Validation accuracy is a necessary, but not sufficient, measure of a neural network classifier's quality. High validation accuracy during development does not guarantee that a model is free of serious flaws, such as vulnerability to adversarial attacks or a tendency to misclassify (with high confidence) data it was not trained on. The model may also be incomprehensible to a human or base its decisions on unreasonable criteria. These problems, which are not unique to classifiers, have been the focus of a substantial amount of recent research. However, they are not prioritized during model development, which almost always optimizes on validation accuracy to the exclusion of everything else. The product of this approach is likely to fail in unexpected ways outside of the training environment. We believe that, in addition to validation accuracy, the model development process must give added weight to other performance metrics such as explainability, resistance to adversarial attacks, and overconfidence on out-of-distribution data.

</details>

<details>

<summary>2019-10-04 20:52:22 - HDMI-Walk: Attacking HDMI Distribution Networks via Consumer Electronic Control Protocol</summary>

- *Luis Puche Rondon, Leonardo Babun, Kemal Akkaya, A. Selcuk Uluagac*

- `1910.02139v1` - [abs](http://arxiv.org/abs/1910.02139v1) - [pdf](http://arxiv.org/pdf/1910.02139v1)

> The High Definition Multimedia Interface (HDMI) is the de-facto standard for Audio/Video interfacing between video-enabled devices. Today, almost tens of billions of HDMI devices exist worldwide and are widely used to distribute A/V signals in smart homes, offices, concert halls, and sporting events making HDMI one of the most highly deployed systems in the world. An important component in HDMI is the Consumer Electronics Control (CEC) protocol, which allows for the interaction between devices within an HDMI distribution network. Nonetheless, existing network security mechanisms only protect traditional networking components, leaving CEC outside of their scope. In this work, we identify and tap into CEC protocol vulnerabilities, using them to implement realistic proof-of-work attacks on HDMI distribution networks. We study, how current insecure CEC protocol practices and HDMI distributions may grant an adversary a novel attack surface for HDMI devices otherwise thought to be unreachable. To introduce this novel attack surface, we present HDMI-Walk, which opens a realm of remote and local CEC attacks to HDMI devices. Specifically, with HDMI-Walk, an attacker can perform malicious analysis of devices, eavesdropping, Denial of Service attacks, targeted device attacks, and even facilitate well-known existing attacks through HDMI. With HDMI-Walk, we prove it is feasible for an attacker to gain arbitrary control of HDMI devices. We demonstrate the implementations of both local and remote attacks with commodity HDMI devices. Finally, we discuss security mechanisms to provide impactful and comprehensive security evaluation to these real-world systems while guaranteeing deployability and providing minimal overhead considering the current limitations of the CEC protocol. To the best of our knowledge, this is the first work solely investigating the security of HDMI device distribution networks.

</details>

<details>

<summary>2019-10-05 09:02:47 - Knowledge Transferring via Model Aggregation for Online Social Care</summary>

- *Shaoxiong Ji, Guodong Long, Shirui Pan, Tianqing Zhu, Jing Jiang, Sen Wang, Xue Li*

- `1905.07665v2` - [abs](http://arxiv.org/abs/1905.07665v2) - [pdf](http://arxiv.org/pdf/1905.07665v2)

> The Internet and the Web are being increasingly used in proactive social care to provide people, especially the vulnerable, with a better life and services, and their derived social services generate enormous data. However, the strict protection of privacy makes user's data become an isolated island and limits the predictive performance of standalone clients. To enable effective proactive social care and knowledge sharing within intelligent agents, this paper develops a knowledge transferring framework via model aggregation. Under this framework, distributed clients perform on-device training, and a third-party server integrates multiple clients' models and redistributes to clients for knowledge transferring among users. To improve the generalizability of the knowledge sharing, we further propose a novel model aggregation algorithm, namely the average difference descent aggregation (AvgDiffAgg for short). In particular, to evaluate the effectiveness of the learning algorithm, we use a case study on the early detection and prevention of suicidal ideation, and the experiment results on four datasets derived from social communities demonstrate the effectiveness of the proposed learning method.

</details>

<details>

<summary>2019-10-06 07:54:54 - Is Word Segmentation Necessary for Deep Learning of Chinese Representations?</summary>

- *Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, Jiwei Li*

- `1905.05526v2` - [abs](http://arxiv.org/abs/1905.05526v2) - [pdf](http://arxiv.org/pdf/1905.05526v2)

> Segmenting a chunk of text into words is usually the first step of processing Chinese text, but its necessity has rarely been explored. In this paper, we ask the fundamental question of whether Chinese word segmentation (CWS) is necessary for deep learning-based Chinese Natural Language Processing. We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. Through direct comparisons between these two types of models, we find that char-based models consistently outperform word-based models. Based on these observations, we conduct comprehensive experiments to study why word-based models underperform char-based models in these deep learning-based NLP tasks. We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. We hope this paper could encourage researchers in the community to rethink the necessity of word segmentation in deep learning-based Chinese Natural Language Processing. \footnote{Yuxian Meng and Xiaoya Li contributed equally to this paper.}

</details>

<details>

<summary>2019-10-07 22:04:35 - Iodine: Verifying Constant-Time Execution of Hardware</summary>

- *Klaus v. Gleissenthall, Rami Gökhan Kıcı, Deian Stefan, Ranjit Jhala*

- `1910.03111v1` - [abs](http://arxiv.org/abs/1910.03111v1) - [pdf](http://arxiv.org/pdf/1910.03111v1)

> To be secure, cryptographic algorithms crucially rely on the underlying hardware to avoid inadvertent leakage of secrets through timing side channels. Unfortunately, such timing channels are ubiquitous in modern hardware, due to its labyrinthine fast-paths and optimizations. A promising way to avoid timing vulnerabilities is to devise --- and verify --- conditions under which a hardware design is free of timing variability, i.e., executes in constant-time. In this paper, we present Iodine: a clock precise, constant-time approach to eliminating timing side channels in hardware. Iodine succeeds in verifying various open source hardware designs in seconds and with little developer effort. Iodine also discovered two constant-time violations: one in a floating-point unit and another one in an RSA encryption module.

</details>

<details>

<summary>2019-10-08 18:22:21 - SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations</summary>

- *Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, Jeremy Dawson, Nasser M. Nasrabadi*

- `1910.03624v1` - [abs](http://arxiv.org/abs/1910.03624v1) - [pdf](http://arxiv.org/pdf/1910.03624v1)

> Deep neural networks are susceptible to adversarial manipulations in the input domain. The extent of vulnerability has been explored intensively in cases of $\ell_p$-bounded and $\ell_p$-minimal adversarial perturbations. However, the vulnerability of DNNs to adversarial perturbations with specific statistical properties or frequency-domain characteristics has not been sufficiently explored. In this paper, we study the smoothness of perturbations and propose SmoothFool, a general and computationally efficient framework for computing smooth adversarial perturbations. Through extensive experiments, we validate the efficacy of the proposed method for both the white-box and black-box attack scenarios. In particular, we demonstrate that: (i) there exist extremely smooth adversarial perturbations for well-established and widely used network architectures, (ii) smoothness significantly enhances the robustness of perturbations against state-of-the-art defense mechanisms, (iii) smoothness improves the transferability of adversarial perturbations across both data points and network architectures, and (iv) class categories exhibit a variable range of susceptibility to smooth perturbations. Our results suggest that smooth APs can play a significant role in exploring the vulnerability extent of DNNs to adversarial examples.

</details>

<details>

<summary>2019-10-09 06:44:23 - Adversarial Learning of Deepfakes in Accounting</summary>

- *Marco Schreyer, Timur Sattarov, Bernd Reimer, Damian Borth*

- `1910.03810v1` - [abs](http://arxiv.org/abs/1910.03810v1) - [pdf](http://arxiv.org/pdf/1910.03810v1)

> Nowadays, organizations collect vast quantities of accounting relevant transactions, referred to as 'journal entries', in 'Enterprise Resource Planning' (ERP) systems. The aggregation of those entries ultimately defines an organization's financial statement. To detect potential misstatements and fraud, international audit standards demand auditors to directly assess journal entries using 'Computer Assisted AuditTechniques' (CAATs). At the same time, discoveries in deep learning research revealed that machine learning models are vulnerable to 'adversarial attacks'. It also became evident that such attack techniques can be misused to generate 'Deepfakes' designed to directly attack the perception of humans by creating convincingly altered media content. The research of such developments and their potential impact on the finance and accounting domain is still in its early stage. We believe that it is of vital relevance to investigate how such techniques could be maliciously misused in this sphere. In this work, we show an adversarial attack against CAATs using deep neural networks. We first introduce a real-world 'thread model' designed to camouflage accounting anomalies such as fraudulent journal entries. Second, we show that adversarial autoencoder neural networks are capable of learning a human interpretable model of journal entries that disentangles the entries latent generative factors. Finally, we demonstrate how such a model can be maliciously misused by a perpetrator to generate robust 'adversarial' journal entries that mislead CAATs.

</details>

<details>

<summary>2019-10-09 08:14:58 - Are Adversarial Robustness and Common Perturbation Robustness Independent Attributes ?</summary>

- *Alfred Laugros, Alice Caplier, Matthieu Ospici*

- `1909.02436v2` - [abs](http://arxiv.org/abs/1909.02436v2) - [pdf](http://arxiv.org/pdf/1909.02436v2)

> Neural Networks have been shown to be sensitive to common perturbations such as blur, Gaussian noise, rotations, etc. They are also vulnerable to some artificial malicious corruptions called adversarial examples. The adversarial examples study has recently become very popular and it sometimes even reduces the term "adversarial robustness" to the term "robustness". Yet, we do not know to what extent the adversarial robustness is related to the global robustness. Similarly, we do not know if a robustness to various common perturbations such as translations or contrast losses for instance, could help with adversarial corruptions. We intend to study the links between the robustnesses of neural networks to both perturbations. With our experiments, we provide one of the first benchmark designed to estimate the robustness of neural networks to common perturbations. We show that increasing the robustness to carefully selected common perturbations, can make neural networks more robust to unseen common perturbations. We also prove that adversarial robustness and robustness to common perturbations are independent. Our results make us believe that neural network robustness should be addressed in a broader sense.

</details>

<details>

<summary>2019-10-09 16:24:12 - Diagnosis of Celiac Disease and Environmental Enteropathy on Biopsy Images Using Color Balancing on Convolutional Neural Networks</summary>

- *Kamran Kowsari, Rasoul Sali, Marium N. Khan, William Adorno, S. Asad Ali, Sean R. Moore, Beatrice C. Amadi, Paul Kelly, Sana Syed, Donald E. Brown*

- `1904.05773v5` - [abs](http://arxiv.org/abs/1904.05773v5) - [pdf](http://arxiv.org/pdf/1904.05773v5)

> Celiac Disease (CD) and Environmental Enteropathy (EE) are common causes of malnutrition and adversely impact normal childhood development. CD is an autoimmune disorder that is prevalent worldwide and is caused by an increased sensitivity to gluten. Gluten exposure destructs the small intestinal epithelial barrier, resulting in nutrient mal-absorption and childhood under-nutrition. EE also results in barrier dysfunction but is thought to be caused by an increased vulnerability to infections. EE has been implicated as the predominant cause of under-nutrition, oral vaccine failure, and impaired cognitive development in low-and-middle-income countries. Both conditions require a tissue biopsy for diagnosis, and a major challenge of interpreting clinical biopsy images to differentiate between these gastrointestinal diseases is striking histopathologic overlap between them. In the current study, we propose a convolutional neural network (CNN) to classify duodenal biopsy images from subjects with CD, EE, and healthy controls. We evaluated the performance of our proposed model using a large cohort containing 1000 biopsy images. Our evaluations show that the proposed model achieves an area under ROC of 0.99, 1.00, and 0.97 for CD, EE, and healthy controls, respectively. These results demonstrate the discriminative power of the proposed model in duodenal biopsies classification.

</details>

<details>

<summary>2019-10-09 21:18:53 - Membership Model Inversion Attacks for Deep Networks</summary>

- *Samyadeep Basu, Rauf Izmailov, Chris Mesterharm*

- `1910.04257v1` - [abs](http://arxiv.org/abs/1910.04257v1) - [pdf](http://arxiv.org/pdf/1910.04257v1)

> With the increasing adoption of AI, inherent security and privacy vulnerabilities formachine learning systems are being discovered. One such vulnerability makes itpossible for an adversary to obtain private information about the types of instancesused to train the targeted machine learning model. This so-called model inversionattack is based on sequential leveraging of classification scores towards obtaininghigh confidence representations for various classes. However, for deep networks,such procedures usually lead to unrecognizable representations that are uselessfor the adversary. In this paper, we introduce a more realistic definition of modelinversion, where the adversary is aware of the general purpose of the attackedmodel (for instance, whether it is an OCR system or a facial recognition system),and the goal is to find realistic class representations within the corresponding lower-dimensional manifold (of, respectively, general symbols or general faces). To thatend, we leverage properties of generative adversarial networks for constructinga connected lower-dimensional manifold, and demonstrate the efficiency of ourmodel inversion attack that is carried out within that manifold.

</details>

<details>

<summary>2019-10-10 06:24:18 - Causality and deceit: Do androids watch action movies?</summary>

- *Dusko Pavlovic, Temra Pavlovic*

- `1910.04383v1` - [abs](http://arxiv.org/abs/1910.04383v1) - [pdf](http://arxiv.org/pdf/1910.04383v1)

> We seek causes through science, religion, and in everyday life. We get excited when a big rock causes a big splash, and we get scared when it tumbles without a cause. But our causal cognition is usually biased. The 'why' is influenced by the 'who'. It is influenced by the 'self', and by 'others'. We share rituals, we watch action movies, and we influence each other to believe in the same causes. Human mind is packed with subjectivity because shared cognitive biases bring us together. But they also make us vulnerable.   An artificial mind is deemed to be more objective than the human mind. After many years of science-fiction fantasies about even-minded androids, they are now sold as personal or expert assistants, as brand advocates, as policy or candidate supporters, as network influencers. Artificial agents have been stunningly successful in disseminating artificial causal beliefs among humans. As malicious artificial agents continue to manipulate human cognitive biases, and deceive human communities into ostensive but expansive causal illusions, the hope for defending us has been vested into developing benevolent artificial agents, tasked with preventing and mitigating cognitive distortions inflicted upon us by their malicious cousins. Can the distortions of human causal cognition be corrected on a more solid foundation of artificial causal cognition?   In the present paper, we study a simple model of causal cognition, viewed as a quest for causal models. We show that, under very mild and hard to avoid assumptions, there are always self-confirming causal models, which perpetrate self-deception, and seem to preclude a royal road to objectivity.

</details>

<details>

<summary>2019-10-10 14:48:22 - Universal Adversarial Perturbation for Text Classification</summary>

- *Hang Gao, Tim Oates*

- `1910.04618v1` - [abs](http://arxiv.org/abs/1910.04618v1) - [pdf](http://arxiv.org/pdf/1910.04618v1)

> Given a state-of-the-art deep neural network text classifier, we show the existence of a universal and very small perturbation vector (in the embedding space) that causes natural text to be misclassified with high probability. Unlike images on which a single fixed-size adversarial perturbation can be found, text is of variable length, so we define the "universality" as "token-agnostic", where a single perturbation is applied to each token, resulting in different perturbations of flexible sizes at the sequence level. We propose an algorithm to compute universal adversarial perturbations, and show that the state-of-the-art deep neural networks are highly vulnerable to them, even though they keep the neighborhood of tokens mostly preserved. We also show how to use these adversarial perturbations to generate adversarial text samples. The surprising existence of universal "token-agnostic" adversarial perturbations may reveal important properties of a text classifier.

</details>

<details>

<summary>2019-10-10 14:59:31 - Security analysis of a blockchain-based protocol for the certification of academic credentials</summary>

- *Marco Baldi, Franco Chiaraluce, Migelan Kodra, Luca Spalazzi*

- `1910.04622v1` - [abs](http://arxiv.org/abs/1910.04622v1) - [pdf](http://arxiv.org/pdf/1910.04622v1)

> We consider a blockchain-based protocol for the certification of academic credentials named Blockcerts, which is currently used worldwide for validating digital certificates of competence compliant with the Open Badges standard. We study the certification steps that are performed by the Blockcerts protocol to validate a certificate, and find that they are vulnerable to a certain type of impersonation attacks. More in detail, authentication of the issuing institution is performed by retrieving an unauthenticated issuer profile online, and comparing some data reported there with those included in the issued certificate. We show that, by fabricating a fake issuer profile and generating a suitably altered certificate, an attacker is able to impersonate a legitimate issuer and can produce certificates that cannot be distinguished from originals by the Blockcerts validation procedure. We also propose some possible countermeasures against an attack of this type, which require the use of a classic public key infrastructure or a decentralized identity system integrated with the Blockcerts protocol.

</details>

<details>

<summary>2019-10-11 03:41:14 - SoK: Hardware Security Support for Trustworthy Execution</summary>

- *Lianying Zhao, He Shuang, Shengjie Xu, Wei Huang, Rongzhen Cui, Pushkar Bettadpur, David Lie*

- `1910.04957v1` - [abs](http://arxiv.org/abs/1910.04957v1) - [pdf](http://arxiv.org/pdf/1910.04957v1)

> In recent years, there have emerged many new hardware mechanisms for improving the security of our computer systems. Hardware offers many advantages over pure software approaches: immutability of mechanisms to software attacks, better execution and power efficiency and a smaller interface allowing it to better maintain secrets. This has given birth to a plethora of hardware mechanisms providing trusted execution environments (TEEs), support for integrity checking and memory safety and widespread uses of hardware roots of trust.   In this paper, we systematize these approaches through the lens of abstraction. Abstraction is key to computing systems, and the interface between hardware and software contains many abstractions. We find that these abstractions, when poorly designed, can both obscure information that is needed for security enforcement, as well as reveal information that needs to be kept secret, leading to vulnerabilities. We summarize such vulnerabilities and discuss several research trends of this area.

</details>

<details>

<summary>2019-10-11 11:27:47 - Hardware Security Evaluation of MAX 10 FPGA</summary>

- *Sergei Skorobogatov*

- `1910.05086v1` - [abs](http://arxiv.org/abs/1910.05086v1) - [pdf](http://arxiv.org/pdf/1910.05086v1)

> With the ubiquity of IoT devices there is a growing demand for confidentiality and integrity of data. Solutions based on reconfigurable logic (CPLD or FPGA) have certain advantages over ASIC and MCU/SoC alternatives. Programmable logic devices are ideal for both confidentiality and upgradability purposes. In this context the hardware security aspects of CPLD/FPGA devices are paramount. This paper shows preliminary evaluation of hardware security in Intel MAX 10 devices. These FPGAs are one of the most suitable candidates for applications demanding extensive features and high level of security. Their strong and week security aspects are revealed and some recommendations are suggested to counter possible security vulnerabilities in real designs. This is a feasibility study paper. Its purpose is to highlight the most vulnerable areas to attacks aimed at data extraction and reverse engineering. That way further investigations could be performed on specific areas of concern.

</details>

<details>

<summary>2019-10-11 11:57:57 - Finding Security Vulnerabilities in Unmanned Aerial Vehicles Using Software Verification</summary>

- *Omar M. Alhawi, Mustafa A. Mustafa, Lucas C. Cordeiro*

- `1906.11488v2` - [abs](http://arxiv.org/abs/1906.11488v2) - [pdf](http://arxiv.org/pdf/1906.11488v2)

> The proliferation of Unmanned Aerial Vehicles (UAVs) embedded with vulnerable monolithic software has recently raised serious concerns about their security due to concurrency aspects and fragile communication links. However, verifying security in UAV software based on traditional testing remains an open challenge mainly due to scalability and deployment issues. Here we investigate software verification techniques to detect security vulnerabilities in typical UAVs. In particular, we investigate existing software analyzers and verifiers, which implement fuzzing and bounded model checking (BMC) techniques, to detect memory safety and concurrency errors. We also investigate fragility aspects related to the UAV communication link. All UAV components (e.g., position, velocity, and attitude control) heavily depend on the communication link. Our preliminary results show that fuzzing and BMC techniques can detect various software vulnerabilities, which are of particular interest to ensure security in UAVs. We were able to perform successful cyber-attacks via penetration testing against the UAV both connection and software system. As a result, we demonstrate real cyber-threats with the possibility of exploiting further security vulnerabilities in real-world UAV software in the foreseeable future.

</details>

<details>

<summary>2019-10-11 12:59:43 - Compositional Fuzzing Aided by Targeted Symbolic Execution</summary>

- *Saahil Ognawala, Fabian Kilger, Alexander Pretschner*

- `1903.02981v2` - [abs](http://arxiv.org/abs/1903.02981v2) - [pdf](http://arxiv.org/pdf/1903.02981v2)

> Guided fuzzing has, in recent years, been able to uncover many new vulnerabilities in real-world software due to its fast input mutation strategies guided by path-coverage. However, most fuzzers are unable to achieve high coverage in deeper parts of programs. Moreover, fuzzers heavily rely on the diversity of the seed inputs, often manually provided, to be able to produce meaningful results.   In this paper, we present Wildfire, a novel open-source compositional fuzzing framework. Wildfire finds vulnerabilities by fuzzing isolated functions in a C-program and, then, using targeted symbolic execution it determines the feasibility of exploitation for these vulnerabilities. Based on our evaluation of 23 open-source programs (nearly 1 million LOC), we show that Wildfire, as a result of the increased coverage, finds more true-positives than baseline symbolic execution and fuzzing tools, as well as state-of-the-art coverage-guided tools, in only 10% of the analysis time taken by them. Additionally, Wildfire finds many other potential vulnerabilities whose feasibility can be determined compositionally to confirm if they are false-positives. Wildfire could also reproduce all of the known vulnerabilities and found several previously-unknown vulnerabilities in three open-source libraries.

</details>

<details>

<summary>2019-10-12 06:18:01 - Comments on a recently proposed Privacy Preserving Lightweight Biometric Authentication System for IoT Security</summary>

- *SrinivasaRao SubramanyaRao, Enrique Argones Rua*

- `1910.01446v2` - [abs](http://arxiv.org/abs/1910.01446v2) - [pdf](http://arxiv.org/pdf/1910.01446v2)

> In this paper, we show that a recently published lightweight adaptation of a Fingerprint matching algorithm called the Minutia Cylinder-Code may not be secure as intruders may be able to illegitimately yet successfully authenticate themselves to the system under consideration. We also show that the lightweight adaptation has other privacy related vulnerabilities that make it unsuitable for use in Biometrics. We make it clear that we are neither investigating nor commenting on the security of the original Minutia Cylinder-Code algorithm by itself, rather we highlight the vulnerabilities of the lightweight adaptation. In the process of doing this, we provide a high-level overview of the role of one-way functions in cryptography and biometrics to provide a context to the aforementioned lightweight algorithm and its deficiencies.

</details>

<details>

<summary>2019-10-12 18:23:42 - Statically Detecting Vulnerabilities by Processing Programming Languages as Natural Languages</summary>

- *Ibéria Medeiros, Nuno Neves, Miguel Correia*

- `1910.06826v1` - [abs](http://arxiv.org/abs/1910.06826v1) - [pdf](http://arxiv.org/pdf/1910.06826v1)

> Web applications continue to be a favorite target for hackers due to a combination of wide adoption and rapid deployment cycles, which often lead to the introduction of high impact vulnerabilities. Static analysis tools are important to search for bugs automatically in the program source code, supporting developers on their removal. However, building these tools requires programming the knowledge on how to discover the vulnerabilities. This paper presents an alternative approach in which tools learn to detect flaws automatically by resorting to artificial intelligence concepts, more concretely to natural language processing. The approach employs a sequence model to learn to characterize vulnerabilities based on an annotated corpus. Afterwards, the model is utilized to discover and identify vulnerabilities in the source code. It was implemented in the DEKANT tool and evaluated experimentally with a large set of PHP applications and WordPress plugins. Overall, we found several hundred vulnerabilities belonging to 12 classes of input validation vulnerabilities, where 62 of them were zero-day.

</details>

<details>

<summary>2019-10-14 05:04:37 - Man-in-the-Middle Attacks against Machine Learning Classifiers via Malicious Generative Models</summary>

- *Derui, Wang, Chaoran Li, Sheng Wen, Surya Nepal, Yang Xiang*

- `1910.06838v1` - [abs](http://arxiv.org/abs/1910.06838v1) - [pdf](http://arxiv.org/pdf/1910.06838v1)

> Deep Neural Networks (DNNs) are vulnerable to deliberately crafted adversarial examples. In the past few years, many efforts have been spent on exploring query-optimisation attacks to find adversarial examples of either black-box or white-box DNN models, as well as the defending countermeasures against those attacks. In this work, we explore vulnerabilities of DNN models under the umbrella of Man-in-the-Middle (MitM) attacks, which has not been investigated before. From the perspective of an MitM adversary, the aforementioned adversarial example attacks are not viable anymore. First, such attacks must acquire the outputs from the models by multiple times before actually launching attacks, which is difficult for the MitM adversary in practice. Second, such attacks are one-off and cannot be directly generalised onto new data examples, which decreases the rate of return for the attacker. In contrast, using generative models to craft adversarial examples on the fly can mitigate the drawbacks. However, the adversarial capability of the generative models, such as Variational Auto-Encoder (VAE), has not been extensively studied. Therefore, given a classifier, we investigate using a VAE decoder to either transform benign inputs to their adversarial counterparts or decode outputs from benign VAE encoders to be adversarial examples. The proposed method can endue more capability to MitM attackers. Based on our evaluation, the proposed attack can achieve above 95% success rate on both MNIST and CIFAR10 datasets, which is better or comparable with state-of-the-art query-optimisation attacks. At the meantime, the attack is 104 times faster than the query-optimisation attacks.

</details>

<details>

<summary>2019-10-14 16:46:33 - Using AI/ML to gain situational understanding from passive network observations</summary>

- *D. Verma, S. Calo*

- `1910.06266v1` - [abs](http://arxiv.org/abs/1910.06266v1) - [pdf](http://arxiv.org/pdf/1910.06266v1)

> The data available in the network traffic fromany Government building contains a significant amount ofinformation. An analysis of the traffic can yield insightsand situational understanding about what is happening inthe building. However, the use of traditional network packet inspection, either deep or shallow, is useful for only a limited understanding of the environment, with applicability limited to some aspects of network and security management. If weuse AI/ML based techniques to understand the network traffic, we can gain significant insights which increase our situational awareness of what is happening in the environment.At IBM, we have created a system which uses a combination of network domain knowledge and machine learning techniques to convert network traffic into actionable insights about the on premise environment. These insights include characterization of the communicating devices, discovering unauthorized devices that may violate policy requirements, identifying hidden components and vulnerability points, detecting leakage of sensitive information, and identifying the presence of people and devices.In this paper, we will describe the overall design of this system, the major use-cases that have been identified for it, and the lessons learnt when deploying this system for some of those use-cases

</details>

<details>

<summary>2019-10-15 03:34:24 - Decision Explanation and Feature Importance for Invertible Networks</summary>

- *Juntang Zhuang, Nicha C. Dvornek, Xiaoxiao Li, Junlin Yang, James S. Duncan*

- `1910.00406v2` - [abs](http://arxiv.org/abs/1910.00406v2) - [pdf](http://arxiv.org/pdf/1910.00406v2)

> Deep neural networks are vulnerable to adversarial attacks and hard to interpret because of their black-box nature. The recently proposed invertible network is able to accurately reconstruct the inputs to a layer from its outputs, thus has the potential to unravel the black-box model. An invertible network classifier can be viewed as a two-stage model: (1) invertible transformation from input space to the feature space; (2) a linear classifier in the feature space. We can determine the decision boundary of a linear classifier in the feature space; since the transform is invertible, we can invert the decision boundary from the feature space to the input space. Furthermore, we propose to determine the projection of a data point onto the decision boundary, and define explanation as the difference between data and its projection. Finally, we propose to locally approximate a neural network with its first-order Taylor expansion, and define feature importance using a local linear model. We provide the implementation of our method: \url{https://github.com/juntang-zhuang/explain_invertible}.

</details>

<details>

<summary>2019-10-15 10:41:04 - Coloring the Black Box: Visualizing neural network behavior with a self-introspective model</summary>

- *Arturo Pardo, José A. Gutiérrez-Gutiérrez, José Miguel López-Higuera, Brian W. Pogue, Olga M. Conde*

- `1910.04903v2` - [abs](http://arxiv.org/abs/1910.04903v2) - [pdf](http://arxiv.org/pdf/1910.04903v2)

> The following work presents how autoencoding all the possible hidden activations of a network for a given problem can provide insight about its structure, behavior, and vulnerabilities. The method, termed self-introspection, can show that a trained model showcases similar activation patterns (albeit randomly distributed due to initialization) when shown data belonging to the same category, and classification errors occur in fringe areas where the activations are not as clearly defined, suggesting some form of random, slowly varying, implicit encoding occurring within deep networks, that can be observed with this representation. Additionally, obtaining a low-dimensional representation of all the activations allows for (1) real-time model evaluation in the context of a multiclass classification problem, (2) the rearrangement of all hidden layers by their relevance in obtaining a specific output, and (3) the obtainment of a framework where studying possible counter-measures to noise and adversarial attacks is possible. Self-introspection can show how damaged input data can modify the hidden activations, producing an erroneous response. A few illustrative are implemented for feedforward and convolutional models and the MNIST and CIFAR-10 datasets, showcasing its capabilities as a model evaluation framework.

</details>

<details>

<summary>2019-10-15 12:15:19 - SafeCritic: Collision-Aware Trajectory Prediction</summary>

- *Tessa van der Heiden, Naveen Shankar Nagaraja, Christian Weiss, Efstratios Gavves*

- `1910.06673v1` - [abs](http://arxiv.org/abs/1910.06673v1) - [pdf](http://arxiv.org/pdf/1910.06673v1)

> Navigating complex urban environments safely is a key to realize fully autonomous systems. Predicting future locations of vulnerable road users, such as pedestrians and cyclists, thus, has received a lot of attention in the recent years. While previous works have addressed modeling interactions with the static (obstacles) and dynamic (humans) environment agents, we address an important gap in trajectory prediction. We propose SafeCritic, a model that synergizes generative adversarial networks for generating multiple "real" trajectories with reinforcement learning to generate "safe" trajectories. The Discriminator evaluates the generated candidates on whether they are consistent with the observed inputs. The Critic network is environmentally aware to prune trajectories that are in collision or are in violation with the environment. The auto-encoding loss stabilizes training and prevents mode-collapse. We demonstrate results on two large scale data sets with a considerable improvement over state-of-the-art. We also show that the Critic is able to classify the safety of trajectories.

</details>

<details>

<summary>2019-10-17 07:46:53 - One pixel attack for fooling deep neural networks</summary>

- *Jiawei Su, Danilo Vasconcellos Vargas, Sakurai Kouichi*

- `1710.08864v7` - [abs](http://arxiv.org/abs/1710.08864v7) - [pdf](http://arxiv.org/pdf/1710.08864v7)

> Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.

</details>

<details>

<summary>2019-10-17 23:09:36 - Certifiably Robust Interpretation in Deep Learning</summary>

- *Alexander Levine, Sahil Singla, Soheil Feizi*

- `1905.12105v3` - [abs](http://arxiv.org/abs/1905.12105v3) - [pdf](http://arxiv.org/pdf/1905.12105v3)

> Deep learning interpretation is essential to explain the reasoning behind model predictions. Understanding the robustness of interpretation methods is important especially in sensitive domains such as medical applications since interpretation results are often used in downstream tasks. Although gradient-based saliency maps are popular methods for deep learning interpretation, recent works show that they can be vulnerable to adversarial attacks. In this paper, we address this problem and provide a certifiable defense method for deep learning interpretation. We show that a sparsified version of the popular SmoothGrad method, which computes the average saliency maps over random perturbations of the input, is certifiably robust against adversarial perturbations. We obtain this result by extending recent bounds for certifiably robust smooth classifiers to the interpretation setting. Experiments on ImageNet samples validate our theory.

</details>

<details>

<summary>2019-10-18 01:18:34 - Adversarial Sampling and Training for Semi-Supervised Information Retrieval</summary>

- *Dae Hoon Park, Yi Chang*

- `1811.04155v2` - [abs](http://arxiv.org/abs/1811.04155v2) - [pdf](http://arxiv.org/pdf/1811.04155v2)

> Ad-hoc retrieval models with implicit feedback often have problems, e.g., the imbalanced classes in the data set. Too few clicked documents may hurt generalization ability of the models, whereas too many non-clicked documents may harm effectiveness of the models and efficiency of training. In addition, recent neural network-based models are vulnerable to adversarial examples due to the linear nature in them. To solve the problems at the same time, we propose an adversarial sampling and training framework to learn ad-hoc retrieval models with implicit feedback. Our key idea is (i) to augment clicked examples by adversarial training for better generalization and (ii) to obtain very informational non-clicked examples by adversarial sampling and training. Experiments are performed on benchmark data sets for common ad-hoc retrieval tasks such as Web search, item recommendation, and question answering. Experimental results indicate that the proposed approaches significantly outperform strong baselines especially for high-ranked documents, and they outperform IRGAN in NDCG@5 using only 5% of labeled data for the Web search task.

</details>

<details>

<summary>2019-10-18 01:53:18 - Adversarial Training and Robustness for Multiple Perturbations</summary>

- *Florian Tramèr, Dan Boneh*

- `1904.13000v2` - [abs](http://arxiv.org/abs/1904.13000v2) - [pdf](http://arxiv.org/pdf/1904.13000v2)

> Defenses against adversarial examples, such as adversarial training, are typically tailored to a single perturbation type (e.g., small $\ell_\infty$-noise). For other perturbations, these defenses offer no guarantees and, at times, even increase the model's vulnerability. Our aim is to understand the reasons underlying this robustness trade-off, and to train models that are simultaneously robust to multiple perturbation types. We prove that a trade-off in robustness to different types of $\ell_p$-bounded and spatial perturbations must exist in a natural and simple statistical setting. We corroborate our formal analysis by demonstrating similar robustness trade-offs on MNIST and CIFAR10. Building upon new multi-perturbation adversarial training schemes, and a novel efficient attack for finding $\ell_1$-bounded adversarial examples, we show that no model trained against multiple attacks achieves robustness competitive with that of models trained on each attack individually. In particular, we uncover a pernicious gradient-masking phenomenon on MNIST, which causes adversarial training with first-order $\ell_\infty, \ell_1$ and $\ell_2$ adversaries to achieve merely $50\%$ accuracy. Our results question the viability and computational scalability of extending adversarial robustness, and adversarial training, to multiple perturbation types.

</details>

<details>

<summary>2019-10-18 12:41:15 - Federated Generative Privacy</summary>

- *Aleksei Triastcyn, Boi Faltings*

- `1910.08385v1` - [abs](http://arxiv.org/abs/1910.08385v1) - [pdf](http://arxiv.org/pdf/1910.08385v1)

> In this paper, we propose FedGP, a framework for privacy-preserving data release in the federated learning setting. We use generative adversarial networks, generator components of which are trained by FedAvg algorithm, to draw privacy-preserving artificial data samples and empirically assess the risk of information disclosure. Our experiments show that FedGP is able to generate labelled data of high quality to successfully train and validate supervised models. Finally, we demonstrate that our approach significantly reduces vulnerability of such models to model inversion attacks.

</details>

<details>

<summary>2019-10-18 22:23:00 - n-m-Variant Systems: Adversarial-Resistant Software Rejuvenation for Cloud-Based Web Applications</summary>

- *Isaac Polinsky, Kyle Martin, William Enck, Michael K. Reiter*

- `1910.08648v1` - [abs](http://arxiv.org/abs/1910.08648v1) - [pdf](http://arxiv.org/pdf/1910.08648v1)

> Web servers are a popular target for adversaries as they are publicly accessible and often vulnerable to compromise. Compromises can go unnoticed for months, if not years, and recovery often involves a complete system rebuild. In this paper, we propose n-m-Variant Systems, an adversarial-resistant software rejuvenation framework for cloud-based web applications. We improve the state-of-the-art by introducing a variable m that provides a knob for administrators to tune an environment to balance resource usage, performance overhead, and security guarantees. Using m, security guarantees can be tuned for seconds, minutes, days, or complete resistance. We design and implement an n-m-Variant System prototype to protect a Mediawiki PHP application serving dynamic content from an external SQL persistent storage. Our performance evaluation shows a throughput reduction of 65% for 108 seconds of resistance and 83% for 12 days of resistance to sophisticated adversaries, given appropriate resource allocation. Furthermore, we use theoretical analysis and simulation to characterize the impact of system parameters on resilience to adversaries. Through these efforts, our work demonstrates how properties of cloud-based servers can enhance the integrity of Web servers.

</details>

<details>

<summary>2019-10-19 07:28:39 - Adversarial Attacks on Spoofing Countermeasures of automatic speaker verification</summary>

- *Songxiang Liu, Haibin Wu, Hung-yi Lee, Helen Meng*

- `1910.08716v1` - [abs](http://arxiv.org/abs/1910.08716v1) - [pdf](http://arxiv.org/pdf/1910.08716v1)

> High-performance spoofing countermeasure systems for automatic speaker verification (ASV) have been proposed in the ASVspoof 2019 challenge. However, the robustness of such systems under adversarial attacks has not been studied yet. In this paper, we investigate the vulnerability of spoofing countermeasures for ASV under both white-box and black-box adversarial attacks with the fast gradient sign method (FGSM) and the projected gradient descent (PGD) method. We implement high-performing countermeasure models in the ASVspoof 2019 challenge and conduct adversarial attacks on them. We compare performance of black-box attacks across spoofing countermeasure models with different network architectures and different amount of model parameters. The experimental results show that all implemented countermeasure models are vulnerable to FGSM and PGD attacks under the scenario of white-box attack. The more dangerous black-box attacks also prove to be effective by the experimental results.

</details>

<details>

<summary>2019-10-19 10:37:35 - Cryptanalysis of a Chaos-Based Fast Image Encryption Algorithm for Embedded Systems</summary>

- *Imad El Hanouti, Hakim El Fadili, Khalid Zenkouar*

- `1910.11679v1` - [abs](http://arxiv.org/abs/1910.11679v1) - [pdf](http://arxiv.org/pdf/1910.11679v1)

> Fairly recently, a new encryption scheme for embedded systems based on continuous third-order hyperbolic sine chaotic system was proposed by Z. Lin et al. The cryptosystem's main objective is to provide a faster algorithm with lowest computational time in order to be qualified for use in embedded systems especially on a program of UAV (unmanned aerial vehicle). In this paper, we scrutinize the design architecture of this recently proposed scheme against conventional attacks e.g., chosen plaintext attack, differential attack, known plaintext attack. We prove in this paper that, negatively, the studied system is vulnerable. For differential attack, only two chosen plain images are required to recover the full equivalent key. Moreover, only one 3x400 size image is sufficient to break the cryptosystem under chosen plaintext attack considering stability of sort algorithm. Therefore, the proposed scheme is not recommended for security purposes.

</details>

<details>

<summary>2019-10-19 10:39:25 - Security analysis of an audio data encryption scheme based on key chaining and DNA encoding</summary>

- *Imad El Hanouti, Hakim El Fadili*

- `1910.11677v1` - [abs](http://arxiv.org/abs/1910.11677v1) - [pdf](http://arxiv.org/pdf/1910.11677v1)

> Fairly recently, a new encryption scheme for audio data encryption has been proposed by Naskar, P.K., et al. The cryptosystem is based on substitution-permutation encryption structure using DNA encoding at the substitution stage, in which the key generation is based on a key chaining algorithm that generates new key block for every plain block using a logistic chaotic map. After some several statistical tests done by the authors of the scheme, they claimed that their cryptosystem is robust and can resist conventional cryptanalysis attacks. Negatively, in this paper we show the opposite: the scheme is extremely weak against chosen ciphertext and plaintext attacks thus only two chosen plaintexts of 32 byte size are sufficient to recover the equivalent key used for encryption. The cryptosystem's shuffling process design is vulnerable which allow us recovering the unknown original plaintext by applying repeated encryptions. Our study proves that the scheme is extremely weak and should not be used for any information security or cryptographic concern. Lessons learned from this cryptanalytic paper are then outlined in order to be considered in further designs and proposals.

</details>

<details>

<summary>2019-10-20 05:51:37 - Secure and Utility-Aware Data Collection with Condensed Local Differential Privacy</summary>

- *Mehmet Emre Gursoy, Acar Tamersoy, Stacey Truex, Wenqi Wei, Ling Liu*

- `1905.06361v2` - [abs](http://arxiv.org/abs/1905.06361v2) - [pdf](http://arxiv.org/pdf/1905.06361v2)

> Local Differential Privacy (LDP) is popularly used in practice for privacy-preserving data collection. Although existing LDP protocols offer high utility for large user populations (100,000 or more users), they perform poorly in scenarios with small user populations (such as those in the cybersecurity domain) and lack perturbation mechanisms that are effective for both ordinal and non-ordinal item sequences while protecting sequence length and content simultaneously. In this paper, we address the small user population problem by introducing the concept of Condensed Local Differential Privacy (CLDP) as a specialization of LDP, and develop a suite of CLDP protocols that offer desirable statistical utility while preserving privacy. Our protocols support different types of client data, ranging from ordinal data types in finite metric spaces (numeric malware infection statistics), to non-ordinal items (OS versions, transaction categories), and to sequences of ordinal and non-ordinal items. Extensive experiments are conducted on multiple datasets, including datasets that are an order of magnitude smaller than those used in existing approaches, which show that proposed CLDP protocols yield high utility. Furthermore, case studies with Symantec datasets demonstrate that our protocols accurately support key cybersecurity-focused tasks of detecting ransomware outbreaks, identifying targeted and vulnerable OSs, and inspecting suspicious activities on infected machines.

</details>

<details>

<summary>2019-10-21 12:34:53 - Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets</summary>

- *Ohad Rozen, Vered Shwartz, Roee Aharoni, Ido Dagan*

- `1910.09302v1` - [abs](http://arxiv.org/abs/1910.09302v1) - [pdf](http://arxiv.org/pdf/1910.09302v1)

> Phenomenon-specific "adversarial" datasets have been recently designed to perform targeted stress-tests for particular inference types. Recent work (Liu et al., 2019a) proposed that such datasets can be utilized for training NLI and other types of models, often allowing to learn the phenomenon in focus and improve on the challenge dataset, indicating a "blind spot" in the original training data. Yet, although a model can improve in such a training process, it might still be vulnerable to other challenge datasets targeting the same phenomenon but drawn from a different distribution, such as having a different syntactic complexity level. In this work, we extend this method to drive conclusions about a model's ability to learn and generalize a target phenomenon rather than to "learn" a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena - dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements.

</details>

<details>

<summary>2019-10-21 17:00:50 - Enforcing Linearity in DNN succours Robustness and Adversarial Image Generation</summary>

- *Anindya Sarkar, Nikhil Kumar Gupta, Raghu Iyengar*

- `1910.08108v2` - [abs](http://arxiv.org/abs/1910.08108v2) - [pdf](http://arxiv.org/pdf/1910.08108v2)

> Recent studies on the adversarial vulnerability of neural networks have shown that models trained with the objective of minimizing an upper bound on the worst-case loss over all possible adversarial perturbations improve robustness against adversarial attacks. Beside exploiting adversarial training framework, we show that by enforcing a Deep Neural Network (DNN) to be linear in transformed input and feature space improves robustness significantly. We also demonstrate that by augmenting the objective function with Local Lipschitz regularizer boost robustness of the model further. Our method outperforms most sophisticated adversarial training methods and achieves state of the art adversarial accuracy on MNIST, CIFAR10 and SVHN dataset. In this paper, we also propose a novel adversarial image generation method by leveraging Inverse Representation Learning and Linearity aspect of an adversarially trained deep neural network classifier.

</details>

<details>

<summary>2019-10-21 18:30:03 - GraphSAC: Detecting anomalies in large-scale graphs</summary>

- *Vassilis N. Ioannidis, Dimitris Berberidis, Georgios B. Giannakis*

- `1910.09589v1` - [abs](http://arxiv.org/abs/1910.09589v1) - [pdf](http://arxiv.org/pdf/1910.09589v1)

> A graph-based sampling and consensus (GraphSAC) approach is introduced to effectively detect anomalous nodes in large-scale graphs. Existing approaches rely on connectivity and attributes of all nodes to assign an anomaly score per node. However, nodal attributes and network links might be compromised by adversaries, rendering these holistic approaches vulnerable. Alleviating this limitation, GraphSAC randomly draws subsets of nodes, and relies on graph-aware criteria to judiciously filter out sets contaminated by anomalous nodes, before employing a semi-supervised learning (SSL) module to estimate nominal label distributions per node. These learned nominal distributions are minimally affected by the anomalous nodes, and hence can be directly adopted for anomaly detection. Rigorous analysis provides performance guarantees for GraphSAC, by bounding the required number of draws. The per-draw complexity grows linearly with the number of edges, which implies efficient SSL, while draws can be run in parallel, thereby ensuring scalability to large graphs. GraphSAC is tested under different anomaly generation models based on random walks, clustered anomalies, as well as contemporary adversarial attacks for graph data. Experiments with real-world graphs showcase the advantage of GraphSAC relative to state-of-the-art alternatives.

</details>

<details>

<summary>2019-10-21 18:30:11 - Edge Dithering for Robust Adaptive Graph Convolutional Networks</summary>

- *Vassilis N. Ioannidis, Georgios B. Giannakis*

- `1910.09590v1` - [abs](http://arxiv.org/abs/1910.09590v1) - [pdf](http://arxiv.org/pdf/1910.09590v1)

> Graph convolutional networks (GCNs) are vulnerable to perturbations of the graph structure that are either random, or, adversarially designed. The perturbed links modify the graph neighborhoods, which critically affects the performance of GCNs in semi-supervised learning (SSL) tasks. Aiming at robustifying GCNs conditioned on the perturbed graph, the present paper generates multiple auxiliary graphs, each having its binary 0-1 edge weights flip values with probabilities designed to enhance robustness. The resultant edge-dithered auxiliary graphs are leveraged by an adaptive (A)GCN that performs SSL. Robustness is enabled through learnable graph-combining weights along with suitable regularizers. Relative to GCN, the novel AGCN achieves markedly improved performance in tests with noisy inputs, graph perturbations, and state-of-the-art adversarial attacks. Further experiments with protein interaction networks showcase the competitive performance of AGCN for SSL over multiple graphs.

</details>

<details>

<summary>2019-10-21 20:40:54 - The SWAX Benchmark: Attacking Biometric Systems with Wax Figures</summary>

- *Rafael Henrique Vareto, Araceli Marcia Sandanha, William Robson Schwartz*

- `1910.09642v1` - [abs](http://arxiv.org/abs/1910.09642v1) - [pdf](http://arxiv.org/pdf/1910.09642v1)

> A face spoofing attack occurs when an intruder attempts to impersonate someone who carries a gainful authentication clearance. It is a trending topic due to the increasing demand for biometric authentication on mobile devices, high-security areas, among others. This work introduces a new database named Sense Wax Attack dataset (SWAX), comprised of real human and wax figure images and videos that endorse the problem of face spoofing detection. The dataset consists of more than 1800 face images and 110 videos of 55 people/waxworks, arranged in training, validation and test sets with a large range in expression, illumination and pose variations. Experiments performed with baseline methods show that despite the progress in recent years, advanced spoofing methods are still vulnerable to high-quality violation attempts.

</details>

<details>

<summary>2019-10-22 03:47:39 - Unsupervised Boosting-based Autoencoder Ensembles for Outlier Detection</summary>

- *Hamed Sarvari, Carlotta Domeniconi, Bardh Prenkaj, Giovanni Stilo*

- `1910.09754v1` - [abs](http://arxiv.org/abs/1910.09754v1) - [pdf](http://arxiv.org/pdf/1910.09754v1)

> Autoencoders, as a dimensionality reduction technique, have been recently applied to outlier detection. However, neural networks are known to be vulnerable to overfitting, and therefore have limited potential in the unsupervised outlier detection setting. Current approaches to ensemble-based autoencoders do not generate a sufficient level of diversity to avoid the overfitting issue. To overcome the aforementioned limitations we develop a Boosting-based Autoencoder Ensemble approach (in short, BAE). BAE is an unsupervised ensemble method that, similarly to the boosting approach, builds an adaptive cascade of autoencoders to achieve improved and robust results. BAE trains the autoencoder components sequentially by performing a weighted sampling of the data, aimed at reducing the amount of outliers used during training, and at injecting diversity in the ensemble. We perform extensive experiments and show that the proposed methodology outperforms state-of-the-art approaches under a variety of conditions.

</details>

<details>

<summary>2019-10-22 14:46:00 - Adversarial Example Detection by Classification for Deep Speech Recognition</summary>

- *Saeid Samizade, Zheng-Hua Tan, Chao Shen, Xiaohong Guan*

- `1910.10013v1` - [abs](http://arxiv.org/abs/1910.10013v1) - [pdf](http://arxiv.org/pdf/1910.10013v1)

> Machine Learning systems are vulnerable to adversarial attacks and will highly likely produce incorrect outputs under these attacks. There are white-box and black-box attacks regarding to adversary's access level to the victim learning algorithm. To defend the learning systems from these attacks, existing methods in the speech domain focus on modifying input signals and testing the behaviours of speech recognizers. We, however, formulate the defense as a classification problem and present a strategy for systematically generating adversarial example datasets: one for white-box attacks and one for black-box attacks, containing both adversarial and normal examples. The white-box attack is a gradient-based method on Baidu DeepSpeech with the Mozilla Common Voice database while the black-box attack is a gradient-free method on a deep model-based keyword spotting system with the Google Speech Command dataset. The generated datasets are used to train a proposed Convolutional Neural Network (CNN), together with cepstral features, to detect adversarial examples. Experimental results show that, it is possible to accurately distinct between adversarial and normal examples for known attacks, in both single-condition and multi-condition training settings, while the performance degrades dramatically for unknown attacks. The adversarial datasets and the source code are made publicly available.

</details>

<details>

<summary>2019-10-23 19:54:27 - Wasserstein Smoothing: Certified Robustness against Wasserstein Adversarial Attacks</summary>

- *Alexander Levine, Soheil Feizi*

- `1910.10783v1` - [abs](http://arxiv.org/abs/1910.10783v1) - [pdf](http://arxiv.org/pdf/1910.10783v1)

> In the last couple of years, several adversarial attack methods based on different threat models have been proposed for the image classification problem. Most existing defenses consider additive threat models in which sample perturbations have bounded L_p norms. These defenses, however, can be vulnerable against adversarial attacks under non-additive threat models. An example of an attack method based on a non-additive threat model is the Wasserstein adversarial attack proposed by Wong et al. (2019), where the distance between an image and its adversarial example is determined by the Wasserstein metric ("earth-mover distance") between their normalized pixel intensities. Until now, there has been no certifiable defense against this type of attack. In this work, we propose the first defense with certified robustness against Wasserstein Adversarial attacks using randomized smoothing. We develop this certificate by considering the space of possible flows between images, and representing this space such that Wasserstein distance between images is upper-bounded by L_1 distance in this flow-space. We can then apply existing randomized smoothing certificates for the L_1 metric. In MNIST and CIFAR-10 datasets, we find that our proposed defense is also practically effective, demonstrating significantly improved accuracy under Wasserstein adversarial attack compared to unprotected models.

</details>

<details>

<summary>2019-10-26 06:23:58 - Denoising and Verification Cross-Layer Ensemble Against Black-box Adversarial Attacks</summary>

- *Ka-Ho Chow, Wenqi Wei, Yanzhao Wu, Ling Liu*

- `1908.07667v2` - [abs](http://arxiv.org/abs/1908.07667v2) - [pdf](http://arxiv.org/pdf/1908.07667v2)

> Deep neural networks (DNNs) have demonstrated impressive performance on many challenging machine learning tasks. However, DNNs are vulnerable to adversarial inputs generated by adding maliciously crafted perturbations to the benign inputs. As a growing number of attacks have been reported to generate adversarial inputs of varying sophistication, the defense-attack arms race has been accelerated. In this paper, we present MODEF, a cross-layer model diversity ensemble framework. MODEF intelligently combines unsupervised model denoising ensemble with supervised model verification ensemble by quantifying model diversity, aiming to boost the robustness of the target model against adversarial examples. Evaluated using eleven representative attacks on popular benchmark datasets, we show that MODEF achieves remarkable defense success rates, compared with existing defense methods, and provides a superior capability of repairing adversarial inputs and making correct predictions with high accuracy in the presence of black-box attacks.

</details>

<details>

<summary>2019-10-26 08:40:56 - DDM: A Demand-based Dynamic Mitigation for SMT Transient Channels</summary>

- *Yue Zhang, Ziyuan Zhu, Dan Meng*

- `1910.12021v1` - [abs](http://arxiv.org/abs/1910.12021v1) - [pdf](http://arxiv.org/pdf/1910.12021v1)

> Different from the traditional software vulnerability, the microarchitecture side channel has three characteristics: extensive influence, potent threat, and tough defense. The main reason for the micro-architecture side channel is resource sharing. There are many reasons for resource sharing, one of which is SMT (Simultaneous Multi-Threading) technology. In this paper, we define the SMT Transient Channel, which uses the transient state of shared resources between threads to steal information. To mitigate it, we designed a security demand-based dynamic mitigation (DDM) to Mitigate the SMT transient channels. The DDM writes the processes' security requirements to the CPU register sets, and the operating system calls the HLT instruction to dynamically turn on and off the hyper-threading according to the register values to avoid the side channels caused by execution resource sharing. During the implementation of the scheme, we modified the Linux kernel and used the MSR register groups of Intel processor. The evaluation results show that DDM can effectively protect against the transient side-channel attacks such as PortsMash that rely on SMT, and the performance loss of DDM is less than 8%.

</details>

<details>

<summary>2019-10-26 23:54:21 - Improving Vulnerability Inspection Efficiency Using Active Learning</summary>

- *Zhe Yu, Christopher Theisen, Laurie Williams, Tim Menzies*

- `1803.06545v3` - [abs](http://arxiv.org/abs/1803.06545v3) - [pdf](http://arxiv.org/pdf/1803.06545v3)

> Software engineers can find vulnerabilities with less effort if they are directed towards code that might contain more vulnerabilities. HARMLESS is an incremental support vector machine tool that builds a vulnerability prediction model from the sourcecode inspected to date, then suggests what source code files should be inspected next. In this way, HARMLESS can reduce the time and effort required to achieve some desired level of recall for finding vulnerabilities. The tool also provides feedback on when to stop (at that desired level of recall) while at the same time, correcting human errors by double-checking suspicious files.   This paper evaluates HARMLESS on Mozilla Firefox vulnerability data. HARMLESS found 80, 90, 95, 99% of the vulnerabilities by inspecting 10, 16, 20, 34% of the source code files. When targeting 90, 95, 99% recall, HARMLESS could stop after inspecting 23, 30, 47% of the source code files. Even when human reviewers fail to identify half of the vulnerabilities (50% false negative rate), HARMLESScould detect 96% of the missing vulnerabilities by double-checking half of the inspected files.   Our results serve to highlight the very steep cost of protecting software from vulnerabilities (in our case study that cost is, for example, the human effort of inspecting 28,750$\times$20% = 5,750 source code files to identify 95% of the vulnerabilities). While this result could benefit the mission-critical projects where human resources are available for inspecting thousands of source code files, the research challenge for future work is how to further reduce that cost. The conclusion of this paper discusses various ways that goal might be achieved.

</details>

<details>

<summary>2019-10-27 01:57:16 - Understanding and Quantifying Adversarial Examples Existence in Linear Classification</summary>

- *Xupeng Shi, A. Adam Ding*

- `1910.12163v1` - [abs](http://arxiv.org/abs/1910.12163v1) - [pdf](http://arxiv.org/pdf/1910.12163v1)

> State-of-art deep neural networks (DNN) are vulnerable to attacks by adversarial examples: a carefully designed small perturbation to the input, that is imperceptible to human, can mislead DNN. To understand the root cause of adversarial examples, we quantify the probability of adversarial example existence for linear classifiers. Previous mathematical definition of adversarial examples only involves the overall perturbation amount, and we propose a more practical relevant definition of strong adversarial examples that separately limits the perturbation along the signal direction also. We show that linear classifiers can be made robust to strong adversarial examples attack in cases where no adversarial robust linear classifiers exist under the previous definition. The quantitative formulas are confirmed by numerical experiments using a linear support vector machine (SVM) classifier. The results suggest that designing general strong-adversarial-robust learning systems is feasible but only through incorporating human knowledge of the underlying classification problem.

</details>

<details>

<summary>2019-10-27 03:21:25 - Eavesdrop the Composition Proportion of Training Labels in Federated Learning</summary>

- *Lixu Wang, Shichao Xu, Xiao Wang, Qi Zhu*

- `1910.06044v2` - [abs](http://arxiv.org/abs/1910.06044v2) - [pdf](http://arxiv.org/pdf/1910.06044v2)

> Federated learning (FL) has recently emerged as a new form of collaborative machine learning, where a common model can be learned while keeping all the training data on local devices. Although it is designed for enhancing the data privacy, we demonstrated in this paper a new direction in inference attacks in the context of FL, where valuable information about training data can be obtained by adversaries with very limited power. In particular, we proposed three new types of attacks to exploit this vulnerability. The first type of attack, Class Sniffing, can detect whether a certain label appears in training. The other two types of attacks can determine the quantity of each label, i.e., Quantity Inference attack determines the composition proportion of the training label owned by the selected clients in a single round, while Whole Determination attack determines that of the whole training process. We evaluated our attacks on a variety of tasks and datasets with different settings, and the corresponding results showed that our attacks work well generally. Finally, we analyzed the impact of major hyper-parameters to our attacks and discussed possible defenses.

</details>

<details>

<summary>2019-10-29 07:50:02 - Algorithmic decision-making in AVs: Understanding ethical and technical concerns for smart cities</summary>

- *Hazel Si Min Lim, Araz Taeihagh*

- `1910.13122v1` - [abs](http://arxiv.org/abs/1910.13122v1) - [pdf](http://arxiv.org/pdf/1910.13122v1)

> Autonomous Vehicles (AVs) are increasingly embraced around the world to advance smart mobility and more broadly, smart, and sustainable cities. Algorithms form the basis of decision-making in AVs, allowing them to perform driving tasks autonomously, efficiently, and more safely than human drivers and offering various economic, social, and environmental benefits. However, algorithmic decision-making in AVs can also introduce new issues that create new safety risks and perpetuate discrimination. We identify bias, ethics, and perverse incentives as key ethical issues in the AV algorithms' decision-making that can create new safety risks and discriminatory outcomes. Technical issues in the AVs' perception, decision-making and control algorithms, limitations of existing AV testing and verification methods, and cybersecurity vulnerabilities can also undermine the performance of the AV system. This article investigates the ethical and technical concerns surrounding algorithmic decision-making in AVs by exploring how driving decisions can perpetuate discrimination and create new safety risks for the public. We discuss steps taken to address these issues, highlight the existing research gaps and the need to mitigate these issues through the design of AV's algorithms and of policies and regulations to fully realise AVs' benefits for smart and sustainable cities.

</details>

<details>

<summary>2019-10-29 15:09:54 - Security of the Internet of Things: Vulnerabilities, Attacks and Countermeasures</summary>

- *Ismail Butun, Patrik Österberg, Houbing Song*

- `1910.13312v1` - [abs](http://arxiv.org/abs/1910.13312v1) - [pdf](http://arxiv.org/pdf/1910.13312v1)

> Wireless Sensor Networks (WSNs) constitute one of the most promising third-millennium technologies and have a wide range of applications in our surrounding environment. The reason behind the vast adoption of WSNs in various applications is that they have tremendously appealing features, e.g., low production cost, low installation cost, unattended network operation, autonomous and longtime operation. WSNs have started to merge with the Internet of Things (IoT) through the introduction of Internet access capability in sensor nodes and sensing ability in Internet-connected devices. Thereby, the IoT is providing access to huge amount of data, collected by the WSNs, over the Internet. However, owing to the absence of a physical line-of-defense, i.e. there is no dedicated infrastructure such as gateways to watch and observe the flowing information in the network, security of WSNs along with IoT is of a big concern to the scientific community. Besides, recent integration and collaboration of WSNs with IoT will open new challenges and problems in terms of security. Hence, this would be a nightmare for the individuals using these systems as well as the security administrators who are managing those networks. Therefore, a detailed review of security attacks towards WSNs and IoT, along with the techniques for prevention, detection, and mitigation of those attacks are provided in this paper. In this text, attacks are categorized and treated into mainly two parts, most or all types of attacks towards WSNs and IoT are investigated under that umbrella: "Passive Attacks" and "Active Attacks". Understanding these attacks and their associated defense mechanisms will help to pave a secure path towards the proliferation and public acceptance of IoT technology.

</details>

<details>

<summary>2019-10-30 10:59:55 - Breaking and (Partially) Fixing Provably Secure Onion Routing</summary>

- *Christiane Kuhn, Martin Beck, Thorsten Strufe*

- `1910.13772v1` - [abs](http://arxiv.org/abs/1910.13772v1) - [pdf](http://arxiv.org/pdf/1910.13772v1)

> After several years of research on onion routing, Camenisch and Lysyanskaya, in an attempt at rigorous analysis, defined an ideal functionality in the universal composability model, together with properties that protocols have to meet to achieve provable security. A whole family of systems based their security proofs on this work. However, analyzing HORNET and Sphinx, two instances from this family, we show that this proof strategy is broken. We discover a previously unknown vulnerability that breaks anonymity completely, and explain a known one. Both should not exist if privacy is proven correctly. In this work, we analyze and fix the proof strategy used for this family of systems. After proving the efficacy of the ideal functionality, we show how the original properties are flawed and suggest improved, effective properties in their place. Finally, we discover another common mistake in the proofs. We demonstrate how to avoid it by showing our improved properties for one protocol, thus partially fixing the family of provably secure onion routing protocols.

</details>

<details>

<summary>2019-10-30 17:14:04 - Representing and Reasoning about Dynamic Code</summary>

- *Jesse Bartels, Jon Stephens, Saumya Debray*

- `1910.09606v3` - [abs](http://arxiv.org/abs/1910.09606v3) - [pdf](http://arxiv.org/pdf/1910.09606v3)

> Dynamic code, i.e., code that is created or modified at runtime, is ubiquitous in today's world. The behavior of dynamic code can depend on the logic of the dynamic code generator in subtle and non-obvious ways, with significant security implications, e.g., JIT compiler bugs can lead to exploitable vulnerabilities in the resulting JIT-compiled code. Existing approaches to program analysis do not provide adequate support for reasoning about such behavioral relationships. This paper takes a first step in addressing this problem by describing a program representation and a new notion of dependency that allows us to reason about dependency and information flow relationships between the dynamic code generator and the generated dynamic code. Experimental results show that analyses based on these concepts are able to capture properties of dynamic code that cannot be identified using traditional program analyses.

</details>

<details>

<summary>2019-10-30 19:55:26 - Investigating Resistance of Deep Learning-based IDS against Adversaries using min-max Optimization</summary>

- *Rana Abou Khamis, Omair Shafiq, Ashraf Matrawy*

- `1910.14107v1` - [abs](http://arxiv.org/abs/1910.14107v1) - [pdf](http://arxiv.org/pdf/1910.14107v1)

> With the growth of adversarial attacks against machine learning models, several concerns have emerged about potential vulnerabilities in designing deep neural network-based intrusion detection systems (IDS). In this paper, we study the resilience of deep learning-based intrusion detection systems against adversarial attacks. We apply the min-max (or saddle-point) approach to train intrusion detection systems against adversarial attack samples in NSW-NB 15 dataset. We have the max approach for generating adversarial samples that achieves maximum loss and attack deep neural networks. On the other side, we utilize the existing min approach [2] [9] as a defense strategy to optimize intrusion detection systems that minimize the loss of the incorporated adversarial samples during the adversarial training. We study and measure the effectiveness of the adversarial attack methods as well as the resistance of the adversarially trained models against such attacks. We find that the adversarial attack methods that were designed in binary domains can be used in continuous domains and exhibit different misclassification levels. We finally show that principal component analysis (PCA) based feature reduction can boost the robustness in intrusion detection system (IDS) using a deep neural network (DNN).

</details>

<details>

<summary>2019-10-30 22:56:46 - Secure Logging with Security against Adaptive Crash Attack</summary>

- *Sepideh Avizheh, Reihaneh Safavi-Naini, Shuai Li*

- `1910.14169v1` - [abs](http://arxiv.org/abs/1910.14169v1) - [pdf](http://arxiv.org/pdf/1910.14169v1)

> Logging systems are an essential component of security systems and their security has been widely studied. Recently (2017) it was shown that existing secure logging protocols are vulnerable to crash attack in which the adversary modifies the log file and then crashes the system to make it indistinguishable from a normal system crash. The attacker was assumed to be non-adaptive and not be able to see the file content before modifying and crashing it (which will be immediately after modifying the file). The authors also proposed a system called SLiC that protects against this attacker. In this paper, we consider an (insider) adaptive adversary who can see the file content as new log operations are performed. This is a powerful adversary who can attempt to rewind the system to a past state. We formalize security against this adversary and introduce a scheme with provable security. We show that security against this attacker requires some (small) protected memory that can become accessible to the attacker after the system compromise. We show that existing secure logging schemes are insecure in this setting, even if the system provides some protected memory as above. We propose a novel mechanism that, in its basic form, uses a pair of keys that evolve at different rates, and employ this mechanism in an existing logging scheme that has forward integrity to obtain a system with provable security against adaptive (and hence non-adaptive) crash attack. We implemented our scheme on a desktop computer and a Raspberry Pi, and showed in addition to higher security, a significant efficiency gain over SLiC.

</details>

<details>

<summary>2019-10-31 21:18:40 - Reducing audio membership inference attack accuracy to chance: 4 defenses</summary>

- *Michael Lomnitz, Nina Lopatina, Paul Gamble, Zigfried Hampel-Arias, Lucas Tindall, Felipe A. Mejia, Maria Alejandra Barrios*

- `1911.01888v1` - [abs](http://arxiv.org/abs/1911.01888v1) - [pdf](http://arxiv.org/pdf/1911.01888v1)

> It is critical to understand the privacy and robustness vulnerabilities of machine learning models, as their implementation expands in scope. In membership inference attacks, adversaries can determine whether a particular set of data was used in training, putting the privacy of the data at risk. Existing work has mostly focused on image related tasks; we generalize this type of attack to speaker identification on audio samples. We demonstrate attack precision of 85.9\% and recall of 90.8\% for LibriSpeech, and 78.3\% precision and 90.7\% recall for VOiCES (Voices Obscured in Complex Environmental Settings). We find that implementing defenses such as prediction obfuscation, defensive distillation or adversarial training, can reduce attack accuracy to chance.

</details>


## 2019-11

<details>

<summary>2019-11-01 00:17:40 - Weird Machines as Insecure Compilation</summary>

- *Jennifer Paykin, Eric Mertens, Mark Tullsen, Luke Maurer, Benoît Razet, Alexander Bakst, Scott Moore*

- `1911.00157v1` - [abs](http://arxiv.org/abs/1911.00157v1) - [pdf](http://arxiv.org/pdf/1911.00157v1)

> Weird machines---the computational models accessible by exploiting security vulnerabilities---arise from the difference between the model a programmer has in her head of how her program should run and the implementation that actually executes. Previous attempts to reason about or identify weird machines have viewed these models through the lens of formal computational structures such as state machines and Turing machines. But because programmers rarely think about programs in this way, it is difficult to effectively apply insights about weird machines to improve security.   We present a new view of weird machines based on techniques from programming languages theory and secure compilation. Instead of an underspecified model drawn from a programmers' head, we start with a program written in a high-level source language that enforces security properties by design. Instead of state machines to describe computation, we use the well-defined semantics of this source language and a target language, into which the source program will be compiled. Weird machines are the sets of behaviors that can be achieved by a compiled source program in the target language that cannot be achieved in the source language directly. That is, exploits are witnesses to insecure compilation.   This paper develops a framework for characterizing weird machines as insecure compilation, and illustrates the framework with examples of common exploits. We study the classes of security properties that exploits violate, the compositionality of exploits in a compiler stack, and the weird machines and mitigations that arise.

</details>

<details>

<summary>2019-11-01 08:50:01 - Robust contrastive learning and nonlinear ICA in the presence of outliers</summary>

- *Hiroaki Sasaki, Takashi Takenouchi, Ricardo Monti, Aapo Hyvärinen*

- `1911.00265v1` - [abs](http://arxiv.org/abs/1911.00265v1) - [pdf](http://arxiv.org/pdf/1911.00265v1)

> Nonlinear independent component analysis (ICA) is a general framework for unsupervised representation learning, and aimed at recovering the latent variables in data. Recent practical methods perform nonlinear ICA by solving a series of classification problems based on logistic regression. However, it is well-known that logistic regression is vulnerable to outliers, and thus the performance can be strongly weakened by outliers. In this paper, we first theoretically analyze nonlinear ICA models in the presence of outliers. Our analysis implies that estimation in nonlinear ICA can be seriously hampered when outliers exist on the tails of the (noncontaminated) target density, which happens in a typical case of contamination by outliers. We develop two robust nonlinear ICA methods based on the {\gamma}-divergence, which is a robust alternative to the KL-divergence in logistic regression. The proposed methods are shown to have desired robustness properties in the context of nonlinear ICA. We also experimentally demonstrate that the proposed methods are very robust and outperform existing methods in the presence of outliers. Finally, the proposed method is applied to ICA-based causal discovery and shown to find a plausible causal relationship on fMRI data.

</details>

<details>

<summary>2019-11-01 17:12:15 - You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle</summary>

- *Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, Bin Dong*

- `1905.00877v6` - [abs](http://arxiv.org/abs/1905.00877v6) - [pdf](http://arxiv.org/pdf/1905.00877v6)

> Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However, recent works have shown that deep networks can be vulnerable to adversarial perturbations, which raised a serious robustness issue of deep networks. Adversarial training, typically formulated as a robust optimization problem, is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples, typically far greater than that of the network training. This leads to the unbearable overall computational cost of adversarial training. In this paper, we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin's Maximal Principle (PMP) of the problem, we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore, we refer to this algorithm YOPO (You Only Propagate Once). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with approximately 1/5 ~ 1/4 GPU time of the projected gradient descent (PGD) algorithm. Our codes are available at https://https://github.com/a1600012888/YOPO-You-Only-Propagate-Once.

</details>

<details>

<summary>2019-11-02 15:00:22 - On resilience and connectivity of secure wireless sensor networks under node capture attacks</summary>

- *Jun Zhao*

- `1911.00725v1` - [abs](http://arxiv.org/abs/1911.00725v1) - [pdf](http://arxiv.org/pdf/1911.00725v1)

> Despite much research on probabilistic key predistribution schemes for wireless sensor networks over the past decade, few formal analyses exist that define schemes' resilience to node-capture attacks precisely and under realistic conditions. In this paper, we analyze the resilience of the q-composite key predistribution scheme, which mitigates the node capture vulnerability of the Eschenauer-Gligor scheme in the neighbor discovery phase. We derive scheme parameters to have a desired level of resiliency, and obtain optimal parameters that defend against different adversaries as much as possible. We also show that this scheme can be easily enhanced to achieve the same "perfect resilience" property as in the random pairwise key predistribution for attacks launched after neighbor discovery. Despite considerable attention to this scheme, much prior work explicitly or implicitly uses an incorrect computation for the probability of link compromise under node-capture attacks and ignores real-world transmission constraints of sensor nodes. Moreover, we derive the critical network parameters to ensure connectivity in both the absence and presence of node-capture attacks. We also investigate node replication attacks by analyzing the adversary's optimal strategy.

</details>

<details>

<summary>2019-11-03 19:18:13 - Calcium Vulnerability Scanner (CVS): A Deeper Look</summary>

- *Sari Sultan, Ayed Salman*

- `1911.00950v1` - [abs](http://arxiv.org/abs/1911.00950v1) - [pdf](http://arxiv.org/pdf/1911.00950v1)

> Traditional vulnerability scanning methods are time-consuming and indecisive, and they negatively affect network performance by generating high network traffic. In this paper, we present a novel vulnerability scanner that is time-efficient, simple, accurate, and safe. We call it a Calcium Vulnerability Scanner (CVS). Our contribution to vulnerability scanning are the following: (i) minimize its required time and network traffic: compared to current technologies, we reduced the former by an average of 79% and the latter by 99.9%, (ii) increase its accuracy: compared to current technologies, we improved this by an average of 2600%, and (iii) enable the scanner to learn from previous scans in order to reduce future scanning time and enhance accuracy: compared to current technologies, CVS reduced scanning time by an average of 97%. CVS enables a new frontier in vulnerability scanning and allow for scalable and efficient deployment of such tools in large-scale networks, containers, edge computing, and cloud computing.

</details>

<details>

<summary>2019-11-04 18:58:06 - Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards</summary>

- *Alexander Trott, Stephan Zheng, Caiming Xiong, Richard Socher*

- `1911.01417v1` - [abs](http://arxiv.org/abs/1911.01417v1) - [pdf](http://arxiv.org/pdf/1911.01417v1)

> While using shaped rewards can be beneficial when solving sparse reward tasks, their successful application often requires careful engineering and is problem specific. For instance, in tasks where the agent must achieve some goal state, simple distance-to-goal reward shaping often fails, as it renders learning vulnerable to local optima. We introduce a simple and effective model-free method to learn from shaped distance-to-goal rewards on tasks where success depends on reaching a goal state. Our method introduces an auxiliary distance-based reward based on pairs of rollouts to encourage diverse exploration. This approach effectively prevents learning dynamics from stabilizing around local optima induced by the naive distance-to-goal reward shaping and enables policies to efficiently solve sparse reward tasks. Our augmented objective does not require any additional reward engineering or domain expertise to implement and converges to the original sparse objective as the agent learns to solve the task. We demonstrate that our method successfully solves a variety of hard-exploration tasks (including maze navigation and 3D construction in a Minecraft environment), where naive distance-based reward shaping otherwise fails, and intrinsic curiosity and reward relabeling strategies exhibit poor performance.

</details>

<details>

<summary>2019-11-04 19:54:25 - A Brief Review on Some Architectures Providing Support for DIFT</summary>

- *Ali Jahanshahi*

- `1911.05664v1` - [abs](http://arxiv.org/abs/1911.05664v1) - [pdf](http://arxiv.org/pdf/1911.05664v1)

> Dynamic Information Flow Tracking (DIFT) is a technique to track potential security vulnerabilities in software and hardware systems at run time. The last fifteen years have seen a lot of research work on DIFT, including both hardware-based and software-based implementations for different types of processor architectures. This survey briefly reviews some hardware architectures that provide DIFT support. Starting from introducing different approaches for hardware based DIFT, this survey focuses on integrated/in-core architectures. Protection schemes, including tagging system, tag propagation, and tag checking for each architecture will be discussed. The survey is organized in such a way that it illustrates the evolution of integrated DIFT architectures, each architecture tries to improve the precious proposed architectures generality/versatility weaknesses. However, improving security while providing generality and versatility is kind of trade-offs. This survey compares the architectures from different aspects to show the trade-offs clearer.

</details>

<details>

<summary>2019-11-05 16:31:23 - DLA: Dense-Layer-Analysis for Adversarial Example Detection</summary>

- *Philip Sperl, Ching-Yu Kao, Peng Chen, Konstantin Böttinger*

- `1911.01921v1` - [abs](http://arxiv.org/abs/1911.01921v1) - [pdf](http://arxiv.org/pdf/1911.01921v1)

> In recent years Deep Neural Networks (DNNs) have achieved remarkable results and even showed super-human capabilities in a broad range of domains. This led people to trust in DNNs' classifications and resulting actions even in security-sensitive environments like autonomous driving.   Despite their impressive achievements, DNNs are known to be vulnerable to adversarial examples. Such inputs contain small perturbations to intentionally fool the attacked model.   In this paper, we present a novel end-to-end framework to detect such attacks during classification without influencing the target model's performance. Inspired by recent research in neuron-coverage guided testing we show that dense layers of DNNs carry security-sensitive information. With a secondary DNN we analyze the activation patterns of the dense layers during classification runtime, which enables effective and real-time detection of adversarial examples.   Our prototype implementation successfully detects adversarial examples in image, natural language, and audio processing. Thereby, we cover a variety of target DNNs, including Long Short Term Memory (LSTM) architectures. In addition, to effectively defend against state-of-the-art attacks, our approach generalizes between different sets of adversarial examples. Thus, our method most likely enables us to detect even future, yet unknown attacks. Finally, during white-box adaptive attacks, we show our method cannot be easily bypassed.

</details>

<details>

<summary>2019-11-05 22:04:30 - Existence of Stack Overflow Vulnerabilities in Well-known Open Source Projects</summary>

- *Md. Masudur Rahman, B M Mainul Hossain*

- `1910.14374v2` - [abs](http://arxiv.org/abs/1910.14374v2) - [pdf](http://arxiv.org/pdf/1910.14374v2)

> A stack overflow occurs when a program or process tries to store more data in a buffer (or stack) than it was intended to hold. If the affected program is running with special privileges or accepts data from untrusted network hosts (e.g. a web-server), then it is a potential security vulnerability. Overflowing a stack, an attacker can corrupt the stack in such a way as to inject executable code into the running program and take control of the process. This is one of the easiest and more reliable methods for attackers to gain unauthorized access to a computer. In this paper, we show that how stack overflow occurs and many open source projects, such as - Linux, Git, PHP, etc. contain such code portions in which it is possible to overflow the stacks as well as inject malicious script to harm the normal execution of the processes. In addition, this paper raises a concern to avoid writing such codes those are potentially sources for stack overflow attack.

</details>

<details>

<summary>2019-11-06 01:32:50 - Sparse DNNs with Improved Adversarial Robustness</summary>

- *Yiwen Guo, Chao Zhang, Changshui Zhang, Yurong Chen*

- `1810.09619v2` - [abs](http://arxiv.org/abs/1810.09619v2) - [pdf](http://arxiv.org/pdf/1810.09619v2)

> Deep neural networks (DNNs) are computationally/memory-intensive and vulnerable to adversarial attacks, making them prohibitive in some real-world applications. By converting dense models into sparse ones, pruning appears to be a promising solution to reducing the computation/memory cost. This paper studies classification models, especially DNN-based ones, to demonstrate that there exists intrinsic relationships between their sparsity and adversarial robustness. Our analyses reveal, both theoretically and empirically, that nonlinear DNN-based classifiers behave differently under $l_2$ attacks from some linear ones. We further demonstrate that an appropriately higher model sparsity implies better robustness of nonlinear DNNs, whereas over-sparsified models can be more difficult to resist adversarial examples.

</details>

<details>

<summary>2019-11-06 10:55:11 - sCompile: Critical Path Identification and Analysis for Smart Contracts</summary>

- *Jialiang Chang, Bo Gao, Hao Xiao, Jun Sun, Yan Cai, Zijiang Yang*

- `1808.00624v2` - [abs](http://arxiv.org/abs/1808.00624v2) - [pdf](http://arxiv.org/pdf/1808.00624v2)

> Ethereum smart contracts are an innovation built on top of the blockchain technology, which provides a platform for automatically executing contracts in an anonymous, distributed, and trusted way. The problem is magnified by the fact that smart contracts, unlike ordinary programs, cannot be patched easily once deployed. It is important for smart contracts to be checked against potential vulnerabilities. In this work, we propose an alternative approach to automatically identify critical program paths (with multiple function calls including inter-contract function calls) in a smart contract, rank the paths according to their criticalness, discard them if they are infeasible or otherwise present them with user friendly warnings for user inspection. We identify paths which involve monetary transaction as critical paths, and prioritize those which potentially violate important properties. For scalability, symbolic execution techniques are only applied to top ranked critical paths. Our approach has been implemented in a tool called sCompile, which has been applied to 36,099 smart contracts. The experiment results show that sCompile is efficient, i.e., 5 seconds on average for one smart contract. Furthermore, we show that many known vulnerabilities can be captured if user inspects as few as 10 program paths generated by sCompile. Lastly, sCompile discovered 224 unknown vulnerabilities with a false positive rate of 15.4% before user inspection.

</details>

<details>

<summary>2019-11-07 14:52:12 - White-Box Target Attack for EEG-Based BCI Regression Problems</summary>

- *Lubin Meng, Chin-Teng Lin, Tzyy-Ring Jung, Dongrui Wu*

- `1911.04606v1` - [abs](http://arxiv.org/abs/1911.04606v1) - [pdf](http://arxiv.org/pdf/1911.04606v1)

> Machine learning has achieved great success in many applications, including electroencephalogram (EEG) based brain-computer interfaces (BCIs). Unfortunately, many machine learning models are vulnerable to adversarial examples, which are crafted by adding deliberately designed perturbations to the original inputs. Many adversarial attack approaches for classification problems have been proposed, but few have considered target adversarial attacks for regression problems. This paper proposes two such approaches. More specifically, we consider white-box target attacks for regression problems, where we know all information about the regression model to be attacked, and want to design small perturbations to change the regression output by a pre-determined amount. Experiments on two BCI regression problems verified that both approaches are effective. Moreover, adversarial examples generated from both approaches are also transferable, which means that we can use adversarial examples generated from one known regression model to attack an unknown regression model, i.e., to perform black-box attacks. To our knowledge, this is the first study on adversarial attacks for EEG-based BCI regression problems, which calls for more attention on the security of BCI systems.

</details>

<details>

<summary>2019-11-07 15:00:24 - Active Learning for Black-Box Adversarial Attacks in EEG-Based Brain-Computer Interfaces</summary>

- *Xue Jiang, Xiao Zhang, Dongrui Wu*

- `1911.04338v1` - [abs](http://arxiv.org/abs/1911.04338v1) - [pdf](http://arxiv.org/pdf/1911.04338v1)

> Deep learning has made significant breakthroughs in many fields, including electroencephalogram (EEG) based brain-computer interfaces (BCIs). However, deep learning models are vulnerable to adversarial attacks, in which deliberately designed small perturbations are added to the benign input samples to fool the deep learning model and degrade its performance. This paper considers transferability-based black-box attacks, where the attacker trains a substitute model to approximate the target model, and then generates adversarial examples from the substitute model to attack the target model. Learning a good substitute model is critical to the success of these attacks, but it requires a large number of queries to the target model. We propose a novel framework which uses query synthesis based active learning to improve the query efficiency in training the substitute model. Experiments on three convolutional neural network (CNN) classifiers and three EEG datasets demonstrated that our method can improve the attack success rate with the same number of queries, or, in other words, our method requires fewer queries to achieve a desired attack performance. To our knowledge, this is the first work that integrates active learning and adversarial attacks for EEG-based BCIs.

</details>

<details>

<summary>2019-11-08 03:45:55 - Why Do Masked Neural Language Models Still Need Common Sense Knowledge?</summary>

- *Sunjae Kwon, Cheongwoong Kang, Jiyeon Han, Jaesik Choi*

- `1911.03024v1` - [abs](http://arxiv.org/abs/1911.03024v1) - [pdf](http://arxiv.org/pdf/1911.03024v1)

> Currently, contextualized word representations are learned by intricate neural network models, such as masked neural language models (MNLMs). The new representations significantly enhanced the performance in automated question answering by reading paragraphs. However, identifying the detailed knowledge trained in the MNLMs is difficult owing to numerous and intermingled parameters. This paper provides empirical but insightful analyses on the pretrained MNLMs with respect to common sense knowledge. First, we propose a test that measures what types of common sense knowledge do pretrained MNLMs understand. From the test, we observed that MNLMs partially understand various types of common sense knowledge but do not accurately understand the semantic meaning of relations. In addition, based on the difficulty of the question-answering task problems, we observed that pretrained MLM-based models are still vulnerable to problems that require common sense knowledge. We also experimentally demonstrated that we can elevate existing MNLM-based models by combining knowledge from an external common sense repository.

</details>

<details>

<summary>2019-11-08 17:26:50 - Towards the Avoidance of Counterfeit Memory: Identifying the DRAM Origin</summary>

- *B. M. S. Bahar Talukder, Vineetha Menon, Biswajit Ray, Tempestt Neal, Md Tauhidur Rahman*

- `1911.03395v1` - [abs](http://arxiv.org/abs/1911.03395v1) - [pdf](http://arxiv.org/pdf/1911.03395v1)

> Due to the globalization in the semiconductor supply chain, counterfeit dynamic random-access memory (DRAM) chips/modules have been spreading worldwide at an alarming rate. Deploying counterfeit DRAM modules into an electronic system can have severe consequences on security and reliability domains because of their sub-standard quality, poor performance, and shorter life span. Besides, studies suggest that a counterfeit DRAM can be more vulnerable to sophisticated attacks. However, detecting counterfeit DRAMs is very challenging because of their nature and ability to pass the initial testing. In this paper, we propose a technique to identify the DRAM origin (i.e., the origin of the manufacturer and the specification of individual DRAM) to detect and prevent counterfeit DRAM modules. A silicon evaluation shows that the proposed method reliably identifies off-the-shelf DRAM modules from three major manufacturers.

</details>

<details>

<summary>2019-11-08 22:25:21 - Attack Trees for Security and Privacy in Social Virtual Reality Learning Environments</summary>

- *Samaikya Valluripally, Aniket Gulhane, Reshmi Mitra, Khaza Anuarul Hoque, Prasad Calyam*

- `1911.03563v1` - [abs](http://arxiv.org/abs/1911.03563v1) - [pdf](http://arxiv.org/pdf/1911.03563v1)

> Social Virtual Reality Learning Environment (VRLE) is a novel edge computing platform for collaboration amongst distributed users. Given that VRLEs are used for critical applications (e.g., special education, public safety training), it is important to ensure security and privacy issues. In this paper, we present a novel framework to obtain quantitative assessments of threats and vulnerabilities for VRLEs. Based on the use cases from an actual social VRLE viz., vSocial, we first model the security and privacy using the attack trees. Subsequently, these attack trees are converted into stochastic timed automata representations that allow for rigorous statistical model checking. Such an analysis helps us adopt pertinent design principles such as hardening, diversity and principle of least privilege to enhance the resilience of social VRLEs. Through experiments in a vSocial case study, we demonstrate the effectiveness of our attack tree modeling with a reduction of 26% in probability of loss of integrity (security) and 80% in privacy leakage (privacy) in before and after scenarios pertaining to the adoption of the design principles.

</details>

<details>

<summary>2019-11-09 17:35:57 - Convergence of Adversarial Training in Overparametrized Neural Networks</summary>

- *Ruiqi Gao, Tianle Cai, Haochuan Li, Liwei Wang, Cho-Jui Hsieh, Jason D. Lee*

- `1906.07916v2` - [abs](http://arxiv.org/abs/1906.07916v2) - [pdf](http://arxiv.org/pdf/1906.07916v2)

> Neural networks are vulnerable to adversarial examples, i.e. inputs that are imperceptibly perturbed from natural data and yet incorrectly classified by the network. Adversarial training, a heuristic form of robust optimization that alternates between minimization and maximization steps, has proven to be among the most successful methods to train networks to be robust against a pre-defined family of perturbations. This paper provides a partial answer to the success of adversarial training, by showing that it converges to a network where the surrogate loss with respect to the the attack algorithm is within $\epsilon$ of the optimal robust loss. Then we show that the optimal robust loss is also close to zero, hence adversarial training finds a robust classifier. The analysis technique leverages recent work on the analysis of neural networks via Neural Tangent Kernel (NTK), combined with motivation from online-learning when the maximization is solved by a heuristic, and the expressiveness of the NTK kernel in the $\ell_\infty$-norm. In addition, we also prove that robust interpolation requires more model capacity, supporting the evidence that adversarial training requires wider networks.

</details>

<details>

<summary>2019-11-10 06:24:53 - Certified Adversarial Robustness with Additive Noise</summary>

- *Bai Li, Changyou Chen, Wenlin Wang, Lawrence Carin*

- `1809.03113v6` - [abs](http://arxiv.org/abs/1809.03113v6) - [pdf](http://arxiv.org/pdf/1809.03113v6)

> The existence of adversarial data examples has drawn significant attention in the deep-learning community; such data are seemingly minimally perturbed relative to the original data, but lead to very different outputs from a deep-learning algorithm. Although a significant body of work on developing defensive models has been considered, most such models are heuristic and are often vulnerable to adaptive attacks. Defensive methods that provide theoretical robustness guarantees have been studied intensively, yet most fail to obtain non-trivial robustness when a large-scale model and data are present. To address these limitations, we introduce a framework that is scalable and provides certified bounds on the norm of the input manipulation for constructing adversarial examples. We establish a connection between robustness against adversarial perturbation and additive random noise, and propose a training strategy that can significantly improve the certified bounds. Our evaluation on MNIST, CIFAR-10 and ImageNet suggests that the proposed method is scalable to complicated models and large data sets, while providing competitive robustness to state-of-the-art provable defense methods.

</details>

<details>

<summary>2019-11-12 02:17:01 - Robust Design of Deep Neural Networks against Adversarial Attacks based on Lyapunov Theory</summary>

- *Arash Rahnama, Andre T. Nguyen, Edward Raff*

- `1911.04636v1` - [abs](http://arxiv.org/abs/1911.04636v1) - [pdf](http://arxiv.org/pdf/1911.04636v1)

> Deep neural networks (DNNs) are vulnerable to subtle adversarial perturbations applied to the input. These adversarial perturbations, though imperceptible, can easily mislead the DNN. In this work, we take a control theoretic approach to the problem of robustness in DNNs. We treat each individual layer of the DNN as a nonlinear dynamical system and use Lyapunov theory to prove stability and robustness locally. We then proceed to prove stability and robustness globally for the entire DNN. We develop empirically tight bounds on the response of the output layer, or any hidden layer, to adversarial perturbations added to the input, or the input of hidden layers. Recent works have proposed spectral norm regularization as a solution for improving robustness against l2 adversarial attacks. Our results give new insights into how spectral norm regularization can mitigate the adversarial effects. Finally, we evaluate the power of our approach on a variety of data sets and network architectures and against some of the well-known adversarial attacks.

</details>

<details>

<summary>2019-11-12 12:39:31 - oo7: Low-overhead Defense against Spectre Attacks via Program Analysis</summary>

- *Guanhua Wang, Sudipta Chattopadhyay, Ivan Gotovchits, Tulika Mitra, Abhik Roychoudhury*

- `1807.05843v6` - [abs](http://arxiv.org/abs/1807.05843v6) - [pdf](http://arxiv.org/pdf/1807.05843v6)

> The Spectre vulnerability in modern processors has been widely reported. The key insight in this vulnerability is that speculative execution in processors can be misused to access the secrets. Subsequently, even though the speculatively executed instructions are squashed, the secret may linger in micro-architectural states such as cache, and can potentially be accessed by an attacker via side channels. In this paper, we propose oo7, a static analysis approach that can mitigate Spectre attacks by detecting potentially vulnerable code snippets in program binaries and protecting them against the attack by patching them. Our key contribution is to balance the concerns of effectiveness, analysis time and run-time overheads. We employ control flow extraction, taint analysis, and address analysis to detect tainted conditional branches and speculative memory accesses. oo7 can detect all fifteen purpose-built Spectre-vulnerable code patterns, whereas Microsoft compiler with Spectre mitigation option can only detect two of them. We also report the results of a large-scale study on applying oo7 to over 500 program binaries (average binary size 261 KB) from different real-world projects. We protect programs against Spectre attack by selectively inserting fences only at vulnerable conditional branches to prevent speculative execution. Our approach is experimentally observed to incur around 5.9% performance overheads on SPECint benchmarks.

</details>

<details>

<summary>2019-11-12 19:26:35 - The Insecurity of Home Digital Voice Assistants -- Amazon Alexa as a Case Study</summary>

- *Xinyu Lei, Guan-Hua Tu, Alex X. Liu, Kamran Ali, Chi-Yu Li, Tian Xie*

- `1712.03327v3` - [abs](http://arxiv.org/abs/1712.03327v3) - [pdf](http://arxiv.org/pdf/1712.03327v3)

> Home Digital Voice Assistants (HDVAs) are getting popular in recent years. Users can control smart devices and get living assistance through those HDVAs (e.g., Amazon Alexa, Google Home) using voice. In this work, we study the insecurity of HDVA service by using Amazon Alexa as a case study. We disclose three security vulnerabilities which root in the insecure access control of Alexa services. We then exploit them to devise two proof-of-concept attacks, home burglary and fake order, where the adversary can remotely command the victim's Alexa device to open a door or place an order from Amazon.com. The insecure access control is that the Alexa device not only relies on a single-factor authentication but also takes voice commands even if no people are around. We thus argue that HDVAs should have another authentication factor, a physical presence based access control; that is, they can accept voice commands only when any person is detected nearby. To this end, we devise a Virtual Security Button (VSButton), which leverages the WiFi technology to detect indoor human motions. Once any indoor human motion is detected, the Alexa device is enabled to accept voice commands. Our evaluation results show that it can effectively differentiate indoor motions from the cases of no motion and outdoor motions in both the laboratory and real world settings.

</details>

<details>

<summary>2019-11-13 04:36:14 - Set-based Obfuscation for Strong PUFs against Machine Learning Attacks</summary>

- *Jiliang Zhang, Chaoqun Shen*

- `1806.02011v4` - [abs](http://arxiv.org/abs/1806.02011v4) - [pdf](http://arxiv.org/pdf/1806.02011v4)

> Strong physical unclonable function (PUF) is a promising solution for device authentication in resourceconstrained applications but vulnerable to machine learning attacks. In order to resist such attack, many defenses have been proposed in recent years. However, these defenses incur high hardware overhead, degenerate reliability and are inefficient against advanced machine learning attacks such as approximation attacks. In order to address these issues, we propose a Random Set-based Obfuscation (RSO) for Strong PUFs to resist machine learning attacks. The basic idea is that several stable responses are derived from the PUF itself and pre-stored as the set for obfuscation in the testing phase, and then a true random number generator is used to select any two keys to obfuscate challenges and responses with XOR operations. When the number of challenge-response pairs (CRPs) collected by the attacker exceeds the given threshold, the set will be updated immediately. In this way, machine learning attacks can be prevented with extremely low hardware overhead. Experimental results show that for a 64x64 Arbiter PUF, when the size of set is 32 and even if 1 million CRPs are collected by attackers, the prediction accuracies of Logistic regression, support vector machines, artificial neural network, convolutional neural network and covariance matrix adaptive evolutionary strategy are about 50% which is equivalent to the random guessing.

</details>

<details>

<summary>2019-11-13 17:53:56 - TPM-FAIL: TPM meets Timing and Lattice Attacks</summary>

- *Daniel Moghimi, Berk Sunar, Thomas Eisenbarth, Nadia Heninger*

- `1911.05673v1` - [abs](http://arxiv.org/abs/1911.05673v1) - [pdf](http://arxiv.org/pdf/1911.05673v1)

> Trusted Platform Module (TPM) serves as a hardware-based root of trust that protects cryptographic keys from privileged system and physical adversaries. In this work, we perform a black-box timing analysis of TPM 2.0 devices deployed on commodity computers. Our analysis reveals that some of these devices feature secret-dependent execution times during signature generation based on elliptic curves. In particular, we discovered timing leakage on an Intel firmware-based TPM as well as a hardware TPM. We show how this information allows an attacker to apply lattice techniques to recover 256-bit private keys for ECDSA and ECSchnorr signatures. On Intel fTPM, our key recovery succeeds after about 1,300 observations and in less than two minutes. Similarly, we extract the private ECDSA key from a hardware TPM manufactured by STMicroelectronics, which is certified at Common Criteria (CC) EAL 4+, after fewer than 40,000 observations. We further highlight the impact of these vulnerabilities by demonstrating a remote attack against a StrongSwan IPsec VPN that uses a TPM to generate the digital signatures for authentication. In this attack, the remote client recovers the server's private authentication key by timing only 45,000 authentication handshakes via a network connection.   The vulnerabilities we have uncovered emphasize the difficulty of correctly implementing known constant-time techniques, and show the importance of evolutionary testing and transparent evaluation of cryptographic implementations. Even certified devices that claim resistance against attacks require additional scrutiny by the community and industry, as we learn more about these attacks.

</details>

<details>

<summary>2019-11-13 19:25:53 - Machine Learning Based Network Vulnerability Analysis of Industrial Internet of Things</summary>

- *Maede Zolanvari, Marcio A. Teixeira, Lav Gupta, Khaled M. Khan, Raj Jain*

- `1911.05771v1` - [abs](http://arxiv.org/abs/1911.05771v1) - [pdf](http://arxiv.org/pdf/1911.05771v1)

> It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of machine learning in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using machine learning models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a machine learning based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.

</details>

<details>

<summary>2019-11-13 21:41:46 - B-Ride: Ride Sharing with Privacy-preservation, Trust and Fair Payment atop Public Blockchain</summary>

- *Mohamed Baza, Noureddine Lasla, Mohamed Mahmoud, Gautam Srivastava, Mohamed Abdallah*

- `1906.09968v2` - [abs](http://arxiv.org/abs/1906.09968v2) - [pdf](http://arxiv.org/pdf/1906.09968v2)

> Ride-sharing is a service that enables drivers to share their trips with other riders, contributing to appealing benefits of shared travel costs. However, the majority of existing platforms rely on a central third party, which make them subject to a single point of failure and privacy disclosure issues. Moreover, they are vulnerable to DDoS and Sybil attacks due to malicious users involvement. Besides, high fees should be paid to the service provider. In this paper, we propose a decentralized ride-sharing service based on public Blockchain, named B-Ride. Both riders and drivers can find rides match while preserving their trip data, including pick-up/drop-off location, and departure/arrival date. However, under the anonymity of the public blockchain, a malicious user may submit multiple ride requests or offers, while not committing to any of them, to discover better offer or to make the system unreliable. B-Ride solves this problem by introducing a time-locked deposit protocol for a ride-sharing by leveraging smart contract and zero-knowledge set membership proof. In a nutshell, both a driver and a rider have to show their commitment by sending a deposit to the blockchain. Later, a driver has to prove to the blockchain on the agreed departure time that he has arrived at the pick-up location. To preserve rider/driver location privacy by hiding the exact pick-up location, the proof is done using zero-knowledge set membership protocol. Moreover, to ensure a fair payment, a pay-as-you-drive methodology is introduced based on the elapsed distance of the driver and the rider. Also, we introduce a reputation-based trust model to rate drivers based on their past trips to allow riders to select them based on their history on the system. Finally, we implement B-Ride in a test net of Ethereum. The experiment results show the applicability of our protocol atop the existing real-world blockchain.

</details>

<details>

<summary>2019-11-14 03:13:17 - Adversarial Margin Maximization Networks</summary>

- *Ziang Yan, Yiwen Guo, Changshui Zhang*

- `1911.05916v1` - [abs](http://arxiv.org/abs/1911.05916v1) - [pdf](http://arxiv.org/pdf/1911.05916v1)

> The tremendous recent success of deep neural networks (DNNs) has sparked a surge of interest in understanding their predictive ability. Unlike the human visual system which is able to generalize robustly and learn with little supervision, DNNs normally require a massive amount of data to learn new concepts. In addition, research works also show that DNNs are vulnerable to adversarial examples-maliciously generated images which seem perceptually similar to the natural ones but are actually formed to fool learning models, which means the models have problem generalizing to unseen data with certain type of distortions. In this paper, we analyze the generalization ability of DNNs comprehensively and attempt to improve it from a geometric point of view. We propose adversarial margin maximization (AMM), a learning-based regularization which exploits an adversarial perturbation as a proxy. It encourages a large margin in the input space, just like the support vector machines. With a differentiable formulation of the perturbation, we train the regularized DNNs simply through back-propagation in an end-to-end manner. Experimental results on various datasets (including MNIST, CIFAR-10/100, SVHN and ImageNet) and different DNN architectures demonstrate the superiority of our method over previous state-of-the-arts. Code and models for reproducing our results will be made publicly available.

</details>

<details>

<summary>2019-11-14 17:35:18 - A Security Perspective on Unikernels</summary>

- *Joshua Talbot, Przemek Pikula, Craig Sweetmore, Samuel Rowe, Hanan Hindy, Christos Tachtatzis, Robert Atkinson, Xavier Bellekens*

- `1911.06260v1` - [abs](http://arxiv.org/abs/1911.06260v1) - [pdf](http://arxiv.org/pdf/1911.06260v1)

> Cloud-based infrastructures have grown in popularity over the last decade leveraging virtualisation, server, storage, compute power and network components to develop flexible applications. The requirements for instantaneous deployment and reduced costs have led the shift from virtual machine deployment to containerisation, increasing the overall flexibility of applications and increasing performances. However, containers require a fully fleshed operating system to execute, increasing the attack surface of an application. Unikernels, on the other hand, provide a lightweight memory footprint, ease of application packaging and reduced start-up times. Moreover, Unikernels reduce the attack surface due to the self-contained environment only enabling low-level features. In this work, we provide an exhaustive description of the unikernel ecosystem; we demonstrate unikernel vulnerabilities and further discuss the security implications of Unikernel-enabled environments through different use-cases.

</details>

<details>

<summary>2019-11-14 18:46:06 - Detecting Safety and Security Faults in PLC Systems with Data Provenance</summary>

- *Abdullah Al Farooq, Jessica Marquard, Kripa George, Thomas Moyer*

- `1911.06304v1` - [abs](http://arxiv.org/abs/1911.06304v1) - [pdf](http://arxiv.org/pdf/1911.06304v1)

> Programmable Logic Controllers are an integral component for managing many different industrial processes (e.g., smart building management, power generation, water and wastewater management, and traffic control systems), and manufacturing and control industries (e.g., oil and natural gas, chemical, pharmaceutical, pulp and paper, food and beverage, automotive, and aerospace). Despite being used widely in many critical infrastructures, PLCs use protocols which make these control systems vulnerable to many common attacks, including man-in-the-middle attacks, denial of service attacks, and memory corruption attacks (e.g., array, stack, and heap overflows, integer overflows, and pointer corruption). In this paper, we propose PLC-PROV, a system for tracking the inputs and outputs of the control system to detect violations in the safety and security policies of the system. We consider a smart building as an example of a PLC-based system and show how PLC-PROV can be applied to ensure that the inputs and outputs are consistent with the intended safety and security policies.

</details>

<details>

<summary>2019-11-15 03:16:12 - Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits</summary>

- *Achyudh Ram, Ji Xin, Meiyappan Nagappan, Yaoliang Yu, Rocío Cabrera Lozoya, Antonino Sabetta, Jimmy Lin*

- `1911.07620v1` - [abs](http://arxiv.org/abs/1911.07620v1) - [pdf](http://arxiv.org/pdf/1911.07620v1)

> Public vulnerability databases such as CVE and NVD account for only 60% of security vulnerabilities present in open-source projects, and are known to suffer from inconsistent quality. Over the last two years, there has been considerable growth in the number of known vulnerabilities across projects available in various repositories such as NPM and Maven Central. Such an increasing risk calls for a mechanism to infer the presence of security threats in a timely manner. We propose novel hierarchical deep learning models for the identification of security-relevant commits from either the commit diff or the source code for the Java classes. By comparing the performance of our model against code2vec, a state-of-the-art model that learns from path-based representations of code, and a logistic regression baseline, we show that deep learning models show promising results in identifying security-related commits. We also conduct a comparative analysis of how various deep learning models learn across different input representations and the effect of regularization on the generalization of our models.

</details>

<details>

<summary>2019-11-15 15:05:57 - Adversarial Robustness Toolbox v1.0.0</summary>

- *Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, Ian M. Molloy, Ben Edwards*

- `1807.01069v4` - [abs](http://arxiv.org/abs/1807.01069v4) - [pdf](http://arxiv.org/pdf/1807.01069v4)

> Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (http://adversarial-robustness-toolbox.readthedocs.io).

</details>

<details>

<summary>2019-11-15 16:26:54 - Breaking the encryption scheme of the Moscow Internet voting system</summary>

- *Pierrick Gaudry, Alexander Golovnev*

- `1908.05127v2` - [abs](http://arxiv.org/abs/1908.05127v2) - [pdf](http://arxiv.org/pdf/1908.05127v2)

> In September 2019, voters for the election at the Parliament of the city of Moscow were allowed to use an Internet voting system. The source code of it had been made available for public testing. In this paper we show two successful attacks on the encryption scheme implemented in the voting system. Both attacks were sent to the developers of the system, and both issues had been fixed after that.The encryption used in this system is a variant of ElGamal over finite fields. In the first attack we show that the used key sizes are too small. We explain how to retrieve the private keys from the public keys in a matter of minutes with easily available resources.When this issue had been fixed and the new system had become available for testing, we discovered that the new implementation was not semantically secure. We demonstrate how this newly found security vulnerability can be used for counting the number of votes cast for a candidate.

</details>

<details>

<summary>2019-11-15 18:17:56 - Computationally Data-Independent Memory Hard Functions</summary>

- *Mohammad Hassan Ameri, Jeremiah Blocki, Samson Zhou*

- `1911.06790v1` - [abs](http://arxiv.org/abs/1911.06790v1) - [pdf](http://arxiv.org/pdf/1911.06790v1)

> Memory hard functions (MHFs) are an important cryptographic primitive that are used to design egalitarian proofs of work and in the construction of moderately expensive key-derivation functions resistant to brute-force attacks. Broadly speaking, MHFs can be divided into two categories: data-dependent memory hard functions (dMHFs) and data-independent memory hard functions (iMHFs). iMHFs are resistant to certain side-channel attacks as the memory access pattern induced by the honest evaluation algorithm is independent of the potentially sensitive input e.g., password. While dMHFs are potentially vulnerable to side-channel attacks (the induced memory access pattern might leak useful information to a brute-force attacker), they can achieve higher cumulative memory complexity (CMC) in comparison than an iMHF. In this paper, we introduce the notion of computationally data-independent memory hard functions (ciMHFs). Intuitively, we require that memory access pattern induced by the (randomized) ciMHF evaluation algorithm appears to be independent from the standpoint of a computationally bounded eavesdropping attacker --- even if the attacker selects the initial input. We then ask whether it is possible to circumvent known upper bound for iMHFs and build a ciMHF with CMC $\Omega(N^2)$. Surprisingly, we answer the question in the affirmative when the ciMHF evaluation algorithm is executed on a two-tiered memory architecture (RAM/Cache).   See paper for the full abstract.

</details>

<details>

<summary>2019-11-15 20:34:37 - Thesis Deployment Optimization of IoT Devices through Attack Graph Analysis</summary>

- *Noga Agmon*

- `1911.06811v1` - [abs](http://arxiv.org/abs/1911.06811v1) - [pdf](http://arxiv.org/pdf/1911.06811v1)

> The Internet of things (IoT) has become an integral part of our life at both work and home. However, these IoT devices are prone to vulnerability exploits due to their low cost, low resources, the diversity of vendors, and proprietary firmware. Moreover, short range communication protocols (e.g., Bluetooth or ZigBee) open additional opportunities for the lateral movement of an attacker within an organization. Thus, the type and location of IoT devices may significantly change the level of network security of the organizational network. In this work, we quantify the level of network security based on an augmented attack graph analysis that accounts for the physical location of IoT devices and their communication capabilities. We use the depth-first branch and bound (DFBnB) heuristic search algorithm to solve two optimization problems: Full Deployment with Minimal Risk (FDMR) and Maximal Utility without Risk Deterioration (MURD). An admissible heuristic is proposed to accelerate the search. The proposed method is evaluated using a real network with simulated deployment of IoT devices. The results demonstrate (1) the contribution of the augmented attack graphs to quantifying the impact of IoT devices deployed within the organization on security, and (2) the effectiveness of the optimized IoT deployment.

</details>

<details>

<summary>2019-11-15 23:07:01 - Adversarial Examples in Modern Machine Learning: A Review</summary>

- *Rey Reza Wiyatno, Anqi Xu, Ousmane Dia, Archy de Berker*

- `1911.05268v2` - [abs](http://arxiv.org/abs/1911.05268v2) - [pdf](http://arxiv.org/pdf/1911.05268v2)

> Recent research has found that many families of machine learning models are vulnerable to adversarial examples: inputs that are specifically designed to cause the target model to produce erroneous outputs. In this survey, we focus on machine learning models in the visual domain, where methods for generating and detecting such examples have been most extensively studied. We explore a variety of adversarial attack methods that apply to image-space content, real world adversarial attacks, adversarial defenses, and the transferability property of adversarial examples. We also discuss strengths and weaknesses of various methods of adversarial attack and defense. Our aim is to provide an extensive coverage of the field, furnishing the reader with an intuitive understanding of the mechanics of adversarial attack and defense mechanisms and enlarging the community of researchers studying this fundamental set of problems.

</details>

<details>

<summary>2019-11-16 03:41:40 - Robust Reading Comprehension with Linguistic Constraints via Posterior Regularization</summary>

- *Mantong Zhou, Minlie Huang, Xiaoyan Zhu*

- `1911.06948v1` - [abs](http://arxiv.org/abs/1911.06948v1) - [pdf](http://arxiv.org/pdf/1911.06948v1)

> In spite of great advancements of machine reading comprehension (RC), existing RC models are still vulnerable and not robust to different types of adversarial examples. Neural models over-confidently predict wrong answers to semantic different adversarial examples, while over-sensitively predict wrong answers to semantic equivalent adversarial examples. Existing methods which improve the robustness of such neural models merely mitigate one of the two issues but ignore the other. In this paper, we address the over-confidence issue and the over-sensitivity issue existing in current RC models simultaneously with the help of external linguistic knowledge. We first incorporate external knowledge to impose different linguistic constraints (entity constraint, lexical constraint, and predicate constraint), and then regularize RC models through posterior regularization. Linguistic constraints induce more reasonable predictions for both semantic different and semantic equivalent adversarial examples, and posterior regularization provides an effective mechanism to incorporate these constraints. Our method can be applied to any existing neural RC models including state-of-the-art BERT models. Extensive experiments show that our method remarkably improves the robustness of base RC models, and is better to cope with these two issues simultaneously.

</details>

<details>

<summary>2019-11-17 21:25:06 - The Limitations of Model Uncertainty in Adversarial Settings</summary>

- *Kathrin Grosse, David Pfaff, Michael Thomas Smith, Michael Backes*

- `1812.02606v2` - [abs](http://arxiv.org/abs/1812.02606v2) - [pdf](http://arxiv.org/pdf/1812.02606v2)

> Machine learning models are vulnerable to adversarial examples: minor perturbations to input samples intended to deliberately cause misclassification. While an obvious security threat, adversarial examples yield as well insights about the applied model itself. We investigate adversarial examples in the context of Bayesian neural network's (BNN's) uncertainty measures. As these measures are highly non-smooth, we use a smooth Gaussian process classifier (GPC) as substitute. We show that both confidence and uncertainty can be unsuspicious even if the output is wrong. Intriguingly, we find subtle differences in the features influencing uncertainty and confidence for most tasks.

</details>

<details>

<summary>2019-11-18 05:53:03 - Simple iterative method for generating targeted universal adversarial perturbations</summary>

- *Hokuto Hirano, Kazuhiro Takemoto*

- `1911.06502v2` - [abs](http://arxiv.org/abs/1911.06502v2) - [pdf](http://arxiv.org/pdf/1911.06502v2)

> Deep neural networks (DNNs) are vulnerable to adversarial attacks. In particular, a single perturbation known as the universal adversarial perturbation (UAP) can foil most classification tasks conducted by DNNs. Thus, different methods for generating UAPs are required to fully evaluate the vulnerability of DNNs. A realistic evaluation would be with cases that consider targeted attacks; wherein the generated UAP causes DNN to classify an input into a specific class. However, the development of UAPs for targeted attacks has largely fallen behind that of UAPs for non-targeted attacks. Therefore, we propose a simple iterative method to generate UAPs for targeted attacks. Our method combines the simple iterative method for generating non-targeted UAPs and the fast gradient sign method for generating a targeted adversarial perturbation for an input. We applied the proposed method to state-of-the-art DNN models for image classification and proved the existence of almost imperceptible UAPs for targeted attacks; further, we demonstrated that such UAPs are easily generatable.

</details>

<details>

<summary>2019-11-18 06:58:20 - Vulnerability Analysis for Data Driven Pricing Schemes</summary>

- *Jingshi Cui, Haoxiang Wang, Chenye Wu, Yang Yu*

- `1911.07453v1` - [abs](http://arxiv.org/abs/1911.07453v1) - [pdf](http://arxiv.org/pdf/1911.07453v1)

> Data analytics and machine learning techniques are being rapidly adopted into the power system, including power system control as well as electricity market design. In this paper, from an adversarial machine learning point of view, we examine the vulnerability of data-driven electricity market design. More precisely, we follow the idea that consumer's load profile should uniquely determine its electricity rate, which yields a clustering oriented pricing scheme. We first identify the strategic behaviors of malicious users by defining a notion of disguising. Based on this notion, we characterize the sensitivity zones to evaluate the percentage of malicious users in each cluster. Based on a thorough cost benefit analysis, we conclude with the vulnerability analysis.

</details>

<details>

<summary>2019-11-18 15:05:09 - A New Ensemble Adversarial Attack Powered by Long-term Gradient Memories</summary>

- *Zhaohui Che, Ali Borji, Guangtao Zhai, Suiyi Ling, Jing Li, Patrick Le Callet*

- `1911.07682v1` - [abs](http://arxiv.org/abs/1911.07682v1) - [pdf](http://arxiv.org/pdf/1911.07682v1)

> Deep neural networks are vulnerable to adversarial attacks.

</details>

<details>

<summary>2019-11-18 15:22:45 - An Attack on the the Encryption Scheme of the Moscow Internet Voting System</summary>

- *Alexander Golovnev*

- `1908.09170v2` - [abs](http://arxiv.org/abs/1908.09170v2) - [pdf](http://arxiv.org/pdf/1908.09170v2)

> The next Moscow City Duma elections will be held on September 8th with an option of Internet voting. Some source code of the voting system is posted online for public testing. Pierrick Gaudry recently showed that due to the relatively small length of the key, the encryption scheme could be easily broken. This issue has been fixed in the current version of the voting system. In this note we show that the new implementation of the ElGamal encryption system is not semantically secure. We also demonstrate how this newly found security vulnerability can be potentially used for counting the number of votes cast for a candidate.

</details>

<details>

<summary>2019-11-18 15:35:16 - Building Fast Fuzzers</summary>

- *Rahul Gopinath, Andreas Zeller*

- `1911.07707v1` - [abs](http://arxiv.org/abs/1911.07707v1) - [pdf](http://arxiv.org/pdf/1911.07707v1)

> Fuzzing is one of the key techniques for evaluating the robustness of programs against attacks. Fuzzing has to be effective in producing inputs that cover functionality and find vulnerabilities. But it also has to be efficient in producing such inputs quickly. Random fuzzers are very efficient, as they can quickly generate random inputs; but they are not very effective, as the large majority of inputs generated is syntactically invalid. Grammar-based fuzzers make use of a grammar (or another model for the input language) to produce syntactically correct inputs, and thus can quickly cover input space and associated functionality. Existing grammar-based fuzzers are surprisingly inefficient, though: Even the fastest grammar fuzzer Dharma still produces inputs about a thousand times slower than the fastest random fuzzer. So far, one can have an effective or an efficient fuzzer, but not both.   In this paper, we describe how to build fast grammar fuzzers from the ground up, treating the problem of fuzzing from a programming language implementation perspective. Starting with a Python textbook approach, we adopt and adapt optimization techniques from functional programming and virtual machine implementation techniques together with other novel domain-specific optimizations in a step-by-step fashion. In our F1 prototype fuzzer, these improve production speed by a factor of 100--300 over the fastest grammar fuzzer Dharma. As F1 is even 5--8 times faster than a lexical random fuzzer, we can find bugs faster and test with much larger valid inputs than previously possible.

</details>

<details>

<summary>2019-11-18 15:52:11 - TaskShuffler++: Real-Time Schedule Randomization for Reducing Worst-Case Vulnerability to Timing Inference Attacks</summary>

- *Man-Ki Yoon, Jung-Eun Kim, Richard Bradford, Zhong Shao*

- `1911.07726v1` - [abs](http://arxiv.org/abs/1911.07726v1) - [pdf](http://arxiv.org/pdf/1911.07726v1)

> This paper presents a schedule randomization algorithm that reduces the vulnerability of real-time systems to timing inference attacks which attempt to learn the timing of task execution. It utilizes run-time information readily available at each scheduling decision point to increase the level of uncertainty in task schedules, while preserving the original schedulability. The randomization algorithm significantly reduces an adversary's best chance to correctly predict what tasks would run at arbitrary times. This paper also proposes an information-theoretic measure that can quantify the worst-case vulnerability, from the defender's perspective, of an arbitrary real-time schedule.

</details>

<details>

<summary>2019-11-19 01:59:59 - Poison as a Cure: Detecting & Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks</summary>

- *Alvin Chan, Yew-Soon Ong*

- `1911.08040v1` - [abs](http://arxiv.org/abs/1911.08040v1) - [pdf](http://arxiv.org/pdf/1911.08040v1)

> Deep learning models have recently shown to be vulnerable to backdoor poisoning, an insidious attack where the victim model predicts clean images correctly but classifies the same images as the target class when a trigger poison pattern is added. This poison pattern can be embedded in the training dataset by the adversary. Existing defenses are effective under certain conditions such as a small size of the poison pattern, knowledge about the ratio of poisoned training samples or when a validated clean dataset is available. Since a defender may not have such prior knowledge or resources, we propose a defense against backdoor poisoning that is effective even when those prerequisites are not met. It is made up of several parts: one to extract a backdoor poison signal, detect poison target and base classes, and filter out poisoned from clean samples with proven guarantees. The final part of our defense involves retraining the poisoned model on a dataset augmented with the extracted poison signal and corrective relabeling of poisoned samples to neutralize the backdoor. Our approach has shown to be effective in defending against backdoor attacks that use both small and large-sized poison patterns on nine different target-base class pairs from the CIFAR10 dataset.

</details>

<details>

<summary>2019-11-19 03:36:57 - Spatiotemporally Constrained Action Space Attacks on Deep Reinforcement Learning Agents</summary>

- *Xian Yeow Lee, Sambit Ghadai, Kai Liang Tan, Chinmay Hegde, Soumik Sarkar*

- `1909.02583v2` - [abs](http://arxiv.org/abs/1909.02583v2) - [pdf](http://arxiv.org/pdf/1909.02583v2)

> Robustness of Deep Reinforcement Learning (DRL) algorithms towards adversarial attacks in real world applications such as those deployed in cyber-physical systems (CPS) are of increasing concern. Numerous studies have investigated the mechanisms of attacks on the RL agent's state space. Nonetheless, attacks on the RL agent's action space (AS) (corresponding to actuators in engineering systems) are equally perverse; such attacks are relatively less studied in the ML literature. In this work, we first frame the problem as an optimization problem of minimizing the cumulative reward of an RL agent with decoupled constraints as the budget of attack. We propose a white-box Myopic Action Space (MAS) attack algorithm that distributes the attacks across the action space dimensions. Next, we reformulate the optimization problem above with the same objective function, but with a temporally coupled constraint on the attack budget to take into account the approximated dynamics of the agent. This leads to the white-box Look-ahead Action Space (LAS) attack algorithm that distributes the attacks across the action and temporal dimensions. Our results shows that using the same amount of resources, the LAS attack deteriorates the agent's performance significantly more than the MAS attack. This reveals the possibility that with limited resource, an adversary can utilize the agent's dynamics to malevolently craft attacks that causes the agent to fail. Additionally, we leverage these attack strategies as a possible tool to gain insights on the potential vulnerabilities of DRL agents.

</details>

<details>

<summary>2019-11-19 04:33:05 - Deep Detector Health Management under Adversarial Campaigns</summary>

- *Javier Echauz, Keith Kenemer, Sarfaraz Hussein, Jay Dhaliwal, Saurabh Shintre, Slawomir Grzonkowski, Andrew Gardner*

- `1911.08090v1` - [abs](http://arxiv.org/abs/1911.08090v1) - [pdf](http://arxiv.org/pdf/1911.08090v1)

> Machine learning models are vulnerable to adversarial inputs that induce seemingly unjustifiable errors. As automated classifiers are increasingly used in industrial control systems and machinery, these adversarial errors could grow to be a serious problem. Despite numerous studies over the past few years, the field of adversarial ML is still considered alchemy, with no practical unbroken defenses demonstrated to date, leaving PHM practitioners with few meaningful ways of addressing the problem. We introduce turbidity detection as a practical superset of the adversarial input detection problem, coping with adversarial campaigns rather than statistically invisible one-offs. This perspective is coupled with ROC-theoretic design guidance that prescribes an inexpensive domain adaptation layer at the output of a deep learning model during an attack campaign. The result aims to approximate the Bayes optimal mitigation that ameliorates the detection model's degraded health. A proactively reactive type of prognostics is achieved via Monte Carlo simulation of various adversarial campaign scenarios, by sampling from the model's own turbidity distribution to quickly deploy the correct mitigation during a real-world campaign.

</details>

<details>

<summary>2019-11-19 19:56:12 - Towards Blockchain-enabled Searchable Encryption</summary>

- *Qiang Tang*

- `1908.09564v2` - [abs](http://arxiv.org/abs/1908.09564v2) - [pdf](http://arxiv.org/pdf/1908.09564v2)

> Distributed Leger Technologies (DLTs), most notably Blockchain technologies, bring decentralised platforms that eliminate a single trusted third party and avoid the notorious single point of failure vulnerability. Since Nakamoto's Bitcoin cryptocurrency system, an enormous number of decentralised applications have been proposed on top of these technologies, aiming at more transparency and trustworthiness than their traditional counterparts. These applications spread over a lot of areas, e.g. financial services, healthcare, transportation, supply chain management, and cloud computing. While Blockchain brings transparency and decentralised trust intuitively due to the consensus of a (very large) group of nodes (or, miners), it introduces very subtle implications for other desirable properties such as privacy. In this work, we demonstrate these subtle implications for Blockchain-based searchable encryption solutions, which are one specific use case of cloud computing services. These solutions rely on Blockchain to achieve both the standard privacy property and the new fairness property, which requires that search operations are carried out faithfully and are rewarded accordingly. We show that directly replacing the server in an existing searchable encryption solution with a Blockchain will cause undesirable operational cost, privacy loss, and security vulnerabilities. The analysis results indicate that a dedicated server is still needed to achieve the desired privacy guarantee. To this end, we propose two frameworks which can be instantiated based on most existing searchable encryption schemes. Through analysing these two frameworks, we affirmatively show that a carefully engineered Blockchain-based solution can achieve the desired fairness property while preserving the privacy guarantee of the original searchable encryption scheme simultaneously.

</details>

<details>

<summary>2019-11-19 21:43:06 - Forbidden knowledge in machine learning -- Reflections on the limits of research and publication</summary>

- *Thilo Hagendorff*

- `1911.08603v1` - [abs](http://arxiv.org/abs/1911.08603v1) - [pdf](http://arxiv.org/pdf/1911.08603v1)

> Certain research strands can yield "forbidden knowledge". This term refers to knowledge that is considered too sensitive, dangerous or taboo to be produced or shared. Discourses about such publication restrictions are already entrenched in scientific fields like IT security, synthetic biology or nuclear physics research. This paper makes the case for transferring this discourse to machine learning research. Some machine learning applications can very easily be misused and unfold harmful consequences, for instance with regard to generative video or text synthesis, personality analysis, behavior manipulation, software vulnerability detection and the like. Up to now, the machine learning research community embraces the idea of open access. However, this is opposed to precautionary efforts to prevent the malicious use of machine learning applications. Information about or from such applications may, if improperly disclosed, cause harm to people, organizations or whole societies. Hence, the goal of this work is to outline norms that can help to decide whether and when the dissemination of such information should be prevented. It proposes review parameters for the machine learning community to establish an ethical framework on how to deal with forbidden knowledge and dual-use applications.

</details>

<details>

<summary>2019-11-19 22:38:16 - A Benchmark Suite for Evaluating Caches' Vulnerability to Timing Attacks</summary>

- *Shuwen Deng, Wenjie Xiong, Jakub Szefer*

- `1911.08619v1` - [abs](http://arxiv.org/abs/1911.08619v1) - [pdf](http://arxiv.org/pdf/1911.08619v1)

> Timing-based side or covert channels in processor caches continue to present a threat to computer systems, and they are the key to many of the recent Spectre and Meltdown attacks. Based on improvements to an existing three-step model for cache timing-based attacks, this work presents 88 Strong types of theoretical timing-based vulnerabilities in processor caches. To understand and evaluate all possible types of vulnerabilities in processor caches, this work further presents and implements a new benchmark suite which can be used to test to which types of cache timing-based attacks a given processor or cache design is vulnerable. In total, there are 1094 automatically-generated test programs which cover the 88 theoretical vulnerabilities. The benchmark suite generates the Cache Timing Vulnerability Score which can be used to evaluate how vulnerable a specific cache implementation is to different attacks. A smaller Cache Timing Vulnerability Score means the design is more secure, and the scores among different machines can be easily compared. Evaluation is conducted on commodity Intel and AMD processors and shows the differences in processor implementations can result in different types of attacks that they are vulnerable to. Beyond testing commodity processors, the benchmarks and the Cache Timing Vulnerability Score can be used to help designers of new secure processor caches evaluate their design's susceptibility to cache timing-based attacks.

</details>

<details>

<summary>2019-11-20 06:11:48 - Deep Minimax Probability Machine</summary>

- *Lirong He, Ziyi Guo, Kaizhu Huang, Zenglin Xu*

- `1911.08723v1` - [abs](http://arxiv.org/abs/1911.08723v1) - [pdf](http://arxiv.org/pdf/1911.08723v1)

> Deep neural networks enjoy a powerful representation and have proven effective in a number of applications. However, recent advances show that deep neural networks are vulnerable to adversarial attacks incurred by the so-called adversarial examples. Although the adversarial example is only slightly different from the input sample, the neural network classifies it as the wrong class. In order to alleviate this problem, we propose the Deep Minimax Probability Machine (DeepMPM), which applies MPM to deep neural networks in an end-to-end fashion. In a worst-case scenario, MPM tries to minimize an upper bound of misclassification probabilities, considering the global information (i.e., mean and covariance information of each class). DeepMPM can be more robust since it learns the worst-case bound on the probability of misclassification of future data. Experiments on two real-world datasets can achieve comparable classification performance with CNN, while can be more robust on adversarial attacks.

</details>

<details>

<summary>2019-11-20 21:22:50 - Defense Methods Against Adversarial Examples for Recurrent Neural Networks</summary>

- *Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, Lior Rokach*

- `1901.09963v5` - [abs](http://arxiv.org/abs/1901.09963v5) - [pdf](http://arxiv.org/pdf/1901.09963v5)

> Adversarial examples are known to mislead deep learning models to incorrectly classify them, even in domains where such models achieve state-of-the-art performance. Until recently, research on both attack and defense methods focused on image recognition, primarily using convolutional neural networks (CNNs). In recent years, adversarial example generation methods for recurrent neural networks (RNNs) have been published, demonstrating that RNN classifiers are also vulnerable to such attacks. In this paper, we present a novel defense method, termed sequence squeezing, to make RNN classifiers more robust against such attacks. Our method differs from previous defense methods which were designed only for non-sequence based models. We also implement four additional RNN defense methods inspired by recently published CNN defense methods. We evaluate our methods against state-of-the-art attacks in the cyber security domain where real adversaries (malware developers) exist, but our methods can be applied against other discrete sequence based adversarial attacks, e.g., in the NLP domain. Using our methods we were able to decrease the effectiveness of such attack from 99.9% to 15%.

</details>

<details>

<summary>2019-11-21 16:25:44 - The Performance of Machine and Deep Learning Classifiers in Detecting Zero-Day Vulnerabilities</summary>

- *Faranak Abri, Sima Siami-Namini, Mahdi Adl Khanghah, Fahimeh Mirza Soltani, Akbar Siami Namin*

- `1911.09586v1` - [abs](http://arxiv.org/abs/1911.09586v1) - [pdf](http://arxiv.org/pdf/1911.09586v1)

> The detection of zero-day attacks and vulnerabilities is a challenging problem. It is of utmost importance for network administrators to identify them with high accuracy. The higher the accuracy is, the more robust the defense mechanism will be. In an ideal scenario (i.e., 100% accuracy) the system can detect zero-day malware without being concerned about mistakenly tagging benign files as malware or enabling disruptive malicious code running as none-malicious ones. This paper investigates different machine learning algorithms to find out how well they can detect zero-day malware. Through the examination of 34 machine/deep learning classifiers, we found that the random forest classifier offered the best accuracy. The paper poses several research questions regarding the performance of machine and deep learning algorithms when detecting zero-day malware with zero rates for false positive and false negative.

</details>

<details>

<summary>2019-11-21 19:07:45 - Blackbox Attacks on Reinforcement Learning Agents Using Approximated Temporal Information</summary>

- *Yiren Zhao, Ilia Shumailov, Han Cui, Xitong Gao, Robert Mullins, Ross Anderson*

- `1909.02918v2` - [abs](http://arxiv.org/abs/1909.02918v2) - [pdf](http://arxiv.org/pdf/1909.02918v2)

> Recent research on reinforcement learning (RL) has suggested that trained agents are vulnerable to maliciously crafted adversarial samples. In this work, we show how such samples can be generalised from White-box and Grey-box attacks to a strong Black-box case, where the attacker has no knowledge of the agents, their training parameters and their training methods. We use sequence-to-sequence models to predict a single action or a sequence of future actions that a trained agent will make. First, we show our approximation model, based on time-series information from the agent, consistently predicts RL agents' future actions with high accuracy in a Black-box setup on a wide range of games and RL algorithms. Second, we find that although adversarial samples are transferable from the target model to our RL agents, they often outperform random Gaussian noise only marginally. This highlights a serious methodological deficiency in previous work on such agents; random jamming should have been taken as the baseline for evaluation. Third, we propose a novel use for adversarial samplesin Black-box attacks of RL agents: they can be used to trigger a trained agent to misbehave after a specific time delay. This appears to be a genuinely new type of attack. It potentially enables an attacker to use devices controlled by RL agents as time bombs.

</details>

<details>

<summary>2019-11-21 22:54:40 - Effects of Differential Privacy and Data Skewness on Membership Inference Vulnerability</summary>

- *Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Wenqi Wei, Lei Yu*

- `1911.09777v1` - [abs](http://arxiv.org/abs/1911.09777v1) - [pdf](http://arxiv.org/pdf/1911.09777v1)

> Membership inference attacks seek to infer the membership of individual training instances of a privately trained model. This paper presents a membership privacy analysis and evaluation system, called MPLens, with three unique contributions. First, through MPLens, we demonstrate how membership inference attack methods can be leveraged in adversarial machine learning. Second, through MPLens, we highlight how the vulnerability of pre-trained models under membership inference attack is not uniform across all classes, particularly when the training data itself is skewed. We show that risk from membership inference attacks is routinely increased when models use skewed training data. Finally, we investigate the effectiveness of differential privacy as a mitigation technique against membership inference attacks. We discuss the trade-offs of implementing such a mitigation strategy with respect to the model complexity, the learning task complexity, the dataset complexity and the privacy parameter settings. Our empirical results reveal that (1) minority groups within skewed datasets display increased risk for membership inference and (2) differential privacy presents many challenging trade-offs as a mitigation technique to membership inference risk.

</details>

<details>

<summary>2019-11-22 06:41:15 - On the Robustness of Signal Characteristic-Based Sender Identification</summary>

- *Marcel Kneib, Oleg Schell, Christopher Huth*

- `1911.09881v1` - [abs](http://arxiv.org/abs/1911.09881v1) - [pdf](http://arxiv.org/pdf/1911.09881v1)

> Vehicles become more vulnerable to remote attackers in modern days due to their increasing connectivity and range of functionality. Such increased attack vectors enable adversaries to access a vehicle Electronic Control Unit (ECU). As of today in-vehicle access can cause drastic consequences, because the most commonly used in-vehicle bus technology, the Controller Area Network (CAN), lacks sender identification. With low limits on bandwidth and payload, as well as resource constrains on hardware, usage of cryptographic measures is limited. As an alternative, sender identification methods were presented, identifying the sending ECU on the basis of its analog message signal. While prior works showed promising results on the security and feasibility for those approaches, the potential changes in signals over a vehicle's lifetime have only been partly addressed. This paper closes this gap. We conduct a 4~months measurement campaign containing more than 80,000 frames from a real vehicle. The data reflects different driving situations, different seasons and weather conditions, a 19-week break, and a car repair altering the physical CAN properties. We demonstrate the impact of temperature dependencies, analyze the signal changes and define strategies for their handling. In the evaluation, the identification rate can be increased from 91.23% to 99.98% by a targeted updating of the system parameters. At the same time, the detection of intrusions can be improved from 76.83% to 99.74%, while no false positives occured during evaluation. Lastly, we show how to increase the overall performance of such systems by double monitoring the bus at different positions.

</details>

<details>

<summary>2019-11-22 10:00:02 - Migration through Machine Learning Lens -- Predicting Sexual and Reproductive Health Vulnerability of Young Migrants</summary>

- *Amber Nigam, Pragati Jaiswal, Uma Girkar, Teertha Arora, Leo A. Celi*

- `1910.02390v4` - [abs](http://arxiv.org/abs/1910.02390v4) - [pdf](http://arxiv.org/pdf/1910.02390v4)

> In this paper, we have discussed initial findings and results of our experiment to predict sexual and reproductive health vulnerabilities of migrants in a data-constrained environment. Notwithstanding the limited research and data about migrants and migration cities, we propose a solution that simultaneously focuses on data gathering from migrants, augmenting awareness of the migrants to reduce mishaps, and setting up a mechanism to present insights to the key stakeholders in migration to act upon. We have designed a webapp for the stakeholders involved in migration: migrants, who would participate in data gathering process and can also use the app for getting to know safety and awareness tips based on analysis of the data received; public health workers, who would have an access to the database of migrants on the app; policy makers, who would have a greater understanding of the ground reality, and of the patterns of migration through machine-learned analysis. Finally, we have experimented with different machine learning models on an artificially curated dataset. We have shown, through experiments, how machine learning can assist in predicting the migrants at risk and can also help in identifying the critical factors that make migration dangerous for migrants. The results for identifying vulnerable migrants through machine learning algorithms are statistically significant at an alpha of 0.05.

</details>

<details>

<summary>2019-11-22 13:54:12 - SolidityCheck : Quickly Detecting Smart Contract Problems Through Regular Expressions</summary>

- *Pengcheng Zhang, Feng Xiao, Xiapu Luo*

- `1911.09425v2` - [abs](http://arxiv.org/abs/1911.09425v2) - [pdf](http://arxiv.org/pdf/1911.09425v2)

> As a blockchain platform that has developed vigorously in recent years, Ethereum is different from Bitcoin in that it introduces smart contracts into blockchain.Solidity is one of the most mature and widely used smart contract programming language,which is used to write smart contracts and deploy them on blockchain. However, once the data in the blockchain is written, it cannot be modified. Ethereum smart contract is stored in the block chain, which makes the smart contract can no longer repair the code problems such as re-entrancy vulnerabilities or integer overflow problems. Currently, there still lacks of an efficient and effective approach for detecting these problems in Solidity. In this paper, we first classify all the possible problems in Solidity, then propose a smart contract problem detection approach for Solidity, namely SolidityCheck. The approach uses regular expressions to define the characteristics of problematic statements and uses regular matching and program instrumentation to prevent or detect problems. Finally, a large number of experiments is performed to show that SolidityCheck is superior to existing approaches.

</details>

<details>

<summary>2019-11-22 15:54:46 - FlipIn: A Game-Theoretic Cyber Insurance Framework for Incentive-Compatible Cyber Risk Management of Internet of Things</summary>

- *Rui Zhang, Quanyan Zhu*

- `1911.10100v1` - [abs](http://arxiv.org/abs/1911.10100v1) - [pdf](http://arxiv.org/pdf/1911.10100v1)

> Internet of Things (IoT) is highly vulnerable to emerging Advanced Persistent Threats (APTs) that are often operated by well-resourced adversaries. Achieving perfect security for IoT networks is often cost-prohibitive if not impossible. Cyber insurance is a valuable mechanism to mitigate cyber risks for IoT systems. In this work, we propose a bi-level game-theoretic framework called FlipIn to design incentive-compatible and welfare-maximizing cyber insurance contracts. The framework captures the strategic interactions among APT attackers, IoT defenders, and cyber insurance insurers, and incorporates influence networks to assess the systemic cyber risks of interconnected IoT devices. The FlipIn framework formulates a game over networks within a principal-agent problem of moral-hazard type to design a cyber risk-aware insurance contract. We completely characterize the equilibrium solutions of the bi-level games for a network of distributed defenders and a semi-homogeneous centralized defender and show that the optimal insurance contracts cover half of the defenders' losses. Our framework predicts the risk compensation of defenders and the Peltzman effect of insurance. We study a centralized security management scenario and its decentralized counterpart, and leverage numerical experiments to show that network connectivity plays an important role in the security of the IoT devices and the insurability of both distributed and centralized defenders.

</details>

<details>

<summary>2019-11-22 23:08:17 - Enhancing Cross-task Black-Box Transferability of Adversarial Examples with Dispersion Reduction</summary>

- *Yantao Lu, Yunhan Jia, Jianyu Wang, Bai Li, Weiheng Chai, Lawrence Carin, Senem Velipasalar*

- `1911.11616v1` - [abs](http://arxiv.org/abs/1911.11616v1) - [pdf](http://arxiv.org/pdf/1911.11616v1)

> Neural networks are known to be vulnerable to carefully crafted adversarial examples, and these malicious samples often transfer, i.e., they remain adversarial even against other models. Although great efforts have been delved into the transferability across models, surprisingly, less attention has been paid to the cross-task transferability, which represents the real-world cybercriminal's situation, where an ensemble of different defense/detection mechanisms need to be evaded all at once. In this paper, we investigate the transferability of adversarial examples across a wide range of real-world computer vision tasks, including image classification, object detection, semantic segmentation, explicit content detection, and text detection. Our proposed attack minimizes the ``dispersion'' of the internal feature map, which overcomes existing attacks' limitation of requiring task-specific loss functions and/or probing a target model. We conduct evaluation on open source detection and segmentation models as well as four different computer vision tasks provided by Google Cloud Vision (GCV) APIs, to show how our approach outperforms existing attacks by degrading performance of multiple CV tasks by a large margin with only modest perturbations linf=16.

</details>

<details>

<summary>2019-11-23 13:02:08 - Procedural Noise Adversarial Examples for Black-Box Attacks on Deep Convolutional Networks</summary>

- *Kenneth T. Co, Luis Muñoz-González, Sixte de Maupeou, Emil C. Lupu*

- `1810.00470v4` - [abs](http://arxiv.org/abs/1810.00470v4) - [pdf](http://arxiv.org/pdf/1810.00470v4)

> Deep Convolutional Networks (DCNs) have been shown to be vulnerable to adversarial examples---perturbed inputs specifically designed to produce intentional errors in the learning algorithms at test time. Existing input-agnostic adversarial perturbations exhibit interesting visual patterns that are currently unexplained. In this paper, we introduce a structured approach for generating Universal Adversarial Perturbations (UAPs) with procedural noise functions. Our approach unveils the systemic vulnerability of popular DCN models like Inception v3 and YOLO v3, with single noise patterns able to fool a model on up to 90% of the dataset. Procedural noise allows us to generate a distribution of UAPs with high universal evasion rates using only a few parameters. Additionally, we propose Bayesian optimization to efficiently learn procedural noise parameters to construct inexpensive untargeted black-box attacks. We demonstrate that it can achieve an average of less than 10 queries per successful attack, a 100-fold improvement on existing methods. We further motivate the use of input-agnostic defences to increase the stability of models to adversarial perturbations. The universality of our attacks suggests that DCN models may be sensitive to aggregations of low-level class-agnostic features. These findings give insight on the nature of some universal adversarial perturbations and how they could be generated in other applications.

</details>

<details>

<summary>2019-11-24 08:41:51 - ContractGuard: Defend Ethereum Smart Contracts with Embedded Intrusion Detection</summary>

- *Xinming Wang, Jiahao He, Zhijian Xie, Gansen Zhao, Shing-Chi Cheung*

- `1911.10472v1` - [abs](http://arxiv.org/abs/1911.10472v1) - [pdf](http://arxiv.org/pdf/1911.10472v1)

> Ethereum smart contracts are programs that can be collectively executed by a network of mutually untrusted nodes. Smart contracts handle and transfer assets of values, offering strong incentives for malicious attacks. Intrusion attacks are a popular type of malicious attacks. In this paper, we propose ContractGuard, the first intrusion detection system (IDS) to defend Ethereum smart contracts against such attacks. Like IDSs for conventional programs, ContractGuard detects intrusion attempts as abnormal control flow. However, existing IDS techniques/tools are inapplicable to Ethereum smart contracts due to Ethereum's decentralized nature and its highly restrictive execution environment. To address these issues, we design ContractGuard by embedding it in the contracts to profile context-tagged acyclic paths, and optimizing it under the Ethereum gas-oriented performance model. The main goal is to minimize the overheads, to which the users will be extremely sensitive since the cost needs to be paid upfront in digital concurrency. Empirical investigation using real-life contracts deployed in the Ethereum mainnet shows that on average, ContractGuard only adds to 36.14% of the deployment overhead and 28.27% of the runtime overhead. Furthermore, we conducted controlled experiments and show that ContractGuard successfully guard against attacks on all real-world vulnerabilities and 83% of the seeded vulnerabilities.

</details>

<details>

<summary>2019-11-25 00:34:14 - Analyzing Federated Learning through an Adversarial Lens</summary>

- *Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, Seraphin Calo*

- `1811.12470v4` - [abs](http://arxiv.org/abs/1811.12470v4) - [pdf](http://arxiv.org/pdf/1811.12470v4)

> Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server. In this work, we explore the threat of model poisoning attacks on federated learning initiated by a single, non-colluding malicious agent where the adversarial objective is to cause the model to misclassify a set of chosen inputs with high confidence. We explore a number of strategies to carry out this attack, starting with simple boosting of the malicious agent's update to overcome the effects of other agents' updates. To increase attack stealth, we propose an alternating minimization strategy, which alternately optimizes for the training loss and the adversarial objective. We follow up by using parameter estimation for the benign agents' updates to improve on attack success. Finally, we use a suite of interpretability techniques to generate visual explanations of model decisions for both benign and malicious models and show that the explanations are nearly visually indistinguishable. Our results indicate that even a highly constrained adversary can carry out model poisoning attacks while simultaneously maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.

</details>

<details>

<summary>2019-11-25 06:05:27 - Attack on Grid Event Cause Analysis: An Adversarial Machine Learning Approach</summary>

- *Iman Niazazari, Hanif Livani*

- `1911.08011v2` - [abs](http://arxiv.org/abs/1911.08011v2) - [pdf](http://arxiv.org/pdf/1911.08011v2)

> With the ever-increasing reliance on data for data-driven applications in power grids, such as event cause analysis, the authenticity of data streams has become crucially important. The data can be prone to adversarial stealthy attacks aiming to manipulate the data such that residual-based bad data detectors cannot detect them, and the perception of system operators or event classifiers changes about the actual event. This paper investigates the impact of adversarial attacks on convolutional neural network-based event cause analysis frameworks. We have successfully verified the ability of adversaries to maliciously misclassify events through stealthy data manipulations. The vulnerability assessment is studied with respect to the number of compromised measurements. Furthermore, a defense mechanism to robustify the performance of the event cause analysis is proposed. The effectiveness of adversarial attacks on changing the output of the framework is studied using the data generated by real-time digital simulator (RTDS) under different scenarios such as type of attacks and level of access to data.

</details>

<details>

<summary>2019-11-27 03:31:04 - Survey of Attacks and Defenses on Edge-Deployed Neural Networks</summary>

- *Mihailo Isakov, Vijay Gadepally, Karen M. Gettings, Michel A. Kinsy*

- `1911.11932v1` - [abs](http://arxiv.org/abs/1911.11932v1) - [pdf](http://arxiv.org/pdf/1911.11932v1)

> Deep Neural Network (DNN) workloads are quickly moving from datacenters onto edge devices, for latency, privacy, or energy reasons. While datacenter networks can be protected using conventional cybersecurity measures, edge neural networks bring a host of new security challenges. Unlike classic IoT applications, edge neural networks are typically very compute and memory intensive, their execution is data-independent, and they are robust to noise and faults. Neural network models may be very expensive to develop, and can potentially reveal information about the private data they were trained on, requiring special care in distribution. The hidden states and outputs of the network can also be used in reconstructing user inputs, potentially violating users' privacy. Furthermore, neural networks are vulnerable to adversarial attacks, which may cause misclassifications and violate the integrity of the output. These properties add challenges when securing edge-deployed DNNs, requiring new considerations, threat models, priorities, and approaches in securely and privately deploying DNNs to the edge. In this work, we cover the landscape of attacks on, and defenses, of neural networks deployed in edge devices and provide a taxonomy of attacks and defenses targeting edge DNNs.

</details>

<details>

<summary>2019-11-27 18:22:51 - XSS Vulnerabilities in Cloud-Application Add-Ons</summary>

- *Thanh Bui, Siddharth Rao, Markku Antikainen, Tuomas Aura*

- `1911.12332v1` - [abs](http://arxiv.org/abs/1911.12332v1) - [pdf](http://arxiv.org/pdf/1911.12332v1)

> Cloud-application add-ons are microservices that extend the functionality of the core applications. Many application vendors have opened their APIs for third-party developers and created marketplaces for add-ons (also add-ins or apps). This is a relatively new phenomenon, and its effects on the application security have not been widely studied. It seems likely that some of the add-ons have lower code quality than the core applications themselves and, thus, may bring in security vulnerabilities. We found that many such add-ons are vulnerable to cross-site scripting (XSS). The attacker can take advantage of the document-sharing and messaging features of the cloud applications to send malicious input to them. The vulnerable add-ons then execute client-side JavaScript from the carefully crafted malicious input. In a major analysis effort, we systematically studied 300 add-ons for three popular application suites, namely Microsoft Office Online, G Suite and Shopify, and discovered a significant percentage of vulnerable add-ons in each marketplace. We present the results of this study, as well as analyze the add-on architectures to understand how the XSS vulnerabilities can be exploited and how the threat can be mitigated.

</details>

<details>

<summary>2019-11-27 21:10:06 - One Man's Trash is Another Man's Treasure: Resisting Adversarial Examples by Adversarial Examples</summary>

- *Chang Xiao, Changxi Zheng*

- `1911.11219v2` - [abs](http://arxiv.org/abs/1911.11219v2) - [pdf](http://arxiv.org/pdf/1911.11219v2)

> Modern image classification systems are often built on deep neural networks, which suffer from adversarial examples--images with deliberately crafted, imperceptible noise to mislead the network's classification. To defend against adversarial examples, a plausible idea is to obfuscate the network's gradient with respect to the input image. This general idea has inspired a long line of defense methods. Yet, almost all of them have proven vulnerable. We revisit this seemingly flawed idea from a radically different perspective. We embrace the omnipresence of adversarial examples and the numerical procedure of crafting them, and turn this harmful attacking process into a useful defense mechanism. Our defense method is conceptually simple: before feeding an input image for classification, transform it by finding an adversarial example on a pre-trained external model. We evaluate our method against a wide range of possible attacks. On both CIFAR-10 and Tiny ImageNet datasets, our method is significantly more robust than state-of-the-art methods. Particularly, in comparison to adversarial training, our method offers lower training cost as well as stronger robustness.

</details>

<details>

<summary>2019-11-28 02:23:19 - Spot Evasion Attacks: Adversarial Examples for License Plate Recognition Systems with Convolutional Neural Networks</summary>

- *Ya-guan Qian, Dan-feng Ma, Bin Wang, Jun Pan, Jia-min Wang, Jian-hai Chen, Wu-jie Zhou, Jing-sheng Lei*

- `1911.00927v2` - [abs](http://arxiv.org/abs/1911.00927v2) - [pdf](http://arxiv.org/pdf/1911.00927v2)

> Recent studies have shown convolution neural networks (CNNs) for image recognition are vulnerable to evasion attacks with carefully manipulated adversarial examples. Previous work primarily focused on how to generate adversarial examples closed to source images, by introducing pixel-level perturbations into the whole or specific part of images. In this paper, we propose an evasion attack on CNN classifiers in the context of License Plate Recognition (LPR), which adds predetermined perturbations to specific regions of license plate images, simulating some sort of naturally formed spots (such as sludge, etc.). Therefore, the problem is modeled as an optimization process searching for optimal perturbation positions, which is different from previous work that consider pixel values as decision variables. Notice that this is a complex nonlinear optimization problem, and we use a genetic-algorithm based approach to obtain optimal perturbation positions. In experiments, we use the proposed algorithm to generate various adversarial examples in the form of rectangle, circle, ellipse and spots cluster. Experimental results show that these adversarial examples are almost ignored by human eyes, but can fool HyperLPR with high attack success rate over 93%. Therefore, we believe that this kind of spot evasion attacks would pose a great threat to current LPR systems, and needs to be investigated further by the security community.

</details>

<details>

<summary>2019-11-29 05:22:47 - Drndalo: Lightweight Control Flow Obfuscation Through Minimal Processor/Compiler Co-Design</summary>

- *Novak Boskov, Mihailo Isakov, Michel A. Kinsy*

- `1912.01560v1` - [abs](http://arxiv.org/abs/1912.01560v1) - [pdf](http://arxiv.org/pdf/1912.01560v1)

> Binary analysis is traditionally used in the realm of malware detection. However, the same technique may be employed by an attacker to analyze the original binaries in order to reverse engineer them and extract exploitable weaknesses. When a binary is distributed to end users, it becomes a common remotely exploitable attack point. Code obfuscation is used to hinder reverse engineering of executable programs. In this paper, we focus on securing binary distribution, where attackers gain access to binaries distributed to end devices, in order to reverse engineer them and find potential vulnerabilities. Attackers do not however have means to monitor the execution of said devices. In particular, we focus on the control flow obfuscation --- a technique that prevents an attacker from restoring the correct reachability conditions for the basic blocks of a program. By doing so, we thwart attackers in their effort to infer the inputs that cause the program to enter a vulnerable state (e.g., buffer overrun). We propose a compiler extension for obfuscation and a minimal hardware modification for dynamic deobfuscation that takes advantage of a secret key stored in hardware. We evaluate our experiments on the LLVM compiler toolchain and the BRISC-V open source processor. On PARSEC benchmarks, our deobfuscation technique incurs only a 5\% runtime overhead. We evaluate the security of Drndalo by training classifiers on pairs of obfuscated and unobfuscated binaries. Our results shine light on the difficulty of producing obfuscated binaries of arbitrary programs in such a way that they are statistically indistinguishable from plain binaries.

</details>

<details>

<summary>2019-11-29 09:04:27 - RESCUE: Interdependent Challenges of Reliability, Security and Quality in Nanoelectronic Systems</summary>

- *Maksim Jenihhin, Said Hamdioui, Matteo Sonza Reorda, Milos Krstic, Peter Langendoerfer, Christian Sauer, Anton Klotz, Michael Huebner, Joerg Nolte, Heinrich Theodor Vierhaus, Georgios Selimis, Dan Alexandrescu, Mottaqiallah Taouil, Geert-Jan Schrijen, Jaan Raik, Luca Sterpone, Giovanni Squillero, Zoya Dyka*

- `1912.01561v1` - [abs](http://arxiv.org/abs/1912.01561v1) - [pdf](http://arxiv.org/pdf/1912.01561v1)

> The recent trends for nanoelectronic computing systems include machine-to-machine communication in the era of Internet-of-Things (IoT) and autonomous systems, complex safety-critical applications, extreme miniaturization of implementation technologies and intensive interaction with the physical world. These set tough requirements on mutually dependent extra-functional design aspects. The H2020 MSCA ITN project RESCUE is focused on key challenges for reliability, security and quality, as well as related electronic design automation tools and methodologies. The objectives include both research advancements and cross-sectoral training of a new generation of interdisciplinary researchers. Notable interdisciplinary collaborative research results for the first half-period include novel approaches for test generation, soft-error and transient faults vulnerability analysis, cross-layer fault-tolerance and error-resilience, functional safety validation, reliability assessment and run-time management, HW security enhancement and initial implementation of these into holistic EDA tools.

</details>

<details>

<summary>2019-11-29 11:10:18 - AdvKnn: Adversarial Attacks On K-Nearest Neighbor Classifiers With Approximate Gradients</summary>

- *Xiaodan Li, Yuefeng Chen, Yuan He, Hui Xue*

- `1911.06591v2` - [abs](http://arxiv.org/abs/1911.06591v2) - [pdf](http://arxiv.org/pdf/1911.06591v2)

> Deep neural networks have been shown to be vulnerable to adversarial examples---maliciously crafted examples that can trigger the target model to misbehave by adding imperceptible perturbations. Existing attack methods for k-nearest neighbor~(kNN) based algorithms either require large perturbations or are not applicable for large k. To handle this problem, this paper proposes a new method called AdvKNN for evaluating the adversarial robustness of kNN-based models. Firstly, we propose a deep kNN block to approximate the output of kNN methods, which is differentiable thus can provide gradients for attacks to cross the decision boundary with small distortions. Second, a new consistency learning for distribution instead of classification is proposed for the effectiveness in distribution based methods. Extensive experimental results indicate that the proposed method significantly outperforms state of the art in terms of attack success rate and the added perturbations.

</details>

<details>

<summary>2019-11-30 14:06:20 - Towards Plausible Graph Anonymization</summary>

- *Yang Zhang, Mathias Humbert, Bartlomiej Surma, Praveen Manoharan, Jilles Vreeken, Michael Backes*

- `1711.05441v2` - [abs](http://arxiv.org/abs/1711.05441v2) - [pdf](http://arxiv.org/pdf/1711.05441v2)

> Social graphs derived from online social interactions contain a wealth of information that is nowadays extensively used by both industry and academia. However, as social graphs contain sensitive information, they need to be properly anonymized before release. Most of the existing graph anonymization mechanisms rely on the perturbation of the original graph's edge set. In this paper, we identify a fundamental weakness of these mechanisms: They neglect the strong structural proximity between friends in social graphs, thus add implausible fake edges for anonymization.   To exploit this weakness, we first propose a metric to quantify an edge's plausibility by relying on graph embedding. Extensive experiments on three real-life social network datasets demonstrate that our plausibility metric can very effectively differentiate fake edges from original edges with AUC (area under the ROC curve) values above 0.95 in most of the cases. We then rely on a Gaussian mixture model to automatically derive the threshold on the edge plausibility values to determine whether an edge is fake, which enables us to recover to a large extent the original graph from the anonymized graph. We further demonstrate that our graph recovery attack jeopardizes the privacy guarantees provided by the considered graph anonymization mechanisms.   To mitigate this vulnerability, we propose a method to generate fake yet plausible edges given the graph structure and incorporate it into the existing anonymization mechanisms. Our evaluation demonstrates that the enhanced mechanisms decrease the chances of graph recovery, reduce the success of graph de-anonymization (up to 30%), and provide even better utility than the existing anonymization mechanisms.

</details>


## 2019-12

<details>

<summary>2019-12-01 04:41:37 - An Observational Investigation of Reverse Engineers' Processes</summary>

- *Daniel Votipka, Seth M. Rabin, Kristopher Micinski, Jeffrey S. Foster, Michelle L. Mazurek*

- `1912.00317v1` - [abs](http://arxiv.org/abs/1912.00317v1) - [pdf](http://arxiv.org/pdf/1912.00317v1)

> Reverse engineering is a complex process essential to software-security tasks such as vulnerability discovery and malware analysis. Significant research and engineering effort has gone into developing tools to support reverse engineers. However, little work has been done to understand the way reverse engineers think when analyzing programs, leaving tool developers to make interface design decisions based only on intuition.   This paper takes a first step toward a better understanding of reverse engineers' processes, with the goal of producing insights for improving interaction design for reverse engineering tools. We present the results of a semi-structured, observational interview study of reverse engineers (N=16). Each observation investigated the questions reverse engineers ask as they probe a program, how they answer these questions, and the decisions they make throughout the reverse engineering process. From the interview responses, we distill a model of the reverse engineering process, divided into three phases: overview, sub-component scanning, and focused experimentation. Each analysis phase's results feed the next as reverse engineers' mental representations become more concrete. We find that reverse engineers typically use static methods in the first two phases, but dynamic methods in the final phase, with experience playing large, but varying, roles in each phase. % and the role of experience varies between phases. Based on these results, we provide five interaction design guidelines for reverse engineering tools.

</details>

<details>

<summary>2019-12-01 06:26:09 - Adversary A3C for Robust Reinforcement Learning</summary>

- *Zhaoyuan Gu, Zhenzhong Jia, Howie Choset*

- `1912.00330v1` - [abs](http://arxiv.org/abs/1912.00330v1) - [pdf](http://arxiv.org/pdf/1912.00330v1)

> Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent's performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments.

</details>

<details>

<summary>2019-12-02 20:16:47 - Effect of Imbalanced Datasets on Security of Industrial IoT Using Machine Learning</summary>

- *Maede Zolanvari, Marcio A. Teixeira, Raj Jain*

- `1912.02651v1` - [abs](http://arxiv.org/abs/1912.02651v1) - [pdf](http://arxiv.org/pdf/1912.02651v1)

> Machine learning algorithms have been shown to be suitable for securing platforms for IT systems. However, due to the fundamental differences between the industrial internet of things (IIoT) and regular IT networks, a special performance review needs to be considered. The vulnerabilities and security requirements of IIoT systems demand different considerations. In this paper, we study the reasons why machine learning must be integrated into the security mechanisms of the IIoT, and where it currently falls short in having a satisfactory performance. The challenges and real-world considerations associated with this matter are studied in our experimental design. We use an IIoT testbed resembling a real industrial plant to show our proof of concept.

</details>

<details>

<summary>2019-12-02 22:37:09 - Adversarially Robust Distillation</summary>

- *Micah Goldblum, Liam Fowl, Soheil Feizi, Tom Goldstein*

- `1905.09747v2` - [abs](http://arxiv.org/abs/1905.09747v2) - [pdf](http://arxiv.org/pdf/1905.09747v2)

> Knowledge distillation is effective for producing small, high-performance neural networks for classification, but these small networks are vulnerable to adversarial attacks. This paper studies how adversarial robustness transfers from teacher to student during knowledge distillation. We find that a large amount of robustness may be inherited by the student even when distilled on only clean images. Second, we introduce Adversarially Robust Distillation (ARD) for distilling robustness onto student networks. In addition to producing small models with high test accuracy like conventional distillation, ARD also passes the superior robustness of large networks onto the student. In our experiments, we find that ARD student models decisively outperform adversarially trained networks of identical architecture in terms of robust accuracy, surpassing state-of-the-art methods on standard robustness benchmarks. Finally, we adapt recent fast adversarial training methods to ARD for accelerated robust distillation.

</details>

<details>

<summary>2019-12-03 04:17:13 - PoTrojan: powerful neural-level trojan designs in deep learning models</summary>

- *Minhui Zou, Yang Shi, Chengliang Wang, Fangyu Li, WenZhan Song, Yu Wang*

- `1802.03043v2` - [abs](http://arxiv.org/abs/1802.03043v2) - [pdf](http://arxiv.org/pdf/1802.03043v2)

> With the popularity of deep learning (DL), artificial intelligence (AI) has been applied in many areas of human life. Neural network or artificial neural network (NN), the main technique behind DL, has been extensively studied to facilitate computer vision and natural language recognition. However, the more we rely on information technology, the more vulnerable we are. That is, malicious NNs could bring huge threat in the so-called coming AI era. In this paper, for the first time in the literature, we propose a novel approach to design and insert powerful neural-level trojans or PoTrojan in pre-trained NN models. Most of the time, PoTrojans remain inactive, not affecting the normal functions of their host NN models. PoTrojans could only be triggered in very rare conditions. Once activated, however, the PoTrojans could cause the host NN models to malfunction, either falsely predicting or classifying, which is a significant threat to human society of the AI era. We would explain the principles of PoTrojans and the easiness of designing and inserting them in pre-trained deep learning models. PoTrojans doesn't modify the existing architecture or parameters of the pre-trained models, without re-training. Hence, the proposed method is very efficient.

</details>

<details>

<summary>2019-12-03 06:11:31 - On the (In)security of Approximate Computing Synthesis</summary>

- *Sheikh Ariful Islam*

- `1912.01209v1` - [abs](http://arxiv.org/abs/1912.01209v1) - [pdf](http://arxiv.org/pdf/1912.01209v1)

> The broad landscape of new applications requires minimal hardware resources without any sacrifice in Quality-of-Results. Approximate Computing (AC) has emerged to meet the demands of data-rich applications. Although AC applies techniques to improve the energy efficiency of error-tolerant applications at the cost of computational accuracy, new challenges in security threats of AC should be simultaneously addressed. In this paper, we introduce the security vulnerability of the concurrent AC synthesis. We analyze the threat landscape and provide a broader view of the attack and defense strategy. As a case study, we utilize AC synthesis technique to perform malicious modifications in the synthesized approximate netlist. Similarly, we provide a scalable defense framework for trustworthy AC synthesis.

</details>

<details>

<summary>2019-12-03 13:44:31 - The most frequent programming mistakes that cause software vulnerabilities</summary>

- *Raul Barbosa, Frederico Cerveira, Luis Goncalo, Henrique Madeira*

- `1912.01948v1` - [abs](http://arxiv.org/abs/1912.01948v1) - [pdf](http://arxiv.org/pdf/1912.01948v1)

> All computer programs have flaws, some of which can be exploited to gain unauthorized access to computer systems. We conducted a field study on publicly reported vulnerabilities affecting three open source software projects in widespread use. This paper highlights the main observations and conclusions from the field data collected in the study.

</details>

<details>

<summary>2019-12-03 23:16:11 - Rearchitecting Classification Frameworks For Increased Robustness</summary>

- *Varun Chandrasekaran, Brian Tang, Nicolas Papernot, Kassem Fawaz, Somesh Jha, Xi Wu*

- `1905.10900v3` - [abs](http://arxiv.org/abs/1905.10900v3) - [pdf](http://arxiv.org/pdf/1905.10900v3)

> While generalizing well over natural inputs, neural networks are vulnerable to adversarial inputs. Existing defenses against adversarial inputs have largely been detached from the real world. These defenses also come at a cost to accuracy. Fortunately, there are invariances of an object that are its salient features; when we break them it will necessarily change the perception of the object. We find that applying invariants to the classification task makes robustness and accuracy feasible together. Two questions follow: how to extract and model these invariances? and how to design a classification paradigm that leverages these invariances to improve the robustness accuracy trade-off? The remainder of the paper discusses solutions to the aformenetioned questions.

</details>

<details>

<summary>2019-12-04 10:55:42 - The Power and Pitfalls of Transparent Privacy Policies in Social Networking Service Platforms</summary>

- *Jana Korunovska, Bernadette Kamleitner, Sarah Spiekermann*

- `1911.09386v2` - [abs](http://arxiv.org/abs/1911.09386v2) - [pdf](http://arxiv.org/pdf/1911.09386v2)

> Users disclose ever-increasing amounts of personal data on Social Network Service platforms (SNS). Unless SNSs' policies are privacy friendly, this leaves them vulnerable to privacy risks because they ignore the privacy policies. Designers and regulators have pushed for shorter, simpler and more prominent privacy policies, however the evidence that transparent policies increase informed consent is lacking. To answer this question, we conducted an online experiment with 214 regular Facebook users asked to join a fictitious SNS. We experimentally manipulated the privacy-friendliness of SNS's policy and varied threats of secondary data use and data visibility. Half of our participants incorrectly recalled even the most formally "perfect" and easy-to-read privacy policies. Mostly, users recalled policies as more privacy friendly than they were. Moreover, participants self-censored their disclosures when aware that visibility threats were present, but were less sensitive to threats of secondary data use. We present design recommendations to increase informed consent.

</details>

<details>

<summary>2019-12-04 14:27:34 - Using Sequence-to-Sequence Learning for Repairing C Vulnerabilities</summary>

- *Zimin Chen, Steve Kommrusch, Martin Monperrus*

- `1912.02015v1` - [abs](http://arxiv.org/abs/1912.02015v1) - [pdf](http://arxiv.org/pdf/1912.02015v1)

> Software vulnerabilities affect all businesses and research is being done to avoid, detect or repair them. In this article, we contribute a new technique for automatic vulnerability fixing. We present a system that uses the rich software development history that can be found on GitHub to train an AI system that generates patches. We apply sequence-to-sequence learning on a big dataset of code changes and we evaluate the trained system on real world vulnerabilities from the CVE database. The result shows the feasibility of using sequence-to-sequence learning for fixing software vulnerabilities.

</details>

<details>

<summary>2019-12-04 17:38:57 - Optimizing Norm-Bounded Weighted Ambiguity Sets for Robust MDPs</summary>

- *Reazul Hasan Russel, Bahram Behzadian, Marek Petrik*

- `1912.02696v1` - [abs](http://arxiv.org/abs/1912.02696v1) - [pdf](http://arxiv.org/pdf/1912.02696v1)

> Optimal policies in Markov decision processes (MDPs) are very sensitive to model misspecification. This raises serious concerns about deploying them in high-stake domains. Robust MDPs (RMDP) provide a promising framework to mitigate vulnerabilities by computing policies with worst-case guarantees in reinforcement learning. The solution quality of an RMDP depends on the ambiguity set, which is a quantification of model uncertainties. In this paper, we propose a new approach for optimizing the shape of the ambiguity sets for RMDPs. Our method departs from the conventional idea of constructing a norm-bounded uniform and symmetric ambiguity set. We instead argue that the structure of a near-optimal ambiguity set is problem specific. Our proposed method computes a weight parameter from the value functions, and these weights then drive the shape of the ambiguity sets. Our theoretical analysis demonstrates the rationale of the proposed idea. We apply our method to several different problem domains, and the empirical results further furnish the practical promise of weighted near-optimal ambiguity sets.

</details>

<details>

<summary>2019-12-04 21:20:28 - SPECCFI: Mitigating Spectre Attacks using CFI Informed Speculation</summary>

- *Esmaeil Mohammadian Koruyeh, Shirin Haji Amin Shirazi, Khaled N. Khasawneh, Chengyu Song, Nael Abu-Ghazaleh*

- `1906.01345v2` - [abs](http://arxiv.org/abs/1906.01345v2) - [pdf](http://arxiv.org/pdf/1906.01345v2)

> Spectre attacks and their many subsequent variants are a new vulnerability class affecting modern CPUs. The attacks rely on the ability to misguide speculative execution, generally by exploiting the branch prediction structures, to execute a vulnerable code sequence speculatively. In this paper, we propose to use Control-Flow Integrity (CFI), a security technique used to stop control-flow hijacking attacks, on the committed path, to prevent speculative control-flow from being hijacked to launch the most dangerous variants of the Spectre attacks (Spectre-BTB and Spectre-RSB). Specifically, CFI attempts to constrain the possible targets of an indirect branch to a set of legal targets defined by a pre-calculated control-flow graph (CFG). As CFI is being adopted by commodity software (e.g., Windows and Android) and commodity hardware (e.g., Intel's CET and ARM's BTI), the CFI information becomes readily available through the hardware CFI extensions. With the CFI information, we apply CFI principles to also constrain illegal control-flow during speculative execution. Specifically, our proposed defense, SPECCFI, ensures that control flow instructions target legal destinations to constrain dangerous speculation on forward control-flow paths (indirect calls and branches). We augment this protection with a precise speculation-aware hardware stack to constrain speculation on backward control-flow edges (returns). We combine this solution with existing solutions against branch target predictor attacks (Spectre-PHT) to close all known non-vendor-specific Spectre vulnerabilities. We show that SPECCFI results in small overheads both in terms of performance and additional hardware complexity.

</details>

<details>

<summary>2019-12-04 21:42:15 - A Survey of Game Theoretic Approaches for Adversarial Machine Learning in Cybersecurity Tasks</summary>

- *Prithviraj Dasgupta, Joseph B. Collins*

- `1912.02258v1` - [abs](http://arxiv.org/abs/1912.02258v1) - [pdf](http://arxiv.org/pdf/1912.02258v1)

> Machine learning techniques are currently used extensively for automating various cybersecurity tasks. Most of these techniques utilize supervised learning algorithms that rely on training the algorithm to classify incoming data into different categories, using data encountered in the relevant domain. A critical vulnerability of these algorithms is that they are susceptible to adversarial attacks where a malicious entity called an adversary deliberately alters the training data to misguide the learning algorithm into making classification errors. Adversarial attacks could render the learning algorithm unsuitable to use and leave critical systems vulnerable to cybersecurity attacks. Our paper provides a detailed survey of the state-of-the-art techniques that are used to make a machine learning algorithm robust against adversarial attacks using the computational framework of game theory. We also discuss open problems and challenges and possible directions for further research that would make deep machine learning-based systems more robust and reliable for cybersecurity tasks.

</details>

<details>

<summary>2019-12-04 22:28:03 - Gobi: WebAssembly as a Practical Path to Library Sandboxing</summary>

- *Shravan Narayan, Tal Garfinkel, Sorin Lerner, Hovav Shacham, Deian Stefan*

- `1912.02285v1` - [abs](http://arxiv.org/abs/1912.02285v1) - [pdf](http://arxiv.org/pdf/1912.02285v1)

> Software based fault isolation (SFI) is a powerful approach to reduce the impact of security vulnerabilities in large C/C++ applications like Firefox and Apache. Unfortunately, practical SFI tools have not been broadly available.   Developing SFI toolchains are a significant engineering challenge. Only in recent years have browser vendors invested in building production quality SFI tools like Native Client (NaCl) to sandbox code. Further, without committed support, these tools are not viable, e.g. NaCl has been discontinued, orphaning projects that relied on it.   WebAssembly (Wasm) offers a promising solution---it can support high performance sandboxing and has been embraced by all major browser vendors---thus seems to have a viable future. However, Wasm presently only offers a solution for sandboxing mobile code. Providing SFI for native application, such as C/C++ libraries requires additional steps.   To reconcile the different worlds of Wasm on the browser and native platforms, we present Gobi. Gobi is a system of compiler changes and runtime support that can sandbox normal C/C++ libraries with Wasm---allowing them to be compiled and linked into native applications. Gobi has been tested on libjpeg, libpng, and zlib.   Based on our experience developing Gobi, we conclude with a call to arms to the Wasm community and SFI research community to make Wasm based module sandboxing a first class use case and describe how this can significantly benefit both communities.   Addendum: This short paper was originally written in January of 2019. Since then, the implementation and design of Gobi has evolved substantially as some of the issues raised in this paper have been addressed by the Wasm community. Nevertheless, several challenges still remain. We have thus left the paper largely intact and only provide a brief update on the state of Wasm tooling as of November 2019 in the last section.

</details>

<details>

<summary>2019-12-05 10:24:12 - Leveraging Operational Technology and the Internet of Things to Attack Smart Buildings</summary>

- *Daniel Ricardo dos Santos, Mario Dagrada, Elisa Costante*

- `1912.02480v1` - [abs](http://arxiv.org/abs/1912.02480v1) - [pdf](http://arxiv.org/pdf/1912.02480v1)

> In recent years, the buildings where we spend most part of our life are rapidly evolving. They are becoming fully automated environments where energy consumption, access control, heating and many other subsystems are all integrated within a single system commonly referred to as smart building (SB). To support the growing complexity of building operations, building automation systems (BAS) powering SBs are integrating consumer range Internet of Things (IoT) devices such as IP cameras alongside with operational technology (OT) controllers and actuators. However, these changes pose important cybersecurity concerns since the attack surface is larger, attack vectors are increasing and attacks can potentially harm building occupants. In this paper, we analyze the threat landscape of BASs by focusing on subsystems which are strongly affected by the advent of IoT devices such as video surveillance systems and smart lightning. We demonstrate how BAS operation can be disrupted by simple attacks to widely used network protocols. Furthermore, using both known and 0-day vulnerabilities reported in the paper and previously disclosed, we present the first (at our knowledge) BAS-specific malware which is able to persist within the BAS network by leveraging both OT and IoT devices connected to the BAS. Our research highlights how BAS networks can be considered as critical as industrial control systems and security concerns in BASs deserve more attention from both industrial and scientific communities. Even within a simulated environment, our proof-of-concept attacks were carried out with relative ease and a limited amount of budget and resources. Therefore, we believe that well-funded attack groups will increasingly shift their focus towards BASs with the potential of impacting the live of thousands of people.

</details>

<details>

<summary>2019-12-05 11:59:06 - Catch Me (On Time) If You Can: Understanding the Effectiveness of Twitter URL Blacklists</summary>

- *Simon Bell, Kenny Paterson, Lorenzo Cavallaro*

- `1912.02520v1` - [abs](http://arxiv.org/abs/1912.02520v1) - [pdf](http://arxiv.org/pdf/1912.02520v1)

> With more than 500 million daily tweets from over 330 million active users, Twitter constantly attracts malicious users aiming to carry out phishing and malware-related attacks against its user base. It therefore becomes of paramount importance to assess the effectiveness of Twitter's use of blacklists in protecting its users from such threats. We collected more than 182 million public tweets containing URLs from Twitter's Stream API over a 2-month period and compared these URLs against 3 popular phishing, social engineering, and malware blacklists, including Google Safe Browsing (GSB). We focus on the delay period between an attack URL first being tweeted to appearing on a blacklist, as this is the timeframe in which blacklists do not warn users, leaving them vulnerable. Experiments show that, whilst GSB is effective at blocking a number of social engineering and malicious URLs within 6 hours of being tweeted, a significant number of URLs go undetected for at least 20 days. For instance, during one month, we discovered 4,930 tweets containing URLs leading to social engineering websites that had been tweeted to over 131 million Twitter users. We also discovered 1,126 tweets containing 376 blacklisted Bitly URLs that had a combined total of 991,012 clicks, posing serious security and privacy threats. In addition, an equally large number of URLs contained within public tweets remain in GSB for at least 150 days, raising questions about potential false positives in the blacklist. We also provide evidence to suggest that Twitter may no longer be using GSB to protect its users.

</details>

<details>

<summary>2019-12-06 02:12:45 - Adversarial Music: Real World Audio Adversary Against Wake-word Detection System</summary>

- *Juncheng B. Li, Shuhui Qu, Xinjian Li, Joseph Szurley, J. Zico Kolter, Florian Metze*

- `1911.00126v3` - [abs](http://arxiv.org/abs/1911.00126v3) - [pdf](http://arxiv.org/pdf/1911.00126v3)

> Voice Assistants (VAs) such as Amazon Alexa or Google Assistant rely on wake-word detection to respond to people's commands, which could potentially be vulnerable to audio adversarial examples. In this work, we target our attack on the wake-word detection system, jamming the model with some inconspicuous background music to deactivate the VAs while our audio adversary is present. We implemented an emulated wake-word detection system of Amazon Alexa based on recent publications. We validated our models against the real Alexa in terms of wake-word detection accuracy. Then we computed our audio adversaries with consideration of expectation over transform and we implemented our audio adversary with a differentiable synthesizer. Next, we verified our audio adversaries digitally on hundreds of samples of utterances collected from the real world. Our experiments show that we can effectively reduce the recognition F1 score of our emulated model from 93.4% to 11.0%. Finally, we tested our audio adversary over the air, and verified it works effectively against Alexa, reducing its F1 score from 92.5% to 11.0%.; We also verified that non-adversarial music does not disable Alexa as effectively as our music at the same sound level. To the best of our knowledge, this is the first real-world adversarial attack against a commercial-grade VA wake-word detection system. Our code and demo videos can be accessed at \url{https://www.junchengbillyli.com/AdversarialMusic}

</details>

<details>

<summary>2019-12-06 08:24:11 - A Model-driven and Data-driven Fusion Framework for Accurate Air Quality Prediction</summary>

- *Haolin Fei, Xiaofeng Wu, Chunbo Luo*

- `1912.07367v1` - [abs](http://arxiv.org/abs/1912.07367v1) - [pdf](http://arxiv.org/pdf/1912.07367v1)

> Air quality is closely related to public health. Health issues such as cardiovascular diseases and respiratory diseases, may have connection with long exposure to highly polluted environment. Therefore, accurate air quality forecasts are extremely important to those who are vulnerable. To estimate the variation of several air pollution concentrations, previous researchers used various approaches, such as the Community Multiscale Air Quality model (CMAQ) or neural networks. Although CMAQ model considers a coverage of the historic air pollution data and meteorological variables, extra bias is introduced due to additional adjustment. In this paper, a combination of model-based strategy and data-driven method namely the physical-temporal collection(PTC) model is proposed, aiming to fix the systematic error that traditional models deliver. In the data-driven part, the first components are the temporal pattern and the weather pattern to measure important features that contribute to the prediction performance. The less relevant input variables will be removed to eliminate negative weights in network training. Then, we deploy a long-short-term-memory (LSTM) to fetch the preliminary results, which will be further corrected by a neural network (NN) involving the meteorological index as well as other pollutants concentrations. The data-set we applied for forecasting is from January 1st, 2016 to December 31st, 2016. According to the results, our PTC achieves an excellent performance compared with the baseline model (CMAQ prediction, GRU, DNN and etc.). This joint model-based data-driven method for air quality prediction can be easily deployed on stations without extra adjustment, providing results with high-time-resolution information for vulnerable members to prevent heavy air pollution ahead.

</details>

<details>

<summary>2019-12-06 23:16:45 - Label-Consistent Backdoor Attacks</summary>

- *Alexander Turner, Dimitris Tsipras, Aleksander Madry*

- `1912.02771v2` - [abs](http://arxiv.org/abs/1912.02771v2) - [pdf](http://arxiv.org/pdf/1912.02771v2)

> Deep neural networks have been demonstrated to be vulnerable to backdoor attacks. Specifically, by injecting a small number of maliciously constructed inputs into the training set, an adversary is able to plant a backdoor into the trained model. This backdoor can then be activated during inference by a backdoor trigger to fully control the model's behavior. While such attacks are very effective, they crucially rely on the adversary injecting arbitrary inputs that are---often blatantly---mislabeled. Such samples would raise suspicion upon human inspection, potentially revealing the attack. Thus, for backdoor attacks to remain undetected, it is crucial that they maintain label-consistency---the condition that injected inputs are consistent with their labels. In this work, we leverage adversarial perturbations and generative models to execute efficient, yet label-consistent, backdoor attacks. Our approach is based on injecting inputs that appear plausible, yet are hard to classify, hence causing the model to rely on the (easier-to-learn) backdoor trigger.

</details>

<details>

<summary>2019-12-07 01:15:40 - Principal Component Properties of Adversarial Samples</summary>

- *Malhar Jere, Sandro Herbig, Christine Lind, Farinaz Koushanfar*

- `1912.03406v1` - [abs](http://arxiv.org/abs/1912.03406v1) - [pdf](http://arxiv.org/pdf/1912.03406v1)

> Deep Neural Networks for image classification have been found to be vulnerable to adversarial samples, which consist of sub-perceptual noise added to a benign image that can easily fool trained neural networks, posing a significant risk to their commercial deployment. In this work, we analyze adversarial samples through the lens of their contributions to the principal components of each image, which is different than prior works in which authors performed PCA on the entire dataset. We investigate a number of state-of-the-art deep neural networks trained on ImageNet as well as several attacks for each of the networks. Our results demonstrate empirically that adversarial samples across several attacks have similar properties in their contributions to the principal components of neural network inputs. We propose a new metric for neural networks to measure their robustness to adversarial samples, termed the (k,p) point. We utilize this metric to achieve 93.36% accuracy in detecting adversarial samples independent of architecture and attack type for models trained on ImageNet.

</details>

<details>

<summary>2019-12-08 00:52:45 - Covert Channel-Based Transmitter Authentication in Controller Area Networks</summary>

- *Xuhang Ying, Giuseppe Bernieri, Mauro Conti, Linda Bushnell, Radha Poovendran*

- `1912.04735v1` - [abs](http://arxiv.org/abs/1912.04735v1) - [pdf](http://arxiv.org/pdf/1912.04735v1)

> In recent years, the security of automotive Cyber-Physical Systems (CPSs) is facing urgent threats due to the widespread use of legacy in-vehicle communication systems. As a representative legacy bus system, the Controller Area Network (CAN) hosts Electronic Control Units (ECUs) that are crucial vehicle functioning. In this scenario, malicious actors can exploit CAN vulnerabilities, such as the lack of built-in authentication and encryption schemes, to launch CAN bus attacks with life-threatening consequences (e.g., disabling brakes). In this paper, we present TACAN (Transmitter Authentication in CAN), which provides secure authentication of ECUs on the legacy CAN bus by exploiting the covert channels, without introducing CAN protocol modifications or traffic overheads. TACAN turns upside-down the originally malicious concept of covert channels and exploits it to build an effective defensive technique that facilitates transmitter authentication via a centralized, trusted Monitor Node. TACAN consists of three different covert channels for ECU authentication: 1) the Inter-Arrival Time (IAT)-based; 2) the Least Significant Bit (LSB)-based; and 3) a hybrid covert channel, exploiting the combination of the first two. In order to validate TACAN, we implement the covert channels on the University of Washington (UW) EcoCAR (Chevrolet Camaro 2016) testbed. We further evaluate the bit error, throughput, and detection performance of TACAN through extensive experiments using the EcoCAR testbed and a publicly available dataset collected from Toyota Camry 2010. We demonstrate the feasibility of TACAN and the effectiveness of detecting CAN bus attacks, highlighting no traffic overheads and attesting the regular functionality of ECUs.

</details>

<details>

<summary>2019-12-08 18:23:23 - Security of Deep Learning Methodologies: Challenges and Opportunities</summary>

- *Shahbaz Rezaei, Xin Liu*

- `1912.03735v1` - [abs](http://arxiv.org/abs/1912.03735v1) - [pdf](http://arxiv.org/pdf/1912.03735v1)

> Despite the plethora of studies about security vulnerabilities and defenses of deep learning models, security aspects of deep learning methodologies, such as transfer learning, have been rarely studied. In this article, we highlight the security challenges and research opportunities of these methodologies, focusing on vulnerabilities and attacks unique to them.

</details>

<details>

<summary>2019-12-09 00:02:19 - Hardening Random Forest Cyber Detectors Against Adversarial Attacks</summary>

- *Giovanni Apruzzese, Mauro Andreolini, Michele Colajanni, Mirco Marchetti*

- `1912.03790v1` - [abs](http://arxiv.org/abs/1912.03790v1) - [pdf](http://arxiv.org/pdf/1912.03790v1)

> Machine learning algorithms are effective in several applications, but they are not as much successful when applied to intrusion detection in cyber security. Due to the high sensitivity to their training data, cyber detectors based on machine learning are vulnerable to targeted adversarial attacks that involve the perturbation of initial samples. Existing defenses assume unrealistic scenarios; their results are underwhelming in non-adversarial settings; or they can be applied only to machine learning algorithms that perform poorly for cyber security. We present an original methodology for countering adversarial perturbations targeting intrusion detection systems based on random forests. As a practical application, we integrate the proposed defense method in a cyber detector analyzing network traffic. The experimental results on millions of labelled network flows show that the new detector has a twofold value: it outperforms state-of-the-art detectors that are subject to adversarial attacks; it exhibits robust results both in adversarial and non-adversarial scenarios.

</details>

<details>

<summary>2019-12-09 10:57:51 - Stealing Knowledge from Protected Deep Neural Networks Using Composite Unlabeled Data</summary>

- *Itay Mosafi, Eli David, Nathan S. Netanyahu*

- `1912.03959v1` - [abs](http://arxiv.org/abs/1912.03959v1) - [pdf](http://arxiv.org/pdf/1912.03959v1)

> As state-of-the-art deep neural networks are deployed at the core of more advanced Al-based products and services, the incentive for copying them (i.e., their intellectual properties) by rival adversaries is expected to increase considerably over time. The best way to extract or steal knowledge from such networks is by querying them using a large dataset of random samples and recording their output, followed by training a student network to mimic these outputs, without making any assumption about the original networks. The most effective way to protect against such a mimicking attack is to provide only the classification result, without confidence values associated with the softmax layer.In this paper, we present a novel method for generating composite images for attacking a mentor neural network using a student model. Our method assumes no information regarding the mentor's training dataset, architecture, or weights. Further assuming no information regarding the mentor's softmax output values, our method successfully mimics the given neural network and steals all of its knowledge. We also demonstrate that our student network (which copies the mentor) is impervious to watermarking protection methods, and thus would not be detected as a stolen model.Our results imply, essentially, that all current neural networks are vulnerable to mimicking attacks, even if they do not divulge anything but the most basic required output, and that the student model which mimics them cannot be easily detected and singled out as a stolen copy using currently available techniques.

</details>

<details>

<summary>2019-12-09 14:03:21 - Building Executable Secure Design Models for Smart Contracts with Formal Methods</summary>

- *Weifeng Xu, Glenn A. Fink*

- `1912.04051v1` - [abs](http://arxiv.org/abs/1912.04051v1) - [pdf](http://arxiv.org/pdf/1912.04051v1)

> Smart contracts are appealing because they are self-executing business agreements between parties with the predefined and immutable obligations and rights. However, as with all software, smart contracts may contain vulnerabilities because of design flaws, which may be exploited by one of the parties to defraud the others. In this paper, we demonstrate a systematic approach to building secure design models for smart contracts using formal methods. To build the secure models, we first model the behaviors of participating parties as state machines, and then, we model the predefined obligations and rights of contracts, which specify the interactions among state machines for achieving the business goal. After that, we illustrate executable secure model design patterns in TLA+ (Temporal Logic of Actions) to against well-known smart contract vulnerabilities in terms of state machines and obligations and rights at the design level. These vulnerabilities are found in Ethereum contracts, including Call to the unknown, Gasless send, Reentrancy, Lost in the transfer, and Unpredictable state. The resultant TLA+ specifications are called secure models. We illustrate our approach to detect the vulnerabilities using a real-estate contract example at the design level.

</details>

<details>

<summary>2019-12-09 21:13:00 - SPEECHMINER: A Framework for Investigating and Measuring Speculative Execution Vulnerabilities</summary>

- *Yuan Xiao, Yinqian Zhang, Radu Teodorescu*

- `1912.00329v2` - [abs](http://arxiv.org/abs/1912.00329v2) - [pdf](http://arxiv.org/pdf/1912.00329v2)

> SPEculative Execution side Channel Hardware (SPEECH) Vulnerabilities have enabled the notorious Meltdown, Spectre, and L1 terminal fault (L1TF) attacks. While a number of studies have reported different variants of SPEECH vulnerabilities, they are still not well understood. This is primarily due to the lack of information about microprocessor implementation details that impact the timing and order of various micro-architectural events. Moreover, to date, there is no systematic approach to quantitatively measure SPEECH vulnerabilities on commodity processors. This paper introduces SPEECHMINER, a software framework for exploring and measuring SPEECH vulnerabilities in an automated manner. SPEECHMINER empirically establishes the link between a novel two-phase fault handling model and the exploitability and speculation windows of SPEECH vulnerabilities. It enables testing of a comprehensive list of exception-triggering instructions under the same software framework, which leverages covert-channel techniques and differential tests to gain visibility into the micro-architectural state changes. We evaluated SPEECHMINER on 9 different processor types, examined 21 potential vulnerability variants, confirmed various known attacks, and identified several new variants.

</details>

<details>

<summary>2019-12-10 04:58:45 - Feature Losses for Adversarial Robustness</summary>

- *Kirthi Shankar Sivamani*

- `1912.04497v1` - [abs](http://arxiv.org/abs/1912.04497v1) - [pdf](http://arxiv.org/pdf/1912.04497v1)

> Deep learning has made tremendous advances in computer vision tasks such as image classification. However, recent studies have shown that deep learning models are vulnerable to specifically crafted adversarial inputs that are quasi-imperceptible to humans. In this work, we propose a novel approach to defending adversarial attacks. We employ an input processing technique based on denoising autoencoders as a defense. It has been shown that the input perturbations grow and accumulate as noise in feature maps while propagating through a convolutional neural network (CNN). We exploit the noisy feature maps by using an additional subnetwork to extract image feature maps and train an auto-encoder on perceptual losses of these feature maps. This technique achieves close to state-of-the-art results on defending MNIST and CIFAR10 datasets, but more importantly, shows a new way of employing a defense that cannot be trivially trained end-to-end by the attacker. Empirical results demonstrate the effectiveness of this approach on the MNIST and CIFAR10 datasets on simple as well as iterative LP attacks. Our method can be applied as a preprocessing technique to any off the shelf CNN.

</details>

<details>

<summary>2019-12-10 12:50:57 - Client-side Vulnerabilities in Commercial VPNs</summary>

- *Thanh Bui, Siddharth Prakash Rao, Markku Antikainen, Tuomas Aura*

- `1912.04669v1` - [abs](http://arxiv.org/abs/1912.04669v1) - [pdf](http://arxiv.org/pdf/1912.04669v1)

> Internet users increasingly rely on commercial virtual private network (VPN) services to protect their security and privacy. The VPN services route the client's traffic over an encrypted tunnel to a VPN gateway in the cloud. Thus, they hide the client's real IP address from online services, and they also shield the user's connections from perceived threats in the access networks. In this paper, we study the security of such commercial VPN services. The focus is on how the client applications set up VPN tunnels, and how the service providers instruct users to configure generic client software. We analyze common VPN protocols and implementations on Windows, macOS and Ubuntu. We find that the VPN clients have various configuration flaws, which an attacker can exploit to strip off traffic encryption or to bypass authentication of the VPN gateway. In some cases, the attacker can also steal the VPN user's username and password. We suggest ways to mitigate each of the discovered vulnerabilities.

</details>

<details>

<summary>2019-12-10 14:50:33 - A Write-Friendly and Fast-Recovery Scheme for Security Metadata in NVM</summary>

- *Jianming Huang, Yu Hua*

- `1912.04726v1` - [abs](http://arxiv.org/abs/1912.04726v1) - [pdf](http://arxiv.org/pdf/1912.04726v1)

> Non-Volatile Memories (NVMs) have attracted the attentions of academia and industry, which is expected to become the next-generation memory. However, due to the nonvolatile property, NVMs become vulnerable to attacks and require security mechanisms, e.g., counter mode encryption and integrity tree, which introduce the security metadata. NVMs promise to recover these security metadata after a system crash, including the counter and integrity tree. However, unlike merkle tree reconstructed from user data, recovering SGX integrity tree (SIT) has to address the challenges from unique top-down hierarchical dependency. Moreover, writing overhead and recovery time are important metrics for evaluating persistent memory system due to the high costs of NVM writes and IT downtime. How to recover the security metadata, i.e., counter blocks and integrity tree nodes, with low write overhead and short recovery time, becomes much important.   To provide a fast recovery scheme with low write overhead, we propose STAR, a cost-efficient scheme for recovering counter blocks and SGX integrity tree nodes after crashes. For fast recovery and verification, STAR synergizes the MAC and correct data, uses bitmap lines in ADR to indicate the location of stale node and constructs a cached merkle tree to verify the correctness of the recovery process. Moreover, STAR uses a multi-layer index to speed up the recovery process. STAR also allows different configurations to meet adaptive requirements for write overhead and recovery time. Our evaluation results show that the proposed STAR reduces the number of memory writes by up to 87\% compared with state-of-the-art work, Anubis, which needs extra 1x memory writes. For a 4MB security metadata cache, STAR needs 0.039s/0.023s/0.004s in three different configurations to recover the metadata cache while Anubis needs 0.020s.

</details>

<details>

<summary>2019-12-10 16:25:20 - That Was Then, This Is Now: A Security Evaluation of Password Generation, Storage, and Autofill in Thirteen Password Managers</summary>

- *Sean Oesch, Scott Ruoti*

- `1908.03296v2` - [abs](http://arxiv.org/abs/1908.03296v2) - [pdf](http://arxiv.org/pdf/1908.03296v2)

> Password managers have the potential to help users more effectively manage their passwords and address many of the concerns surrounding password-based authentication, however prior research has identified significant vulnerabilities in existing password managers. Since that time, five years has passed, leaving it unclear whether password managers remain vulnerable or whether they are now ready for broad adoption. To answer this question, we evaluate thirteen popular password managers and consider all three stages of the password manager lifecycle--password generation, storage, and autofill. Our evaluation is the first analysis of password generation in password managers, finding several non-random character distributions and identifying instances where generated passwords were vulnerable to online and offline guessing attacks. For password storage and autofill, we replicate past evaluations, demonstrating that while password managers have improved in the half-decade since those prior evaluations, there are still significant issues, particularly with browser-based password managers; these problems include unencrypted metadata, unsafe defaults, and vulnerabilities to clickjacking attacks. Based on our results, we identify password managers to avoid, provide recommendations on how to improve existing password managers, and identify areas of future research.

</details>

<details>

<summary>2019-12-10 18:24:58 - V0LTpwn: Attacking x86 Processor Integrity from Software</summary>

- *Zijo Kenjar, Tommaso Frassetto, David Gens, Michael Franz, Ahmad-Reza Sadeghi*

- `1912.04870v1` - [abs](http://arxiv.org/abs/1912.04870v1) - [pdf](http://arxiv.org/pdf/1912.04870v1)

> Fault-injection attacks have been proven in the past to be a reliable way of bypassing hardware-based security measures, such as cryptographic hashes, privilege and access permission enforcement, and trusted execution environments. However, traditional fault-injection attacks require physical presence, and hence, were often considered out of scope in many real-world adversary settings.   In this paper we show this assumption may no longer be justified. We present V0LTpwn, a novel hardware-oriented but software-controlled attack that affects the integrity of computation in virtually any execution mode on modern x86 processors. To the best of our knowledge, this represents the first attack on x86 integrity from software. The key idea behind our attack is to undervolt a physical core to force non-recoverable hardware faults. Under a V0LTpwn attack, CPU instructions will continue to execute with erroneous results and without crashes, allowing for exploitation. In contrast to recently presented side-channel attacks that leverage vulnerable speculative execution, V0LTpwn is not limited to information disclosure, but allows adversaries to affect execution, and hence, effectively breaks the integrity goals of modern x86 platforms. In our detailed evaluation we successfully launch software-based attacks against Intel SGX enclaves from a privileged process to demonstrate that a V0LTpwn attack can successfully change the results of computations within enclave execution across multiple CPU revisions.

</details>

<details>

<summary>2019-12-11 10:24:33 - Snoopy: Sniffing Your Smartwatch Passwords via Deep Sequence Learning</summary>

- *Chris Xiaoxuan Lu, Bowen Du, Hongkai Wen, Sen Wang, Andrew Markham, Ivan Martinovic, Yiran Shen, Niki Trigoni*

- `1912.04836v2` - [abs](http://arxiv.org/abs/1912.04836v2) - [pdf](http://arxiv.org/pdf/1912.04836v2)

> Demand for smartwatches has taken off in recent years with new models which can run independently from smartphones and provide more useful features, becoming first-class mobile platforms. One can access online banking or even make payments on a smartwatch without a paired phone. This makes smartwatches more attractive and vulnerable to malicious attacks, which to date have been largely overlooked. In this paper, we demonstrate Snoopy, a password extraction and inference system which is able to accurately infer passwords entered on Android/Apple watches within 20 attempts, just by eavesdropping on motion sensors. Snoopy uses a uniform framework to extract the segments of motion data when passwords are entered, and uses novel deep neural networks to infer the actual passwords. We evaluate the proposed Snoopy system in the real-world with data from 362 participants and show that our system offers a 3-fold improvement in the accuracy of inferring passwords compared to the state-of-the-art, without consuming excessive energy or computational resources. We also show that Snoopy is very resilient to user and device heterogeneity: it can be trained on crowd-sourced motion data (e.g. via Amazon Mechanical Turk), and then used to attack passwords from a new user, even if they are wearing a different model. This paper shows that, in the wrong hands, Snoopy can potentially cause serious leaks of sensitive information. By raising awareness, we invite the community and manufacturers to revisit the risks of continuous motion sensing on smart wearable devices.

</details>

<details>

<summary>2019-12-11 13:00:43 - Metamorphic Security Testing for Web Systems</summary>

- *Phu X. Mai, Fabrizio Pastore, Arda Goknil, Lionel Briand*

- `1912.05278v1` - [abs](http://arxiv.org/abs/1912.05278v1) - [pdf](http://arxiv.org/pdf/1912.05278v1)

> Security testing verifies that the data and the resources of software systems are protected from attackers. Unfortunately, it suffers from the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior. In many situations where potential vulnerabilities are tested, a test oracle may not exist, or it might be impractical due to the many inputs for which specific oracles have to be defined. In this paper, we propose a metamorphic testing approach that alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture security properties of the system. Such MRs are then used to automate testing and detect vulnerabilities. We provide a catalog of 22 system-agnostic MRs to automate security testing in Web systems. Our approach targets 39% of the OWASP security testing activities not automated by state-of-the-art techniques. It automatically detected 10 out of 12 vulnerabilities affecting two widely used systems, one commercial and the other open source (Jenkins).

</details>

<details>

<summary>2019-12-12 02:06:02 - Robust Data-driven Profile-based Pricing Schemes</summary>

- *Jingshi Cui, Haoxiang Wang, Chenye Wu, Yang Yu*

- `1912.05731v1` - [abs](http://arxiv.org/abs/1912.05731v1) - [pdf](http://arxiv.org/pdf/1912.05731v1)

> To enable an efficient electricity market, a good pricing scheme is of vital importance. Among many practical schemes, customized pricing is commonly believed to be able to best exploit the flexibility in the demand side. However, due to the large volume of consumers in the electricity sector, such task is simply too overwhelming. In this paper, we first compare two data driven schemes: one based on load profile and the other based on user's marginal system cost. Vulnerability analysis shows that the former approach may lead to loopholes in the electricity market while the latter one is able to guarantee the robustness, which yields our robust data-driven pricing scheme. Although k-means clustering is in general NP-hard, surprisingly, by exploiting the structure of our problem, we design an efficient yet optimal k-means clustering algorithm to implement our proposed scheme.

</details>

<details>

<summary>2019-12-12 13:35:09 - Inferring Input Grammars from Dynamic Control Flow</summary>

- *Rahul Gopinath, Björn Mathis, Andreas Zeller*

- `1912.05937v1` - [abs](http://arxiv.org/abs/1912.05937v1) - [pdf](http://arxiv.org/pdf/1912.05937v1)

> A program is characterized by its input model, and a formal input model can be of use in diverse areas including vulnerability analysis, reverse engineering, fuzzing and software testing, clone detection and refactoring. Unfortunately, input models for typical programs are often unavailable or out of date. While there exist algorithms that can mine the syntactical structure of program inputs, they either produce unwieldy and incomprehensible grammars, or require heuristics that target specific parsing patterns.   In this paper, we present a general algorithm that takes a program and a small set of sample inputs and automatically infers a readable context-free grammar capturing the input language of the program. We infer the syntactic input structure only by observing access of input characters at different locations of the input parser. This works on all program stack based recursive descent input parsers, including PEG and parser combinators, and can do entirely without program specific heuristics. Our Mimid prototype produced accurate and readable grammars for a variety of evaluation subjects, including expr, URLparse, and microJSON.

</details>

<details>

<summary>2019-12-12 15:54:08 - Revisiting and Evaluating Software Side-channel Vulnerabilities and Countermeasures in Cryptographic Applications</summary>

- *Tianwei Zhang, Jun Jiang, Yinqian Zhang*

- `1911.09312v2` - [abs](http://arxiv.org/abs/1911.09312v2) - [pdf](http://arxiv.org/pdf/1911.09312v2)

> We systematize software side-channel attacks with a focus on vulnerabilities and countermeasures in the cryptographic implementations. Particularly, we survey past research literature to categorize vulnerable implementations, and identify common strategies to eliminate them. We then evaluate popular libraries and applications, quantitatively measuring and comparing the vulnerability severity, response time and coverage. Based on these characterizations and evaluations, we offer some insights for side-channel researchers, cryptographic software developers and users. We hope our study can inspire the side-channel research community to discover new vulnerabilities, and more importantly, to fortify applications against them.

</details>

<details>

<summary>2019-12-13 00:07:14 - False Data Injection Attacks in Internet of Things and Deep Learning enabled Predictive Analytics</summary>

- *Gautam Raj Mode, Prasad Calyam, Khaza Anuarul Hoque*

- `1910.01716v4` - [abs](http://arxiv.org/abs/1910.01716v4) - [pdf](http://arxiv.org/pdf/1910.01716v4)

> Industry 4.0 is the latest industrial revolution primarily merging automation with advanced manufacturing to reduce direct human effort and resources. Predictive maintenance (PdM) is an industry 4.0 solution, which facilitates predicting faults in a component or a system powered by state-of-the-art machine learning (ML) algorithms and the Internet-of-Things (IoT) sensors. However, IoT sensors and deep learning (DL) algorithms, both are known for their vulnerabilities to cyber-attacks. In the context of PdM systems, such attacks can have catastrophic consequences as they are hard to detect due to the nature of the attack. To date, the majority of the published literature focuses on the accuracy of DL enabled PdM systems and often ignores the effect of such attacks. In this paper, we demonstrate the effect of IoT sensor attacks on a PdM system. At first, we use three state-of-the-art DL algorithms, specifically, Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Convolutional Neural Network (CNN) for predicting the Remaining Useful Life (RUL) of a turbofan engine using NASA's C-MAPSS dataset. The obtained results show that the GRU-based PdM model outperforms some of the recent literature on RUL prediction using the C-MAPSS dataset. Afterward, we model two different types of false data injection attacks (FDIA) on turbofan engine sensor data and evaluate their impact on CNN, LSTM, and GRU-based PdM systems. The obtained results demonstrate that FDI attacks on even a few IoT sensors can strongly defect the RUL prediction. However, the GRU-based PdM model performs better in terms of accuracy and resiliency. Lastly, we perform a study on the GRU-based PdM model using four different GRU networks with different sequence lengths. Our experiments reveal an interesting relationship between the accuracy, resiliency and sequence length for the GRU-based PdM models.

</details>

<details>

<summary>2019-12-13 11:09:35 - Potential adversarial samples for white-box attacks</summary>

- *Amir Nazemi, Paul Fieguth*

- `1912.06409v1` - [abs](http://arxiv.org/abs/1912.06409v1) - [pdf](http://arxiv.org/pdf/1912.06409v1)

> Deep convolutional neural networks can be highly vulnerable to small perturbations of their inputs, potentially a major issue or limitation on system robustness when using deep networks as classifiers. In this paper we propose a low-cost method to explore marginal sample data near trained classifier decision boundaries, thus identifying potential adversarial samples. By finding such adversarial samples it is possible to reduce the search space of adversarial attack algorithms while keeping a reasonable successful perturbation rate. In our developed strategy, the potential adversarial samples represent only 61% of the test data, but in fact cover more than 82% of the adversarial samples produced by iFGSM and 92% of the adversarial samples successfully perturbed by DeepFool on CIFAR10.

</details>

<details>

<summary>2019-12-13 13:16:00 - Early Detection Of Mirai-Like IoT Bots In Large-Scale Networks Through Sub-Sampled Packet Traffic Analysis</summary>

- *Ayush Kumar, Teng Joon Lim*

- `1901.04805v3` - [abs](http://arxiv.org/abs/1901.04805v3) - [pdf](http://arxiv.org/pdf/1901.04805v3)

> The widespread adoption of Internet of Things has led to many security issues. Recently, there have been malware attacks on IoT devices, the most prominent one being that of Mirai. IoT devices such as IP cameras, DVRs and routers were compromised by the Mirai malware and later large-scale DDoS attacks were propagated using those infected devices (bots) in October 2016. In this research, we develop a network-based algorithm which can be used to detect IoT bots infected by Mirai or similar malware in large-scale networks (e.g. ISP network). The algorithm particularly targets bots scanning the network for vulnerable devices since the typical scanning phase for botnets lasts for months and the bots can be detected much before they are involved in an actual attack. We analyze the unique signatures of the Mirai malware to identify its presence in an IoT device. The prospective deployment of our bot detection solution is discussed next along with the countermeasures which can be taken post detection. Further, to optimize the usage of computational resources, we use a two-dimensional (2D) packet sampling approach, wherein we sample the packets transmitted by IoT devices both across time and across the devices. Leveraging the Mirai signatures identified and the 2D packet sampling approach, a bot detection algorithm is proposed. Subsequently, we use testbed measurements and simulations to study the relationship between bot detection delays and the sampling frequencies for device packets. Finally, we derive insights from the obtained results and use them to design our proposed bot detection algorithm.

</details>

<details>

<summary>2019-12-13 13:54:48 - Implementing a Protocol Native Managed Cryptocurrency</summary>

- *Peter Mell, Aurelien Delaitre, Frederic de Vaulx, Philippe Dessauw*

- `1912.06491v1` - [abs](http://arxiv.org/abs/1912.06491v1) - [pdf](http://arxiv.org/pdf/1912.06491v1)

> Previous work presented a theoretical model based on the implicit Bitcoin specification for how an entity might issue a protocol native cryptocurrency that mimics features of fiat currencies. Protocol native means that it is built into the blockchain platform itself and is not simply a token running on another platform. Novel to this work were mechanisms by which the issuing entity could manage the cryptocurrency but where their power was limited and transparency was enforced by the cryptocurrency being implemented using a publicly mined blockchain. In this work we demonstrate the feasibility of this theoretical model by implementing such a managed cryptocurrency architecture through forking the Bitcoin code base. We discovered that the theoretical model contains several vulnerabilities and security issues that needed to be mitigated. It also contains architectural features that presented significant implementation challenges; some aspects of the proposed changes to the Bitcoin specification were not practical or even workable. In this work we describe how we mitigated the security vulnerabilities and overcame the architectural hurdles to build a working prototype.

</details>

<details>

<summary>2019-12-13 23:32:38 - An Unsupervised Domain-Independent Framework for Automated Detection of Persuasion Tactics in Text</summary>

- *Rahul Radhakrishnan Iyer, Katia Sycara*

- `1912.06745v1` - [abs](http://arxiv.org/abs/1912.06745v1) - [pdf](http://arxiv.org/pdf/1912.06745v1)

> With the increasing growth of social media, people have started relying heavily on the information shared therein to form opinions and make decisions. While such a reliance is motivation for a variety of parties to promote information, it also makes people vulnerable to exploitation by slander, misinformation, terroristic and predatorial advances. In this work, we aim to understand and detect such attempts at persuasion. Existing works on detecting persuasion in text make use of lexical features for detecting persuasive tactics, without taking advantage of the possible structures inherent in the tactics used. We formulate the task as a multi-class classification problem and propose an unsupervised, domain-independent machine learning framework for detecting the type of persuasion used in text, which exploits the inherent sentence structure present in the different persuasion tactics. Our work shows promising results as compared to existing work.

</details>

<details>

<summary>2019-12-14 07:01:44 - Artificial Intelligence Techniques for Security Vulnerability Prevention</summary>

- *Steve Kommrusch*

- `1912.06796v1` - [abs](http://arxiv.org/abs/1912.06796v1) - [pdf](http://arxiv.org/pdf/1912.06796v1)

> Computer security has been a concern for decades and artificial intelligence techniques have been applied to the area for nearly as long. Most of the techniques are being applied to the detection of attacks to running systems, but recent improvements in machine learning (for example, in natural language processing) have enabled the opportunity to process software and specifications to detect vulnerabilities in a system before it is deployed. This paper presents a survey of artificial intelligence techniques (including machine learning) to detect or repair security vulnerabilities before product introduction. In the surveyed papers, techniques are presented for using NLP to analyze requirements documents for security standard completeness, performing neural fuzz testing of software, generating exploits to detect risk, and more. We categorize current techniques into 3 groups: vulnerability detection, vulnerability repair, and specification analysis. Generally, while AI techniques have become quite useful in this area, we show that AI techniques still tend to be limited in scope, providing a collection of tools which can augment but not replace careful system development to reduce vulnerability risks.

</details>

<details>

<summary>2019-12-15 04:14:27 - Graph Adversarial Training: Dynamically Regularizing Based on Graph Structure</summary>

- *Fuli Feng, Xiangnan He, Jie Tang, Tat-Seng Chua*

- `1902.08226v2` - [abs](http://arxiv.org/abs/1902.08226v2) - [pdf](http://arxiv.org/pdf/1902.08226v2)

> Recent efforts show that neural networks are vulnerable to small but intentional perturbations on input features in visual classification tasks. Due to the additional consideration of connections between examples (\eg articles with citation link tend to be in the same class), graph neural networks could be more sensitive to the perturbations, since the perturbations from connected examples exacerbate the impact on a target example. Adversarial Training (AT), a dynamic regularization technique, can resist the worst-case perturbations on input features and is a promising choice to improve model robustness and generalization. However, existing AT methods focus on standard classification, being less effective when training models on graph since it does not model the impact from connected examples.   In this work, we explore adversarial training on graph, aiming to improve the robustness and generalization of models learned on graph. We propose Graph Adversarial Training (GraphAT), which takes the impact from connected examples into account when learning to construct and resist perturbations. We give a general formulation of GraphAT, which can be seen as a dynamic regularization scheme based on the graph structure. To demonstrate the utility of GraphAT, we employ it on a state-of-the-art graph neural network model --- Graph Convolutional Network (GCN). We conduct experiments on two citation graphs (Citeseer and Cora) and a knowledge graph (NELL), verifying the effectiveness of GraphAT which outperforms normal training on GCN by 4.51% in node classification accuracy. Codes are available via: https://github.com/fulifeng/GraphAT.

</details>

<details>

<summary>2019-12-16 02:11:24 - DAmageNet: A Universal Adversarial Dataset</summary>

- *Sizhe Chen, Xiaolin Huang, Zhengbao He, Chengjin Sun*

- `1912.07160v1` - [abs](http://arxiv.org/abs/1912.07160v1) - [pdf](http://arxiv.org/pdf/1912.07160v1)

> It is now well known that deep neural networks (DNNs) are vulnerable to adversarial attack. Adversarial samples are similar to the clean ones, but are able to cheat the attacked DNN to produce incorrect predictions in high confidence. But most of the existing adversarial attacks have high success rate only when the information of the attacked DNN is well-known or could be estimated by massive queries. A promising way is to generate adversarial samples with high transferability. By this way, we generate 96020 transferable adversarial samples from original ones in ImageNet. The average difference, measured by root means squared deviation, is only around 3.8 on average. However, the adversarial samples are misclassified by various models with an error rate up to 90\%. Since the images are generated independently with the attacked DNNs, this is essentially zero-query adversarial attack. We call the dataset \emph{DAmageNet}, which is the first universal adversarial dataset that beats many models trained in ImageNet. By finding the drawbacks, DAmageNet could serve as a benchmark to study and improve robustness of DNNs. DAmageNet could be downloaded in http://www.pami.sjtu.edu.cn/Show/56/122.

</details>

<details>

<summary>2019-12-16 09:48:04 - Pre- and post-quantum Diffie-Hellman from groups, actions, and isogenies</summary>

- *Benjamin Smith*

- `1809.04803v3` - [abs](http://arxiv.org/abs/1809.04803v3) - [pdf](http://arxiv.org/pdf/1809.04803v3)

> Diffie-Hellman key exchange is at the foundations of public-key cryptography, but conventional group-based Diffie-Hellman is vulnerable to Shor's quantum algorithm. A range of "post-quantum Diffie-Hellman" protocols have been proposed to mitigate this threat, including the Couveignes, Rostovtsev-Stolbunov, SIDH, and CSIDH schemes, all based on the combinatorial and number-theoretic structures formed by isogenies of elliptic curves. Pre-and post-quantum Diffie-Hellman schemes resemble each other at the highest level, but the further down we dive, the more differences emerge-differences that are critical when we use Diffie-Hellman as a basic component in more complicated constructions. In this survey we compare and contrast pre-and post-quantum Diffie-Hellman algorithms, highlighting some important subtleties.

</details>

<details>

<summary>2019-12-16 14:04:42 - Algorithmic Injustices: Towards a Relational Ethics</summary>

- *Abeba Birhane, Fred Cummins*

- `1912.07376v1` - [abs](http://arxiv.org/abs/1912.07376v1) - [pdf](http://arxiv.org/pdf/1912.07376v1)

> It has become trivial to point out how decision-making processes in various social, political and economical sphere are assisted by automated systems. Improved efficiency, the hallmark of these systems, drives the mass scale integration of automated systems into daily life. However, as a robust body of research in the area of algorithmic injustice shows, algorithmic tools embed and perpetuate societal and historical biases and injustice. In particular, a persistent recurring trend within the literature indicates that society's most vulnerable are disproportionally impacted. When algorithmic injustice and bias is brought to the fore, most of the solutions on offer 1) revolve around technical solutions and 2) do not focus centre disproportionally impacted groups. This paper zooms out and draws the bigger picture. It 1) argues that concerns surrounding algorithmic decision making and algorithmic injustice require fundamental rethinking above and beyond technical solutions, and 2) outlines a way forward in a manner that centres vulnerable groups through the lens of relational ethics.

</details>

<details>

<summary>2019-12-16 18:19:59 - Constructing a provably adversarially-robust classifier from a high accuracy one</summary>

- *Grzegorz Głuch, Rüdiger Urbanke*

- `1912.07561v1` - [abs](http://arxiv.org/abs/1912.07561v1) - [pdf](http://arxiv.org/pdf/1912.07561v1)

> Modern machine learning models with very high accuracy have been shown to be vulnerable to small, adversarially chosen perturbations of the input. Given black-box access to a high-accuracy classifier $f$, we show how to construct a new classifier $g$ that has high accuracy and is also robust to adversarial $\ell_2$-bounded perturbations. Our algorithm builds upon the framework of \textit{randomized smoothing} that has been recently shown to outperform all previous defenses against $\ell_2$-bounded adversaries. Using techniques like random partitions and doubling dimension, we are able to bound the adversarial error of $g$ in terms of the optimum error. In this paper we focus on our conceptual contribution, but we do present two examples to illustrate our framework. We will argue that, under some assumptions, our bounds are optimal for these cases.

</details>

<details>

<summary>2019-12-16 21:39:16 - Industrial robot ransomware: Akerbeltz</summary>

- *Víctor Mayoral-Vilches, Lander Usategui San Juan, Unai Ayucar Carbajo, Rubén Campo, Xabier Sáez de Cámara, Oxel Urzelai, Nuria García, Endika Gil-Uriarte*

- `1912.07714v1` - [abs](http://arxiv.org/abs/1912.07714v1) - [pdf](http://arxiv.org/pdf/1912.07714v1)

> Cybersecurity lessons have not been learnt from the dawn of other technological industries. In robotics, the existing insecurity landscape needs to be addressed immediately. Several manufacturers profiting from the lack of general awareness are systematically ignoring their responsibilities by claiming their insecure (open) systems facilitate system integration, disregarding the safety, privacy and ethical consequences that their (lack of) actions have. In an attempt to raise awareness and illustrate the "insecurity by design in robotics" we have created Akerbeltz, the first known instance of industrial robot ransomware. Our malware is demonstrated using a leading brand for industrial collaborative robots, Universal Robots. We describe the rationale behind our target and discuss the general flow of the attack including the initial cyber-intrusion, lateral movement and later control phase. We urge security researchers to adopt some sort of disclosure policy that forces manufacturers to react promptly. We advocate against security by obscurity and encourage the release of similar actions once vulnerability reports fall into a dead-end. Actions are now to be taken to abide a future free of zero-days for robotics.

</details>

<details>

<summary>2019-12-16 22:48:38 - CAG: A Real-time Low-cost Enhanced-robustness High-transferability Content-aware Adversarial Attack Generator</summary>

- *Huy Phan, Yi Xie, Siyu Liao, Jie Chen, Bo Yuan*

- `1912.07742v1` - [abs](http://arxiv.org/abs/1912.07742v1) - [pdf](http://arxiv.org/pdf/1912.07742v1)

> Deep neural networks (DNNs) are vulnerable to adversarial attack despite their tremendous success in many AI fields. Adversarial attack is a method that causes the intended misclassfication by adding imperceptible perturbations to legitimate inputs. Researchers have developed numerous types of adversarial attack methods. However, from the perspective of practical deployment, these methods suffer from several drawbacks such as long attack generating time, high memory cost, insufficient robustness and low transferability. We propose a Content-aware Adversarial Attack Generator (CAG) to achieve real-time, low-cost, enhanced-robustness and high-transferability adversarial attack. First, as a type of generative model-based attack, CAG shows significant speedup (at least 500 times) in generating adversarial examples compared to the state-of-the-art attacks such as PGD and C\&W. CAG only needs a single generative model to perform targeted attack to any targeted class. Because CAG encodes the label information into a trainable embedding layer, it differs from prior generative model-based adversarial attacks that use $n$ different copies of generative models for $n$ different targeted classes. As a result, CAG significantly reduces the required memory cost for generating adversarial examples. CAG can generate adversarial perturbations that focus on the critical areas of input by integrating the class activation maps information in the training process, and hence improve the robustness of CAG attack against the state-of-art adversarial defenses. In addition, CAG exhibits high transferability across different DNN classifier models in black-box attack scenario by introducing random dropout in the process of generating perturbations. Extensive experiments on different datasets and DNN models have verified the real-time, low-cost, enhanced-robustness, and high-transferability benefits of CAG.

</details>

<details>

<summary>2019-12-18 08:54:51 - Enjoy the Untrusted Cloud: A Secure, Scalable and Efficient SQL-like Query Framework for Outsourcing Data</summary>

- *Yaxing Chen, Qinghua Zheng, Dan Liu, Zheng Yan, Wenhai Sun, Ning Zhang, Wenjing Lou, Y. Thomas Hou*

- `1912.08454v1` - [abs](http://arxiv.org/abs/1912.08454v1) - [pdf](http://arxiv.org/pdf/1912.08454v1)

> While the security of the cloud remains a concern, a common practice is to encrypt data before outsourcing them for utilization. One key challenging issue is how to efficiently perform queries over the ciphertext. Conventional crypto-based solutions, e.g. partially/fully homomorphic encryption and searchable encryption, suffer from low performance, poor expressiveness and weak compatibility. An alternative method that utilizes hardware-assisted trusted execution environment, i.e., Intel SGX, has emerged recently. On one hand, such work lacks of supporting scalable access control over multiple data users. On the other hand, existing solutions are subjected to the key revocation problem and knowledge extractor vulnerability. In this work, we leverage the newly hardware-assisted methodology and propose a secure, scalable and efficient SQL-like query framework named QShield. Building upon Intel SGX, QShield can guarantee the confidentiality and integrity of sensitive data when being processed on an untrusted cloud platform. Moreover, we present a novel lightweight secret sharing method to enable multi-user access control in QShield, while tackling the key revocation problem. Furthermore, with an additional trust proof mechanism, QShield guarantees the correctness of queries and significantly alleviates the possibility to build a knowledge extractor. We implemented a prototype for QShield and show that QShield incurs minimum performance cost.

</details>

<details>

<summary>2019-12-18 12:55:15 - Harzer Roller: Linker-Based Instrumentation for Enhanced Embedded Security Testing</summary>

- *Katharina Bogad, Manuel Huber*

- `1912.08573v1` - [abs](http://arxiv.org/abs/1912.08573v1) - [pdf](http://arxiv.org/pdf/1912.08573v1)

> Due to the rise of the Internet of Things, there are many new chips and platforms available for hobbyists and industry alike to build smart devices. The SDKs for these new platforms usually include closed-source binaries containing wireless protocol implementations, cryptographic implementations, or other library functions, which are shared among all user code across the platform. Leveraging such a library vulnerability has a high impact on a given platform. However, as these platforms are often shipped ready-to-use, classic debug infrastructure like JTAG is often times not available.   In this paper, we present a method, called Harzer Roller, to enhance embedded firmware security testing on resource-constrained devices. With the Harzer Roller, we hook instrumentation code into function call and return. The hooking not only applies to the user application code but to the SDK used to build firmware as well. While we keep the design of the Harzer Rollergenerally architecture independent, we provide an implementation for the ESP8266 Wi-Fi IoT chip based on the xtensa architecture.   We show that the Harzer Roller can be leveraged to trace execution flow through libraries without available source code and to detect stack-based buffer-overflows. Additionally, we showcase how the overflow detection can be used to dump debugging information for later analysis. This enables better usage of a variety of software security testing methods like fuzzing of wireless protocol implementations or proof-of-concept attack development.

</details>

<details>

<summary>2019-12-18 20:28:15 - PAGURUS: Low-Overhead Dynamic Information Flow Tracking on Loosely Coupled Accelerators</summary>

- *Luca Piccolboni, Giuseppe Di Guglielmo, Luca P. Carloni*

- `1912.11153v1` - [abs](http://arxiv.org/abs/1912.11153v1) - [pdf](http://arxiv.org/pdf/1912.11153v1)

> Software-based attacks exploit bugs or vulnerabilities to get unauthorized access or leak confidential information. Dynamic information flow tracking (DIFT) is a security technique to track spurious information flows and provide strong security guarantees against such attacks. To secure heterogeneous systems, the spurious information flows must be tracked through all their components, including processors, accelerators (i.e., application-specific hardware components) and memories. We present PAGURUS, a flexible methodology to design a low-overhead shell circuit that adds DIFT support to accelerators. The shell uses a coarse-grain DIFT approach, thus not requiring to make modifications to the accelerator's implementation. We analyze the performance and area overhead of the DIFT shell on FPGAs and we propose a metric, called information leakage, to measure its security guarantees. We perform a design-space exploration to show that we can synthesize accelerators with different characteristics in terms of performance, cost and security guarantees. We also present a case study where we use the DIFT shell to secure an accelerator running on a embedded platform with a DIFT-enhanced RISC-V core.

</details>

<details>

<summary>2019-12-18 21:38:34 - MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples</summary>

- *Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, Neil Zhenqiang Gong*

- `1909.10594v3` - [abs](http://arxiv.org/abs/1909.10594v3) - [pdf](http://arxiv.org/pdf/1909.10594v3)

> In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs.   In this work, we propose MemGuard, the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples. Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.

</details>

<details>

<summary>2019-12-18 23:54:10 - Defend Deep Neural Networks Against Adversarial Examples via Fixed and Dynamic Quantized Activation Functions</summary>

- *Adnan Siraj Rakin, Jinfeng Yi, Boqing Gong, Deliang Fan*

- `1807.06714v2` - [abs](http://arxiv.org/abs/1807.06714v2) - [pdf](http://arxiv.org/pdf/1807.06714v2)

> Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks. To this end, many defense approaches that attempt to improve the robustness of DNNs have been proposed. In a separate and yet related area, recent works have explored to quantize neural network weights and activation functions into low bit-width to compress model size and reduce computational complexity. In this work, we find that these two different tracks, namely the pursuit of network compactness and robustness, can be merged into one and give rise to networks of both advantages. To the best of our knowledge, this is the first work that uses quantization of activation functions to defend against adversarial examples. We also propose to train robust neural networks by using adaptive quantization techniques for the activation functions. Our proposed Dynamic Quantized Activation (DQA) is verified through a wide range of experiments with the MNIST and CIFAR-10 datasets under different white-box attack methods, including FGSM, PGD, and C & W attacks. Furthermore, Zeroth Order Optimization and substitute model-based black-box attacks are also considered in this work. The experimental results clearly show that the robustness of DNNs could be greatly improved using the proposed DQA.

</details>

<details>

<summary>2019-12-19 10:06:09 - SAFE: Self-Attentive Function Embeddings for Binary Similarity</summary>

- *Luca Massarelli, Giuseppe Antonio Di Luna, Fabio Petroni, Leonardo Querzoni, Roberto Baldoni*

- `1811.05296v4` - [abs](http://arxiv.org/abs/1811.05296v4) - [pdf](http://arxiv.org/pdf/1811.05296v4)

> The binary similarity problem consists in determining if two functions are similar by only considering their compiled form. Advanced techniques for binary similarity recently gained momentum as they can be applied in several fields, such as copyright disputes, malware analysis, vulnerability detection, etc., and thus have an immediate practical impact. Current solutions compare functions by first transforming their binary code in multi-dimensional vector representations (embeddings), and then comparing vectors through simple and efficient geometric operations. However, embeddings are usually derived from binary code using manual feature extraction, that may fail in considering important function characteristics, or may consider features that are not important for the binary similarity problem. In this paper we propose SAFE, a novel architecture for the embedding of functions based on a self-attentive neural network. SAFE works directly on disassembled binary functions, does not require manual feature extraction, is computationally more efficient than existing solutions (i.e., it does not incur in the computational overhead of building or manipulating control flow graphs), and is more general as it works on stripped binaries and on multiple architectures. We report the results from a quantitative and qualitative analysis that show how SAFE provides a noticeable performance improvement with respect to previous solutions. Furthermore, we show how clusters of our embedding vectors are closely related to the semantic of the implemented algorithms, paving the way for further interesting applications (e.g. semantic-based binary function search).

</details>

<details>

<summary>2019-12-19 10:56:36 - A New Ensemble Method for Concessively Targeted Multi-model Attack</summary>

- *Ziwen He, Wei Wang, Xinsheng Xuan, Jing Dong, Tieniu Tan*

- `1912.10833v1` - [abs](http://arxiv.org/abs/1912.10833v1) - [pdf](http://arxiv.org/pdf/1912.10833v1)

> It is well known that deep learning models are vulnerable to adversarial examples crafted by maliciously adding perturbations to original inputs. There are two types of attacks: targeted attack and non-targeted attack, and most researchers often pay more attention to the targeted adversarial examples. However, targeted attack has a low success rate, especially when aiming at a robust model or under a black-box attack protocol. In this case, non-targeted attack is the last chance to disable AI systems. Thus, in this paper, we propose a new attack mechanism which performs the non-targeted attack when the targeted attack fails. Besides, we aim to generate a single adversarial sample for different deployed models of the same task, e.g. image classification models. Hence, for this practical application, we focus on attacking ensemble models by dividing them into two groups: easy-to-attack and robust models. We alternately attack these two groups of models in the non-targeted or targeted manner. We name it a bagging and stacking ensemble (BAST) attack. The BAST attack can generate an adversarial sample that fails multiple models simultaneously. Some of the models classify the adversarial sample as a target label, and other models which are not attacked successfully may give wrong labels at least. The experimental results show that the proposed BAST attack outperforms the state-of-the-art attack methods on the new defined criterion that considers both targeted and non-targeted attack performance.

</details>

<details>

<summary>2019-12-19 11:51:15 - Certifiable Robustness to Graph Perturbations</summary>

- *Aleksandar Bojchevski, Stephan Günnemann*

- `1910.14356v2` - [abs](http://arxiv.org/abs/1910.14356v2) - [pdf](http://arxiv.org/pdf/1910.14356v2)

> Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.

</details>

<details>

<summary>2019-12-19 17:04:20 - Detecting brute-force attacks on cryptocurrency wallets</summary>

- *E. O. Kiktenko, M. A. Kudinov, A. K. Fedorov*

- `1904.06943v2` - [abs](http://arxiv.org/abs/1904.06943v2) - [pdf](http://arxiv.org/pdf/1904.06943v2)

> Blockchain is a distributed ledger, which is protected against malicious modifications by means of cryptographic tools, e.g. digital signatures and hash functions. One of the most prominent applications of blockchains is cryptocurrencies, such as Bitcoin. In this work, we consider a particular attack on wallets for collecting assets in a cryptocurrency network based on brute-force search attacks. Using Bitcoin as an example, we demonstrate that if the attack is implemented successfully, a legitimate user is able to prove that fact of this attack with a high probability. We also consider two options for modification of existing cryptocurrency protocols for dealing with this type of attacks. First, we discuss a modification that requires introducing changes in the Bitcoin protocol and allows diminishing the motivation to attack wallets. Second, an alternative option is the construction of special smart-contracts, which reward the users for providing evidence of the brute-force attack. The execution of this smart-contract can work as an automatic alarm that the employed cryptographic mechanisms, and (particularly) hash functions, have an evident vulnerability.

</details>

<details>

<summary>2019-12-19 21:43:15 - Blockchain-based Application Security Risks: A Systematic Literature Review</summary>

- *Mubashar Iqbal, Raimundas Matulevicius*

- `1912.09556v1` - [abs](http://arxiv.org/abs/1912.09556v1) - [pdf](http://arxiv.org/pdf/1912.09556v1)

> Although the blockchain-based applications are considered to be less vulnerable due to the nature of the distributed ledger, they did not become the silver bullet with respect to securing the information against different security risks. In this paper, we present a literature review on the security risks that can be mitigated by introducing the blockchain technology, and on the security risks that are identified in the blockchain-based applications. In addition, we highlight the application and technology domains where these security risks are observed. The results of this study could be seen as a preliminary checklist of security risks when implementing blockchain-based applications.

</details>

<details>

<summary>2019-12-20 04:56:47 - Game-theory-based analysis on interactions among secondary and malicious users in coordinated jamming attack in cognitive radio systems</summary>

- *Ehsan Meamari, Khadijeh Afhamisisi, Hadi Shahriar Shahhoseini*

- `1912.12173v1` - [abs](http://arxiv.org/abs/1912.12173v1) - [pdf](http://arxiv.org/pdf/1912.12173v1)

> IEEE 802.22 standard utilizes cognitive radio (CR) techniques to allow sharing unused spectrum band. The cognitive radio is vulnerable to various attacks such as jamming attacks. This paper has focused on coordinated jamming attacks. A simple strategy for secondary users is to change their bands and switch to other appropriate bands when the jamming attack is occurred. Also, the malicious users should switch to other bands in order to jam the secondary users. To address this problem, a game theoretical method is proposed to analyze coordinated jamming attacks in CR. Then, using Nash equilibrium on the proposed game, the most appropriate bands have been found to switch as well as the optimal switching probabilities for both secondary and malicious users. Meanwhile, effects of different parameters like the number of malicious users are investigated in changing the optimal switching probabilities by analysis on the model.

</details>

<details>

<summary>2019-12-20 11:51:22 - Pentest on an Internet Mobile App: A Case Study using Tramonto</summary>

- *Daniel Dalalana Bertoglio, Guilherme Girotto, Charles Varlei Neu, Roben Castagna Lunardi, and Avelino Francisco Zorzo*

- `1912.09779v1` - [abs](http://arxiv.org/abs/1912.09779v1) - [pdf](http://arxiv.org/pdf/1912.09779v1)

> Mobile applications are used to handle different types of data. Commonly, there is a set of personal identifiable information present in the data stored, shared and used by these applications. From that, attackers can try to exploit the mobile application in order to obtain or to cause private data leakage. Therefore, performing security assessments is an important practice to find vulnerabilities in the applications and systems before the application is deployed, or even during their use. Regarding security assessments, Penetration Test (Pentest) is one of the security test types that can be used to detect vulnerabilities through simulated attacks. Additionally, Pentest can be performed using different methodologies and best practices, through several frameworks to: organize the test execution, execute tools, provide estimations, provide reports and document a Pentest. One such framework is Tramonto, which aims to assist a cybersecurity expert during the Pentest execution by providing organization, standardization and flexibility to the whole Pentest process. This paper presents a Pentest case study applied to a Brazilian university Mobile App using the Tramonto framework. The main goal of this case study is to present how Tramonto can be applied during a Pentest execution, assisting cybersecurity experts in the tasks included in the Pentest process. Our results show details on how to perform a Pentest using Tramonto and the found vulnerabilities in the Mobile App. Besides that, there is a discussion about the main contributions obtained from our results, and we were able to verify that Tramonto managed, organized and optimized the whole Pentest process.

</details>

<details>

<summary>2019-12-20 15:54:51 - Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing</summary>

- *Jinyuan Jia, Xiaoyu Cao, Binghui Wang, Neil Zhenqiang Gong*

- `1912.09899v1` - [abs](http://arxiv.org/abs/1912.09899v1) - [pdf](http://arxiv.org/pdf/1912.09899v1)

> It is well-known that classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top-$k$ predictions are more relevant. In this work, we aim to derive certified robustness for top-$k$ predictions. In particular, our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any classifier. We derive a tight robustness in $\ell_2$ norm for top-$k$ predictions when using randomized smoothing with Gaussian noise. We find that generalizing the certified robustness from top-1 to top-$k$ predictions faces significant technical challenges. We also empirically evaluate our method on CIFAR10 and ImageNet. For example, our method can obtain an ImageNet classifier with a certified top-5 accuracy of 62.8\% when the $\ell_2$-norms of the adversarial perturbations are less than 0.5 (=127/255). Our code is publicly available at: \url{https://github.com/jjy1994/Certify_Topk}.

</details>

<details>

<summary>2019-12-20 18:21:49 - Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation</summary>

- *Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krishnamurthy Dvijotham, Pushmeet Kohli*

- `1909.01492v2` - [abs](http://arxiv.org/abs/1909.01492v2) - [pdf](http://arxiv.org/pdf/1909.01492v2)

> Neural networks are part of many contemporary NLP systems, yet their empirical successes come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite direction: to formally verify a system's robustness against a predefined class of adversarial attacks. We study text classification under synonym replacements or character flip perturbations. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation -- a formal model verification method. We modify the conventional log-likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search complexity. The resulting models show only little difference in terms of nominal accuracy, but have much improved verified accuracy under perturbations and come with an efficiently computable formal guarantee on worst case adversaries.

</details>

<details>

<summary>2019-12-21 00:44:52 - Measuring Dataset Granularity</summary>

- *Yin Cui, Zeqi Gu, Dhruv Mahajan, Laurens van der Maaten, Serge Belongie, Ser-Nam Lim*

- `1912.10154v1` - [abs](http://arxiv.org/abs/1912.10154v1) - [pdf](http://arxiv.org/pdf/1912.10154v1)

> Despite the increasing visibility of fine-grained recognition in our field, "fine-grained'' has thus far lacked a precise definition. In this work, building upon clustering theory, we pursue a framework for measuring dataset granularity. We argue that dataset granularity should depend not only on the data samples and their labels, but also on the distance function we choose. We propose an axiomatic framework to capture desired properties for a dataset granularity measure and provide examples of measures that satisfy these properties. We assess each measure via experiments on datasets with hierarchical labels of varying granularity. When measuring granularity in commonly used datasets with our measure, we find that certain datasets that are widely considered fine-grained in fact contain subsets of considerable size that are substantially more coarse-grained than datasets generally regarded as coarse-grained. We also investigate the interplay between dataset granularity with a variety of factors and find that fine-grained datasets are more difficult to learn from, more difficult to transfer to, more difficult to perform few-shot learning with, and more vulnerable to adversarial attacks.

</details>

<details>

<summary>2019-12-21 18:31:55 - Socio-network Analysis of RTL Designs for Hardware Trojan Localization</summary>

- *Sheikh Ariful Islam, Farha Islam Mime, S M Asaduzzaman, Farzana Islam*

- `1912.10312v1` - [abs](http://arxiv.org/abs/1912.10312v1) - [pdf](http://arxiv.org/pdf/1912.10312v1)

> The recent surge in hardware security is significant due to offshoring the proprietary Intellectual property (IP). One distinct dimension of the disruptive threat is malicious logic insertion, also known as Hardware Trojan (HT). HT subverts the normal operations of a device stealthily. The diversity in HTs activation mechanisms and their location in design brings no catch-all detection techniques. In this paper, we propose to leverage principle features of social network analysis to security analysis of Register Transfer Level (RTL) designs against HT. The approach is based on investigating design properties, and it extends the current detection techniques. In particular, we perform both node- and graph-level analysis to determine the direct and indirect interactions between nets in a design. This technique helps not only in finding vulnerable nets that can act as HT triggering signals but also their interactions to influence a particular net to act as HT payload signal. We experiment the technique on 420 combinational HT instances, and on average, we can detect both triggering and payload signals with accuracy up to 97.37%.

</details>

<details>

<summary>2019-12-23 07:04:24 - Monte-Carlo Tree Search for Policy Optimization</summary>

- *Xiaobai Ma, Katherine Driggs-Campbell, Zongzhang Zhang, Mykel J. Kochenderfer*

- `1912.10648v1` - [abs](http://arxiv.org/abs/1912.10648v1) - [pdf](http://arxiv.org/pdf/1912.10648v1)

> Gradient-based methods are often used for policy optimization in deep reinforcement learning, despite being vulnerable to local optima and saddle points. Although gradient-free methods (e.g., genetic algorithms or evolution strategies) help mitigate these issues, poor initialization and local optima are still concerns in highly nonconvex spaces. This paper presents a method for policy optimization based on Monte-Carlo tree search and gradient-free optimization. Our method, called Monte-Carlo tree search for policy optimization (MCTSPO), provides a better exploration-exploitation trade-off through the use of the upper confidence bound heuristic. We demonstrate improved performance on reinforcement learning tasks with deceptive or sparse reward functions compared to popular gradient-based and deep genetic algorithm baselines.

</details>

<details>

<summary>2019-12-24 10:20:38 - Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer</summary>

- *Hongyan Chang, Virat Shejwalkar, Reza Shokri, Amir Houmansadr*

- `1912.11279v1` - [abs](http://arxiv.org/abs/1912.11279v1) - [pdf](http://arxiv.org/pdf/1912.11279v1)

> Collaborative (federated) learning enables multiple parties to train a model without sharing their private data, but through repeated sharing of the parameters of their local models. Despite its advantages, this approach has many known privacy and security weaknesses and performance overhead, in addition to being limited only to models with homogeneous architectures. Shared parameters leak a significant amount of information about the local (and supposedly private) datasets. Besides, federated learning is severely vulnerable to poisoning attacks, where some participants can adversarially influence the aggregate parameters. Large models, with high dimensional parameter vectors, are in particular highly susceptible to privacy and security attacks: curse of dimensionality in federated learning. We argue that sharing parameters is the most naive way of information exchange in collaborative learning, as they open all the internal state of the model to inference attacks, and maximize the model's malleability by stealthy poisoning attacks. We propose Cronus, a robust collaborative machine learning framework. The simple yet effective idea behind designing Cronus is to control, unify, and significantly reduce the dimensions of the exchanged information between parties, through robust knowledge transfer between their black-box local models. We evaluate all existing federated learning algorithms against poisoning attacks, and we show that Cronus is the only secure method, due to its tight robustness guarantee. Treating local models as black-box, reduces the information leakage through models, and enables us using existing privacy-preserving algorithms that mitigate the risk of information leakage through the model's output (predictions). Cronus also has a significantly lower sample complexity, compared to federated learning, which does not bind its security to the number of participants.

</details>

<details>

<summary>2019-12-26 10:47:35 - Anomalous Communications Detection in IoT Networks Using Sparse Autoencoders</summary>

- *Mustafizur Rahman Shahid, Gregory Blanc, Zonghua Zhang, Hervé Debar*

- `1912.11831v1` - [abs](http://arxiv.org/abs/1912.11831v1) - [pdf](http://arxiv.org/pdf/1912.11831v1)

> Nowadays, IoT devices have been widely deployed for enabling various smart services, such as, smart home or e-healthcare. However, security remains as one of the paramount concern as many IoT devices are vulnerable. Moreover, IoT malware are constantly evolving and getting more sophisticated. IoT devices are intended to perform very specific tasks, so their networking behavior is expected to be reasonably stable and predictable. Any significant behavioral deviation from the normal patterns would indicate anomalous events. In this paper, we present a method to detect anomalous network communications in IoT networks using a set of sparse autoencoders. The proposed approach allows us to differentiate malicious communications from legitimate ones. So that, if a device is compromised only malicious communications can be dropped while the service provided by the device is not totally interrupted. To characterize network behavior, bidirectional TCP flows are extracted and described using statistics on the size of the first N packets sent and received, along with statistics on the corresponding inter-arrival times between packets. A set of sparse autoencoders is then trained to learn the profile of the legitimate communications generated by an experimental smart home network. Depending on the value of N, the developed model achieves attack detection rates ranging from 86.9% to 91.2%, and false positive rates ranging from 0.1% to 0.5%.

</details>

<details>

<summary>2019-12-26 12:37:01 - Benchmarking Adversarial Robustness</summary>

- *Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, Jun Zhu*

- `1912.11852v1` - [abs](http://arxiv.org/abs/1912.11852v1) - [pdf](http://arxiv.org/pdf/1912.11852v1)

> Deep neural networks are vulnerable to adversarial examples, which becomes one of the most important research problems in the development of deep learning. While a lot of efforts have been made in recent years, it is of great significance to perform correct and complete evaluations of the adversarial attack and defense algorithms. In this paper, we establish a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness on image classification tasks. After briefly reviewing plenty of representative attack and defense methods, we perform large-scale experiments with two robustness curves as the fair-minded evaluation criteria to fully understand the performance of these methods. Based on the evaluation results, we draw several important findings and provide insights for future research.

</details>

<details>

<summary>2019-12-28 00:07:13 - How Secure Is Your IoT Network?</summary>

- *Josh Payne, Karan K. Budhraja, Ashish Kundu*

- `1912.12373v1` - [abs](http://arxiv.org/abs/1912.12373v1) - [pdf](http://arxiv.org/pdf/1912.12373v1)

> The proliferation of IoT devices in smart homes, hospitals, and enterprise networks is widespread and continuing to increase in a superlinear manner. With this unprecedented growth, how can one assess the security of an IoT network holistically? In this article, we explore two dimensions of security assessment, using vulnerability information of IoT devices and their underlying components ($\textit{compositional security scores}$) and SIEM logs captured from the communications and operations of such devices in a network ($\textit{dynamic activity metrics}$) to propose the notion of an $\textit{attack circuit}$. These measures are used to evaluate the security of IoT devices and the overall IoT network, demonstrating the effectiveness of attack circuits as practical tools for computing security metrics (exploitability, impact, and risk to confidentiality, integrity, and availability) of heterogeneous networks. We propose methods for generating attack circuits with input/output pairs constructed from CVEs using natural language processing (NLP) and with weights computed using standard security scoring procedures, as well as efficient optimization methods for evaluating attack circuits. Our system provides insight into possible attack paths an adversary may utilize based on their exploitability, impact, or overall risk. We have performed experiments on IoT networks to demonstrate the efficacy of the proposed techniques.

</details>

<details>

<summary>2019-12-28 10:00:22 - Locally Weighted Ensemble Clustering</summary>

- *Dong Huang, Chang-Dong Wang, Jian-Huang Lai*

- `1605.05011v3` - [abs](http://arxiv.org/abs/1605.05011v3) - [pdf](http://arxiv.org/pdf/1605.05011v3)

> Due to its ability to combine multiple base clusterings into a probably better and more robust clustering, the ensemble clustering technique has been attracting increasing attention in recent years. Despite the significant success, one limitation to most of the existing ensemble clustering methods is that they generally treat all base clusterings equally regardless of their reliability, which makes them vulnerable to low-quality base clusterings. Although some efforts have been made to (globally) evaluate and weight the base clusterings, yet these methods tend to view each base clustering as an individual and neglect the local diversity of clusters inside the same base clustering. It remains an open problem how to evaluate the reliability of clusters and exploit the local diversity in the ensemble to enhance the consensus performance, especially in the case when there is no access to data features or specific assumptions on data distribution. To address this, in this paper, we propose a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. In particular, the uncertainty of each cluster is estimated by considering the cluster labels in the entire ensemble via an entropic criterion. A novel ensemble-driven cluster validity measure is introduced, and a locally weighted co-association matrix is presented to serve as a summary for the ensemble of diverse clusters. With the local diversity in ensembles exploited, two novel consensus functions are further proposed. Extensive experiments on a variety of real-world datasets demonstrate the superiority of the proposed approach over the state-of-the-art.

</details>

<details>

<summary>2019-12-30 11:05:01 - Detecting and Correcting Adversarial Images Using Image Processing Operations</summary>

- *Huy H. Nguyen, Minoru Kuribayashi, Junichi Yamagishi, Isao Echizen*

- `1912.05391v2` - [abs](http://arxiv.org/abs/1912.05391v2) - [pdf](http://arxiv.org/pdf/1912.05391v2)

> Deep neural networks (DNNs) have achieved excellent performance on several tasks and have been widely applied in both academia and industry. However, DNNs are vulnerable to adversarial machine learning attacks, in which noise is added to the input to change the network output. We have devised an image-processing-based method to detect adversarial images based on our observation that adversarial noise is reduced after applying these operations while the normal images almost remain unaffected. In addition to detection, this method can be used to restore the adversarial images' original labels, which is crucial to restoring the normal functionalities of DNN-based systems. Testing using an adversarial machine learning database we created for generating several types of attack using images from the ImageNet Large Scale Visual Recognition Challenge database demonstrated the efficiency of our proposed method for both detection and correction.

</details>

<details>

<summary>2019-12-30 12:46:20 - ATMPA: Attacking Machine Learning-based Malware Visualization Detection Methods via Adversarial Examples</summary>

- *Xinbo Liu, Jiliang Zhang, Yaping Lin, He Li*

- `1808.01546v3` - [abs](http://arxiv.org/abs/1808.01546v3) - [pdf](http://arxiv.org/pdf/1808.01546v3)

> Since the threat of malicious software (malware) has become increasingly serious, automatic malware detection techniques have received increasing attention, where machine learning (ML)-based visualization detection methods become more and more popular. In this paper, we demonstrate that the state-of-the-art ML-based visualization detection methods are vulnerable to Adversarial Example (AE) attacks. We develop a novel Adversarial Texture Malware Perturbation Attack (ATMPA) method based on the gradient descent and L-norm optimization method, where attackers can introduce some tiny perturbations on the transformed dataset such that ML-based malware detection methods will completely fail. The experimental results on the MS BIG malware dataset show that a small interference can reduce the accuracy rate down to 0% for several ML-based detection methods, and the rate of transferability is 74.1% on average.

</details>

<details>

<summary>2019-12-31 14:36:29 - Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification</summary>

- *Xiaoyu Cao, Neil Zhenqiang Gong*

- `1709.05583v4` - [abs](http://arxiv.org/abs/1709.05583v4) - [pdf](http://arxiv.org/pdf/1709.05583v4)

> Deep neural networks (DNNs) have transformed several artificial intelligence research areas including computer vision, speech recognition, and natural language processing. However, recent studies demonstrated that DNNs are vulnerable to adversarial manipulations at testing time. Specifically, suppose we have a testing example, whose label can be correctly predicted by a DNN classifier. An attacker can add a small carefully crafted noise to the testing example such that the DNN classifier predicts an incorrect label, where the crafted testing example is called adversarial example. Such attacks are called evasion attacks. Evasion attacks are one of the biggest challenges for deploying DNNs in safety and security critical applications such as self-driving cars. In this work, we develop new methods to defend against evasion attacks. Our key observation is that adversarial examples are close to the classification boundary. Therefore, we propose region-based classification to be robust to adversarial examples. For a benign/adversarial testing example, we ensemble information in a hypercube centered at the example to predict its label. In contrast, traditional classifiers are point-based classification, i.e., given a testing example, the classifier predicts its label based on the testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets demonstrate that our region-based classification can significantly mitigate evasion attacks without sacrificing classification accuracy on benign examples. Specifically, our region-based classification achieves the same classification accuracy on testing benign examples as point-based classification, but our region-based classification is significantly more robust than point-based classification to various evasion attacks.

</details>

<details>

<summary>2019-12-31 15:55:03 - Privacy for Rescue: A New Testimony Why Privacy is Vulnerable In Deep Models</summary>

- *Ruiyuan Gao, Ming Dun, Hailong Yang, Zhongzhi Luan, Depei Qian*

- `2001.00493v1` - [abs](http://arxiv.org/abs/2001.00493v1) - [pdf](http://arxiv.org/pdf/2001.00493v1)

> The huge computation demand of deep learning models and limited computation resources on the edge devices calls for the cooperation between edge device and cloud service by splitting the deep models into two halves. However, transferring the intermediates results from the partial models between edge device and cloud service makes the user privacy vulnerable since the attacker can intercept the intermediate results and extract privacy information from them. Existing research works rely on metrics that are either impractical or insufficient to measure the effectiveness of privacy protection methods in the above scenario, especially from the aspect of a single user. In this paper, we first present a formal definition of the privacy protection problem in the edge-cloud system running DNN models. Then, we analyze the-state-of-the-art methods and point out the drawbacks of their methods, especially the evaluation metrics such as the Mutual Information (MI). In addition, we perform several experiments to demonstrate that although existing methods perform well under MI, they are not effective enough to protect the privacy of a single user. To address the drawbacks of the evaluation metrics, we propose two new metrics that are more accurate to measure the effectiveness of privacy protection methods. Finally, we highlight several potential research directions to encourage future efforts addressing the privacy protection problem.

</details>

<details>

<summary>2019-12-31 20:48:13 - A Performance Comparison of Data Mining Algorithms Based Intrusion Detection System for Smart Grid</summary>

- *Zakaria El Mrabet, Hassan El Ghazi, Naima Kaabouch*

- `2001.00917v1` - [abs](http://arxiv.org/abs/2001.00917v1) - [pdf](http://arxiv.org/pdf/2001.00917v1)

> Smart grid is an emerging and promising technology. It uses the power of information technologies to deliver intelligently the electrical power to customers, and it allows the integration of the green technology to meet the environmental requirements. Unfortunately, information technologies have its inherent vulnerabilities and weaknesses that expose the smart grid to a wide variety of security risks. The Intrusion detection system (IDS) plays an important role in securing smart grid networks and detecting malicious activity, yet it suffers from several limitations. Many research papers have been published to address these issues using several algorithms and techniques. Therefore, a detailed comparison between these algorithms is needed. This paper presents an overview of four data mining algorithms used by IDS in Smart Grid. An evaluation of performance of these algorithms is conducted based on several metrics including the probability of detection, probability of false alarm, probability of miss detection, efficiency, and processing time. Results show that Random Forest outperforms the other three algorithms in detecting attacks with higher probability of detection, lower probability of false alarm, lower probability of miss detection, and higher accuracy.

</details>

<details>

<summary>2019-12-31 21:06:20 - Deep Learning-Based Intrusion Detection System for Advanced Metering Infrastructure</summary>

- *Zakaria El Mrabet, Mehdi Ezzari, Hassan Elghazi, Badr Abou El Majd*

- `2001.00916v1` - [abs](http://arxiv.org/abs/2001.00916v1) - [pdf](http://arxiv.org/pdf/2001.00916v1)

> Smart grid is an alternative solution of the conventional power grid which harnesses the power of the information technology to save the energy and meet today's environment requirements. Due to the inherent vulnerabilities in the information technology, the smart grid is exposed to a wide variety of threats that could be translated into cyber-attacks. In this paper, we develop a deep learning-based intrusion detection system to defend against cyber-attacks in the advanced metering infrastructure network. The proposed machine learning approach is trained and tested extensively on an empirical industrial dataset which is composed of several attack categories including the scanning, buffer overflow, and denial of service attacks. Then, an experimental comparison in terms of detection accuracy is conducted to evaluate the performance of the proposed approach with Naive Bayes, Support Vector Machine, and Random Forest. The obtained results suggest that the proposed approaches produce optimal results comparing to the other algorithms. Finally, we propose a network architecture to deploy the proposed anomaly-based intrusion detection system across the Advanced Metering Infrastructure network. In addition, we propose a network security architecture composed of two types of Intrusion detection system types, Host and Network-based, deployed across the Advanced Metering Infrastructure network to inspect the traffic and detect the malicious one at all the levels.

</details>

