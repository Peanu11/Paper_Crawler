# 2017

## TOC

- [2017-01](#2017-01)
- [2017-02](#2017-02)
- [2017-03](#2017-03)
- [2017-04](#2017-04)
- [2017-05](#2017-05)
- [2017-06](#2017-06)
- [2017-07](#2017-07)
- [2017-08](#2017-08)
- [2017-09](#2017-09)
- [2017-10](#2017-10)
- [2017-11](#2017-11)
- [2017-12](#2017-12)

## 2017-01

<details>

<summary>2017-01-02 14:21:54 - Study and Development of a Symmetric protocol to secure communications in WSN</summary>

- *Yassine Maleh, Abdellah Ezzati*

- `1701.00401v1` - [abs](http://arxiv.org/abs/1701.00401v1) - [pdf](http://arxiv.org/pdf/1701.00401v1)

> Wireless Sensor Network (WSN) is consisting of independent and distributed sensors to monitor physical or environmental conditions, such as temperature, sound, pressure, etc. The most crucial and fundamental challenge facing WSN is security. Due to minimum capacity in-term of memory cost, processing and physical accessibility to sensors devices the security attacks are problematic. They are mostly deployed in open area, which expose them to different kinds of attacks. In this paper, we present an illustration of different attacks and vulnerabilities in WSN. Then we proposed a new lightweight cryptography algorithm for identifying compromised node in WSN called Leap Enhanced. Our evaluations on TOSSIM give a precise and detailed idea of the extra cost of consumption of resources needed to ensure the high level of expected security compared to other cryptography schemes in literature.

</details>

<details>

<summary>2017-01-03 18:56:39 - Security-related Research in Ubiquitous Computing -- Results of a Systematic Literature Review</summary>

- *Ema Kušen, Mark Strembeck*

- `1701.00773v1` - [abs](http://arxiv.org/abs/1701.00773v1) - [pdf](http://arxiv.org/pdf/1701.00773v1)

> In an endeavor to reach the vision of ubiquitous computing where users are able to use pervasive services without spatial and temporal constraints, we are witnessing a fast growing number of mobile and sensor-enhanced devices becoming available. However, in order to take full advantage of the numerous benefits offered by novel mobile devices and services, we must address the related security issues. In this paper, we present results of a systematic literature review (SLR) on security-related topics in ubiquitous computing environments. In our study, we found 5165 scientific contributions published between 2003 and 2015. We applied a systematic procedure to identify the threats, vulnerabilities, attacks, as well as corresponding defense mechanisms that are discussed in those publications. While this paper mainly discusses the results of our study, the corresponding SLR protocol which provides all details of the SLR is also publicly available for download.

</details>

<details>

<summary>2017-01-15 09:45:52 - The German eID as an Authentication Token on Android Devices</summary>

- *Florian Otterbein, Tim Ohlendorf, Marian Margraf*

- `1701.04013v1` - [abs](http://arxiv.org/abs/1701.04013v1) - [pdf](http://arxiv.org/pdf/1701.04013v1)

> Due to the rapid increase of digitization within our society, digital identities gain more and more importance. Provided by the German eID solution, every citizen has the ability to identify himself against various governmental and private organizations with the help of his personal electronic ID card and a corresponding card reader. While there are several solutions available for desktop use of the eID infrastructure, mobile approaches have to be payed more attention. In this paper we present a new approach for using the German eID concept on an Android device without the need of the actual identity card and card reader. A security evaluation of our approach reveals that two non-critical vulnerabilities on the architecture can't be avoided. Nevertheless, no sensitive information are compromised. A proof of concept shows that an actual implementation faces some technical issues which have to be solved in the future.

</details>

<details>

<summary>2017-01-15 14:05:07 - Static Detection of DoS Vulnerabilities in Programs that use Regular Expressions (Extended Version)</summary>

- *Valentin Wüstholz, Oswaldo Olivo, Marijn J. H. Heule, Isil Dillig*

- `1701.04045v1` - [abs](http://arxiv.org/abs/1701.04045v1) - [pdf](http://arxiv.org/pdf/1701.04045v1)

> In an algorithmic complexity attack, a malicious party takes advantage of the worst-case behavior of an algorithm to cause denial-of-service. A prominent algorithmic complexity attack is regular expression denial-of-service (ReDoS), in which the attacker exploits a vulnerable regular expression by providing a carefully-crafted input string that triggers worst-case behavior of the matching algorithm. This paper proposes a technique for automatically finding ReDoS vulnerabilities in programs. Specifically, our approach automatically identifies vulnerable regular expressions in the program and determines whether an "evil" input string can be matched against a vulnerable regular expression. We have implemented our proposed approach in a tool called REXPLOITER and found 41 exploitable security vulnerabilities in Java web applications.

</details>

<details>

<summary>2017-01-16 02:39:01 - Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks</summary>

- *Vahid Behzadan, Arslan Munir*

- `1701.04143v1` - [abs](http://arxiv.org/abs/1701.04143v1) - [pdf](http://arxiv.org/pdf/1701.04143v1)

> Deep learning classifiers are known to be inherently vulnerable to manipulation by intentionally perturbed inputs, named adversarial examples. In this work, we establish that reinforcement learning techniques based on Deep Q-Networks (DQNs) are also vulnerable to adversarial input perturbations, and verify the transferability of adversarial examples across different DQN models. Furthermore, we present a novel class of attacks based on this vulnerability that enable policy manipulation and induction in the learning process of DQNs. We propose an attack mechanism that exploits the transferability of adversarial examples to implement policy induction attacks on DQNs, and demonstrate its efficacy and impact through experimental study of a game-learning scenario.

</details>

<details>

<summary>2017-01-16 23:21:23 - Network-on-Chip Firewall: Countering Defective and Malicious System-on-Chip Hardware</summary>

- *Michael LeMay, Carl A. Gunter*

- `1404.3465v2` - [abs](http://arxiv.org/abs/1404.3465v2) - [pdf](http://arxiv.org/pdf/1404.3465v2)

> Mobile devices are in roles where the integrity and confidentiality of their apps and data are of paramount importance. They usually contain a System-on-Chip (SoC), which integrates microprocessors and peripheral Intellectual Property (IP) connected by a Network-on-Chip (NoC). Malicious IP or software could compromise critical data. Some types of attacks can be blocked by controlling data transfers on the NoC using Memory Management Units (MMUs) and other access control mechanisms. However, commodity processors do not provide strong assurances regarding the correctness of such mechanisms, and it is challenging to verify that all access control mechanisms in the system are correctly configured. We propose a NoC Firewall (NoCF) that provides a single locus of control and is amenable to formal analysis. We demonstrate an initial analysis of its ability to resist malformed NoC commands, which we believe is the first effort to detect vulnerabilities that arise from NoC protocol violations perpetrated by erroneous or malicious IP.

</details>

<details>

<summary>2017-01-17 04:43:36 - Cyber-Physical Systems Security -- A Survey</summary>

- *Abdulmalik Humayed, Jingqiang Lin, Fengjun Li, Bo Luo*

- `1701.04525v1` - [abs](http://arxiv.org/abs/1701.04525v1) - [pdf](http://arxiv.org/pdf/1701.04525v1)

> With the exponential growth of cyber-physical systems (CPS), new security challenges have emerged. Various vulnerabilities, threats, attacks, and controls have been introduced for the new generation of CPS. However, there lack a systematic study of CPS security issues. In particular, the heterogeneity of CPS components and the diversity of CPS systems have made it very difficult to study the problem with one generalized model.   In this paper, we capture and systematize existing research on CPS security under a unified framework. The framework consists of three orthogonal coordinates: (1) from the \emph{security} perspective, we follow the well-known taxonomy of threats, vulnerabilities, attacks and controls; (2)from the \emph{CPS components} perspective, we focus on cyber, physical, and cyber-physical components; and (3) from the \emph{CPS systems} perspective, we explore general CPS features as well as representative systems (e.g., smart grids, medical CPS and smart cars). The model can be both abstract to show general interactions of a CPS application and specific to capture any details when needed. By doing so, we aim to build a model that is abstract enough to be applicable to various heterogeneous CPS applications; and to gain a modular view of the tightly coupled CPS components. Such abstract decoupling makes it possible to gain a systematic understanding of CPS security, and to highlight the potential sources of attacks and ways of protection.

</details>

<details>

<summary>2017-01-17 15:59:17 - Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning</summary>

- *Rock Stevens, Octavian Suciu, Andrew Ruef, Sanghyun Hong, Michael Hicks, Tudor Dumitraş*

- `1701.04739v1` - [abs](http://arxiv.org/abs/1701.04739v1) - [pdf](http://arxiv.org/pdf/1701.04739v1)

> Governments and businesses increasingly rely on data analytics and machine learning (ML) for improving their competitive edge in areas such as consumer satisfaction, threat intelligence, decision making, and product efficiency. However, by cleverly corrupting a subset of data used as input to a target's ML algorithms, an adversary can perturb outcomes and compromise the effectiveness of ML technology. While prior work in the field of adversarial machine learning has studied the impact of input manipulation on correct ML algorithms, we consider the exploitation of bugs in ML implementations. In this paper, we characterize the attack surface of ML programs, and we show that malicious inputs exploiting implementation bugs enable strictly more powerful attacks than the classic adversarial machine learning techniques. We propose a semi-automated technique, called steered fuzzing, for exploring this attack surface and for discovering exploitable bugs in machine learning programs, in order to demonstrate the magnitude of this threat. As a result of our work, we responsibly disclosed five vulnerabilities, established three new CVE-IDs, and illuminated a common insecure practice across many machine learning systems. Finally, we outline several research directions for further understanding and mitigating this threat.

</details>

<details>

<summary>2017-01-19 07:43:21 - An Implementation of SCADA Network Security Testbed</summary>

- *Liao Zhang*

- `1701.05323v1` - [abs](http://arxiv.org/abs/1701.05323v1) - [pdf](http://arxiv.org/pdf/1701.05323v1)

> The security of industrial network has become an increasing concern in industry infrastructure operation. Motivated by on-going collaborations with Fortinet Corp., a security company, this project implements a testbed for supervisory control and data acquisition (SCADA) network security research by software emulation. Concepts about SCADA and Modbus protocol are reviewed in the report. Besides Modbus, vulnerabilities about several other industrial protocols are also studied for this project. In this report, a typical tank system following Modbus protocol is built as a testbed. Both attack and defense toolkits are introduced to emulate the attack and protection of the Modbus network. The emulation platform is also capable of entrapping hackers and gathering their activity data.

</details>

<details>

<summary>2017-01-19 21:31:16 - A Hybrid DOS-Tolerant PKC-Based Key Management System for WSNs</summary>

- *Hamzeh Ghasemzadeh, Ali Payandeh, Mohammad Reza Aref*

- `1701.05608v1` - [abs](http://arxiv.org/abs/1701.05608v1) - [pdf](http://arxiv.org/pdf/1701.05608v1)

> Security is a critical and vital task in wireless sensor networks, therefore different key management systems have been proposed, many of which are based on symmetric cryptography. Such systems are very energy efficient, but they lack some other desirable characteristics. On the other hand, systems based on public key cryptography have those desirable characteristics, but they consume more energy. Recently based on authenticated messages from base station a new PKC based key agreement protocol was proposed. We show this method is susceptible to a form of denial of service attack where resources of the network can be exhausted with bogus messages. Then, we propose two different improvements to solve this vulnerability. Simulation results show that these new protocols retain desirable characteristics of the basic method and solve its deficiencies.

</details>

<details>

<summary>2017-01-21 12:05:31 - Quantifying vulnerability of secret generation using hyper-distributions (extended version)</summary>

- *Mário S. Alvim, Piotr Mardziel, Michael Hicks*

- `1701.04174v2` - [abs](http://arxiv.org/abs/1701.04174v2) - [pdf](http://arxiv.org/pdf/1701.04174v2)

> Traditional approaches to Quantitative Information Flow (QIF) represent the adversary's prior knowledge of possible secret values as a single probability distribution. This representation may miss important structure. For instance, representing prior knowledge about passwords of a system's users in this way overlooks the fact that many users generate passwords using some strategy. Knowledge of such strategies can help the adversary in guessing a secret, so ignoring them may underestimate the secret's vulnerability. In this paper we explicitly model strategies as distributions on secrets, and generalize the representation of the adversary's prior knowledge from a distribution on secrets to an environment, which is a distribution on strategies (and, thus, a distribution on distributions on secrets, called a hyper-distribution). By applying information-theoretic techniques to environments we derive several meaningful generalizations of the traditional approach to QIF. In particular, we disentangle the vulnerability of a secret from the vulnerability of the strategies that generate secrets, and thereby distinguish security by aggregation--which relies on the uncertainty over strategies--from security by strategy--which relies on the intrinsic uncertainty within a strategy. We also demonstrate that, in a precise way, no further generalization of prior knowledge (e.g., by using distributions of even higher order) is needed to soundly quantify the vulnerability of the secret.

</details>

<details>

<summary>2017-01-23 07:21:43 - dna2vec: Consistent vector representations of variable-length k-mers</summary>

- *Patrick Ng*

- `1701.06279v1` - [abs](http://arxiv.org/abs/1701.06279v1) - [pdf](http://arxiv.org/pdf/1701.06279v1)

> One of the ubiquitous representation of long DNA sequence is dividing it into shorter k-mer components. Unfortunately, the straightforward vector encoding of k-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse yet, the distance between any pair of one-hot vectors is equidistant. This is particularly problematic when applying the latest machine learning algorithms to solve problems in biological sequence analysis. In this paper, we propose a novel method to train distributed representations of variable-length k-mers. Our method is based on the popular word embedding model word2vec, which is trained on a shallow two-layer neural network. Our experiments provide evidence that the summing of dna2vec vectors is akin to nucleotides concatenation. We also demonstrate that there is correlation between Needleman-Wunsch similarity score and cosine similarity of dna2vec vectors.

</details>

<details>

<summary>2017-01-25 10:01:39 - Learn&Fuzz: Machine Learning for Input Fuzzing</summary>

- *Patrice Godefroid, Hila Peleg, Rishabh Singh*

- `1701.07232v1` - [abs](http://arxiv.org/abs/1701.07232v1) - [pdf](http://arxiv.org/pdf/1701.07232v1)

> Fuzzing consists of repeatedly testing an application with modified, or fuzzed, inputs with the goal of finding security vulnerabilities in input-parsing code. In this paper, we show how to automate the generation of an input grammar suitable for input fuzzing using sample inputs and neural-network-based statistical machine-learning techniques. We present a detailed case study with a complex input format, namely PDF, and a large complex security-critical parser for this format, namely, the PDF parser embedded in Microsoft's new Edge browser. We discuss (and measure) the tension between conflicting learning and fuzzing goals: learning wants to capture the structure of well-formed inputs, while fuzzing wants to break that structure in order to cover unexpected code paths and find bugs. We also present a new algorithm for this learn&fuzz challenge which uses a learnt input probability distribution to intelligently guide where to fuzz inputs.

</details>

<details>

<summary>2017-01-26 19:59:32 - JSForce: A Forced Execution Engine for Malicious JavaScript Detection</summary>

- *Xunchao Hu, Yao Cheng, Yue Duan, Andrew Henderson, Heng Yin*

- `1701.07860v1` - [abs](http://arxiv.org/abs/1701.07860v1) - [pdf](http://arxiv.org/pdf/1701.07860v1)

> The drastic increase of JavaScript exploitation attacks has led to a strong interest in developing techniques to enable malicious JavaScript analysis. Existing analysis tech- niques fall into two general categories: static analysis and dynamic analysis. Static analysis tends to produce inaccurate results (both false positive and false negative) and is vulnerable to a wide series of obfuscation techniques. Thus, dynamic analysis is constantly gaining popularity for exposing the typical features of malicious JavaScript. However, existing dynamic analysis techniques possess limitations such as limited code coverage and incomplete environment setup, leaving a broad attack surface for evading the detection. To overcome these limitations, we present the design and implementation of a novel JavaScript forced execution engine named JSForce which drives an arbitrary JavaScript snippet to execute along different paths without any input or environment setup. We evaluate JSForce using 220,587 HTML and 23,509 PDF real- world samples. Experimental results show that by adopting our forced execution engine, the malicious JavaScript detection rate can be substantially boosted by 206.29% using same detection policy without any noticeable false positive increase. We also make JSForce publicly available as an online service and will release the source code to the security community upon the acceptance for publication.

</details>

<details>

<summary>2017-01-27 15:33:32 - Avoiding The Man on the Wire: Improving Tor's Security with Trust-Aware Path Selection</summary>

- *Aaron Johnson, Rob Jansen, Aaron D. Jaggard, Joan Feigenbaum, Paul Syverson*

- `1511.05453v3` - [abs](http://arxiv.org/abs/1511.05453v3) - [pdf](http://arxiv.org/pdf/1511.05453v3)

> Tor users are vulnerable to deanonymization by an adversary that can observe some Tor relays or some parts of the network. We demonstrate that previous network-aware path-selection algorithms that propose to solve this problem are vulnerable to attacks across multiple Tor connections. We suggest that users use trust to choose the paths through Tor that are less likely to be observed, where trust is flexibly modeled as a probability distribution on the location of the user's adversaries, and we present the Trust-Aware Path Selection algorithm for Tor that helps users avoid traffic-analysis attacks while still choosing paths that could have been selected by many other users. We evaluate this algorithm in two settings using a high-level map of Internet routing: (i) users try to avoid a single global adversary that has an independent chance to control each Autonomous System organization, Internet Exchange Point organization, and Tor relay family, and (ii) users try to avoid deanonymization by any single country. We also examine the performance of Trust-Aware Path selection using the Shadow network simulator.

</details>

<details>

<summary>2017-01-27 16:23:22 - Control-Flow Integrity: Precision, Security, and Performance</summary>

- *Nathan Burow, Scott A. Carr, Joseph Nash, Per Larsen, Michael Franz, Stefan Brunthaler, Mathias Payer*

- `1602.04056v3` - [abs](http://arxiv.org/abs/1602.04056v3) - [pdf](http://arxiv.org/pdf/1602.04056v3)

> Memory corruption errors in C/C++ programs remain the most common source of security vulnerabilities in today's systems. Control-flow hijacking attacks exploit memory corruption vulnerabilities to divert program execution away from the intended control flow. Researchers have spent more than a decade studying and refining defenses based on Control-Flow Integrity (CFI), and this technique is now integrated into several production compilers. However, so far no study has systematically compared the various proposed CFI mechanisms, nor is there any protocol on how to compare such mechanisms.   We compare a broad range of CFI mechanisms using a unified nomenclature based on (i) a qualitative discussion of the conceptual security guarantees, (ii) a quantitative security evaluation, and (iii) an empirical evaluation of their performance in the same test environment. For each mechanism, we evaluate (i) protected types of control-flow transfers, (ii) the precision of the protection for forward and backward edges. For open-source compiler-based implementations, we additionally evaluate (iii) the generated equivalence classes and target sets, and (iv) the runtime performance.

</details>


## 2017-02

<details>

<summary>2017-02-04 08:04:39 - Cyber-Physical Attacks on UAS Networks- Challenges and Open Research Problems</summary>

- *Vahid Behzadan*

- `1702.01251v1` - [abs](http://arxiv.org/abs/1702.01251v1) - [pdf](http://arxiv.org/pdf/1702.01251v1)

> Assignment of critical missions to unmanned aerial vehicles (UAV) is bound to widen the grounds for adversarial intentions in the cyber domain, potentially ranging from disruption of command and control links to capture and use of airborne nodes for kinetic attacks. Ensuring the security of electronic and communications in multi-UAV systems is of paramount importance for their safe and reliable integration with military and civilian airspaces. Over the past decade, this active field of research has produced many notable studies and novel proposals for attacks and mitigation techniques in UAV networks. Yet, the generic modeling of such networks as typical MANETs and isolated systems has left various vulnerabilities out of the investigative focus of the research community. This paper aims to emphasize on some of the critical challenges in securing UAV networks against attacks targeting vulnerabilities specific to such systems and their cyber-physical aspects.

</details>

<details>

<summary>2017-02-08 04:33:55 - Adversarial Attacks on Neural Network Policies</summary>

- *Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, Pieter Abbeel*

- `1702.02284v1` - [abs](http://arxiv.org/abs/1702.02284v1) - [pdf](http://arxiv.org/pdf/1702.02284v1)

> Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.

</details>

<details>

<summary>2017-02-08 22:15:31 - Vulnerability of Fixed-Time Control of Signalized Intersections to Cyber-Tampering</summary>

- *Amin Ghafouri, Waseem Abbas, Yevgeniy Vorobeychik, Xenofon Koutsoukos*

- `1606.06698v3` - [abs](http://arxiv.org/abs/1606.06698v3) - [pdf](http://arxiv.org/pdf/1606.06698v3)

> Recent experimental studies have shown that traffic management systems are vulnerable to cyber-attacks on sensor data. This paper studies the vulnerability of fixed-time control of signalized intersections when sensors measuring traffic flow information are compromised and perturbed by an adversary. The problems are formulated by considering three malicious objectives: 1) worst-case network accumulation, which aims to destabilize the overall network as much as possible; 2) worst-case lane accumulation, which aims to cause worst-case accumulation on some target lanes; and 3) risk-averse target accumulation, which aims to reach a target accumulation by making the minimum perturbation to sensor data. The problems are solved using bilevel programming optimization methods. Finally, a case study of a real network is used to illustrate the results.

</details>

<details>

<summary>2017-02-11 00:39:39 - Adversarial examples in the physical world</summary>

- *Alexey Kurakin, Ian Goodfellow, Samy Bengio*

- `1607.02533v4` - [abs](http://arxiv.org/abs/1607.02533v4) - [pdf](http://arxiv.org/pdf/1607.02533v4)

> Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.

</details>

<details>

<summary>2017-02-13 09:21:08 - Turning Internet of Things(IoT) into Internet of Vulnerabilities (IoV) : IoT Botnets</summary>

- *Kishore Angrishi*

- `1702.03681v1` - [abs](http://arxiv.org/abs/1702.03681v1) - [pdf](http://arxiv.org/pdf/1702.03681v1)

> Internet of Things (IoT) is the next big evolutionary step in the world of internet. The main intention behind the IoT is to enable safer living and risk mitigation on different levels of life. With the advent of IoT botnets, the view towards IoT devices has changed from enabler of enhanced living into Internet of vulnerabilities for cyber criminals. IoT botnets has exposed two different glaring issues, 1) A large number of IoT devices are accessible over public Internet. 2) Security (if considered at all) is often an afterthought in the architecture of many wide spread IoT devices. In this article, we briefly outline the anatomy of the IoT botnets and their basic mode of operations. Some of the major DDoS incidents using IoT botnets in recent times along with the corresponding exploited vulnerabilities will be discussed. We also provide remedies and recommendations to mitigate IoT related cyber risks and briefly illustrate the importance of cyber insurance in the modern connected world.

</details>

<details>

<summary>2017-02-14 02:54:21 - Does Your DNS Recursion Really Time Out as Intended? A Timeout Vulnerability of DNS Recursive Servers</summary>

- *Zheng Wang*

- `1607.00269v2` - [abs](http://arxiv.org/abs/1607.00269v2) - [pdf](http://arxiv.org/pdf/1607.00269v2)

> Parallelization is featured by DNS recursive servers to do time-consuming recursions on behalf on clients. As common DNS configurations, recursive servers should allow a reasonable timeout for each recursion which may take as long as several seconds. However, it is proposed in this paper that recursion parallelization may be exploited by attackers to compromise the recursion timeout mechanism for the purpose of DoS or DDoS attacks. Attackers can have recursive servers drop early existing recursions in service by saturating recursion parallelization. The key of the proposed attack model is to reliably prolong service times for any attacking queries. As means of prolong service times, serval techniques are proposed to effectively avoiding cache hit and prolonging overall latency of external DNS lookups respectively. The impacts of saturated recursion parallelization on timeout are analytically provided. The testing on BIND servers demonstrates that with carefully crafted queries, an attacker can use a low or moderate level of query load to successfully overwhelm a target recursive server from serving the legitimate clients.

</details>

<details>

<summary>2017-02-17 05:50:46 - Reasoning about Probabilistic Defense Mechanisms against Remote Attacks</summary>

- *Martín Ochoa, Sebastian Banescu, Cynthia Disenfeld, Gilles Barthe, Vijay Ganesh*

- `1701.06743v2` - [abs](http://arxiv.org/abs/1701.06743v2) - [pdf](http://arxiv.org/pdf/1701.06743v2)

> Despite numerous countermeasures proposed by practitioners and researchers, remote control-flow alteration of programs with memory-safety vulnerabilities continues to be a realistic threat. Guaranteeing that complex software is completely free of memory-safety vulnerabilities is extremely expensive. Probabilistic countermeasures that depend on random secret keys are interesting, because they are an inexpensive way to raise the bar for attackers who aim to exploit memory-safety vulnerabilities. Moreover, some countermeasures even support legacy systems. However, it is unclear how to quantify and compare the effectiveness of different probabilistic countermeasures or combinations of such countermeasures. In this paper we propose a methodology to rigorously derive security bounds for probabilistic countermeasures. We argue that by representing security notions in this setting as events in probabilistic games, similarly as done with cryptographic security definitions, concrete and asymptotic guarantees can be obtained against realistic attackers. These guarantees shed light on the effectiveness of single countermeasures and their composition and allow practitioners to more precisely gauge the risk of an attack.

</details>

<details>

<summary>2017-02-17 07:18:01 - On Ladder Logic Bombs in Industrial Control Systems</summary>

- *Naman Govil, Anand Agrawal, Nils Ole Tippenhauer*

- `1702.05241v1` - [abs](http://arxiv.org/abs/1702.05241v1) - [pdf](http://arxiv.org/pdf/1702.05241v1)

> In industrial control systems, devices such as Programmable Logic Controllers (PLCs) are commonly used to directly interact with sensors and actuators, and perform local automatic control. PLCs run software on two different layers: a) firmware (i.e. the OS) and b) control logic (processing sensor readings to determine control actions). In this work, we discuss ladder logic bombs, i.e. malware written in ladder logic (or one of the other IEC 61131-3-compatible languages). Such malware would be inserted by an attacker into existing control logic on a PLC, and either persistently change the behavior, or wait for specific trigger signals to activate malicious behaviour. For example, the LLB could replace legitimate sensor readings with manipulated values. We see the concept of LLBs as a generalization of attacks such as the Stuxnet attack. We introduce LLBs on an abstract level, and then demonstrate several designs based on real PLC devices in our lab. In particular, we also focus on stealthy LLBs, i.e. LLBs that are hard to detect by human operators manually validating the program running in PLCs. In addition to introducing vulnerabilities on the logic layer, we also discuss countermeasures and we propose two detection techniques.

</details>

<details>

<summary>2017-02-21 06:53:38 - On Detecting Adversarial Perturbations</summary>

- *Jan Hendrik Metzen, Tim Genewein, Volker Fischer, Bastian Bischoff*

- `1702.04267v2` - [abs](http://arxiv.org/abs/1702.04267v2) - [pdf](http://arxiv.org/pdf/1702.04267v2)

> Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small "detector" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.

</details>

<details>

<summary>2017-02-21 15:19:56 - Contract-Theoretic Resource Allocation for Critical Infrastructure Protection</summary>

- *AbdelRahman Eldosouky, Walid Saad, Charles Kamhoua, and Kevin Kwiat*

- `1702.06436v1` - [abs](http://arxiv.org/abs/1702.06436v1) - [pdf](http://arxiv.org/pdf/1702.06436v1)

> Critical infrastructure protection (CIP) is envisioned to be one of the most challenging security problems in the coming decade. One key challenge in CIP is the ability to allocate resources, either personnel or cyber, to critical infrastructures with different vulnerability and criticality levels. In this work, a contract-theoretic approach is proposed to solve the problem of resource allocation in critical infrastructure with asymmetric information. A control center (CC) is used to design contracts and offer them to infrastructures' owners. A contract can be seen as an agreement between the CC and infrastructures using which the CC allocates resources and gets rewards in return. Contracts are designed in a way to maximize the CC's benefit and motivate each infrastructure to accept a contract and obtain proper resources for its protection. Infrastructures are defined by both vulnerability levels and criticality levels which are unknown to the CC. Therefore, each infrastructure can claim that it is the most vulnerable or critical to gain more resources. A novel mechanism is developed to handle such an asymmetric information while providing the optimal contract that motivates each infrastructure to reveal its actual type. The necessary and sufficient conditions for such resource allocation contracts under asymmetric information are derived. Simulation results show that the proposed contract-theoretic approach maximizes the CC's utility while ensuring that no infrastructure has an incentive to ask for another contract, despite the lack of exact information at the CC.

</details>

<details>

<summary>2017-02-22 15:11:25 - Adversarial examples for generative models</summary>

- *Jernej Kos, Ian Fischer, Dawn Song*

- `1702.06832v1` - [abs](http://arxiv.org/abs/1702.06832v1) - [pdf](http://arxiv.org/pdf/1702.06832v1)

> We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.

</details>

<details>

<summary>2017-02-22 15:29:15 - Parameterized Shifted Combinatorial Optimization</summary>

- *Jakub Gajarský, Petr Hliněný, Martin Koutecký, Shmuel Onn*

- `1702.06844v1` - [abs](http://arxiv.org/abs/1702.06844v1) - [pdf](http://arxiv.org/pdf/1702.06844v1)

> Shifted combinatorial optimization is a new nonlinear optimization framework which is a broad extension of standard combinatorial optimization, involving the choice of several feasible solutions at a time. This framework captures well studied and diverse problems ranging from so-called vulnerability problems to sharing and partitioning problems. In particular, every standard combinatorial optimization problem has its shifted counterpart, which is typically much harder. Already with explicitly given input set the shifted problem may be NP-hard. In this article we initiate a study of the parameterized complexity of this framework. First we show that shifting over an explicitly given set with its cardinality as the parameter may be in XP, FPT or P, depending on the objective function. Second, we study the shifted problem over sets definable in MSO logic (which includes, e.g., the well known MSO partitioning problems). Our main results here are that shifted combinatorial optimization over MSO definable sets is in XP with respect to the MSO formula and the treewidth (or more generally clique-width) of the input graph, and is W[1]-hard even under further severe restrictions.

</details>

<details>

<summary>2017-02-23 05:48:22 - Discriminating Traces with Time</summary>

- *Saeid Tizpaz-Niari, Pavol Cerny, Bor-Yuh Evan Chang, Sriram Sankaranarayanan, Ashutosh Trivedi*

- `1702.07103v1` - [abs](http://arxiv.org/abs/1702.07103v1) - [pdf](http://arxiv.org/pdf/1702.07103v1)

> What properties about the internals of a program explain the possible differences in its overall running time for different inputs? In this paper, we propose a formal framework for considering this question we dub trace-set discrimination. We show that even though the algorithmic problem of computing maximum likelihood discriminants is NP-hard, approaches based on integer linear programming (ILP) and decision tree learning can be useful in zeroing-in on the program internals. On a set of Java benchmarks, we find that compactly-represented decision trees scalably discriminate with high accuracy---more scalably than maximum likelihood discriminants and with comparable accuracy. We demonstrate on three larger case studies how decision-tree discriminants produced by our tool are useful for debugging timing side-channel vulnerabilities (i.e., where a malicious observer infers secrets simply from passively watching execution times) and availability vulnerabilities.

</details>

<details>

<summary>2017-02-23 14:24:49 - GANDALF: A fine-grained hardware-software co-design for preventing memory attacks</summary>

- *Gnanambikai Krishnakumar, Patanjali SLPSK, Prasanna Karthik Vairam, Chester Rebeiro*

- `1702.07223v1` - [abs](http://arxiv.org/abs/1702.07223v1) - [pdf](http://arxiv.org/pdf/1702.07223v1)

> Reading or writing outside the bounds of a buffer is a serious security vulnerability that has been exploited in numerous occasions. These attacks can be prevented by ensuring that every buffer is only accessed within its specified bounds. In this paper we present Gandalf, a compiler-assisted hardware extension for the OpenRISC processor that thwarts all forms of memory based attacks including buffer overflows and over-reads.The feature associates lightweight base and bound capabilities to all pointer variables, which are checked at run time by the hardware. Gandalf is transparent to the user and does not require significant OS modifications. Moreover, it achieves locality, thus resulting in small performance penalties.

</details>

<details>

<summary>2017-02-24 10:07:27 - Software Grand Exposure: SGX Cache Attacks Are Practical</summary>

- *Ferdinand Brasser, Urs Müller, Alexandra Dmitrienko, Kari Kostiainen, Srdjan Capkun, Ahmad-Reza Sadeghi*

- `1702.07521v1` - [abs](http://arxiv.org/abs/1702.07521v1) - [pdf](http://arxiv.org/pdf/1702.07521v1)

> Side-channel information leakage is a known limitation of SGX. Researchers have demonstrated that secret-dependent information can be extracted from enclave execution through page-fault access patterns. Consequently, various recent research efforts are actively seeking countermeasures to SGX side-channel attacks. It is widely assumed that SGX may be vulnerable to other side channels, such as cache access pattern monitoring, as well. However, prior to our work, the practicality and the extent of such information leakage was not studied.   In this paper we demonstrate that cache-based attacks are indeed a serious threat to the confidentiality of SGX-protected programs. Our goal was to design an attack that is hard to mitigate using known defenses, and therefore we mount our attack without interrupting enclave execution. This approach has major technical challenges, since the existing cache monitoring techniques experience significant noise if the victim process is not interrupted. We designed and implemented novel attack techniques to reduce this noise by leveraging the capabilities of the privileged adversary. Our attacks are able to recover confidential information from SGX enclaves, which we illustrate in two example cases: extraction of an entire RSA-2048 key during RSA decryption, and detection of specific human genome sequences during genomic indexing. We show that our attacks are more effective than previous cache attacks and harder to mitigate than previous SGX side-channel attacks.

</details>

<details>

<summary>2017-02-27 16:09:48 - Big Data for Social Sciences: Measuring patterns of human behavior through large-scale mobile phone data</summary>

- *Pål Sundsøy*

- `1702.08349v1` - [abs](http://arxiv.org/abs/1702.08349v1) - [pdf](http://arxiv.org/pdf/1702.08349v1)

> Through seven publications this dissertation shows how anonymized mobile phone data can contribute to the social good and provide insights into human behaviour on a large scale. The size of the datasets analysed ranges from 500 million to 300 billion phone records, covering millions of people. The key contributions are two-fold:   1. Big Data for Social Good: Through prediction algorithms the results show how mobile phone data can be useful to predict important socio-economic indicators, such as income, illiteracy and poverty in developing countries. Such knowledge can be used to identify where vulnerable groups in society are, reduce economic shocks and is a critical component for monitoring poverty rates over time. Further, the dissertation demonstrates how mobile phone data can be used to better understand human behaviour during large shocks in society, exemplified by an analysis of data from the terror attack in Norway and a natural disaster on the south-coast in Bangladesh. This work leads to an increased understanding of how information spreads, and how millions of people move around. The intention is to identify displaced people faster, cheaper and more accurately than existing survey-based methods.   2. Big Data for efficient marketing: Finally, the dissertation offers an insight into how anonymised mobile phone data can be used to map out large social networks, covering millions of people, to understand how products spread inside these networks. Results show that by including social patterns and machine learning techniques in a large-scale marketing experiment in Asia, the adoption rate is increased by 13 times compared to the approach used by experienced marketers. A data-driven and scientific approach to marketing, through more tailored campaigns, contributes to less irrelevant offers for the customers, and better cost efficiency for the companies.

</details>

<details>

<summary>2017-02-28 16:51:02 - Frugal Bribery in Voting</summary>

- *Palash Dey, Neeldhara Misra, Y. Narahari*

- `1504.08248v3` - [abs](http://arxiv.org/abs/1504.08248v3) - [pdf](http://arxiv.org/pdf/1504.08248v3)

> Bribery in elections is an important problem in computational social choice theory. However, bribery with money is often illegal in elections. Motivated by this, we introduce the notion of frugal bribery and formulate two new pertinent computational problems which we call Frugal-bribery and Frugal- $bribery to capture bribery without money in elections. In the proposed model, the briber is frugal in nature and this is captured by her inability to bribe votes of a certain kind, namely, non-vulnerable votes. In the Frugal-bribery problem, the goal is to make a certain candidate win the election by changing only vulnerable votes. In the Frugal-{dollar}bribery problem, the vulnerable votes have prices and the goal is to make a certain candidate win the election by changing only vulnerable votes, subject to a budget constraint of the briber. We further formulate two natural variants of the Frugal-{dollar}bribery problem namely Uniform-frugal-{dollar}bribery and Nonuniform-frugal-{dollar}bribery where the prices of the vulnerable votes are, respectively, all the same or different.   We study the computational complexity of the above problems for unweighted and weighted elections for several commonly used voting rules. We observe that, even if we have only a small number of candidates, the problems are intractable for all voting rules studied here for weighted elections, with the sole exception of the Frugal-bribery problem for the plurality voting rule. In contrast, we have polynomial time algorithms for the Frugal-bribery problem for plurality, veto, k-approval, k-veto, and plurality with runoff voting rules for unweighted elections. However, the Frugal-{dollar}bribery problem is intractable for all the voting rules studied here barring the plurality and the veto voting rules for unweighted elections.

</details>


## 2017-03

<details>

<summary>2017-03-03 10:27:16 - Adversarial Examples for Semantic Image Segmentation</summary>

- *Volker Fischer, Mummadi Chaithanya Kumar, Jan Hendrik Metzen, Thomas Brox*

- `1703.01101v1` - [abs](http://arxiv.org/abs/1703.01101v1) - [pdf](http://arxiv.org/pdf/1703.01101v1)

> Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations that lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.

</details>

<details>

<summary>2017-03-07 03:30:46 - A Covert Data Transport Protocol</summary>

- *Yu Fu, Zhe Jia, Lu Yu, Xingsi Zhong, Richard Brooks*

- `1703.02201v1` - [abs](http://arxiv.org/abs/1703.02201v1) - [pdf](http://arxiv.org/pdf/1703.02201v1)

> Both enterprise and national firewalls filter network connections. For data forensics and botnet removal applications, it is important to establish the information source. In this paper, we describe a data transport layer which allows a client to transfer encrypted data that provides no discernible information regarding the data source. We use a domain generation algorithm (DGA) to encode AES encrypted data into domain names that current tools are unable to reliably differentiate from valid domain names. The domain names are registered using (free) dynamic DNS services. The data transmission format is not vulnerable to Deep Packet Inspection (DPI).

</details>

<details>

<summary>2017-03-07 09:42:44 - Quantum Differential and Linear Cryptanalysis</summary>

- *Marc Kaplan, Gaëtan Leurent, Anthony Leverrier, María Naya-Plasencia*

- `1510.05836v3` - [abs](http://arxiv.org/abs/1510.05836v3) - [pdf](http://arxiv.org/pdf/1510.05836v3)

> Quantum computers, that may become available one day, would impact many scientific fields, most notably cryptography since many asymmetric primitives are insecure against an adversary with quantum capabilities. Cryptographers are already anticipating this threat by proposing and studying a number of potentially quantum-safe alternatives for those primitives. On the other hand, symmetric primitives seem less vulnerable against quantum computing: the main known applicable result is Grover's algorithm that gives a quadratic speed-up for exhaustive search.   In this work, we examine more closely the security of symmetric ciphers against quantum attacks. Since our trust in symmetric ciphers relies mostly on their ability to resist cryptanalysis techniques, we investigate quantum cryptanalysis techniques. More specifically, we consider quantum versions of differential and linear cryptanalysis. We show that it is usually possible to use quantum computations to obtain a quadratic speed-up for these attack techniques, but the situation must be nuanced: we don't get a quadratic speed-up for all variants of the attacks. This allows us to demonstrate the following non-intuitive result: the best attack in the classical world does not necessarily lead to the best quantum one. We give some examples of application on ciphers LAC and KLEIN. We also discuss the important difference between an adversary that can only perform quantum computations, and an adversary that can also make quantum queries to a keyed primitive.

</details>

<details>

<summary>2017-03-09 15:48:45 - Recommendations for Model-Driven Paradigms for Integrated Approaches to Cyber Defense</summary>

- *Mona Lange, Alexander Kott, Noam Ben-Asher, Wim Mees, Nazife Baykal, Cristian-Mihai Vidu, Matteo Merialdo, Marek Malowidzki, Bhopinder Madahar*

- `1703.03306v1` - [abs](http://arxiv.org/abs/1703.03306v1) - [pdf](http://arxiv.org/pdf/1703.03306v1)

> The North Atlantic Treaty Organization (NATO) Exploratory Team meeting, "Model-Driven Paradigms for Integrated Approaches to Cyber Defense," was organized by the NATO Science and Technology Organization's (STO) Information Systems and Technology (IST) panel and conducted its meetings and electronic exchanges during 2016. This report describes the proceedings and outcomes of the team's efforts.   Many of the defensive activities in the fields of cyber warfare and information assurance rely on essentially ad hoc techniques. The cyber community recognizes that comprehensive, systematic, principle-based modeling and simulation are more likely to produce long-term, lasting, reusable approaches to defensive cyber operations.   A model-driven paradigm is predicated on creation and validation of mechanisms of modeling the organization whose mission is subject to assessment, the mission (or missions) itself, and the cyber-vulnerable systems that support the mission. This by any definition is a complex socio-technical system (of systems), and the level of detail of this class of problems ranges from the level of host and network events to the systems' functions up to the function of the enterprise. Solving this class of problems is of medium to high difficulty and can draw in part on advances in Systems Engineering (SE). Such model-based approaches and analysis could be used to explore multiple alternative mitigation and work-around strategies and to select the optimal course of mitigating actions. Furthermore, the model-driven paradigm applied to cyber operations is likely to benefit traditional disciplines of cyber defense such as security, vulnerability analysis, intrusion prevention, intrusion detection, analysis, forensics, attribution, and recovery.

</details>

<details>

<summary>2017-03-09 17:01:25 - Universal adversarial perturbations</summary>

- *Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard*

- `1610.08401v3` - [abs](http://arxiv.org/abs/1610.08401v3) - [pdf](http://arxiv.org/pdf/1610.08401v3)

> Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.

</details>

<details>

<summary>2017-03-10 16:39:18 - Quantum-Secure Symmetric-Key Cryptography Based on Hidden Shifts</summary>

- *Gorjan Alagic, Alexander Russell*

- `1610.01187v2` - [abs](http://arxiv.org/abs/1610.01187v2) - [pdf](http://arxiv.org/pdf/1610.01187v2)

> Recent results of Kaplan et al., building on previous work by Kuwakado and Morii, have shown that a wide variety of classically-secure symmetric-key cryptosystems can be completely broken by quantum chosen-plaintext attacks (qCPA). In such an attack, the quantum adversary has the ability to query the cryptographic functionality in superposition. The vulnerable cryptosystems include the Even-Mansour block cipher, the three-round Feistel network, the Encrypted-CBC-MAC, and many others. In this work, we study simple algebraic adaptations of such schemes that replace $(\mathbb Z/2)^n$ addition with operations over alternate finite groups--such as $\mathbb Z/{2^n}$--and provide evidence that these adaptations are qCPA-secure. These adaptations furthermore retain the classical security properties (and basic structural features) enjoyed by the original schemes.   We establish security by treating the (quantum) hardness of the well-studied Hidden Shift problem as a basic cryptographic assumption. We observe that this problem has a number of attractive features in this cryptographic context, including random self-reducibility, hardness amplification, and--in many cases of interest--a reduction from the "search version" to the "decisional version." We then establish, under this assumption, the qCPA-security of several such Hidden Shift adaptations of symmetric-key constructions. We show that a Hidden Shift version of the Even-Mansour block cipher yields a quantum-secure pseudorandom function, and that a Hidden Shift version of the Encrypted CBC-MAC yields a collision-resistant hash function. Finally, we observe that such adaptations frustrate the direct Simon's algorithm-based attacks in more general circumstances, e.g., Feistel networks and slide attacks.

</details>

<details>

<summary>2017-03-10 19:53:26 - Ozone: Efficient Execution with Zero Timing Leakage for Modern Microarchitectures</summary>

- *Zelalem Birhanu Aweke, Todd Austin*

- `1703.07706v1` - [abs](http://arxiv.org/abs/1703.07706v1) - [pdf](http://arxiv.org/pdf/1703.07706v1)

> Time variation during program execution can leak sensitive information. Time variations due to program control flow and hardware resource contention have been used to steal encryption keys in cipher implementations such as AES and RSA. A number of approaches to mitigate timing-based side-channel attacks have been proposed including cache partitioning, control-flow obfuscation and injecting timing noise into the outputs of code. While these techniques make timing-based side-channel attacks more difficult, they do not eliminate the risks. Prior techniques are either too specific or too expensive, and all leave remnants of the original timing side channel for later attackers to attempt to exploit.   In this work, we show that the state-of-the-art techniques in timing side-channel protection, which limit timing leakage but do not eliminate it, still have significant vulnerabilities to timing-based side-channel attacks. To provide a means for total protection from timing-based side-channel attacks, we develop Ozone, the first zero timing leakage execution resource for a modern microarchitecture. Code in Ozone execute under a special hardware thread that gains exclusive access to a single core's resources for a fixed (and limited) number of cycles during which it cannot be interrupted. Memory access under Ozone thread execution is limited to a fixed size uncached scratchpad memory, and all Ozone threads begin execution with a known fixed microarchitectural state. We evaluate Ozone using a number of security sensitive kernels that have previously been targets of timing side-channel attacks, and show that Ozone eliminates timing leakage with minimal performance overhead.

</details>

<details>

<summary>2017-03-13 07:38:16 - Security Support in Continuous Deployment Pipeline</summary>

- *Faheem Ullah, Adam Johannes Raft, Mojtaba Shahin, Mansooreh Zahedi, Muhammad Ali Babar*

- `1703.04277v1` - [abs](http://arxiv.org/abs/1703.04277v1) - [pdf](http://arxiv.org/pdf/1703.04277v1)

> Continuous Deployment (CD) has emerged as a new practice in the software industry to continuously and automatically deploy software changes into production. Continuous Deployment Pipeline (CDP) supports CD practice by transferring the changes from the repository to production. Since most of the CDP components run in an environment that has several interfaces to the Internet, these components are vulnerable to various kinds of malicious attacks. This paper reports our work aimed at designing secure CDP by utilizing security tactics. We have demonstrated the effectiveness of five security tactics in designing a secure pipeline by conducting an experiment on two CDPs - one incorporates security tactics while the other does not. Both CDPs have been analyzed qualitatively and quantitatively. We used assurance cases with goal-structured notations for qualitative analysis. For quantitative analysis, we used penetration tools. Our findings indicate that the applied tactics improve the security of the major components (i.e., repository, continuous integration server, main server) of a CDP by controlling access to the components and establishing secure connections.

</details>

<details>

<summary>2017-03-13 10:28:24 - Blocking Transferability of Adversarial Examples in Black-Box Learning Systems</summary>

- *Hossein Hosseini, Yize Chen, Sreeram Kannan, Baosen Zhang, Radha Poovendran*

- `1703.04318v1` - [abs](http://arxiv.org/abs/1703.04318v1) - [pdf](http://arxiv.org/pdf/1703.04318v1)

> Advances in Machine Learning (ML) have led to its adoption as an integral component in many applications, including banking, medical diagnosis, and driverless cars. To further broaden the use of ML models, cloud-based services offered by Microsoft, Amazon, Google, and others have developed ML-as-a-service tools as black-box systems. However, ML classifiers are vulnerable to adversarial examples: inputs that are maliciously modified can cause the classifier to provide adversary-desired outputs. Moreover, it is known that adversarial examples generated on one classifier are likely to cause another classifier to make the same mistake, even if the classifiers have different architectures or are trained on disjoint datasets. This property, which is known as transferability, opens up the possibility of attacking black-box systems by generating adversarial examples on a substitute classifier and transferring the examples to the target classifier. Therefore, the key to protect black-box learning systems against the adversarial examples is to block their transferability. To this end, we propose a training method that, as the input is more perturbed, the classifier smoothly outputs lower confidence on the original label and instead predicts that the input is "invalid". In essence, we augment the output class set with a NULL label and train the classifier to reject the adversarial examples by classifying them as NULL. In experiments, we apply a wide range of attacks based on adversarial examples on the black-box systems. We show that a classifier trained with the proposed method effectively resists against the adversarial examples, while maintaining the accuracy on clean data.

</details>

<details>

<summary>2017-03-16 11:15:28 - Fraternal Twins: Unifying Attacks on Machine Learning and Digital Watermarking</summary>

- *Erwin Quiring, Daniel Arp, Konrad Rieck*

- `1703.05561v1` - [abs](http://arxiv.org/abs/1703.05561v1) - [pdf](http://arxiv.org/pdf/1703.05561v1)

> Machine learning is increasingly used in security-critical applications, such as autonomous driving, face recognition and malware detection. Most learning methods, however, have not been designed with security in mind and thus are vulnerable to different types of attacks. This problem has motivated the research field of adversarial machine learning that is concerned with attacking and defending learning methods. Concurrently, a different line of research has tackled a very similar problem: In digital watermarking information are embedded in a signal in the presence of an adversary. As a consequence, this research field has also extensively studied techniques for attacking and defending watermarking methods.   The two research communities have worked in parallel so far, unnoticeably developing similar attack and defense strategies. This paper is a first effort to bring these communities together. To this end, we present a unified notation of black-box attacks against machine learning and watermarking that reveals the similarity of both settings. To demonstrate the efficacy of this unified view, we apply concepts from watermarking to machine learning and vice versa. We show that countermeasures from watermarking can mitigate recent model-extraction attacks and, similarly, that techniques for hardening machine learning can fend off oracle attacks against watermarks. Our work provides a conceptual link between two research fields and thereby opens novel directions for improving the security of both, machine learning and digital watermarking.

</details>

<details>

<summary>2017-03-17 16:18:31 - Robust Assignments with Vulnerable Nodes</summary>

- *David Adjiashvili, Viktor Bindewald, Dennis Michaels*

- `1703.06074v1` - [abs](http://arxiv.org/abs/1703.06074v1) - [pdf](http://arxiv.org/pdf/1703.06074v1)

> Various real-life planning problems require making upfront decisions before all parameters of the problem have been disclosed. An important special case of such problem especially arises in scheduling and staff rostering problems, where a set of tasks needs to be assigned to an available set of resources (personnel or machines), in a way that each task is assigned to one resource, while no task is allowed to share a resource with another task. In its nominal form, the resulting computational problem reduces to the well-known assignment problem that can be modeled as matching problems on bipartite graphs.   In recent work \cite{adjiashvili_bindewald_michaels_icalp2016}, a new robust model for the assignment problem was introduced that can deal with situations in which certain resources, i.e.\ nodes or edges of the underlying bipartite graph, are vulnerable and may become unavailable after a solution has been chosen. In the original version from \cite{adjiashvili_bindewald_michaels_icalp2016} the resources subject to uncertainty are the edges of the underlying bipartite graph.   In this follow-up work, we complement our previous study by considering nodes as being vulnerable, instead of edges. The goal is now to choose a minimum-cost collection of nodes such that, if any vulnerable node becomes unavailable, the remaining part of the solution still contains sufficient nodes to perform all tasks. From a practical point of view, such type of unavailability is interesting as it is typically caused e.g.\ by an employee's sickness, or machine failure. We present algorithms and hardness of approximation results for several variants of the problem.

</details>

<details>

<summary>2017-03-19 14:50:18 - Practical Black-Box Attacks against Machine Learning</summary>

- *Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami*

- `1602.02697v4` - [abs](http://arxiv.org/abs/1602.02697v4) - [pdf](http://arxiv.org/pdf/1602.02697v4)

> Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.

</details>

<details>

<summary>2017-03-22 13:41:25 - On the Content Security Policy Violations due to the Same-Origin Policy</summary>

- *Dolière Francis Somé, Nataliia Bielova, Tamara Rezk*

- `1611.02875v2` - [abs](http://arxiv.org/abs/1611.02875v2) - [pdf](http://arxiv.org/pdf/1611.02875v2)

> Modern browsers implement different security policies such as the Content Security Policy (CSP), a mechanism designed to mitigate popular web vulnerabilities, and the Same Origin Policy (SOP), a mechanism that governs interactions between resources of web pages. In this work, we describe how CSP may be violated due to the SOP when a page contains an embedded iframe from the same origin. We analyse 1 million pages from 10,000 top Alexa sites and report that at least 31.1% of current CSP-enabled pages are potentially vulnerable to CSP violations. Further considering real-world situations where those pages are involved in same-origin nested browsing contexts, we found that in at least 23.5% of the cases, CSP violations are possible. During our study, we also identified a divergence among browsers implementations in the enforcement of CSP in srcdoc sandboxed iframes, which actually reveals a problem in Gecko-based browsers CSP implementation. To ameliorate the problematic conflicts of the security mechanisms, we discuss measures to avoid CSP violations.

</details>

<details>

<summary>2017-03-22 17:46:16 - Towards Evaluating the Robustness of Neural Networks</summary>

- *Nicholas Carlini, David Wagner*

- `1608.04644v2` - [abs](http://arxiv.org/abs/1608.04644v2) - [pdf](http://arxiv.org/pdf/1608.04644v2)

> Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input $x$ and any target classification $t$, it is possible to find a new input $x'$ that is similar to $x$ but classified as $t$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from $95\%$ to $0.5\%$.   In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with $100\%$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.

</details>

<details>

<summary>2017-03-23 02:40:36 - Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial Domains</summary>

- *Tegjyot Singh Sethi, Mehmed Kantardzic*

- `1703.07909v1` - [abs](http://arxiv.org/abs/1703.07909v1) - [pdf](http://arxiv.org/pdf/1703.07909v1)

> While modern day web applications aim to create impact at the civilization level, they have become vulnerable to adversarial activity, where the next cyber-attack can take any shape and can originate from anywhere. The increasing scale and sophistication of attacks, has prompted the need for a data driven solution, with machine learning forming the core of many cybersecurity systems. Machine learning was not designed with security in mind, and the essential assumption of stationarity, requiring that the training and testing data follow similar distributions, is violated in an adversarial domain. In this paper, an adversary's view point of a classification based system, is presented. Based on a formal adversarial model, the Seed-Explore-Exploit framework is presented, for simulating the generation of data driven and reverse engineering attacks on classifiers. Experimental evaluation, on 10 real world datasets and using the Google Cloud Prediction Platform, demonstrates the innate vulnerability of classifiers and the ease with which evasion can be carried out, without any explicit information about the classifier type, the training data or the application domain. The proposed framework, algorithms and empirical evaluation, serve as a white hat analysis of the vulnerabilities, and aim to foster the development of secure machine learning frameworks.

</details>

<details>

<summary>2017-03-23 05:55:24 - "Influence Sketching": Finding Influential Samples In Large-Scale Regressions</summary>

- *Mike Wojnowicz, Ben Cruz, Xuan Zhao, Brian Wallace, Matt Wolff, Jay Luan, Caleb Crable*

- `1611.05923v3` - [abs](http://arxiv.org/abs/1611.05923v3) - [pdf](http://arxiv.org/pdf/1611.05923v3)

> There is an especially strong need in modern large-scale data analysis to prioritize samples for manual inspection. For example, the inspection could target important mislabeled samples or key vulnerabilities exploitable by an adversarial attack. In order to solve the "needle in the haystack" problem of which samples to inspect, we develop a new scalable version of Cook's distance, a classical statistical technique for identifying samples which unusually strongly impact the fit of a regression model (and its downstream predictions). In order to scale this technique up to very large and high-dimensional datasets, we introduce a new algorithm which we call "influence sketching." Influence sketching embeds random projections within the influence computation; in particular, the influence score is calculated using the randomly projected pseudo-dataset from the post-convergence Generalized Linear Model (GLM). We validate that influence sketching can reliably and successfully discover influential samples by applying the technique to a malware detection dataset of over 2 million executable files, each represented with almost 100,000 features. For example, we find that randomly deleting approximately 10% of training samples reduces predictive accuracy only slightly from 99.47% to 99.45%, whereas deleting the same number of samples with high influence sketch scores reduces predictive accuracy all the way down to 90.24%. Moreover, we find that influential samples are especially likely to be mislabeled. In the case study, we manually inspect the most influential samples, and find that influence sketching pointed us to new, previously unidentified pieces of malware.

</details>

<details>

<summary>2017-03-23 18:03:47 - Early Methods for Detecting Adversarial Images</summary>

- *Dan Hendrycks, Kevin Gimpel*

- `1608.00530v2` - [abs](http://arxiv.org/abs/1608.00530v2) - [pdf](http://arxiv.org/pdf/1608.00530v2)

> Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.

</details>

<details>

<summary>2017-03-24 15:16:26 - Secure Management of Low Power Fitness Trackers</summary>

- *Mahmudur Rahman, Bogdan Carbunar, Umut Topkara*

- `1703.08455v1` - [abs](http://arxiv.org/abs/1703.08455v1) - [pdf](http://arxiv.org/pdf/1703.08455v1)

> The increasing popular interest in personal telemetry, also called the Quantified Self or "lifelogging", has induced a popularity surge for wearable personal fitness trackers. Fitness trackers automatically collect sensor data about the user throughout the day, and integrate it into social network accounts. Solution providers have to strike a balance between many constraints, leading to a design process that often puts security in the back seat. Case in point, we reverse engineered and identified security vulnerabilities in Fitbit Ultra and Gammon Forerunner 610, two popular and representative fitness tracker products. We introduce FitBite and GarMax, tools to launch efficient attacks against Fitbit and Garmin.   We devise SensCrypt, a protocol for secure data storage and communication, for use by makers of affordable and lightweight personal trackers. SensCrypt thwarts not only the attacks we introduced, but also defends against powerful JTAG Read attacks. We have built Sens.io, an Arduino Uno based tracker platform, of similar capabilities but at a fraction of the cost of current solutions. On Sens.io, SensCrypt imposes a negligible write overhead and significantly reduces the end-to-end sync overhead of Fitbit and Garmin.

</details>

<details>

<summary>2017-03-27 10:28:28 - A Study on the Vulnerabilities of Mobile Apps associated with Software Modules</summary>

- *Takuya Watanabe, Mitsuaki Akiyama, Fumihiro Kanei, Eitaro Shioji, Yuta Takata, Bo Sun, Yuta Ishi, Toshiki Shibahara, Takeshi Yagi, Tatsuya Mori*

- `1702.03112v3` - [abs](http://arxiv.org/abs/1702.03112v3) - [pdf](http://arxiv.org/pdf/1702.03112v3)

> This paper reports a large-scale study that aims to understand how mobile application (app) vulnerabilities are associated with software libraries. We analyze both free and paid apps. Studying paid apps was quite meaningful because it helped us understand how differences in app development/maintenance affect the vulnerabilities associated with libraries. We analyzed 30k free and paid apps collected from the official Android marketplace. Our extensive analyses revealed that approximately 70%/50% of vulnerabilities of free/paid apps stem from software libraries, particularly from third-party libraries. Somewhat paradoxically, we found that more expensive/popular paid apps tend to have more vulnerabilities. This comes from the fact that more expensive/popular paid apps tend to have more functionality, i.e., more code and libraries, which increases the probability of vulnerabilities. Based on our findings, we provide suggestions to stakeholders of mobile app distribution ecosystems.

</details>

<details>

<summary>2017-03-28 05:07:38 - Diving Deep into Clickbaits: Who Use Them to What Extents in Which Topics with What Effects?</summary>

- *Md Main Uddin Rony, Naeemul Hassan, Mohammad Yousuf*

- `1703.09400v1` - [abs](http://arxiv.org/abs/1703.09400v1) - [pdf](http://arxiv.org/pdf/1703.09400v1)

> The use of alluring headlines (clickbait) to tempt the readers has become a growing practice nowadays. For the sake of existence in the highly competitive media industry, most of the on-line media including the mainstream ones, have started following this practice. Although the wide-spread practice of clickbait makes the reader's reliability on media vulnerable, a large scale analysis to reveal this fact is still absent. In this paper, we analyze 1.67 million Facebook posts created by 153 media organizations to understand the extent of clickbait practice, its impact and user engagement by using our own developed clickbait detection model. The model uses distributed sub-word embeddings learned from a large corpus. The accuracy of the model is 98.3%. Powered with this model, we further study the distribution of topics in clickbait and non-clickbait contents.

</details>

<details>

<summary>2017-03-28 19:31:08 - AutoLock: Why Cache Attacks on ARM Are Harder Than You Think</summary>

- *Marc Green, Leandro Rodrigues-Lima, Andreas Zankl, Gorka Irazoqui, Johann Heyszl, Thomas Eisenbarth*

- `1703.09763v1` - [abs](http://arxiv.org/abs/1703.09763v1) - [pdf](http://arxiv.org/pdf/1703.09763v1)

> Attacks on the microarchitecture of modern processors have become a practical threat to security and privacy in desktop and cloud computing. Recently, cache attacks have successfully been demonstrated on ARM based mobile devices, suggesting they are as vulnerable as their desktop or server counterparts. In this work, we show that previous literature might have left an overly pessimistic conclusion of ARM's security as we unveil AutoLock: an internal performance enhancement found in inclusive cache levels of ARM processors that adversely affects Evict+Time, Prime+Probe, and Evict+Reload attacks. AutoLock's presence on system-on-chips (SoCs) is not publicly documented, yet knowing that it is implemented is vital to correctly assess the risk of cache attacks. We therefore provide a detailed description of the feature and propose three ways to detect its presence on actual SoCs. We illustrate how AutoLock impedes cross-core cache evictions, but show that its effect can also be compensated in a practical attack. Our findings highlight the intricacies of cache attacks on ARM and suggest that a fair and comprehensive vulnerability assessment requires an in-depth understanding of ARM's cache architectures and rigorous testing across a broad range of ARM based devices.

</details>

<details>

<summary>2017-03-29 00:25:44 - A Generic Cognitive Dimensions Questionnaire to Evaluate the Usability of Security APIs</summary>

- *Chamila Wijayarathna, Nalin A. G. Arachchilage, Jill Slay*

- `1703.09846v1` - [abs](http://arxiv.org/abs/1703.09846v1) - [pdf](http://arxiv.org/pdf/1703.09846v1)

> Programmers use security APIs to embed security into the applications they develop. Security vulnerabilities get introduced into those applications, due to the usability issues that exist in the security APIs. Improving usability of security APIs would contribute to improve the security of applications that programmers develop. However, currently there is no methodology to evaluate the usability of security APIs. In this study, we attempt to improve the Cognitive Dimensions framework based API usability evaluation methodology, to evaluate the usability of security APIs.

</details>

<details>

<summary>2017-03-29 18:17:55 - Logic Locking for Secure Outsourced Chip Fabrication: A New Attack and Provably Secure Defense Mechanism</summary>

- *Mohamed El Massad, Jun Zhang, Siddharth Garg, Mahesh V. Tripunitara*

- `1703.10187v1` - [abs](http://arxiv.org/abs/1703.10187v1) - [pdf](http://arxiv.org/pdf/1703.10187v1)

> Chip designers outsource chip fabrication to external foundries, but at the risk of IP theft. Logic locking, a promising solution to mitigate this threat, adds extra logic gates (key gates) and inputs (key bits) to the chip so that it functions correctly only when the correct key, known only to the designer but not the foundry, is applied. In this paper, we identify a new vulnerability in all existing logic locking schemes. Prior attacks on logic locking have assumed that, in addition to the design of the locked chip, the attacker has access to a working copy of the chip. Our attack does not require a working copy and yet we successfully recover a significant fraction of key bits from the design of the locked chip only. Empirically, we demonstrate the success of our attack on eight large benchmark circuits from a benchmark suite that has been tailored specifically for logic synthesis research, for two different logic locking schemes. Then, to address this vulnerability, we initiate the study of provably secure logic locking mechanisms. We formalize, for the first time to our knowledge, a precise notion of security for logic locking. We establish that any locking procedure that is secure under our definition is guaranteed to counter our desynthesis attack, and all other such known attacks. We then devise a new logic locking procedure, Meerkat, that guarantees that the locked chip reveals no information about the key or the designer's intended functionality. A main insight behind Meerkat is that canonical representations of boolean functionality via Reduced Ordered Binary Decision Diagrams (ROBDDs) can be leveraged effectively to provide security. We analyze Meerkat with regards to its security properties and the overhead it incurs. As such, our work is a contribution to both the foundations and practice of securing digital ICs.

</details>

<details>

<summary>2017-03-29 21:41:32 - Generating Predicate Callback Summaries for the Android Framework</summary>

- *Danilo Dominguez Perez, Wei Le*

- `1703.08902v3` - [abs](http://arxiv.org/abs/1703.08902v3) - [pdf](http://arxiv.org/pdf/1703.08902v3)

> One of the challenges of analyzing, testing and debugging Android apps is that the potential execution orders of callbacks are missing from the apps' source code. However, bugs, vulnerabilities and refactoring transformations have been found to be related to callback sequences. Existing work on control flow analysis of Android apps have mainly focused on analyzing GUI events. GUI events, although being a key part of determining control flow of Android apps, do not offer a complete picture. Our observation is that orthogonal to GUI events, the Android API calls also play an important role in determining the order of callbacks. In the past, such control flow information has been modeled manually. This paper presents a complementary solution of constructing program paths for Android apps. We proposed a specification technique, called Predicate Callback Summary (PCS), that represents the callback control flow information (including callback sequences as well as the conditions under which the callbacks are invoked) in Android API methods and developed static analysis techniques to automatically compute and apply such summaries to construct apps' callback sequences. Our experiments show that by applying PCSs, we are able to construct Android apps' control flow graphs, including inter-callback relations, and also to detect infeasible paths involving multiple callbacks. Such control flow information can help program analysis and testing tools to report more precise results. Our detailed experimental data is available at: http://goo.gl/NBPrKs

</details>

<details>

<summary>2017-03-31 22:17:07 - Membership Inference Attacks against Machine Learning Models</summary>

- *Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov*

- `1610.05820v2` - [abs](http://arxiv.org/abs/1610.05820v2) - [pdf](http://arxiv.org/pdf/1610.05820v2)

> We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on.   We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.

</details>


## 2017-04

<details>

<summary>2017-04-03 19:49:56 - Covert Attacks in Cyber-Physical Control Systems</summary>

- *A. O. Sa, L. F. R. C. Carmo, R. C. S. Machado*

- `1609.09537v3` - [abs](http://arxiv.org/abs/1609.09537v3) - [pdf](http://arxiv.org/pdf/1609.09537v3)

> The advantages of using communication networks to interconnect controllers and physical plants motivate the increasing number of Networked Control Systems, in industrial and critical infrastructure facilities. However, this integration also exposes such control systems to new threats, typical of the cyber domain. In this context, studies have been conduced, aiming to explore vulnerabilities and propose security solutions for cyber-physical systems. In this paper, it is proposed a covert attack for service degradation, which is planned based on the intelligence gathered by another attack, herein proposed, referred as System Identification attack. The simulation results demonstrate that the joint operation of the two attacks is capable to affect, in a covert and accurate way, the physical behavior of a system.

</details>

<details>

<summary>2017-04-04 20:33:04 - Towards an IT Security Risk Assessment Framework for Railway Automation</summary>

- *Jens Braband*

- `1704.01175v1` - [abs](http://arxiv.org/abs/1704.01175v1) - [pdf](http://arxiv.org/pdf/1704.01175v1)

> Some recent incidents have shown that possibly the vulnerability of IT systems in railway automation has been underestimated. Fortunately, so far, almost only denial-of-service attacks were successful, but due to several trends, such as the use of commercial IT and communication systems or privatization, the threat potential could increase in the near future. However, up to now, no harmonized IT security risk assessment framework for railway automation exists. This paper defines an IT security risk assessment framework which aims to separate IT security and safety requirements as well as certification processes as far as possible. It builds on the well-known safety and approval processes from IEC 62425 and integrates IT security requirements based on the ISA99/IEC62443 standard series. While the detailed results are related to railway automation the general concepts are also applicable to other safety-critical application areas.

</details>

<details>

<summary>2017-04-06 02:38:00 - Counter-RAPTOR: Safeguarding Tor Against Active Routing Attacks</summary>

- *Yixin Sun, Anne Edmundson, Nick Feamster, Mung Chiang, Prateek Mittal*

- `1704.00843v2` - [abs](http://arxiv.org/abs/1704.00843v2) - [pdf](http://arxiv.org/pdf/1704.00843v2)

> Tor is vulnerable to network-level adversaries who can observe both ends of the communication to deanonymize users. Recent work has shown that Tor is susceptible to the previously unknown active BGP routing attacks, called RAPTOR attacks, which expose Tor users to more network-level adversaries. In this paper, we aim to mitigate and detect such active routing attacks against Tor. First, we present a new measurement study on the resilience of the Tor network to active BGP prefix attacks. We show that ASes with high Tor bandwidth can be less resilient to attacks than other ASes. Second, we present a new Tor guard relay selection algorithm that incorporates resilience of relays into consideration to proactively mitigate such attacks. We show that the algorithm successfully improves the security for Tor clients by up to 36% on average (up to 166% for certain clients). Finally, we build a live BGP monitoring system that can detect routing anomalies on the Tor network in real time by performing an AS origin check and novel detection analytics. Our monitoring system successfully detects simulated attacks that are modeled after multiple known attack types as well as a real-world hijack attack (performed by us), while having low false positive rates.

</details>

<details>

<summary>2017-04-08 02:34:16 - Towards Attack-Tolerant Networks: Concurrent Multipath Routing and the Butterfly Network</summary>

- *Edward L. Platt, Daniel M. Romero*

- `1704.02426v1` - [abs](http://arxiv.org/abs/1704.02426v1) - [pdf](http://arxiv.org/pdf/1704.02426v1)

> Targeted attacks against network infrastructure are notoriously difficult to guard against. In the case of communication networks, such attacks can leave users vulnerable to censorship and surveillance, even when cryptography is used. Much of the existing work on network fault-tolerance focuses on random faults and does not apply to adversarial faults (attacks). Centralized networks have single points of failure by definition, leading to a growing popularity in decentralized architectures and protocols for greater fault-tolerance. However, centralized network structure can arise even when protocols are decentralized. Despite their decentralized protocols, the Internet and World-Wide Web have been shown both theoretically and historically to be highly susceptible to attack, in part due to emergent structural centralization. When single points of failure exist, they are potentially vulnerable to non-technological (i.e., coercive) attacks, suggesting the importance of a structural approach to attack-tolerance. We show how the assumption of partial trust transitivity, while more realistic than the assumption underlying webs of trust, can be used to quantify the effective redundancy of a network as a function of trust transitivity. We also prove that the effective redundancy of the wrap-around butterfly topology increases exponentially with trust transitivity and describe a novel concurrent multipath routing algorithm for constructing paths to utilize that redundancy. When portions of network structure can be dictated our results can be used to create scalable, attack-tolerant infrastructures. More generally, our results provide a theoretical formalism for evaluating the effects of network structure on adversarial fault-tolerance.

</details>

<details>

<summary>2017-04-10 10:02:37 - Leveraging Flawed Tutorials for Seeding Large-Scale Web Vulnerability Discovery</summary>

- *Tommi Unruh, Bhargava Shastry, Malte Skoruppa, Federico Maggi, Konrad Rieck, Jean-Pierre Seifert, Fabian Yamaguchi*

- `1704.02786v1` - [abs](http://arxiv.org/abs/1704.02786v1) - [pdf](http://arxiv.org/pdf/1704.02786v1)

> The Web is replete with tutorial-style content on how to accomplish programming tasks. Unfortunately, even top-ranked tutorials suffer from severe security vulnerabilities, such as cross-site scripting (XSS), and SQL injection (SQLi). Assuming that these tutorials influence real-world software development, we hypothesize that code snippets from popular tutorials can be used to bootstrap vulnerability discovery at scale. To validate our hypothesis, we propose a semi-automated approach to find recurring vulnerabilities starting from a handful of top-ranked tutorials that contain vulnerable code snippets. We evaluate our approach by performing an analysis of tens of thousands of open-source web applications to check if vulnerabilities originating in the selected tutorials recur. Our analysis framework has been running on a standard PC, analyzed 64,415 PHP codebases hosted on GitHub thus far, and found a total of 117 vulnerabilities that have a strong syntactic similarity to vulnerable code snippets present in popular tutorials. In addition to shedding light on the anecdotal belief that programmers reuse web tutorial code in an ad hoc manner, our study finds disconcerting evidence of insufficiently reviewed tutorials compromising the security of open-source projects. Moreover, our findings testify to the feasibility of large-scale vulnerability discovery using poorly written tutorials as a starting point.

</details>

<details>

<summary>2017-04-10 20:34:47 - Security Analytics of Network Flow Data of IoT and Mobile Devices (Work-in-progress)</summary>

- *Ashish Kundu, Chinmay Kundu, Karan K. Budhraja*

- `1704.03049v1` - [abs](http://arxiv.org/abs/1704.03049v1) - [pdf](http://arxiv.org/pdf/1704.03049v1)

> Given that security threats and privacy breaches are com- monplace today, it is an important problem for one to know whether their device(s) are in a "good state of security", or is there a set of high- risk vulnerabilities that need to be addressed. In this paper, we address this simple yet challenging problem. Instead of gaining white-box access to the device, which offers privacy and other system issues, we rely on network logs and events collected offine as well as in realtime. Our approach is to apply analytics and machine learning for network security analysis as well as analysis of the security of the overall device - apps, the OS and the data on the device. We propose techniques based on analytics in order to determine sensitivity of the device, vulnerability rank of apps and of the device, degree of compromise of apps and of the device, as well as how to define the state of security of the device based on these metrics. Such metrics can be used further in machine learning models in order to predict the users of the device of high risk states, and how to avoid such risks.

</details>

<details>

<summary>2017-04-11 15:29:55 - An Empirical Study on Android-related Vulnerabilities</summary>

- *Mario Linares-Vasquez, Gabriele Bavota, Camilo Escobar-Velasquez*

- `1704.03356v1` - [abs](http://arxiv.org/abs/1704.03356v1) - [pdf](http://arxiv.org/pdf/1704.03356v1)

> Mobile devices are used more and more in everyday life. They are our cameras, wallets, and keys. Basically, they embed most of our private information in our pocket. For this and other reasons, mobile devices, and in particular the software that runs on them, are considered first-class citizens in the software-vulnerabilities landscape. Several studies investigated the software-vulnerabilities phenomenon in the context of mobile apps and, more in general, mobile devices. Most of these studies focused on vulnerabilities that could affect mobile apps, while just few investigated vulnerabilities affecting the underlying platform on which mobile apps run: the Operating System (OS). Also, these studies have been run on a very limited set of vulnerabilities.   In this paper we present the largest study at date investigating Android-related vulnerabilities, with a specific focus on the ones affecting the Android OS. In particular, we (i) define a detailed taxonomy of the types of Android-related vulnerability; (ii) investigate the layers and subsystems from the Android OS affected by vulnerabilities; and (iii) study the survivability of vulnerabilities (i.e., the number of days between the vulnerability introduction and its fixing). Our findings could help OS and apps developers in focusing their verification & validation activities, and researchers in building vulnerability detection tools tailored for the mobile world.

</details>

<details>

<summary>2017-04-12 19:29:06 - Advancing the State-of-the-Art in Hardware Trojans Design</summary>

- *Syed Kamran Haider, Chenglu Jin, Marten van Dijk*

- `1605.08413v2` - [abs](http://arxiv.org/abs/1605.08413v2) - [pdf](http://arxiv.org/pdf/1605.08413v2)

> Electronic Design Automation (EDA) industry heavily reuses third party IP cores. These IP cores are vulnerable to insertion of Hardware Trojans (HTs) at design time by third party IP core providers or by malicious insiders in the design team. State of the art research has shown that existing HT detection techniques, which claim to detect all publicly available HT benchmarks, can still be defeated by carefully designing new sophisticated HTs. The reason being that these techniques consider the HT landscape to be limited only to the publicly known HT benchmarks, or other similar (simple) HTs. However the adversary is not limited to these HTs and may devise new HT design principles to bypass these countermeasures.   In this paper, we discover certain crucial properties of HTs which lead to the definition of an exponentially large class of Deterministic Hardware Trojans $H_D$ that an adversary can (but is not limited to) design. The discovered properties serve as HT design principles, based on which we design a new HT called 'XOR-LFSR' and present it as a 'proof-of-concept' example from the class $H_D$. These design principles help us understand the tremendous ways an adversary has to design a HT, and show that the existing publicly known HT benchmarks are just the tip of the iceberg on this huge landscape. This work, therefore, stresses that instead of guaranteeing a certain (low) false negative rate for a small constant set of publicly known HTs, a rigorous HT detection tool should take into account these newly discovered HT design principles and hence guarantee the detection of an exponentially large class (exponential in number of wires in IP core) of HTs with negligible false negative rate.

</details>

<details>

<summary>2017-04-14 08:34:23 - Improved Cryptanalysis of Rank Metric Schemes Based on Gabidulin Codes</summary>

- *Ayoub Otmani, Hervé Talé Kalachi, Sélestin Ndjeya*

- `1602.08549v2` - [abs](http://arxiv.org/abs/1602.08549v2) - [pdf](http://arxiv.org/pdf/1602.08549v2)

> We prove that any variant of the GPT cryptosystem which uses a right column scrambler over the extension field as advocated by the works of Gabidulin et al. with the goal to resist to Overbeck's structural attack are actually still vulnerable to that attack. We show that by applying the Frobenius operator appropriately on the public key, it is possible to build a Gabidulin code having the same dimension as the original secret Gabidulin code but with a lower length. In particular, the code obtained by this way correct less errors than the secret one but its error correction capabilities are beyond the number of errors added by a sender, and consequently an attacker is able to decrypt any ciphertext with this degraded Gabidulin code. We also considered the case where an isometric transformation is applied in conjunction with a right column scrambler which has its entries in the extension field. We proved that this protection is useless both in terms of performance and security. Consequently, our results show that all the existing techniques aiming to hide the inherent algebraic structure of Gabidulin codes have failed.

</details>

<details>

<summary>2017-04-14 08:44:44 - Polynomial-Time Key Recovery Attack on the Faure-Loidreau Scheme based on Gabidulin Codes</summary>

- *Philippe Gaborit, Ayoub Otmani, Hervé Talé Kalachi*

- `1606.07760v2` - [abs](http://arxiv.org/abs/1606.07760v2) - [pdf](http://arxiv.org/pdf/1606.07760v2)

> Encryption schemes based on the rank metric lead to small public key sizes of order of few thousands bytes which represents a very attractive feature compared to Hamming metric-based encryption schemes where public key sizes are of order of hundreds of thousands bytes even with additional structures like the cyclicity. The main tool for building public key encryption schemes in rank metric is the McEliece encryption setting used with the family of Gabidulin codes. Since the original scheme proposed in 1991 by Gabidulin, Paramonov and Tretjakov, many systems have been proposed based on different masking techniques for Gabidulin codes. Nevertheless, over the years all these systems were attacked essentially by the use of an attack proposed by Overbeck.   In 2005 Faure and Loidreau designed a rank-metric encryption scheme which was not in the McEliece setting. The scheme is very efficient, with small public keys of size a few kiloBytes and with security closely related to the linearized polynomial reconstruction problem which corresponds to the decoding problem of Gabidulin codes. The structure of the scheme differs considerably from the classical McEliece setting and until our work, the scheme had never been attacked. We show in this article that this scheme like other schemes based on Gabidulin codes, is also vulnerable to a polynomial-time attack that recovers the private key by applying Overbeck's attack on an appropriate public code. As an example we break concrete proposed $80$ bits security parameters in a few seconds.

</details>

<details>

<summary>2017-04-14 23:25:33 - Pseudo-Separation for Assessment of Structural Vulnerability of a Network</summary>

- *Alan Kuhnle, Tianyi Pan, Victoria G. Crawford, Md Abdul Alim, My T. Thai*

- `1704.04555v1` - [abs](http://arxiv.org/abs/1704.04555v1) - [pdf](http://arxiv.org/pdf/1704.04555v1)

> Based upon the idea that network functionality is impaired if two nodes in a network are sufficiently separated in terms of a given metric, we introduce two combinatorial \emph{pseudocut} problems generalizing the classical min-cut and multi-cut problems. We expect the pseudocut problems will find broad relevance to the study of network reliability. We comprehensively analyze the computational complexity of the pseudocut problems and provide three approximation algorithms for these problems.   Motivated by applications in communication networks with strict Quality-of-Service (QoS) requirements, we demonstrate the utility of the pseudocut problems by proposing a targeted vulnerability assessment for the structure of communication networks using QoS metrics; we perform experimental evaluations of our proposed approximation algorithms in this context.

</details>

<details>

<summary>2017-04-17 15:39:06 - CUP: Comprehensive User-Space Protection for C/C++</summary>

- *Nathan Burow, Derrick McKee, Scott A. Carr, Mathias Payer*

- `1704.05004v1` - [abs](http://arxiv.org/abs/1704.05004v1) - [pdf](http://arxiv.org/pdf/1704.05004v1)

> Memory corruption vulnerabilities in C/C++ applications enable attackers to execute code, change data, and leak information. Current memory sanitizers do no provide comprehensive coverage of a program's data. In particular, existing tools focus primarily on heap allocations with limited support for stack allocations and globals. Additionally, existing tools focus on the main executable with limited support for system libraries. Further, they suffer from both false positives and false negatives.   We present Comprehensive User-Space Protection for C/C++, CUP, an LLVM sanitizer that provides complete spatial and probabilistic temporal memory safety for C/C++ program on 64-bit architectures (with a prototype implementation for x86_64). CUP uses a hybrid metadata scheme that supports all program data including globals, heap, or stack and maintains the ABI. Compared to existing approaches with the NIST Juliet test suite, CUP reduces false negatives by 10x (0.1%) compared to the state of the art LLVM sanitizers, and produces no false positives. CUP instruments all user-space code, including libc and other system libraries, removing them from the trusted code base.

</details>

<details>

<summary>2017-04-17 21:54:30 - DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples</summary>

- *Ji Gao, Beilun Wang, Zeming Lin, Weilin Xu, Yanjun Qi*

- `1702.06763v8` - [abs](http://arxiv.org/abs/1702.06763v8) - [pdf](http://arxiv.org/pdf/1702.06763v8)

> Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task. To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples.

</details>

<details>

<summary>2017-04-18 08:09:26 - Know Your Master: Driver Profiling-based Anti-theft Method</summary>

- *Byung Il Kwak, JiYoung Woo, Huy Kang Kim*

- `1704.05223v1` - [abs](http://arxiv.org/abs/1704.05223v1) - [pdf](http://arxiv.org/pdf/1704.05223v1)

> Although many anti-theft technologies are implemented, auto-theft is still increasing. Also, security vulnerabilities of cars can be used for auto-theft by neutralizing anti-theft system. This keyless auto-theft attack will be increased as cars adopt computerized electronic devices more. To detect auto-theft efficiently, we propose the driver verification method that analyzes driving patterns using measurements from the sensor in the vehicle. In our model, we add mechanical features of automotive parts that are excluded in previous works, but can be differentiated by drivers' driving behaviors. We design the model that uses significant features through feature selection to reduce the time cost of feature processing and improve the detection performance. Further, we enrich the feature set by deriving statistical features such as mean, median, and standard deviation. This minimizes the effect of fluctuation of feature values per driver and finally generates the reliable model. We also analyze the effect of the size of sliding window on performance to detect the time point when the detection becomes reliable and to inform owners the theft event as soon as possible. We apply our model with real driving and show the contribution of our work to the literature of driver identification.

</details>

<details>

<summary>2017-04-20 00:01:28 - SAFS: A Deep Feature Selection Approach for Precision Medicine</summary>

- *Milad Zafar Nezhad, Dongxiao Zhu, Xiangrui Li, Kai Yang, Phillip Levy*

- `1704.05960v1` - [abs](http://arxiv.org/abs/1704.05960v1) - [pdf](http://arxiv.org/pdf/1704.05960v1)

> In this paper, we propose a new deep feature selection method based on deep architecture. Our method uses stacked auto-encoders for feature representation in higher-level abstraction. We developed and applied a novel feature learning approach to a specific precision medicine problem, which focuses on assessing and prioritizing risk factors for hypertension (HTN) in a vulnerable demographic subgroup (African-American). Our approach is to use deep learning to identify significant risk factors affecting left ventricular mass indexed to body surface area (LVMI) as an indicator of heart damage risk. The results show that our feature learning and representation approach leads to better results in comparison with others.

</details>

<details>

<summary>2017-04-20 15:47:44 - Intrusion Prevention and Detection in Grid Computing - The ALICE Case</summary>

- *Andres Gomez, Camilo Lara, Udo Kebschull*

- `1704.06193v1` - [abs](http://arxiv.org/abs/1704.06193v1) - [pdf](http://arxiv.org/pdf/1704.06193v1)

> Grids allow users flexible on-demand usage of computing resources through remote communication networks. A remarkable example of a Grid in High Energy Physics (HEP) research is used in the ALICE experiment at European Organization for Nuclear Research CERN. Physicists can submit jobs used to process the huge amount of particle collision data produced by the Large Hadron Collider (LHC). Grids face complex security challenges. They are interesting targets for attackers seeking for huge computational resources. Since users can execute arbitrary code in the worker nodes on the Grid sites, special care should be put in this environment. Automatic tools to harden and monitor this scenario are required. Currently, there is no integrated solution for such requirement. This paper describes a new security framework to allow execution of job payloads in a sandboxed context. It also allows process behavior monitoring to detect intrusions, even when new attack methods or zero day vulnerabilities are exploited, by a Machine Learning approach. We plan to implement the proposed framework as a software prototype that will be tested as a component of the ALICE Grid middleware.

</details>

<details>

<summary>2017-04-21 16:52:31 - A Formal Approach to Cyber-Physical Attacks</summary>

- *Ruggero Lanotte, Massimo Merro, Riccardo Muradore, Luca Viganò*

- `1611.01377v2` - [abs](http://arxiv.org/abs/1611.01377v2) - [pdf](http://arxiv.org/pdf/1611.01377v2)

> We apply formal methods to lay and streamline theoretical foundations to reason about Cyber-Physical Systems (CPSs) and cyber-physical attacks. We focus on %a formal treatment of both integrity and DoS attacks to sensors and actuators of CPSs, and on the timing aspects of these attacks. Our contributions are threefold: (1) we define a hybrid process calculus to model both CPSs and cyber-physical attacks; (2) we define a threat model of cyber-physical attacks and provide the means to assess attack tolerance/vulnerability with respect to a given attack; (3) we formalise how to estimate the impact of a successful attack on a CPS and investigate possible quantifications of the success chances of an attack. We illustrate definitions and results by means of a non-trivial engineering application.

</details>

<details>

<summary>2017-04-21 21:48:23 - A dynamic resource allocation decision model for IT security</summary>

- *Lotfi Hajjem, Salah Benabdallah, Fouad Ben Abdelaziz*

- `1704.06713v1` - [abs](http://arxiv.org/abs/1704.06713v1) - [pdf](http://arxiv.org/pdf/1704.06713v1)

> Today, with the continued growth in using information and communication technologies (ICT) for business purposes, business organizations become increasingly dependent on their information systems. Thus, they need to protect them from the different attacks exploiting their vulnerabilities. To do so, the organization has to use security technologies, which may be proactive or reactive ones. Each security technology has a relative cost and addresses specific vulnerabilities. Therefore, the organization has to put in place the appropriate security technologies set that minimizes the information system s vulnerabilities with a minimal cost. This bi objective problem will be considered as a resources allocation problem (RAP) where security technologies represent the resources to be allocated. However, the set of vulnerabilities may change, periodically, with the continual appearance of new ones. Therefore, the security technologies set should be flexible to face these changes, in real time, and the problem becomes a dynamic one. In this paper, we propose a harmony search based algorithm to solve the bi objective dynamic resource allocation decision model. This approach was compared to a genetic algorithm and provided good results.

</details>

<details>

<summary>2017-04-27 01:06:12 - Source File Set Search for Clone-and-Own Reuse Analysis</summary>

- *Takashi Ishio, Yusuke Sakaguchi, Kaoru Ito, Katsuro Inoue*

- `1704.08395v1` - [abs](http://arxiv.org/abs/1704.08395v1) - [pdf](http://arxiv.org/pdf/1704.08395v1)

> Clone-and-own approach is a natural way of source code reuse for software developers. To assess how known bugs and security vulnerabilities of a cloned component affect an application, developers and security analysts need to identify an original version of the component and understand how the cloned component is different from the original one. Although developers may record the original version information in a version control system and/or directory names, such information is often either unavailable or incomplete. In this research, we propose a code search method that takes as input a set of source files and extracts all the components including similar files from a software ecosystem (i.e., a collection of existing versions of software packages). Our method employs an efficient file similarity computation using b-bit minwise hashing technique. We use an aggregated file similarity for ranking components. To evaluate the effectiveness of this tool, we analyzed 75 cloned components in Firefox and Android source code. The tool took about two hours to report the original components from 10 million files in Debian GNU/Linux packages. Recall of the top-five components in the extracted lists is 0.907, while recall of a baseline using SHA-1 file hash is 0.773, according to the ground truth recorded in the source code repositories.

</details>

<details>

<summary>2017-04-27 17:25:30 - Adversary Resistant Deep Neural Networks with an Application to Malware Detection</summary>

- *Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G. Ororbia II, Xinyu Xing, C. Lee Giles, Xue Liu*

- `1610.01239v4` - [abs](http://arxiv.org/abs/1610.01239v4) - [pdf](http://arxiv.org/pdf/1610.01239v4)

> Beyond its highly publicized victories in Go, there have been numerous successful applications of deep learning in information retrieval, computer vision and speech recognition. In cybersecurity, an increasing number of companies have become excited about the potential of deep learning, and have started to use it for various security incidents, the most popular being malware detection. These companies assert that deep learning (DL) could help turn the tide in the battle against malware infections. However, deep neural networks (DNNs) are vulnerable to adversarial samples, a flaw that plagues most if not all statistical learning models. Recent research has demonstrated that those with malicious intent can easily circumvent deep learning-powered malware detection by exploiting this flaw.   In order to address this problem, previous work has developed various defense mechanisms that either augmenting training data or enhance model's complexity. However, after a thorough analysis of the fundamental flaw in DNNs, we discover that the effectiveness of current defenses is limited and, more importantly, cannot provide theoretical guarantees as to their robustness against adversarial sampled-based attacks. As such, we propose a new adversary resistant technique that obstructs attackers from constructing impactful adversarial samples by randomly nullifying features within samples. In this work, we evaluate our proposed technique against a real world dataset with 14,679 malware variants and 17,399 benign programs. We theoretically validate the robustness of our technique, and empirically show that our technique significantly boosts DNN robustness to adversarial samples while maintaining high accuracy in classification. To demonstrate the general applicability of our proposed method, we also conduct experiments using the MNIST and CIFAR-10 datasets, generally used in image recognition research.

</details>

<details>

<summary>2017-04-28 16:29:57 - Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection</summary>

- *Ambra Demontis, Marco Melis, Battista Biggio, Davide Maiorca, Daniel Arp, Konrad Rieck, Igino Corona, Giorgio Giacinto, Fabio Roli*

- `1704.08996v1` - [abs](http://arxiv.org/abs/1704.08996v1) - [pdf](http://arxiv.org/pdf/1704.08996v1)

> To cope with the increasing variability and sophistication of modern attacks, machine learning has been widely adopted as a statistically-sound tool for malware detection. However, its security against well-crafted attacks has not only been recently questioned, but it has been shown that machine learning exhibits inherent vulnerabilities that can be exploited to evade detection at test time. In other words, machine learning itself can be the weakest link in a security system. In this paper, we rely upon a previously-proposed attack framework to categorize potential attack scenarios against learning-based malware detection tools, by modeling attackers with different skills and capabilities. We then define and implement a set of corresponding evasion attacks to thoroughly assess the security of Drebin, an Android malware detector. The main contribution of this work is the proposal of a simple and scalable secure-learning paradigm that mitigates the impact of evasion attacks, while only slightly worsening the detection rate in the absence of attack. We finally argue that our secure-learning approach can also be readily applied to other malware detection tasks.

</details>

<details>

<summary>2017-04-29 20:33:11 - Funnel Libraries for Real-Time Robust Feedback Motion Planning</summary>

- *Anirudha Majumdar, Russ Tedrake*

- `1601.04037v3` - [abs](http://arxiv.org/abs/1601.04037v3) - [pdf](http://arxiv.org/pdf/1601.04037v3)

> We consider the problem of generating motion plans for a robot that are guaranteed to succeed despite uncertainty in the environment, parametric model uncertainty, and disturbances. Furthermore, we consider scenarios where these plans must be generated in real-time, because constraints such as obstacles in the environment may not be known until they are perceived (with a noisy sensor) at runtime. Our approach is to pre-compute a library of "funnels" along different maneuvers of the system that the state is guaranteed to remain within (despite bounded disturbances) when the feedback controller corresponding to the maneuver is executed. We leverage powerful computational machinery from convex optimization (sums-of-squares programming in particular) to compute these funnels. The resulting funnel library is then used to sequentially compose motion plans at runtime while ensuring the safety of the robot. A major advantage of the work presented here is that by explicitly taking into account the effect of uncertainty, the robot can evaluate motion plans based on how vulnerable they are to disturbances.   We demonstrate and validate our method using extensive hardware experiments on a small fixed-wing airplane avoiding obstacles at high speed (~12 mph), along with thorough simulation experiments of ground vehicle and quadrotor models navigating through cluttered environments. To our knowledge, these demonstrations constitute one of the first examples of provably safe and robust control for robotic systems with complex nonlinear dynamics that need to plan in real-time in environments with complex geometric constraints.

</details>


## 2017-05

<details>

<summary>2017-05-05 04:15:46 - Steal Your Life Using 5 Cents: Hacking Android Smartphones with NFC Tags</summary>

- *Carlos Bermejo, Pan Hui*

- `1705.02081v1` - [abs](http://arxiv.org/abs/1705.02081v1) - [pdf](http://arxiv.org/pdf/1705.02081v1)

> Nowadays privacy in the connected world is a big user's concern. The ubiquity of mobile devices permits billions of users browse the web at anytime, anywhere. Near Field Communication (NFC) appeared as a seamlessly and simply communication protocol between devices. Commercial services such as Android Pay, and Apple Pay offer contactless payment methods that are spreading in more and more scenarios. However, we take risks while using NFC on Android devices, we can be hacked, and our privacy can be affected. In this paper we study the current vulnerabilities in the NFC-Android ecosystem. We conduct a series of experiments and we expose that with NFC and Android devices are vulnerable to URL/URI spoofing, Bank/social network information hacking, and user's device tracking via fingerprint and geo-location. It is important for the community to understand the problem and come up solution that can tackle these issues and inform the users about privacy awareness and risks on using these contactless services.

</details>

<details>

<summary>2017-05-08 05:49:21 - A study of cyber security in hospitality industry- threats and countermeasures: case study in Reno, Nevada</summary>

- *Neda Shabani*

- `1705.02749v1` - [abs](http://arxiv.org/abs/1705.02749v1) - [pdf](http://arxiv.org/pdf/1705.02749v1)

> The purpose of this study is to analyze cyber security and security practices of electronic information and network system, network threats, and techniques to prevent the cyber attacks in hotels. Helping the information technology directors and chief information officers (CIO) is the aim of this study to advance policy for security of electronic information in hotels and suggesting some techniques and tools to secure the computer networks. This research is completely qualitative while the case study and interviews have done in 5 random hotels in Reno, Nevada, United States of America. The interview has done with 50 hotel guests, 10 front desk employees, 3 IT manager and 2 assistant of General manager. The results show that hotels' cyber security is very low and hotels are very vulnerable in this regard and at the end, the implications and contribution of the study is mentioned.

</details>

<details>

<summary>2017-05-08 14:55:32 - Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression</summary>

- *Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Li Chen, Michael E. Kounavis, Duen Horng Chau*

- `1705.02900v1` - [abs](http://arxiv.org/abs/1705.02900v1) - [pdf](http://arxiv.org/pdf/1705.02900v1)

> Deep neural networks (DNNs) have achieved great success in solving a variety of machine learning (ML) problems, especially in the domain of image recognition. However, recent research showed that DNNs can be highly vulnerable to adversarially generated instances, which look seemingly normal to human observers, but completely confuse DNNs. These adversarial samples are crafted by adding small perturbations to normal, benign images. Such perturbations, while imperceptible to the human eye, are picked up by DNNs and cause them to misclassify the manipulated instances with high confidence. In this work, we explore and demonstrate how systematic JPEG compression can work as an effective pre-processing step in the classification pipeline to counter adversarial attacks and dramatically reduce their effects (e.g., Fast Gradient Sign Method, DeepFool). An important component of JPEG compression is its ability to remove high frequency signal components, inside square blocks of an image. Such an operation is equivalent to selective blurring of the image, helping remove additive perturbations. Further, we propose an ensemble-based technique that can be constructed quickly from a given well-performing DNN, and empirically show how such an ensemble that leverages JPEG compression can protect a model from multiple types of adversarial attacks, without requiring knowledge about the model.

</details>

<details>

<summary>2017-05-09 20:00:48 - MISRA C, for Security's Sake!</summary>

- *Roberto Bagnara*

- `1705.03517v1` - [abs](http://arxiv.org/abs/1705.03517v1) - [pdf](http://arxiv.org/pdf/1705.03517v1)

> A third of United States new cellular subscriptions in Q1 2016 were for cars. There are now more than 112 million vehicles connected around the world. The percentage of new cars shipped with Internet connectivity is expected to rise from 13% in 2015 to 75% in 2020, and 98% of all vehicles will likely be connected by 2025. Moreover, the news continuously report about "white hat" hackers intruding on car software. For these reasons, security concerns in automotive and other industries have skyrocketed. MISRA C, which is widely respected as a safety-related coding standard, is equally applicable as a security-related coding standard. In this presentation, we will show that security-critical and safety-critical software have the same requirements. We will then introduce the new documents MISRA C:2012 Amendment 1 (Additional security guidelines for MISRA C:2012) and MISRA C:2012 Addendum 2 (Coverage of MISRA C:2012 against ISO/IEC TS 17961:2013 "C Secure Coding Rules"). We will illustrate the relationship between MISRA C, CERT C and ISO/IEC TS 17961, with a particular focus on the objective of preventing security vulnerabilities (and of course safety hazards) as opposed to trying to eradicate them once they have been inserted in the code.

</details>

<details>

<summary>2017-05-10 08:30:34 - A Formal Approach to Exploiting Multi-Stage Attacks based on File-System Vulnerabilities of Web Applications (Extended Version)</summary>

- *Federico De Meo, Luca Viganò*

- `1705.03658v1` - [abs](http://arxiv.org/abs/1705.03658v1) - [pdf](http://arxiv.org/pdf/1705.03658v1)

> Web applications require access to the file-system for many different tasks. When analyzing the security of a web application, secu- rity analysts should thus consider the impact that file-system operations have on the security of the whole application. Moreover, the analysis should take into consideration how file-system vulnerabilities might in- teract with other vulnerabilities leading an attacker to breach into the web application. In this paper, we first propose a classification of file- system vulnerabilities, and then, based on this classification, we present a formal approach that allows one to exploit file-system vulnerabilities. We give a formal representation of web applications, databases and file- systems, and show how to reason about file-system vulnerabilities. We also show how to combine file-system vulnerabilities and SQL-Injection vulnerabilities for the identification of complex, multi-stage attacks. We have developed an automatic tool that implements our approach and we show its efficiency by discussing several real-world case studies, which are witness to the fact that our tool can generate, and exploit, complex attacks that, to the best of our knowledge, no other state-of-the-art-tool for the security of web applications can find.

</details>

<details>

<summary>2017-05-15 14:25:15 - Extending Defensive Distillation</summary>

- *Nicolas Papernot, Patrick McDaniel*

- `1705.05264v1` - [abs](http://arxiv.org/abs/1705.05264v1) - [pdf](http://arxiv.org/pdf/1705.05264v1)

> Machine learning is vulnerable to adversarial examples: inputs carefully modified to force misclassification. Designing defenses against such inputs remains largely an open problem. In this work, we revisit defensive distillation---which is one of the mechanisms proposed to mitigate adversarial examples---to address its limitations. We view our results not only as an effective way of addressing some of the recently discovered attacks but also as reinforcing the importance of improved training techniques.

</details>

<details>

<summary>2017-05-15 14:29:51 - Learning from Clinical Judgments: Semi-Markov-Modulated Marked Hawkes Processes for Risk Prognosis</summary>

- *Ahmed M. Alaa, Scott Hu, Mihaela van der Schaar*

- `1705.05267v1` - [abs](http://arxiv.org/abs/1705.05267v1) - [pdf](http://arxiv.org/pdf/1705.05267v1)

> Critically ill patients in regular wards are vulnerable to unanticipated adverse events which require prompt transfer to the intensive care unit (ICU). To allow for accurate prognosis of deteriorating patients, we develop a novel continuous-time probabilistic model for a monitored patient's temporal sequence of physiological data. Our model captures "informatively sampled" patient episodes: the clinicians' decisions on when to observe a hospitalized patient's vital signs and lab tests over time are represented by a marked Hawkes process, with intensity parameters that are modulated by the patient's latent clinical states, and with observable physiological data (mark process) modeled as a switching multi-task Gaussian process. In addition, our model captures "informatively censored" patient episodes by representing the patient's latent clinical states as an absorbing semi-Markov jump process. The model parameters are learned from offline patient episodes in the electronic health records via an EM-based algorithm. Experiments conducted on a cohort of patients admitted to a major medical center over a 3-year period show that risk prognosis based on our model significantly outperforms the currently deployed medical risk scores and other baseline machine learning algorithms.

</details>

<details>

<summary>2017-05-15 17:33:47 - Software Vulnerability Analysis Using CPE and CVE</summary>

- *Luis Alberto Benthin Sanguino, Rafael Uetz*

- `1705.05347v1` - [abs](http://arxiv.org/abs/1705.05347v1) - [pdf](http://arxiv.org/pdf/1705.05347v1)

> In this paper, we analyze the Common Platform Enumeration (CPE) dictionary and the Common Vulnerabilities and Exposures (CVE) feeds. These repositories are widely used in Vulnerability Management Systems (VMSs) to check for known vulnerabilities in software products. The analysis shows, among other issues, a lack of synchronization between both datasets that can lead to incorrect results output by VMSs relying on those datasets. To deal with these problems, we developed a method that recommends to a user a prioritized list of CPE identifiers for a given software product. The user can then assign (and, if necessary, adapt) the most suitable CPE identifier to the software so that regular (e.g., daily) checks can find known vulnerabilities for this software in the CVE feeds. Our evaluation of this method shows that this interaction is indeed necessary because a fully automated CPE assignment is prone to errors due to the CPE and CVE shortcomings. We implemented an open-source VMS that employs the proposed method and published it on GitHub.

</details>

<details>

<summary>2017-05-18 16:53:36 - Limited-Memory Matrix Adaptation for Large Scale Black-box Optimization</summary>

- *Ilya Loshchilov, Tobias Glasmachers, Hans-Georg Beyer*

- `1705.06693v1` - [abs](http://arxiv.org/abs/1705.06693v1) - [pdf](http://arxiv.org/pdf/1705.06693v1)

> The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a popular method to deal with nonconvex and/or stochastic optimization problems when the gradient information is not available. Being based on the CMA-ES, the recently proposed Matrix Adaptation Evolution Strategy (MA-ES) provides a rather surprising result that the covariance matrix and all associated operations (e.g., potentially unstable eigendecomposition) can be replaced in the CMA-ES by a updated transformation matrix without any loss of performance. In order to further simplify MA-ES and reduce its $\mathcal{O}\big(n^2\big)$ time and storage complexity to $\mathcal{O}\big(n\log(n)\big)$, we present the Limited-Memory Matrix Adaptation Evolution Strategy (LM-MA-ES) for efficient zeroth order large-scale optimization. The algorithm demonstrates state-of-the-art performance on a set of established large-scale benchmarks. We explore the algorithm on the problem of generating adversarial inputs for a (non-smooth) random forest classifier, demonstrating a surprising vulnerability of the classifier.

</details>

<details>

<summary>2017-05-18 17:38:06 - Continuous Implicit Authentication for Mobile Devices based on Adaptive Neuro-Fuzzy Inference System</summary>

- *Feng Yao, Suleiman Y. Yerima, BooJoong Kang, Sakir Sezer*

- `1705.06715v1` - [abs](http://arxiv.org/abs/1705.06715v1) - [pdf](http://arxiv.org/pdf/1705.06715v1)

> As mobile devices have become indispensable in modern life, mobile security is becoming much more important. Traditional password or PIN-like point-of-entry security measures score low on usability and are vulnerable to brute force and other types of attacks. In order to improve mobile security, an adaptive neuro-fuzzy inference system(ANFIS)-based implicit authentication system is proposed in this paper to provide authentication in a continuous and transparent manner.To illustrate the applicability and capability of ANFIS in our implicit authentication system, experiments were conducted on behavioural data collected for up to 12 weeks from different Android users. The ability of the ANFIS-based system to detect an adversary is also tested with scenarios involving an attacker with varying levels of knowledge. The results demonstrate that ANFIS is a feasible and efficient approach for implicit authentication with an average of 95% user recognition rate. Moreover, the use of ANFIS-based system for implicit authentication significantly reduces manual tuning and configuration tasks due to its selflearning capability.

</details>

<details>

<summary>2017-05-18 21:25:51 - A Smart Home is No Castle: Privacy Vulnerabilities of Encrypted IoT Traffic</summary>

- *Noah Apthorpe, Dillon Reisman, Nick Feamster*

- `1705.06805v1` - [abs](http://arxiv.org/abs/1705.06805v1) - [pdf](http://arxiv.org/pdf/1705.06805v1)

> The increasing popularity of specialized Internet-connected devices and appliances, dubbed the Internet-of-Things (IoT), promises both new conveniences and new privacy concerns. Unlike traditional web browsers, many IoT devices have always-on sensors that constantly monitor fine-grained details of users' physical environments and influence the devices' network communications. Passive network observers, such as Internet service providers, could potentially analyze IoT network traffic to infer sensitive details about users. Here, we examine four IoT smart home devices (a Sense sleep monitor, a Nest Cam Indoor security camera, a WeMo switch, and an Amazon Echo) and find that their network traffic rates can reveal potentially sensitive user interactions even when the traffic is encrypted. These results indicate that a technological solution is needed to protect IoT device owner privacy, and that IoT-specific concerns must be considered in the ongoing policy debate around ISP data collection and usage.

</details>

<details>

<summary>2017-05-19 09:30:32 - Optimized Certificate Revocation List Distribution for Secure V2X Communications</summary>

- *Giovanni Rigazzi, Andrea Tassi, Robert J. Piechocki, Theo Tryfonas, Andrew Nix*

- `1705.06903v1` - [abs](http://arxiv.org/abs/1705.06903v1) - [pdf](http://arxiv.org/pdf/1705.06903v1)

> The successful deployment of safe and trustworthy Connected and Autonomous Vehicles (CAVs) will highly depend on the ability to devise robust and effective security solutions to resist sophisticated cyber attacks and patch up critical vulnerabilities. Pseudonym Public Key Infrastructure (PPKI) is a promising approach to secure vehicular networks as well as ensure data and location privacy, concealing the vehicles' real identities. Nevertheless, pseudonym distribution and management affect PPKI scalability due to the significant number of digital certificates required by a single vehicle. In this paper, we focus on the certificate revocation process and propose a versatile and low-complexity framework to facilitate the distribution of the Certificate Revocation Lists (CRL) issued by the Certification Authority (CA). CRL compression is achieved through optimized Bloom filters, which guarantee a considerable overhead reduction with a configurable rate of false positives. Our results show that the distribution of compressed CRLs can significantly enhance the system scalability without increasing the complexity of the revocation process.

</details>

<details>

<summary>2017-05-22 16:14:22 - Tools for improving resilience of electric distribution systems with networked microgrids</summary>

- *Arthur Barnes, Harsha Nagarajan, Emre Yamangil, Russell Bent, Scott Backhaus*

- `1705.08229v1` - [abs](http://arxiv.org/abs/1705.08229v1) - [pdf](http://arxiv.org/pdf/1705.08229v1)

> In the electrical grid, the distribution system is themost vulnerable to severe weather events. Well-placed and coordinatedupgrades, such as the combination of microgrids, systemhardening and additional line redundancy, can greatly reduce thenumber of electrical outages during extreme events. Indeed, ithas been suggested that resilience is one of the primary benefitsof networked microgrids. We formulate a resilient distributiongrid design problem as a two-stage stochastic program andmake use of decomposition-based heuristic algorithms to scaleto problems of practical size. We demonstrate the feasibilityof a resilient distribution design tool on a model of an actualdistribution network. We vary the study parameters, i.e., thecapital cost of microgrid generation relative to system hardeningand target system resilience metrics, and find regions in thisparametric space corresponding to different distribution systemarchitectures, such as individual microgrids, hardened networks,and a transition region that suggests the benefits of microgridsnetworked via hardened circuit segments.

</details>

<details>

<summary>2017-05-23 08:51:37 - Black-Box Attacks against RNN based Malware Detection Algorithms</summary>

- *Weiwei Hu, Ying Tan*

- `1705.08131v1` - [abs](http://arxiv.org/abs/1705.08131v1) - [pdf](http://arxiv.org/pdf/1705.08131v1)

> Recent researches have shown that machine learning based malware detection algorithms are very vulnerable under the attacks of adversarial examples. These works mainly focused on the detection algorithms which use features with fixed dimension, while some researchers have begun to use recurrent neural networks (RNN) to detect malware based on sequential API features. This paper proposes a novel algorithm to generate sequential adversarial examples, which are used to attack a RNN based malware detection system. It is usually hard for malicious attackers to know the exact structures and weights of the victim RNN. A substitute RNN is trained to approximate the victim RNN. Then we propose a generative RNN to output sequential adversarial examples from the original sequential malware inputs. Experimental results showed that RNN based malware detection algorithms fail to detect most of the generated malicious adversarial examples, which means the proposed model is able to effectively bypass the detection algorithms.

</details>

<details>

<summary>2017-05-23 14:58:27 - From Physical to Cyber: Escalating Protection for Personalized Auto Insurance</summary>

- *Le Guan, Jun Xu, Shuai Wang, Xinyu Xing, Lin Lin, Heqing Huang, Peng Liu, Wenke Lee*

- `1609.02234v2` - [abs](http://arxiv.org/abs/1609.02234v2) - [pdf](http://arxiv.org/pdf/1609.02234v2)

> Nowadays, auto insurance companies set personalized insurance rate based on data gathered directly from their customers' cars. In this paper, we show such a personalized insurance mechanism -- wildly adopted by many auto insurance companies -- is vulnerable to exploit. In particular, we demonstrate that an adversary can leverage off-the-shelf hardware to manipulate the data to the device that collects drivers' habits for insurance rate customization and obtain a fraudulent insurance discount. In response to this type of attack, we also propose a defense mechanism that escalates the protection for insurers' data collection. The main idea of this mechanism is to augment the insurer's data collection device with the ability to gather unforgeable data acquired from the physical world, and then leverage these data to identify manipulated data points. Our defense mechanism leveraged a statistical model built on unmanipulated data and is robust to manipulation methods that are not foreseen previously. We have implemented this defense mechanism as a proof-of-concept prototype and tested its effectiveness in the real world. Our evaluation shows that our defense mechanism exhibits a false positive rate of 0.032 and a false negative rate of 0.013.

</details>

<details>

<summary>2017-05-23 18:14:30 - The Space of Transferable Adversarial Examples</summary>

- *Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel*

- `1704.03453v2` - [abs](http://arxiv.org/abs/1704.03453v2) - [pdf](http://arxiv.org/pdf/1704.03453v2)

> Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.   In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability.   In the first quantitative analysis of the similarity of different models' decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks.

</details>

<details>

<summary>2017-05-23 23:51:37 - Towards Interrogating Discriminative Machine Learning Models</summary>

- *Wenbo Guo, Kaixuan Zhang, Lin Lin, Sui Huang, Xinyu Xing*

- `1705.08564v1` - [abs](http://arxiv.org/abs/1705.08564v1) - [pdf](http://arxiv.org/pdf/1705.08564v1)

> It is oftentimes impossible to understand how machine learning models reach a decision. While recent research has proposed various technical approaches to provide some clues as to how a learning model makes individual decisions, they cannot provide users with ability to inspect a learning model as a complete entity. In this work, we propose a new technical approach that augments a Bayesian regression mixture model with multiple elastic nets. Using the enhanced mixture model, we extract explanations for a target model through global approximation. To demonstrate the utility of our approach, we evaluate it on different learning models covering the tasks of text mining and image recognition. Our results indicate that the proposed approach not only outperforms the state-of-the-art technique in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of a learning model.

</details>

<details>

<summary>2017-05-24 16:58:03 - Anti-spoofing Methods for Automatic SpeakerVerification System</summary>

- *Galina Lavrentyeva, Sergey Novoselov, Konstantin Simonchik*

- `1705.08865v1` - [abs](http://arxiv.org/abs/1705.08865v1) - [pdf](http://arxiv.org/pdf/1705.08865v1)

> Growing interest in automatic speaker verification (ASV)systems has lead to significant quality improvement of spoofing attackson them. Many research works confirm that despite the low equal er-ror rate (EER) ASV systems are still vulnerable to spoofing attacks. Inthis work we overview different acoustic feature spaces and classifiersto determine reliable and robust countermeasures against spoofing at-tacks. We compared several spoofing detection systems, presented so far,on the development and evaluation datasets of the Automatic SpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge 2015.Experimental results presented in this paper demonstrate that the useof magnitude and phase information combination provides a substantialinput into the efficiency of the spoofing detection systems. Also wavelet-based features show impressive results in terms of equal error rate. Inour overview we compare spoofing performance for systems based on dif-ferent classifiers. Comparison results demonstrate that the linear SVMclassifier outperforms the conventional GMM approach. However, manyresearchers inspired by the great success of deep neural networks (DNN)approaches in the automatic speech recognition, applied DNN in thespoofing detection task and obtained quite low EER for known and un-known type of spoofing attacks.

</details>

<details>

<summary>2017-05-25 04:32:43 - Adequacy of the Gradient-Descent Method for Classifier Evasion Attacks</summary>

- *Yi Han, Benjamin I. P. Rubinstein*

- `1704.01704v2` - [abs](http://arxiv.org/abs/1704.01704v2) - [pdf](http://arxiv.org/pdf/1704.01704v2)

> Despite the wide use of machine learning in adversarial settings including computer security, recent studies have demonstrated vulnerabilities to evasion attacks---carefully crafted adversarial samples that closely resemble legitimate instances, but cause misclassification. In this paper, we examine the adequacy of the leading approach to generating adversarial samples---the gradient descent approach. In particular (1) we perform extensive experiments on three datasets, MNIST, USPS and Spambase, in order to analyse the effectiveness of the gradient-descent method against non-linear support vector machines, and conclude that carefully reduced kernel smoothness can significantly increase robustness to the attack; (2) we demonstrate that separated inter-class support vectors lead to more secure models, and propose a quantity similar to margin that can efficiently predict potential susceptibility to gradient-descent attacks, before the attack is launched; and (3) we design a new adversarial sample construction algorithm based on optimising the multiplicative ratio of class decision functions.

</details>

<details>

<summary>2017-05-26 12:38:48 - Classification regions of deep neural networks</summary>

- *Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, Stefano Soatto*

- `1705.09552v1` - [abs](http://arxiv.org/abs/1705.09552v1) - [pdf](http://arxiv.org/pdf/1705.09552v1)

> The goal of this paper is to analyze the geometric properties of deep neural network classifiers in the input space. We specifically study the topology of classification regions created by deep networks, as well as their associated decision boundary. Through a systematic empirical investigation, we show that state-of-the-art deep nets learn connected classification regions, and that the decision boundary in the vicinity of datapoints is flat along most directions. We further draw an essential connection between two seemingly unrelated properties of deep networks: their sensitivity to additive perturbations in the inputs, and the curvature of their decision boundary. The directions where the decision boundary is curved in fact remarkably characterize the directions to which the classifier is the most vulnerable. We finally leverage a fundamental asymmetry in the curvature of the decision boundary of deep nets, and propose a method to discriminate between original images, and images perturbed with small adversarial examples. We show the effectiveness of this purely geometric approach for detecting small adversarial perturbations in images, and for recovering the labels of perturbed images.

</details>

<details>

<summary>2017-05-28 12:47:50 - Fast and Accurate Likelihood Ratio Based Biometric Comparison in the Encrypted Domain</summary>

- *Joep Peeters, Andreas Peter, Raymond N. J. Veldhuis*

- `1705.09936v1` - [abs](http://arxiv.org/abs/1705.09936v1) - [pdf](http://arxiv.org/pdf/1705.09936v1)

> As applications of biometric verification proliferate, users become more vulnerable to privacy infringement. Biometric data is very privacy sensitive as it may contain information as gender, ethnicity and health conditions which should not be shared with third parties during the verification process. Moreover, biometric data that has fallen into the wrong hands often leads to identity theft. Secure biometric verification schemes try to overcome such privacy threats. Unfortunately, existing secure solutions either introduce a heavy computational or communication overhead or have to accept a high loss in accuracy; both of which make them impractical in real-world settings. This paper presents a novel approach to secure biometric verification aiming at a practical trade-off between efficiency and accuracy, while guaranteeing full security against honest-but-curious adversaries. The system performs verification in the encrypted domain using elliptic curve based homomorphic ElGamal encryption for high efficiency. Classification is based on a log-likelihood ratio classifier which has proven to be very accurate. No private information is leaked during the verification process using a two-party secure protocol. Initial tests show highly accurate results that have been computed within milliseconds range.

</details>

<details>

<summary>2017-05-29 20:00:09 - A Hazard Analysis Technique for Additive Manufacturing</summary>

- *Gregory Pope, Mark Yampolskiy*

- `1706.00497v1` - [abs](http://arxiv.org/abs/1706.00497v1) - [pdf](http://arxiv.org/pdf/1706.00497v1)

> The promise of Additive Manufacturing (AM) includes reduced transportation and warehousing costs, reduction of source material waste, and reduced environmental impact. AM is extremely useful for making prototypes and has demonstrated the ability to manufacture complex parts not possible (or prohibitively expensive) with conventional machining. Scientists and manufactures are finding increased uses for AM in creation of all types of finished products including those built from polymers, biological material, and metals. Although companies such as GE have been using 3D printing for Additive Manufacturing for over thirty years to make mandrels for light bulb manufacturing, application areas of Additive Manufacturing have increased substantially in recent years, particularly due to the reduction in cost of 3D printers. Like most emergent technologies, there are bound to be growing pains with AM. This paper looks at the software that supports AM and 3D printing and their vulnerability to cyber-attacks, intellectual property theft, defect rates of AM software (which can cause undesired consequences themselves and also create vulnerabilities that a hacker may exploit), part reliability and safety of devices incorporating 3D printed parts (when making mission critical parts), and security/throughput issues of computer networks. Literature searches, consulting with technical experts and a relatively new hazard analysis technique will be used, one especially developed for software intensive systems called Systemic Theoretic Process Analysis (STPA). The purpose of this white paper is to identify risks (or hazards for mission critical parts) for AM in this emergent stage so that mitigations can be applied before accidents occur. A second purpose of this white paper is to evaluate the effectiveness of STPA as a hazard analysis technique in a field that is still relatively new.

</details>

<details>

<summary>2017-05-30 16:57:20 - Lighting Two Candles With One Flame: An Unaided Human Identification Protocol With Security Beyond Conventional Limit</summary>

- *Nilesh Chakraborty, Samrat Mondal*

- `1705.10747v1` - [abs](http://arxiv.org/abs/1705.10747v1) - [pdf](http://arxiv.org/pdf/1705.10747v1)

> Designing an efficient protocol for avoiding the threat of recording based attack in presence of a powerful eavesdropper remains a challenge for more than two decades. During authentication, the absence of any secure link between the prover and verifier makes things even more vulnerable as, after observing a threshold challenge-response pair, users' secret may easily get derived due to information leakage. Existing literature only present new methodologies with ensuring superior aspects over previous ones, while ignoring the aspects on which their proposed schemes cope poorly. Unsurprisingly, most of them are far from satisfactory - either are found far from usable or lack of security features.   To overcome this issue, we first introduce the concept of "leakage control" which puts a bar on the natural information leakage rate and greatly helps in increasing both the usability and security standards. Not just prevention, but also, by introducing the threat detection strategy (based on the concept of honeyword), our scheme "lights two candles". It not only eliminates the long terms security and usability conflict under the practical scenario, but along with threat detection from the client side, it is capable of protecting the secret at the server side under the distributed framework, and thus, guaranteeing security beyond the conventional limit.

</details>


## 2017-06

<details>

<summary>2017-06-01 04:56:21 - PAPS: A Scalable Framework for Prioritization and Partial Selection of Security Requirements</summary>

- *Davoud Mougouei*

- `1706.00166v1` - [abs](http://arxiv.org/abs/1706.00166v1) - [pdf](http://arxiv.org/pdf/1706.00166v1)

> Owing to resource constraints, the existing prioritization and selection techniques for software security requirements (countermeasures) find a subset of higher-priority security requirements ignoring lower-priority requirements or postponing them to the future releases. Ignoring or postponing security requirements however, may on one hand leave some of the security threats (vulnerabilities) unattended and on the other hand influence other security requirements that rely on the ignored or postponed requirements. To address this, we have proposed considering partial satisfaction of security requirements when tolerated rather than ignoring those requirements or postponing them to the future. In doing so, we have contributed a goal-based framework that enables prioritization and partial selection of security requirements with respect to security goals. The proposed framework helps reduce the number of ignored (postponed) security requirements and consequently reduce the adverse impacts of ignoring security requirements in software products.

</details>

<details>

<summary>2017-06-01 08:40:37 - Static Exploration of Taint-Style Vulnerabilities Found by Fuzzing</summary>

- *Bhargava Shastry, Federico Maggi, Fabian Yamaguchi, Konrad Rieck, Jean-Pierre Seifert*

- `1706.00206v1` - [abs](http://arxiv.org/abs/1706.00206v1) - [pdf](http://arxiv.org/pdf/1706.00206v1)

> Taint-style vulnerabilities comprise a majority of fuzzer discovered program faults. These vulnerabilities usually manifest as memory access violations caused by tainted program input. Although fuzzers have helped uncover a majority of taint-style vulnerabilities in software to date, they are limited by (i) extent of test coverage; and (ii) the availability of fuzzable test cases. Therefore, fuzzing alone cannot provide a high assurance that all taint-style vulnerabilities have been uncovered. In this paper, we use static template matching to find recurrences of fuzzer-discovered vulnerabilities. To compensate for the inherent incompleteness of template matching, we implement a simple yet effective match-ranking algorithm that uses test coverage data to focus attention on those matches that comprise untested code. We prototype our approach using the Clang/LLVM compiler toolchain and use it in conjunction with afl-fuzz, a modern coverage-guided fuzzer. Using a case study carried out on the Open vSwitch codebase, we show that our prototype uncovers corner cases in modules that lack a fuzzable test harness. Our work demonstrates that static analysis can effectively complement fuzz testing, and is a useful addition to the security assessment tool-set. Furthermore, our techniques hold promise for increasing the effectiveness of program analysis and testing, and serve as a building block for a hybrid vulnerability discovery framework.

</details>

<details>

<summary>2017-06-02 19:28:19 - CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases</summary>

- *Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Tarek F. Abdelzaher, Jiawei Han*

- `1610.08763v2` - [abs](http://arxiv.org/abs/1610.08763v2) - [pdf](http://arxiv.org/pdf/1610.08763v2)

> Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domain-independent framework, called CoType, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. CoType, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object "translation" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of CoType across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method.

</details>

<details>

<summary>2017-06-09 15:45:23 - Securing Application with Software Partitioning: A case study using SGX</summary>

- *Ahmad Atamli-Reineh, Andrew Martin*

- `1706.03006v1` - [abs](http://arxiv.org/abs/1706.03006v1) - [pdf](http://arxiv.org/pdf/1706.03006v1)

> Application size and complexity are the underlying cause of numerous security vulnerabilities in code. In order to mitigate the risks arising from such vulnerabilities, various techniques have been proposed to isolate the execution of sensitive code from the rest of the application and from other software on the platform (e.g. the operating system). However, even with these partitioning techniques, it is not immediately clear exactly how they can and should be used to partition applications. What overall partitioning scheme should be followed; what granularity of the partitions should be. To some extent, this is dependent on the capabilities and performance of the partitioning technology in use. For this work, we focus on the upcoming Intel Software Guard Extensions (SGX) technology as the state-of-the-art in this field. SGX provides a trusted execution environment, called an enclave, that protects the integrity of the code and the confidentiality of the data inside it from other software, including the operating system. We present a novel framework consisting of four possible schemes under which an application can be partitioned. These schemes range from coarse-grained partitioning, in which the full application is included in a single enclave, through ultra-fine partitioning, in which each application secret is protected in an individual enclave. We explain the specific security benefits provided by each of the partitioning schemes and discuss how the performance of the application would be affected. To compare the different partitioning schemes, we have partitioned OpenSSL using four different schemes. We discuss SGX properties together with the implications of our design choices in this paper.

</details>

<details>

<summary>2017-06-11 07:30:40 - Using Cognitive Dimensions Questionnaire to Evaluate the Usability of Security APIs</summary>

- *Chamila Wijayarathna, Nalin Asanka Gamagedara Arachchilage, Jill Slay*

- `1706.00138v2` - [abs](http://arxiv.org/abs/1706.00138v2) - [pdf](http://arxiv.org/pdf/1706.00138v2)

> Usability issues that exist in security APIs cause programmers to embed those security APIs incorrectly to the applications they develop. This results in introduction of security vulnerabilities to those applications. One of the main reasons for security APIs to be not usable is currently there is no proper method by which the usability issues of security APIs can be identified. We conducted a study to assess the effectiveness of the cognitive dimensions questionnaire based usability evaluation methodology in evaluating the usability of security APIs. We used a cognitive dimensions based generic questionnaire to collect feedback from programmers who participated in the study. Results revealed interesting facts about the prevailing usability issues in four commonly used security APIs and the capability of the methodology to identify those issues.

</details>

<details>

<summary>2017-06-11 21:17:43 - Scenario and Sensitivity Analysis for Flooding Vulnerability using Genetic Algorithms</summary>

- *Vena Pearl Boñgolan, Oreste Terranova, Edward Nataniel Apostol, Joshua Kevin Cruz*

- `1706.03407v1` - [abs](http://arxiv.org/abs/1706.03407v1) - [pdf](http://arxiv.org/pdf/1706.03407v1)

> We try to answer the question: "can we 'modify' our neighborhoods to make them less vulnerable to flooding?" We minimize flooding vulnerability for a city in the central plain of Luzon, by modeling the city as a biological organism with 'traits', and try to 'breed' a 'champion' city (with a low flooding vulnerability) via a genetic algorithm. The result is a description of the traits the barangays (neighborhoods) should have (the 'design' of the city). As far as we can tell, this kind of modeling has not been attempted before. The different components of flooding vulnerability were investigated, and each was given a weight, which allows us to express vulnerability as a weighted sum; this serves as the fitness function for the genetic algorithm. We also allowed non-linear interactions among related but independent components, viz, poverty and mortality rate, and literacy and radio/TV penetration. The two-table system we used to prioritize the components of vulnerability is prone to subjectivity, a common problem in analyses of vulnerability. Thus, a sensitivity analysis was done, which gave a design with a 24% decrease in vulnerability alongside a 14% percent decrease in cost, a significant improvement over this initial scenario analysis, where the proposed design had a 12% decrease in vulnerability with a one percent increase in cost.

</details>

<details>

<summary>2017-06-16 09:05:37 - Intel MPX Explained: An Empirical Study of Intel MPX and Software-based Bounds Checking Approaches</summary>

- *Oleksii Oleksenko, Dmitrii Kuvaiskii, Pramod Bhatotia, Pascal Felber, Christof Fetzer*

- `1702.00719v2` - [abs](http://arxiv.org/abs/1702.00719v2) - [pdf](http://arxiv.org/pdf/1702.00719v2)

> Memory-safety violations are a prevalent cause of both reliability and security vulnerabilities in systems software written in unsafe languages like C/C++. Unfortunately, all the existing software-based solutions to this problem exhibit high performance overheads preventing them from wide adoption in production runs. To address this issue, Intel recently released a new ISA extension - Memory Protection Extensions (Intel MPX), a hardware-assisted full-stack solution to protect against memory safety violations. In this work, we perform an exhaustive study of the Intel MPX architecture to understand its advantages and caveats. We base our study along three dimensions: (a) performance overheads, (b) security guarantees, and (c) usability issues. To put our results in perspective, we compare Intel MPX with three prominent software-based approaches: (1) trip-wire - AddressSanitizer, (2) object-based - SAFECode, and (3) pointer-based - SoftBound. Our main conclusion is that Intel MPX is a promising technique that is not yet practical for widespread adoption. Intel MPX's performance overheads are still high (roughly 50% on average), and the supporting infrastructure has bugs which may cause compilation or runtime errors. Moreover, we showcase the design limitations of Intel MPX: it cannot detect temporal errors, may have false positives and false negatives in multithreaded code, and its restrictions on memory layout require substantial code changes for some programs.

</details>

<details>

<summary>2017-06-19 04:37:15 - Hey, you, keep away from my device: remotely implanting a virus expeller to defeat Mirai on IoT devices</summary>

- *Chen Cao, Le Guan, Peng Liu, Neng Gao, Jingqiang Lin, Ji Xiang*

- `1706.05779v1` - [abs](http://arxiv.org/abs/1706.05779v1) - [pdf](http://arxiv.org/pdf/1706.05779v1)

> Mirai is botnet which targets out-of-date Internet-of-Things (IoT) devices. The disruptive Distributed Denial of Service (DDoS) attack last year has hit major Internet companies, causing intermittent service for millions of Internet users. Since the affected devices typically do not support firmware update, it becomes challenging to expel these vulnerable devices in the wild.   Both industry and academia have made great efforts in amending the situation. However, none of these efforts is simple to deploy, and at the same time effective in solving the problem. In this work, we design a collaborative defense strategy to tackle Mirai. Our key idea is to take advantage of human involvement in the least aggressive way. In particular, at a negotiated time slot, a customer is required to reboot the compromised device, then a "white" Mirai operated by the manufacturer breaks into the clean-state IoT devices immediately. The "white" Mirai expels other malicious Mirai variants, blocks vulnerable ports, and keeps a heart-beat connection with the server operated by the manufacturer. Once the heart-beat is lost, the server re-implants the "white" Mirai instantly. We have implemented a full prototype of the designed system, and the results show that our system can evade Mirai attacks effectively.

</details>

<details>

<summary>2017-06-24 20:59:22 - OS Fingerprinting: New Techniques and a Study of Information Gain and Obfuscation</summary>

- *Blake Anderson, David McGrew*

- `1706.08003v1` - [abs](http://arxiv.org/abs/1706.08003v1) - [pdf](http://arxiv.org/pdf/1706.08003v1)

> Passive operating system fingerprinting reveals valuable information to the defenders of heterogeneous private networks; at the same time, attackers can use fingerprinting to reconnoiter networks, so defenders need obfuscation techniques to foil them. We present an effective approach for passive fingerprinting that uses data features from TLS as well as the TCP/IP and HTTP protocols in a multi-session model, which is applicable whenever several sessions can be observed within a time window. In experiments on a real-world private network, our approach identified operating system major and minor versions with accuracies of 99.4% and 97.5%, respectively, and provided significant information gain. We also show that obfuscation strategies can often be defeated due to the difficulty of manipulating data features from all protocols, especially TLS, by studying how obfuscation affects our fingerprinting system. Because devices running unpatched operating systems on private networks create significant vulnerabilities, their detection is critical; our approach achieved over 98% accuracy at this important goal.

</details>

<details>

<summary>2017-06-25 01:04:29 - Web Vulnerability Scanners: A Case Study</summary>

- *Emre Erturk, Angel Rajan*

- `1706.08017v1` - [abs](http://arxiv.org/abs/1706.08017v1) - [pdf](http://arxiv.org/pdf/1706.08017v1)

> Cloud security is one of the biggest concerns for many companies. The growth in the number and size of websites increases the need for better securing those websites. Manual testing and detection of web vulnerabilities can be very time consuming. Automated Web Vulnerability Scanners (WVS) help with the detection of vulnerabilities in web applications. Acunetix is one of the widely used vulnerability scanners. Acunetix is also easy to implement and to use. The scan results not only provide the details of the vulnerabilities, but also give information about fixing the vulnerabilities. AcuSensor and AcuMonitor (technologies used by Acunetix) help generate more accurate potential vulnerability results. One of the purposes of this paper is to orient current students of computer security with using vulnerability scanners. Secondly, this paper provides a literature review related to the topic of security vulnerability scanners. Finally, web vulnerabilities are addressed from the mobile device and browser perspectives.

</details>

<details>

<summary>2017-06-26 08:25:56 - WebPol: Fine-grained Information Flow Policies for Web Browsers</summary>

- *Abhishek Bichhawat, Vineet Rajani, Jinank Jain, Deepak Garg, Christian Hammer*

- `1706.06932v3` - [abs](http://arxiv.org/abs/1706.06932v3) - [pdf](http://arxiv.org/pdf/1706.06932v3)

> In the standard web browser programming model, third-party scripts included in an application execute with the same privilege as the application's own code. This leaves the application's confidential data vulnerable to theft and leakage by malicious code and inadvertent bugs in the third-party scripts. Security mechanisms in modern browsers (the same-origin policy, cross-origin resource sharing and content security policies) are too coarse to suit this programming model. All these mechanisms (and their extensions) describe whether or not a script can access certain data, whereas the meaningful requirement is to allow untrusted scripts access to confidential data that they need and to prevent the scripts from leaking data on the side. Motivated by this gap, we propose WebPol, a policy mechanism that allows a website developer to include fine-grained policies on confidential application data in the familiar syntax of the JavaScript programming language. The policies can be associated with any webpage element, and specify what aspects of the element can be accessed by which third-party domains. A script can access data that the policy allows it to, but it cannot pass the data (or data derived from it) to other scripts or remote hosts in contravention of the policy. To specify the policies, we expose a small set of new native APIs in JavaScript. Our policies can be enforced using any of the numerous existing proposals for information flow tracking in web browsers. We have integrated our policies into one such proposal that we use to evaluate performance overheads and to test our examples.

</details>

<details>

<summary>2017-06-26 10:11:16 - The Spatial Ecology of War and Peace</summary>

- *Weisi Guo, Xueke Lu, Guillem Mosquera Donate, Samuel Johnson*

- `1604.01693v3` - [abs](http://arxiv.org/abs/1604.01693v3) - [pdf](http://arxiv.org/pdf/1604.01693v3)

> Human flourishing is often severely limited by persistent violence. Quantitative conflict research has found common temporal and other statistical patterns in warfare, but very little is understood about its general spatial patterns. While the importance of topology in geostrategy has long been recognised, the role of spatial patterns of cities in determining a region's vulnerability to conflict has gone unexplored. Here, we show that global patterns in war and peace are closely related to the relative position of cities in a global interaction network. We find that regions with betweenness centrality above a certain threshold are often engulfed in entrenched conflict, while a high degree correlates with peace. In fact, betweenness accounts for over 80% of the variance in number of attacks. This metric is also a good predictor of the distance to a conflict zone and can estimate the risk of conflict. We conjecture that a high betweenness identifies areas with fuzzy cultural boundaries, whereas high degree cities are in cores where peace is more easily maintained. This is supported by a simple agent-based model in which cities influence their neighbours, which exhibits the same threshold behaviour with betweenness as seen in conflict data. These findings not only shed new light on the causes of violence, but could be used to estimate the risk associated with actions such as the merging of cities, construction of transportation infrastructure, or interventions in trade or migration patterns.

</details>

<details>

<summary>2017-06-28 01:29:57 - Antropologia de la Informatica Social: Teoria de la Convergencia Tecno-Social</summary>

- *Rommel Salas*

- `1706.09094v1` - [abs](http://arxiv.org/abs/1706.09094v1) - [pdf](http://arxiv.org/pdf/1706.09094v1)

> The traditional humanism of the twentieth century, inspired by the culture of the book, systematically distanced itself from the new society of digital information; the Internet and tools of information processing revolutionized the world, society during this period developed certain adaptive characteristics based on coexistence (Human - Machine), this transformation sets based on the impact of three technology segments: devices, applications and infrastructure of social communication, which are involved in various physical, behavioural and cognitive changes of the human being; and the emergence of new models of influence and social control through the new ubiquitous communication; however in this new process of conviviality new models like the "collaborative thinking" and "InfoSharing" develop; managing social information under three Human ontological dimensions (h) - Information (i) - Machine (m), which is the basis of a new physical-cyber ecosystem, where they coexist and develop new social units called "virtual communities ". This new communication infrastructure and social management of information given discovered areas of vulnerability "social perspective of risk", impacting all social units through massive impact vector (i); The virtual environment "H + i + M"; and its components, as well as the life cycle management of social information allows us to understand the path of integration "Techno - Social" and setting a new contribution to cybernetics, within the convergence of technology with society and the new challenges of coexistence, aimed at a new holistic and not pragmatic vision, as the human component (h) in the virtual environment is the precursor of the future and needs to be studied not as an application, but as the hub of a new society.

</details>

<details>

<summary>2017-06-28 08:19:43 - Breaking Fitness Records without Moving: Reverse Engineering and Spoofing Fitbit</summary>

- *Hossein Fereidooni, Jiska Classen, Tom Spink, Paul Patras, Markus Miettinen, Ahmad-Reza Sadeghi, Matthias Hollick, Mauro Conti*

- `1706.09165v1` - [abs](http://arxiv.org/abs/1706.09165v1) - [pdf](http://arxiv.org/pdf/1706.09165v1)

> Tens of millions of wearable fitness trackers are shipped yearly to consumers who routinely collect information about their exercising patterns. Smartphones push this health-related data to vendors' cloud platforms, enabling users to analyze summary statistics on-line and adjust their habits. Third-parties including health insurance providers now offer discounts and financial rewards in exchange for such private information and evidence of healthy lifestyles. Given the associated monetary value, the authenticity and correctness of the activity data collected becomes imperative. In this paper, we provide an in-depth security analysis of the operation of fitness trackers commercialized by Fitbit, the wearables market leader. We reveal an intricate security through obscurity approach implemented by the user activity synchronization protocol running on the devices we analyze. Although non-trivial to interpret, we reverse engineer the message semantics, demonstrate how falsified user activity reports can be injected, and argue that based on our discoveries, such attacks can be performed at scale to obtain financial gains. We further document a hardware attack vector that enables circumvention of the end-to-end protocol encryption present in the latest Fitbit firmware, leading to the spoofing of valid encrypted fitness data. Finally, we give guidelines for avoiding similar vulnerabilities in future system designs.

</details>

<details>

<summary>2017-06-28 14:10:21 - Stealthy Deception Attacks Against SCADA Systems</summary>

- *Amit Kleinmann, Ori Amichay, Avishai Wool, David Tenenbaum, Ofer Bar, Leonid Lev*

- `1706.09303v1` - [abs](http://arxiv.org/abs/1706.09303v1) - [pdf](http://arxiv.org/pdf/1706.09303v1)

> SCADA protocols for Industrial Control Systems (ICS) are vulnerable to network attacks such as session hijacking. Hence, research focuses on network anomaly detection based on meta--data (message sizes, timing, command sequence), or on the state values of the physical process. In this work we present a class of semantic network-based attacks against SCADA systems that are undetectable by the above mentioned anomaly detection. After hijacking the communication channels between the Human Machine Interface (HMI) and Programmable Logic Controllers (PLCs), our attacks cause the HMI to present a fake view of the industrial process, deceiving the human operator into taking manual actions. Our most advanced attack also manipulates the messages generated by the operator's actions, reversing their semantic meaning while causing the HMI to present a view that is consistent with the attempted human actions. The attacks are totaly stealthy because the message sizes and timing, the command sequences, and the data values of the ICS's state all remain legitimate.   We implemented and tested several attack scenarios in the test lab of our local electric company, against a real HMI and real PLCs, separated by a commercial-grade firewall. We developed a real-time security assessment tool, that can simultaneously manipulate the communication to multiple PLCs and cause the HMI to display a coherent system--wide fake view. Our tool is configured with message-manipulating rules written in an ICS Attack Markup Language (IAML) we designed, which may be of independent interest. Our semantic attacks all successfully fooled the operator and brought the system to states of blackout and possible equipment damage.

</details>

<details>

<summary>2017-06-28 15:10:50 - Loophole: Timing Attacks on Shared Event Loops in Chrome</summary>

- *Pepe Vila, Boris Köpf*

- `1702.06764v2` - [abs](http://arxiv.org/abs/1702.06764v2) - [pdf](http://arxiv.org/pdf/1702.06764v2)

> Event-driven programming (EDP) is the prevalent paradigm for graphical user interfaces, web clients, and it is rapidly gaining importance for server-side and network programming. Central components of EDP are {\em event loops}, which act as FIFO queues that are used by processes to store and dispatch messages received from other processes.   In this paper we demonstrate that shared event loops are vulnerable to side-channel attacks, where a spy process monitors the loop usage pattern of other processes by enqueueing events and measuring the time it takes for them to be dispatched. Specifically, we exhibit attacks against the two central event loops in Google's Chrome web browser: that of the I/O thread of the host process, which multiplexes all network events and user actions, and that of the main thread of the renderer processes, which handles rendering and Javascript tasks.   For each of these loops, we show how the usage pattern can be monitored with high resolution and low overhead, and how this can be abused for malicious purposes, such as web page identification, user behavior detection, and covert communication.

</details>


## 2017-07

<details>

<summary>2017-07-02 16:44:24 - DeltaPhish: Detecting Phishing Webpages in Compromised Websites</summary>

- *Igino Corona, Battista Biggio, Matteo Contini, Luca Piras, Roberto Corda, Mauro Mereu, Guido Mureddu, Davide Ariu, Fabio Roli*

- `1707.00317v1` - [abs](http://arxiv.org/abs/1707.00317v1) - [pdf](http://arxiv.org/pdf/1707.00317v1)

> The large-scale deployment of modern phishing attacks relies on the automatic exploitation of vulnerable websites in the wild, to maximize profit while hindering attack traceability, detection and blacklisting. To the best of our knowledge, this is the first work that specifically leverages this adversarial behavior for detection purposes. We show that phishing webpages can be accurately detected by highlighting HTML code and visual differences with respect to other (legitimate) pages hosted within a compromised website. Our system, named DeltaPhish, can be installed as part of a web application firewall, to detect the presence of anomalous content on a website after compromise, and eventually prevent access to it. DeltaPhish is also robust against adversarial attempts in which the HTML code of the phishing page is carefully manipulated to evade detection. We empirically evaluate it on more than 5,500 webpages collected in the wild from compromised websites, showing that it is capable of detecting more than 99% of phishing webpages, while only misclassifying less than 1% of legitimate pages. We further show that the detection rate remains higher than 70% even under very sophisticated attacks carefully designed to evade our system.

</details>

<details>

<summary>2017-07-04 01:16:29 - Task Interruptions in Requirements Engineering: Reality versus Perceptions!</summary>

- *Zahra Shakeri Hossein Abad, Guenther Ruhe, Mike Bauer*

- `1707.00794v1` - [abs](http://arxiv.org/abs/1707.00794v1) - [pdf](http://arxiv.org/pdf/1707.00794v1)

> Task switching and interruptions are a daily reality in software development projects: developers switch between Requirements Engineering (RE), coding, testing, daily meetings, and other tasks. Task switching may increase productivity through increased information flow and effective time management. However, it might also cause a cognitive load to reorient the primary task, which accounts for the decrease in developers' productivity and increases in errors. This cognitive load is even greater in cases of cognitively demanding tasks as the ones typical for RE activities. In this paper, to compare the reality of task switching in RE with the perception of developers, we conducted two studies: (i) a case study analysis on 5,076 recorded tasks of 19 developers and (ii) a survey of 25 developers. The results of our retrospective analysis show that in ALL of the cases that the disruptiveness of RE interruptions is statistically different from other software development tasks, RE related tasks are more vulnerable to interruptions compared to other task types. Moreover, we found that context switching, the priority of the interrupting task, and the interruption source and timing are key factors that impact RE interruptions. We also provided a set of RE task switching patterns along with recommendations for both practitioners and researchers. While the results of our retrospective analysis show that self-interruptions are more disruptive than external interruptions, developers have different perceptions about the disruptiveness of various sources of interruptions.

</details>

<details>

<summary>2017-07-05 09:37:00 - SADA: A General Framework to Support Robust Causation Discovery with Theoretical Guarantee</summary>

- *Ruichu Cai, Zhenjie Zhang, Zhifeng Hao*

- `1707.01283v1` - [abs](http://arxiv.org/abs/1707.01283v1) - [pdf](http://arxiv.org/pdf/1707.01283v1)

> Causation discovery without manipulation is considered a crucial problem to a variety of applications. The state-of-the-art solutions are applicable only when large numbers of samples are available or the problem domain is sufficiently small. Motivated by the observations of the local sparsity properties on causal structures, we propose a general Split-and-Merge framework, named SADA, to enhance the scalability of a wide class of causation discovery algorithms. In SADA, the variables are partitioned into subsets, by finding causal cut on the sparse causal structure over the variables. By running mainstream causation discovery algorithms as basic causal solvers on the subproblems, complete causal structure can be reconstructed by combining the partial results. SADA benefits from the recursive division technique, since each small subproblem generates more accurate result under the same number of samples. We theoretically prove that SADA always reduces the scales of problems without sacrifice on accuracy, under the condition of local causal sparsity and reliable conditional independence tests. We also present sufficient condition to accuracy enhancement by SADA, even when the conditional independence tests are vulnerable. Extensive experiments on both simulated and real-world datasets verify the improvements on scalability and accuracy by applying SADA together with existing causation discovery algorithms.

</details>

<details>

<summary>2017-07-10 16:34:20 - Internet of Things: Survey on Security and Privacy</summary>

- *Diego M. Mendez, Ioannis Papapanagiotou, Baijian Yang*

- `1707.01879v2` - [abs](http://arxiv.org/abs/1707.01879v2) - [pdf](http://arxiv.org/pdf/1707.01879v2)

> The Internet of Things (IoT) is intended for ubiquitous connectivity among different entities or "things". While its purpose is to provide effective and efficient solutions, security of the devices and network is a challenging issue. The number of devices connected along with the ad-hoc nature of the system further exacerbates the situation. Therefore, security and privacy has emerged as a significant challenge for the IoT. In this paper,we aim to provide a thorough survey related to the privacy and security challenges of the IoT. This document addresses these challenges from the perspective of technologies and architecture used. This work focuses also in IoT intrinsic vulnerabilities as well as the security challenges of various layers based on the security principles of data confidentiality, integrity and availability. This survey analyzes articles published for the IoT at the time and relates it to the security conjuncture of the field and its projection to the future.

</details>

<details>

<summary>2017-07-14 09:31:26 - Robust Imitation of Diverse Behaviors</summary>

- *Ziyu Wang, Josh Merel, Scott Reed, Greg Wayne, Nando de Freitas, Nicolas Heess*

- `1707.02747v2` - [abs](http://arxiv.org/abs/1707.02747v2) - [pdf](http://arxiv.org/pdf/1707.02747v2)

> Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.

</details>

<details>

<summary>2017-07-15 05:55:44 - Partitioning Patches into Test-equivalence Classes for Scaling Program Repair</summary>

- *Sergey Mechtaev, Xiang Gao, Shin Hwei Tan, Abhik Roychoudhury*

- `1707.03139v2` - [abs](http://arxiv.org/abs/1707.03139v2) - [pdf](http://arxiv.org/pdf/1707.03139v2)

> Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading assignments and patching security vulnerabilities. A common challenge faced by all existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider.   The correctness criterion for program repair is often given as a suite of tests, since a formal specification of the intended program behavior may not be available. Current repair techniques do not scale due to the large number of test executions performed by the underlying search algorithms. We address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are "test-equivalent" for a given test, they produce indistinguishable results on this test). We propose two test-equivalence relations based on runtime values and dependencies respectively and present an algorithm that performs on-the-fly partitioning of patches into test-equivalence classes.   Our experiments on real-world programs reveal that the proposed methodology drastically reduces the number of test executions and therefore provides an order of magnitude efficiency improvement over existing repair techniques, without sacrificing patch quality.

</details>

<details>

<summary>2017-07-17 17:05:43 - On the Pitfalls of End-to-End Encrypted Communications: A Study of Remote Key-Fingerprint Verification</summary>

- *Maliheh Shirvanian, Nitesh Saxena, Jesvin James George*

- `1707.05285v1` - [abs](http://arxiv.org/abs/1707.05285v1) - [pdf](http://arxiv.org/pdf/1707.05285v1)

> Many widely used Internet messaging and calling apps, such as WhatsApp, Viber, Telegram, and Signal, have deployed an end-to-end encryption functionality. To defeat potential MITM attackers against the key exchange protocol, the approach relies on users to perform a code verification task whereby each user must compare the code (a fingerprint of the cryptographic keys) computed by her app with the one computed by the other user's app and reject the session if the two do not match.   In this paper, we study the security and usability of this human-centered code verification task for a setting where the end users are remotely located, and compare it as a baseline to a less frequent scenario where the users are in close proximity. We consider several variations of the code presentation and verification methods, incorporated into representative real-world apps, including codes encoded as numbers or images, displayed on the screen, and verbally spoken by the users. We perform a human factors study in a lab setting to quantify the security and usability of these different methods.   Our study results expose key weaknesses in the security and usability of the code verification methods employed in the apps. First, we show that most code verification methods offer poor security (high false accepts) and low usability (high false rejects and low user experience ratings) in the remote setting. Second, we demonstrate that, security and usability under the remote code verification setting is significantly lower than that in the proximity setting. We attribute this result to the increased cognitive overhead associated with comparing the codes across two apps on the same device (remote setting) rather than across two devices (proximity setting). Overall, our work serves to highlight a serious vulnerability of Internet-based communication apps in the remote setting stemming from human errors.

</details>

<details>

<summary>2017-07-18 05:17:14 - Downgrade Attack on TrustZone</summary>

- *Yue Chen, Yulong Zhang, Zhi Wang, Tao Wei*

- `1707.05082v2` - [abs](http://arxiv.org/abs/1707.05082v2) - [pdf](http://arxiv.org/pdf/1707.05082v2)

> Security-critical tasks require proper isolation from untrusted software. Chip manufacturers design and include trusted execution environments (TEEs) in their processors to secure these tasks. The integrity and security of the software in the trusted environment depend on the verification process of the system.   We find a form of attack that can be performed on the current implementations of the widely deployed ARM TrustZone technology. The attack exploits the fact that the trustlet (TA) or TrustZone OS loading verification procedure may use the same verification key and may lack proper rollback prevention across versions. If an exploit works on an out-of-date version, but the vulnerability is patched on the latest version, an attacker can still use the same exploit to compromise the latest system by downgrading the software to an older and exploitable version.   We did experiments on popular devices on the market including those from Google, Samsung and Huawei, and found that all of them have the risk of being attacked. Also, we show a real-world example to exploit Qualcomm's QSEE.   In addition, in order to find out which device images share the same verification key, pattern matching schemes for different vendors are analyzed and summarized.

</details>

<details>

<summary>2017-07-20 05:31:16 - Google's Cloud Vision API Is Not Robust To Noise</summary>

- *Hossein Hosseini, Baicen Xiao, Radha Poovendran*

- `1704.05051v2` - [abs](http://arxiv.org/abs/1704.05051v2) - [pdf](http://arxiv.org/pdf/1704.05051v2)

> Google has recently introduced the Cloud Vision API for image analysis. According to the demonstration website, the API "quickly classifies images into thousands of categories, detects individual objects and faces within images, and finds and reads printed words contained within images." It can be also used to "detect different types of inappropriate content from adult to violent content."   In this paper, we evaluate the robustness of Google Cloud Vision API to input perturbation. In particular, we show that by adding sufficient noise to the image, the API generates completely different outputs for the noisy image, while a human observer would perceive its original content. We show that the attack is consistently successful, by performing extensive experiments on different image types, including natural images, images containing faces and images with texts. For instance, using images from ImageNet dataset, we found that adding an average of 14.25% impulse noise is enough to deceive the API. Our findings indicate the vulnerability of the API in adversarial environments. For example, an adversary can bypass an image filtering system by adding noise to inappropriate images. We then show that when a noise filter is applied on input images, the API generates mostly the same outputs for restored images as for original images. This observation suggests that cloud vision API can readily benefit from noise filtering, without the need for updating image analysis algorithms.

</details>

<details>

<summary>2017-07-20 09:28:13 - Blockchain Based Intelligent Vehicle Data sharing Framework</summary>

- *Madhusudan Singh, Shiho Kim*

- `1708.09721v1` - [abs](http://arxiv.org/abs/1708.09721v1) - [pdf](http://arxiv.org/pdf/1708.09721v1)

> The Intelligent vehicle (IV) is experiencing revolutionary growth in research and industry, but it still suffers from many security vulnerabilities. Traditional security methods are incapable to provide secure IV data sharing. The major issues in IV data sharing are trust, data accuracy and reliability of data sharing data in the communication channel. Blockchain technology works for the crypto currency, Bit-coin, which is recently used to build trust and reliability in peer-to-peer networks having similar topologies as IV Data sharing. In this paper, we have proposed Intelligent Vehicle data sharing we are proposing a trust environment based Intelligent Vehicle framework. In proposed framework, we have use the blockchain technology as backbone of the IV data-sharing environment. The blockchain technology is provide the trust environment between the vehicles with the based on proof of driving.

</details>

<details>

<summary>2017-07-25 14:40:18 - Predicting Exploitation of Disclosed Software Vulnerabilities Using Open-source Data</summary>

- *Benjamin L. Bullough, Anna K. Yanchenko, Christopher L. Smith, Joseph R. Zipkin*

- `1707.08015v1` - [abs](http://arxiv.org/abs/1707.08015v1) - [pdf](http://arxiv.org/pdf/1707.08015v1)

> Each year, thousands of software vulnerabilities are discovered and reported to the public. Unpatched known vulnerabilities are a significant security risk. It is imperative that software vendors quickly provide patches once vulnerabilities are known and users quickly install those patches as soon as they are available. However, most vulnerabilities are never actually exploited. Since writing, testing, and installing software patches can involve considerable resources, it would be desirable to prioritize the remediation of vulnerabilities that are likely to be exploited. Several published research studies have reported moderate success in applying machine learning techniques to the task of predicting whether a vulnerability will be exploited. These approaches typically use features derived from vulnerability databases (such as the summary text describing the vulnerability) or social media posts that mention the vulnerability by name. However, these prior studies share multiple methodological shortcomings that inflate predictive power of these approaches. We replicate key portions of the prior work, compare their approaches, and show how selection of training and test data critically affect the estimated performance of predictive models. The results of this study point to important methodological considerations that should be taken into account so that results reflect real-world utility.

</details>

<details>

<summary>2017-07-26 17:39:05 - Private Social Network Data Sharing</summary>

- *Jinxue Zhang*

- `1701.01900v4` - [abs](http://arxiv.org/abs/1701.01900v4) - [pdf](http://arxiv.org/pdf/1701.01900v4)

> The increasing popularity of online social network brings huge privacy threat for the end users. While existing work focus on inferring sensitive attributes from the social network such as age, location and gender, little has been done on how to protect the users' privacy by preventing the malicious inference. In this paper we investigated the privacy vulnerability of the existing social network and designed a privacy-preserving framework. We evaluated the framework's privacy and usefulness guarantees, demonstrated its effectiveness on classification and the defense against the privacy attack.

</details>

<details>

<summary>2017-07-28 07:18:51 - Intelligent Vehicle-Trust Point: Reward based Intelligent Vehicle Communication using Blockchain</summary>

- *Madhusudan Singh, Shiho Kim*

- `1707.07442v2` - [abs](http://arxiv.org/abs/1707.07442v2) - [pdf](http://arxiv.org/pdf/1707.07442v2)

> The Intelligent vehicle (IV) is experiencing revolutionary growth in research and industry, but it still suffers from many security vulnerabilities. Traditional security methods are incapable to provide secure IV communication. The major issues in IV communication, are trust, data accuracy and reliability of communication data in the communication channel. Blockchain technology works for the crypto currency, Bit-coin, which is recently used to build trust and reliability in peer-to-peer networks having similar topologies as IV Communication. In this paper, we are proposing, Intelligent Vehicle-Trust Point (IV-TP) mechanism for IV communication among IVs using Blockchain technology. The IVs communicated data provides security and reliability using our proposed IV-TP. Our IV-TP mechanism provides trustworthiness for vehicles behavior, and vehicles legal and illegal action. Our proposal presents a reward based system, an exchange of some IV-TP among IVs, during successful communication. For the data management of the IV-TP, we are using blockchain technology in the intelligent transportation system (ITS), which stores all IV-TP details of every vehicle and is accessed ubiquitously by IVs. In this paper, we evaluate our proposal with the help of intersection use case scenario for intelligent vehicles communication.

</details>

<details>

<summary>2017-07-31 18:55:54 - Universal Adversarial Perturbations Against Semantic Image Segmentation</summary>

- *Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, Volker Fischer*

- `1704.05712v3` - [abs](http://arxiv.org/abs/1704.05712v3) - [pdf](http://arxiv.org/pdf/1704.05712v3)

> While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.

</details>


## 2017-08

<details>

<summary>2017-08-01 14:34:35 - Adversarial-Playground: A Visualization Suite Showing How Adversarial Examples Fool Deep Learning</summary>

- *Andrew P. Norton, Yanjun Qi*

- `1708.00807v1` - [abs](http://arxiv.org/abs/1708.00807v1) - [pdf](http://arxiv.org/pdf/1708.00807v1)

> Recent studies have shown that attackers can force deep learning models to misclassify so-called "adversarial examples": maliciously generated images formed by making imperceptible modifications to pixel values. With growing interest in deep learning for security applications, it is important for security experts and users of machine learning to recognize how learning systems may be attacked. Due to the complex nature of deep learning, it is challenging to understand how deep models can be fooled by adversarial examples. Thus, we present a web-based visualization tool, Adversarial-Playground, to demonstrate the efficacy of common adversarial methods against a convolutional neural network (CNN) system. Adversarial-Playground is educational, modular and interactive. (1) It enables non-experts to compare examples visually and to understand why an adversarial example can fool a CNN-based image classifier. (2) It can help security experts explore more vulnerability of deep learning as a software module. (3) Building an interactive visualization is challenging in this domain due to the large feature space of image classification (generating adversarial examples is slow in general and visualizing images are costly). Through multiple novel design choices, our tool can provide fast and accurate responses to user requests. Empirically, we find that our client-server division strategy reduced the response time by an average of 1.5 seconds per sample. Our other innovation, a faster variant of JSMA evasion algorithm, empirically performed twice as fast as JSMA and yet maintains a comparable evasion rate.   Project source code and data from our experiments available at: https://github.com/QData/AdversarialDNN-Playground

</details>

<details>

<summary>2017-08-01 19:29:43 - Using Transfer Learning for Image-Based Cassava Disease Detection</summary>

- *Amanda Ramcharan, Kelsee Baranowski, Peter McCloskey, Babuali Ahmed, James Legg, David Hughes*

- `1707.03717v2` - [abs](http://arxiv.org/abs/1707.03717v2) - [pdf](http://arxiv.org/pdf/1707.03717v2)

> Cassava is the third largest source of carbohydrates for human food in the world but is vulnerable to virus diseases, which threaten to destabilize food security in sub-Saharan Africa. Novel methods of cassava disease detection are needed to support improved control which will prevent this crisis. Image recognition offers both a cost effective and scalable technology for disease detection. New transfer learning methods offer an avenue for this technology to be easily deployed on mobile devices. Using a dataset of cassava disease images taken in the field in Tanzania, we applied transfer learning to train a deep convolutional neural network to identify three diseases and two types of pest damage (or lack thereof). The best trained model accuracies were 98% for brown leaf spot (BLS), 96% for red mite damage (RMD), 95% for green mite damage (GMD), 98% for cassava brown streak disease (CBSD), and 96% for cassava mosaic disease (CMD). The best model achieved an overall accuracy of 93% for data not used in the training process. Our results show that the transfer learning approach for image recognition of field images offers a fast, affordable, and easily deployable strategy for digital plant disease detection.

</details>

<details>

<summary>2017-08-02 14:14:49 - Fairness-aware machine learning: a perspective</summary>

- *Indre Zliobaite*

- `1708.00754v1` - [abs](http://arxiv.org/abs/1708.00754v1) - [pdf](http://arxiv.org/pdf/1708.00754v1)

> Algorithms learned from data are increasingly used for deciding many aspects in our life: from movies we see, to prices we pay, or medicine we get. Yet there is growing evidence that decision making by inappropriately trained algorithms may unintentionally discriminate people. For example, in automated matching of candidate CVs with job descriptions, algorithms may capture and propagate ethnicity related biases. Several repairs for selected algorithms have already been proposed, but the underlying mechanisms how such discrimination happens from the computational perspective are not yet scientifically understood. We need to develop theoretical understanding how algorithms may become discriminatory, and establish fundamental machine learning principles for prevention. We need to analyze machine learning process as a whole to systematically explain the roots of discrimination occurrence, which will allow to devise global machine learning optimization criteria for guaranteed prevention, as opposed to pushing empirical constraints into existing algorithms case-by-case. As a result, the state-of-the-art will advance from heuristic repairing, to proactive and theoretically supported prevention. This is needed not only because law requires to protect vulnerable people. Penetration of big data initiatives will only increase, and computer science needs to provide solid explanations and accountability to the public, before public concerns lead to unnecessarily restrictive regulations against machine learning.

</details>

<details>

<summary>2017-08-07 15:10:10 - Practical Attacks Against Privacy and Availability in 4G/LTE Mobile Communication Systems</summary>

- *Altaf Shaik, Ravishankar Borgaonkar, N. Asokan, Valtteri Niemi, Jean-Pierre Seifert*

- `1510.07563v3` - [abs](http://arxiv.org/abs/1510.07563v3) - [pdf](http://arxiv.org/pdf/1510.07563v3)

> Mobile communication systems now constitute an essential part of life throughout the world. Fourth generation "Long Term Evolution" (LTE) mobile communication networks are being deployed. The LTE suite of specifications is considered to be significantly better than its predecessors not only in terms of functionality but also with respect to security and privacy for subscribers.   We carefully analyzed LTE access network protocol specifications and uncovered several vulnerabilities. Using commercial LTE mobile devices in real LTE networks, we demonstrate inexpensive, and practical attacks exploiting these vulnerabilities. Our first class of attacks consists of three different ways of making an LTE device leak its location: A semi-passive attacker can locate an LTE device within a 2 sq.km area within a city whereas an active attacker can precisely locate an LTE device using GPS co-ordinates or trilateration via cell-tower signal strength information. Our second class of attacks can persistently deny some or all services to a target LTE device. To the best of our knowledge, our work constitutes the first publicly reported practical attacks against LTE access network protocols.   We present several countermeasures to resist our specific attacks. We also discuss possible trade-offs that may explain why these vulnerabilities exist and recommend that safety margins introduced into future specifications to address such trade-offs should incorporate greater agility to accommodate subsequent changes in the trade-off equilibrium.

</details>

<details>

<summary>2017-08-08 04:38:17 - Automatic feature learning for vulnerability prediction</summary>

- *Hoa Khanh Dam, Truyen Tran, Trang Pham, Shien Wee Ng, John Grundy, Aditya Ghose*

- `1708.02368v1` - [abs](http://arxiv.org/abs/1708.02368v1) - [pdf](http://arxiv.org/pdf/1708.02368v1)

> Code flaws or vulnerabilities are prevalent in software systems and can potentially cause a variety of problems including deadlock, information loss, or system failure. A variety of approaches have been developed to try and detect the most likely locations of such code vulnerabilities in large code bases. Most of them rely on manually designing features (e.g. complexity metrics or frequencies of code tokens) that represent the characteristics of the code. However, all suffer from challenges in sufficiently capturing both semantic and syntactic representation of source code, an important capability for building accurate prediction models. In this paper, we describe a new approach, built upon the powerful deep learning Long Short Term Memory model, to automatically learn both semantic and syntactic features in code. Our evaluation on 18 Android applications demonstrates that the prediction power obtained from our learned features is equal or even superior to what is achieved by state of the art vulnerability prediction models: 3%--58% improvement for within-project prediction and 85% for cross-project prediction.

</details>

<details>

<summary>2017-08-08 06:29:02 - Ghera: A Repository of Android App Vulnerability Benchmarks</summary>

- *Joydeep Mitra, Venkatesh-Prasad Ranganath*

- `1708.02380v1` - [abs](http://arxiv.org/abs/1708.02380v1) - [pdf](http://arxiv.org/pdf/1708.02380v1)

> Security of mobile apps affects the security of their users. This has fueled the development of techniques to automatically detect vulnerabilities in mobile apps and help developers secure their apps; specifically, in the context of Android platform due to openness and ubiquitousness of the platform. Despite a slew of research efforts in this space, there is no comprehensive repository of up-to-date and lean benchmarks that contain most of the known Android app vulnerabilities and, consequently, can be used to rigorously evaluate both existing and new vulnerability detection techniques and help developers learn about Android app vulnerabilities. In this paper, we describe Ghera, an open source repository of benchmarks that capture 25 known vulnerabilities in Android apps (as pairs of exploited/benign and exploiting/malicious apps). We also present desirable characteristics of vulnerability benchmarks and repositories that we uncovered while creating Ghera.

</details>

<details>

<summary>2017-08-09 08:04:50 - Rise of the HaCRS: Augmenting Autonomous Cyber Reasoning Systems with Human Assistance</summary>

- *Yan Shoshitaishvili, Michael Weissbacher, Lukas Dresel, Christopher Salls, Ruoyu Wang, Christopher Kruegel, Giovanni Vigna*

- `1708.02749v1` - [abs](http://arxiv.org/abs/1708.02749v1) - [pdf](http://arxiv.org/pdf/1708.02749v1)

> As the size and complexity of software systems increase, the number and sophistication of software security flaws increase as well. The analysis of these flaws began as a manual approach, but it soon became apparent that tools were necessary to assist human experts in this task, resulting in a number of techniques and approaches that automated aspects of the vulnerability analysis process.   Recently, DARPA carried out the Cyber Grand Challenge, a competition among autonomous vulnerability analysis systems designed to push the tool-assisted human-centered paradigm into the territory of complete automation. However, when the autonomous systems were pitted against human experts it became clear that certain tasks, albeit simple, could not be carried out by an autonomous system, as they require an understanding of the logic of the application under analysis.   Based on this observation, we propose a shift in the vulnerability analysis paradigm, from tool-assisted human-centered to human-assisted tool-centered. In this paradigm, the automated system orchestrates the vulnerability analysis process, and leverages humans (with different levels of expertise) to perform well-defined sub-tasks, whose results are integrated in the analysis. As a result, it is possible to scale the analysis to a larger number of programs, and, at the same time, optimize the use of expensive human resources.   In this paper, we detail our design for a human-assisted automated vulnerability analysis system, describe its implementation atop an open-sourced autonomous vulnerability analysis system that participated in the Cyber Grand Challenge, and evaluate and discuss the significant improvements that non-expert human assistance can offer to automated analysis approaches.

</details>

<details>

<summary>2017-08-11 17:34:23 - Systematic Testing of Convolutional Neural Networks for Autonomous Driving</summary>

- *Tommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-Vincentelli, Sanjit A. Seshia*

- `1708.03309v2` - [abs](http://arxiv.org/abs/1708.03309v2) - [pdf](http://arxiv.org/pdf/1708.03309v2)

> We present a framework to systematically analyze convolutional neural networks (CNNs) used in classification of cars in autonomous vehicles. Our analysis procedure comprises an image generator that produces synthetic pictures by sampling in a lower dimension image modification subspace and a suite of visualization tools. The image generator produces images which can be used to test the CNN and hence expose its vulnerabilities. The presented framework can be used to extract insights of the CNN classifier, compare across classification models, or generate training and validation datasets.

</details>

<details>

<summary>2017-08-14 18:25:34 - Attributing Hacks</summary>

- *Ziqi Liu, Alexander J. Smola, Kyle Soska, Yu-Xiang Wang, Qinghua Zheng, Jun Zhou*

- `1611.03021v2` - [abs](http://arxiv.org/abs/1611.03021v2) - [pdf](http://arxiv.org/pdf/1611.03021v2)

> In this paper we describe an algorithm for estimating the provenance of hacks on websites. That is, given properties of sites and the temporal occurrence of attacks, we are able to attribute individual attacks to joint causes and vulnerabilities, as well as estimating the evolution of these vulnerabilities over time. Specifically, we use hazard regression with a time-varying additive hazard function parameterized in a generalized linear form. The activation coefficients on each feature are continuous-time functions over time. We formulate the problem of learning these functions as a constrained variational maximum likelihood estimation problem with total variation penalty and show that the optimal solution is a 0th order spline (a piecewise constant function) with a finite number of known knots. This allows the inference problem to be solved efficiently and at scale by solving a finite dimensional optimization problem. Extensive experiments on real data sets show that our method significantly outperforms Cox's proportional hazard model. We also conduct a case study and verify that the fitted functions are indeed recovering vulnerable features and real-life events such as the release of code to exploit these features in hacker blogs.

</details>

<details>

<summary>2017-08-14 23:47:02 - Graph Classification via Deep Learning with Virtual Nodes</summary>

- *Trang Pham, Truyen Tran, Hoa Dam, Svetha Venkatesh*

- `1708.04357v1` - [abs](http://arxiv.org/abs/1708.04357v1) - [pdf](http://arxiv.org/pdf/1708.04357v1)

> Learning representation for graph classification turns a variable-size graph into a fixed-size vector (or matrix). Such a representation works nicely with algebraic manipulations. Here we introduce a simple method to augment an attributed graph with a virtual node that is bidirectionally connected to all existing nodes. The virtual node represents the latent aspects of the graph, which are not immediately available from the attributes and local connectivity structures. The expanded graph is then put through any node representation method. The representation of the virtual node is then the representation of the entire graph. In this paper, we use the recently introduced Column Network for the expanded graph, resulting in a new end-to-end graph classification model dubbed Virtual Column Network (VCN). The model is validated on two tasks: (i) predicting bio-activity of chemical compounds, and (ii) finding software vulnerability from source code. Results demonstrate that VCN is competitive against well-established rivals.

</details>

<details>

<summary>2017-08-15 15:25:16 - Resilient Linear Classification: An Approach to Deal with Attacks on Training Data</summary>

- *Sangdon Park, James Weimer, Insup Lee*

- `1708.03366v2` - [abs](http://arxiv.org/abs/1708.03366v2) - [pdf](http://arxiv.org/pdf/1708.03366v2)

> Data-driven techniques are used in cyber-physical systems (CPS) for controlling autonomous vehicles, handling demand responses for energy management, and modeling human physiology for medical devices. These data-driven techniques extract models from training data, where their performance is often analyzed with respect to random errors in the training data. However, if the training data is maliciously altered by attackers, the effect of these attacks on the learning algorithms underpinning data-driven CPS have yet to be considered. In this paper, we analyze the resilience of classification algorithms to training data attacks. Specifically, a generic metric is proposed that is tailored to measure resilience of classification algorithms with respect to worst-case tampering of the training data. Using the metric, we show that traditional linear classification algorithms are resilient under restricted conditions. To overcome these limitations, we propose a linear classification algorithm with a majority constraint and prove that it is strictly more resilient than the traditional algorithms. Evaluations on both synthetic data and a real-world retrospective arrhythmia medical case-study show that the traditional algorithms are vulnerable to tampered training data, whereas the proposed algorithm is more resilient (as measured by worst-case tampering).

</details>

<details>

<summary>2017-08-16 08:15:28 - Design-Time Quantification of Integrity in Cyber-Physical-Systems</summary>

- *Eric Rothstein Morris, Carlos G. Murguia, Martín Ochoa*

- `1708.04798v1` - [abs](http://arxiv.org/abs/1708.04798v1) - [pdf](http://arxiv.org/pdf/1708.04798v1)

> In a software system it is possible to quantify the amount of information that is leaked or corrupted by analysing the flows of information present in the source code. In a cyber-physical system, information flows are not only present at the digital level, but also at a physical level, and to and fro the two levels. In this work, we provide a methodology to formally analyse a Cyber-Physical System composite model (combining physics and control) using an information flow-theoretic approach. We use this approach to quantify the level of vulnerability of a system with respect to attackers with different capabilities. We illustrate our approach by means of a water distribution case study.

</details>

<details>

<summary>2017-08-22 13:32:17 - Given Enough Eyeballs, All Bugs Are Shallow? Revisiting Eric Raymond with Bug Bounty Programs</summary>

- *Thomas Maillart, Mingyi Zhao, Jens Grossklags, John Chuang*

- `1608.03445v2` - [abs](http://arxiv.org/abs/1608.03445v2) - [pdf](http://arxiv.org/pdf/1608.03445v2)

> Bug bounty programs offer a modern platform for organizations to crowdsource their software security and for security researchers to be fairly rewarded for the vulnerabilities they find. Little is known however on the incentives set by bug bounty programs: How they drive new bug discoveries, and how they supposedly improve security through the progressive exhaustion of discoverable vulnerabilities. Here, we recognize that bug bounty programs create tensions, for organizations running them on the one hand, and for security researchers on the other hand. At the level of one bug bounty program, security researchers face a sort of St-Petersburg paradox: The probability of finding additional bugs decays fast, and thus can hardly be matched with a sufficient increase of monetary rewards. Furthermore, bug bounty program managers have an incentive to gather the largest possible crowd to ensure a larger pool of expertise, which in turn increases competition among security researchers. As a result, we find that researchers have high incentives to switch to newly launched programs, for which a reserve of low-hanging fruit vulnerabilities is still available. Our results inform on the technical and economic mechanisms underlying the dynamics of bug bounty program contributions, and may in turn help improve the mechanism design of bug bounty programs that get increasingly adopted by cybersecurity savvy organizations.

</details>

<details>

<summary>2017-08-22 16:08:18 - Herding Vulnerable Cats: A Statistical Approach to Disentangle Joint Responsibility for Web Security in Shared Hosting</summary>

- *Samaneh Tajalizadehkhoob, Tom van Goethem, Maciej Korczyński, Arman Noroozian, Rainer Böhme, Tyler Moore, Wouter Joosen, Michel van Eeten*

- `1708.06693v1` - [abs](http://arxiv.org/abs/1708.06693v1) - [pdf](http://arxiv.org/pdf/1708.06693v1)

> Hosting providers play a key role in fighting web compromise, but their ability to prevent abuse is constrained by the security practices of their own customers. {\em Shared} hosting, offers a unique perspective since customers operate under restricted privileges and providers retain more control over configurations. We present the first empirical analysis of the distribution of web security features and software patching practices in shared hosting providers, the influence of providers on these security practices, and their impact on web compromise rates. We construct provider-level features on the global market for shared hosting -- containing 1,259 providers -- by gathering indicators from 442,684 domains. Exploratory factor analysis of 15 indicators identifies four main latent factors that capture security efforts: content security, webmaster security, web infrastructure security and web application security. We confirm, via a fixed-effect regression model, that providers exert significant influence over the latter two factors, which are both related to the software stack in their hosting environment. Finally, by means of GLM regression analysis of these factors on phishing and malware abuse, we show that the four security and software patching factors explain between 10\% and 19\% of the variance in abuse at providers, after controlling for size. For web-application security for instance, we found that when a provider moves from the bottom 10\% to the best-performing 10\%, it would experience 4 times fewer phishing incidents. We show that providers have influence over patch levels--even higher in the stack, where CMSes can run as client-side software--and that this influence is tied to a substantial reduction in abuse levels.

</details>

<details>

<summary>2017-08-23 09:12:31 - Evading Classifiers by Morphing in the Dark</summary>

- *Hung Dang, Yue Huang, Ee-Chien Chang*

- `1705.07535v3` - [abs](http://arxiv.org/abs/1705.07535v3) - [pdf](http://arxiv.org/pdf/1705.07535v3)

> Learning-based systems have been shown to be vulnerable to evasion through adversarial data manipulation. These attacks have been studied under assumptions that the adversary has certain knowledge of either the target model internals, its training dataset or at least classification scores it assigns to input samples. In this paper, we investigate a much more constrained and realistic attack scenario wherein the target classifier is minimally exposed to the adversary, revealing on its final classification decision (e.g., reject or accept an input sample). Moreover, the adversary can only manipulate malicious samples using a blackbox morpher. That is, the adversary has to evade the target classifier by morphing malicious samples "in the dark". We present a scoring mechanism that can assign a real-value score which reflects evasion progress to each sample based on the limited information available. Leveraging on such scoring mechanism, we propose an evasion method -- EvadeHC -- and evaluate it against two PDF malware detectors, namely PDFRate and Hidost. The experimental evaluation demonstrates that the proposed evasion attacks are effective, attaining $100\%$ evasion rate on the evaluation dataset. Interestingly, EvadeHC outperforms the known classifier evasion technique that operates based on classification scores output by the classifiers. Although our evaluations are conducted on PDF malware classifier, the proposed approaches are domain-agnostic and is of wider application to other learning-based systems.

</details>

<details>

<summary>2017-08-23 10:01:35 - Is Deep Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid</summary>

- *Marco Melis, Ambra Demontis, Battista Biggio, Gavin Brown, Giorgio Fumera, Fabio Roli*

- `1708.06939v1` - [abs](http://arxiv.org/abs/1708.06939v1) - [pdf](http://arxiv.org/pdf/1708.06939v1)

> Deep neural networks have been widely adopted in recent years, exhibiting impressive performances in several application domains. It has however been shown that they can be fooled by adversarial examples, i.e., images altered by a barely-perceivable adversarial noise, carefully crafted to mislead classification. In this work, we aim to evaluate the extent to which robot-vision systems embodying deep-learning algorithms are vulnerable to adversarial examples, and propose a computationally efficient countermeasure to mitigate this threat, based on rejecting classification of anomalous inputs. We then provide a clearer understanding of the safety properties of deep networks through an intuitive empirical analysis, showing that the mapping learned by such networks essentially violates the smoothness assumption of learning algorithms. We finally discuss the main limitations of this work, including the creation of real-world adversarial examples, and sketch promising research directions.

</details>

<details>

<summary>2017-08-28 10:23:29 - Megatrend and Intervention Impact Analyser for Jobs: A European Big Data Hackathon Entry</summary>

- *Rain Opik, Toomas Kirt, Innar Liiv*

- `1708.08262v1` - [abs](http://arxiv.org/abs/1708.08262v1) - [pdf](http://arxiv.org/pdf/1708.08262v1)

> This paper presents results of a European Big Data Hackathon entry to facilitate data analysis and visualization of patterns in provided and external datasets about European jobs and skills mismatch and labour market in general. The main contributions of this work are: the development of a method to represent the complex labour market internal structure from the perspective of occupations sharing skills; developing and presenting the prototype, together with an extended description of constructing a graph and related necessary data processing. Since labour market is not an isolated phenomenon and is constantly impacted with external trends and interventions, the presented tool is designed to enable adding extra layers of external information: what is the impact of a megatrend or an intervention to the labour market? Which parts of labour market of what country is most vulnerable to approaching megatrend or planned intervention? A case of analysing the labour market together with the megatrend of automation and computerization of jobs is presented. The source code of the prototype is released as open source for better repeat-ability.

</details>

<details>

<summary>2017-08-28 14:13:20 - Co-simulation for Cyber Security Analysis: Data Attacks against Energy Management System</summary>

- *Kaikai Pan, André Teixeira, Claudio López, Peter Palensky*

- `1708.08322v1` - [abs](http://arxiv.org/abs/1708.08322v1) - [pdf](http://arxiv.org/pdf/1708.08322v1)

> It is challenging to assess the vulnerability of a cyber-physical power system to data attacks from an integral perspective. In order to support vulnerability assessment except analytic analysis, suitable platform for security tests needs to be developed. In this paper we analyze the cyber security of energy management system (EMS) against data attacks. First we extend our analytic framework that characterizes data attacks as optimization problems with the objectives specified as security metrics and constraints corresponding to the communication network properties. Second, we build a platform in the form of co-simulation - coupling the power system simulator DIgSILENT PowerFactory with communication network simulator OMNeT++, and Matlab for EMS applications (state estimation, optimal power flow). Then the framework is used to conduct attack simulations on the co-simulation based platform for a power grid test case. The results indicate how vulnerable of EMS to data attacks and how co-simulation can help assess vulnerability.

</details>

<details>

<summary>2017-08-28 14:43:14 - Cyber Risk Analysis of Combined Data Attacks Against Power System State Estimation</summary>

- *Kaikai Pan, André Teixeira, Milos Cvetkovic, Peter Palensky*

- `1708.08349v1` - [abs](http://arxiv.org/abs/1708.08349v1) - [pdf](http://arxiv.org/pdf/1708.08349v1)

> Understanding smart grid cyber attacks is key for developing appropriate protection and recovery measures. Advanced attacks pursue maximized impact at minimized costs and detectability. This paper conducts risk analysis of combined data integrity and availability attacks against the power system state estimation. We compare the combined attacks with pure integrity attacks - false data injection (FDI) attacks. A security index for vulnerability assessment to these two kinds of attacks is proposed and formulated as a mixed integer linear programming problem. We show that such combined attacks can succeed with fewer resources than FDI attacks. The combined attacks with limited knowledge of the system model also expose advantages in keeping stealth against the bad data detection. Finally, the risk of combined attacks to reliable system operation is evaluated using the results from vulnerability assessment and attack impact analysis. The findings in this paper are validated and supported by a detailed case study.

</details>

<details>

<summary>2017-08-28 14:54:04 - Data Attacks on Power System State Estimation: Limited Adversarial Knowledge vs. Limited Attack Resources</summary>

- *Kaikai Pan, André Teixeira, Milos Cvetkovic, Peter Palensky*

- `1708.08355v1` - [abs](http://arxiv.org/abs/1708.08355v1) - [pdf](http://arxiv.org/pdf/1708.08355v1)

> A class of data integrity attack, known as false data injection (FDI) attack, has been studied with a considerable amount of work. It has shown that with perfect knowledge of the system model and the capability to manipulate a certain number of measurements, the FDI attacks can coordinate measurements corruption to keep stealth against the bad data detection. However, a more realistic attack is essentially an attack with limited adversarial knowledge of the system model and limited attack resources due to various reasons. In this paper, we generalize the data attacks that they can be pure FDI attacks or combined with availability attacks (e.g., DoS attacks) and analyze the attacks with limited adversarial knowledge or limited attack resources. The attack impact is evaluated by the proposed metrics and the detection probability of attacks is calculated using the distribution property of data with or without attacks. The analysis is supported with results from a power system use case. The results show how important the knowledge is to the attacker and which measurements are more vulnerable to attacks with limited resources.

</details>

<details>

<summary>2017-08-28 17:47:14 - SlowFuzz: Automated Domain-Independent Detection of Algorithmic Complexity Vulnerabilities</summary>

- *Theofilos Petsios, Jason Zhao, Angelos D. Keromytis, Suman Jana*

- `1708.08437v1` - [abs](http://arxiv.org/abs/1708.08437v1) - [pdf](http://arxiv.org/pdf/1708.08437v1)

> Algorithmic complexity vulnerabilities occur when the worst-case time/space complexity of an application is significantly higher than the respective average case for particular user-controlled inputs. When such conditions are met, an attacker can launch Denial-of-Service attacks against a vulnerable application by providing inputs that trigger the worst-case behavior. Such attacks have been known to have serious effects on production systems, take down entire websites, or lead to bypasses of Web Application Firewalls.   Unfortunately, existing detection mechanisms for algorithmic complexity vulnerabilities are domain-specific and often require significant manual effort. In this paper, we design, implement, and evaluate SlowFuzz, a domain-independent framework for automatically finding algorithmic complexity vulnerabilities. SlowFuzz automatically finds inputs that trigger worst-case algorithmic behavior in the tested binary. SlowFuzz uses resource-usage-guided evolutionary search techniques to automatically find inputs that maximize computational resource utilization for a given application.

</details>

<details>

<summary>2017-08-29 14:37:27 - Deemon: Detecting CSRF with Dynamic Analysis and Property Graphs</summary>

- *Giancarlo Pellegrino, Martin Johns, Simon Koch, Michael Backes, Christian Rossow*

- `1708.08786v1` - [abs](http://arxiv.org/abs/1708.08786v1) - [pdf](http://arxiv.org/pdf/1708.08786v1)

> Cross-Site Request Forgery (CSRF) vulnerabilities are a severe class of web vulnerabilities that have received only marginal attention from the research and security testing communities. While much effort has been spent on countermeasures and detection of XSS and SQLi, to date, the detection of CSRF vulnerabilities is still performed predominantly manually.   In this paper, we present Deemon, to the best of our knowledge the first automated security testing framework to discover CSRF vulnerabilities. Our approach is based on a new modeling paradigm which captures multiple aspects of web applications, including execution traces, data flows, and architecture tiers in a unified, comprehensive property graph. We present the paradigm and show how a concrete model can be built automatically using dynamic traces. Then, using graph traversals, we mine for potentially vulnerable operations. Using the information captured in the model, our approach then automatically creates and conducts security tests, to practically validate the found CSRF issues. We evaluate the effectiveness of Deemon with 10 popular open source web applications. Our experiments uncovered 14 previously unknown CSRF vulnerabilities that can be exploited, for instance, to take over user accounts or entire websites.

</details>

<details>

<summary>2017-08-31 02:31:06 - A Novel Scheduling Framework Leveraging Hardware Cache Partitioning for Cache-Side-Channel Elimination in Clouds</summary>

- *Read Sprabery, Konstantin Evchenko, Abhilash Raj, Rakesh B. Bobba, Sibin Mohan, Roy H. Campbell*

- `1708.09538v1` - [abs](http://arxiv.org/abs/1708.09538v1) - [pdf](http://arxiv.org/pdf/1708.09538v1)

> While there exist many isolation mechanisms that are available to cloud service providers, including virtual machines, containers, etc., the problem of side-channel increases in importance as a remaining security vulnerability, particularly in the presence of shared caches and multicore processors. In this paper we present a hardware-software mechanism that improves the isolation of cloud processes in the presence of shared caches on multicore chips. Combining the Intel CAT architecture that enables cache partitioning on the fly with novel scheduling techniques and state cleansing mechanisms, we enable cache-side-channel free computing for Linux-based containers and virtual machines, in particular, those managed by KVM. We do a preliminary evaluation of our system using a CPU bound workload. Our system allows Simultaneous Multithreading (SMT) to remain enabled and does not require application level changes.

</details>

<details>

<summary>2017-08-31 19:11:56 - On Security and Sparsity of Linear Classifiers for Adversarial Settings</summary>

- *Ambra Demontis, Paolo Russu, Battista Biggio, Giorgio Fumera, Fabio Roli*

- `1709.00045v1` - [abs](http://arxiv.org/abs/1709.00045v1) - [pdf](http://arxiv.org/pdf/1709.00045v1)

> Machine-learning techniques are widely used in security-related applications, like spam and malware detection. However, in such settings, they have been shown to be vulnerable to adversarial attacks, including the deliberate manipulation of data at test time to evade detection. In this work, we focus on the vulnerability of linear classifiers to evasion attacks. This can be considered a relevant problem, as linear classifiers have been increasingly used in embedded systems and mobile devices for their low processing time and memory requirements. We exploit recent findings in robust optimization to investigate the link between regularization and security of linear classifiers, depending on the type of attack. We also analyze the relationship between the sparsity of feature weights, which is desirable for reducing processing cost, and the security of linear classifiers. We further propose a novel octagonal regularizer that allows us to achieve a proper trade-off between them. Finally, we empirically show how this regularizer can improve classifier security and sparsity in real-world application examples including spam and malware detection.

</details>


## 2017-09

<details>

<summary>2017-09-02 17:38:45 - Security Evaluation of Pattern Classifiers under Attack</summary>

- *Battista Biggio, Giorgio Fumera, Fabio Roli*

- `1709.00609v1` - [abs](http://arxiv.org/abs/1709.00609v1) - [pdf](http://arxiv.org/pdf/1709.00609v1)

> Pattern classification systems are commonly used in adversarial applications, like biometric authentication, network intrusion detection, and spam filtering, in which data can be purposely manipulated by humans to undermine their operation. As this adversarial scenario is not taken into account by classical design methods, pattern classification systems may exhibit vulnerabilities, whose exploitation may severely affect their performance, and consequently limit their practical utility. Extending pattern classification theory and design methods to adversarial settings is thus a novel and very relevant research direction, which has not yet been pursued in a systematic way. In this paper, we address one of the main open issues: evaluating at design phase the security of pattern classifiers, namely, the performance degradation under potential attacks they may incur during operation. We propose a framework for empirical evaluation of classifier security that formalizes and generalizes the main ideas proposed in the literature, and give examples of its use in three real applications. Reported results show that security evaluation can provide a more complete understanding of the classifier's behavior in adversarial environments, and lead to better design choices.

</details>

<details>

<summary>2017-09-05 18:51:03 - Did we learn from LLC Side Channel Attacks? A Cache Leakage Detection Tool for Crypto Libraries</summary>

- *Gorka Irazoqui, Kai Cong, Xiaofei Guo, Hareesh Khattri, Arun Kanuparthi, Thomas Eisenbarth, Berk Sunar*

- `1709.01552v1` - [abs](http://arxiv.org/abs/1709.01552v1) - [pdf](http://arxiv.org/pdf/1709.01552v1)

> This work presents a new tool to verify the correctness of cryptographic implementations with respect to cache attacks. Our methodology discovers vulnerabilities that are hard to find with other techniques, observed as exploitable leakage. The methodology works by identifying secret dependent memory and introducing forced evictions inside potentially vulnerable code to obtain cache traces that are analyzed using Mutual Information. If dependence is observed, the cryptographic implementation is classified as to leak information.   We demonstrate the viability of our technique in the design of the three main cryptographic primitives, i.e., AES, RSA and ECC, in eight popular up to date cryptographic libraries, including OpenSSL, Libgcrypt, Intel IPP and NSS. Our results show that cryptographic code designers are far away from incorporating the appropriate countermeasures to avoid cache leakages, as we found that 50% of the default implementations analyzed leaked information that lead to key extraction. We responsibly notified the designers of all the leakages found and suggested patches to solve these vulnerabilities.

</details>

<details>

<summary>2017-09-10 11:10:43 - TestREx: a Framework for Repeatable Exploits</summary>

- *Stanislav Dashevskyi, Daniel Ricardo dos Santos, Fabio Massacci, Antonino Sabetta*

- `1709.03084v1` - [abs](http://arxiv.org/abs/1709.03084v1) - [pdf](http://arxiv.org/pdf/1709.03084v1)

> Web applications are the target of many well known exploits and also a fertile ground for the discovery of security vulnerabilities. Yet, the success of an exploit depends both on the vulnerability in the application source code and the environment in which the application is deployed and run. As execution environments are complex (application servers, databases and other supporting applications), we need to have a reliable framework to test whether known exploits can be reproduced in different settings, better understand their effects, and facilitate the discovery of new vulnerabilities. In this paper, we present TestREx - a framework that allows for highly automated, easily repeatable exploit testing in a variety of contexts, so that a security tester may quickly and efficiently perform large-scale experiments with vulnerability exploits. It supports packing and running applications with their environments, injecting exploits, monitoring their success, and generating security reports. We also provide a corpus of example applications, taken from related works or implemented by us.

</details>

<details>

<summary>2017-09-11 02:41:15 - MagNet: a Two-Pronged Defense against Adversarial Examples</summary>

- *Dongyu Meng, Hao Chen*

- `1705.09064v2` - [abs](http://arxiv.org/abs/1705.09064v2) - [pdf](http://arxiv.org/pdf/1705.09064v2)

> Deep learning has shown promising results on hard perceptual problems in recent years. However, deep learning systems are found to be vulnerable to small adversarial perturbations that are nearly imperceptible to human. Such specially crafted perturbations cause deep learning systems to output incorrect decisions, with potentially disastrous consequences. These vulnerabilities hinder the deployment of deep learning systems where safety or security is important. Attempts to secure deep learning systems either target specific attacks or have been shown to be ineffective.   In this paper, we propose MagNet, a framework for defending neural network classifiers against adversarial examples. MagNet does not modify the protected classifier or know the process for generating adversarial examples. MagNet includes one or more separate detector networks and a reformer network. Different from previous work, MagNet learns to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since it does not rely on any process for generating adversarial examples, it has substantial generalization power. Moreover, MagNet reconstructs adversarial examples by moving them towards the manifold, which is effective for helping classify adversarial examples with small perturbation correctly. We discuss the intrinsic difficulty in defending against whitebox attack and propose a mechanism to defend against graybox attack. Inspired by the use of randomness in cryptography, we propose to use diversity to strengthen MagNet. We show empirically that MagNet is effective against most advanced state-of-the-art attacks in blackbox and graybox scenarios while keeping false positive rate on normal examples very low.

</details>

<details>

<summary>2017-09-12 01:18:53 - MeshCloak: A Map-Based Approach for Personalized Location Privacy</summary>

- *Hiep H. Nguyen*

- `1709.03642v1` - [abs](http://arxiv.org/abs/1709.03642v1) - [pdf](http://arxiv.org/pdf/1709.03642v1)

> Protecting location privacy in mobile services has recently received significant consideration as Location-Based Service (LBS) can reveal user locations to attackers. A problem in the existing cloaking schemes is that location vulnerabilities may be exposed when an attacker exploits a street map in their attacks. While both real and synthetic trajectories are based on real street maps, most of previous cloaking schemes assume free space movements to define the distance between users, resulting in the mismatch between privacy models and user movements. In this paper, we present MeshCloak, a novel map-based model for personalized location privacy, which is formulated entirely in map-based setting and resists inference attacks at a minimal performance overhead. The key idea of MeshCloak is to quickly build a sparse constraint graph based on the mutual coverage relationship between queries by pre-computing the distance matrix and applying quadtree search. MeshCloak also takes into account real speed profiles and query frequencies. We evaluate the efficiency and effectiveness of the proposed scheme via a suite of carefully designed experiments on five real maps.

</details>

<details>

<summary>2017-09-13 05:14:48 - Models and Framework for Adversarial Attacks on Complex Adaptive Systems</summary>

- *Vahid Behzadan, Arslan Munir*

- `1709.04137v1` - [abs](http://arxiv.org/abs/1709.04137v1) - [pdf](http://arxiv.org/pdf/1709.04137v1)

> We introduce the paradigm of adversarial attacks that target the dynamics of Complex Adaptive Systems (CAS). To facilitate the analysis of such attacks, we present multiple approaches to the modeling of CAS as dynamical, data-driven, and game-theoretic systems, and develop quantitative definitions of attack, vulnerability, and resilience in the context of CAS security. Furthermore, we propose a comprehensive set of schemes for classification of attacks and attack surfaces in CAS, complemented with examples of practical attacks. Building on this foundation, we propose a framework based on reinforcement learning for simulation and analysis of attacks on CAS, and demonstrate its performance through three real-world case studies of targeting power grids, destabilization of terrorist organizations, and manipulation of machine learning agents. We also discuss potential mitigation techniques, and remark on future research directions in analysis and design of secure complex adaptive systems.

</details>

<details>

<summary>2017-09-14 05:59:50 - Do Developers Update Their Library Dependencies? An Empirical Study on the Impact of Security Advisories on Library Migration</summary>

- *Raula Gaikovina Kula, Daniel M. German, Ali Ouni, Takashi Ishio, Katsuro Inoue*

- `1709.04621v1` - [abs](http://arxiv.org/abs/1709.04621v1) - [pdf](http://arxiv.org/pdf/1709.04621v1)

> Third-party library reuse has become common practice in contemporary software development, as it includes several benefits for developers. Library dependencies are constantly evolving, with newly added features and patches that fix bugs in older versions. To take full advantage of third-party reuse, developers should always keep up to date with the latest versions of their library dependencies. In this paper, we investigate the extent of which developers update their library dependencies. Specifically, we conducted an empirical study on library migration that covers over 4,600 GitHub software projects and 2,700 library dependencies. Results show that although many of these systems rely heavily on dependencies, 81.5% of the studied systems still keep their outdated dependencies. In the case of updating a vulnerable dependency, the study reveals that affected developers are not likely to respond to a security advisory. Surveying these developers, we find that 69% of the interviewees claim that they were unaware of their vulnerable dependencies. Furthermore, developers are not likely to prioritize library updates, citing it as extra effort and added responsibility. This study concludes that even though third-party reuse is commonplace, the practice of updating a dependency is not as common for many developers.

</details>

<details>

<summary>2017-09-14 06:14:43 - Modeling Library Dependencies and Updates in Large Software Repository Universes</summary>

- *Raula Gaikovina Kula, Coen De Roover, Daniel M. German, Takashi Ishio, Katsuro Inoue*

- `1709.04626v1` - [abs](http://arxiv.org/abs/1709.04626v1) - [pdf](http://arxiv.org/pdf/1709.04626v1)

> Popular (re)use of third-party open-source software (OSS) is evidence of the impact of hosting repositories like maven on software development today. Updating libraries is crucial, with recent studies highlighting the associated vulnerabilities with aging OSS libraries. The decision to migrate to a newer library can range from trivial (security threat) to complex (assessment of work required to accommodate the changes). By leveraging the `wisdom of the software repository crowd' we propose a simple and efficient approach to recommending `consented' library updates. Our Software Universe Graph (SUG) models library dependency and update information mined from super repositories to provide different metrics and visualizations that aid in the update decision. To evaluate, we first constructed a SUG from 188,951 nodes of 6,374 maven unique artifacts. Then, we demonstrate how our metrics and visualizations are applied through real-world examples. As an extension, we show how the SUG can compare dependencies between different super repositories. From a sample of 100 GitHub applications, our method found that on average 79% similar overlapping dependencies combinations exist between the maven and github super repository universes.

</details>

<details>

<summary>2017-09-14 17:16:09 - ClouNS - A Cloud-native Application Reference Model for Enterprise Architects</summary>

- *Nane Kratzke, René Peinl*

- `1709.04883v1` - [abs](http://arxiv.org/abs/1709.04883v1) - [pdf](http://arxiv.org/pdf/1709.04883v1)

> The capability to operate cloud-native applications can generate enormous business growth and value. But enterprise architects should be aware that cloud-native applications are vulnerable to vendor lock-in. We investigated cloud-native application design principles, public cloud service providers, and industrial cloud standards. All results indicate that most cloud service categories seem to foster vendor lock-in situations which might be especially problematic for enterprise architectures. This might sound disillusioning at first. However, we present a reference model for cloud-native applications that relies only on a small subset of well standardized IaaS services. The reference model can be used for codifying cloud technologies. It can guide technology identification, classification, adoption, research and development processes for cloud-native application and for vendor lock-in aware enterprise architecture engineering methodologies.

</details>

<details>

<summary>2017-09-15 01:09:00 - Attack-Graph Threat Modeling Assessment of Ambulatory Medical Devices</summary>

- *Patrick Luckett, J Todd McDonald, William Bradley Glisson*

- `1709.05026v1` - [abs](http://arxiv.org/abs/1709.05026v1) - [pdf](http://arxiv.org/pdf/1709.05026v1)

> The continued integration of technology into all aspects of society stresses the need to identify and understand the risk associated with assimilating new technologies. This necessity is heightened when technology is used for medical purposes like ambulatory devices that monitor a patient's vital signs. This integration creates environments that are conducive to malicious activities. The potential impact presents new challenges for the medical community.   Hence, this research presents attack graph modeling as a viable solution to identifying vulnerabilities, assessing risk, and forming mitigation strategies to defend ambulatory medical devices from attackers. Common and frequent vulnerabilities and attack strategies related to the various aspects of ambulatory devices, including Bluetooth enabled sensors and Android applications are identified in the literature. Based on this analysis, this research presents an attack graph modeling example on a theoretical device that highlights vulnerabilities and mitigation strategies to consider when designing ambulatory devices with similar components.

</details>

<details>

<summary>2017-09-18 13:51:27 - Data Integrity Threats and Countermeasures in Railway Spot Transmission Systems</summary>

- *Hoon Wei Lim, William G. Temple, Bao Anh N. Tran, Binbin Chen, Zbigniew Kalbarczyk, Jianying Zhou*

- `1709.05935v1` - [abs](http://arxiv.org/abs/1709.05935v1) - [pdf](http://arxiv.org/pdf/1709.05935v1)

> Modern trains rely on balises (communication beacons) located on the track to provide location information as they traverse a rail network. Balises, such as those conforming to the Eurobalise standard, were not designed with security in mind and are thus vulnerable to cyber attacks targeting data availability, integrity, or authenticity. In this work, we discuss data integrity threats to balise transmission modules and use high-fidelity simulation to study the risks posed by data integrity attacks. To mitigate such risk, we propose a practical two-layer solution: at the device level, we design a lightweight and low-cost cryptographic solution to protect the integrity of the location information; at the system layer, we devise a secure hybrid train speed controller to mitigate the impact under various attacks. Our simulation results demonstrate the effectiveness of our proposed solutions.

</details>

<details>

<summary>2017-09-19 14:18:13 - Scalable Support Vector Clustering Using Budget</summary>

- *Tung Pham, Trung Le, Hang Dang*

- `1709.06444v1` - [abs](http://arxiv.org/abs/1709.06444v1) - [pdf](http://arxiv.org/pdf/1709.06444v1)

> Owing to its application in solving the difficult and diverse clustering or outlier detection problem, support-based clustering has recently drawn plenty of attention. Support-based clustering method always undergoes two phases: finding the domain of novelty and performing clustering assignment. To find the domain of novelty, the training time given by the current solvers is typically over-quadratic in the training size, and hence precluding the usage of support-based clustering method for large-scale datasets. In this paper, we propose applying Stochastic Gradient Descent (SGD) framework to the first phase of support-based clustering for finding the domain of novelty and a new strategy to perform the clustering assignment. However, the direct application of SGD to the first phase of support-based clustering is vulnerable to the curse of kernelization, that is, the model size linearly grows up with the data size accumulated overtime. To address this issue, we invoke the budget approach which allows us to restrict the model size to a small budget. Our new strategy for clustering assignment enables a fast computation by means of reducing the task of clustering assignment on the full training set to the same task on a significantly smaller set. We also provide a rigorous theoretical analysis about the convergence rate for the proposed method. Finally, we validate our proposed method on the well-known datasets for clustering to show that the proposed method offers a comparable clustering quality while simultaneously achieving significant speedup in comparison with the baselines.

</details>

<details>

<summary>2017-09-20 21:28:01 - How Unique is Your .onion? An Analysis of the Fingerprintability of Tor Onion Services</summary>

- *Rebekah Overdorf, Marc Juarez, Gunes Acar, Rachel Greenstadt, Claudia Diaz*

- `1708.08475v2` - [abs](http://arxiv.org/abs/1708.08475v2) - [pdf](http://arxiv.org/pdf/1708.08475v2)

> Recent studies have shown that Tor onion (hidden) service websites are particularly vulnerable to website fingerprinting attacks due to their limited number and sensitive nature. In this work we present a multi-level feature analysis of onion site fingerprintability, considering three state-of-the-art website fingerprinting methods and 482 Tor onion services, making this the largest analysis of this kind completed on onion services to date.   Prior studies typically report average performance results for a given website fingerprinting method or countermeasure. We investigate which sites are more or less vulnerable to fingerprinting and which features make them so. We find that there is a high variability in the rate at which sites are classified (and misclassified) by these attacks, implying that average performance figures may not be informative of the risks that website fingerprinting attacks pose to particular sites.   We analyze the features exploited by the different website fingerprinting methods and discuss what makes onion service sites more or less easily identifiable, both in terms of their traffic traces as well as their webpage design. We study misclassifications to understand how onion service sites can be redesigned to be less vulnerable to website fingerprinting attacks. Our results also inform the design of website fingerprinting countermeasures and their evaluation considering disparate impact across sites.

</details>

<details>

<summary>2017-09-20 23:20:42 - FairFuzz: Targeting Rare Branches to Rapidly Increase Greybox Fuzz Testing Coverage</summary>

- *Caroline Lemieux, Koushik Sen*

- `1709.07101v1` - [abs](http://arxiv.org/abs/1709.07101v1) - [pdf](http://arxiv.org/pdf/1709.07101v1)

> In recent years, fuzz testing has proven itself to be one of the most effective techniques for finding correctness bugs and security vulnerabilities in practice. One particular fuzz testing tool, American Fuzzy Lop or AFL, has become popular thanks to its ease-of-use and bug-finding power. However, AFL remains limited in the depth of program coverage it achieves, in particular because it does not consider which parts of program inputs should not be mutated in order to maintain deep program coverage. We propose an approach, FairFuzz, that helps alleviate this limitation in two key steps. First, FairFuzz automatically prioritizes inputs exercising rare parts of the program under test. Second, it automatically adjusts the mutation of inputs so that the mutated inputs are more likely to exercise these same rare parts of the program. We conduct evaluation on real-world programs against state-of-the-art versions of AFL, thoroughly repeating experiments to get good measures of variability. We find that on certain benchmarks FairFuzz shows significant coverage increases after 24 hours compared to state-of-the-art versions of AFL, while on others it achieves high program coverage at a significantly faster rate.

</details>

<details>

<summary>2017-09-21 12:31:24 - Android Inter-App Communication Threats and Detection Techniques</summary>

- *Shweta Bhandari, Wafa Ben Jaballah, Vineeta Jain, Vijay Laxmi, Akka Zemmari, Manoj Singh Gaur, Mohamed Mosbah, Mauro Conti*

- `1611.10076v2` - [abs](http://arxiv.org/abs/1611.10076v2) - [pdf](http://arxiv.org/pdf/1611.10076v2)

> With the digital breakthrough, smart phones have become very essential component. Mobile devices are very attractive attack surface for cyber thieves as they hold personal details (accounts, locations, contacts, photos) and have potential capabilities for eavesdropping (with cameras/microphone, wireless connections). Android, being the most popular, is the target of malicious hackers who are trying to use Android app as a tool to break into and control device. Android malware authors use many anti-analysis techniques to hide from analysis tools. Academic researchers and commercial anti-malware companies are putting great effort to detect such malicious apps. They are making use of the combinations of static, dynamic and behavior based analysis techniques. Despite of all the security mechanisms provided by Android, apps can carry out malicious actions through collusion. In collusion malicious functionality is divided across multiple apps. Each participating app accomplish its part and communicate information to another app through Inter Component Communication (ICC). ICC do not require any special permissions. Also, there is no compulsion to inform user about the communication. Each participating app needs to request a minimal set of privileges, which may make it appear benign to current state-of-the-art techniques that analyze one app at a time. There are many surveys on app analysis techniques in Android; however they focus on single-app analysis. This survey augments this through focusing only on collusion among multiple-apps. In this paper, we present Android vulnerabilities that may be exploited for a possible collusion attack. We cover the existing threat analysis, scenarios, and a detailed comparison of tools for intra and inter-app analysis. To the best of our knowledge this is the first survey on app collusion and state-of-the-art detection tools in Android.

</details>

<details>

<summary>2017-09-21 18:17:36 - Learning Domain-Specific Word Embeddings from Sparse Cybersecurity Texts</summary>

- *Arpita Roy, Youngja Park, SHimei Pan*

- `1709.07470v1` - [abs](http://arxiv.org/abs/1709.07470v1) - [pdf](http://arxiv.org/pdf/1709.07470v1)

> Word embedding is a Natural Language Processing (NLP) technique that automatically maps words from a vocabulary to vectors of real numbers in an embedding space. It has been widely used in recent years to boost the performance of a vari-ety of NLP tasks such as Named Entity Recognition, Syntac-tic Parsing and Sentiment Analysis. Classic word embedding methods such as Word2Vec and GloVe work well when they are given a large text corpus. When the input texts are sparse as in many specialized domains (e.g., cybersecurity), these methods often fail to produce high-quality vectors. In this pa-per, we describe a novel method to train domain-specificword embeddings from sparse texts. In addition to domain texts, our method also leverages diverse types of domain knowledge such as domain vocabulary and semantic relations. Specifi-cally, we first propose a general framework to encode diverse types of domain knowledge as text annotations. Then we de-velop a novel Word Annotation Embedding (WAE) algorithm to incorporate diverse types of text annotations in word em-bedding. We have evaluated our method on two cybersecurity text corpora: a malware description corpus and a Common Vulnerability and Exposure (CVE) corpus. Our evaluation re-sults have demonstrated the effectiveness of our method in learning domain-specific word embeddings.

</details>

<details>

<summary>2017-09-22 01:47:34 - Stochastic Tools for Network Intrusion Detection</summary>

- *Lu Yu, Richard R. Brooks*

- `1709.07567v1` - [abs](http://arxiv.org/abs/1709.07567v1) - [pdf](http://arxiv.org/pdf/1709.07567v1)

> With the rapid development of Internet and the sharp increase of network crime, network security has become very important and received a lot of attention. We model security issues as stochastic systems. This allows us to find weaknesses in existing security systems and propose new solutions. Exploring the vulnerabilities of existing security tools can prevent cyber-attacks from taking advantages of the system weaknesses. We propose a hybrid network security scheme including intrusion detection systems (IDSs) and honeypots scattered throughout the network. This combines the advantages of two security technologies. A honeypot is an activity-based network security system, which could be the logical supplement of the passive detection policies used by IDSs. This integration forces us to balance security performance versus cost by scheduling device activities for the proposed system. By formulating the scheduling problem as a decentralized partially observable Markov decision process (DEC-POMDP), decisions are made in a distributed manner at each device without requiring centralized control. The partially observable Markov decision process (POMDP) is a useful choice for controlling stochastic systems. As a combination of two Markov models, POMDPs combine the strength of hidden Markov Model (HMM) (capturing dynamics that depend on unobserved states) and that of Markov decision process (MDP) (taking the decision aspect into account). Decision making under uncertainty is used in many parts of business and science.We use here for security tools.We adopt a high-quality approximation solution for finite-space POMDPs with the average cost criterion, and their extension to DEC-POMDPs. We show how this tool could be used to design a network security framework.

</details>

<details>

<summary>2017-09-22 02:32:28 - Modeling and Detecting False Data Injection Attacks against Railway Traction Power Systems</summary>

- *Subhash Lakshminarayana, Teo Zhan Teng, Rui Tan, David K. Y. Yau*

- `1709.07574v1` - [abs](http://arxiv.org/abs/1709.07574v1) - [pdf](http://arxiv.org/pdf/1709.07574v1)

> Modern urban railways extensively use computerized sensing and control technologies to achieve safe, reliable, and well-timed operations. However, the use of these technologies may provide a convenient leverage to cyber-attackers who have bypassed the air gaps and aim at causing safety incidents and service disruptions. In this paper, we study false data injection (FDI) attacks against railways' traction power systems (TPSes). Specifically, we analyze two types of FDI attacks on the train-borne voltage, current, and position sensor measurements - which we call efficiency attack and safety attack -- that (i) maximize the system's total power consumption and (ii) mislead trains' local voltages to exceed given safety-critical thresholds, respectively. To counteract, we develop a global attack detection (GAD) system that serializes a bad data detector and a novel secondary attack detector designed based on unique TPS characteristics. With intact position data of trains, our detection system can effectively detect the FDI attacks on trains' voltage and current measurements even if the attacker has full and accurate knowledge of the TPS, attack detection, and real-time system state. In particular, the GAD system features an adaptive mechanism that ensures low false positive and negative rates in detecting the attacks under noisy system measurements. Extensive simulations driven by realistic running profiles of trains verify that a TPS setup is vulnerable to the FDI attacks, but these attacks can be detected effectively by the proposed GAD while ensuring a low false positive rate.

</details>

<details>

<summary>2017-09-23 21:15:03 - Towards Baselines for Shoulder Surfing on Mobile Authentication</summary>

- *Adam J. Aviv, John T. Davin, Flynn Wolf, Ravi Kuber*

- `1709.04959v2` - [abs](http://arxiv.org/abs/1709.04959v2) - [pdf](http://arxiv.org/pdf/1709.04959v2)

> Given the nature of mobile devices and unlock procedures, unlock authentication is a prime target for credential leaking via shoulder surfing, a form of an observation attack. While the research community has investigated solutions to minimize or prevent the threat of shoulder surfing, our understanding of how the attack performs on current systems is less well studied. In this paper, we describe a large online experiment (n=1173) that works towards establishing a baseline of shoulder surfing vulnerability for current unlock authentication systems. Using controlled video recordings of a victim entering in a set of 4- and 6-length PINs and Android unlock patterns on different phones from different angles, we asked participants to act as attackers, trying to determine the authentication input based on the observation. We find that 6-digit PINs are the most elusive attacking surface where a single observation leads to just 10.8% successful attacks, improving to 26.5\% with multiple observations. As a comparison, 6-length Android patterns, with one observation, suffered 64.2% attack rate and 79.9% with multiple observations. Removing feedback lines for patterns improves security from 35.3\% and 52.1\% for single and multiple observations, respectively. This evidence, as well as other results related to hand position, phone size, and observation angle, suggests the best and worst case scenarios related to shoulder surfing vulnerability which can both help inform users to improve their security choices, as well as establish baselines for researchers.

</details>

<details>

<summary>2017-09-24 09:08:24 - A Model for Enhancing Human Behaviour with Security Questions: A Theoretical Perspective</summary>

- *Nicholas Micallef, Nalin Asanka Gamagedara Arachchilage*

- `1709.08165v1` - [abs](http://arxiv.org/abs/1709.08165v1) - [pdf](http://arxiv.org/pdf/1709.08165v1)

> Security questions are one of the mechanisms used to recover passwords. Strong answers to security questions (i.e. high entropy) are hard for attackers to guess or obtain using social engineering techniques (e.g. monitoring of social networking profiles), but at the same time are difficult to remember. Instead, weak answers to security questions (i.e. low entropy) are easy to remember, which makes them more vulnerable to cyber-attacks. Convenience leads users to use the same answers to security questions on multiple accounts, which exposes these accounts to numerous cyber-threats. Hence, current security questions implementations rarely achieve the required security and memorability requirements. This research study is the first step in the development of a model which investigates the determinants that influence users' behavioural intentions through motivation to select strong and memorable answers to security questions. This research also provides design recommendations for novel security questions mechanisms.

</details>

<details>

<summary>2017-09-25 14:17:50 - Key Management and Learning based Two Level Data Security for Metering Infrastructure of Smart Grid</summary>

- *Imtiaz Parvez, Maryamossadat Aghili, Arif Sarwat*

- `1709.08505v1` - [abs](http://arxiv.org/abs/1709.08505v1) - [pdf](http://arxiv.org/pdf/1709.08505v1)

> In the smart grid, smart meters, and numerous control and monitoring applications employ bidirectional wireless communication, where security is a critical issue. In key management based encryption method for the smart grid, the Trusted Third Party (TTP), and links between the smart meter and the third party are assumed to be fully trusted and reliable. However, in wired/wireless medium, a man-in-middle may want to interfere, monitor and control the network, thus exposing its vulnerability. Acknowledging this, in this paper, we propose a novel two level encryption method based on two partially trusted simple servers (constitutes the TTP) which implement this method without increasing packet overhead. One server is responsible for data encryption between the meter and control center/central database, and the other server manages the random sequence of data transmission. Numerical calculation shows that the number of iterations required to decode a message is large which is quite impractical. Furthermore, we introduce One-class support vector machine (machine learning) algorithm for node-to-node authentication utilizing the location information and the data transmission history (node identity, packet size and frequency of transmission). This secures data communication privacy without increasing the complexity of the conventional key management scheme.

</details>

<details>

<summary>2017-09-26 19:49:18 - Stacco: Differentially Analyzing Side-Channel Traces for Detecting SSL/TLS Vulnerabilities in Secure Enclaves</summary>

- *Yuan Xiao, Mengyuan Li, Sanchuan Chen, Yinqian Zhang*

- `1707.03473v2` - [abs](http://arxiv.org/abs/1707.03473v2) - [pdf](http://arxiv.org/pdf/1707.03473v2)

> Intel Software Guard Extension (SGX) offers software applications enclave to protect their confidentiality and integrity from malicious operating systems. The SSL/TLS protocol, which is the de facto standard for protecting transport-layer network communications, has been broadly deployed for a secure communication channel. However, in this paper, we show that the marriage between SGX and SSL may not be smooth sailing.   Particularly, we consider a category of side-channel attacks against SSL/TLS implementations in secure enclaves, which we call the control-flow inference attacks. In these attacks, the malicious operating system kernel may perform a powerful man-in-the-kernel attack to collect execution traces of the enclave programs at page, cacheline, or branch level, while positioning itself in the middle of the two communicating parties. At the center of our work is a differential analysis framework, dubbed Stacco, to dynamically analyze the SSL/TLS implementations and detect vulnerabilities that can be exploited as decryption oracles. Surprisingly, we found exploitable vulnerabilities in the latest versions of all the SSL/TLS libraries we have examined.   To validate the detected vulnerabilities, we developed a man-in-the-kernel adversary to demonstrate Bleichenbacher attacks against the latest OpenSSL library running in the SGX enclave (with the help of Graphene) and completely broke the PreMasterSecret encrypted by a 4096-bit RSA public key with only 57286 queries. We also conducted CBC padding oracle attacks against the latest GnuTLS running in Graphene-SGX and an open-source SGX-implementation of mbedTLS (i.e., mbedTLS-SGX) that runs directly inside the enclave, and showed that it only needs 48388 and 25717 queries, respectively, to break one block of AES ciphertext. Empirical evaluation suggests these man-in-the-kernel attacks can be completed within 1 or 2 hours.

</details>

<details>

<summary>2017-09-26 22:21:46 - SUBIC: A Supervised Bi-Clustering Approach for Precision Medicine</summary>

- *Milad Zafar Nezhad, Dongxiao Zhu, Najibesadat Sadati, Kai Yang, Phillip Levy*

- `1709.09929v1` - [abs](http://arxiv.org/abs/1709.09929v1) - [pdf](http://arxiv.org/pdf/1709.09929v1)

> Traditional medicine typically applies one-size-fits-all treatment for the entire patient population whereas precision medicine develops tailored treatment schemes for different patient subgroups. The fact that some factors may be more significant for a specific patient subgroup motivates clinicians and medical researchers to develop new approaches to subgroup detection and analysis, which is an effective strategy to personalize treatment. In this study, we propose a novel patient subgroup detection method, called Supervised Biclustring (SUBIC) using convex optimization and apply our approach to detect patient subgroups and prioritize risk factors for hypertension (HTN) in a vulnerable demographic subgroup (African-American). Our approach not only finds patient subgroups with guidance of a clinically relevant target variable but also identifies and prioritizes risk factors by pursuing sparsity of the input variables and encouraging similarity among the input variables and between the input and target variables

</details>

<details>

<summary>2017-09-27 16:02:48 - A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples</summary>

- *Beilun Wang, Ji Gao, Yanjun Qi*

- `1612.00334v12` - [abs](http://arxiv.org/abs/1612.00334v12) - [pdf](http://arxiv.org/pdf/1612.00334v12)

> Most machine learning classifiers, including deep neural networks, are vulnerable to adversarial examples. Such inputs are typically generated by adding small but purposeful modifications that lead to incorrect outputs while imperceptible to human eyes. The goal of this paper is not to introduce a single method, but to make theoretical steps towards fully understanding adversarial examples. By using concepts from topology, our theoretical analysis brings forth the key reasons why an adversarial example can fool a classifier ($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By investigating the topological relationship between two (pseudo)metric spaces corresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and sufficient conditions that can determine if $f_1$ is always robust (strong-robust) against adversarial examples according to $f_2$. Interestingly our theorems indicate that just one unnecessary feature can make $f_1$ not strong-robust, and the right feature representation learning is the key to getting a classifier that is both accurate and strong-robust.

</details>

<details>

<summary>2017-09-28 13:59:42 - Secure Coding Practices in Java: Challenges and Vulnerabilities</summary>

- *Na Meng, Stefan Nagy, Daphne Yao, Wenjie Zhuang, Gustavo Arango Argoty*

- `1709.09970v1` - [abs](http://arxiv.org/abs/1709.09970v1) - [pdf](http://arxiv.org/pdf/1709.09970v1)

> Java platform and third-party libraries provide various security features to facilitate secure coding. However, misusing these features can cost tremendous time and effort of developers or cause security vulnerabilities in software. Prior research was focused on the misuse of cryptography and SSL APIs, but did not explore the key fundamental research question: what are the biggest challenges and vulnerabilities in secure coding practices? In this paper, we conducted a comprehensive empirical study on StackOverflow posts to understand developers' concerns on Java secure coding, their programming obstacles, and potential vulnerabilities in their code. We observed that developers have shifted their effort to the usage of authentication and authorization features provided by Spring security--a third-party framework designed to secure enterprise applications. Multiple programming challenges are related to APIs or libraries, including the complicated cross-language data handling of cryptography APIs, and the complex Java-based or XML-based approaches to configure Spring security. More interestingly, we identified security vulnerabilities in the suggested code of accepted answers. The vulnerabilities included using insecure hash functions such as MD5, breaking SSL/TLS security through bypassing certificate validation, and insecurely disabling the default protection against Cross Site Request Forgery (CSRF) attacks. Our findings reveal the insufficiency of secure coding assistance and education, and the gap between security theory and coding practices.

</details>


## 2017-10

<details>

<summary>2017-10-01 18:53:03 - Computation on Encrypted Data using Data Flow Authentication</summary>

- *Andreas Fischer, Benny Fuhry, Florian Kerschbaum, Eric Bodden*

- `1710.00390v1` - [abs](http://arxiv.org/abs/1710.00390v1) - [pdf](http://arxiv.org/pdf/1710.00390v1)

> Encrypting data before sending it to the cloud protects it against hackers and malicious insiders, but requires the cloud to compute on encrypted data. Trusted (hardware) modules, e.g., secure enclaves like Intel's SGX, can very efficiently run entire programs in encrypted memory. However, it already has been demonstrated that software vulnerabilities give an attacker ample opportunity to insert arbitrary code into the program. This code can then modify the data flow of the program and leak any secret in the program to an observer in the cloud via SGX side-channels. Since any larger program is rife with software vulnerabilities, it is not a good idea to outsource entire programs to an SGX enclave. A secure alternative with a small trusted code base would be fully homomorphic encryption (FHE) -- the holy grail of encrypted computation. However, due to its high computational complexity it is unlikely to be adopted in the near future. As a result researchers have made several proposals for transforming programs to perform encrypted computations on less powerful encryption schemes. Yet, current approaches fail on programs that make control-flow decisions based on encrypted data. In this paper, we introduce the concept of data flow authentication (DFAuth). DFAuth prevents an adversary from arbitrarily deviating from the data flow of a program. Hence, an attacker cannot perform an attack as outlined before on SGX. This enables that all programs, even those including operations on control-flow decision variables, can be computed on encrypted data. We implemented DFAuth using a novel authenticated homomorphic encryption scheme, a Java bytecode-to-bytecode compiler producing fully executable programs, and SGX enclaves. A transformed neural network that performs machine learning on sensitive medical data can be evaluated on encrypted inputs and encrypted weights in 0.86 seconds.

</details>

<details>

<summary>2017-10-02 17:56:26 - Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight</summary>

- *Yen-Chen Lin, Ming-Yu Liu, Min Sun, Jia-Bin Huang*

- `1710.00814v1` - [abs](http://arxiv.org/abs/1710.00814v1) - [pdf](http://arxiv.org/pdf/1710.00814v1)

> Deep reinforcement learning has shown promising results in learning control policies for complex sequential decision-making tasks. However, these neural network-based policies are known to be vulnerable to adversarial examples. This vulnerability poses a potentially serious threat to safety-critical systems such as autonomous vehicles. In this paper, we propose a defense mechanism to defend reinforcement learning agents from adversarial attacks by leveraging an action-conditioned frame prediction module. Our core idea is that the adversarial examples targeting at a neural network-based policy are not effective for the frame prediction model. By comparing the action distribution produced by a policy from processing the current observed frame to the action distribution produced by the same policy from processing the predicted frame from the action-conditioned frame prediction module, we can detect the presence of adversarial examples. Beyond detecting the presence of adversarial examples, our method allows the agent to continue performing the task using the predicted frame when the agent is under attack. We evaluate the performance of our algorithm using five games in Atari 2600. Our results demonstrate that the proposed defense mechanism achieves favorable performance against baseline algorithms in detecting adversarial examples and in earning rewards when the agents are under attack.

</details>

<details>

<summary>2017-10-03 08:26:47 - An Iterative and Toolchain-Based Approach to Automate Scanning and Mapping Computer Networks</summary>

- *Stefan Marksteiner, Harald Lernbeiß, Bernhard Jandl-Scherf*

- `1710.01026v1` - [abs](http://arxiv.org/abs/1710.01026v1) - [pdf](http://arxiv.org/pdf/1710.01026v1)

> As today's organizational computer networks are ever evolving and becoming more and more complex, finding potential vulnerabilities and conducting security audits has become a crucial element in securing these networks. The first step in auditing a network is reconnaissance by mapping it to get a comprehensive overview over its structure. The growing complexity, however, makes this task increasingly effortful, even more as mapping (instead of plain scanning), presently, still involves a lot of manual work. Therefore, the concept proposed in this paper automates the scanning and mapping of unknown and non-cooperative computer networks in order to find security weaknesses or verify access controls. It further helps to conduct audits by allowing comparing documented with actual networks and finding unauthorized network devices, as well as evaluating access control methods by conducting delta scans. It uses a novel approach of augmenting data from iteratively chained existing scanning tools with context, using genuine analytics modules to allow assessing a network's topology instead of just generating a list of scanned devices. It further contains a visualization model that provides a clear, lucid topology map and a special graph for comparative analysis. The goal is to provide maximum insight with a minimum of a priori knowledge.

</details>

<details>

<summary>2017-10-09 21:08:01 - Security considerations for Galois non-dual RLWE families</summary>

- *Hao Chen, Kristin Lauter, Katherine E. Stange*

- `1710.03316v1` - [abs](http://arxiv.org/abs/1710.03316v1) - [pdf](http://arxiv.org/pdf/1710.03316v1)

> We explore further the hardness of the non-dual discrete variant of the Ring-LWE problem for various number rings, give improved attacks for certain rings satisfying some additional assumptions, construct a new family of vulnerable Galois number fields, and apply some number theoretic results on Gauss sums to deduce the likely failure of these attacks for 2-power cyclotomic rings and unramified moduli.

</details>

<details>

<summary>2017-10-10 13:39:48 - Horcrux: A Password Manager for Paranoids</summary>

- *Hannah Li, David Evans*

- `1706.05085v2` - [abs](http://arxiv.org/abs/1706.05085v2) - [pdf](http://arxiv.org/pdf/1706.05085v2)

> Vulnerabilities in password managers are unremitting because current designs provide large attack surfaces, both at the client and server. We describe and evaluate Horcrux, a password manager that is designed holistically to minimize and decentralize trust, while retaining the usability of a traditional password manager. The prototype Horcrux client, implemented as a Firefox add-on, is split into two components, with code that has access to the user's master's password and any key material isolated into a small auditable component, separate from the complexity of managing the user interface. Instead of exposing actual credentials to the DOM, a dummy username and password are autofilled by the untrusted component. The trusted component intercepts and modifies POST requests before they are encrypted and sent over the network. To avoid trusting a centralized store, stored credentials are secret-shared over multiple servers. To provide domain and username privacy, while maintaining resilience to off-line attacks on a compromised password store, we incorporate cuckoo hashing in a way that ensures an attacker cannot determine if a guessed master password is correct. Our approach only works for websites that do not manipulate entered credentials in the browser client, so we conducted a large-scale experiment that found the technique appears to be compatible with over 98% of tested login forms.

</details>

<details>

<summary>2017-10-10 16:04:40 - A Polynomial-time Algorithm for Detecting the Possibility of Braess Paradox in Directed Graphs</summary>

- *Pietro Cenciarelli, Daniele Gorla, Ivano Salvo*

- `1610.09320v4` - [abs](http://arxiv.org/abs/1610.09320v4) - [pdf](http://arxiv.org/pdf/1610.09320v4)

> A directed multigraph is said vulnerable if it can generate Braess paradox in Traffic Networks. In this paper, we give a graph-theoretic characterisation of vulnerable directed multigraphs; analogous results appeared in the literature only for undirected multigraphs and for a specific family of directed multigraphs. The proof of our characterisation also provides an algorithm that checks if a multigraph is vulnerable in O(|V| |E|^2); this is the first polynomial time algorithm that checks vulnerability for general directed multigraphs. The resulting algorithm also contributes to another well known problem, i.e. the directed subgraph homeomorphism problem without node mapping, by providing another pattern graph for which a polynomial time algorithm exists.

</details>

<details>

<summary>2017-10-10 17:40:58 - Attacks on the Search-RLWE problem with small errors</summary>

- *Hao Chen, Kristin Lauter, Katherine E. Stange*

- `1710.03739v1` - [abs](http://arxiv.org/abs/1710.03739v1) - [pdf](http://arxiv.org/pdf/1710.03739v1)

> The Ring Learning-With-Errors (RLWE) problem shows great promise for post-quantum cryptography and homomorphic encryption. We describe a new attack on the non-dual search RLWE problem with small error widths, using ring homomorphisms to finite fields and the chi-squared statistical test. In particular, we identify a "subfield vulnerability" (Section 5.2) and give a new attack which finds this vulnerability by mapping to a finite field extension and detecting non-uniformity with respect to the number of elements in the subfield. We use this attack to give examples of vulnerable RLWE instances in Galois number fields. We also extend the well-known search-to-decision reduction result to Galois fields with any unramified prime modulus q, regardless of the residue degree f of q, and we use this in our attacks. The time complexity of our attack is O(nq2f), where n is the degree of K and f is the residue degree of q in K. We also show an attack on the non-dual (resp. dual) RLWE problem with narrow error distributions in prime cyclotomic rings when the modulus is a ramified prime (resp. any integer). We demonstrate the attacks in practice by finding many vulnerable instances and successfully attacking them. We include the code for all attacks.

</details>

<details>

<summary>2017-10-16 08:34:24 - Classifying Web Exploits with Topic Modeling</summary>

- *Jukka Ruohonen*

- `1710.05561v1` - [abs](http://arxiv.org/abs/1710.05561v1) - [pdf](http://arxiv.org/pdf/1710.05561v1)

> This short empirical paper investigates how well topic modeling and database meta-data characteristics can classify web and other proof-of-concept (PoC) exploits for publicly disclosed software vulnerabilities. By using a dataset comprised of over 36 thousand PoC exploits, near a 0.9 accuracy rate is obtained in the empirical experiment. Text mining and topic modeling are a significant boost factor behind this classification performance. In addition to these empirical results, the paper contributes to the research tradition of enhancing software vulnerability information with text mining, providing also a few scholarly observations about the potential for semi-automatic classification of exploits in the existing tracking infrastructures.

</details>

<details>

<summary>2017-10-16 16:13:40 - Phishing for Phools in the Internet of Things: Modeling One-to-Many Deception using Poisson Signaling Games</summary>

- *Jeffrey Pawlick, Quanyan Zhu*

- `1703.05234v2` - [abs](http://arxiv.org/abs/1703.05234v2) - [pdf](http://arxiv.org/pdf/1703.05234v2)

> Strategic interactions ranging from politics and pharmaceuticals to e-commerce and social networks support equilibria in which agents with private information manipulate others which are vulnerable to deception. Especially in cyberspace and the Internet of things, deception is difficult to detect and trust is complicated to establish. For this reason, effective policy-making, profitable entrepreneurship, and optimal technological design demand quantitative models of deception. In this paper, we use game theory to model specifically one-to-many deception. We combine a signaling game with a model called a Poisson game. The resulting Poisson signaling game extends traditional signaling games to include 1) exogenous evidence of deception, 2) an unknown number of receivers, and 3) receivers of multiple types. We find closed-form equilibrium solutions for a subset of Poisson signaling games, and characterize the rates of deception that they support. We show that receivers with higher abilities to detect deception can use crowd-defense tactics to mitigate deception for receivers with lower abilities to detect deception. Finally, we discuss how Poisson signaling games could be used to defend against the process by which the Mirai botnet recruits IoT devices in preparation for a distributed denial-of-service attack.

</details>

<details>

<summary>2017-10-16 16:14:20 - Proactive Population-Risk Based Defense Against Denial of Cyber-Physical Service Attacks</summary>

- *Jeffrey Pawlick, Quanyan Zhu*

- `1705.00682v2` - [abs](http://arxiv.org/abs/1705.00682v2) - [pdf](http://arxiv.org/pdf/1705.00682v2)

> While the Internet of things (IoT) promises to improve areas such as energy efficiency, health care, and transportation, it is highly vulnerable to cyberattacks. In particular, DDoS attacks work by overflowing the bandwidth of a server. But many IoT devices form part of cyber-physical systems (CPS). Therefore, they can be used to launch a "physical" denial-of-service attack (PDoS) in which IoT devices overflow the "physical bandwidth" of a CPS. In this paper, we quantify the population-based risk to a group of IoT devices targeted by malware for a PDoS attack. To model the recruitment of bots, we extend a traditional game-theoretic concept and create a "Poisson signaling game." Then we analyze two different mechanisms (legal and economic) to deter botnet recruitment. We find that 1) defenders can bound botnet activity and 2) legislating a minimum level of security has only a limited effect, while incentivizing active defense can decrease botnet activity arbitrarily. This work provides a quantitative foundation for designing proactive defense against PDoS attacks.

</details>

<details>

<summary>2017-10-16 16:26:04 - Proactive Defense Against Physical Denial of Service Attacks using Poisson Signaling Games</summary>

- *Jeffrey Pawlick, Quanyan Zhu*

- `1707.03708v2` - [abs](http://arxiv.org/abs/1707.03708v2) - [pdf](http://arxiv.org/pdf/1707.03708v2)

> While the Internet of things (IoT) promises to improve areas such as energy efficiency, health care, and transportation, it is highly vulnerable to cyberattacks. In particular, distributed denial-of-service (DDoS) attacks overload the bandwidth of a server. But many IoT devices form part of cyber-physical systems (CPS). Therefore, they can be used to launch "physical" denial-of-service attacks (PDoS) in which IoT devices overflow the "physical bandwidth" of a CPS. In this paper, we quantify the population-based risk to a group of IoT devices targeted by malware for a PDoS attack. In order to model the recruitment of bots, we develop a "Poisson signaling game," a signaling game with an unknown number of receivers, which have varying abilities to detect deception. Then we use a version of this game to analyze two mechanisms (legal and economic) to deter botnet recruitment. Equilibrium results indicate that 1) defenders can bound botnet activity, and 2) legislating a minimum level of security has only a limited effect, while incentivizing active defense can decrease botnet activity arbitrarily. This work provides a quantitative foundation for proactive PDoS defense.

</details>

<details>

<summary>2017-10-17 02:53:43 - Phish Phinder: A Game Design Approach to Enhance User Confidence in Mitigating Phishing Attacks</summary>

- *Gaurav Misra, Nalin Asanka Gamagedara Arachchilage, Shlomo Berkovsky*

- `1710.06064v1` - [abs](http://arxiv.org/abs/1710.06064v1) - [pdf](http://arxiv.org/pdf/1710.06064v1)

> Phishing is an especially challenging cyber security threat as it does not attack computer systems, but targets the user who works on that system by relying on the vulnerability of their decision-making ability. Phishing attacks can be used to gather sensitive information from victims and can have devastating impact if they are successful in deceiving the user. Several anti-phishing tools have been designed and implemented but they have been unable to solve the problem adequately. This failure is often due to security experts overlooking the human element and ignoring their fallibility in making trust decisions online. In this paper, we present Phish Phinder, a serious game designed to enhance the user's confidence in mitigating phishing attacks by providing them with both conceptual and procedural knowledge about phishing. The user is trained through a series of gamified challenges, designed to educate them about important phishing related concepts, through an interactive user interface. Key elements of the game interface were identified through an empirical study with the aim of enhancing user interaction with the game. We also adopted several persuasive design principles while designing Phish Phinder to enhance phishing avoidance behaviour among users.

</details>

<details>

<summary>2017-10-17 09:07:13 - Towards Linux Kernel Memory Safety</summary>

- *Elena Reshetova, Hans Liljestrand, Andrew Paverd, N. Asokan*

- `1710.06175v1` - [abs](http://arxiv.org/abs/1710.06175v1) - [pdf](http://arxiv.org/pdf/1710.06175v1)

> The security of billions of devices worldwide depends on the security and robustness of the mainline Linux kernel. However, the increasing number of kernel-specific vulnerabilities, especially memory safety vulnerabilities, shows that the kernel is a popular and practically exploitable target. Two major causes of memory safety vulnerabilities are reference counter overflows (temporal memory errors) and lack of pointer bounds checking (spatial memory errors).   To succeed in practice, security mechanisms for critical systems like the Linux kernel must also consider performance and deployability as critical design objectives. We present and systematically analyze two such mechanisms for improving memory safety in the Linux kernel: (a) an overflow-resistant reference counter data structure designed to accommodate typical reference counter usage in kernel source code, and (b) runtime pointer bounds checking using Intel MPX in the kernel.

</details>

<details>

<summary>2017-10-17 11:12:44 - On the (Statistical) Detection of Adversarial Examples</summary>

- *Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, Patrick McDaniel*

- `1702.06280v2` - [abs](http://arxiv.org/abs/1702.06280v2) - [pdf](http://arxiv.org/pdf/1702.06280v2)

> Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or Malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem. As a step towards understanding adversarial examples, we show that they are not drawn from the same distribution than the original data, and can thus be detected using statistical tests. Using thus knowledge, we introduce a complimentary approach to identify specific inputs that are adversarial. Specifically, we augment our ML model with an additional output, in which the model is trained to classify all adversarial inputs. We evaluate our approach on multiple adversarial example crafting methods (including the fast gradient sign and saliency map methods) with several datasets. The statistical test flags sample sets containing adversarial inputs confidently at sample sizes between 10 and 100 data points. Furthermore, our augmented model either detects adversarial examples as outliers with high accuracy (> 80%) or increases the adversary's cost - the perturbation added - by more than 150%. In this way, we show that statistical properties of adversarial examples are essential to their detection.

</details>

<details>

<summary>2017-10-18 17:51:22 - Can Machine Learning Create an Advocate for Foster Youth?</summary>

- *Meredith Brindley, James Heyes, Darrell Booker*

- `1710.06845v1` - [abs](http://arxiv.org/abs/1710.06845v1) - [pdf](http://arxiv.org/pdf/1710.06845v1)

> Statistics are bleak for youth aging out of the United States foster care system. They are often left with few resources, are likely to experience homelessness, and are at increased risk of incarceration and exploitation. The Think of Us platform is a service for foster youth and their advocates to create personalized goals and access curated content specific to aging out of the foster care system. In this paper, we propose the use of a machine learning algorithm within the Think of Us platform to better serve youth transitioning to life outside of foster care. The algorithm collects and collates publicly available figures and data to inform caseworkers and other mentors chosen by the youth on how to best assist foster youth. It can then provide valuable resources for the youth and their advocates targeted directly towards their specific needs. Finally, we examine machine learning as a support system and aid for caseworkers to buttress and protect vulnerable young adults during their transition to adulthood.

</details>

<details>

<summary>2017-10-20 06:47:58 - Self-adaptive static analysis</summary>

- *Eric Bodden*

- `1710.07430v1` - [abs](http://arxiv.org/abs/1710.07430v1) - [pdf](http://arxiv.org/pdf/1710.07430v1)

> Static code analysis is a powerful approach to detect quality deficiencies such as performance bottlenecks, safety violations or security vulnerabilities already during a software system's implementation. Yet, as current software systems continue to grow, current static-analysis systems more frequently face the problem of insufficient scalability. We argue that this is mainly due to the fact that current static analyses are implemented fully manually, often in general-purpose programming languages such as Java or C, or in declarative languages such as Datalog. This design choice predefines the way in which the static analysis evaluates, and limits the optimizations and extensions static-analysis designers can apply.   To boost scalability to a new level, we propose to fuse static-analysis with just-in-time-optimization technology, introducing for the first time static analyses that are managed and inherently self-adaptive. Those analyses automatically adapt themselves to yield a performance/precision tradeoff that is optimal with respect to the analyzed software system and to the analysis itself.   Self-adaptivity is enabled by the novel idea of designing a dedicated intermediate representation, not for the analyzed program but for the analysis itself. This representation allows for an automatic optimization and adaptation of the analysis code, both ahead-of-time (through static analysis of the static analysis) as well as just-in-time during the analysis' execution, similar to just-in-time compilers.

</details>

<details>

<summary>2017-10-23 01:16:25 - Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent</summary>

- *Yudong Chen, Lili Su, Jiaming Xu*

- `1705.05491v2` - [abs](http://arxiv.org/abs/1705.05491v2) - [pdf](http://arxiv.org/pdf/1705.05491v2)

> We consider the problem of distributed statistical machine learning in adversarial settings, where some unknown and time-varying subset of working machines may be compromised and behave arbitrarily to prevent an accurate model from being learned. This setting captures the potential adversarial attacks faced by Federated Learning -- a modern machine learning paradigm that is proposed by Google researchers and has been intensively studied for ensuring user privacy. Formally, we focus on a distributed system consisting of a parameter server and $m$ working machines. Each working machine keeps $N/m$ data samples, where $N$ is the total number of samples. The goal is to collectively learn the underlying true model parameter of dimension $d$.   In classical batch gradient descent methods, the gradients reported to the server by the working machines are aggregated via simple averaging, which is vulnerable to a single Byzantine failure. In this paper, we propose a Byzantine gradient descent method based on the geometric median of means of the gradients. We show that our method can tolerate $q \le (m-1)/2$ Byzantine failures, and the parameter estimate converges in $O(\log N)$ rounds with an estimation error of $\sqrt{d(2q+1)/N}$, hence approaching the optimal error rate $\sqrt{d/N}$ in the centralized and failure-free setting. The total computational complexity of our algorithm is of $O((Nd/m) \log N)$ at each working machine and $O(md + kd \log^3 N)$ at the central server, and the total communication cost is of $O(m d \log N)$. We further provide an application of our general results to the linear regression problem.   A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. We prove that the aggregated gradient converges uniformly to the true gradient function.

</details>

<details>

<summary>2017-10-30 16:25:32 - Measuring the Impact of Urban Street Trees on Air Quality and Respiratory Illness</summary>

- *Yuan Lai, Constantine E. Kontokosta*

- `1710.11046v1` - [abs](http://arxiv.org/abs/1710.11046v1) - [pdf](http://arxiv.org/pdf/1710.11046v1)

> New streams of data enable us to associate physical objects with rich multi-dimensional data on the urban environment. This study presents how open data integration can contribute to deeper insights into urban ecology. We analyze street trees in New York City (NYC) with cross-domain data integration methods by combining crowd-sourced tree census data - which includes geolocation, species, size, and condition of each street tree - with pollen activity and allergen severity, neighborhood demographics, and spatial-temporal data on tree condition from NYC 311 complaints. We further integrate historical data on neighborhood asthma hospitalization rates by Zip Code and in-situ air quality monitoring data (PM 2.5) to investigate how street trees impact local air quality and the prevalence of respiratory illnesses. The results indicate although the number of trees contributes to better air quality, species with severe allergens may increase local asthma hospitalization rates in vulnerable populations.

</details>


## 2017-11

<details>

<summary>2017-11-01 04:07:05 - Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</summary>

- *Nicholas Carlini, David Wagner*

- `1705.07263v2` - [abs](http://arxiv.org/abs/1705.07263v2) - [pdf](http://arxiv.org/pdf/1705.07263v2)

> Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.

</details>

<details>

<summary>2017-11-02 03:55:22 - Security Against Impersonation Attacks in Distributed Systems</summary>

- *Philip N. Brown, Holly Borowski, Jason R. Marden*

- `1711.00609v1` - [abs](http://arxiv.org/abs/1711.00609v1) - [pdf](http://arxiv.org/pdf/1711.00609v1)

> In a multi-agent system, transitioning from a centralized to a distributed decision-making strategy can introduce vulnerability to adversarial manipulation. We study the potential for adversarial manipulation in a class of graphical coordination games where the adversary can pose as a friendly agent in the game, thereby influencing the decision-making rules of a subset of agents. The adversary's influence can cascade throughout the system, indirectly influencing other agents' behavior and significantly impacting the emergent collective behavior. The main results in this paper focus on characterizing conditions under which the adversary's local influence can dramatically impact the emergent global behavior, e.g., destabilize efficient Nash equilibria.

</details>

<details>

<summary>2017-11-02 04:18:44 - ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models</summary>

- *Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh*

- `1708.03999v2` - [abs](http://arxiv.org/abs/1708.03999v2) - [pdf](http://arxiv.org/pdf/1708.03999v2)

> Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs.   Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack and significantly outperforms existing black-box attacks via substitute models.

</details>

<details>

<summary>2017-11-02 16:10:14 - Talos: Neutralizing Vulnerabilities with Security Workarounds for Rapid Response</summary>

- *Zhen Huang, Mariana D'Angelo, Dhaval Miyani, David Lie*

- `1711.00795v1` - [abs](http://arxiv.org/abs/1711.00795v1) - [pdf](http://arxiv.org/pdf/1711.00795v1)

> Considerable delays often exist between the discovery of a vulnerability and the issue of a patch. One way to mitigate this window of vulnerability is to use a configuration workaround, which prevents the vulnerable code from being executed at the cost of some lost functionality -- but only if one is available. Since program configurations are not specifically designed to mitigate software vulnerabilities, we find that they only cover 25.2% of vulnerabilities.   To minimize patch delay vulnerabilities and address the limitations of configuration workarounds, we propose Security Workarounds for Rapid Response (SWRRs), which are designed to neutralize security vulnerabilities in a timely, secure, and unobtrusive manner. Similar to configuration workarounds, SWRRs neutralize vulnerabilities by preventing vulnerable code from being executed at the cost of some lost functionality. However, the key difference is that SWRRs use existing error-handling code within programs, which enables them to be mechanically inserted with minimal knowledge of the program and minimal developer effort. This allows SWRRs to achieve high coverage while still being fast and easy to deploy.   We have designed and implemented Talos, a system that mechanically instruments SWRRs into a given program, and evaluate it on five popular Linux server programs. We run exploits against 11 real-world software vulnerabilities and show that SWRRs neutralize the vulnerabilities in all cases. Quantitative measurements on 320 SWRRs indicate that SWRRs instrumented by Talos can neutralize 75.1% of all potential vulnerabilities and incur a loss of functionality similar to configuration workarounds in 71.3% of those cases. Our overall conclusion is that automatically generated SWRRs can safely mitigate 2.1x more vulnerabilities, while only incurring a loss of functionality comparable to that of traditional configuration workarounds.

</details>

<details>

<summary>2017-11-02 17:28:36 - BinPro: A Tool for Binary Source Code Provenance</summary>

- *Dhaval Miyani, Zhen Huang, David Lie*

- `1711.00830v1` - [abs](http://arxiv.org/abs/1711.00830v1) - [pdf](http://arxiv.org/pdf/1711.00830v1)

> Enforcing open source licenses such as the GNU General Public License (GPL), analyzing a binary for possible vulnerabilities, and code maintenance are all situations where it is useful to be able to determine the source code provenance of a binary. While previous work has either focused on computing binary-to-binary similarity or source-to-source similarity, BinPro is the first work we are aware of to tackle the problem of source-to-binary similarity. BinPro can match binaries with their source code even without knowing which compiler was used to produce the binary, or what optimization level was used with the compiler. To do this, BinPro utilizes machine learning to compute optimal code features for determining binary-to-source similarity and a static analysis pipeline to extract and compute similarity based on those features. Our experiments show that on average BinPro computes a similarity of 81% for matching binaries and source code of the same applications, and an average similarity of 25% for binaries and source code of similar but different applications. This shows that BinPro's similarity score is useful for determining if a binary was derived from a particular source code.

</details>

<details>

<summary>2017-11-02 17:46:22 - A Systems Approach for Eliciting Mission-Centric Security Requirements</summary>

- *Bryan Carter, Georgios Bakirtzis, Carl Elks, Cody Fleming*

- `1711.00838v1` - [abs](http://arxiv.org/abs/1711.00838v1) - [pdf](http://arxiv.org/pdf/1711.00838v1)

> The security of cyber-physical systems is first and foremost a safety problem, yet it is typically handled as a traditional security problem, which means that solutions are based on defending against threats and are often implemented too late. This approach neglects to take into consideration the context in which the system is intended to operate, thus system safety may be compromised. This paper presents a systems-theoretic analysis approach that combines stakeholder perspectives with a modified version of Systems-Theoretic Accident Model and Process (STAMP) that allows decision-makers to strategically enhance the safety, resilience, and security of a cyber-physical system against potential threats. This methodology allows the capture of vital mission-specific information in a model, which then allows analysts to identify and mitigate vulnerabilities in the locations most critical to mission success. We present an overview of the general approach followed by a real example using an unmanned aerial vehicle conducting a reconnaissance mission.

</details>

<details>

<summary>2017-11-03 10:09:05 - Practical Integer Overflow Prevention</summary>

- *Paul Muntean, Jens Grossklags, Claudia Eckert*

- `1710.03720v9` - [abs](http://arxiv.org/abs/1710.03720v9) - [pdf](http://arxiv.org/pdf/1710.03720v9)

> Integer overflows in commodity software are a main source for software bugs, which can result in exploitable memory corruption vulnerabilities and may eventually contribute to powerful software based exploits, i.e., code reuse attacks (CRAs).   In this paper, we present IntGuard , a tool that can repair integer overflows with high-quality source code repairs. Specifically, given the source code of a program, IntGuard first discovers the location of an integer overflow error by using static source code analysis and satisfiability modulo theories (SMT) solving. IntGuard then generates integer multi-precision code repairs based on modular manipulation of SMT constraints as well as an extensible set of customizable code repair patterns.   We have implemented and evaluated IntGuard with 2052 C programs (approx. 1 Mil. LOC) available in the currently largest open- source test suite for C/C++ programs and with a benchmark containing large and complex programs. The evaluation results show that IntGuard can precisely (i.e., no false positives are accidentally repaired), with low computational and runtime overhead repair programs with very small binary and source code blow-up. In a controlled experiment, we show that IntGuard is more time-effective and achieves a higher repair success rate than manually generated code repairs.

</details>

<details>

<summary>2017-11-03 15:04:05 - Design and Analysis of a Secure Three Factor User Authentication Scheme Using Biometric and Smart Card</summary>

- *Hossen Asiful Mustafa, Hasan Muhammad Kafi*

- `1711.01198v1` - [abs](http://arxiv.org/abs/1711.01198v1) - [pdf](http://arxiv.org/pdf/1711.01198v1)

> Password security can no longer provide enough security in the area of remote user authentication. Considering this security drawback, researchers are trying to find solution with multifactor remote user authentication system. Recently, three factor remote user authentication using biometric and smart card has drawn a considerable attention of the researchers. However, most of the current proposed schemes have security flaws. They are vulnerable to attacks like user impersonation attack, server masquerading attack, password guessing attack, insider attack, denial of service attack, forgery attack, etc. Also, most of them are unable to provide mutual authentication, session key agreement and password, or smart card recovery system. Considering these drawbacks, we propose a secure three factor user authentication scheme using biometric and smart card. Through security analysis, we show that our proposed scheme can overcome drawbacks of existing systems and ensure high security in remote user authentication.

</details>

<details>

<summary>2017-11-05 13:46:45 - Inference-Based Similarity Search in Randomized Montgomery Domains for Privacy-Preserving Biometric Identification</summary>

- *Yi Wang, Jianwu Wan, Jun Guo, Yiu-Ming Cheung, Pong C Yuen*

- `1711.01587v1` - [abs](http://arxiv.org/abs/1711.01587v1) - [pdf](http://arxiv.org/pdf/1711.01587v1)

> Similarity search is essential to many important applications and often involves searching at scale on high-dimensional data based on their similarity to a query. In biometric applications, recent vulnerability studies have shown that adversarial machine learning can compromise biometric recognition systems by exploiting the biometric similarity information. Existing methods for biometric privacy protection are in general based on pairwise matching of secured biometric templates and have inherent limitations in search efficiency and scalability. In this paper, we propose an inference-based framework for privacy-preserving similarity search in Hamming space. Our approach builds on an obfuscated distance measure that can conceal Hamming distance in a dynamic interval. Such a mechanism enables us to systematically design statistically reliable methods for retrieving most likely candidates without knowing the exact distance values. We further propose to apply Montgomery multiplication for generating search indexes that can withstand adversarial similarity analysis, and show that information leakage in randomized Montgomery domains can be made negligibly small. Our experiments on public biometric datasets demonstrate that the inference-based approach can achieve a search accuracy close to the best performance possible with secure computation methods, but the associated cost is reduced by orders of magnitude compared to cryptographic primitives.

</details>

<details>

<summary>2017-11-05 20:58:09 - Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation</summary>

- *Matthias Hein, Maksym Andriushchenko*

- `1705.08475v2` - [abs](http://arxiv.org/abs/1705.08475v2) - [pdf](http://arxiv.org/pdf/1705.08475v2)

> Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific lower bounds on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier without any loss in prediction performance.

</details>

<details>

<summary>2017-11-07 13:01:18 - Detection of Wordpress Content Injection Vulnerability</summary>

- *Md. Maruf Hassan, Kaushik Sarker, Saikat Biswas, Md. Hasan Sharif*

- `1711.02447v1` - [abs](http://arxiv.org/abs/1711.02447v1) - [pdf](http://arxiv.org/pdf/1711.02447v1)

> The popularity of content management software (CMS) is growing vastly to the web developers and the business people because of its capacity for easy accessibility, manageability and usability of the distributed website contents. As per the statistics of Built with, 32% of the web applications are developed with WordPress(WP) among all other CMSs [1]. It is obvious that quite a good number of web applications were built with WP in version 4.7.0 and 4.7.1. A recent research reveals that content injection vulnerability was found available in the above two versions of WP [2]. Unauthorized content injection by an intruder in a CMS managed application is one of the serious problems for the business as well as for the web owner.Therefore, detection of the vulnerability becomes a critical issue for this time. In this paper, we have discussed about the root cause of WP content injection of the above versions and have also proposed a detection model for the given vulnerability. A tool, SAISAN has been implemented as per our anticipated model and conducted an examination on 176 WP developed web applications using SAISAN. We achieved the accuracy of 92% of the result of SAISAN as compared to manual black box testing outcome.

</details>

<details>

<summary>2017-11-08 04:43:46 - Probability Risk Identification Based Intrusion Detection System for SCADA Systems</summary>

- *Thomas Marsden, Nour Moustafa, Elena Sitnikova, Gideon Creech*

- `1711.02826v1` - [abs](http://arxiv.org/abs/1711.02826v1) - [pdf](http://arxiv.org/pdf/1711.02826v1)

> . As Supervisory Control and Data Acquisition (SCADA) systems control several critical infrastructures, they have connected to the internet. Consequently, SCADA systems face different sophisticated types of cyber adversaries. This paper suggests a Probability Risk Identification based Intrusion Detection System (PRI-IDS) technique based on analysing network traffic of Modbus TCP/IP for identifying replay attacks. It is acknowledged that Modbus TCP is usually vulnerable due to its unauthenticated and unencrypted nature. Our technique is evaluated using a simulation environment by configuring a testbed, which is a cus- tom SCADA network that is cheap, accurate and scalable. The testbed is exploited when testing the IDS by sending individual packets from an attacker located on the same LAN as the Modbus master and slave. The experimental results demonstrated that the proposed technique can effectively and efficiently recognise replay attacks.

</details>

<details>

<summary>2017-11-08 05:00:42 - Collaborative Anomaly Detection Framework for handling Big Data of Cloud Computing</summary>

- *Nour Moustafa, Gideon Creech, Elena Sitnikova, Marwa Keshk*

- `1711.02829v1` - [abs](http://arxiv.org/abs/1711.02829v1) - [pdf](http://arxiv.org/pdf/1711.02829v1)

> With the ubiquitous computing of providing services and applications at anywhere and anytime, cloud computing is the best option as it offers flexible and pay-per-use based services to its customers. Nevertheless, security and privacy are the main challenges to its success due to its dynamic and distributed architecture, resulting in generating big data that should be carefully analysed for detecting network vulnerabilities. In this paper, we propose a Collaborative Anomaly Detection Framework CADF for detecting cyber attacks from cloud computing environments. We provide the technical functions and deployment of the framework to illustrate its methodology of implementation and installation. The framework is evaluated on the UNSW-NB15 dataset to check its credibility while deploying it in cloud computing environments. The experimental results showed that this framework can easily handle large-scale systems as its implementation requires only estimating statistical measures from network observations. Moreover, the evaluation performance of the framework outperforms three state-of-the-art techniques in terms of false positive rate and detection rate.

</details>

<details>

<summary>2017-11-08 06:54:49 - Intriguing Properties of Adversarial Examples</summary>

- *Ekin D. Cubuk, Barret Zoph, Samuel S. Schoenholz, Quoc V. Le*

- `1711.02846v1` - [abs](http://arxiv.org/abs/1711.02846v1) - [pdf](http://arxiv.org/pdf/1711.02846v1)

> It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we argue that the origin of adversarial examples is primarily due to an inherent uncertainty that neural networks have about their predictions. We show that the functional form of this uncertainty is independent of architecture, dataset, and training protocol; and depends only on the statistics of the logit differences of the network, which do not change significantly during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \emph{and} black box attacks compared to previous attempts.

</details>

<details>

<summary>2017-11-08 18:30:29 - Advanced Analytics for Connected Cars Cyber Security</summary>

- *Matan Levi, Yair Allouche, Aryeh Kontorovich*

- `1711.01939v2` - [abs](http://arxiv.org/abs/1711.01939v2) - [pdf](http://arxiv.org/pdf/1711.01939v2)

> The vehicular connectivity revolution is fueling the automotive industry's most significant transformation seen in decades. However, as modern vehicles become more connected, they also become much more vulnerable to cyber-attacks. In this paper, a fully working machine learning approach is proposed to protect connected vehicles (fleets and individuals) against such attacks. We present a system that monitors different vehicle's interfaces (Network, CAN and OS), extracts relevant information based on configurable rules and sends it to a trained generative model to detect deviations from normal behavior. Using configurable data collector, we provide a higher level of data abstraction as the model is trained based on events instead of raw data, which has a noise-filtering effect and eliminates the need to retrain the model whenever a protocol changes. We present a new approach for detecting anomalies, tailored to the temporal nature of our domain. Adapting the hybrid approach of Gutflaish et al. (2017) to the fully temporal setting, we first train a Hidden Markov Model to learn normal vehicle behavior, and then a regression model to calibrate the likelihood threshold for anomaly. Using this architecture, our method detects sophisticated and realistic anomalies, which are missed by other existing methods monitoring the CAN bus only. We also demonstrate the superiority of adaptive thresholds over static ones. Furthermore, our approach scales efficiently from monitoring individual cars to serving large fleets. We demonstrate the competitive advantage of our model via encouraging empirical results.

</details>

<details>

<summary>2017-11-10 01:29:47 - Not all bytes are equal: Neural byte sieve for fuzzing</summary>

- *Mohit Rajpal, William Blum, Rishabh Singh*

- `1711.04596v1` - [abs](http://arxiv.org/abs/1711.04596v1) - [pdf](http://arxiv.org/pdf/1711.04596v1)

> Fuzzing is a popular dynamic program analysis technique used to find vulnerabilities in complex software. Fuzzing involves presenting a target program with crafted malicious input designed to cause crashes, buffer overflows, memory errors, and exceptions. Crafting malicious inputs in an efficient manner is a difficult open problem and often the best approach to generating such inputs is through applying uniform random mutations to pre-existing valid inputs (seed files). We present a learning technique that uses neural networks to learn patterns in the input files from past fuzzing explorations to guide future fuzzing explorations. In particular, the neural models learn a function to predict good (and bad) locations in input files to perform fuzzing mutations based on the past mutations and corresponding code coverage information. We implement several neural models including LSTMs and sequence-to-sequence models that can encode variable length input files. We incorporate our models in the state-of-the-art AFL (American Fuzzy Lop) fuzzer and show significant improvements in terms of code coverage, unique code paths, and crashes for various input formats including ELF, PNG, PDF, and XML.

</details>

<details>

<summary>2017-11-10 18:21:05 - Manipulative Elicitation -- A New Attack on Elections with Incomplete Preferences</summary>

- *Palash Dey*

- `1711.03948v1` - [abs](http://arxiv.org/abs/1711.03948v1) - [pdf](http://arxiv.org/pdf/1711.03948v1)

> Lu and Boutilier proposed a novel approach based on "minimax regret" to use classical score based voting rules in the setting where preferences can be any partial (instead of complete) orders over the set of alternatives. We show here that such an approach is vulnerable to a new kind of manipulation which was not present in the classical (where preferences are complete orders) world of voting. We call this attack "manipulative elicitation." More specifically, it may be possible to (partially) elicit the preferences of the agents in a way that makes some distinguished alternative win the election who may not be a winner if we elicit every preference completely. More alarmingly, we show that the related computational task is polynomial time solvable for a large class of voting rules which includes all scoring rules, maximin, Copeland$^\alpha$ for every $\alpha\in[0,1]$, simplified Bucklin voting rules, etc. We then show that introducing a parameter per pair of alternatives which specifies the minimum number of partial preferences where this pair of alternatives must be comparable makes the related computational task of manipulative elicitation \NPC for all common voting rules including a class of scoring rules which includes the plurality, $k$-approval, $k$-veto, veto, and Borda voting rules, maximin, Copeland$^\alpha$ for every $\alpha\in[0,1]$, and simplified Bucklin voting rules. Hence, in this work, we discover a fundamental vulnerability in using minimax regret based approach in partial preferential setting and propose a novel way to tackle it.

</details>

<details>

<summary>2017-11-15 23:31:59 - Detecting Adversarial Samples from Artifacts</summary>

- *Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, Andrew B. Gardner*

- `1703.00410v3` - [abs](http://arxiv.org/abs/1703.00410v3) - [pdf](http://arxiv.org/pdf/1703.00410v3)

> Deep neural networks (DNNs) are powerful nonlinear architectures that are known to be robust to random perturbations of the input. However, these models are vulnerable to adversarial perturbations--small input changes crafted explicitly to fool the model. In this paper, we ask whether a DNN can distinguish adversarial samples from their normal and noisy counterparts. We investigate model confidence on adversarial samples by looking at Bayesian uncertainty estimates, available in dropout neural networks, and by performing density estimation in the subspace of deep features learned by the model. The result is a method for implicit adversarial detection that is oblivious to the attack algorithm. We evaluate this method on a variety of standard datasets including MNIST and CIFAR-10 and show that it generalizes well across different architectures and attacks. Our findings report that 85-93% ROC-AUC can be achieved on a number of standard classification tasks with a negative class that consists of both normal and noisy samples.

</details>

<details>

<summary>2017-11-17 09:27:02 - Security Issues in Controller Area Networks in Automobiles</summary>

- *Robert Buttigieg, Mario Farrugia, Clyde Meli*

- `1711.05824v2` - [abs](http://arxiv.org/abs/1711.05824v2) - [pdf](http://arxiv.org/pdf/1711.05824v2)

> Modern vehicles may contain a considerable number of ECUs (Electronic Control Units) which are connected through various means of communication, with the CAN (Controller Area Network) protocol being the most widely used. However, several vulnerabilities such as the lack of authentication and the lack of data encryption have been pointed out by several authors, which ultimately render vehicles unsafe to their users and surroundings. Moreover, the lack of security in modern automobiles has been studied and analyzed by other researchers as well as several reports about modern car hacking have (already) been published. The contribution of this work aimed to analyze and test the level of security and how resilient is the CAN protocol by taking a BMW E90 (3-series) instrument cluster as a sample for a proof of concept study. This investigation was carried out by building and developing a rogue device using cheap commercially available components while being connected to the same CAN-Bus as a man in the middle device in order to send spoofed messages to the instrument cluster.

</details>

<details>

<summary>2017-11-20 03:09:07 - Art of singular vectors and universal adversarial perturbations</summary>

- *Valentin Khrulkov, Ivan Oseledets*

- `1709.03582v2` - [abs](http://arxiv.org/abs/1709.03582v2) - [pdf](http://arxiv.org/pdf/1709.03582v2)

> Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been attracting a lot of attention in recent studies. It has been shown that for many state of the art DNNs performing image classification there exist universal adversarial perturbations --- image-agnostic perturbations mere addition of which to natural images with high probability leads to their misclassification. In this work we propose a new algorithm for constructing such universal perturbations. Our approach is based on computing the so-called $(p, q)$-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns, and by using only 64 images we were able to construct universal perturbations with more than 60 \% fooling rate on the dataset consisting of 50000 images. We also investigate a correlation between the maximal singular value of the Jacobian matrix and the fooling rate of the corresponding singular vector, and show that the constructed perturbations generalize across networks.

</details>

<details>

<summary>2017-11-22 12:02:53 - Adversarial Phenomenon in the Eyes of Bayesian Deep Learning</summary>

- *Ambrish Rawat, Martin Wistuba, Maria-Irina Nicolae*

- `1711.08244v1` - [abs](http://arxiv.org/abs/1711.08244v1) - [pdf](http://arxiv.org/pdf/1711.08244v1)

> Deep Learning models are vulnerable to adversarial examples, i.e.\ images obtained via deliberate imperceptible perturbations, such that the model misclassifies them with high confidence. However, class confidence by itself is an incomplete picture of uncertainty. We therefore use principled Bayesian methods to capture model uncertainty in prediction for observing adversarial misclassification. We provide an extensive study with different Bayesian neural networks attacked in both white-box and black-box setups. The behaviour of the networks for noise, attacks and clean test data is compared. We observe that Bayesian neural networks are uncertain in their predictions for adversarial perturbations, a behaviour similar to the one observed for random Gaussian perturbations. Thus, we conclude that Bayesian neural networks can be considered for detecting adversarial examples.

</details>

<details>

<summary>2017-11-23 04:05:28 - Key management system for WSNs based on hash functions and elliptic curve cryptography</summary>

- *Hamzeh Ghasemzadeh, Ali Payandeh, Mohammad Reza Aref*

- `1711.08570v1` - [abs](http://arxiv.org/abs/1711.08570v1) - [pdf](http://arxiv.org/pdf/1711.08570v1)

> Due to hostile environment and wireless communication channel, security mechanisms are essential for wireless sensor networks (WSNs). Existence of a pair of shared key is a prerequisite for many of these security mechanisms; a task that key management system addresses. Recently, an energy efficient method based on public key cryptography (PKC) was proposed. We analyze this protocol and show that it is vulnerable to denial of service (DOS) attacks and adversary can exhaust memory and battery of nodes. Then, we analyze this protocol and show that using a more knowledgeable BS this vulnerability can be solved very efficiently. Based on this observation we propose a modified version of the protocol that achieves immediate authentication and can prevent DOS attacks. We show that the improved protocol achieves immediate authentication at the expense of 1.82 mj extra energy consumption while retaining other desirable characteristics of the basic method.

</details>

<details>

<summary>2017-11-24 19:32:57 - Geometric robustness of deep networks: analysis and improvement</summary>

- *Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard*

- `1711.09115v1` - [abs](http://arxiv.org/abs/1711.09115v1) - [pdf](http://arxiv.org/pdf/1711.09115v1)

> Deep convolutional neural networks have been shown to be vulnerable to arbitrary geometric transformations. However, there is no systematic method to measure the invariance properties of deep networks to such transformations. We propose ManiFool as a simple yet scalable algorithm to measure the invariance of deep networks. In particular, our algorithm measures the robustness of deep networks to geometric transformations in a worst-case regime as they can be problematic for sensitive applications. Our extensive experimental results show that ManiFool can be used to measure the invariance of fairly complex networks on high dimensional datasets and these values can be used for analyzing the reasons for it. Furthermore, we build on Manifool to propose a new adversarial training scheme and we show its effectiveness on improving the invariance properties of deep neural networks.

</details>

<details>

<summary>2017-11-26 03:05:42 - Designing Secure Ethereum Smart Contracts: A Finite State Machine Based Approach</summary>

- *Anastasia Mavridou, Aron Laszka*

- `1711.09327v1` - [abs](http://arxiv.org/abs/1711.09327v1) - [pdf](http://arxiv.org/pdf/1711.09327v1)

> The adoption of blockchain-based distributed computation platforms is growing fast. Some of these platforms, such as Ethereum, provide support for implementing smart contracts, which are envisioned to have novel applications in a broad range of areas, including finance and Internet-of-Things. However, a significant number of smart contracts deployed in practice suffer from security vulnerabilities, which enable malicious users to steal assets from a contract or to cause damage. Vulnerabilities present a serious issue since contracts may handle financial assets of considerable value, and contract bugs are non-fixable by design. To help developers create more secure smart contracts, we introduce FSolidM, a framework rooted in rigorous semantics for designing con- tracts as Finite State Machines (FSM). We present a tool for creating FSM on an easy-to-use graphical interface and for automatically generating Ethereum contracts. Further, we introduce a set of design patterns, which we implement as plugins that developers can easily add to their contracts to enhance security and functionality.

</details>

<details>

<summary>2017-11-27 16:04:07 - The Status of Quantum-Based Long-Term Secure Communication over the Internet</summary>

- *Matthias Geihs, Oleg Nikiforov, Denise Demirel, Alexander Sauer, Denis Butin, Felix Günther, Gernot Alber, Thomas Walther, Johannes Buchmann*

- `1711.09793v1` - [abs](http://arxiv.org/abs/1711.09793v1) - [pdf](http://arxiv.org/pdf/1711.09793v1)

> Sensitive digital data, such as health information or governmental archives, are often stored for decades or centuries. The processing of such data calls for long-term security. Secure channels on the Internet require robust key establishment methods. Currently used key distribution protocols are either vulnerable to future attacks based on Shor's algorithm, or vulnerable in principle due to their reliance on computational problems. Quantum-based key distribution protocols are information-theoretically secure and offer long-term security. However, significant obstacles to their real-world use remain. This paper, which results from a multidisciplinary project involving computer scientists and physicists, systematizes knowledge about obstacles to and strategies for the realization of long-term secure Internet communication from quantum-based key distribution. We discuss performance and security particulars, consider the specific challenges arising from multi-user network settings, and identify key challenges for actual deployment.

</details>

<details>

<summary>2017-11-28 08:53:39 - A Novel Approach for Security Situational Awareness in the Internet of Things</summary>

- *Fannv He, Yuqing Zhang, Huizheng Liu*

- `1711.10182v1` - [abs](http://arxiv.org/abs/1711.10182v1) - [pdf](http://arxiv.org/pdf/1711.10182v1)

> Internet of Things (IoT) is characterized by various of heterogeneous devices and facing numerous threats. Modeling security of IoT is still a certain challenge. This paper defines a Stochastic Colored Petri Net (SCPN) for IoT-based smart environment and then proposes a Markov Game model for security situational awareness (SSA) in the defined SCPN. All possible attack paths are computed by the SCPN, and antagonistic behavior of both attackers and defenders are taken into consideration dynamically according to Game Theory. Two attack scenarios in smart home environment are taken into consideration to demonstrate the effectiveness of the proposed model. The proposed model can form a macroscopic trend curve of security situation. Analysis of the results shows the capabilities of the proposed model in finding vulnerable devices and potential attack paths, and even mitigating the impact of attacks. To our knowledge, this is the first attempt to establish a dynamic SSA model for a complex IoT-based smart environment.

</details>

<details>

<summary>2017-11-29 02:39:52 - Cybersecurity Attacks in Vehicle-to-Infrastructure (V2I) Applications and their Prevention</summary>

- *Mhafuzul Islam, Mashrur Chowdhury, Hongda Li, Hongxin Hu*

- `1711.10651v1` - [abs](http://arxiv.org/abs/1711.10651v1) - [pdf](http://arxiv.org/pdf/1711.10651v1)

> A connected vehicle (CV) environment is composed of a diverse data collection, data communication and dissemination, and computing infrastructure systems that are vulnerable to the same cyberattacks as all traditional computing environments. Cyberattacks can jeopardize the expected safety, mobility, energy, and environmental benefits from connected vehicle applications. As cyberattacks can lead to severe traffic incidents, it has become one of the primary concerns in connected vehicle applications. In this paper, we investigate the impact of cyberattacks on the vehicle-to-infrastructure (V2I) network from a V2I application point of view. Then, we develop a novel V2I cybersecurity architecture, named CVGuard, which can detect and prevent cyberattacks on the V2I environment. In designing CVGuard, key challenges, such as scalability, resiliency and future usability were considered. A case study using a distributed denial of service (DDoS) on a V2I application, i.e., the Stop Sign Gap Assist (SSGA) application, shows that CVGuard was effective in mitigating the adverse effects created by a DDoS attack. In our case study, because of the DDoS attack, conflicts between the minor and major road vehicles occurred in an unsignalized intersection, which could have caused potential crashes. A reduction of conflicts between vehicles occurred because CVGuard was in operation. The reduction of conflicts was compared based on the number of conflicts before and after the implementation and operation of the CVGuard security platform. Analysis revealed that the strategies adopted by the CVGuard were successful in reducing the inter-vehicle conflicts by 60% where a DDoS attack compromised the SSGA application at an unsignalized intersection.

</details>

<details>

<summary>2017-11-29 18:33:27 - Security Risks in Deep Learning Implementations</summary>

- *Qixue Xiao, Kang Li, Deyue Zhang, Weilin Xu*

- `1711.11008v1` - [abs](http://arxiv.org/abs/1711.11008v1) - [pdf](http://arxiv.org/pdf/1711.11008v1)

> Advance in deep learning algorithms overshadows their security risk in software implementations. This paper discloses a set of vulnerabilities in popular deep learning frameworks including Caffe, TensorFlow, and Torch. Contrast to the small code size of deep learning models, these deep learning frameworks are complex and contain heavy dependencies on numerous open source packages. This paper considers the risks caused by these vulnerabilities by studying their impact on common deep learning applications such as voice recognition and image classifications. By exploiting these framework implementations, attackers can launch denial-of-service attacks that crash or hang a deep learning application, or control-flow hijacking attacks that cause either system compromise or recognition evasions. The goal of this paper is to draw attention on the software implementations and call for the community effort to improve the security of deep learning frameworks.

</details>

<details>

<summary>2017-11-30 02:06:47 - Neural Response Generation with Dynamic Vocabularies</summary>

- *Yu Wu, Wei Wu, Dejian Yang, Can Xu, Zhoujun Li, Ming Zhou*

- `1711.11191v1` - [abs](http://arxiv.org/abs/1711.11191v1) - [pdf](http://arxiv.org/pdf/1711.11191v1)

> We study response generation for open domain conversation in chatbots. Existing methods assume that words in responses are generated from an identical vocabulary regardless of their inputs, which not only makes them vulnerable to generic patterns and irrelevant noise, but also causes a high cost in decoding. We propose a dynamic vocabulary sequence-to-sequence (DVS2S) model which allows each input to possess their own vocabulary in decoding. In training, vocabulary construction and response generation are jointly learned by maximizing a lower bound of the true objective with a Monte Carlo sampling method. In inference, the model dynamically allocates a small vocabulary for an input with the word prediction model, and conducts decoding only with the small vocabulary. Because of the dynamic vocabulary mechanism, DVS2S eludes many generic patterns and irrelevant words in generation, and enjoys efficient decoding at the same time. Experimental results on both automatic metrics and human annotations show that DVS2S can significantly outperform state-of-the-art methods in terms of response quality, but only requires 60% decoding time compared to the most efficient baseline.

</details>


## 2017-12

<details>

<summary>2017-12-02 06:18:49 - Where Classification Fails, Interpretation Rises</summary>

- *Chanh Nguyen, Georgi Georgiev, Yujie Ji, Ting Wang*

- `1712.00558v1` - [abs](http://arxiv.org/abs/1712.00558v1) - [pdf](http://arxiv.org/pdf/1712.00558v1)

> An intriguing property of deep neural networks is their inherent vulnerability to adversarial inputs, which significantly hinders their application in security-critical domains. Most existing detection methods attempt to use carefully engineered patterns to distinguish adversarial inputs from their genuine counterparts, which however can often be circumvented by adaptive adversaries. In this work, we take a completely different route by leveraging the definition of adversarial inputs: while deceiving for deep neural networks, they are barely discernible for human visions. Building upon recent advances in interpretable models, we construct a new detection framework that contrasts an input's interpretation against its classification. We validate the efficacy of this framework through extensive experiments using benchmark datasets and attacks. We believe that this work opens a new direction for designing adversarial input detection methods.

</details>

<details>

<summary>2017-12-04 01:43:41 - Vulnerabilities in the use of similarity tables in combination with pseudonymisation to preserve data privacy in the UK Office for National Statistics' Privacy-Preserving Record Linkage</summary>

- *Chris Culnane, Benjamin I. P. Rubinstein, Vanessa Teague*

- `1712.00871v1` - [abs](http://arxiv.org/abs/1712.00871v1) - [pdf](http://arxiv.org/pdf/1712.00871v1)

> In the course of a survey of privacy-preserving record linkage, we reviewed the approach taken by the UK Office for National Statistics (ONS) as described in their series of reports "Beyond 2011". Our review identifies a number of matters of concern. Some of the issues discovered are sufficiently severe to present a risk to privacy.

</details>

<details>

<summary>2017-12-05 02:30:05 - MISSION AWARE: Evidence-Based, Mission-Centric Cybersecurity Analysis</summary>

- *Georgios Bakirtzis, Bryan T. Carter, Cody H. Fleming, Carl R. Elks*

- `1712.01448v1` - [abs](http://arxiv.org/abs/1712.01448v1) - [pdf](http://arxiv.org/pdf/1712.01448v1)

> Currently, perimeter-based approaches are the mainstay of cybersecurity. While this paradigm is necessary, there is mounting evidence of its insufficiency with respect to sophisticated and coordinated attacks. In contrast to perimeter-based security, mission-centric cybersecurity provides awareness of how attacks can influence mission success and therefore focuses resources for mitigating vulnerabilities and protecting critical assets. This is strategic as opposed to tactical perimeter-based cybersecurity. We propose MISSION AWARE, which assists in the identification of parts of a system that destabilize the overall mission of the system if compromised. MSSION AWARE starts with a structured elicitation process that leads to hazards analysis. It employs hierarchical modeling methods to capture mission requirements, admissible functional behaviors, and system architectures. It then generates evidence---attacks applicable to elements that directly correlate with mission success. Finally, MISSION AWARE traces evidence back to mission requirements to determine the evidence with the highest impact relative to mission objectives.

</details>

<details>

<summary>2017-12-05 13:13:29 - Memory-based Combination PUFs for Device Authentication in Embedded Systems</summary>

- *Soubhagya Sutar, Arnab Raha, Vijay Raghunathan*

- `1712.01611v1` - [abs](http://arxiv.org/abs/1712.01611v1) - [pdf](http://arxiv.org/pdf/1712.01611v1)

> Embedded systems play a crucial role in fueling the growth of the Internet-of-Things (IoT) in application domains such as healthcare, home automation, transportation, etc. However, their increasingly network-connected nature, coupled with their ability to access potentially sensitive/confidential information, has given rise to many security and privacy concerns. An additional challenge is the growing number of counterfeit components in these devices, resulting in serious reliability and financial implications. Physically Unclonable Functions (PUFs) are a promising security primitive to help address these concerns. Memory-based PUFs are particularly attractive as they require minimal or no additional hardware for their operation. However, current memory-based PUFs utilize only a single memory technology for constructing the PUF, which has several disadvantages including making them vulnerable to security attacks. In this paper, we propose the design of a new memory-based combination PUF that intelligently combines two memory technologies, SRAM and DRAM, to overcome these shortcomings. The proposed combination PUF exhibits high entropy, supports a large number of challenge-response pairs, and is intrinsically reconfigurable. We have implemented the proposed combination PUF using a Terasic TR4-230 FPGA board and several off-the-shelf SRAMs and DRAMs. Experimental results demonstrate substantial improvements over current memory-based PUFs including the ability to resist various attacks. Extensive authentication tests across a wide temperature range (20 - 60 deg. Celsius) and accelerated aging (12 months) demonstrate the robustness of the proposed design, which achieves a 100% true-positive rate and 0% false-positive rate for authentication across these parameter ranges.

</details>

<details>

<summary>2017-12-05 21:18:20 - Ghanaian Consumers Online Privacy Concerns: Causes and its Effects on E-Commerce Adoption</summary>

- *E. T. Tchao, Kwasi Diawuo, Christiana Selorm Aggor, Seth Djane Kotey*

- `1801.01086v1` - [abs](http://arxiv.org/abs/1801.01086v1) - [pdf](http://arxiv.org/pdf/1801.01086v1)

> Online privacy has gradually become a concern for internet users over the years as a result of the interconnection of customers devices with other devices supporting the internet technology. This research investigates and discusses the factors that influence the privacy concerns faced by online consumers of internet services and the possible outcomes of these privacy concerns on the African online market with Ghana being the primary focus. Results from this study indicated that only 10.1 percent of respondents felt that the internet was safe for purchase and payment transaction in Ghana. However, respondents were willing to shop online if e-Commerce was the only means of getting their products. Respondents also had a high sense of perceived vulnerability and their perceived vulnerability to unauthorized data collection and misuse of personal information could affect Ghanaian e-Commerce platform adoption. The perceived ability of users of e-Commerce platforms in Ghana to control data collection and its subsequent use by other third parties was also found to negatively impact customers willingness to wholly transact and share their personal information online. The perceived vulnerability was found to be affected by the high levels of internet illiteracy whiles the perceived ability to control the collection of information and use was influenced by both the internet literacy level as well as the level of social awareness of the Ghanaian internet consumer.

</details>

<details>

<summary>2017-12-08 10:02:57 - Privacy Risks from Public Data Sources</summary>

- *Zacharias Tzermias, Panagiotis Papadopoulos, Sotiris Ioannidis, Vassilis Prevelakis*

- `1711.09260v2` - [abs](http://arxiv.org/abs/1711.09260v2) - [pdf](http://arxiv.org/pdf/1711.09260v2)

> In the fight against tax evaders and other cheats, governments seek to gather more information about their citizens. In this paper we claim that this increased transparency, combined with ineptitude, or corruption, can lead to widespread violations of privacy, ultimately harming law-abiding individuals while helping those engaged in criminal activities such as stalking, identity theft and so on. In this paper we survey a number of data sources administrated by the Greek state, offered as web services, to investigate whether they can lead to leakage of sensitive information. Our study shows that we were able to download significant portions of the data stored in some of these data sources (scraping). Moreover, for those data sources that were not amenable to scraping we looked at ways of extracting information for specific individuals that we had identified by looking at other data sources. The vulnerabilities we have discovered enable the collection of personal data and, thus, open the way for a variety of impersonation attacks, identity theft, confidence trickster attacks and so on. We believe that the lack of a big picture which was caused by the piecemeal development of these data sources hides the true extent of the threat. Hence, by looking at all these data sources together, we outline a number of mitigation strategies that can alleviate some of the most obvious attack strategies. Finally, we look at measures that can be taken in the longer term to safeguard the privacy of the citizens.

</details>

<details>

<summary>2017-12-12 12:34:04 - Improving Function Coverage with Munch: A Hybrid Fuzzing and Directed Symbolic Execution Approach</summary>

- *Saahil Ognawala, Thomas Hutzelmann, Eirini Psallida, Alexander Pretschner*

- `1711.09362v2` - [abs](http://arxiv.org/abs/1711.09362v2) - [pdf](http://arxiv.org/pdf/1711.09362v2)

> Fuzzing and symbolic execution are popular techniques for finding vulnerabilities and generating test-cases for programs. Fuzzing, a blackbox method that mutates seed input values, is generally incapable of generating diverse inputs that exercise all paths in the program. Due to the path-explosion problem and dependence on SMT solvers, symbolic execution may also not achieve high path coverage. A hybrid technique involving fuzzing and symbolic execution may achieve better function coverage than fuzzing or symbolic execution alone. In this paper, we present Munch, an open source framework implementing two hybrid techniques based on fuzzing and symbolic execution. We empirically show using nine large open-source programs that overall, Munch achieves higher (in-depth) function coverage than symbolic execution or fuzzing alone. Using metrics based on total analyses time and number of queries issued to the SMT solver, we also show that Munch is more efficient at achieving better function coverage.

</details>

<details>

<summary>2017-12-14 19:00:57 - DANCin SEQ2SEQ: Fooling Text Classifiers with Adversarial Text Example Generation</summary>

- *Catherine Wong*

- `1712.05419v1` - [abs](http://arxiv.org/abs/1712.05419v1) - [pdf](http://arxiv.org/pdf/1712.05419v1)

> Machine learning models are powerful but fallible. Generating adversarial examples - inputs deliberately crafted to cause model misclassification or other errors - can yield important insight into model assumptions and vulnerabilities. Despite significant recent work on adversarial example generation targeting image classifiers, relatively little work exists exploring adversarial example generation for text classifiers; additionally, many existing adversarial example generation algorithms require full access to target model parameters, rendering them impractical for many real-world attacks. In this work, we introduce DANCin SEQ2SEQ, a GAN-inspired algorithm for adversarial text example generation targeting largely black-box text classifiers. We recast adversarial text example generation as a reinforcement learning problem, and demonstrate that our algorithm offers preliminary but promising steps towards generating semantically meaningful adversarial text examples in a real-world attack scenario.

</details>

<details>

<summary>2017-12-16 07:42:46 - CycleGAN, a Master of Steganography</summary>

- *Casey Chu, Andrey Zhmoginov, Mark Sandler*

- `1712.02950v2` - [abs](http://arxiv.org/abs/1712.02950v2) - [pdf](http://arxiv.org/pdf/1712.02950v2)

> CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a transformation between two image distributions. In a series of experiments, we demonstrate an intriguing property of the model: CycleGAN learns to "hide" information about a source image into the images it generates in a nearly imperceptible, high-frequency signal. This trick ensures that the generator can recover the original sample and thus satisfy the cyclic consistency requirement, while the generated image remains realistic. We connect this phenomenon with adversarial attacks by viewing CycleGAN's training procedure as training a generator of adversarial examples and demonstrate that the cyclic consistency loss causes CycleGAN to be especially vulnerable to adversarial attacks.

</details>

<details>

<summary>2017-12-16 08:22:34 - Attack and Defense of Dynamic Analysis-Based, Adversarial Neural Malware Classification Models</summary>

- *Jack W. Stokes, De Wang, Mady Marinescu, Marc Marino, Brian Bussone*

- `1712.05919v1` - [abs](http://arxiv.org/abs/1712.05919v1) - [pdf](http://arxiv.org/pdf/1712.05919v1)

> Recently researchers have proposed using deep learning-based systems for malware detection. Unfortunately, all deep learning classification systems are vulnerable to adversarial attacks. Previous work has studied adversarial attacks against static analysis-based malware classifiers which only classify the content of the unknown file without execution. However, since the majority of malware is either packed or encrypted, malware classification based on static analysis often fails to detect these types of files. To overcome this limitation, anti-malware companies typically perform dynamic analysis by emulating each file in the anti-malware engine or performing in-depth scanning in a virtual machine. These strategies allow the analysis of the malware after unpacking or decryption. In this work, we study different strategies of crafting adversarial samples for dynamic analysis. These strategies operate on sparse, binary inputs in contrast to continuous inputs such as pixels in images. We then study the effects of two, previously proposed defensive mechanisms against crafted adversarial samples including the distillation and ensemble defenses. We also propose and evaluate the weight decay defense. Experiments show that with these three defensive strategies, the number of successfully crafted adversarial samples is reduced compared to a standard baseline system without any defenses. In particular, the ensemble defense is the most resilient to adversarial attacks. Importantly, none of the defenses significantly reduce the classification accuracy for detecting malware. Finally, we demonstrate that while adding additional hidden layers to neural models does not significantly improve the malware classification accuracy, it does significantly increase the classifier's robustness to adversarial attacks.

</details>

<details>

<summary>2017-12-18 07:41:17 - TorPolice: Towards Enforcing Service-Defined Access Policies in Anonymous Systems</summary>

- *Zhuotao Liu, Yushan Liu, Philipp Winter, Prateek Mittal, Yih-Chun Hu*

- `1708.08162v3` - [abs](http://arxiv.org/abs/1708.08162v3) - [pdf](http://arxiv.org/pdf/1708.08162v3)

> Tor is the most widely used anonymity network, currently serving millions of users each day. However, there is no access control in place for all these users, leaving the network vulnerable to botnet abuse and attacks. For example, criminals frequently use exit relays as stepping stones for attacks, causing service providers to serve CAPTCHAs to exit relay IP addresses or blacklisting them altogether, which leads to severe usability issues for legitimate Tor users. To address this problem, we propose TorPolice, the first privacy-preserving access control framework for Tor. TorPolice enables abuse-plagued service providers such as Yelp to enforce access rules to police and throttle malicious requests coming from Tor while still providing service to legitimate Tor users. Further, TorPolice equips Tor with global access control for relays, enhancing Tor's resilience to botnet abuse. We show that TorPolice preserves the privacy of Tor users, implement a prototype of TorPolice, and perform extensive evaluations to validate our design goals.

</details>

<details>

<summary>2017-12-20 12:08:50 - Rethinking Split Manufacturing: An Information-Theoretic Approach with Secure Layout Techniques</summary>

- *Abhrajit Sengupta, Satwik Patnaik, Johann Knechtel, Mohammed Ashraf, Siddharth Garg, Ozgur Sinanoglu*

- `1710.02026v3` - [abs](http://arxiv.org/abs/1710.02026v3) - [pdf](http://arxiv.org/pdf/1710.02026v3)

> Split manufacturing is a promising technique to defend against fab-based malicious activities such as IP piracy, overbuilding, and insertion of hardware Trojans. However, a network flow-based proximity attack, proposed by Wang et al. (DAC'16) [1], has demonstrated that most prior art on split manufacturing is highly vulnerable. Here in this work, we present two practical layout techniques towards secure split manufacturing: (i) gate-level graph coloring and (ii) clustering of same-type gates. Our approach shows promising results against the advanced proximity attack, lowering its success rate by 5.27x, 3.19x, and 1.73x on average compared to the unprotected layouts when splitting at metal layers M1, M2, and M3, respectively. Also, it largely outperforms previous defense efforts; we observe on average 8x higher resilience when compared to representative prior art. At the same time, extensive simulations on ISCAS'85 and MCNC benchmarks reveal that our techniques incur an acceptable layout overhead. Apart from this empirical study, we provide---for the first time---a theoretical framework for quantifying the layout-level resilience against any proximity-induced information leakage. Towards this end, we leverage the notion of mutual information and provide extensive results to validate our model.

</details>

<details>

<summary>2017-12-20 19:04:50 - PKC-PC: A Variant of the McEliece Public Key Cryptosystem based on Polar Codes</summary>

- *Reza Hooshmand, Masoumeh Koochak Shooshtari, Mohammad Reza Aref*

- `1712.07672v1` - [abs](http://arxiv.org/abs/1712.07672v1) - [pdf](http://arxiv.org/pdf/1712.07672v1)

> Polar codes are novel and efficient error correcting codes with low encoding and decoding complexities. These codes have a channel dependent generator matrix which is determined by the code dimension, code length and transmission channel parameters. This paper studies a variant of the McEliece public key cryptosystem based on polar codes, called "PKC-PC". Due to the fact that the structure of polar codes' generator matrix depends on the parameters of channel, we used an efficient approach to conceal their generator matrix. Then, by the help of the characteristics of polar codes and also introducing an efficient approach, we reduced the public and private key sizes of the PKC-PC and increased its information rate compared to the McEliece cryptosystem. It was shown that polar codes are able to yield an increased security level against conventional attacks and possible vulnerabilities on the code-based public key cryptosystems. Moreover, it is indicated that the security of the PKC-PC is reduced to solve NP-complete problems. Compared to other post-quantum public key schemes, we believe that the PKC-PC is a promising candidate for NIST post-quantum crypto standardization.

</details>

<details>

<summary>2017-12-20 20:19:37 - Intel SGX Enabled Key Manager Service with OpenStack Barbican</summary>

- *Somnath Chakrabarti, Brandon Baker, Mona Vij*

- `1712.07694v1` - [abs](http://arxiv.org/abs/1712.07694v1) - [pdf](http://arxiv.org/pdf/1712.07694v1)

> Protecting data in the cloud continues to gain in importance, with encryption being used to achieve the desired data protection. While there is desire to use encryption, various cloud components do not want to deal with key management, which points to a strong need for a separate key management system. OpenStack Barbican is a platform developed by the OpenStack community aimed at providing cryptographic functions useful for all environments, including large ephemeral clouds. Barbican exposes REST APIs designed for the secure storage, provisioning and management of secrets such as passwords, encryption keys, and X.509 certificates, and supports plugins for a variety of crypto solutions in the backend. Crypto plugins store secrets as encrypted blobs within the Barbican database. Software based crypto plugins offer a scalable solution, but are vulnerable to system software attacks. Hardware Security Module or HSM plugins offer strong security guarantees, but they are expensive and don't scale well. We propose to build an Intel Software Guard Extension or SGX based software crypto plugin that offers security similar to an HSM with the low cost and scalability of a software based solution. We extend OpenStack Barbican API to support attestation of an Intel SGX crypto plugin, to allow clients higher confidence in the software they are using for storing keys. In addition, the API provides support for mutual attestation for Intel SGX enabled clients, multi-user key distribution, and extensions for protecting the confidentiality and integrity of the backend database.

</details>

<details>

<summary>2017-12-21 02:05:25 - Bit-Vector Model Counting using Statistical Estimation</summary>

- *Seonmo Kim, Stephen McCamant*

- `1712.07770v1` - [abs](http://arxiv.org/abs/1712.07770v1) - [pdf](http://arxiv.org/pdf/1712.07770v1)

> Approximate model counting for bit-vector SMT formulas (generalizing \#SAT) has many applications such as probabilistic inference and quantitative information-flow security, but it is computationally difficult. Adding random parity constraints (XOR streamlining) and then checking satisfiability is an effective approximation technique, but it requires a prior hypothesis about the model count to produce useful results. We propose an approach inspired by statistical estimation to continually refine a probabilistic estimate of the model count for a formula, so that each XOR-streamlined query yields as much information as possible. We implement this approach, with an approximate probability model, as a wrapper around an off-the-shelf SMT solver or SAT solver. Experimental results show that the implementation is faster than the most similar previous approaches which used simpler refinement strategies. The technique also lets us model count formulas over floating-point constraints, which we demonstrate with an application to a vulnerability in differential privacy mechanisms.

</details>

<details>

<summary>2017-12-21 18:54:36 - A ReRAM Physically Unclonable Function (ReRAM PUF)-based Approach to Enhance Authentication Security in Software Defined Wireless Networks</summary>

- *Fatemeh Afghah, Bertrand Cambou, Masih Abedini, Sherali Zeadally*

- `1712.09916v1` - [abs](http://arxiv.org/abs/1712.09916v1) - [pdf](http://arxiv.org/pdf/1712.09916v1)

> The exponentially increasing number of ubiquitous wireless devices connected to the Internet in Internet of Things (IoT) networks highlights the need for a new paradigm of data flow management in such large-scale networks under software defined wireless networking (SDWN). The limited power and computation capability available at IoT devices as well as the centralized management and decision-making approach in SDWN introduce a whole new set of security threats to the networks. In particular, the authentication mechanism between the controllers and the forwarding devices in SDWNs is a key challenge from both secrecy and integrity aspects. Conventional authentication protocols based on public key infrastructure (PKI) are no longer sufficient for these networks considering the large-scale and heterogeneity nature of the networks as well as their deployment cost, and security vulnerabilities due to key distribution and storage. We propose a novel security protocol based on physical unclonable functions (PUFs) known as hardware security primitives to enhance the authentication security in SDWNs. In this approach, digital PUFs are developed using the inherent randomness of the nanomaterials of Resistive Random Access Memory (ReRAM) that are embedded in most IoT devices to enable a secure authentication and access control in these networks. These PUFs are developed based on a novel approach of multi-states, in which the natural drifts due to the physical variations in the environment are predicted to reduce the potential errors in challenge-response pairs of PUFs being tested in different situations. We also proposed a PUF-based PKI protocol to secure the controller in SDWNs. The performance of the developed ReRAM-based PUFs are evaluated in the experimental results.

</details>

<details>

<summary>2017-12-21 23:24:37 - ReabsNet: Detecting and Revising Adversarial Examples</summary>

- *Jiefeng Chen, Zihang Meng, Changtian Sun, Wei Tang, Yinglun Zhu*

- `1712.08250v1` - [abs](http://arxiv.org/abs/1712.08250v1) - [pdf](http://arxiv.org/pdf/1712.08250v1)

> Though deep neural network has hit a huge success in recent studies and applica- tions, it still remains vulnerable to adversarial perturbations which are imperceptible to humans. To address this problem, we propose a novel network called ReabsNet to achieve high classification accuracy in the face of various attacks. The approach is to augment an existing classification network with a guardian network to detect if a sample is natural or has been adversarially perturbed. Critically, instead of simply rejecting adversarial examples, we revise them to get their true labels. We exploit the observation that a sample containing adversarial perturbations has a possibility of returning to its true class after revision. We demonstrate that our ReabsNet outperforms the state-of-the-art defense method under various adversarial attacks.

</details>

<details>

<summary>2017-12-23 23:57:55 - Whatever Does Not Kill Deep Reinforcement Learning, Makes It Stronger</summary>

- *Vahid Behzadan, Arslan Munir*

- `1712.09344v1` - [abs](http://arxiv.org/abs/1712.09344v1) - [pdf](http://arxiv.org/pdf/1712.09344v1)

> Recent developments have established the vulnerability of deep Reinforcement Learning (RL) to policy manipulation attacks via adversarial perturbations. In this paper, we investigate the robustness and resilience of deep RL to training-time and test-time attacks. Through experimental results, we demonstrate that under noncontiguous training-time attacks, Deep Q-Network (DQN) agents can recover and adapt to the adversarial conditions by reactively adjusting the policy. Our results also show that policies learned under adversarial perturbations are more robust to test-time attacks. Furthermore, we compare the performance of $\epsilon$-greedy and parameter-space noise exploration methods in terms of robustness and resilience against adversarial perturbations.

</details>

<details>

<summary>2017-12-25 18:25:27 - A Survey on Security and Privacy Issues of Bitcoin</summary>

- *Mauro Conti, Sandeep Kumar E, Chhagan Lal, Sushmita Ruj*

- `1706.00916v3` - [abs](http://arxiv.org/abs/1706.00916v3) - [pdf](http://arxiv.org/pdf/1706.00916v3)

> Bitcoin is a popular cryptocurrency that records alltransactions in a distributed append-only public ledger calledblockchain. The security of Bitcoin heavily relies on the incentive-compatible proof-of-work (PoW) based distributed consensus pro-tocol, which is run by network nodes called miners. In exchangefor the incentive, the miners are expected to honestly maintainthe blockchain. Since its launch in 2009, Bitcoin economy hasgrown at an enormous rate, and it is now worth about 170 billions of dollars. This exponential growth in the market valueof Bitcoin motivates adversaries to exploit weaknesses for profit,and researchers to discover new vulnerabilities in the system,propose countermeasures, and predict upcoming trends.In this paper, we present a systematic survey that covers thesecurity and privacy aspects of Bitcoin. We start by presenting anoverview of the Bitcoin protocol and its major components alongwith their functionality and interactions within the system. Wereview the existing vulnerabilities in Bitcoin and its underlyingmajor technologies such as blockchain and PoW based consensusprotocol. These vulnerabilities lead to the execution of varioussecurity threats to the normal functionality of Bitcoin. Wethen discuss the feasibility and robustness of the state-of-the-art security solutions. Additionally, we present current privacyand anonymity considerations in Bitcoin and discuss the privacy-related threats to Bitcoin users along with the analysis of theexisting privacy-preserving solutions. Finally, we summarize thecritical open challenges and suggest directions for future researchtowards provisioning stringent security and privacy techniquesfor Bitcoin.

</details>

<details>

<summary>2017-12-28 09:22:15 - Learning to Customize Network Security Rules</summary>

- *Michael Bargury, Roy Levin, Royi Ronen*

- `1712.09795v1` - [abs](http://arxiv.org/abs/1712.09795v1) - [pdf](http://arxiv.org/pdf/1712.09795v1)

> Security is a major concern for organizations who wish to leverage cloud computing. In order to reduce security vulnerabilities, public cloud providers offer firewall functionalities. When properly configured, a firewall protects cloud networks from cyber-attacks. However, proper firewall configuration requires intimate knowledge of the protected system, high expertise and on-going maintenance.   As a result, many organizations do not use firewalls effectively, leaving their cloud resources vulnerable. In this paper, we present a novel supervised learning method, and prototype, which compute recommendations for firewall rules. Recommendations are based on sampled network traffic meta-data (NetFlow) collected from a public cloud provider. Labels are extracted from firewall configurations deemed to be authored by experts. NetFlow is collected from network routers, avoiding expensive collection from cloud VMs, as well as relieving privacy concerns.   The proposed method captures network routines and dependencies between resources and firewall configuration. The method predicts IPs to be allowed by the firewall. A grouping algorithm is subsequently used to generate a manageable number of IP ranges. Each range is a parameter for a firewall rule.   We present results of experiments on real data, showing ROC AUC of 0.92, compared to 0.58 for an unsupervised baseline. The results prove the hypothesis that firewall rules can be automatically generated based on router data, and that an automated method can be effective in blocking a high percentage of malicious traffic.

</details>

<details>

<summary>2017-12-28 14:00:33 - A non-biased trust model for wireless mesh networks</summary>

- *Heng Chuan Tan, Maode Ma, Houda Labiod, Peter Han Joo Chong, Jun Zhang*

- `1712.09862v1` - [abs](http://arxiv.org/abs/1712.09862v1) - [pdf](http://arxiv.org/pdf/1712.09862v1)

> Trust models that rely on recommendation trusts are vulnerable to badmouthing and ballot-stuffing attacks. To cope with these attacks, existing trust models use different trust aggregation techniques to process the recommendation trusts and combine them with the direct trust values to form a combined trust value. However, these trust models are biased as recommendation trusts that deviate too much from one's own opinion are discarded. In this paper, we propose a non-biased trust model that considers every recommendation trusts available regardless they are good or bad. Our trust model is based on a combination of 2 techniques: the dissimilarity test and the Dempster-Shafer Theory. The dissimilarity test determines the amount of conflict between 2 trust records, whereas the Dempster-Shafer Theory assigns belief functions based on the results of the dissimilarity test. Numerical results show that our trust model is robust against reputation-based attacks when compared to trust aggregation techniques such as the linear opinion pooling, subjective logic model, entropy-based probability model, and regression analysis. In addition, our model has been extensively tested using network simulator NS-3 in an Infrastructure-based wireless mesh networks and a Hybrid-based wireless mesh networks to demonstrate that it can mitigate blackhole and grayhole attacks.

</details>

