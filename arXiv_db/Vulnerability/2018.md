# 2018

## TOC

- [2018-01](#2018-01)
- [2018-02](#2018-02)
- [2018-03](#2018-03)
- [2018-04](#2018-04)
- [2018-05](#2018-05)
- [2018-06](#2018-06)
- [2018-07](#2018-07)
- [2018-08](#2018-08)
- [2018-09](#2018-09)
- [2018-10](#2018-10)
- [2018-11](#2018-11)
- [2018-12](#2018-12)

## 2018-01

<details>

<summary>2018-01-03 09:47:34 - A Look at the Time Delays in CVSS Vulnerability Scoring</summary>

- *Jukka Ruohonen*

- `1801.00938v1` - [abs](http://arxiv.org/abs/1801.00938v1) - [pdf](http://arxiv.org/pdf/1801.00938v1)

> This empirical paper examines the time delays that occur between the publication of Common Vulnerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD) and the Common Vulnerability Scoring System (CVSS) information attached to published CVEs. According to the empirical results based on regularized regression analysis of over eighty thousand archived vulnerabilities, (i) the CVSS content does not statistically influence the time delays, which, however, (ii) are strongly affected by a decreasing annual trend. In addition to these results, the paper contributes to the empirical research tradition of software vulnerabilities by a couple of insights on misuses of statistical methodology.

</details>

<details>

<summary>2018-01-03 19:26:32 - Economic Factors of Vulnerability Trade and Exploitation</summary>

- *Luca Allodi*

- `1708.04866v5` - [abs](http://arxiv.org/abs/1708.04866v5) - [pdf](http://arxiv.org/pdf/1708.04866v5)

> Cybercrime markets support the development and diffusion of new attack technologies, vulnerability exploits, and malware. Whereas the revenue streams of cyber attackers have been studied multiple times in the literature, no quantitative account currently exists on the economics of attack acquisition and deployment. Yet, this understanding is critical to characterize the production of (traded) exploits, the economy that drives it, and its effects on the overall attack scenario. In this paper we provide an empirical investigation of the economics of vulnerability exploitation, and the effects of market factors on likelihood of exploit. Our data is collected first-handedly from a prominent Russian cybercrime market where the trading of the most active attack tools reported by the security industry happens. Our findings reveal that exploits in the underground are priced similarly or above vulnerabilities in legitimate bug-hunting programs, and that the refresh cycle of exploits is slower than currently often assumed. On the other hand, cybercriminals are becoming faster at introducing selected vulnerabilities, and the market is in clear expansion both in terms of players, traded exploits, and exploit pricing. We then evaluate the effects of these market variables on likelihood of attack realization, and find strong evidence of the correlation between market activity and exploit deployment. We discuss implications on vulnerability metrics, economics, and exploit measurement.

</details>

<details>

<summary>2018-01-03 23:16:14 - Spectre Attacks: Exploiting Speculative Execution</summary>

- *Paul Kocher, Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas Prescher, Michael Schwarz, Yuval Yarom*

- `1801.01203v1` - [abs](http://arxiv.org/abs/1801.01203v1) - [pdf](http://arxiv.org/pdf/1801.01203v1)

> Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects.   Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices.   While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak.

</details>

<details>

<summary>2018-01-03 23:36:24 - Meltdown</summary>

- *Moritz Lipp, Michael Schwarz, Daniel Gruss, Thomas Prescher, Werner Haas, Stefan Mangard, Paul Kocher, Daniel Genkin, Yuval Yarom, Mike Hamburg*

- `1801.01207v1` - [abs](http://arxiv.org/abs/1801.01207v1) - [pdf](http://arxiv.org/pdf/1801.01207v1)

> The security of computer systems fundamentally relies on memory isolation, e.g., kernel address ranges are marked as non-accessible and are protected from user access. In this paper, we present Meltdown. Meltdown exploits side effects of out-of-order execution on modern processors to read arbitrary kernel-memory locations including personal data and passwords. Out-of-order execution is an indispensable performance feature and present in a wide range of modern processors. The attack works on different Intel microarchitectures since at least 2010 and potentially other processors are affected. The root cause of Meltdown is the hardware. The attack is independent of the operating system, and it does not rely on any software vulnerabilities. Meltdown breaks all security assumptions given by address space isolation as well as paravirtualized environments and, thus, every security mechanism building upon this foundation. On affected systems, Meltdown enables an adversary to read memory of other processes or virtual machines in the cloud without any permissions or privileges, affecting millions of customers and virtually every user of a personal computer. We show that the KAISER defense mechanism for KASLR has the important (but inadvertent) side effect of impeding Meltdown. We stress that KAISER must be deployed immediately to prevent large-scale exploitation of this severe information leakage.

</details>

<details>

<summary>2018-01-05 09:37:18 - VulDeePecker: A Deep Learning-Based System for Vulnerability Detection</summary>

- *Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, Yuyi Zhong*

- `1801.01681v1` - [abs](http://arxiv.org/abs/1801.01681v1) - [pdf](http://arxiv.org/pdf/1801.01681v1)

> The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were "silently" patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.

</details>

<details>

<summary>2018-01-05 14:12:48 - Learning Universal Adversarial Perturbations with Generative Models</summary>

- *Jamie Hayes, George Danezis*

- `1708.05207v3` - [abs](http://arxiv.org/abs/1708.05207v3) - [pdf](http://arxiv.org/pdf/1708.05207v3)

> Neural networks are known to be vulnerable to adversarial examples, inputs that have been intentionally perturbed to remain visually similar to the source input, but cause a misclassification. It was recently shown that given a dataset and classifier, there exists so called universal adversarial perturbations, a single perturbation that causes a misclassification when applied to any input. In this work, we introduce universal adversarial networks, a generative network that is capable of fooling a target classifier when it's generated output is added to a clean sample from a dataset. We show that this technique improves on known universal adversarial attacks.

</details>

<details>

<summary>2018-01-06 01:37:30 - Adversarial Perturbation Intensity Achieving Chosen Intra-Technique Transferability Level for Logistic Regression</summary>

- *Martin Gubri*

- `1801.01953v1` - [abs](http://arxiv.org/abs/1801.01953v1) - [pdf](http://arxiv.org/pdf/1801.01953v1)

> Machine Learning models have been shown to be vulnerable to adversarial examples, ie. the manipulation of data by a attacker to defeat a defender's classifier at test time. We present a novel probabilistic definition of adversarial examples in perfect or limited knowledge setting using prior probability distributions on the defender's classifier. Using the asymptotic properties of the logistic regression, we derive a closed-form expression of the intensity of any adversarial perturbation, in order to achieve a given expected misclassification rate. This technique is relevant in a threat model of known model specifications and unknown training data. To our knowledge, this is the first method that allows an attacker to directly choose the probability of attack success. We evaluate our approach on two real-world datasets.

</details>

<details>

<summary>2018-01-09 19:03:32 - Spatially Transformed Adversarial Examples</summary>

- *Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, Dawn Song*

- `1801.02612v2` - [abs](http://arxiv.org/abs/1801.02612v2) - [pdf](http://arxiv.org/pdf/1801.02612v2)

> Recent studies show that widely used deep neural networks (DNNs) are vulnerable to carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the $\mathcal{L}_p$ distance for penalizing perturbations. Researchers have explored different defense methods to defend against such adversarial attacks. While the effectiveness of $\mathcal{L}_p$ distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large $\mathcal{L}_p$ distance measures, but our extensive experiments show that such spatially transformed adversarial examples are perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted.

</details>

<details>

<summary>2018-01-10 02:26:18 - Fusion of ANN and SVM Classifiers for Network Attack Detection</summary>

- *Takwa Omrani, Adel Dallali, Bilgacem Chibani Rhaimi, Jaouhar Fattahi*

- `1801.02746v2` - [abs](http://arxiv.org/abs/1801.02746v2) - [pdf](http://arxiv.org/pdf/1801.02746v2)

> With the progressive increase of network application and electronic devices (computers, mobile phones, android, etc.) attack and intrusion, detection has become a very challenging task in cybercrime detection area. in this context, most of the existing approaches of attack detection rely mainly on a finite set of attacks. These solutions are vulnerable, that is, they fail in detecting some attacks when sources of informations are ambiguous or imperfect. However, few approaches started investigating in this direction. This paper investigates the role of machine learning approach (ANN, SVM) in detecting a TCP connection traffic as a normal or a suspicious one. But, using ANN and SVM is an expensive technique individually. In this paper, combining two classifiers are proposed, where artificial neural network (ANN) classifier and support vector machine (SVM) are both employed. Additionally, our proposed solution allows to visualize obtained classification results. Accuracy of the proposed solution has been compared with other classifier results. Experiments have been conducted with different network connections selected from NSL-KDD DARPA dataset. Empirical results show that combining ANN and SVM techniques for attack detection is a promising direction.

</details>

<details>

<summary>2018-01-12 04:34:17 - A3T: Adversarially Augmented Adversarial Training</summary>

- *Akram Erraqabi, Aristide Baratin, Yoshua Bengio, Simon Lacoste-Julien*

- `1801.04055v1` - [abs](http://arxiv.org/abs/1801.04055v1) - [pdf](http://arxiv.org/pdf/1801.04055v1)

> Recent research showed that deep neural networks are highly sensitive to so-called adversarial perturbations, which are tiny perturbations of the input data purposely designed to fool a machine learning classifier. Most classification models, including deep learning models, are highly vulnerable to adversarial attacks. In this work, we investigate a procedure to improve adversarial robustness of deep neural networks through enforcing representation invariance. The idea is to train the classifier jointly with a discriminator attached to one of its hidden layer and trained to filter the adversarial noise. We perform preliminary experiments to test the viability of the approach and to compare it to other standard adversarial training methods.

</details>

<details>

<summary>2018-01-13 03:30:33 - SCLib: A Practical and Lightweight Defense against Component Hijacking in Android Applications</summary>

- *Daoyuan Wu, Yao Cheng, Debin Gao, Yingjiu Li, Robert H. Deng*

- `1801.04372v1` - [abs](http://arxiv.org/abs/1801.04372v1) - [pdf](http://arxiv.org/pdf/1801.04372v1)

> Cross-app collaboration via inter-component communication is a fundamental mechanism on Android. Although it brings the benefits such as functionality reuse and data sharing, a threat called component hijacking is also introduced. By hijacking a vulnerable component in victim apps, an attack app can escalate its privilege for operations originally prohibited. Many prior studies have been performed to understand and mitigate this issue, but no defense is being deployed in the wild, largely due to the deployment difficulties and performance concerns. In this paper we present SCLib, a secure component library that performs in-app mandatory access control on behalf of app components. It does not require firmware modification or app repackaging as in previous works. The library-based nature also makes SCLib more accessible to app developers, and enables them produce secure components in the first place over fragmented Android devices. As a proof of concept, we design six mandatory policies and overcome unique implementation challenges to mitigate attacks originated from both system weaknesses and common developer mistakes. Our evaluation using ten high-profile open source apps shows that SCLib can protect their 35 risky components with negligible code footprint (less than 0.3% stub code) and nearly no slowdown to normal intra-app communications. The worst-case performance overhead to stop attacks is about 5%.

</details>

<details>

<summary>2018-01-14 15:05:34 - Towards Realistic Threat Modeling: Attack Commodification, Irrelevant Vulnerabilities, and Unrealistic Assumptions</summary>

- *Luca Allodi, Sandro Etalle*

- `1801.04569v1` - [abs](http://arxiv.org/abs/1801.04569v1) - [pdf](http://arxiv.org/pdf/1801.04569v1)

> Current threat models typically consider all possible ways an attacker can penetrate a system and assign probabilities to each path according to some metric (e.g. time-to-compromise). In this paper we discuss how this view hinders the realness of both technical (e.g. attack graphs) and strategic (e.g. game theory) approaches of current threat modeling, and propose to steer away by looking more carefully at attack characteristics and attacker environment. We use a toy threat model for ICS attacks to show how a realistic view of attack instances can emerge from a simple analysis of attack phases and attacker limitations.

</details>

<details>

<summary>2018-01-14 17:46:17 - Deep Reinforcement Fuzzing</summary>

- *Konstantin Böttinger, Patrice Godefroid, Rishabh Singh*

- `1801.04589v1` - [abs](http://arxiv.org/abs/1801.04589v1) - [pdf](http://arxiv.org/pdf/1801.04589v1)

> Fuzzing is the process of finding security vulnerabilities in input-processing code by repeatedly testing the code with modified inputs. In this paper, we formalize fuzzing as a reinforcement learning problem using the concept of Markov decision processes. This in turn allows us to apply state-of-the-art deep Q-learning algorithms that optimize rewards, which we define from runtime properties of the program under test. By observing the rewards caused by mutating with a specific set of actions performed on an initial program input, the fuzzing agent learns a policy that can next generate new higher-reward inputs. We have implemented this new approach, and preliminary empirical evidence shows that reinforcement fuzzing can outperform baseline random fuzzing.

</details>

<details>

<summary>2018-01-15 08:15:33 - Towards Imperceptible and Robust Adversarial Example Attacks against Neural Networks</summary>

- *Bo Luo, Yannan Liu, Lingxiao Wei, Qiang Xu*

- `1801.04693v1` - [abs](http://arxiv.org/abs/1801.04693v1) - [pdf](http://arxiv.org/pdf/1801.04693v1)

> Machine learning systems based on deep neural networks, being able to produce state-of-the-art results on various perception tasks, have gained mainstream adoption in many applications. However, they are shown to be vulnerable to adversarial example attack, which generates malicious output by adding slight perturbations to the input. Previous adversarial example crafting methods, however, use simple metrics to evaluate the distances between the original examples and the adversarial ones, which could be easily detected by human eyes. In addition, these attacks are often not robust due to the inevitable noises and deviation in the physical world. In this work, we present a new adversarial example attack crafting method, which takes the human perceptual system into consideration and maximizes the noise tolerance of the crafted adversarial example. Experimental results demonstrate the efficacy of the proposed technique.

</details>

<details>

<summary>2018-01-15 09:03:27 - Attack Potential in Impact and Complexity</summary>

- *Luca Allodi, Fabio Massacci*

- `1801.04703v1` - [abs](http://arxiv.org/abs/1801.04703v1) - [pdf](http://arxiv.org/pdf/1801.04703v1)

> Vulnerability exploitation is reportedly one of the main attack vectors against computer systems. Yet, most vulnerabilities remain unexploited by attackers. It is therefore of central importance to identify vulnerabilities that carry a high `potential for attack'. In this paper we rely on Symantec data on real attacks detected in the wild to identify a trade-off in the Impact and Complexity of a vulnerability, in terms of attacks that it generates; exploiting this effect, we devise a readily computable estimator of the vulnerability's Attack Potential that reliably estimates the expected volume of attacks against the vulnerability. We evaluate our estimator performance against standard patching policies by measuring foiled attacks and demanded workload expressed as the number of vulnerabilities entailed to patch. We show that our estimator significantly improves over standard patching policies by ruling out low-risk vulnerabilities, while maintaining invariant levels of coverage against attacks in the wild. Our estimator can be used as a first aid for vulnerability prioritisation to focus assessment efforts on high-potential vulnerabilities.

</details>

<details>

<summary>2018-01-15 18:02:48 - Debugging Static Analysis</summary>

- *Lisa Nguyen Quang Do, Stefan Krüger, Patrick Hill, Karim Ali, Eric Bodden*

- `1801.04894v1` - [abs](http://arxiv.org/abs/1801.04894v1) - [pdf](http://arxiv.org/pdf/1801.04894v1)

> To detect and fix bugs and security vulnerabilities, software companies use static analysis as part of the development process. However, static analysis code itself is also prone to bugs. To ensure a consistent level of precision, as analyzed programs grow more complex, a static analysis has to handle more code constructs, frameworks, and libraries that the programs use. While more complex analyses are written and used in production systems every day, the cost of debugging and fixing them also increases tremendously.   To better understand the difficulties of debugging static analyses, we surveyed 115 static analysis writers. From their responses, we extracted the core requirements to build a debugger for static analysis, which revolve around two main issues: (1) abstracting from two code bases at the same time (the analysis code and the analyzed code) and (2) tracking the analysis internal state throughout both code bases. Most current debugging tools that our survey participants use lack the capabilities to address both issues.   Focusing on those requirements, we introduce VisuFlow, a debugging environment for static data-flow analysis that is integrated in the Eclipse development environment. VisuFlow features graph visualizations that enable users to view the state of a data-flow analysis and its intermediate results at any time. Special breakpoints in VisuFlow help users step through the analysis code and the analyzed simultaneously. To evaluate the usefulness of VisuFlow, we have conducted a user study on 20 static analysis writers. Using VisuFlow helped our sample of analysis writers identify 25% and fix 50% more errors in the analysis code compared to using the standard Eclipse debugging environment.

</details>

<details>

<summary>2018-01-15 19:36:01 - Encrypt Flip-Flop: A Novel Logic Encryption Technique For Sequential Circuits</summary>

- *Rajit Karmakar, Santanu Chatopadhyay, Rohit Kapur*

- `1801.04961v1` - [abs](http://arxiv.org/abs/1801.04961v1) - [pdf](http://arxiv.org/pdf/1801.04961v1)

> Logic Encryption is one of the most popular hardware security techniques which can prevent IP piracy and illegal IC overproduction. It introduces obfuscation by inserting some extra hardware into a design to hide its functionality from unauthorized users. Correct functionality of an encrypted design depends upon the application of correct keys, shared only with the authorized users. In the recent past, extensive efforts have been devoted in extracting the secret key of an encrypted design. At the same time, several countermeasures have also been proposed by the research community to thwart different state-of-the-art attacks on logic encryption. However, most of the proposed countermeasures fail to prevent the powerful SAT attack. Although a few researchers have proposed different solutions to withstand SAT attack, those solutions suffer from several drawbacks such as high design overheads, low output corruptibility, and vulnerability against removal attack. Almost all the known logic encryption strategies are vulnerable to scan based attack. In this paper, we propose a novel encryption technique called Encrypt Flip-Flop, which encrypts the outputs of selected flip-flops by inserting multiplexers (MUX). The proposed strategy can thwart all the known attacks including SAT and scan based attacks. The scheme has low design overhead and implementation complexity. Experimental results on several ISCAS'89 and ITC'99 benchmarks show that our proposed method can produce reasonable output corruption for wrong keys.

</details>

<details>

<summary>2018-01-16 19:34:38 - Considerations regarding security issues impact on systems availability</summary>

- *Emil Pricop, Sanda Florentina Mihalache, Nicolae Paraschiv, Jaouhar Fattahi, Florin Zamfir*

- `1801.05459v1` - [abs](http://arxiv.org/abs/1801.05459v1) - [pdf](http://arxiv.org/pdf/1801.05459v1)

> Control systems behavior can be analyzed taking into account a large number of parameters: performances, reliability, availability, security. Each control system presents various security vulnerabilities that affect in lower or higher measure its functioning. In this paper the authors present a method to assess the impact of security issues on the systems availability. A fuzzy model for estimating the availability of the system based on the security level and achieved availability coefficient (depending on MTBF and MTR) is developed and described. The results of the fuzzy inference system (FIS) are presented in the last section of the paper.

</details>

<details>

<summary>2018-01-17 10:38:20 - Automatic Detection of Cyberbullying in Social Media Text</summary>

- *Cynthia Van Hee, Gilles Jacobs, Chris Emmery, Bart Desmet, Els Lefever, Ben Verhoeven, Guy De Pauw, Walter Daelemans, Véronique Hoste*

- `1801.05617v1` - [abs](http://arxiv.org/abs/1801.05617v1) - [pdf](http://arxiv.org/pdf/1801.05617v1)

> While social media offer great communication opportunities, they also increase the vulnerability of young people to threatening situations online. Recent studies report that cyberbullying constitutes a growing problem among youngsters. Successful prevention depends on the adequate detection of potentially harmful messages and the information overload on the Web requires intelligent systems to identify potential risks automatically. The focus of this paper is on automatic cyberbullying detection in social media text by modelling posts written by bullies, victims, and bystanders of online bullying. We describe the collection and fine-grained annotation of a training corpus for English and Dutch and perform a series of binary classification experiments to determine the feasibility of automatic cyberbullying detection. We make use of linear support vector machines exploiting a rich feature set and investigate which information sources contribute the most for this particular task. Experiments on a holdout test set reveal promising results for the detection of cyberbullying-related posts. After optimisation of the hyperparameters, the classifier yields an F1-score of 64% and 61% for English and Dutch respectively, and considerably outperforms baseline systems based on keywords and word unigrams.

</details>

<details>

<summary>2018-01-17 17:32:00 - M-STAR: A Modular, Evidence-based Software Trustworthiness Framework</summary>

- *Nikolaos Alexopoulos, Sheikh Mahbub Habib, Steffen Schulz, Max Mühlhäuser*

- `1801.05764v1` - [abs](http://arxiv.org/abs/1801.05764v1) - [pdf](http://arxiv.org/pdf/1801.05764v1)

> Despite years of intensive research in the field of software vulnerabilities discovery, exploits are becoming ever more common. Consequently, it is more necessary than ever to choose software configurations that minimize systems' exposure surface to these threats. In order to support users in assessing the security risks induced by their software configurations and in making informed decisions, we introduce M-STAR, a Modular Software Trustworthiness ARchitecture and framework for probabilistically assessing the trustworthiness of software systems, based on evidence, such as their vulnerability history and source code properties.   Integral to M-STAR is a software trustworthiness model, consistent with the concept of computational trust. Computational trust models are rooted in Bayesian probability and Dempster-Shafer Belief theory, offering mathematical soundness and expressiveness to our framework. To evaluate our framework, we instantiate M-STAR for Debian Linux packages, and investigate real-world deployment scenarios. In our experiments with real-world data, M-STAR could assess the relative trustworthiness of complete software configurations with an error of less than 10%. Due to its modular design, our proposed framework is agile, as it can incorporate future advances in the field of code analysis and vulnerability prediction. Our results point out that M-STAR can be a valuable tool for system administrators, regular users and developers, helping them assess and manage risks associated with their software configurations.

</details>

<details>

<summary>2018-01-18 02:56:02 - Security in Mobile Edge Caching with Reinforcement Learning</summary>

- *Liang Xiao, Xiaoyue Wan, Canhuang Dai, Xiaojiang Du, Xiang Chen, Mohsen Guizani*

- `1801.05915v1` - [abs](http://arxiv.org/abs/1801.05915v1) - [pdf](http://arxiv.org/pdf/1801.05915v1)

> Mobile edge computing usually uses cache to support multimedia contents in 5G mobile Internet to reduce the computing overhead and latency. Mobile edge caching (MEC) systems are vulnerable to various attacks such as denial of service attacks and rogue edge attacks. This article investigates the attack models in MEC systems, focusing on both the mobile offloading and the caching procedures. In this paper, we propose security solutions that apply reinforcement learning (RL) techniques to provide secure offloading to the edge nodes against jamming attacks. We also present light-weight authentication and secure collaborative caching schemes to protect data privacy. We evaluate the performance of the RL-based security solution for mobile edge caching and discuss the challenges that need to be addressed in the future.

</details>

<details>

<summary>2018-01-22 15:10:14 - SecSens: Secure State Estimation with Application to Localization and Time Synchronization</summary>

- *Amr Alanwar, Bernhard Etzlinger, Henrique Ferraz, Joao Hespanha, Mani Srivastava*

- `1801.07132v1` - [abs](http://arxiv.org/abs/1801.07132v1) - [pdf](http://arxiv.org/pdf/1801.07132v1)

> Research evidence in Cyber-Physical Systems (CPS) shows that the introduced tight coupling of information technology with physical sensing and actuation leads to more vulnerability and security weaknesses. But, the traditional security protection mechanisms of CPS focus on data encryption while neglecting the sensors which are vulnerable to attacks in the physical domain. Accordingly, researchers attach utmost importance to the problem of state estimation in the presence of sensor attacks. In this work, we present SecSens, a novel approach for secure nonlinear state estimation in the presence of modeling and measurement noise. SecSens consists of two independent algorithms, namely, SecEKF and SecOPT, which are based on Extended Kalman Filter and Maximum Likelihood Estimation, respectively. We adopt a holistic approach to introduce security awareness among state estimation algorithms without requiring specialized hardware, or cryptographic techniques. We apply SecSens to securely localize and time synchronize networked mobile devices. SecSens provides good performance at run-time several order of magnitude faster than the state of art solutions under the presence of powerful attacks. Our algorithms are evaluated on a testbed with static nodes and a mobile quadrotor all equipped with commercial ultra-wide band wireless devices.

</details>

<details>

<summary>2018-01-24 13:39:33 - Mitigating CSRF attacks on OAuth 2.0 and OpenID Connect</summary>

- *Wanpeng Li, Chris J Mitchell, Thomas Chen*

- `1801.07983v1` - [abs](http://arxiv.org/abs/1801.07983v1) - [pdf](http://arxiv.org/pdf/1801.07983v1)

> Many millions of users routinely use their Google, Facebook and Microsoft accounts to log in to websites supporting OAuth 2.0 and/or OpenID Connect-based single sign on. The security of OAuth 2.0 and OpenID Connect is therefore of critical importance, and it has been widely examined both in theory and in practice. Unfortunately, as these studies have shown, real-world implementations of both schemes are often vulnerable to attack, and in particular to cross-site request forgery (CSRF) attacks. In this paper we propose a new technique which can be used to mitigate CSRF attacks against both OAuth 2.0 and OpenID Connect.

</details>

<details>

<summary>2018-01-24 20:54:15 - Exposing Vulnerabilities in Mobile Networks: A Mobile Data Consumption Attack</summary>

- *Dean Wasil, Omar Nakhila, Salih Safa Bacanli, Cliff Zou, Damla Turgut*

- `1801.08185v1` - [abs](http://arxiv.org/abs/1801.08185v1) - [pdf](http://arxiv.org/pdf/1801.08185v1)

> Smartphone carrier companies rely on mobile networks for keeping an accurate record of customer data usage for billing purposes. In this paper, we present a vulnerability that allows an attacker to force the victim's smartphone to consume data through the cellular network by starting the data download on the victim's cell phone without the victim's knowledge. The attack is based on switching the victim's smartphones from the Wi-Fi network to the cellular network while downloading a large data file. This attack has been implemented in real-life scenarios where the test's outcomes demonstrate that the attack is feasible and that mobile networks do not record customer data usage accurately.

</details>

<details>

<summary>2018-01-25 15:11:16 - Agile development for vulnerable populations: lessons learned and recommendations</summary>

- *Marcos Baez, Fabio Casati*

- `1802.04100v1` - [abs](http://arxiv.org/abs/1802.04100v1) - [pdf](http://arxiv.org/pdf/1802.04100v1)

> In this paper we draw attention to the challenges of managing software projects for vulnerable populations, i.e., people potentially exposed to harm or not capable of protecting their own interests. The focus on human aspects, and particularly, the inclusion of human-centered approaches, has been a popular topic in the software engineering community. We argue, however, that current literature provides little understanding and guidance on how to approach these type of scenarios. Here, we shed some light on the topic by reporting on our experiences in developing innovative solutions for the residential care scenario, outlining potential issues and recommendations.

</details>

<details>

<summary>2018-01-27 00:30:55 - Linking Received Packet to the Transmitter Through Physical-Fingerprinting of Controller Area Network</summary>

- *Omid Avatefipour, Azeem Hafeez, Muhammad Tayyab, Hafiz Malik*

- `1801.09011v1` - [abs](http://arxiv.org/abs/1801.09011v1) - [pdf](http://arxiv.org/pdf/1801.09011v1)

> The Controller Area Network (CAN) bus serves as a legacy protocol for in-vehicle data communication. Simplicity, robustness, and suitability for real-time systems are the salient features of the CAN bus protocol. However, it lacks the basic security features such as massage authentication, which makes it vulnerable to the spoofing attacks. In a CAN network, linking CAN packet to the sender node is a challenging task. This paper aims to address this issue by developing a framework to link each CAN packet to its source. Physical signal attributes of the received packet consisting of channel and node (or device) which contains specific unique artifacts are considered to achieve this goal. Material and design imperfections in the physical channel and digital device, which are the main contributing factors behind the device-channel specific unique artifacts, are leveraged to link the received electrical signal to the transmitter. Generally, the inimitable patterns of signals from each ECUs exist over the course of time that can manifest the stability of the proposed method. Uniqueness of the channel-device specific attributes are also investigated for time- and frequency-domain. Feature vector is made up of both time and frequency domain physical attributes and then employed to train a neural network-based classifier. Performance of the proposed fingerprinting method is evaluated by using a dataset collected from 16 different channels and four identical ECUs transmitting same message. Experimental results indicate that the proposed method achieves correct detection rates of 95.2% and 98.3% for channel and ECU classification, respectively.

</details>

<details>

<summary>2018-01-27 04:08:56 - A Learning and Masking Approach to Secure Learning</summary>

- *Linh Nguyen, Sky Wang, Arunesh Sinha*

- `1709.04447v4` - [abs](http://arxiv.org/abs/1709.04447v4) - [pdf](http://arxiv.org/pdf/1709.04447v4)

> Deep Neural Networks (DNNs) have been shown to be vulnerable against adversarial examples, which are data points cleverly constructed to fool the classifier. Such attacks can be devastating in practice, especially as DNNs are being applied to ever increasing critical tasks like image recognition in autonomous driving. In this paper, we introduce a new perspective on the problem. We do so by first defining robustness of a classifier to adversarial exploitation. Next, we show that the problem of adversarial example generation can be posed as learning problem. We also categorize attacks in literature into high and low perturbation attacks; well-known attacks like fast-gradient sign method (FGSM) and our attack produce higher perturbation adversarial examples while the more potent but computationally inefficient Carlini-Wagner (CW) attack is low perturbation. Next, we show that the dual approach of the attack learning problem can be used as a defensive technique that is effective against high perturbation attacks. Finally, we show that a classifier masking method achieved by adding noise to the a neural network's logit output protects against low distortion attacks such as the CW attack. We also show that both our learning and masking defense can work simultaneously to protect against multiple attacks. We demonstrate the efficacy of our techniques by experimenting with the MNIST and CIFAR-10 datasets.

</details>

<details>

<summary>2018-01-30 12:33:02 - Web password recovery --- a necessary evil?</summary>

- *Fatma Al Maqbali, Chris J Mitchell*

- `1801.06730v2` - [abs](http://arxiv.org/abs/1801.06730v2) - [pdf](http://arxiv.org/pdf/1801.06730v2)

> Web password recovery, enabling a user who forgets their password to re-establish a shared secret with a website, is very widely implemented. However, use of such a fall-back system brings with it additional vulnerabilities to user authentication. This paper provides a framework within which such systems can be analysed systematically, and uses this to help gain a better understanding of how such systems are best implemented. To this end, a model for web password recovery is given, and existing techniques are documented and analysed within the context of this model. This leads naturally to a set of recommendations governing how such systems should be implemented to maximise security. A range of issues for further research are also highlighted.

</details>

<details>

<summary>2018-01-31 22:58:32 - Keyshuffling Attack for Persistent Early Code Execution in the Nintendo 3DS Secure Bootchain</summary>

- *Matthew McClintic, Devon Maloney, Michael Scires, Gabriel Marcano, Matthew Norman, Aurora Wright*

- `1802.00092v1` - [abs](http://arxiv.org/abs/1802.00092v1) - [pdf](http://arxiv.org/pdf/1802.00092v1)

> We demonstrate an attack on the secure bootchain of the Nintendo 3DS in order to gain early code execution. The attack utilizes the block shuffling vulnerability of the ECB cipher mode to rearrange keys in the Nintendo 3DS's encrypted keystore. Because the shuffled keys will deterministically decrypt the encrypted firmware binary to incorrect plaintext data and execute it, and because the device's memory contents are kept between hard reboots, it is possible to reliably reach a branching instruction to a payload in memory. This payload, due to its execution by a privileged processor and its early execution, is able to extract the hash of hardware secrets necessary to decrypt the device's encrypted keystore and set up a persistent exploit of the system.

</details>


## 2018-02

<details>

<summary>2018-02-01 10:30:20 - Integrity Coded Databases: An Evaluation of Performance, Efficiency, and Practicality</summary>

- *Dan Kondratyuk, Jake Rodden, Elmer Duran*

- `1802.00230v1` - [abs](http://arxiv.org/abs/1802.00230v1) - [pdf](http://arxiv.org/pdf/1802.00230v1)

> In recent years, cloud database storage has become an inexpensive and convenient option for businesses and individuals to store information. While its positive aspects make the cloud extremely attractive for data storage, it is a relatively new area of service, making it vulnerable to cyber-attacks and security breaches. Storing data in a foreign location also requires the owner to relinquish control of their information to system administrators of these online database services. This opens the possibility for malicious, internal attacks on the data that may involve the manipulation, omission, or addition of data. The retention of the data as it was intended to be stored is referred to as the database's integrity. Our research tests a potential solution for maintaining the integrity of these cloud-storage databases by converting the original databases to Integrity Coded Databases (ICDB). ICDBs utilize Integrity Codes: cryptographic codes created alongside the data by a private key that only the data owner has access to. When the database is queried, an integrity code is returned along with the queried information. The owner is then able to verify that the information is correct, complete, and fresh. Consequently, ICDBs also incur performance and memory penalties. In our research, we explore, test, and benchmark ICDBs to determine the costs and benefits of maintaining an ICDB versus a standard database.

</details>

<details>

<summary>2018-02-03 07:47:27 - Weighting Scheme for a Pairwise Multi-label Classifier Based on the Fuzzy Confusion Matrix</summary>

- *Pawel Trajdos, Marek Kurzynski*

- `1710.09710v2` - [abs](http://arxiv.org/abs/1710.09710v2) - [pdf](http://arxiv.org/pdf/1710.09710v2)

> In this work we addressed the issue of applying a stochastic classifier and a local, fuzzy confusion matrix under the framework of multi-label classification. We proposed a novel solution to the problem of correcting label pairwise ensembles. The main step of the correction procedure is to compute classifier-specific competence and cross-competence measures, which estimates error pattern of the underlying classifier. At the fusion phase we employed two weighting approaches based on information theory. The classifier weights promote base classifiers which are the most susceptible to the correction based on the fuzzy confusion matrix. During the experimental study, the proposed approach was compared against two reference methods. The comparison was made in terms of six different quality criteria. The conducted experiments reveals that the proposed approach eliminates one of main drawbacks of the original FCM-based approach i.e. the original approach is vulnerable to the imbalanced class/label distribution. What is more, the obtained results shows that the introduced method achieves satisfying classification quality under all considered quality criteria. Additionally, the impact of fluctuations of data set characteristics is reduced.

</details>

<details>

<summary>2018-02-03 22:24:39 - Software Fault Isolation for Robust Compilation</summary>

- *Ana Nora Evans*

- `1802.01044v1` - [abs](http://arxiv.org/abs/1802.01044v1) - [pdf](http://arxiv.org/pdf/1802.01044v1)

> Memory corruption vulnerabilities are endemic to unsafe languages, such as C, and they can even be found in safe languages that themselves are implemented in unsafe languages or linked with libraries implemented in unsafe languages. Robust compilation mitigates the threat of linking with memory-unsafe libraries. The source language is a C-like language, enriched with a notion of a component which encapsulates data and code, exposing functionality through well-defined interfaces. Robust compilation defines what security properties a component still has, even, if one or more components are compromised. The main contribution of this work is to demonstrate that the compartmentalization necessary for a compiler that has the robust compilation property can be realized on a basic RISC processor using software fault isolation.

</details>

<details>

<summary>2018-02-04 23:12:14 - Deceptive Games</summary>

- *Damien Anderson, Matthew Stephenson, Julian Togelius, Christian Salge, John Levine, Jochen Renz*

- `1802.00048v2` - [abs](http://arxiv.org/abs/1802.00048v2) - [pdf](http://arxiv.org/pdf/1802.00048v2)

> Deceptive games are games where the reward structure or other aspects of the game are designed to lead the agent away from a globally optimal policy. While many games are already deceptive to some extent, we designed a series of games in the Video Game Description Language (VGDL) implementing specific types of deception, classified by the cognitive biases they exploit. VGDL games can be run in the General Video Game Artificial Intelligence (GVGAI) Framework, making it possible to test a variety of existing AI agents that have been submitted to the GVGAI Competition on these deceptive games. Our results show that all tested agents are vulnerable to several kinds of deception, but that different agents have different weaknesses. This suggests that we can use deception to understand the capabilities of a game-playing algorithm, and game-playing algorithms to characterize the deception displayed by a game.

</details>

<details>

<summary>2018-02-05 23:14:26 - State-of-the-Art Survey on In-Vehicle Network Communication (CAN-Bus) Security and Vulnerabilities</summary>

- *Omid Avatefipour, Hafiz Malik*

- `1802.01725v1` - [abs](http://arxiv.org/abs/1802.01725v1) - [pdf](http://arxiv.org/pdf/1802.01725v1)

> Nowadays with the help of advanced technology, modern vehicles are not only made up of mechanical devices but also consist of highly complex electronic devices and connections to the outside world. There are around 70 Electronic Control Units (ECUs) in modern vehicle which are communicating with each other over the standard communication protocol known as Controller Area Network (CAN-Bus) that provides the communication rate up to 1Mbps. There are different types of in-vehicle network protocol and bus system namely Controlled Area Network (CAN), Local Interconnected Network (LIN), Media Oriented System Transport (MOST), and FlexRay. Even though CAN-Bus is considered as de-facto standard for in-vehicle network communication, it inherently lacks the fundamental security features by design like message authentication. This security limitation has paved the way for adversaries to penetrate into the vehicle network and do malicious activities which can pose a dangerous situation for both driver and passengers. In particular, nowadays vehicular networks are not only closed systems, but also they are open to different external interfaces namely Bluetooth, GPS, to the outside world. Therefore, it creates new opportunities for attackers to remotely take full control of the vehicle. The objective of this research is to survey the current limitations of CAN-Bus protocol in terms of secure communication and different solutions that researchers in the society of automotive have provided to overcome the CAN-Bus limitation on different layers.

</details>

<details>

<summary>2018-02-06 00:51:15 - ModelChain: Decentralized Privacy-Preserving Healthcare Predictive Modeling Framework on Private Blockchain Networks</summary>

- *Tsung-Ting Kuo, Lucila Ohno-Machado*

- `1802.01746v1` - [abs](http://arxiv.org/abs/1802.01746v1) - [pdf](http://arxiv.org/pdf/1802.01746v1)

> Cross-institutional healthcare predictive modeling can accelerate research and facilitate quality improvement initiatives, and thus is important for national healthcare delivery priorities. For example, a model that predicts risk of re-admission for a particular set of patients will be more generalizable if developed with data from multiple institutions. While privacy-protecting methods to build predictive models exist, most are based on a centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure (and single-point-of-breach) and accidental or malicious modification of records. In this article, we describe a new framework, ModelChain, to adapt Blockchain technology for privacy-preserving machine learning. Each participating site contributes to model parameter estimation without revealing any patient health information (i.e., only model data, no observation-level data, are exchanged across institutions). We integrate privacy-preserving online machine learning with a private Blockchain network, apply transaction metadata to disseminate partial models, and design a new proof-of-information algorithm to determine the order of the online learning process. We also discuss the benefits and potential issues of applying Blockchain technology to solve the privacy-preserving healthcare predictive modeling task and to increase interoperability between institutions, to support the Nationwide Interoperability Roadmap and national healthcare delivery priorities such as Patient-Centered Outcomes Research (PCOR).

</details>

<details>

<summary>2018-02-06 16:24:06 - A Survey on Sensor-based Threats to Internet-of-Things (IoT) Devices and Applications</summary>

- *Amit Kumar Sikder, Giuseppe Petracca, Hidayet Aksu, Trent Jaeger, A. Selcuk Uluagac*

- `1802.02041v1` - [abs](http://arxiv.org/abs/1802.02041v1) - [pdf](http://arxiv.org/pdf/1802.02041v1)

> The concept of Internet of Things (IoT) has become more popular in the modern era of technology than ever before. From small household devices to large industrial machines, the vision of IoT has made it possible to connect the devices with the physical world around them. This increasing popularity has also made the IoT devices and applications in the center of attention among attackers. Already, several types of malicious activities exist that attempt to compromise the security and privacy of the IoT devices. One interesting emerging threat vector is the attacks that abuse the use of sensors on IoT devices. IoT devices are vulnerable to sensor-based threats due to the lack of proper security measurements available to control use of sensors by apps. By exploiting the sensors (e.g., accelerometer, gyroscope, microphone, light sensor, etc.) on an IoT device, attackers can extract information from the device, transfer malware to a device, or trigger a malicious activity to compromise the device. In this survey, we explore various threats targeting IoT devices and discuss how their sensors can be abused for malicious purposes. Specifically, we present a detailed survey about existing sensor-based threats to IoT devices and countermeasures that are developed specifically to secure the sensors of IoT devices. Furthermore, we discuss security and privacy issues of IoT devices in the context of sensor-based threats and conclude with future research directions.

</details>

<details>

<summary>2018-02-07 17:46:02 - Blind Pre-Processing: A Robust Defense Method Against Adversarial Examples</summary>

- *Adnan Siraj Rakin, Zhezhi He, Boqing Gong, Deliang Fan*

- `1802.01549v2` - [abs](http://arxiv.org/abs/1802.01549v2) - [pdf](http://arxiv.org/pdf/1802.01549v2)

> Deep learning algorithms and networks are vulnerable to perturbed inputs which is known as the adversarial attack. Many defense methodologies have been investigated to defend against such adversarial attack. In this work, we propose a novel methodology to defend the existing powerful attack model. We for the first time introduce a new attacking scheme for the attacker and set a practical constraint for white box attack. Under this proposed attacking scheme, we present the best defense ever reported against some of the recent strong attacks. It consists of a set of nonlinear function to process the input data which will make it more robust over the adversarial attack. However, we make this processing layer completely hidden from the attacker. Blind pre-processing improves the white box attack accuracy of MNIST from 94.3\% to 98.7\%. Even with increasing defense when others defenses completely fail, blind pre-processing remains one of the strongest ever reported. Another strength of our defense is that it eliminates the need for adversarial training as it can significantly increase the MNIST accuracy without adversarial training as well. Additionally, blind pre-processing can also increase the inference accuracy in the face of a powerful attack on CIFAR-10 and SVHN data set as well without much sacrificing clean data accuracy.

</details>

<details>

<summary>2018-02-07 18:06:48 - A Game-Theoretic Approach to Design Secure and Resilient Distributed Support Vector Machines</summary>

- *Rui Zhang, Quanyan Zhu*

- `1802.02907v1` - [abs](http://arxiv.org/abs/1802.02907v1) - [pdf](http://arxiv.org/pdf/1802.02907v1)

> Distributed Support Vector Machines (DSVM) have been developed to solve large-scale classification problems in networked systems with a large number of sensors and control units. However, the systems become more vulnerable as detection and defense are increasingly difficult and expensive. This work aims to develop secure and resilient DSVM algorithms under adversarial environments in which an attacker can manipulate the training data to achieve his objective. We establish a game-theoretic framework to capture the conflicting interests between an adversary and a set of distributed data processing units. The Nash equilibrium of the game allows predicting the outcome of learning algorithms in adversarial environments, and enhancing the resilience of the machine learning through dynamic distributed learning algorithms. We prove that the convergence of the distributed algorithm is guaranteed without assumptions on the training data or network topologies. Numerical experiments are conducted to corroborate the results. We show that network topology plays an important role in the security of DSVM. Networks with fewer nodes and higher average degrees are more secure. Moreover, a balanced network is found to be less vulnerable to attacks.

</details>

<details>

<summary>2018-02-07 19:59:46 - A Diversity-based Substation Cyber Defense Strategy utilizing Coloring Games</summary>

- *Md Touhiduzzaman, Adam Hahn, Anurag Srivastava*

- `1802.02618v1` - [abs](http://arxiv.org/abs/1802.02618v1) - [pdf](http://arxiv.org/pdf/1802.02618v1)

> Growing cybersecurity risks in the power grid require that utilities implement a variety of security mechanism (SM) composed mostly of VPNs, firewalls, or other custom security components. While they provide some protection, they might contain software vulnerabilities which can lead to a cyber-attack. In this paper, the severity of a cyber-attack has been decreased by employing a diverse set of SM that reduce repetition of a single vulnerability. This paper focuses on the allocation of diverse SM and tries to increase the security of the cyber assets located within the electronic security perimeter(ESP) of a substation. We have used a graph-based coloring game in a distributed manner to allocate diverse SM for protecting the cyber assets. The vulnerability assessment for power grid network is also analyzed using this game theoretic method. An improved, diversified SMs for worst-case scenario has been demonstrated by reaching the Nash equilibrium of graph coloring game. As a case study, we analyze the IEEE-14 and IEEE-118 bus system, observe the different distributed coloring algorithm for allocating diverse SM and calculating the overall network criticality.

</details>

<details>

<summary>2018-02-08 08:48:03 - Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks</summary>

- *Thilo Strauss, Markus Hanselmann, Andrej Junginger, Holger Ulmer*

- `1709.03423v2` - [abs](http://arxiv.org/abs/1709.03423v2) - [pdf](http://arxiv.org/pdf/1709.03423v2)

> Deep learning has become the state of the art approach in many machine learning problems such as classification. It has recently been shown that deep learning is highly vulnerable to adversarial perturbations. Taking the camera systems of self-driving cars as an example, small adversarial perturbations can cause the system to make errors in important tasks, such as classifying traffic signs or detecting pedestrians. Hence, in order to use deep learning without safety concerns a proper defense strategy is required. We propose to use ensemble methods as a defense strategy against adversarial perturbations. We find that an attack leading one model to misclassify does not imply the same for other networks performing the same task. This makes ensemble methods an attractive defense strategy against adversarial attacks. We empirically show for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve the accuracy of neural networks on test data but also increase their robustness against adversarial perturbations.

</details>

<details>

<summary>2018-02-08 20:34:19 - Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection</summary>

- *Andrea Paudice, Luis Muñoz-González, Andras Gyorgy, Emil C. Lupu*

- `1802.03041v1` - [abs](http://arxiv.org/abs/1802.03041v1) - [pdf](http://arxiv.org/pdf/1802.03041v1)

> Machine learning has become an important component for many systems and applications including computer vision, spam filtering, malware and network intrusion detection, among others. Despite the capabilities of machine learning algorithms to extract valuable information from data and produce accurate predictions, it has been shown that these algorithms are vulnerable to attacks. Data poisoning is one of the most relevant security threats against machine learning systems, where attackers can subvert the learning process by injecting malicious samples in the training data. Recent work in adversarial machine learning has shown that the so-called optimal attack strategies can successfully poison linear classifiers, degrading the performance of the system dramatically after compromising a small fraction of the training dataset. In this paper we propose a defence mechanism to mitigate the effect of these optimal poisoning attacks based on outlier detection. We show empirically that the adversarial examples generated by these attack strategies are quite different from genuine points, as no detectability constrains are considered to craft the attack. Hence, they can be detected with an appropriate pre-filtering of the training dataset.

</details>

<details>

<summary>2018-02-10 04:49:12 - EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples</summary>

- *Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh*

- `1709.04114v3` - [abs](http://arxiv.org/abs/1709.04114v3) - [pdf](http://arxiv.org/pdf/1709.04114v3)

> Recent studies have highlighted the vulnerability of deep neural networks (DNNs) to adversarial examples - a visually indistinguishable adversarial image can easily be crafted to cause a well-trained model to misclassify. Existing methods for crafting adversarial examples are based on $L_2$ and $L_\infty$ distortion metrics. However, despite the fact that $L_1$ distortion accounts for the total variation and encourages sparsity in the perturbation, little has been developed for crafting $L_1$-based adversarial examples. In this paper, we formulate the process of attacking DNNs via adversarial examples as an elastic-net regularized optimization problem. Our elastic-net attacks to DNNs (EAD) feature $L_1$-oriented adversarial examples and include the state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST, CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial examples with small $L_1$ distortion and attains similar attack performance to the state-of-the-art methods in different attack scenarios. More importantly, EAD leads to improved attack transferability and complements adversarial training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in adversarial machine learning and security implications of DNNs.

</details>

<details>

<summary>2018-02-10 06:35:23 - Aurora: Providing Trusted System Services for Enclaves On an Untrusted System</summary>

- *Hongliang Liang, Mingyu Li, Qiong Zhang, Yue Yu, Lin Jiang, Yixiu Chen*

- `1802.03530v1` - [abs](http://arxiv.org/abs/1802.03530v1) - [pdf](http://arxiv.org/pdf/1802.03530v1)

> Intel SGX provisions shielded executions for security-sensitive computation, but lacks support for trusted system services (TSS), such as clock, network and filesystem. This makes \textit{enclaves} vulnerable to Iago attacks~\cite{DBLP:conf/asplos/CheckowayS13} in the face of a powerful malicious system. To mitigate this problem, we present Aurora, a novel architecture that provides TSSes via a secure channel between enclaves and devices on top of an untrusted system, and implement two types of TSSes, i.e. clock and end-to-end network. We evaluate our solution by porting SQLite and OpenSSL into Aurora, experimental results show that SQLite benefits from a \textit{microsecond} accuracy trusted clock and OpenSSL gains end-to-end secure network with about 1ms overhead.

</details>

<details>

<summary>2018-02-10 10:16:20 - About being the Tortoise or the Hare? - A Position Paper on Making Cloud Applications too Fast and Furious for Attackers</summary>

- *Nane Kratzke*

- `1802.03565v1` - [abs](http://arxiv.org/abs/1802.03565v1) - [pdf](http://arxiv.org/pdf/1802.03565v1)

> Cloud applications expose - beside service endpoints - also potential or actual vulnerabilities. And attackers have several advantages on their side. They can select the weapons, the point of time and the point of attack. Very often cloud application security engineering efforts focus to harden the fortress walls but seldom assume that attacks may be successful. So, cloud applications rely on their defensive walls but seldom attack intruders actively. Biological systems are different. They accept that defensive "walls" can be breached at several layers and therefore make use of an active and adaptive defense system to attack potential intruders - an immune system. This position paper proposes such an immune system inspired approach to ensure that even undetected intruders can be purged out of cloud applications. This makes it much harder for intruders to maintain a presence on victim systems. Evaluation experiments with popular cloud service infrastructures (Amazon Web Services, Google Compute Engine, Azure and OpenStack) showed that this could minimize the undetected acting period of intruders down to minutes.

</details>

<details>

<summary>2018-02-10 14:51:24 - Automatic Phone Slip Detection System</summary>

- *Karthik R, Preetam Satapath, Srivatsa Patnaik, Saurabh Priyadarshi, Rajesh Kumar M*

- `1802.04252v1` - [abs](http://arxiv.org/abs/1802.04252v1) - [pdf](http://arxiv.org/pdf/1802.04252v1)

> Mobile phones are becoming increasingly advanced and the latest ones are equipped with many diverse and powerful sensors. These sensors can be used to study different position and orientation of the phone which can help smartphone manufacture to track about their customers handling from the recorded log. The inbuilt sensors such as the accelerometer and gyroscope present in our phones are used to obtain data for acceleration and orientation of the phone in the three axes for different phone vulnerable position. From the data obtained appropriate features are extracted using various feature extraction techniques. The extracted features are then given to classifier such as neural network to classify them and decide whether the phone is in a vulnerable position to fall or it is in a safe position .In this paper we mainly concentrated on various case of handling the smartphone and classified by training the neural network.

</details>

<details>

<summary>2018-02-11 19:36:53 - MeltdownPrime and SpectrePrime: Automatically-Synthesized Attacks Exploiting Invalidation-Based Coherence Protocols</summary>

- *Caroline Trippel, Daniel Lustig, Margaret Martonosi*

- `1802.03802v1` - [abs](http://arxiv.org/abs/1802.03802v1) - [pdf](http://arxiv.org/pdf/1802.03802v1)

> The recent Meltdown and Spectre attacks highlight the importance of automated verification techniques for identifying hardware security vulnerabilities. We have developed a tool for synthesizing microarchitecture-specific programs capable of producing any user-specified hardware execution pattern of interest. Our tool takes two inputs: a formal description of (i) a microarchitecture in a domain-specific language, and (ii) a microarchitectural execution pattern of interest, e.g. a threat pattern. All programs synthesized by our tool are capable of producing the specified execution pattern on the supplied microarchitecture.   We used our tool to specify a hardware execution pattern common to Flush+Reload attacks and automatically synthesized security litmus tests representative of those that have been publicly disclosed for conducting Meltdown and Spectre attacks. We also formulated a Prime+Probe threat pattern, enabling our tool to synthesize a new variant of each---MeltdownPrime and SpectrePrime. Both of these new exploits use Prime+Probe approaches to conduct the timing attack. They are both also novel in that they are 2-core attacks which leverage the cache line invalidation mechanism in modern cache coherence protocols. These are the first proposed Prime+Probe variants of Meltdown and Spectre. But more importantly, both Prime attacks exploit invalidation-based coherence protocols to achieve the same level of precision as a Flush+Reload attack. While mitigation techniques in software (e.g., barriers that prevent speculation) will likely be the same for our Prime variants as for original Spectre and Meltdown, we believe that hardware protection against them will be distinct. As a proof of concept, we implemented SpectrePrime as a C program and ran it on an Intel x86 processor, averaging about the same accuracy as Spectre over 100 runs---97.9% for Spectre and 99.95% for SpectrePrime.

</details>

<details>

<summary>2018-02-13 03:45:28 - Predicting Adversarial Examples with High Confidence</summary>

- *Angus Galloway, Graham W. Taylor, Medhat Moussa*

- `1802.04457v1` - [abs](http://arxiv.org/abs/1802.04457v1) - [pdf](http://arxiv.org/pdf/1802.04457v1)

> It has been suggested that adversarial examples cause deep learning models to make incorrect predictions with high confidence. In this work, we take the opposite stance: an overly confident model is more likely to be vulnerable to adversarial examples. This work is one of the most proactive approaches taken to date, as we link robustness with non-calibrated model confidence on noisy images, providing a data-augmentation-free path forward. The adversarial examples phenomenon is most easily explained by the trend of increasing non-regularized model capacity, while the diversity and number of samples in common datasets has remained flat. Test accuracy has incorrectly been associated with true generalization performance, ignoring that training and test splits are often extremely similar in terms of the overall representation space. The transferability property of adversarial examples was previously used as evidence against overfitting arguments, a perceived random effect, but overfitting is not always random.

</details>

<details>

<summary>2018-02-13 19:10:12 - Identify Susceptible Locations in Medical Records via Adversarial Attacks on Deep Predictive Models</summary>

- *Mengying Sun, Fengyi Tang, Jinfeng Yi, Fei Wang, Jiayu Zhou*

- `1802.04822v1` - [abs](http://arxiv.org/abs/1802.04822v1) - [pdf](http://arxiv.org/pdf/1802.04822v1)

> The surging availability of electronic medical records (EHR) leads to increased research interests in medical predictive modeling. Recently many deep learning based predicted models are also developed for EHR data and demonstrated impressive performance. However, a series of recent studies showed that these deep models are not safe: they suffer from certain vulnerabilities. In short, a well-trained deep network can be extremely sensitive to inputs with negligible changes. These inputs are referred to as adversarial examples. In the context of medical informatics, such attacks could alter the result of a high performance deep predictive model by slightly perturbing a patient's medical records. Such instability not only reflects the weakness of deep architectures, more importantly, it offers guide on detecting susceptible parts on the inputs. In this paper, we propose an efficient and effective framework that learns a time-preferential minimum attack targeting the LSTM model with EHR inputs, and we leverage this attack strategy to screen medical records of patients and identify susceptible events and measurements. The efficient screening procedure can assist decision makers to pay extra attentions to the locations that can cause severe consequence if not measured correctly. We conduct extensive empirical studies on a real-world urgent care cohort and demonstrate the effectiveness of the proposed screening approach.

</details>

<details>

<summary>2018-02-13 23:05:05 - Understanding Membership Inferences on Well-Generalized Learning Models</summary>

- *Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, Kai Chen*

- `1802.04889v1` - [abs](http://arxiv.org/abs/1802.04889v1) - [pdf](http://arxiv.org/pdf/1802.04889v1)

> Membership Inference Attack (MIA) determines the presence of a record in a machine learning model's training data by querying the model. Prior work has shown that the attack is feasible when the model is overfitted to its training data or when the adversary controls the training algorithm. However, when the model is not overfitted and the adversary does not control the training algorithm, the threat is not well understood. In this paper, we report a study that discovers overfitting to be a sufficient but not a necessary condition for an MIA to succeed. More specifically, we demonstrate that even a well-generalized model contains vulnerable instances subject to a new generalized MIA (GMIA). In GMIA, we use novel techniques for selecting vulnerable instances and detecting their subtle influences ignored by overfitting metrics. Specifically, we successfully identify individual records with high precision in real-world datasets by querying black-box machine learning models. Further we show that a vulnerable record can even be indirectly attacked by querying other related records and existing generalization techniques are found to be less effective in protecting the vulnerable instances. Our findings sharpen the understanding of the fundamental cause of the problem: the unique influences the training instance may have on the model.

</details>

<details>

<summary>2018-02-14 18:50:53 - Towards Reverse-Engineering Black-Box Neural Networks</summary>

- *Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz*

- `1711.01768v3` - [abs](http://arxiv.org/abs/1711.01768v3) - [pdf](http://arxiv.org/pdf/1711.01768v3)

> Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks -- we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models.

</details>

<details>

<summary>2018-02-15 02:08:19 - Fooling OCR Systems with Adversarial Text Images</summary>

- *Congzheng Song, Vitaly Shmatikov*

- `1802.05385v1` - [abs](http://arxiv.org/abs/1802.05385v1) - [pdf](http://arxiv.org/pdf/1802.05385v1)

> We demonstrate that state-of-the-art optical character recognition (OCR) based on deep learning is vulnerable to adversarial images. Minor modifications to images of printed text, which do not change the meaning of the text to a human reader, cause the OCR system to "recognize" a different text where certain words chosen by the adversary are replaced by their semantic opposites. This completely changes the meaning of the output produced by the OCR system and by the NLP applications that use OCR for preprocessing their inputs.

</details>

<details>

<summary>2018-02-16 13:40:02 - Fooling End-to-end Speaker Verification by Adversarial Examples</summary>

- *Felix Kreuk, Yossi Adi, Moustapha Cisse, Joseph Keshet*

- `1801.03339v2` - [abs](http://arxiv.org/abs/1801.03339v2) - [pdf](http://arxiv.org/pdf/1801.03339v2)

> Automatic speaker verification systems are increasingly used as the primary means to authenticate costumers. Recently, it has been proposed to train speaker verification systems using end-to-end deep neural models. In this paper, we show that such systems are vulnerable to adversarial example attack. Adversarial examples are generated by adding a peculiar noise to original speaker examples, in such a way that they are almost indistinguishable from the original examples by a human listener. Yet, the generated waveforms, which sound as speaker A can be used to fool such a system by claiming as if the waveforms were uttered by speaker B. We present white-box attacks on an end-to-end deep network that was either trained on YOHO or NTIMIT. We also present two black-box attacks: where the adversarial examples were generated with a system that was trained on YOHO, but the attack is on a system that was trained on NTIMIT; and when the adversarial examples were generated with a system that was trained on Mel-spectrum feature set, but the attack is on a system that was trained on MFCC. Results suggest that the accuracy of the attacked system was decreased and the false-positive rate was dramatically increased.

</details>

<details>

<summary>2018-02-16 14:40:42 - Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models</summary>

- *Wieland Brendel, Jonas Rauber, Matthias Bethge*

- `1712.04248v2` - [abs](http://arxiv.org/abs/1712.04248v2) - [pdf](http://arxiv.org/pdf/1712.04248v2)

> Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at https://github.com/bethgelab/foolbox .

</details>

<details>

<summary>2018-02-18 00:18:01 - Autonomous Vehicle Speed Control for Safe Navigation of Occluded Pedestrian Crosswalk</summary>

- *Sarah Thornton*

- `1802.06314v1` - [abs](http://arxiv.org/abs/1802.06314v1) - [pdf](http://arxiv.org/pdf/1802.06314v1)

> Both humans and the sensors on an autonomous vehicle have limited sensing capabilities. When these limitations coincide with scenarios involving vulnerable road users, it becomes important to account for these limitations in the motion planner. For the scenario of an occluded pedestrian crosswalk, the speed of the approaching vehicle should be a function of the amount of uncertainty on the roadway. In this work, the longitudinal controller is formulated as a partially observable Markov decision process and dynamic programming is used to compute the control policy. The control policy scales the speed profile to be used by a model predictive steering controller.

</details>

<details>

<summary>2018-02-19 19:13:42 - Shield: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression</summary>

- *Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Siwei Li, Li Chen, Michael E. Kounavis, Duen Horng Chau*

- `1802.06816v1` - [abs](http://arxiv.org/abs/1802.06816v1) - [pdf](http://arxiv.org/pdf/1802.06816v1)

> The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed Shield defense framework, utilizing its capability to effectively "compress away" such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, Shield "vaccinates" a model by re-training it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, Shield adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes Shield a fortified multi-pronged protection. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 94% of black-box attacks and 98% of gray-box attacks delivered by the recent, strongest attacks, such as Carlini-Wagner's L2 and DeepFool. Our approaches are fast and work without requiring knowledge about the model.

</details>

<details>

<summary>2018-02-20 05:24:14 - ISA-Based Trusted Network Functions And Server Applications In The Untrusted Cloud</summary>

- *Spyridon Mastorakis, Tahrina Ahmed, Jayaprakash Pisharath*

- `1802.06970v1` - [abs](http://arxiv.org/abs/1802.06970v1) - [pdf](http://arxiv.org/pdf/1802.06970v1)

> Nowadays, enterprises widely deploy Network Functions (NFs) and server applications in the cloud. However, processing of sensitive data and trusted execution cannot be securely deployed in the untrusted cloud. Cloud providers themselves could accidentally leak private information (e.g., due to misconfigurations) or rogue users could exploit vulnerabilities of the providers' systems to compromise execution integrity, posing a threat to the confidentiality of internal enterprise and customer data. In this paper, we identify (i) a number of NF and server application use-cases that trusted execution can be applied to, (ii) the assets and impact of compromising the private data and execution integrity of each use-case, and (iii) we leverage Intel's Software Guard Extensions (SGX) architecture to design Trusted Execution Environments (TEEs) for cloud-based NFs and server applications. We combine SGX with the Data Plane Development KIT (DPDK) to prototype and evaluate our TEEs for a number of application scenarios (Layer 2 frame and Layer 3 packet processing for plain and encrypted traffic, traffic load-balancing and back-end server processing). Our results indicate that NFs involving plain traffic can achieve almost native performance (e.g., ~22 Million Packets Per Second for Layer 3 forwarding for 64-byte frames), while NFs involving encrypted traffic and server processing can still achieve competitive performance (e.g., ~12 Million Packets Per Second for server processing for 64-byte frames).

</details>

<details>

<summary>2018-02-20 05:35:36 - Provably Minimally-Distorted Adversarial Examples</summary>

- *Nicholas Carlini, Guy Katz, Clark Barrett, David L. Dill*

- `1709.10207v2` - [abs](http://arxiv.org/abs/1709.10207v2) - [pdf](http://arxiv.org/pdf/1709.10207v2)

> The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the presence of adversarial examples: slightly perturbed inputs that are misclassified by the network. In recent years, several techniques have been proposed for increasing robustness to adversarial examples --- and yet most of these have been quickly shown to be vulnerable to future attacks. For example, over half of the defenses proposed by papers accepted at ICLR 2018 have already been broken. We propose to address this difficulty through formal verification techniques. We show how to construct provably minimally distorted adversarial examples: given an arbitrary neural network and input sample, we can construct adversarial examples which we prove are of minimal distortion. Using this approach, we demonstrate that one of the recent ICLR defense proposals, adversarial retraining, provably succeeds at increasing the distortion required to construct adversarial examples by a factor of 4.2.

</details>

<details>

<summary>2018-02-20 09:19:11 - BAN-GZKP: Optimal Zero Knowledge Proof based Scheme for Wireless Body Area Networks</summary>

- *Gewu Bu, Maria Potop-Butucaru*

- `1802.07023v1` - [abs](http://arxiv.org/abs/1802.07023v1) - [pdf](http://arxiv.org/pdf/1802.07023v1)

> BANZKP is the best to date Zero Knowledge Proof (ZKP) based secure lightweight and energy efficient authentication scheme designed for Wireless Area Network (WBAN). It is vulnerable to several security attacks such as the replay attack, Distributed Denial-of-Service (DDoS) attacks at sink and redundancy information crack. However, BANZKP needs an end-to-end authentication which is not compliant with the human body postural mobility. We propose a new scheme BAN-GZKP. Our scheme improves both the security and postural mobility resilience of BANZKP. Moreover, BAN-GZKP uses only a three-phase authentication which is optimal in the class of ZKP protocols. To fix the security vulnerabilities of BANZKP, BAN-GZKP uses a novel random key allocation and a Hop-by-Hop authentication definition. We further prove the reliability of our scheme to various attacks including those to which BANZKP is vulnerable. Furthermore, via extensive simulations we prove that our scheme, BAN-GZKP, outperforms BANZKP in terms of reliability to human body postural mobility for various network parameters (end-to-end delay, number of packets exchanged in the network, number of transmissions). We compared both schemes using representative convergecast strategies with various transmission rates and human postural mobility. Finally, it is important to mention that BAN-GZKP has no additional cost compared to BANZKP in terms memory, computational complexity or energy consumption.

</details>

<details>

<summary>2018-02-21 17:43:56 - Predicting Natural Hazards with Neuronal Networks</summary>

- *Matthias Rauter, Daniel Winkler*

- `1802.07257v1` - [abs](http://arxiv.org/abs/1802.07257v1) - [pdf](http://arxiv.org/pdf/1802.07257v1)

> Gravitational mass flows, such as avalanches, debris flows and rockfalls are common events in alpine regions with high impact on transport routes. Within the last few decades, hazard zone maps have been developed to systematically approach this threat. These maps mark vulnerable zones in habitable areas to allow effective planning of hazard mitigation measures and development of settlements. Hazard zone maps have shown to be an effective tool to reduce fatalities during extreme events. They are created in a complex process, based on experience, empirical models, physical simulations and historical data. The generation of such maps is therefore expensive and limited to crucially important regions, e.g. permanently inhabited areas.   In this work we interpret the task of hazard zone mapping as a classification problem. Every point in a specific area has to be classified according to its vulnerability. On a regional scale this leads to a segmentation problem, where the total area has to be divided in the respective hazard zones. The recent developments in artificial intelligence, namely convolutional neuronal networks, have led to major improvement in a very similar task, image classification and semantic segmentation, i.e. computer vision. We use a convolutional neuronal network to identify terrain formations with the potential for catastrophic snow avalanches and label points in their reach as vulnerable. Repeating this procedure for all points allows us to generate an artificial hazard zone map. We demonstrate that the approach is feasible and promising based on the hazard zone map of the Tirolean Oberland. However, more training data and further improvement of the method is required before such techniques can be applied reliably.

</details>

<details>

<summary>2018-02-25 22:16:20 - Blindsight: Blinding EM Side-Channel Leakage using Built-In Fully Integrated Inductive Voltage Regulator</summary>

- *Monodeep Kar, Arvind Singh, Sanu Mathew, Santosh Ghosh, Anand Rajan, Vivek De, Raheem Beyah, Saibal Mukhopadhyay*

- `1802.09096v1` - [abs](http://arxiv.org/abs/1802.09096v1) - [pdf](http://arxiv.org/pdf/1802.09096v1)

> Modern high-performance as well as power-constrained System-on-Chips (SoC) are increasingly using hardware accelerated encryption engines to secure computation, memory access, and communication operations. The electromagnetic (EM) emission from a chip leaks information of the underlying logical operations and can be collected using low-cost non-invasive measurements. EM based side-channel attacks (EMSCA) have emerged as a major threat to security of encryption engines in a SoC. This paper presents the concept of Blindsight where a high-frequency inductive voltage regulator (IVR) integrated on the same chip with an encryption engine is used to increase resistance against EMSCA. High-frequency (~100MHz) IVRs are present in modern microprocessors to improve energy-efficiency. We show that an IVR with a randomized control loop (R-IVR) can reduce EMSCA as the integrated inductance acts as a strong EM emitter and blinds an adversary from EM emission of the encryption engine. The EM measurements are performed on a test-chip containing two architectures of a 128-bit Advanced Encryption Standard (AES) engine powered by a high-frequency R-IVR and under two attack scenarios, one, where an adversary gains complete physical access of the target device and the other, where the adversary is only in proximity of the device. In both attack modes, an adversary can observe information leakage in Test Vector Leakage Assessment (TVLA) test in a baseline IVR (B-IVR, without control loop randomization). However, we show that EM emission from the R-IVR blinds the attacker and significantly reduces SCA vulnerability of the AES engine. A range of practical side-channel analysis including TVLA, Correlation Electromagnetic Analysis (CEMA), and a template based CEMA shows that R-IVR can reduce information leakage and prevent key extraction even against a skilled adversary.

</details>

<details>

<summary>2018-02-26 18:10:26 - Tool Demonstration: FSolidM for Designing Secure Ethereum Smart Contracts</summary>

- *Anastasia Mavridou, Aron Laszka*

- `1802.09949v1` - [abs](http://arxiv.org/abs/1802.09949v1) - [pdf](http://arxiv.org/pdf/1802.09949v1)

> Blockchain-based distributed computing platforms enable the trusted execution of computation - defined in the form of smart contracts - without trusted agents. Smart contracts are envisioned to have a variety of applications, ranging from financial to IoT asset tracking. Unfortunately, the development of smart contracts has proven to be extremely error prone. In practice, contracts are riddled with security vulnerabilities comprising a critical issue since bugs are by design non-fixable and contracts may handle financial assets of significant value. To facilitate the development of secure smart contracts, we have created the FSolidM framework, which allows developers to define contracts as finite state machines (FSMs) with rigorous and clear semantics. FSolidM provides an easy-to-use graphical editor for specifying FSMs, a code generator for creating Ethereum smart contracts, and a set of plugins that developers may add to their FSMs to enhance security and functionality.

</details>

<details>

<summary>2018-02-27 03:46:38 - Understanding and Enhancing the Transferability of Adversarial Examples</summary>

- *Lei Wu, Zhanxing Zhu, Cheng Tai, Weinan E*

- `1802.09707v1` - [abs](http://arxiv.org/abs/1802.09707v1) - [pdf](http://arxiv.org/pdf/1802.09707v1)

> State-of-the-art deep neural networks are known to be vulnerable to adversarial examples, formed by applying small but malicious perturbations to the original inputs. Moreover, the perturbations can \textit{transfer across models}: adversarial examples generated for a specific model will often mislead other unseen models. Consequently the adversary can leverage it to attack deployed systems without any query, which severely hinder the application of deep learning, especially in the areas where security is crucial. In this work, we systematically study how two classes of factors that might influence the transferability of adversarial examples. One is about model-specific factors, including network architecture, model capacity and test accuracy. The other is the local smoothness of loss function for constructing adversarial examples. Based on these understanding, a simple but effective strategy is proposed to enhance transferability. We call it variance-reduced attack, since it utilizes the variance-reduced gradient to generate adversarial example. The effectiveness is confirmed by a variety of experiments on both CIFAR-10 and ImageNet datasets.

</details>

<details>

<summary>2018-02-27 17:32:11 - Know Your Enemy: Characteristics of Cyber-Attacks on Medical Imaging Devices</summary>

- *Tom Mahler, Nir Nissim, Erez Shalom, Israel Goldenberg, Guy Hassman, Arnon Makori, Itzik Kochav, Yuval Elovici, Yuval Shahar*

- `1801.05583v2` - [abs](http://arxiv.org/abs/1801.05583v2) - [pdf](http://arxiv.org/pdf/1801.05583v2)

> Purpose: Used extensively in the diagnosis, treatment, and prevention of disease, Medical Imaging Devices (MIDs), such as Magnetic Resonance Imaging (MRI) or Computed Tomography (CT) machines, play an important role in medicine today. MIDs are increasingly connected to hospital networks, making them vulnerable to sophisticated cyber-attacks targeting the devices' infrastructure and components, which can disrupt digital patient records, and potentially jeopardize patients' health. Attacks on MIDs are likely to increase, as attackers' skills improve and the number of unpatched devices with known vulnerabilities that can be easily exploited grows. Attackers may also block access to MIDs or disable them, as part of ransomware attacks, which have been shown to be successful against hospitals. Method and Materials: We conducted a comprehensive risk analysis survey at the Malware-Lab, based on the Confidentiality, Integrity, and Availability (CIA) model, in collaboration with our country's largest health maintenance organization, to define the characteristics of cyber-attacks on MIDs. The survey includes a range of vulnerabilities and potential attacks aimed at MIDs, medical and imaging information systems, and medical protocols and standards such as DICOM and HL7. Results: Based on our survey, we found that CT devices face the greatest risk of cyber-attack, due to their pivotal role in acute care imaging. Thus, we identified several possible attack vectors that target the infrastructure and functionality of CT devices, which can cause: 1. Disruption of the parameters' values used in the scanning protocols within the CT devices (e.g., tampering with the radiation exposure levels); 2. Mechanical disruption of the CT device (e.g., changing the pitch); 3. Disruption of the tomography scan signals constructing the digital images; and 4. Denial-of-Service attacks against the CT device.

</details>


## 2018-03

<details>

<summary>2018-03-01 10:12:27 - Differentially Private Federated Learning: A Client Level Perspective</summary>

- *Robin C. Geyer, Tassilo Klein, Moin Nabi*

- `1712.07557v2` - [abs](http://arxiv.org/abs/1712.07557v2) - [pdf](http://arxiv.org/pdf/1712.07557v2)

> Federated learning is a recent advance in privacy protection. In this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. However, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. We tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. Empirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance.

</details>

<details>

<summary>2018-03-05 00:17:05 - Stochastic Activation Pruning for Robust Adversarial Defense</summary>

- *Guneet S. Dhillon, Kamyar Azizzadenesheli, Zachary C. Lipton, Jeremy Bernstein, Jean Kossaifi, Aran Khanna, Anima Anandkumar*

- `1803.01442v1` - [abs](http://arxiv.org/abs/1803.01442v1) - [pdf](http://arxiv.org/pdf/1803.01442v1)

> Neural networks are known to be vulnerable to adversarial examples. Carefully chosen perturbations to real images, while imperceptible to humans, induce misclassification and threaten the reliability of deep learning systems in the wild. To guard against adversarial examples, we take inspiration from game theory and cast the problem as a minimax zero-sum game between the adversary and the model. In general, for such games, the optimal strategy for both players requires a stochastic policy, also known as a mixed strategy. In this light, we propose Stochastic Activation Pruning (SAP), a mixed strategy for adversarial defense. SAP prunes a random subset of activations (preferentially pruning those with smaller magnitude) and scales up the survivors to compensate. We can apply SAP to pretrained networks, including adversarially trained models, without fine-tuning, providing robustness against adversarial examples. Experiments demonstrate that SAP confers robustness against attacks, increasing accuracy and preserving calibration.

</details>

<details>

<summary>2018-03-06 10:34:47 - Where is my Device? - Detecting the Smart Device's Wearing Location in the Context of Active Safety for Vulnerable Road Users</summary>

- *Maarten Bieshaar*

- `1803.02097v1` - [abs](http://arxiv.org/abs/1803.02097v1) - [pdf](http://arxiv.org/pdf/1803.02097v1)

> This article describes an approach to detect the wearing location of smart devices worn by pedestrians and cyclists. The detection, which is based solely on the sensors of the smart devices, is important context-information which can be used to parametrize subsequent algorithms, e.g. for dead reckoning or intention detection to improve the safety of vulnerable road users. The wearing location recognition can in terms of Organic Computing (OC) be seen as a step towards self-awareness and self-adaptation. For the wearing location detection a two-stage process is presented. It is subdivided into moving detection followed by the wearing location classification. Finally, the approach is evaluated on a real world dataset consisting of pedestrians and cyclists.

</details>

<details>

<summary>2018-03-07 15:01:40 - Co-processor-based Behavior Monitoring: Application to the Detection of Attacks Against the System Management Mode</summary>

- *Ronny Chevalier, Maugan Villatel, David Plaquin, Guillaume Hiet*

- `1803.02700v1` - [abs](http://arxiv.org/abs/1803.02700v1) - [pdf](http://arxiv.org/pdf/1803.02700v1)

> Highly privileged software, such as firmware, is an attractive target for attackers. Thus, BIOS vendors use cryptographic signatures to ensure firmware integrity at boot time. Nevertheless, such protection does not prevent an attacker from exploiting vulnerabilities at runtime. To detect such attacks, we propose an event-based behavior monitoring approach that relies on an isolated co-processor. We instrument the code executed on the main CPU to send information about its behavior to the monitor. This information helps to resolve the semantic gap issue. Our approach does not depend on a specific model of the behavior nor on a specific target. We apply this approach to detect attacks targeting the System Management Mode (SMM), a highly privileged x86 execution mode executing firmware code at runtime. We model the behavior of SMM using invariants of its control-flow and relevant CPU registers (CR3 and SMBASE). We instrument two open-source firmware implementations: EDK II and coreboot. We evaluate the ability of our approach to detect state-of-the-art attacks and its runtime execution overhead by simulating an x86 system coupled with an ARM Cortex A5 co-processor. The results show that our solution detects intrusions from the state of the art, without any false positives, while remaining acceptable in terms of performance overhead in the context of the SMM (i.e., less than the 150 $\mu$s threshold defined by Intel).

</details>

<details>

<summary>2018-03-08 23:15:12 - Deep RNN-Oriented Paradigm Shift through BOCANet: Broken Obfuscated Circuit Attack</summary>

- *Fatemeh Tehranipoor, Nima Karimian, Mehran Mozaffari Kermani, Hamid Mahmoodi*

- `1803.03332v1` - [abs](http://arxiv.org/abs/1803.03332v1) - [pdf](http://arxiv.org/pdf/1803.03332v1)

> This is the first work augmenting hardware attacks mounted on obfuscated circuits by incorporating deep recurrent neural network (D-RNN). Logic encryption obfuscation has been used for thwarting counterfeiting, overproduction, and reverse engineering but vulnerable to attacks. There have been efficient schemes, e.g., satisfiability-checking (SAT) based attack, which can potentially compromise hardware obfuscation circuits. Nevertheless, not only there exist countermeasures against such attacks in the state-of-the-art (including the recent delay+logic locking (DLL) scheme in DAC'17), but the sheer amount of time/resources to mount the attack could hinder its efficacy. In this paper, we propose a deep RNN-oriented approach, called BOCANet, to (i) compromise the obfuscated hardware at least an order-of magnitude more efficiently (>20X faster with relatively high success rate) compared to existing attacks; (ii) attack such locked hardware even when the resources to the attacker are only limited to insignificant number of I/O pairs (< 0.5\%) to reconstruct the secret key; and (iii) break a number of experimented benchmarks (ISCAS-85 c423, c1355, c1908, and c7552) successfully.

</details>

<details>

<summary>2018-03-09 11:57:36 - Highly Automated Learning for Improved Active Safety of Vulnerable Road Users</summary>

- *Maarten Bieshaar, Günther Reitberger, Viktor Kreß, Stefan Zernetsch, Konrad Doll, Erich Fuchs, Bernhard Sick*

- `1803.03479v1` - [abs](http://arxiv.org/abs/1803.03479v1) - [pdf](http://arxiv.org/pdf/1803.03479v1)

> Highly automated driving requires precise models of traffic participants. Many state of the art models are currently based on machine learning techniques. Among others, the required amount of labeled data is one major challenge. An autonomous learning process addressing this problem is proposed. The initial models are iteratively refined in three steps: (1) detection and context identification, (2) novelty detection and active learning and (3) online model adaption.

</details>

<details>

<summary>2018-03-09 17:40:20 - Detecting Adversarial Examples - A Lesson from Multimedia Forensics</summary>

- *Pascal Schöttle, Alexander Schlögl, Cecilia Pasquini, Rainer Böhme*

- `1803.03613v1` - [abs](http://arxiv.org/abs/1803.03613v1) - [pdf](http://arxiv.org/pdf/1803.03613v1)

> Adversarial classification is the task of performing robust classification in the presence of a strategic attacker. Originating from information hiding and multimedia forensics, adversarial classification recently received a lot of attention in a broader security context. In the domain of machine learning-based image classification, adversarial classification can be interpreted as detecting so-called adversarial examples, which are slightly altered versions of benign images. They are specifically crafted to be misclassified with a very high probability by the classifier under attack. Neural networks, which dominate among modern image classifiers, have been shown to be especially vulnerable to these adversarial examples.   However, detecting subtle changes in digital images has always been the goal of multimedia forensics and steganalysis. In this paper, we highlight the parallels between these two fields and secure machine learning.   Furthermore, we adapt a linear filter, similar to early steganalysis methods, to detect adversarial examples that are generated with the projected gradient descent (PGD) method, the state-of-the-art algorithm for this task. We test our method on the MNIST database and show for several parameter combinations of PGD that our method can reliably detect adversarial examples.   Additionally, the combination of adversarial re-training and our detection method effectively reduces the attack surface of attacks against neural networks. Thus, we conclude that adversarial examples for image classification possibly do not withstand detection methods from steganalysis, and future work should explore the effectiveness of known techniques from multimedia forensics in other adversarial settings.

</details>

<details>

<summary>2018-03-11 22:44:10 - Mind Your Credit: Assessing the Health of the Ripple Credit Network</summary>

- *Pedro Moreno-Sanchez, Navin Modi, Raghuvir Songhela, Aniket Kate, Sonia Fahmy*

- `1706.02358v2` - [abs](http://arxiv.org/abs/1706.02358v2) - [pdf](http://arxiv.org/pdf/1706.02358v2)

> The Ripple credit network has emerged as a payment backbone with key advantages for financial institutions and the remittance industry. Its path-based IOweYou (IOU) settlements across different (crypto)currencies conceptually distinguishes the Ripple blockchain from cryptocurrencies, and makes it highly suitable to an orthogonal yet vast set of applications in the remittance world for cross-border transactions and beyond.   This work studies the structure and evolution of the Ripple network since its inception, and investigates its vulnerability to devilry attacks that affect the credit of linnet users' wallets. We find that about 13M USD are at risk in the current Ripple network due to inappropriate configuration of the rippling flag on credit links, facilitating undesired redistribution of credit across those links. Although the Ripple network has grown around a few highly connected hub (gateway) wallets that constitute the network's core and provide high liquidity to users, such a credit link distribution results in a user base of around 112,000 wallets that can be financially isolated by as few as 10 highly connected gateway wallets. Indeed, today about 4.9M USD cannot be withdrawn by their owners from the Ripple network due to PayRoutes, a gateway tagged as faulty by the Ripple community. Finally, we observe that stale exchange offers pose a real problem, and exchanges (market makers) have not always been vigilant about periodically updating their exchange offers according to current real-world exchange rates. For example, stale offers were used by 84 Ripple wallets to gain more than 4.5M USD from mid-July to mid-August 2017. Our findings should prompt the Ripple community to improve the health of the network by educating its users on increasing their connectivity, and by appropriately maintaining the credit limits, rippling flags, and exchange offers on their credit links.

</details>

<details>

<summary>2018-03-12 10:27:17 - Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables</summary>

- *Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert, Fabio Roli*

- `1803.04173v1` - [abs](http://arxiv.org/abs/1803.04173v1) - [pdf](http://arxiv.org/pdf/1803.04173v1)

> Machine-learning methods have already been exploited as useful tools for detecting malicious executable files. They leverage data retrieved from malware samples, such as header fields, instruction sequences, or even raw bytes, to learn models that discriminate between benign and malicious software. However, it has also been shown that machine learning and deep neural networks can be fooled by evasion attacks (also referred to as adversarial examples), i.e., small changes to the input data that cause misclassification at test time. In this work, we investigate the vulnerability of malware detection methods that use deep networks to learn from raw bytes. We propose a gradient-based attack that is capable of evading a recently-proposed deep network suited to this purpose by only changing few specific bytes at the end of each malware sample, while preserving its intrusive functionality. Promising results show that our adversarial malware binaries evade the targeted network with high probability, even though less than 1% of their bytes are modified.

</details>

<details>

<summary>2018-03-12 15:01:14 - HardScope: Thwarting DOP with Hardware-assisted Run-time Scope Enforcement</summary>

- *Thomas Nyman, Ghada Dessouky, Shaza Zeitouni, Aaro Lehikoinen, Andrew Paverd, N. Asokan, Ahmad-Reza Sadeghi*

- `1705.10295v2` - [abs](http://arxiv.org/abs/1705.10295v2) - [pdf](http://arxiv.org/pdf/1705.10295v2)

> Widespread use of memory unsafe programming languages (e.g., C and C++) leaves many systems vulnerable to memory corruption attacks. A variety of defenses have been proposed to mitigate attacks that exploit memory errors to hijack the control flow of the code at run-time, e.g., (fine-grained) randomization or Control Flow Integrity. However, recent work on data-oriented programming (DOP) demonstrated highly expressive (Turing-complete) attacks, even in the presence of these state-of-the-art defenses. Although multiple real-world DOP attacks have been demonstrated, no efficient defenses are yet available. We propose run-time scope enforcement (RSE), a novel approach designed to efficiently mitigate all currently known DOP attacks by enforcing compile-time memory safety constraints (e.g., variable visibility rules) at run-time. We present HardScope, a proof-of-concept implementation of hardware-assisted RSE for the new RISC-V open instruction set architecture. We discuss our systematic empirical evaluation of HardScope which demonstrates that it can mitigate all currently known DOP attacks, and has a real-world performance overhead of 3.2% in embedded benchmarks.

</details>

<details>

<summary>2018-03-13 08:27:39 - A Systematic Evaluation of Static API-Misuse Detectors</summary>

- *Sven Amann, Hoan Anh Nguyen, Sarah Nadi, Tien N. Nguyen, Mira Mezini*

- `1712.00242v3` - [abs](http://arxiv.org/abs/1712.00242v3) - [pdf](http://arxiv.org/pdf/1712.00242v3)

> Application Programming Interfaces (APIs) often have usage constraints, such as restrictions on call order or call conditions. API misuses, i.e., violations of these constraints, may lead to software crashes, bugs, and vulnerabilities. Though researchers developed many API-misuse detectors over the last two decades, recent studies show that API misuses are still prevalent. Therefore, we need to understand the capabilities and limitations of existing detectors in order to advance the state of the art. In this paper, we present the first-ever qualitative and quantitative evaluation that compares static API-misuse detectors along the same dimensions, and with original author validation. To accomplish this, we develop MUC, a classification of API misuses, and MUBenchPipe, an automated benchmark for detector comparison, on top of our misuse dataset, MUBench. Our results show that the capabilities of existing detectors vary greatly and that existing detectors, though capable of detecting misuses, suffer from extremely low precision and recall. A systematic root-cause analysis reveals that, most importantly, detectors need to go beyond the naive assumption that a deviation from the most-frequent usage corresponds to a misuse and need to obtain additional usage examples to train their models. We present possible directions towards more-powerful API-misuse detectors.

</details>

<details>

<summary>2018-03-13 13:02:13 - Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning</summary>

- *Nicolas Papernot, Patrick McDaniel*

- `1803.04765v1` - [abs](http://arxiv.org/abs/1803.04765v1) - [pdf](http://arxiv.org/pdf/1803.04765v1)

> Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.

</details>

<details>

<summary>2018-03-13 15:29:24 - Reviewing KLEE's Sonar-Search Strategy in Context of Greybox Fuzzing</summary>

- *Saahil Ognawala, Alexander Pretschner, Thomas Hutzelmann, Eirini Psallida, Ricardo Nales Amato*

- `1803.04881v1` - [abs](http://arxiv.org/abs/1803.04881v1) - [pdf](http://arxiv.org/pdf/1803.04881v1)

> Automatic test-case generation techniques of symbolic execution and fuzzing are the most widely used methods to discover vulnerabilities in, both, academia and industry. However, both these methods suffer from fundamental drawbacks that stop them from achieving high path coverage that may, consequently, lead to discovering vulnerabilities at the numerical scale of static analysis. In this presentation, we examine systems-under-test (SUTs) at the granularity level of functions and postulate that achieving higher function coverage (execution of functions in a program at least once) than, both, symbolic execution and fuzzing may be a necessary condition for discovering more vulnerabilities than both. We will start this presentation with the design of a targeted search strategy for KLEE, sonar-search, that prioritizes paths leading to a target function, rather than maximizing overall path coverage in the program. Then, we will show that examining SUTs at the level of functions (compositional analysis) leads to discovering more vulnerabilities than symbolic execution from a single entry point. Using this finding, we will, then, demonstrate a greybox fuzzing method that can achieve higher function coverage than symbolic execution. Finally, we will present a framework to effectively manage vulnerabilities and assess their severities.

</details>

<details>

<summary>2018-03-13 20:41:14 - Android Inter-App Communication Threats, Solutions, and Challenges</summary>

- *Jice Wang, Hongqi Wu*

- `1803.05039v1` - [abs](http://arxiv.org/abs/1803.05039v1) - [pdf](http://arxiv.org/pdf/1803.05039v1)

> Researchers and commercial companies have made a lot of efforts on detecting malware in Android platform. However, a recent malware threat, App collusion, makes malware detection challenging. In App collusion, two or more Apps collaborate to perform malicious actions by communicating with each other, which makes single App analysis insufficient. In this paper, we first introduce Android security mechanism and communication channels used by android Applications. Then we summarize the security vulnerabilities and potential threats introduced by App communication. Finally, we discuss state of art researches and challenges on App collusion detection.

</details>

<details>

<summary>2018-03-14 07:24:10 - Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality</summary>

- *Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E. Houle, James Bailey*

- `1801.02613v3` - [abs](http://arxiv.org/abs/1801.02613v3) - [pdf](http://arxiv.org/pdf/1801.02613v3)

> Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called 'adversarial subspaces') in which adversarial examples lie. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the distinction of adversarial examples generated using state-of-the-art attacks. As a proof-of-concept, we show that a potential application of LID is to distinguish adversarial examples, and the preliminary results show that it can outperform several state-of-the-art detection measures by large margins for five attack strategies considered in this paper across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs.

</details>

<details>

<summary>2018-03-14 09:12:38 - Finding The Greedy, Prodigal, and Suicidal Contracts at Scale</summary>

- *Ivica Nikolic, Aashish Kolluri, Ilya Sergey, Prateek Saxena, Aquinas Hobor*

- `1802.06038v2` - [abs](http://arxiv.org/abs/1802.06038v2) - [pdf](http://arxiv.org/pdf/1802.06038v2)

> Smart contracts---stateful executable objects hosted on blockchains like Ethereum---carry billions of dollars worth of coins and cannot be updated once deployed. We present a new systematic characterization of a class of trace vulnerabilities, which result from analyzing multiple invocations of a contract over its lifetime. We focus attention on three example properties of such trace vulnerabilities: finding contracts that either lock funds indefinitely, leak them carelessly to arbitrary users, or can be killed by anyone. We implemented MAIAN, the first tool for precisely specifying and reasoning about trace properties, which employs inter-procedural symbolic analysis and concrete validator for exhibiting real exploits. Our analysis of nearly one million contracts flags 34,200 (2,365 distinct) contracts vulnerable, in 10 seconds per contract. On a subset of3,759 contracts which we sampled for concrete validation and manual analysis, we reproduce real exploits at a true positive rate of 89%, yielding exploits for3,686 contracts. Our tool finds exploits for the infamous Parity bug that indirectly locked 200 million dollars worth in Ether, which previous analyses failed to capture.

</details>

<details>

<summary>2018-03-14 22:56:31 - Machine learning-assisted virtual patching of web applications</summary>

- *Gustavo Betarte, Eduardo Giménez, Rodrigo Martínez, Álvaro Pardo*

- `1803.05529v1` - [abs](http://arxiv.org/abs/1803.05529v1) - [pdf](http://arxiv.org/pdf/1803.05529v1)

> Web applications are permanently being exposed to attacks that exploit their vulnerabilities. In this work we investigate the application of machine learning techniques to leverage Web Application Firewall (WAF), a technology that is used to detect and prevent attacks. We propose a combined approach of machine learning models, based on one-class classification and n-gram analysis, to enhance the detection and accuracy capabilities of MODSECURITY, an open source and widely used WAF. The results are promising and outperform MODSECURITY when configured with the OWASP Core Rule Set, the baseline configuration setting of a widely deployed, rule-based WAF technology. The proposed solution, combining both approaches, allow us to deploy a WAF when no training data for the application is available (using one-class classification), and an improved one using n-grams when training data is available.

</details>

<details>

<summary>2018-03-16 08:52:04 - Vulnerability of Deep Learning</summary>

- *Richard Kenway*

- `1803.06111v1` - [abs](http://arxiv.org/abs/1803.06111v1) - [pdf](http://arxiv.org/pdf/1803.06111v1)

> The Renormalisation Group (RG) provides a framework in which it is possible to assess whether a deep-learning network is sensitive to small changes in the input data and hence prone to error, or susceptible to adversarial attack. Distinct classification outputs are associated with different RG fixed points and sensitivity to small changes in the input data is due to the presence of relevant operators at a fixed point. A numerical scheme, based on Monte Carlo RG ideas, is proposed for identifying the existence of relevant operators and the corresponding directions of greatest sensitivity in the input data. Thus, a trained deep-learning network may be tested for its robustness and, if it is vulnerable to attack, dangerous perturbations of the input data identified.

</details>

<details>

<summary>2018-03-16 14:28:30 - The Hsu-Harn-Mu-Zhang-Zhu group key establishment protocol is insecure</summary>

- *Chris J Mitchell*

- `1803.05365v2` - [abs](http://arxiv.org/abs/1803.05365v2) - [pdf](http://arxiv.org/pdf/1803.05365v2)

> A significant security vulnerability in a recently published group key establishment protocol is described. This vulnerability allows a malicious insider to fraudulently establish a group key with an innocent victim, with the key chosen by the attacker. This shortcoming is sufficiently serious that the protocol should not be used.

</details>

<details>

<summary>2018-03-16 18:02:14 - Semantic Adversarial Examples</summary>

- *Hossein Hosseini, Radha Poovendran*

- `1804.00499v1` - [abs](http://arxiv.org/abs/1804.00499v1) - [pdf](http://arxiv.org/pdf/1804.00499v1)

> Deep neural networks are known to be vulnerable to adversarial examples, i.e., images that are maliciously perturbed to fool the model. Generating adversarial examples has been mostly limited to finding small perturbations that maximize the model prediction error. Such images, however, contain artificial perturbations that make them somewhat distinguishable from natural images. This property is used by several defense methods to counter adversarial examples by applying denoising filters or training the model to be robust to small perturbations.   In this paper, we introduce a new class of adversarial examples, namely "Semantic Adversarial Examples," as images that are arbitrarily perturbed to fool the model, but in such a way that the modified image semantically represents the same object as the original image. We formulate the problem of generating such images as a constrained optimization problem and develop an adversarial transformation based on the shape bias property of human cognitive system. In our method, we generate adversarial images by first converting the RGB image into the HSV (Hue, Saturation and Value) color space and then randomly shifting the Hue and Saturation components, while keeping the Value component the same. Our experimental results on CIFAR10 dataset show that the accuracy of VGG16 network on adversarial color-shifted images is 5.7%.

</details>

<details>

<summary>2018-03-19 09:39:06 - Cloud Provider Capacity Augmentation Through Automated Resource Bartering</summary>

- *Syeda ZarAfshan Gohera, Peter Bloodsworth, Raihan Ur Rasool, Richard McClatchey*

- `1803.06845v1` - [abs](http://arxiv.org/abs/1803.06845v1) - [pdf](http://arxiv.org/pdf/1803.06845v1)

> Growing interest in Cloud Computing places a heavy workload on cloud providers which is becoming increasingly difficult for them to manage with their primary datacenter infrastructures. Resource limitations can make providers vulnerable to significant reputational damage and it often forces customers to select services from the larger, more established companies, sometimes at a higher price. Funding limitations, however, commonly prevent emerging and even established providers from making continual investment in hardware speculatively assuming a certain level of growth in demand. As an alternative, they may strive to use the current inter-cloud resource sharing platforms which mainly rely on monetary payments and thus putting pressure on already stretched cash flows. To address such issues, we have designed and implemented a new multi-agent based Cloud Resource Bartering System (CRBS) that fosters the management and bartering of pooled resources without requiring costly financial transactions between providers. Agents in CRBS not only strengthen the trading relationship among providers but also enable them to handle surges in demand with their primary setup. Unlike existing systems, CRBS assigns resources by considering resource urgency which comparatively improves customers satisfaction and the resource utilization rate by more than 50%.The evaluation results provide evidence that our system assists providers to timely acquire the additional resources and to maintain sustainable service delivery. We conclude that the existence of such a system is economically beneficial for cloud providers and enables them to adapt to fluctuating workloads.

</details>

<details>

<summary>2018-03-19 18:02:11 - Security Analysis and Enhancement of Model Compressed Deep Learning Systems under Adversarial Attacks</summary>

- *Qi Liu, Tao Liu, Zihao Liu, Yanzhi Wang, Yier Jin, Wujie Wen*

- `1802.05193v2` - [abs](http://arxiv.org/abs/1802.05193v2) - [pdf](http://arxiv.org/pdf/1802.05193v2)

> DNN is presenting human-level performance for many complex intelligent tasks in real-world applications. However, it also introduces ever-increasing security concerns. For example, the emerging adversarial attacks indicate that even very small and often imperceptible adversarial input perturbations can easily mislead the cognitive function of deep learning systems (DLS). Existing DNN adversarial studies are narrowly performed on the ideal software-level DNN models with a focus on single uncertainty factor, i.e. input perturbations, however, the impact of DNN model reshaping on adversarial attacks, which is introduced by various hardware-favorable techniques such as hash-based weight compression during modern DNN hardware implementation, has never been discussed. In this work, we for the first time investigate the multi-factor adversarial attack problem in practical model optimized deep learning systems by jointly considering the DNN model-reshaping (e.g. HashNet based deep compression) and the input perturbations. We first augment adversarial example generating method dedicated to the compressed DNN models by incorporating the software-based approaches and mathematical modeled DNN reshaping. We then conduct a comprehensive robustness and vulnerability analysis of deep compressed DNN models under derived adversarial attacks. A defense technique named "gradient inhibition" is further developed to ease the generating of adversarial examples thus to effectively mitigate adversarial attacks towards both software and hardware-oriented DNNs. Simulation results show that "gradient inhibition" can decrease the average success rate of adversarial attacks from 87.99% to 4.77% (from 86.74% to 4.64%) on MNIST (CIFAR-10) benchmark with marginal accuracy degradation across various DNNs.

</details>

<details>

<summary>2018-03-20 20:51:07 - Identifying Relevant Information Cues for Vulnerability Assessment Using CVSS</summary>

- *Luca Allodi, Sebastian Banescu, Henning Femmer, Kristian Beckers*

- `1803.07648v1` - [abs](http://arxiv.org/abs/1803.07648v1) - [pdf](http://arxiv.org/pdf/1803.07648v1)

> The assessment of new vulnerabilities is an activity that accounts for information from several data sources and produces a `severity' score for the vulnerability. The Common Vulnerability Scoring System (\CVSS) is the reference standard for this assessment. Yet, no guidance currently exists on \emph{which information} aids a correct assessment and should therefore be considered.   In this paper we address this problem by evaluating which information cues increase (or decrease) assessment accuracy.   We devise a block design experiment with 67 software engineering students with varying vulnerability information and measure scoring accuracy under different information sets.   We find that baseline vulnerability descriptions provided by standard vulnerability sources provide only part of the information needed to achieve an accurate vulnerability assessment. Further, we find that additional information on \texttt{assets}, \texttt{attacks}, and \texttt{vulnerability type} contributes in increasing the accuracy of the assessment; conversely, information on \texttt{known threats} misleads the assessor and decreases assessment accuracy and should be avoided when assessing vulnerabilities. These results go in the direction of formalizing the vulnerability communication to, for example, fully automate security assessments.

</details>

<details>

<summary>2018-03-22 03:52:57 - A Topological Approach to Secure Message Dissemination in Vehicular Networks</summary>

- *Jieqiong Chen, Guoqiang Mao, Changle Li, Degan Zhang*

- `1803.08221v1` - [abs](http://arxiv.org/abs/1803.08221v1) - [pdf](http://arxiv.org/pdf/1803.08221v1)

> Secure message dissemination is an important issue in vehicular networks, especially considering the vulnerability of vehicle to vehicle message dissemination to malicious attacks. Traditional security mechanisms, largely based on message encryption and key management, can only guarantee secure message exchanges between known source and destination pairs. In vehicular networks however, every vehicle may learn its surrounding environment and contributes as a source, while in the meantime act as a destination or a relay of information from other vehicles, message exchanges often occur between "stranger" vehicles. For secure message dissemination in vehicular networks against insider attackers, who may tamper the content of the disseminated messages, ensuring the consistency and integrity of the transmitted messages becomes a major concern that traditional message encryption and key management based approaches fall short to provide. In this paper, by incorporating the underlying network topology information, we propose an optimal decision algorithm that is able to maximize the chance of making a correct decision on the message content, assuming the prior knowledge of the percentage of malicious vehicles in the network. Furthermore, a novel heuristic decision algorithm is proposed that can make decisions without the aforementioned knowledge of the percentage of malicious vehicles. Simulations are conducted to compare the security performance achieved by our proposed decision algorithms with that achieved by existing ones that do not consider or only partially consider the topological information, to verify the effectiveness of the algorithms. Our results show that by incorporating the network topology information, the security performance can be much improved. This work shed light on the optimum algorithm design for secure message dissemination.

</details>

<details>

<summary>2018-03-22 10:17:50 - Sneak into Devil's Colony- A study of Fake Profiles in Online Social Networks and the Cyber Law</summary>

- *Mudasir Ahmad Wani, Suraiya Jabin, Ghulam Yazdani, Nehaluddin Ahmadd*

- `1803.08810v1` - [abs](http://arxiv.org/abs/1803.08810v1) - [pdf](http://arxiv.org/pdf/1803.08810v1)

> Massive content about user's social, personal and professional life stored on Online Social Networks (OSNs) has attracted not only the attention of researchers and social analysts but also the cyber criminals. These cyber criminals penetrate illegally into an OSN by establishing fake profiles or by designing bots and exploit the vulnerabilities of an OSN to carry out illegal activities. With the growth of technology cyber crimes have been increasing manifold. Daily reports of the security and privacy threats in the OSNs demand not only the intelligent automated detection systems that can identify and alleviate fake profiles in real time but also the reinforcement of the security and privacy laws to curtail the cyber crime. In this paper, we have studied various categories of fake profiles like compromised profiles, cloned profiles and online bots (spam-bots, social-bots, like-bots and influential-bots) on different OSN sites along with existing cyber laws to mitigate their threats. In order to design fake profile detection systems, we have highlighted different category of fake profile features which are capable to distinguish different kinds of fake entities from real ones. Another major challenges faced by researchers while building the fake profile detection systems is the unavailability of data specific to fake users. The paper addresses this challenge by providing extremely obliging data collection techniques along with some existing data sources. Furthermore, an attempt is made to present several machine learning techniques employed to design different fake profile detection systems.

</details>

<details>

<summary>2018-03-22 12:46:44 - Boosting Adversarial Attacks with Momentum</summary>

- *Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, Jianguo Li*

- `1710.06081v3` - [abs](http://arxiv.org/abs/1710.06081v3) - [pdf](http://arxiv.org/pdf/1710.06081v3)

> Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.

</details>

<details>

<summary>2018-03-24 19:41:25 - An Overview of Vulnerabilities of Voice Controlled Systems</summary>

- *Yuan Gong, Christian Poellabauer*

- `1803.09156v1` - [abs](http://arxiv.org/abs/1803.09156v1) - [pdf](http://arxiv.org/pdf/1803.09156v1)

> Over the last few years, a rapidly increasing number of Internet-of-Things (IoT) systems that adopt voice as the primary user input have emerged. These systems have been shown to be vulnerable to various types of voice spoofing attacks. However, how exactly these techniques differ or relate to each other has not been extensively studied. In this paper, we provide a survey of recent attack and defense techniques for voice controlled systems and propose a classification of these techniques. We also discuss the need for a universal defense strategy that protects a system from various types of attacks.

</details>

<details>

<summary>2018-03-24 20:30:50 - Handling Adversarial Concept Drift in Streaming Data</summary>

- *Tegjyot Singh Sethi, Mehmed Kantardzic*

- `1803.09160v1` - [abs](http://arxiv.org/abs/1803.09160v1) - [pdf](http://arxiv.org/pdf/1803.09160v1)

> Classifiers operating in a dynamic, real world environment, are vulnerable to adversarial activity, which causes the data distribution to change over time. These changes are traditionally referred to as concept drift, and several approaches have been developed in literature to deal with the problem of drift handling and detection. However, most concept drift handling techniques, approach it as a domain independent task, to make them applicable to a wide gamut of reactive systems. These techniques were developed from an adversarial agnostic perspective, where they are naive and assume that drift is a benign change, which can be fixed by updating the model. However, this is not the case when an active adversary is trying to evade the deployed classification system. In such an environment, the properties of concept drift are unique, as the drift is intended to degrade the system and at the same time designed to avoid detection by traditional concept drift detection techniques. This special category of drift is termed as adversarial drift, and this paper analyzes its characteristics and impact, in a streaming environment. A novel framework for dealing with adversarial concept drift is proposed, called the Predict-Detect streaming framework. Experimental evaluation of the framework, on generated adversarial drifting data streams, demonstrates that this framework is able to provide reliable unsupervised indication of drift, and is able to recover from drifts swiftly. While traditional partially labeled concept drift detection methodologies fail to detect adversarial drifts, the proposed framework is able to detect such drifts and operates with <6% labeled data, on average. Also, the framework provides benefits for active learning over imbalanced data streams, by innately providing for feature space honeypots, where minority class adversarial samples may be captured.

</details>

<details>

<summary>2018-03-24 20:55:20 - A Dynamic-Adversarial Mining Approach to the Security of Machine Learning</summary>

- *Tegjyot Singh Sethi, Mehmed Kantardzic, Lingyu Lyua, Jiashun Chen*

- `1803.09162v1` - [abs](http://arxiv.org/abs/1803.09162v1) - [pdf](http://arxiv.org/pdf/1803.09162v1)

> Operating in a dynamic real world environment requires a forward thinking and adversarial aware design for classifiers, beyond fitting the model to the training data. In such scenarios, it is necessary to make classifiers - a) harder to evade, b) easier to detect changes in the data distribution over time, and c) be able to retrain and recover from model degradation. While most works in the security of machine learning has concentrated on the evasion resistance (a) problem, there is little work in the areas of reacting to attacks (b and c). Additionally, while streaming data research concentrates on the ability to react to changes to the data distribution, they often take an adversarial agnostic view of the security problem. This makes them vulnerable to adversarial activity, which is aimed towards evading the concept drift detection mechanism itself. In this paper, we analyze the security of machine learning, from a dynamic and adversarial aware perspective. The existing techniques of Restrictive one class classifier models, Complex learning models and Randomization based ensembles, are shown to be myopic as they approach security as a static task. These methodologies are ill suited for a dynamic environment, as they leak excessive information to an adversary, who can subsequently launch attacks which are indistinguishable from the benign data. Based on empirical vulnerability analysis against a sophisticated adversary, a novel feature importance hiding approach for classifier design, is proposed. The proposed design ensures that future attacks on classifiers can be detected and recovered from. The proposed work presents motivation, by serving as a blueprint, for future work in the area of Dynamic-Adversarial mining, which combines lessons learned from Streaming data mining, Adversarial learning and Cybersecurity.

</details>

<details>

<summary>2018-03-24 21:10:00 - Security Theater: On the Vulnerability of Classifiers to Exploratory Attacks</summary>

- *Tegjyot Singh Sethi, Mehmed Kantardzic, Joung Woo Ryu*

- `1803.09163v1` - [abs](http://arxiv.org/abs/1803.09163v1) - [pdf](http://arxiv.org/pdf/1803.09163v1)

> The increasing scale and sophistication of cyberattacks has led to the adoption of machine learning based classification techniques, at the core of cybersecurity systems. These techniques promise scale and accuracy, which traditional rule or signature based methods cannot. However, classifiers operating in adversarial domains are vulnerable to evasion attacks by an adversary, who is capable of learning the behavior of the system by employing intelligently crafted probes. Classification accuracy in such domains provides a false sense of security, as detection can easily be evaded by carefully perturbing the input samples. In this paper, a generic data driven framework is presented, to analyze the vulnerability of classification systems to black box probing based attacks. The framework uses an exploration exploitation based strategy, to understand an adversary's point of view of the attack defense cycle. The adversary assumes a black box model of the defender's classifier and can launch indiscriminate attacks on it, without information of the defender's model type, training data or the domain of application. Experimental evaluation on 10 real world datasets demonstrates that even models having high perceived accuracy (>90%), by a defender, can be effectively circumvented with a high evasion rate (>95%, on average). The detailed attack algorithms, adversarial model and empirical evaluation, serve.

</details>

<details>

<summary>2018-03-25 14:11:07 - DEFenD: A Secure and Privacy-Preserving Decentralized System for Freight Declaration</summary>

- *Daniël Vos, Leon Overweel, Wouter Raateland, Jelle Vos, Matthijs Bijman, Max Pigmans, Zekeriya Erkin*

- `1803.09257v1` - [abs](http://arxiv.org/abs/1803.09257v1) - [pdf](http://arxiv.org/pdf/1803.09257v1)

> Millions of shipping containers filled with goods move around the world every day. Before such a container may enter a trade bloc, the customs agency of the goods' destination country must ensure that it does not contain illegal or mislabeled goods. Due to the high volume of containers, customs agencies make a selection of containers to audit through a risk analysis procedure. Customs agencies perform risk analysis using data sourced from a centralized system that is potentially vulnerable to manipulation and malpractice. Therefore we propose an alternative: DEFenD, a decentralized system that stores data about goods and containers in a secure and privacy-preserving manner. In our system, economic operators make claims to the network about goods they insert into or remove from containers, and encrypt these claims so that they can only be read by the destination country's customs agency. Economic operators also make unencrypted claims about containers with which they interact. Unencrypted claims can be validated by the entire network of customs agencies. Our key contribution is a data partitioning scheme and several protocols that enable such a system to utilize blockchain and its powerful validation principle, while also preserving the privacy of the involved economic operators. Using our protocol, customs agencies can improve their risk analysis and economic operators can get through customs with less delay. We also present a reference implementation built with Hyperledger Fabric and analyze to what extent our implementation meets the requirements in terms of privacy-preservation, security, scalability, and decentralization.

</details>

<details>

<summary>2018-03-25 14:17:03 - Adversarial Deep Learning for Robust Detection of Binary Encoded Malware</summary>

- *Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, Una-May O'Reilly*

- `1801.02950v3` - [abs](http://arxiv.org/abs/1801.02950v3) - [pdf](http://arxiv.org/pdf/1801.02950v3)

> Malware is constantly adapting in order to avoid detection. Model based malware detectors, such as SVM and neural networks, are vulnerable to so-called adversarial examples which are modest changes to detectable malware that allows the resulting malware to evade detection. Continuous-valued methods that are robust to adversarial examples of images have been developed using saddle-point optimization formulations. We are inspired by them to develop similar methods for the discrete, e.g. binary, domain which characterizes the features of malware. A specific extra challenge of malware is that the adversarial examples must be generated in a way that preserves their malicious functionality. We introduce methods capable of generating functionally preserved adversarial malware examples in the binary domain. Using the saddle-point formulation, we incorporate the adversarial examples into the training of models that are robust to them. We evaluate the effectiveness of the methods and others in the literature on a set of Portable Execution~(PE) files. Comparison prompts our introduction of an online measure computed during training to assess general expectation of robustness.

</details>

<details>

<summary>2018-03-27 20:58:24 - A Formal TLS Handshake Model in LNT</summary>

- *Josip Bozic, Lina Marsso, Radu Mateescu, Franz Wotawa*

- `1803.10319v1` - [abs](http://arxiv.org/abs/1803.10319v1) - [pdf](http://arxiv.org/pdf/1803.10319v1)

> Testing of network services represents one of the biggest challenges in cyber security. Because new vulnerabilities are detected on a regular basis, more research is needed. These faults have their roots in the software development cycle or because of intrinsic leaks in the system specification. Conformance testing checks whether a system behaves according to its specification. Here model-based testing provides several methods for automated detection of shortcomings. The formal specification of a system behavior represents the starting point of the testing process. In this paper, a widely used cryptographic protocol is specified and tested for conformance with a test execution framework. The first empirical results are presented and discussed.

</details>

<details>

<summary>2018-03-28 07:44:38 - Clipping free attacks against artificial neural networks</summary>

- *Boussad Addad, Jerome Kodjabachian, Christophe Meyer*

- `1803.09468v2` - [abs](http://arxiv.org/abs/1803.09468v2) - [pdf](http://arxiv.org/pdf/1803.09468v2)

> During the last years, a remarkable breakthrough has been made in AI domain thanks to artificial deep neural networks that achieved a great success in many machine learning tasks in computer vision, natural language processing, speech recognition, malware detection and so on. However, they are highly vulnerable to easily crafted adversarial examples. Many investigations have pointed out this fact and different approaches have been proposed to generate attacks while adding a limited perturbation to the original data. The most robust known method so far is the so called C&W attack [1]. Nonetheless, a countermeasure known as feature squeezing coupled with ensemble defense showed that most of these attacks can be destroyed [6]. In this paper, we present a new method we call Centered Initial Attack (CIA) whose advantage is twofold : first, it insures by construction the maximum perturbation to be smaller than a threshold fixed beforehand, without the clipping process that degrades the quality of attacks. Second, it is robust against recently introduced defenses such as feature squeezing, JPEG encoding and even against a voting ensemble of defenses. While its application is not limited to images, we illustrate this using five of the current best classifiers on ImageNet dataset among which two are adversarialy retrained on purpose to be robust against attacks. With a fixed maximum perturbation of only 1.5% on any pixel, around 80% of attacks (targeted) fool the voting ensemble defense and nearly 100% when the perturbation is only 6%. While this shows how it is difficult to defend against CIA attacks, the last section of the paper gives some guidelines to limit their impact.

</details>

<details>

<summary>2018-03-29 10:02:09 - Protection against Cloning for Deep Learning</summary>

- *Richard Kenway*

- `1803.10995v1` - [abs](http://arxiv.org/abs/1803.10995v1) - [pdf](http://arxiv.org/pdf/1803.10995v1)

> The susceptibility of deep learning to adversarial attack can be understood in the framework of the Renormalisation Group (RG) and the vulnerability of a specific network may be diagnosed provided the weights in each layer are known. An adversary with access to the inputs and outputs could train a second network to clone these weights and, having identified a weakness, use them to compute the perturbation of the input data which exploits it. However, the RG framework also provides a means to poison the outputs of the network imperceptibly, without affecting their legitimate use, so as to prevent such cloning of its weights and thereby foil the generation of adversarial data.

</details>

<details>

<summary>2018-03-31 03:36:08 - A Survey of Techniques for Improving Security of GPUs</summary>

- *Sparsh Mittal, S. B. Abhinaya, Manish Reddy, Irfan Ali*

- `1804.00114v1` - [abs](http://arxiv.org/abs/1804.00114v1) - [pdf](http://arxiv.org/pdf/1804.00114v1)

> Graphics processing unit (GPU), although a powerful performance-booster, also has many security vulnerabilities. Due to these, the GPU can act as a safe-haven for stealthy malware and the weakest `link' in the security `chain'. In this paper, we present a survey of techniques for analyzing and improving GPU security. We classify the works on key attributes to highlight their similarities and differences. More than informing users and researchers about GPU security techniques, this survey aims to increase their awareness about GPU security vulnerabilities and potential countermeasures.

</details>


## 2018-04

<details>

<summary>2018-04-01 06:28:22 - Software-Defined Network (SDN) Data Plane Security: Issues, Solutions and Future Directions</summary>

- *Arash Shaghaghi, Mohamed Ali Kaafar, Rajkumar Buyya, Sanjay Jha*

- `1804.00262v1` - [abs](http://arxiv.org/abs/1804.00262v1) - [pdf](http://arxiv.org/pdf/1804.00262v1)

> Software-Defined Network (SDN) radically changes the network architecture by decoupling the network logic from the underlying forwarding devices. This architectural change rejuvenates the network-layer granting centralized management and re-programmability of the networks. From a security perspective, SDN separates security concerns into control and data plane, and this architectural recomposition brings up exciting opportunities and challenges. The overall perception is that SDN capabilities will ultimately result in improved security. However, in its raw form, SDN could potentially make networks more vulnerable to attacks and harder to protect. In this paper, we focus on identifying challenges faced in securing the data plane of SDN - one of the least explored but most critical components of this technology. We formalize this problem space, identify potential attack scenarios while highlighting possible vulnerabilities and establish a set of requirements and challenges to protect the data plane of SDNs. Moreover, we undertake a survey of existing solutions with respect to the identified threats, identifying their limitations and offer future research directions.

</details>

<details>

<summary>2018-04-02 22:52:21 - Using Unit Testing to Detect Sanitization Flaws</summary>

- *Mahmoud Mohammadi, Bill Chu, Heather Richter Lipford*

- `1804.00753v1` - [abs](http://arxiv.org/abs/1804.00753v1) - [pdf](http://arxiv.org/pdf/1804.00753v1)

> Input sanitization mechanisms are widely used to mitigate vulnerabilities to injection attacks such as cross-site scripting. Static analysis tools and techniques commonly used to ensure that applications utilize sanitization functions. Dynamic analysis must be to evaluate the correctness of sanitization functions. The proposed approach is based on unit testing to bring the advantages of both static and dynamic techniques to the development time. Our approach introduces a technique to automatically extract the sanitization functions and then evaluate their effectiveness against attacks using automatically generated attack vectors. The empirical results show that the proposed technique can detect security flaws cannot find by the static analysis tools.

</details>

<details>

<summary>2018-04-02 22:56:33 - Automatic Web Security Unit Testing: XSS Vulnerability Detection</summary>

- *Mahmoud Mohammadi, Bill Chu, Heather Richter Lipford, Emerson Murphy-Hill*

- `1804.00754v1` - [abs](http://arxiv.org/abs/1804.00754v1) - [pdf](http://arxiv.org/pdf/1804.00754v1)

> Integrating security testing into the workflow of software developers not only can save resources for separate security testing but also reduce the cost of fixing security vulnerabilities by detecting them early in the development cycle. We present an automatic testing approach to detect a common type of Cross Site Scripting (XSS) vulnerability caused by improper encoding of untrusted data. We automatically extract encoding functions used in a web application to sanitize untrusted inputs and then evaluate their effectiveness by automatically generating XSS attack strings. Our evaluations show that this technique can detect 0-day XSS vulnerabilities that cannot be found by static analysis tools. We will also show that our approach can efficiently cover a common type of XSS vulnerability. This approach can be generalized to test for input validation against other types injections such as command line injection.

</details>

<details>

<summary>2018-04-02 22:59:18 - Detecting Cross-Site Scripting Vulnerabilities through Automated Unit Testing</summary>

- *Mahmoud Mohammadi, Bill Chu, Heather Richter Lipford*

- `1804.00755v1` - [abs](http://arxiv.org/abs/1804.00755v1) - [pdf](http://arxiv.org/pdf/1804.00755v1)

> The best practice to prevent Cross Site Scripting (XSS) attacks is to apply encoders to sanitize untrusted data. To balance security and functionality, encoders should be applied to match the web page context, such as HTML body, JavaScript, and style sheets. A common programming error is the use of a wrong encoder to sanitize untrusted data, leaving the application vulnerable. We present a security unit testing approach to detect XSS vulnerabilities caused by improper encoding of untrusted data. Unit tests for the XSS vulnerability are automatically constructed out of each web page and then evaluated by a unit test execution framework. A grammar-based attack generator is used to automatically generate test inputs. We evaluate our approach on a large open source medical records application, demonstrating that we can detect many 0-day XSS vulnerabilities with very low false positives, and that the grammar-based attack generator has better test coverage than industry best practices.

</details>

<details>

<summary>2018-04-03 01:16:58 - Automated Detecting and Repair of Cross-Site Scripting Vulnerabilities</summary>

- *Mahmoud Mohammadi, Bei-Tseng Chu, Heather Richter Lipford*

- `1804.01862v1` - [abs](http://arxiv.org/abs/1804.01862v1) - [pdf](http://arxiv.org/pdf/1804.01862v1)

> The best practice to prevent Cross Site Scripting (XSS) attacks is to apply encoders to sanitize untrusted data. To balance security and functionality, encoders should be applied to match the web page context, such as HTML body, JavaScript, and style sheets. A common programming error is the use of a wrong type of encoder to sanitize untrusted data, leaving the application vulnerable. We present a security unit testing approach to detect XSS vulnerabilities caused by improper encoding of untrusted data. Unit tests for the XSS vulnerability are constructed out of each web page and then evaluated by a unit test execution framework. A grammar-based attack generator is devised to automatically generate test inputs. We also propose a vulnerability repair technique that can automatically fix detected vulnerabilities in many situations. Evaluation of this approach has been conducted on an open source medical record application with over 200 web pages written in JSP.

</details>

<details>

<summary>2018-04-04 02:09:59 - STADS: Software Testing as Species Discovery</summary>

- *Marcel Böhme*

- `1803.02130v2` - [abs](http://arxiv.org/abs/1803.02130v2) - [pdf](http://arxiv.org/pdf/1803.02130v2)

> A fundamental challenge of software testing is the statistically well-grounded extrapolation from program behaviors observed during testing. For instance, a security researcher who has run the fuzzer for a week has currently no means (i) to estimate the total number of feasible program branches, given that only a fraction has been covered so far, (ii) to estimate the additional time required to cover 10% more branches, or (iii) to assess the residual risk that a vulnerability exists when no vulnerability has been discovered. Failing to discover a vulnerability, does not mean that none exists---even if the fuzzer was run for a week (or a year). Hence, testing provides no formal correctness guarantees.   In this article, I establish an unexpected connection with the otherwise unrelated scientific field of ecology, and introduce a statistical framework that models Software Testing and Analysis as Discovery of Species (STADS). For instance, in order to study the species diversity of arthropods in a tropical rain forest, ecologists would first sample a large number of individuals from that forest, determine their species, and extrapolate from the properties observed in the sample to properties of the whole forest. The estimation (i) of the total number of species, (ii) of the additional sampling effort required to discover 10% more species, or (iii) of the probability to discover a new species are classical problems in ecology. The STADS framework draws from over three decades of research in ecological biostatistics to address the fundamental extrapolation challenge for automated test generation. Our preliminary empirical study demonstrates a good estimator performance even for a fuzzer with adaptive sampling bias---AFL, a state-of-the-art vulnerability detection tool. The STADS framework provides statistical correctness guarantees with quantifiable accuracy.

</details>

<details>

<summary>2018-04-04 12:00:22 - Analysing and Patching SPEKE in ISO/IEC</summary>

- *Feng Hao, Roberto Metere, Siamak F. Shahandashti, Changyu Dong*

- `1802.04900v2` - [abs](http://arxiv.org/abs/1802.04900v2) - [pdf](http://arxiv.org/pdf/1802.04900v2)

> Simple Password Exponential Key Exchange (SPEKE) is a well-known Password Authenticated Key Exchange (PAKE) protocol that has been used in Blackberry phones for secure messaging and Entrust's TruePass end-to-end web products. It has also been included into international standards such as ISO/IEC 11770-4 and IEEE P1363.2. In this paper, we analyse the SPEKE protocol as specified in the ISO/IEC and IEEE standards. We identify that the protocol is vulnerable to two new attacks: an impersonation attack that allows an attacker to impersonate a user without knowing the password by launching two parallel sessions with the victim, and a key-malleability attack that allows a man-in-the-middle (MITM) to manipulate the session key without being detected by the end users. Both attacks have been acknowledged by the technical committee of ISO/IEC SC 27, and ISO/IEC 11770-4 revised as a result. We propose a patched SPEKE called P-SPEKE and present a formal analysis in the Applied Pi Calculus using ProVerif to show that the proposed patch prevents both attacks. The proposed patch has been included into the latest revision of ISO/IEC 11770-4 published in 2017.

</details>

<details>

<summary>2018-04-05 03:10:37 - The structure of evolved representations across different substrates for artificial intelligence</summary>

- *Arend Hintze, Douglas Kirkpatrick, Christoph Adami*

- `1804.01660v1` - [abs](http://arxiv.org/abs/1804.01660v1) - [pdf](http://arxiv.org/pdf/1804.01660v1)

> Artificial neural networks (ANNs), while exceptionally useful for classification, are vulnerable to misdirection. Small amounts of noise can significantly affect their ability to correctly complete a task. Instead of generalizing concepts, ANNs seem to focus on surface statistical regularities in a given task. Here we compare how recurrent artificial neural networks, long short-term memory units, and Markov Brains sense and remember their environments. We show that information in Markov Brains is localized and sparsely distributed, while the other neural network substrates "smear" information about the environment across all nodes, which makes them vulnerable to noise.

</details>

<details>

<summary>2018-04-05 18:26:12 - Inferring transportation modes from GPS trajectories using a convolutional neural network</summary>

- *Sina Dabiri, Kevin Heaslip*

- `1804.02386v1` - [abs](http://arxiv.org/abs/1804.02386v1) - [pdf](http://arxiv.org/pdf/1804.02386v1)

> Identifying the distribution of users' transportation modes is an essential part of travel demand analysis and transportation planning. With the advent of ubiquitous GPS-enabled devices (e.g., a smartphone), a cost-effective approach for inferring commuters' mobility mode(s) is to leverage their GPS trajectories. A majority of studies have proposed mode inference models based on hand-crafted features and traditional machine learning algorithms. However, manual features engender some major drawbacks including vulnerability to traffic and environmental conditions as well as possessing human's bias in creating efficient features. One way to overcome these issues is by utilizing Convolutional Neural Network (CNN) schemes that are capable of automatically driving high-level features from the raw input. Accordingly, in this paper, we take advantage of CNN architectures so as to predict travel modes based on only raw GPS trajectories, where the modes are labeled as walk, bike, bus, driving, and train. Our key contribution is designing the layout of the CNN's input layer in such a way that not only is adaptable with the CNN schemes but represents fundamental motion characteristics of a moving object including speed, acceleration, jerk, and bearing rate. Furthermore, we ameliorate the quality of GPS logs through several data preprocessing steps. Using the clean input layer, a variety of CNN configurations are evaluated to achieve the best CNN architecture. The highest accuracy of 84.8% has been achieved through the ensemble of the best CNN configuration. In this research, we contrast our methodology with traditional machine learning algorithms as well as the seminal and most related studies to demonstrate the superiority of our framework.

</details>

<details>

<summary>2018-04-06 01:56:16 - Fooling Vision and Language Models Despite Localization and Attention Mechanism</summary>

- *Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell, Dawn Song*

- `1709.08693v2` - [abs](http://arxiv.org/abs/1709.08693v2) - [pdf](http://arxiv.org/pdf/1709.08693v2)

> Adversarial attacks are known to succeed on classifiers, but it has been an open question whether more complex vision systems are vulnerable. In this paper, we study adversarial examples for vision and language models, which incorporate natural language understanding and complex structures such as attention, localization, and modular architectures. In particular, we investigate attacks on a dense captioning model and on two visual question answering (VQA) models. Our evaluation shows that we can generate adversarial examples with a high success rate (i.e., > 90%) for these models. Our work sheds new light on understanding adversarial attacks on vision systems which have a language component and shows that attention, bounding box localization, and compositional internal structures are vulnerable to adversarial attacks. These observations will inform future work towards building effective defenses.

</details>

<details>

<summary>2018-04-06 20:35:32 - e-SAFE: Secure, Efficient and Forensics-Enabled Access to Implantable Medical Devices</summary>

- *Haotian Chi, Longfei Wu, Xiaojiang Du, Qiang Zeng, Paul Ratazzi*

- `1804.02447v1` - [abs](http://arxiv.org/abs/1804.02447v1) - [pdf](http://arxiv.org/pdf/1804.02447v1)

> To facilitate monitoring and management, modern Implantable Medical Devices (IMDs) are often equipped with wireless capabilities, which raise the risk of malicious access to IMDs. Although schemes are proposed to secure the IMD access, some issues are still open. First, pre-sharing a long-term key between a patient's IMD and a doctor's programmer is vulnerable since once the doctor's programmer is compromised, all of her patients suffer; establishing a temporary key by leveraging proximity gets rid of pre-shared keys, but as the approach lacks real authentication, it can be exploited by nearby adversaries or through man-in-the-middle attacks. Second, while prolonging the lifetime of IMDs is one of the most important design goals, few schemes explore to lower the communication and computation overhead all at once. Finally, how to safely record the commands issued by doctors for the purpose of forensics, which can be the last measure to protect the patients' rights, is commonly omitted in the existing literature. Motivated by these important yet open problems, we propose an innovative scheme e-SAFE, which significantly improves security and safety, reduces the communication overhead and enables IMD-access forensics. We present a novel lightweight compressive sensing based encryption algorithm to encrypt and compress the IMD data simultaneously, reducing the data transmission overhead by over 50% while ensuring high data confidentiality and usability. Furthermore, we provide a suite of protocols regarding device pairing, dual-factor authentication, and accountability-enabled access. The security analysis and performance evaluation show the validity and efficiency of the proposed scheme.

</details>

<details>

<summary>2018-04-09 19:09:08 - CIoTA: Collaborative IoT Anomaly Detection via Blockchain</summary>

- *Tomer Golomb, Yisroel Mirsky, Yuval Elovici*

- `1803.03807v2` - [abs](http://arxiv.org/abs/1803.03807v2) - [pdf](http://arxiv.org/pdf/1803.03807v2)

> Due to their rapid growth and deployment, Internet of things (IoT) devices have become a central aspect of our daily lives. However, they tend to have many vulnerabilities which can be exploited by an attacker. Unsupervised techniques, such as anomaly detection, can help us secure the IoT devices. However, an anomaly detection model must be trained for a long time in order to capture all benign behaviors. This approach is vulnerable to adversarial attacks since all observations are assumed to be benign while training the anomaly detection model.   In this paper, we propose CIoTA, a lightweight framework that utilizes the blockchain concept to perform distributed and collaborative anomaly detection for devices with limited resources. CIoTA uses blockchain to incrementally update a trusted anomaly detection model via self-attestation and consensus among IoT devices. We evaluate CIoTA on our own distributed IoT simulation platform, which consists of 48 Raspberry Pis, to demonstrate CIoTA's ability to enhance the security of each device and the security of the network as a whole.

</details>

<details>

<summary>2018-04-09 19:23:01 - An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural Networks</summary>

- *Pu Zhao, Sijia Liu, Yanzhi Wang, Xue Lin*

- `1804.03193v1` - [abs](http://arxiv.org/abs/1804.03193v1) - [pdf](http://arxiv.org/pdf/1804.03193v1)

> Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That is, adversarial examples, obtained by adding delicately crafted distortions onto original legal inputs, can mislead a DNN to classify them as any target labels. In a successful adversarial attack, the targeted mis-classification should be achieved with the minimal distortion added. In the literature, the added distortions are usually measured by L0, L1, L2, and L infinity norms, namely, L0, L1, L2, and L infinity attacks, respectively. However, there lacks a versatile framework for all types of adversarial attacks.   This work for the first time unifies the methods of generating adversarial examples by leveraging ADMM (Alternating Direction Method of Multipliers), an operator splitting optimization approach, such that L0, L1, L2, and L infinity attacks can be effectively implemented by this general framework with little modifications. Comparing with the state-of-the-art attacks in each category, our ADMM-based attacks are so far the strongest, achieving both the 100% attack success rate and the minimal distortion.

</details>

<details>

<summary>2018-04-10 04:54:29 - On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses</summary>

- *Anish Athalye, Nicholas Carlini*

- `1804.03286v1` - [abs](http://arxiv.org/abs/1804.03286v1) - [pdf](http://arxiv.org/pdf/1804.03286v1)

> Neural networks are known to be vulnerable to adversarial examples. In this note, we evaluate the two white-box defenses that appeared at CVPR 2018 and find they are ineffective: when applying existing techniques, we can reduce the accuracy of the defended models to 0%.

</details>

<details>

<summary>2018-04-10 07:26:20 - PULP: Inner-process Isolation based on the Program Counter and Data Memory Address</summary>

- *Xiaojing Zhu, Mingyu Chen, Yangyang Zhao, Zonghui Hong, Yunge Guo*

- `1804.03379v1` - [abs](http://arxiv.org/abs/1804.03379v1) - [pdf](http://arxiv.org/pdf/1804.03379v1)

> Plenty of in-process vulnerabilities are blamed on various out of bound memory accesses. Previous prevention methods are mainly based on software checking associated with performance overhead, while traditional hardware protection mechanisms only work for inter-process memory accesses. In this paper we propose a novel hardware based in-process isolation system called PULP (Protection by User Level Partition). PULP modifies processor core by associating program counter and virtual memory address to achieve in-process data isolation. PULP partitions the program into two distinct parts, one is reliable, called primary functions, and the other is unreliable, called secondary functions, the accessible memory range of which can be configured via APIs. PULP automatically checks the memory bound when executing load/store operations in secondary functions. A RISC-V based FPGA prototype is implementated and functional test shows that PULP can effectively prevent in-process bug, including the Heartbleed and other buffer overflow vulnerabilities, etc. The total runtime overhead of PULP is negligible, as there is no extra runtime overhead besides configuring the API. We run SPEC2006 to evaluate the average performance, considering the LIBC functions as secondary functions. Experimental timing results show that, running bzip2, mcf, and libquantum, PULP bears low runtime overhead (less than 0.1%). Analysis also shows that PULP can be used effectively to prevent the newest "Spectre" bug which threats nearly all out-of-order processors.

</details>

<details>

<summary>2018-04-10 16:22:47 - Robust Physical-World Attacks on Deep Learning Models</summary>

- *Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, Dawn Song*

- `1707.08945v5` - [abs](http://arxiv.org/abs/1707.08945v5) - [pdf](http://arxiv.org/pdf/1707.08945v5)

> Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8%of the captured video frames obtained on a moving vehicle(field test) for the target classifier.

</details>

<details>

<summary>2018-04-11 10:15:10 - How vulnerable are the Indian banks: A cryptographers' view</summary>

- *Anirban Pathak, Rishi Dutt Sharma, Dhananjoy Dey*

- `1804.03910v1` - [abs](http://arxiv.org/abs/1804.03910v1) - [pdf](http://arxiv.org/pdf/1804.03910v1)

> With the advent of e-commerce and online banking it has become extremely important that the websites of the financial institutes (especially, banks) implement up-to-date measures of cyber security (in accordance with the recommendations of the regulatory authority) and thus circumvent the possibilities of financial frauds that may occur due to vulnerabilities of the website. Here, we systematically investigate whether Indian banks are following the above requirement. To perform the investigation, recommendations of Reserve Bank of India (RBI), National Institute of Standards and Technology (NIST), European Union Agency for Network and Information Security (ENISA) and Internet Engineering Task Force (IETF) are considered as the benchmarks. Further, the validity and quality of the security certificates of various Indian banks have been tested with the help of a set of tools (e.g., SSL Certificate Checker provided by Digicert and SSL server test provided by SSL Labs). The analysis performed by using these tools and a comparison with the benchmarks, have revealed that the security measures taken by a set of Indian banks are not up-to-date and are vulnerable under some known attacks.

</details>

<details>

<summary>2018-04-12 21:13:20 - MPSM: Multi-prospective PaaS Security Model</summary>

- *Robail Yasrab*

- `1804.04731v1` - [abs](http://arxiv.org/abs/1804.04731v1) - [pdf](http://arxiv.org/pdf/1804.04731v1)

> Cloud computing has brought a revolution in the field of information technology and improving the efficiency of computational resources. It offers computing as a service enabling huge cost and resource efficiency. Despite its advantages, certain security issues still hinder organizations and enterprises from it being adopted. This study mainly focused on the security of Platform-as-a-Service (PaaS) as well as the most critical security issues that were documented regarding PaaS infrastructure. The prime outcome of this study was a security model proposed to mitigate security vulnerabilities of PaaS. This security model consists of a number of tools, techniques and guidelines to mitigate and neutralize security issues of PaaS. The security vulnerabilities along with mitigation strategies were discussed to offer a deep insight into PaaS security for both vendor and client that may facilitate future design to implement secure PaaS platforms.

</details>

<details>

<summary>2018-04-14 10:01:48 - Detecting Malicious PowerShell Commands using Deep Neural Networks</summary>

- *Danny Hendler, Shay Kels, Amir Rubin*

- `1804.04177v2` - [abs](http://arxiv.org/abs/1804.04177v2) - [pdf](http://arxiv.org/pdf/1804.04177v2)

> Microsoft's PowerShell is a command-line shell and scripting language that is installed by default on Windows machines. While PowerShell can be configured by administrators for restricting access and reducing vulnerabilities, these restrictions can be bypassed. Moreover, PowerShell commands can be easily generated dynamically, executed from memory, encoded and obfuscated, thus making the logging and forensic analysis of code executed by PowerShell challenging.For all these reasons, PowerShell is increasingly used by cybercriminals as part of their attacks' tool chain, mainly for downloading malicious contents and for lateral movement. Indeed, a recent comprehensive technical report by Symantec dedicated to PowerShell's abuse by cybercrimials reported on a sharp increase in the number of malicious PowerShell samples they received and in the number of penetration tools and frameworks that use PowerShell. This highlights the urgent need of developing effective methods for detecting malicious PowerShell commands.In this work, we address this challenge by implementing several novel detectors of malicious PowerShell commands and evaluating their performance. We implemented both "traditional" natural language processing (NLP) based detectors and detectors based on character-level convolutional neural networks (CNNs). Detectors' performance was evaluated using a large real-world dataset.Our evaluation results show that, although our detectors individually yield high performance, an ensemble detector that combines an NLP-based classifier with a CNN-based classifier provides the best performance, since the latter classifier is able to detect malicious commands that succeed in evading the former. Our analysis of these evasive commands reveals that some obfuscation patterns automatically detected by the CNN classifier are intrinsically difficult to detect using the NLP techniques we applied.

</details>

<details>

<summary>2018-04-14 22:57:48 - The Challenges in SDN/ML Based Network Security : A Survey</summary>

- *Tam N. Nguyen*

- `1804.03539v2` - [abs](http://arxiv.org/abs/1804.03539v2) - [pdf](http://arxiv.org/pdf/1804.03539v2)

> Machine Learning is gaining popularity in the network security domain as many more network-enabled devices get connected, as malicious activities become stealthier, and as new technologies like Software Defined Networking (SDN) emerge. Sitting at the application layer and communicating with the control layer, machine learning based SDN security models exercise a huge influence on the routing/switching of the entire SDN. Compromising the models is consequently a very desirable goal. Previous surveys have been done on either adversarial machine learning or the general vulnerabilities of SDNs but not both. Through examination of the latest ML-based SDN security applications and a good look at ML/SDN specific vulnerabilities accompanied by common attack methods on ML, this paper serves as a unique survey, making a case for more secure development processes of ML-based SDN security applications.

</details>

<details>

<summary>2018-04-16 08:45:55 - A framework for mitigating zero-day attacks in IoT</summary>

- *Vishal Sharma, Jiyoon Kim, Soonhyun Kwon, Ilsun You, Kyungroul Lee, Kangbin Yim*

- `1804.05549v1` - [abs](http://arxiv.org/abs/1804.05549v1) - [pdf](http://arxiv.org/pdf/1804.05549v1)

> Internet of Things (IoT) aims at providing connectivity between every computing entity. However, this facilitation is also leading to more cyber threats which may exploit the presence of a vulnerability of a period of time. One such vulnerability is the zero-day threat that may lead to zero-day attacks which are detrimental to an enterprise as well as the network security. In this article, a study is presented on the zero-day threats for IoT networks and a context graph-based framework is presented to provide a strategy for mitigating these attacks. The proposed approach uses a distributed diagnosis system for classifying the context at the central service provider as well as at the local user site. Once a potential zero-day attack is identified, a critical data sharing protocol is used to transmit alert messages and reestablish the trust between the network entities and the IoT devices. The results show that the distributed approach is capable of mitigating the zero-day threats efficiently with 33% and 21% improvements in terms of cost of operation and communication overheads, respectively, in comparison with the centralized diagnosis system.

</details>

<details>

<summary>2018-04-16 19:23:19 - Investigating Cybersecurity Issues In Active Traffic Management Systems</summary>

- *Zulqarnain H. Khattak, Hyungjun Park, Seongah Hong, Richard Atta Boateng, Brian L. Smith*

- `1804.05901v1` - [abs](http://arxiv.org/abs/1804.05901v1) - [pdf](http://arxiv.org/pdf/1804.05901v1)

> Active Traffic Management (ATM) systems have been introduced by transportation agencies to manage recurrent and non-recurrent congestion. ATM systems rely on the interconnectivity of components made possible by wired and/or wireless networks. Unfortunately, this connectivity that supports ATM systems also provides potential system access points that results in vulnerability to cyberattacks. This is becoming more pronounced as ATM systems begin to integrate internet of things (IoT) devices. Hence, there is a need to rigorously evaluate ATM systems for cyberattack vulnerabilities, and explore design concepts that provide stability and graceful degradation in the face of cyberattacks. In this research, a prototype ATM system along with a real-time cyberattack monitoring system were developed for a 1.5-mile section of I-66 in Northern Virginia. The monitoring system detects deviation from expected operation of an ATM system by comparing lane control states generated by the ATM system with lane control states deemed most likely by the monitoring system. In case of any deviation between two sets of states, the monitoring system displays the lane control states generated by the back-up data source. In a simulation experiment, the prototype ATM system and cyberattack monitoring system were subject to emulated cyberattacks. The evaluation results showed that when subject to cyberattack, the mean speed reduced by 15% compared to the case with the ATM system and was similar to the baseline case. This illustrates that the effectiveness of the ATM system was negated by cyberattacks. The monitoring system however, allowed the ATM system to revert to an expected safe state and reduced the negative impact of cyberattacks. These results illustrate the need to revisit ATM system design concepts as a means to protect against cyberattacks in addition to traditional system intrusion prevention approaches.

</details>

<details>

<summary>2018-04-17 19:37:23 - Review of Mobile Apps Permissions and Associated Intrusive Privacy Threats</summary>

- *Akosua Boakyewaa Teye, Ezer Osei Yeboah-Boateng*

- `1804.06450v1` - [abs](http://arxiv.org/abs/1804.06450v1) - [pdf](http://arxiv.org/pdf/1804.06450v1)

> The age of technology has created a huge market for smartphones and Apps usage and a new generation has been created based on knowledge sharing. Now knowledge has been made easily accessible by Apps but; are users even aware of the permissions that these Apps require and the privacy issues involved? The study was conducted on the basis of how users make use of Apps. It was conducted through the assessment of permissions required by various Apps through carefully selected third-party Apps and the devices settings and also a review of existing literature that has been conducted in fields within Apps and privacy. It will be unearthed that a many different but exhaustive lists of permission are sought by each App installed and the device it is installed on can quite give the user the information. Also not all permissions sought were found to be risky but some just created a path or a vulnerable point for other malicious programs to take advantage of.

</details>

<details>

<summary>2018-04-18 15:35:32 - Attentive Interaction Model: Modeling Changes in View in Argumentation</summary>

- *Yohan Jo, Shivani Poddar, Byungsoo Jeon, Qinlan Shen, Carolyn P. Rose, Graham Neubig*

- `1804.00065v2` - [abs](http://arxiv.org/abs/1804.00065v2) - [pdf](http://arxiv.org/pdf/1804.00065v2)

> We present a neural architecture for modeling argumentative dialogue that explicitly models the interplay between an Opinion Holder's (OH's) reasoning and a challenger's argument, with the goal of predicting if the argument successfully changes the OH's view. The model has two components: (1) vulnerable region detection, an attention model that identifies parts of the OH's reasoning that are amenable to change, and (2) interaction encoding, which identifies the relationship between the content of the OH's reasoning and that of the challenger's argument. Based on evaluation on discussions from the Change My View forum on Reddit, the two components work together to predict an OH's change in view, outperforming several baselines. A posthoc analysis suggests that sentences picked out by the attention model are addressed more frequently by successful arguments than by unsuccessful ones.

</details>

<details>

<summary>2018-04-21 00:38:33 - Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation Size</summary>

- *Ian Goodfellow*

- `1804.07870v1` - [abs](http://arxiv.org/abs/1804.07870v1) - [pdf](http://arxiv.org/pdf/1804.07870v1)

> A key problem in research on adversarial examples is that vulnerability to adversarial examples is usually measured by running attack algorithms. Because the attack algorithms are not optimal, the attack algorithms are prone to overestimating the size of perturbation needed to fool the target model. In other words, the attack-based methodology provides an upper-bound on the size of a perturbation that will fool the model, but security guarantees require a lower bound. CLEVER is a proposed scoring method to estimate a lower bound. Unfortunately, an estimate of a bound is not a bound. In this report, we show that gradient masking, a common problem that causes attack methodologies to provide only a very loose upper bound, causes CLEVER to overestimate the size of perturbation needed to fool the model. In other words, CLEVER does not resolve the key problem with the attack-based methodology, because it fails to provide a lower bound.

</details>

<details>

<summary>2018-04-23 08:44:04 - A Semantic Framework for the Security Analysis of Ethereum smart contracts</summary>

- *Ilya Grishchenko, Matteo Maffei, Clara Schneidewind*

- `1802.08660v2` - [abs](http://arxiv.org/abs/1802.08660v2) - [pdf](http://arxiv.org/pdf/1802.08660v2)

> Smart contracts are programs running on cryptocurrency (e.g., Ethereum) blockchains, whose popularity stem from the possibility to perform financial transactions, such as payments and auctions, in a distributed environment without need for any trusted third party. Given their financial nature, bugs or vulnerabilities in these programs may lead to catastrophic consequences, as witnessed by recent attacks. Unfortunately, programming smart contracts is a delicate task that requires strong expertise: Ethereum smart contracts are written in Solidity, a dedicated language resembling JavaScript, and shipped over the blockchain in the EVM bytecode format. In order to rigorously verify the security of smart contracts, it is of paramount importance to formalize their semantics as well as the security properties of interest, in particular at the level of the bytecode being executed.   In this paper, we present the first complete small-step semantics of EVM bytecode, which we formalize in the F* proof assistant, obtaining executable code that we successfully validate against the official Ethereum test suite. Furthermore, we formally define for the first time a number of central security properties for smart contracts, such as call integrity, atomicity, and independence from miner controlled parameters. This formalization relies on a combination of hyper- and safety properties. Along this work, we identified various mistakes and imprecisions in existing semantics and verification tools for Ethereum smart contracts, thereby demonstrating once more the importance of rigorous semantic foundations for the design of security verification techniques.

</details>

<details>

<summary>2018-04-23 17:29:27 - An Empirical Analysis of Traceability in the Monero Blockchain</summary>

- *Malte Möser, Kyle Soska, Ethan Heilman, Kevin Lee, Henry Heffan, Shashvat Srivastava, Kyle Hogan, Jason Hennessey, Andrew Miller, Arvind Narayanan, Nicolas Christin*

- `1704.04299v4` - [abs](http://arxiv.org/abs/1704.04299v4) - [pdf](http://arxiv.org/pdf/1704.04299v4)

> Monero is a privacy-centric cryptocurrency that allows users to obscure their transactions by including chaff coins, called "mixins," along with the actual coins they spend. In this paper, we empirically evaluate two weaknesses in Monero's mixin sampling strategy. First, about 62% of transaction inputs with one or more mixins are vulnerable to "chain-reaction" analysis -- that is, the real input can be deduced by elimination. Second, Monero mixins are sampled in such a way that they can be easily distinguished from the real coins by their age distribution; in short, the real input is usually the "newest" input. We estimate that this heuristic can be used to guess the real input with 80% accuracy over all transactions with 1 or more mixins. Next, we turn to the Monero ecosystem and study the importance of mining pools and the former anonymous marketplace AlphaBay on the transaction volume. We find that after removing mining pool activity, there remains a large amount of potentially privacy-sensitive transactions that are affected by these weaknesses. We propose and evaluate two countermeasures that can improve the privacy of future transactions.

</details>

<details>

<summary>2018-04-24 17:28:56 - A Spark is Enough in a Straw World: a Study of Websites Password Management in the Wild</summary>

- *Simone Raponi, Roberto Di Pietro*

- `1804.07016v3` - [abs](http://arxiv.org/abs/1804.07016v3) - [pdf](http://arxiv.org/pdf/1804.07016v3)

> The widespread usage of password authentication in online websites leads to an ever-increasing concern, especially when considering the possibility for an attacker to recover the user password by leveraging the loopholes in the password recovery mechanisms. Indeed, if a website adopts a poor password management system, this choice makes useless even the most robust password chosen by its users. In this paper, we first provide a survey of currently adopted password recovery mechanisms. Later, we model an attacker with different capabilities and we show how current password recovery mechanisms can be exploited in our attacker model. Then, we provide a thorough analysis of the password management of some of the Alexa's top 200 websites in different countries, including England, France, Germany, Spain and Italy. Of these 1,000 websites, 722 do not require authentication -- and hence are excluded by our study -- while out of the remaining 278 we focused on 174, since 104 demanded a complex registration procedure. Of these 174, almost 25% of the them have critical vulnerabilities, while 44% have some form of vulnerability. Finally, we propose some effective countermeasures and we point out that, by considering the entry into force of the General Data Protection Regulation (GDPR) in May, 2018, most of websites are not compliant with the legislation and may incur in heavy fines. This study, other than being important on its own since it highlights some severe current vulnerabilities and proposes corresponding remedies, has the potential to also have a relevant impact on the EU industrial ecosystem.

</details>

<details>

<summary>2018-04-25 01:34:08 - Vulnerability Analysis of Smart Grids to GPS Spoofing</summary>

- *Paresh Risbud, Nikolaos Gatsis, Ahmad Taha*

- `1804.09310v1` - [abs](http://arxiv.org/abs/1804.09310v1) - [pdf](http://arxiv.org/pdf/1804.09310v1)

> Sensors such as phasor measurement units (PMUs) endowed with GPS receivers are ubiquitously installed providing real-time grid visibility. A number of PMUs can cooperatively enable state estimation routines. However, GPS spoofing attacks can notably alter the PMU measurements, mislead the network operator, and drastically impact subsequent corrective control actions. Leveraging a novel measurement model that explicitly accounts for the GPS spoofing attacks, this paper formulates an optimization problem to identify the most vulnerable PMUs in the network. A greedy algorithm is developed to solve the aforementioned problem. Furthermore, the paper develops a computationally efficient alternating minimization algorithm for joint state estimation and attack reconstruction. Numerical tests on IEEE benchmark networks validate the developed methods.

</details>

<details>

<summary>2018-04-26 08:14:16 - NEXUS: Using Geo-fencing Services without revealing your Location</summary>

- *Michael Guldner, Torsten Spieldenner, René Schubotz*

- `1804.09933v1` - [abs](http://arxiv.org/abs/1804.09933v1) - [pdf](http://arxiv.org/pdf/1804.09933v1)

> While becoming more and more present in our every day lives, services that operate on users' locations or location trajectories suffer from general fear of misappropriation of the transmitted location data. Several works have investigated of how to cope with this drawback. Respective systems claim location-privacy, i.e. keeping users' locations secret, by employing anonymisation techniques concerning a user's identity, or by obfuscating the transmitted location. These approaches lead to a degrade of quality-of-service and can be vulnerable to de-anonymisation attacks, or allow to learn at least the approximate location of a user. Focusing on the application domain of geo-fencing, we present as remedy a protocol that is based on homomorphic encryption of a user's location. The protocol provably provides full location-privacy by non-exposure of the users' location data, while producing exact geo-fencing results. We provide a detailed definition of the protocol, show its applicability in an actual geo-fencing application, and show that the resulting system fulfills all security properties we see for a location-privacy preserving system.

</details>

<details>

<summary>2018-04-27 12:44:04 - A Hybrid Q-Learning Sine-Cosine-based Strategy for Addressing the Combinatorial Test Suite Minimization Problem</summary>

- *Kamal Z. Zamli, Fakhrud Din, Bestoun S. Ahmed, Miroslav Bures*

- `1805.00873v1` - [abs](http://arxiv.org/abs/1805.00873v1) - [pdf](http://arxiv.org/pdf/1805.00873v1)

> The sine-cosine algorithm (SCA) is a new population-based meta-heuristic algorithm. In addition to exploiting sine and cosine functions to perform local and global searches (hence the name sine-cosine), the SCA introduces several random and adaptive parameters to facilitate the search process. Although it shows promising results, the search process of the SCA is vulnerable to local minima/maxima due to the adoption of a fixed switch probability and the bounded magnitude of the sine and cosine functions (from -1 to 1). In this paper, we propose a new hybrid Q-learning sine-cosine- based strategy, called the Q-learning sine-cosine algorithm (QLSCA). Within the QLSCA, we eliminate the switching probability. Instead, we rely on the Q-learning algorithm (based on the penalty and reward mechanism) to dynamically identify the best operation during runtime. Additionally, we integrate two new operations (L\'evy flight motion and crossover) into the QLSCA to facilitate jumping out of local minima/maxima and enhance the solution diversity. To assess its performance, we adopt the QLSCA for the combinatorial test suite minimization problem. Experimental results reveal that the QLSCA is statistically superior with regard to test suite size reduction compared to recent state-of-the-art strategies, including the original SCA, the particle swarm test generator (PSTG), adaptive particle swarm optimization (APSO) and the cuckoo search strategy (CS) at the 95% confidence level. However, concerning the comparison with discrete particle swarm optimization (DPSO), there is no significant difference in performance at the 95% confidence level. On a positive note, the QLSCA statistically outperforms the DPSO in certain configurations at the 90% confidence level.

</details>

<details>

<summary>2018-04-30 02:09:25 - Adversarial Regression for Detecting Attacks in Cyber-Physical Systems</summary>

- *Amin Ghafouri, Yevgeniy Vorobeychik, Xenofon Koutsoukos*

- `1804.11022v1` - [abs](http://arxiv.org/abs/1804.11022v1) - [pdf](http://arxiv.org/pdf/1804.11022v1)

> Attacks in cyber-physical systems (CPS) which manipulate sensor readings can cause enormous physical damage if undetected. Detection of attacks on sensors is crucial to mitigate this issue. We study supervised regression as a means to detect anomalous sensor readings, where each sensor's measurement is predicted as a function of other sensors. We show that several common learning approaches in this context are still vulnerable to \emph{stealthy attacks}, which carefully modify readings of compromised sensors to cause desired damage while remaining undetected. Next, we model the interaction between the CPS defender and attacker as a Stackelberg game in which the defender chooses detection thresholds, while the attacker deploys a stealthy attack in response. We present a heuristic algorithm for finding an approximately optimal threshold for the defender in this game, and show that it increases system resilience to attacks without significantly increasing the false alarm rate.

</details>


## 2018-05

<details>

<summary>2018-05-02 16:14:14 - A Survey of Symbolic Execution Techniques</summary>

- *Roberto Baldoni, Emilio Coppa, Daniele Cono D'Elia, Camil Demetrescu, Irene Finocchi*

- `1610.00502v3` - [abs](http://arxiv.org/abs/1610.00502v3) - [pdf](http://arxiv.org/pdf/1610.00502v3)

> Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program's authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the last four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience.   The present survey has been accepted for publication at ACM Computing Surveys. If you are considering citing this survey, we would appreciate if you could use the following BibTeX entry: http://goo.gl/Hf5Fvc

</details>

<details>

<summary>2018-05-03 17:11:18 - Siamese networks for generating adversarial examples</summary>

- *Mandar Kulkarni, Aria Abubakar*

- `1805.01431v1` - [abs](http://arxiv.org/abs/1805.01431v1) - [pdf](http://arxiv.org/pdf/1805.01431v1)

> Machine learning models are vulnerable to adversarial examples. An adversary modifies the input data such that humans still assign the same label, however, machine learning models misclassify it. Previous approaches in the literature demonstrated that adversarial examples can even be generated for the remotely hosted model. In this paper, we propose a Siamese network based approach to generate adversarial examples for a multiclass target CNN. We assume that the adversary do not possess any knowledge of the target data distribution, and we use an unlabeled mismatched dataset to query the target, e.g., for the ResNet-50 target, we use the Food-101 dataset as the query. Initially, the target model assigns labels to the query dataset, and a Siamese network is trained on the image pairs derived from these multiclass labels. We learn the \emph{adversarial perturbations} for the Siamese model and show that these perturbations are also adversarial w.r.t. the target model. In experimental results, we demonstrate effectiveness of our approach on MNIST, CIFAR-10 and ImageNet targets with TinyImageNet/Food-101 query datasets.

</details>

<details>

<summary>2018-05-11 18:29:36 - MAT: A Multi-strength Adversarial Training Method to Mitigate Adversarial Attacks</summary>

- *Chang Song, Hsin-Pai Cheng, Huanrui Yang, Sicheng Li, Chunpeng Wu, Qing Wu, Hai Li, Yiran Chen*

- `1705.09764v2` - [abs](http://arxiv.org/abs/1705.09764v2) - [pdf](http://arxiv.org/pdf/1705.09764v2)

> Some recent works revealed that deep neural networks (DNNs) are vulnerable to so-called adversarial attacks where input examples are intentionally perturbed to fool DNNs. In this work, we revisit the DNN training process that includes adversarial examples into the training dataset so as to improve DNN's resilience to adversarial attacks, namely, adversarial training. Our experiments show that different adversarial strengths, i.e., perturbation levels of adversarial examples, have different working zones to resist the attack. Based on the observation, we propose a multi-strength adversarial training method (MAT) that combines the adversarial training examples with different adversarial strengths to defend adversarial attacks. Two training structures - mixed MAT and parallel MAT - are developed to facilitate the tradeoffs between training time and memory occupation. Our results show that MAT can substantially minimize the accuracy degradation of deep learning systems to adversarial attacks on MNIST, CIFAR-10, CIFAR-100, and SVHN.

</details>

<details>

<summary>2018-05-11 21:20:09 - Quantifying Users' Beliefs about Software Updates</summary>

- *Arunesh Mathur, Nathan Malkin, Marian Harbach, Eyal Peer, Serge Egelman*

- `1805.04594v1` - [abs](http://arxiv.org/abs/1805.04594v1) - [pdf](http://arxiv.org/pdf/1805.04594v1)

> Software updates are critical to the performance, compatibility, and security of software systems. However, users do not always install updates, leaving their machines vulnerable to attackers' exploits. While recent studies have highlighted numerous reasons why users ignore updates, little is known about how prevalent each of these beliefs is. Gaining a better understanding of the prevalence of each belief may help software designers better target their efforts in understanding what specific user concerns to address when developing and deploying software updates. In our study, we performed a survey to quantify the prevalence of users' reasons for not updating uncovered by previous studies. We used this data to derive three factors underlying these beliefs: update costs, update necessity, and update risks. Based on our results, we provide recommendations for how software developers can better improve users' software updating experiences, thereby increasing compliance and, with it, security.

</details>

<details>

<summary>2018-05-12 19:46:43 - Requirements for Secure Clock Synchronization</summary>

- *Lakshay Narula, Todd Humphreys*

- `1710.05798v3` - [abs](http://arxiv.org/abs/1710.05798v3) - [pdf](http://arxiv.org/pdf/1710.05798v3)

> This paper establishes a fundamental theory of secure clock synchronization. Accurate clock synchronization is the backbone of systems managing power distribution, financial transactions, telecommunication operations, database services, etc. Some clock synchronization (time transfer) systems, such as the Global Navigation Satellite Systems (GNSS), are based on one-way communication from a master to a slave clock. Others, such as the Network Transport Protocol (NTP), and the IEEE 1588 Precision Time Protocol (PTP), involve two-way communication between the master and slave. This paper shows that all one-way time transfer protocols are vulnerable to replay attacks that can potentially compromise timing information. A set of conditions for secure two-way clock synchronization is proposed and proved to be necessary and sufficient. It is shown that IEEE 1588 PTP, although a two-way synchronization protocol, is not compliant with these conditions, and is therefore insecure. Requirements for secure IEEE 1588 PTP are proposed, and a second example protocol is offered to illustrate the range of compliant systems.

</details>

<details>

<summary>2018-05-14 03:53:25 - Double-Spending Risk Quantification in Private, Consortium and Public Ethereum Blockchains</summary>

- *Parinya Ekparinya, Vincent Gramoli, Guillaume Jourjon*

- `1805.05004v1` - [abs](http://arxiv.org/abs/1805.05004v1) - [pdf](http://arxiv.org/pdf/1805.05004v1)

> Recently, several works conjectured the vulnerabilities of mainstream blockchains under several network attacks. All these attacks translate into showing that the assumptions of these blockchains can be violated in theory or under simulation at best. Unfortunately, previous results typically omit both the nature of the network under which the blockchain code runs and whether blockchains are private, consortium or public. In this paper, we study the public Ethereum blockchain as well as a consortium and private blockchains and quantify the feasibility of man-in-the-middle and double spending attacks against them. To this end, we list important properties of the Ethereum public blockchain topology, we deploy VMs with constrained CPU quantum to mimic the top-10 mining pools of Ethereum and we develop full-fledged attacks, that first partition the network through BGP hijacking or ARP spoofing before issuing a Balance Attack to steal coins. Our results demonstrate that attacking Ethereum is remarkably devastating in a consortium or private context as the adversary can multiply her digital assets by 200, 000x in 10 hours through BGP hijacking whereas it would be almost impossible in a public context.

</details>

<details>

<summary>2018-05-14 09:37:59 - User Blocking Considered Harmful? An Attacker-controllable Side Channel to Identify Social Accounts</summary>

- *Takuya Watanabe, Eitaro Shioji, Mitsuaki Akiyama, Keito Sasaoka, Takeshi Yagi, Tatsuya Mori*

- `1805.05085v1` - [abs](http://arxiv.org/abs/1805.05085v1) - [pdf](http://arxiv.org/pdf/1805.05085v1)

> This paper presents a practical side-channel attack that identifies the social web service account of a visitor to an attacker's website. Our attack leverages the widely adopted user-blocking mechanism, abusing its inherent property that certain pages return different web content depending on whether a user is blocked from another user. Our key insight is that an account prepared by an attacker can hold an attacker-controllable binary state of blocking/non-blocking with respect to an arbitrary user on the same service; provided that the user is logged in to the service, this state can be retrieved as one-bit data through the conventional cross-site timing attack when a user visits the attacker's website. We generalize and refer to such a property as visibility control, which we consider as the fundamental assumption of our attack. Building on this primitive, we show that an attacker with a set of controlled accounts can gain a complete and flexible control over the data leaked through the side channel. Using this mechanism, we show that it is possible to design and implement a robust, large-scale user identification attack on a wide variety of social web services. To verify the feasibility of our attack, we perform an extensive empirical study using 16 popular social web services and demonstrate that at least 12 of these are vulnerable to our attack. Vulnerable services include not only popular social networking sites such as Twitter and Facebook, but also other types of web services that provide social features, e.g., eBay and Xbox Live. We also demonstrate that the attack can achieve nearly 100% accuracy and can finish within a sufficiently short time in a practical setting. We discuss the fundamental principles, practical aspects, and limitations of the attack as well as possible defenses.

</details>

<details>

<summary>2018-05-15 00:48:46 - PartiSan: Fast and Flexible Sanitization via Run-time Partitioning</summary>

- *Julian Lettner, Dokyung Song, Taemin Park, Stijn Volckaert, Per Larsen, Michael Franz*

- `1711.08108v2` - [abs](http://arxiv.org/abs/1711.08108v2) - [pdf](http://arxiv.org/pdf/1711.08108v2)

> Sanitizers can detect security vulnerabilities in C/C++ code that elude static analysis. Current practice is to continuously fuzz and sanitize internal pre-release builds. Sanitization-enabled builds are rarely released publicly. This is in large part due to the high memory and processing requirements of sanitizers.   We present PartiSan, a run-time partitioning technique that speeds up sanitizers and allows them to be used in a more flexible manner. Our core idea is to partition the execution into sanitized slices that incur a run-time overhead, and unsanitized slices running at full speed. With PartiSan, sanitization is no longer an all-or-nothing proposition. A single build can be distributed to every user regardless of their willingness to enable sanitization and the capabilities of their host system. PartiSan can automatically adjust the amount of sanitization to fit within a performance budget or disable sanitization if the host lacks sufficient resources. The flexibility afforded by run-time partitioning also means that we can alternate between different types of sanitizers dynamically; today, developers have to pick a single type of sanitizer ahead of time. Finally, we show that run-time partitioning can speed up fuzzing by running the sanitized partition only when the fuzzer discovers an input that causes a crash or uncovers new execution paths.

</details>

<details>

<summary>2018-05-15 00:59:03 - Task Interruption in Software Development Projects: What Makes some Interruptions More Disruptive than Others?</summary>

- *Zahra Shakeri Hossein Abad, Oliver Karras, Kurt Schneider, Ken Barker, Mike Bauer*

- `1805.05508v1` - [abs](http://arxiv.org/abs/1805.05508v1) - [pdf](http://arxiv.org/pdf/1805.05508v1)

> Multitasking has always been an inherent part of software development and is known as the primary source of interruptions due to task switching in software development teams. Developing software involves a mix of analytical and creative work, and requires a significant load on brain functions, such as working memory and decision making. Thus, task switching in the context of software development imposes a cognitive load that causes software developers to lose focus and concentration while working thereby taking a toll on productivity. To investigate the disruptiveness of task switching and interruptions in software development projects, and to understand the reasons for and perceptions of the disruptiveness of task switching we used a mixed-methods approach including a longitudinal data analysis on 4,910 recorded tasks of 17 professional software developers, and a survey of 132 software developers. We found that, compared to task-specific factors (e.g. priority, level, and temporal stage), contextual factors such as interruption type (e.g. self/external), time of day, and task type and context are a more potent determinant of task switching disruptiveness in software development tasks. Furthermore, while most survey respondents believe external interruptions are more disruptive than self-interruptions, the results of our retrospective analysis reveals otherwise. We found that self-interruptions (i.e. voluntary task switchings) are more disruptive than external interruptions and have a negative effect on the performance of the interrupted tasks. Finally, we use the results of both studies to provide a set of comparative vulnerability and interaction patterns which can be used as a mean to guide decision-making and forecasting the consequences of task switching in software development teams.

</details>

<details>

<summary>2018-05-15 01:21:02 - A Formal Model to Facilitate Security Testing in Modern Automotive Systems</summary>

- *Eduardo dos Santos, Andrew Simpson, Dominik Schoop*

- `1805.05520v1` - [abs](http://arxiv.org/abs/1805.05520v1) - [pdf](http://arxiv.org/pdf/1805.05520v1)

> Ensuring a car's internal systems are free from security vulnerabilities is of utmost importance, especially due to the relationship between security and other properties, such as safety and reliability. We provide the starting point for a model-based framework designed to support the security testing of modern cars. We use Communicating Sequential Processes (CSP) to create architectural models of the vehicle bus systems, as well as an initial set of attacks against these systems. While this contribution represents initial steps, we are mindful of the ultimate objective of generating test code to exercise the security of vehicle bus systems. We present the way forward from the models created and consider their potential integration with commercial engineering tools

</details>

<details>

<summary>2018-05-15 15:36:17 - IoT Security: An End-to-End View and Case Study</summary>

- *Zhen Ling, Kaizheng Liu, Yiling Xu, Chao Gao, Yier Jin, Cliff Zou, Xinwen Fu, Wei Zhao*

- `1805.05853v1` - [abs](http://arxiv.org/abs/1805.05853v1) - [pdf](http://arxiv.org/pdf/1805.05853v1)

> In this paper, we present an end-to-end view of IoT security and privacy and a case study. Our contribution is three-fold. First, we present our end-to-end view of an IoT system and this view can guide risk assessment and design of an IoT system. We identify 10 basic IoT functionalities that are related to security and privacy. Based on this view, we systematically present security and privacy requirements in terms of IoT system, software, networking and big data analytics in the cloud. Second, using the end-to-end view of IoT security and privacy, we present a vulnerability analysis of the Edimax IP camera system. We are the first to exploit this system and have identified various attacks that can fully control all the cameras from the manufacturer. Our real-world experiments demonstrate the effectiveness of the discovered attacks and raise the alarms again for the IoT manufacturers. Third, such vulnerabilities found in the exploit of Edimax cameras and our previous exploit of Edimax smartplugs can lead to another wave of Mirai attacks, which can be either botnets or worm attacks. To systematically understand the damage of the Mirai malware, we model propagation of the Mirai and use the simulations to validate the modeling. The work in this paper raises the alarm again for the IoT device manufacturers to better secure their products in order to prevent malware attacks like Mirai.

</details>

<details>

<summary>2018-05-17 15:24:31 - Robustness of Rotation-Equivariant Networks to Adversarial Perturbations</summary>

- *Beranger Dumont, Simona Maggio, Pablo Montalvo*

- `1802.06627v2` - [abs](http://arxiv.org/abs/1802.06627v2) - [pdf](http://arxiv.org/pdf/1802.06627v2)

> Deep neural networks have been shown to be vulnerable to adversarial examples: very small perturbations of the input having a dramatic impact on the predictions. A wealth of adversarial attacks and distance metrics to quantify the similarity between natural and adversarial images have been proposed, recently enlarging the scope of adversarial examples with geometric transformations beyond pixel-wise attacks. In this context, we investigate the robustness to adversarial attacks of new Convolutional Neural Network architectures providing equivariance to rotations. We found that rotation-equivariant networks are significantly less vulnerable to geometric-based attacks than regular networks on the MNIST, CIFAR-10, and ImageNet datasets.

</details>

<details>

<summary>2018-05-17 19:51:44 - A note on some algebraic trapdoors for block ciphers</summary>

- *Marco Calderini*

- `1705.08151v2` - [abs](http://arxiv.org/abs/1705.08151v2) - [pdf](http://arxiv.org/pdf/1705.08151v2)

> We provide sufficient conditions to guarantee that a translation based cipher is not vulnerable with respect to the partition-based trapdoor. This trapdoor has been introduced, recently, by Bannier et al. (2016) and it generalizes that introduced by Paterson in 1999. Moreover, we discuss the fact that studying the group generated by the round functions of a block cipher may not be sufficient to guarantee security against these trapdoors for the cipher.

</details>

<details>

<summary>2018-05-17 20:42:07 - Counterexample-Guided Data Augmentation</summary>

- *Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue, Kurt Keutzer, Alberto Sangiovanni-Vincentelli, Sanjit A. Seshia*

- `1805.06962v1` - [abs](http://arxiv.org/abs/1805.06962v1) - [pdf](http://arxiv.org/pdf/1805.06962v1)

> We present a novel framework for augmenting data sets for machine learning based on counterexamples. Counterexamples are misclassified examples that have important properties for retraining and improving the model. Key components of our framework include a counterexample generator, which produces data items that are misclassified by the model and error tables, a novel data structure that stores information pertaining to misclassifications. Error tables can be used to explain the model's vulnerabilities and are used to efficiently generate counterexamples for augmentation. We show the efficacy of the proposed framework by comparing it to classical augmentation techniques on a case study of object detection in autonomous driving based on deep neural networks.

</details>

<details>

<summary>2018-05-17 22:05:33 - A Secret Key Generation Scheme for Internet of Things using Ternary-States ReRAM-based Physical Unclonable Functions</summary>

- *Ashwija Reddy Korenda, Fatemeh Afghah, Bertrand Cambou*

- `1805.06980v1` - [abs](http://arxiv.org/abs/1805.06980v1) - [pdf](http://arxiv.org/pdf/1805.06980v1)

> Some of the main challenges towards utilizing conventional cryptographic techniques in Internet of Things (IoT) include the need for generating secret keys for such a large-scale network, distributing the generated keys to all the devices, key storage as well as the vulnerability to security attacks when an adversary gets physical access to the devices. In this paper, a novel secret key generation method is proposed for IoTs that utilize the intrinsic randomness embedded in the devices' memories introduced in the manufacturing process. A fuzzy extractor structure using serially concatenated BCH-Polar codes is proposed to generate reproducible keys from a ReRAM-based \emph{ternary-state} Physical Unclonable Functions (PUFs) for device authentication and secret key generation. The ReRAM based PUFs are the most practical choice for authentication and key generation in IoT, as they operate at or below the systems' noise level and therefore are less vulnerable to side channel attacks compared to the alternative memory technologies. However, the current ReRAM-based PUFs present a high false negative authentication rate since the behavior of these devices can vary in different physical conditions that results in a low probability of regenerating the same response in different attempts. In this paper, we propose a secret key generation scheme for ternary state PUFs that enables reliable reconstruction of the desired secret keys utilizing a serially concatenated BCH-Polar fuzzy extractor. The experimental results show that the proposed model can offer a significantly lower probability of mismatch between the original key and the regenerated ones, while a less number of \textit{Helper data} bits were used to extract the \textit{Key} when compared to previously proposed fuzzy extractor techniques.

</details>

<details>

<summary>2018-05-18 00:20:52 - Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models</summary>

- *Pouya Samangouei, Maya Kabkab, Rama Chellappa*

- `1805.06605v2` - [abs](http://arxiv.org/abs/1805.06605v2) - [pdf](http://arxiv.org/pdf/1805.06605v2)

> In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at https://github.com/kabkabm/defensegan

</details>

<details>

<summary>2018-05-18 09:56:27 - Security Vulnerabilities Against Fingerprint Biometric System</summary>

- *Mahesh Joshi, Bodhisatwa Mazumdar, Somnath Dey*

- `1805.07116v1` - [abs](http://arxiv.org/abs/1805.07116v1) - [pdf](http://arxiv.org/pdf/1805.07116v1)

> The biometric system is an automatic identification and authentication system that uses unique biological traits, such as fingerprint, face, iris, voice, retina, etc. of an individual. Of all these systems, fingerprint biometric system is the most widely used because of its low cost, high matching speed, and relatively high matching accuracy. Due to the high efficiency of fingerprint biometric system in verifying a legitimate user, numerous government and private organizations are using this system for security purpose. This paper provides an overview of the fingerprint biometric system and gives details about various current security aspects related to the system. The security concerns that we address include multiple attacks on the system, associated threat models, biometric cryptosystems, current issues, challenges, opportunities, and open problems that exist in present day fingerprint biometric systems

</details>

<details>

<summary>2018-05-20 04:27:09 - Predicting drug response of tumors from integrated genomic profiles by deep neural networks</summary>

- *Yu-Chiao Chiu, Hung-I Harry Chen, Tinghe Zhang, Songyao Zhang, Aparna Gorthi, Li-Ju Wang, Yufei Huang, Yidong Chen*

- `1805.07702v1` - [abs](http://arxiv.org/abs/1805.07702v1) - [pdf](http://arxiv.org/pdf/1805.07702v1)

> The study of high-throughput genomic profiles from a pharmacogenomics viewpoint has provided unprecedented insights into the oncogenic features modulating drug response. A recent screening of ~1,000 cancer cell lines to a collection of anti-cancer drugs illuminated the link between genotypes and vulnerability. However, due to essential differences between cell lines and tumors, the translation into predicting drug response in tumors remains challenging. Here we proposed a DNN model to predict drug response based on mutation and expression profiles of a cancer cell or a tumor. The model contains a mutation and an expression encoders pre-trained using a large pan-cancer dataset to abstract core representations of high-dimension data, followed by a drug response predictor network. Given a pair of mutation and expression profiles, the model predicts IC50 values of 265 drugs. We trained and tested the model on a dataset of 622 cancer cell lines and achieved an overall prediction performance of mean squared error at 1.96 (log-scale IC50 values). The performance was superior in prediction error or stability than two classical methods and four analog DNNs of our model. We then applied the model to predict drug response of 9,059 tumors of 33 cancer types. The model predicted both known, including EGFR inhibitors in non-small cell lung cancer and tamoxifen in ER+ breast cancer, and novel drug targets. The comprehensive analysis further revealed the molecular mechanisms underlying the resistance to a chemotherapeutic drug docetaxel in a pan-cancer setting and the anti-cancer potential of a novel agent, CX-5461, in treating gliomas and hematopoietic malignancies. Overall, our model and findings improve the prediction of drug response and the identification of novel therapeutic options.

</details>

<details>

<summary>2018-05-21 03:41:37 - Learning with Non-Convex Truncated Losses by SGD</summary>

- *Yi Xu, Shenghuo Zhu, Sen Yang, Chi Zhang, Rong Jin, Tianbao Yang*

- `1805.07880v1` - [abs](http://arxiv.org/abs/1805.07880v1) - [pdf](http://arxiv.org/pdf/1805.07880v1)

> Learning with a {\it convex loss} function has been a dominating paradigm for many years. It remains an interesting question how non-convex loss functions help improve the generalization of learning with broad applicability. In this paper, we study a family of objective functions formed by truncating traditional loss functions, which is applicable to both shallow learning and deep learning. Truncating loss functions has potential to be less vulnerable and more robust to large noise in observations that could be adversarial. More importantly, it is a generic technique without assuming the knowledge of noise distribution. To justify non-convex learning with truncated losses, we establish excess risk bounds of empirical risk minimization based on truncated losses for heavy-tailed output, and statistical error of an approximate stationary point found by stochastic gradient descent (SGD) method. Our experiments for shallow and deep learning for regression with outliers, corrupted data and heavy-tailed noise further justify the proposed method.

</details>

<details>

<summary>2018-05-22 03:02:41 - Adversarial Examples that Fool both Computer Vision and Time-Limited Humans</summary>

- *Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein*

- `1802.08195v3` - [abs](http://arxiv.org/abs/1802.08195v3) - [pdf](http://arxiv.org/pdf/1802.08195v3)

> Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.

</details>

<details>

<summary>2018-05-24 02:30:46 - Why Johnny Can't Store Passwords Securely? A Usability Evaluation of Bouncycastle Password Hashing</summary>

- *Chamila Wijayarathna, Nalin Asanka Gamagedara Arachchilage*

- `1805.09487v1` - [abs](http://arxiv.org/abs/1805.09487v1) - [pdf](http://arxiv.org/pdf/1805.09487v1)

> Lack of usability of security Application Programming In- terfaces (APIs) is one of the main reasons for mistakes that programmers make that result in security vulnerabilities in software applications they develop. Especially, APIs that pro- vide cryptographic functionalities such as password hashing are sometimes too complex for programmers to learn and use. To improve the usability of these APIs to make them easy to learn and use, it is important to identify the usability issues exist on those APIs that make those harder to learn and use. In this work, we evaluated the usability of SCrypt password hashing functionality of Bouncycastle API to identify usabil- ity issues in it that persuade programmers to make mistakes while developing applications that would result in security vulnerabilities. We conducted a study with 10 programmers where each of them spent around 2 hours for the study and attempted to develop a secure password storage solution us- ing Bouncycastle API. From data we collected, we identified 63 usability issues that exist in the SCrypt implementation of Bouncycastle API. Results of our study provided useful insights about how security/cryptographic APIs should be designed, developed and improved to provide a better experi- ence for programmers who use them. Furthermore, we expect that this work will provide a guidance on how to conduct usability evaluations for security APIs to identify usability issues exist in them.

</details>

<details>

<summary>2018-05-24 07:56:06 - Evaluation of Static Analysis Tools for Finding Vulnerabilities in Java and C/C++ Source Code</summary>

- *Rahma Mahmood, Qusay H. Mahmoud*

- `1805.09040v2` - [abs](http://arxiv.org/abs/1805.09040v2) - [pdf](http://arxiv.org/pdf/1805.09040v2)

> It is quite common for security testing to be delayed until after the software has been developed, but vulnerabilities may get noticed throughout the implementation phase and the earlier they are discovered, the easier and cheaper it will be to fix them. Software development processes such as the secure software development lifecycle incorporates security at every stage of the design and development process. Static code scanning tools find vulnerabilities in code by highlighting potential security flaws and offer examples on how to resolve them, and some may even modify the code to remove the susceptibility. This paper compares static analysis tools for Java and C/C++ source code, and explores their pros and cons.

</details>

<details>

<summary>2018-05-24 09:37:56 - Optimal noise functions for location privacy on continuous regions</summary>

- *Ehab ElSalamouny, Sébastien Gambs*

- `1805.09571v1` - [abs](http://arxiv.org/abs/1805.09571v1) - [pdf](http://arxiv.org/pdf/1805.09571v1)

> Users of location-based services (LBSs) are highly vulnerable to privacy risks since they need to disclose, at least partially, their locations to benefit from these services. One possibility to limit these risks is to obfuscate the location of a user by adding random noise drawn from a noise function. In this paper, we require the noise functions to satisfy a generic location privacy notion called $\ell$-privacy, which makes the position of the user in a given region $\mathcal{X}$ relatively indistinguishable from other points in $\mathcal{X}$. We also aim at minimizing the loss in the service utility due to such obfuscation. While existing optimization frameworks regard the region $\mathcal{X}$ restrictively as a finite set of points, we consider the more realistic case in which the region is rather continuous with a non-zero area. In this situation, we demonstrate that circular noise functions are enough to satisfy $\ell$-privacy on $\mathcal{X}$ and equivalently on the entire space without any penalty in the utility. Afterwards, we describe a large parametric space of noise functions that satisfy $\ell$-privacy on $\mathcal{X}$, and show that this space has always an optimal member, regardless of $\ell$ and $\mathcal{X}$. We also investigate the recent notion of $\epsilon$-geo-indistinguishability as an instance of $\ell$-privacy, and prove in this case that with respect to any increasing loss function, the planar Laplace noise function is optimal for any region having a nonzero area.

</details>

<details>

<summary>2018-05-24 18:50:36 - A Bug Bounty Perspective on the Disclosure of Web Vulnerabilities</summary>

- *Jukka Ruohonen, Luca Allodi*

- `1805.09850v1` - [abs](http://arxiv.org/abs/1805.09850v1) - [pdf](http://arxiv.org/pdf/1805.09850v1)

> Bug bounties have become increasingly popular in recent years. This paper discusses bug bounties by framing these theoretically against so-called platform economy. Empirically the interest is on the disclosure of web vulnerabilities through the Open Bug Bounty (OBB) platform between 2015 and late 2017. According to the empirical results based on a dataset covering nearly 160 thousand web vulnerabilities, (i) OBB has been successful as a community-based platform for the dissemination of web vulnerabilities. The platform has also attracted many productive hackers, (ii) but there exists a large productivity gap, which likely relates to (iii) a knowledge gap and the use of automated tools for web vulnerability discovery. While the platform (iv) has been exceptionally fast to evaluate new vulnerability submissions, (v) the patching times of the web vulnerabilities disseminated have been long. With these empirical results and the accompanying theoretical discussion, the paper contributes to the small but rapidly growing amount of research on bug bounties. In addition, the paper makes a practical contribution by discussing the business models behind bug bounties from the viewpoints of platforms, ecosystems, and vulnerability markets.

</details>

<details>

<summary>2018-05-24 23:18:12 - The spread of low-credibility content by social bots</summary>

- *Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kaicheng Yang, Alessandro Flammini, Filippo Menczer*

- `1707.07592v4` - [abs](http://arxiv.org/abs/1707.07592v4) - [pdf](http://arxiv.org/pdf/1707.07592v4)

> The massive spread of digital misinformation has been identified as a major global risk and has been alleged to influence elections and threaten democracies. Communication, cognitive, social, and computer scientists are engaged in efforts to study the complex causes for the viral diffusion of misinformation online and to develop solutions, while search and social media platforms are beginning to deploy countermeasures. With few exceptions, these efforts have been mainly informed by anecdotal evidence rather than systematic data. Here we analyze 14 million messages spreading 400 thousand articles on Twitter during and following the 2016 U.S. presidential campaign and election. We find evidence that social bots played a disproportionate role in amplifying low-credibility content. Accounts that actively spread articles from low-credibility sources are significantly more likely to be bots. Automated accounts are particularly active in amplifying content in the very early spreading moments, before an article goes viral. Bots also target users with many followers through replies and mentions. Humans are vulnerable to this manipulation, retweeting bots who post links to low-credibility content. Successful low-credibility sources are heavily supported by social bots. These results suggest that curbing social bots may be an effective strategy for mitigating the spread of online misinformation.

</details>

<details>

<summary>2018-05-25 15:39:06 - Adversarial examples from computational constraints</summary>

- *Sébastien Bubeck, Eric Price, Ilya Razenshteyn*

- `1805.10204v1` - [abs](http://arxiv.org/abs/1805.10204v1) - [pdf](http://arxiv.org/pdf/1805.10204v1)

> Why are classifiers in high dimension vulnerable to "adversarial" perturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to computational constraints.   First we prove that, for a broad set of classification tasks, the mere existence of a robust classifier implies that it can be found by a possibly exponential-time algorithm with relatively few training examples. Then we give a particular classification task where learning a robust classifier is computationally intractable. More precisely we construct a binary classification task in high dimensional space which is (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (non-robustly) by a simple linear separator, (iii) yet is not efficiently robustly learnable, even for small perturbations, by any algorithm in the statistical query (SQ) model. This example gives an exponential separation between classical learning and robust learning in the statistical query model. It suggests that adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.

</details>

<details>

<summary>2018-05-27 16:47:31 - Defending Against Adversarial Attacks by Leveraging an Entire GAN</summary>

- *Gokula Krishnan Santhanam, Paulina Grnarova*

- `1805.10652v1` - [abs](http://arxiv.org/abs/1805.10652v1) - [pdf](http://arxiv.org/pdf/1805.10652v1)

> Recent work has shown that state-of-the-art models are highly vulnerable to adversarial perturbations of the input. We propose cowboy, an approach to detecting and defending against adversarial attacks by using both the discriminator and generator of a GAN trained on the same dataset. We show that the discriminator consistently scores the adversarial samples lower than the real samples across multiple attacks and datasets. We provide empirical evidence that adversarial samples lie outside of the data manifold learned by the GAN. Based on this, we propose a cleaning method which uses both the discriminator and generator of the GAN to project the samples back onto the data manifold. This cleaning procedure is independent of the classifier and type of attack and thus can be deployed in existing systems.

</details>

<details>

<summary>2018-05-27 17:59:26 - An Evaluation of Score Level Fusion Approaches for Fingerprint and Finger-vein Biometrics</summary>

- *Kamer Vishi, Vasileios Mavroeidis*

- `1805.10666v1` - [abs](http://arxiv.org/abs/1805.10666v1) - [pdf](http://arxiv.org/pdf/1805.10666v1)

> Biometric systems have to address many requirements, such as large population coverage, demographic diversity, varied deployment environment, as well as practical aspects like performance and spoofing attacks. Traditional unimodal biometric systems do not fully meet the aforementioned requirements making them vulnerable and susceptible to different types of attacks. In response to that, modern biometric systems combine multiple biometric modalities at different fusion levels. The fused score is decisive to classify an unknown user as a genuine or impostor. In this paper, we evaluate combinations of score normalization and fusion techniques using two modalities (fingerprint and finger-vein) with the goal of identifying which one achieves better improvement rate over traditional unimodal biometric systems. The individual scores obtained from finger-veins and fingerprints are combined at score level using three score normalization techniques (min-max, z-score, hyperbolic tangent) and four score fusion approaches (minimum score, maximum score, simple sum, user weighting). The experimental results proved that the combination of hyperbolic tangent score normalization technique with the simple sum fusion approach achieve the best improvement rate of 99.98%.

</details>

<details>

<summary>2018-05-28 08:45:31 - NETRA: Enhancing IoT Security using NFV-based Edge Traffic Analysis</summary>

- *Rishi Sairam, Suman Sankar Bhunia, Vijayanand Thangavelu, Mohan Gurusamy*

- `1805.10815v1` - [abs](http://arxiv.org/abs/1805.10815v1) - [pdf](http://arxiv.org/pdf/1805.10815v1)

> This is the era of smart devices or things which are fueling the growth of Internet of Things (IoT). It is impacting every sphere around us, making our life dependent on this technological feat. It is of high concern that these smart things are being targeted by cyber criminals taking advantage of heterogeneity, minuscule security features and vulnerabilities within these devices. Conventional centralized IT security measures have limitations in terms of scalability and cost. Therefore, these smart devices are required to be monitored closer to their location ideally at the edge of IoT networks. In this paper, we explore how some security features can be implemented at the network edge to secure these smart devices. We explain the importance of Network Function Virtualization (NFV) in order to deploy security functions at the network edge. To achieve this goal, we introduce NETRA - a novel lightweight Docker-based architecture for virtualizing network functions to provide IoT security. Also, we highlight the advantages of the proposed architecture over the standardized NFV architecture in terms of storage, memory usage, latency, throughput, load average, scalability and explain why the standardized architecture is not suitable for IoT. We study the performance of proposed NFV based edge analysis for IoT security and show that attacks can be detected with more than 95% accuracy in less than a second.

</details>

<details>

<summary>2018-05-28 12:28:38 - An evaluation of the security of the Bitcoin Peer-to- Peer Network</summary>

- *James Tapsell, Raja Naeem Akram, Konstantinos Markantonakis*

- `1805.10259v2` - [abs](http://arxiv.org/abs/1805.10259v2) - [pdf](http://arxiv.org/pdf/1805.10259v2)

> Bitcoin is a decentralised digital currency that relies on cryptography rather than trusted third parties such as central banks for its security. Underpinning the operation of the currency is a peer-to-peer (P2P) network that facilitates the execution of transactions by end users, as well as the transaction confirmation process known as bitcoin mining. The security of this P2P network is vital for the currency to function and subversion of the underlying network can lead to attacks on bitcoin users including theft of bitcoins, manipulation of the mining process and denial of service (DoS). As part of this paper the network protocol and bitcoin core software are analysed, with three bitcoin message exchanges (the connection handshake, GETHEADERS/HEADERS and MEMPOOL/INV) found to be potentially vulnerable to spoofing and use in distributed denial of service (DDoS) attacks. Possible solutions to the identified weaknesses and vulnerabilities are evaluated, such as the introduction of random nonces into network messages exchanges.

</details>

<details>

<summary>2018-05-28 17:12:33 - Dandelion++: Lightweight Cryptocurrency Networking with Formal Anonymity Guarantees</summary>

- *Giulia Fanti, Shaileshh Bojja Venkatakrishnan, Surya Bakshi, Bradley Denby, Shruti Bhargava, Andrew Miller, Pramod Viswanath*

- `1805.11060v1` - [abs](http://arxiv.org/abs/1805.11060v1) - [pdf](http://arxiv.org/pdf/1805.11060v1)

> Recent work has demonstrated significant anonymity vulnerabilities in Bitcoin's networking stack. In particular, the current mechanism for broadcasting Bitcoin transactions allows third-party observers to link transactions to the IP addresses that originated them. This lays the groundwork for low-cost, large-scale deanonymization attacks. In this work, we present Dandelion++, a first-principles defense against large-scale deanonymization attacks with near-optimal information-theoretic guarantees. Dandelion++ builds upon a recent proposal called Dandelion that exhibited similar goals. However, in this paper, we highlight simplifying assumptions made in Dandelion, and show how they can lead to serious deanonymization attacks when violated. In contrast, Dandelion++ defends against stronger adversaries that are allowed to disobey protocol. Dandelion++ is lightweight, scalable, and completely interoperable with the existing Bitcoin network. We evaluate it through experiments on Bitcoin's mainnet (i.e., the live Bitcoin network) to demonstrate its interoperability and low broadcast latency overhead.

</details>

<details>

<summary>2018-05-30 02:37:13 - Detecting Data Leakage from Databases on Android Apps with Concept Drift</summary>

- *Gokhan Kul, Shambhu Upadhyaya, Varun Chandola*

- `1805.11780v1` - [abs](http://arxiv.org/abs/1805.11780v1) - [pdf](http://arxiv.org/pdf/1805.11780v1)

> Mobile databases are the statutory backbones of many applications on smartphones, and they store a lot of sensitive information. However, vulnerabilities in the operating system or the app logic can lead to sensitive data leakage by giving the adversaries unauthorized access to the app's database. In this paper, we study such vulnerabilities to define a threat model, and we propose an OS-version independent protection mechanism that app developers can utilize to detect such attacks. To do so, we model the user behavior with the database query workload created by the original apps. Here, we model the drift in behavior by comparing probability distributions of the query workload features over time. We then use this model to determine if the app behavior drift is anomalous. We evaluate our framework on real-world workloads of three different popular Android apps, and we show that our system was able to detect more than 90% of such attacks.

</details>

<details>

<summary>2018-05-30 12:09:45 - Lord of the x86 Rings: A Portable User Mode Privilege Separation Architecture on x86</summary>

- *Hojoon Lee, Chihyun Song, Brent Byunghoon Kang*

- `1805.11912v1` - [abs](http://arxiv.org/abs/1805.11912v1) - [pdf](http://arxiv.org/pdf/1805.11912v1)

> Modern applications are increasingly advanced and complex, and inevitably contain exploitable software bugs despite the ongoing efforts. The applications today often involve processing of sensitive information. However, the lack of privilege separation within the user space leaves sensitive application secret such as cryptographic keys just as unprotected as a "hello world" string. Cutting-edge hardware-supported security features are being introduced. However, the features are often vendor-specific or lack compatibility with older generations of the processors. The situation leaves developers with no portable solution to incorporate protection for the sensitive application component.   We propose LOTRx86, a fundamental and portable approach for user space privilege separation. Our approach creates a more privileged user execution layer called PrivUser through harnessing the underused intermediate privilege levels on the x86 architecture. The PrivUser memory space, a set of pages within process address space that are inaccessible to user mode, is a safe place for application secrets and routines that access them. We implement the LOTRx86 ABI that exports the privilege-based, accessing the protected application secret only requires a change in the privilege, eliminating the need for costly remote procedure calls or change in address space.   We evaluated our platform by developing a proof-of-concept LOTRx86-enabled web server that employs our architecture to securely access its private key during SSL connection and thereby mitigating the HeartBleed vulnerability by design. We conducted a set of experiments including a performance measurement on the PoC on both Intel and AMD PCs, and confirmed that LOTRx86 incurs only a limited performance overhead.

</details>

<details>

<summary>2018-05-30 15:47:05 - The Coming Era of AlphaHacking? A Survey of Automatic Software Vulnerability Detection, Exploitation and Patching Techniques</summary>

- *Tiantian Ji, Yue Wu, Chang Wang, Xi Zhang, Zhongru Wang*

- `1805.11001v2` - [abs](http://arxiv.org/abs/1805.11001v2) - [pdf](http://arxiv.org/pdf/1805.11001v2)

> With the success of the Cyber Grand Challenge (CGC) sponsored by DARPA, the topic of Autonomous Cyber Reasoning System (CRS) has recently attracted extensive attention from both industry and academia. Utilizing automated system to detect, exploit and patch software vulnerabilities seems so attractive because of its scalability and cost-efficiency compared with the human expert based solution. In this paper, we give an extensive survey of former representative works related to the underlying technologies of a CRS, including vulnerability detection, exploitation and patching. As an important supplement, we then review several pioneer studies that explore the potential of machine learning technologies in this field, and point out that the future development of Autonomous CRS is inseparable from machine learning.

</details>

<details>

<summary>2018-05-31 02:10:51 - Data-driven root-cause analysis for distributed system anomalies</summary>

- *Chao Liu, Kin Gwn Lore, Soumik Sarkar*

- `1605.06421v2` - [abs](http://arxiv.org/abs/1605.06421v2) - [pdf](http://arxiv.org/pdf/1605.06421v2)

> Modern distributed cyber-physical systems encounter a large variety of anomalies and in many cases, they are vulnerable to catastrophic fault propagation scenarios due to strong connectivity among the sub-systems. In this regard, root-cause analysis becomes highly intractable due to complex fault propagation mechanisms in combination with diverse operating modes. This paper presents a new data-driven framework for root-cause analysis for addressing such issues. The framework is based on a spatiotemporal feature extraction scheme for distributed cyber-physical systems built on the concept of symbolic dynamics for discovering and representing causal interactions among subsystems of a complex system. We present two approaches for root-cause analysis, namely the sequential state switching ($S^3$, based on free energy concept of a Restricted Boltzmann Machine, RBM) and artificial anomaly association ($A^3$, a multi-class classification framework using deep neural networks, DNN). Synthetic data from cases with failed pattern(s) and anomalous node are simulated to validate the proposed approaches, then compared with the performance of vector autoregressive (VAR) model-based root-cause analysis. Real dataset based on Tennessee Eastman process (TEP) is also used for validation. The results show that: (1) $S^3$ and $A^3$ approaches can obtain high accuracy in root-cause analysis and successfully handle multiple nominal operation modes, and (2) the proposed tool-chain is shown to be scalable while maintaining high accuracy.

</details>

<details>

<summary>2018-05-31 20:33:21 - PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks</summary>

- *Jan Svoboda, Jonathan Masci, Federico Monti, Michael M. Bronstein, Leonidas Guibas*

- `1806.00088v1` - [abs](http://arxiv.org/abs/1806.00088v1) - [pdf](http://arxiv.org/pdf/1806.00088v1)

> Deep learning systems have become ubiquitous in many aspects of our lives. Unfortunately, it has been shown that such systems are vulnerable to adversarial attacks, making them prone to potential unlawful uses. Designing deep neural networks that are robust to adversarial attacks is a fundamental step in making such systems safer and deployable in a broader variety of applications (e.g. autonomous driving), but more importantly is a necessary step to design novel and more advanced architectures built on new computational paradigms rather than marginally building on the existing ones. In this paper we introduce PeerNets, a novel family of convolutional networks alternating classical Euclidean convolutions with graph convolutions to harness information from a graph of peer samples. This results in a form of non-local forward propagation in the model, where latent features are conditioned on the global structure induced by the graph, that is up to 3 times more robust to a variety of white- and black-box adversarial attacks compared to conventional architectures with almost no drop in accuracy.

</details>

<details>

<summary>2018-05-31 22:29:29 - Key Management Systems for Smart Grid Advanced Metering Infrastructure: A Survey</summary>

- *Amrita Ghosal, Mauro Conti*

- `1806.00121v1` - [abs](http://arxiv.org/abs/1806.00121v1) - [pdf](http://arxiv.org/pdf/1806.00121v1)

> Smart Grids are evolving as the next generation power systems that involve changes in the traditional ways of generation, transmission and distribution of power. Advanced Metering Infrastructure (AMI) is one of the key components in smart grids. An AMI comprises of systems and networks, that collects and analyzes data received from smart meters. In addition, AMI also provides intelligent management of various power-related applications and services based on the data collected from smart meters. Thus, AMI plays a significant role in the smooth functioning of smart grids.   AMI is a privileged target for security attacks as it is made up of systems that are highly vulnerable to such attacks. Providing security to AMI is necessary as adversaries can cause potential damage against infrastructures and privacy in smart grid. One of the most effective and challenging topic's identified, is the Key Management System (KMS), for sustaining the security concerns in AMI. Therefore, KMS seeks to be a promising research area for future development of AMI. This survey work highlights the key security issues of advanced metering infrastructures and focuses on how key management techniques can be utilized for safeguarding AMI. First of all, we explore the main features of advanced metering infrastructures and identify the relationship between smart grid and AMI. Then, we introduce the security issues and challenges of AMI. We also provide a classification of the existing works in literature that deal with secure key management system in AMI. Finally, we identify possible future research directions of KMS in AMI.

</details>


## 2018-06

<details>

<summary>2018-06-02 04:13:02 - Detecting Adversarial Examples via Key-based Network</summary>

- *Pinlong Zhao, Zhouyu Fu, Ou wu, Qinghua Hu, Jun Wang*

- `1806.00580v1` - [abs](http://arxiv.org/abs/1806.00580v1) - [pdf](http://arxiv.org/pdf/1806.00580v1)

> Though deep neural networks have achieved state-of-the-art performance in visual classification, recent studies have shown that they are all vulnerable to the attack of adversarial examples. Small and often imperceptible perturbations to the input images are sufficient to fool the most powerful deep neural networks. Various defense methods have been proposed to address this issue. However, they either require knowledge on the process of generating adversarial examples, or are not robust against new attacks specifically designed to penetrate the existing defense. In this work, we introduce key-based network, a new detection-based defense mechanism to distinguish adversarial examples from normal ones based on error correcting output codes, using the binary code vectors produced by multiple binary classifiers applied to randomly chosen label-sets as signatures to match normal images and reject adversarial examples. In contrast to existing defense methods, the proposed method does not require knowledge of the process for generating adversarial examples and can be applied to defend against different types of attacks. For the practical black-box and gray-box scenarios, where the attacker does not know the encoding scheme, we show empirically that key-based network can effectively detect adversarial examples generated by several state-of-the-art attacks.

</details>

<details>

<summary>2018-06-03 00:54:30 - SgxPectre Attacks: Stealing Intel Secrets from SGX Enclaves via Speculative Execution</summary>

- *Guoxing Chen, Sanchuan Chen, Yuan Xiao, Yinqian Zhang, Zhiqiang Lin, Ten H. Lai*

- `1802.09085v3` - [abs](http://arxiv.org/abs/1802.09085v3) - [pdf](http://arxiv.org/pdf/1802.09085v3)

> This paper presents SgxPectre Attacks that exploit the recently disclosed CPU bugs to subvert the confidentiality and integrity of SGX enclaves. Particularly, we show that when branch prediction of the enclave code can be influenced by programs outside the enclave, the control flow of the enclave program can be temporarily altered to execute instructions that lead to observable cache-state changes. An adversary observing such changes can learn secrets inside the enclave memory or its internal registers, thus completely defeating the confidentiality guarantee offered by SGX. To demonstrate the practicality of our SgxPectre Attacks, we have systematically explored the possible attack vectors of branch target injection, approaches to win the race condition during enclave's speculative execution, and techniques to automatically search for code patterns required for launching the attacks. Our study suggests that any enclave program could be vulnerable to SgxPectre Attacks since the desired code patterns are available in most SGX runtimes (e.g., Intel SGX SDK, Rust-SGX, and Graphene-SGX). Most importantly, we have applied SgxPectre Attacks to steal seal keys and attestation keys from Intel signed quoting enclaves. The seal key can be used to decrypt sealed storage outside the enclaves and forge valid sealed data; the attestation key can be used to forge attestation signatures. For these reasons, SgxPectre Attacks practically defeat SGX's security protection. This paper also systematically evaluates Intel's existing countermeasures against SgxPectre Attacks and discusses the security implications.

</details>

<details>

<summary>2018-06-03 08:19:38 - Location Privacy in Cognitive Radio Networks: A Survey</summary>

- *Mohamed Grissa, Bechir Hamdaoui, Attila A. Yavuz*

- `1806.00750v1` - [abs](http://arxiv.org/abs/1806.00750v1) - [pdf](http://arxiv.org/pdf/1806.00750v1)

> Cognitive radio networks (CRNs) have emerged as an essential technology to enable dynamic and opportunistic spectrum access which aims to exploit underutilized licensed channels to solve the spectrum scarcity problem. Despite the great benefits that CRNs offer in terms of their ability to improve spectrum utilization efficiency, they suffer from user location privacy issues. Knowing that their whereabouts may be exposed can discourage users from joining and participating in the CRNs, thereby potentially hindering the adoption and deployment of this technology in future generation networks. The location information leakage issue in the CRN context has recently started to gain attention from the research community due to its importance, and several research efforts have been made to tackle it. However, to the best of our knowledge, none of these works have tried to identify the vulnerabilities that are behind this issue or discuss the approaches that could be deployed to prevent it. In this paper, we try to fill this gap by providing a comprehensive survey that investigates the various location privacy risks and threats that may arise from the different components of this CRN technology, and explores the different privacy attacks and countermeasure solutions that have been proposed in the literature to cope with this location privacy issue. We also discuss some open research problems, related to this issue, that need to be overcome by the research community to take advantage of the benefits of this key CRN technology without having to sacrifice the users' privacy.

</details>

<details>

<summary>2018-06-03 10:51:11 - Quantum-secured blockchain</summary>

- *E. O. Kiktenko, N. O. Pozhar, M. N. Anufriev, A. S. Trushechkin, R. R. Yunusov, Y. V. Kurochkin, A. I. Lvovsky, A. K. Fedorov*

- `1705.09258v3` - [abs](http://arxiv.org/abs/1705.09258v3) - [pdf](http://arxiv.org/pdf/1705.09258v3)

> Blockchain is a distributed database which is cryptographically protected against malicious modifications. While promising for a wide range of applications, current blockchain platforms rely on digital signatures, which are vulnerable to attacks by means of quantum computers. The same, albeit to a lesser extent, applies to cryptographic hash functions that are used in preparing new blocks, so parties with access to quantum computation would have unfair advantage in procuring mining rewards. Here we propose a possible solution to the quantum era blockchain challenge and report an experimental realization of a quantum-safe blockchain platform that utilizes quantum key distribution across an urban fiber network for information-theoretically secure authentication. These results address important questions about realizability and scalability of quantum-safe blockchains for commercial and governmental applications.

</details>

<details>

<summary>2018-06-03 16:46:39 - Studying Politically Vulnerable Communities Online: Ethical Dilemmas, Questions, and Solutions</summary>

- *Robert Gorwa, Philip N. Howard*

- `1806.00830v1` - [abs](http://arxiv.org/abs/1806.00830v1) - [pdf](http://arxiv.org/pdf/1806.00830v1)

> This short article introduces the concept of political vulnerability for social media researchers. How are traditional notions of harm challenged by research subjects in politically vulnerable communities? Through a selection of case studies, we explore some of the trade-offs, challenges, and questions raised by research that seeks be robust and transparent while also preserving anonymity and privacy, especially in high-stakes, politically fraught contexts.

</details>

<details>

<summary>2018-06-04 01:25:06 - How Much Are You Willing to Share? A "Poker-Styled" Selective Privacy Preserving Framework for Recommender Systems</summary>

- *Manoj Reddy Dareddy, Ariyam Das, Junghoo Cho, Carlo Zaniolo*

- `1806.00914v1` - [abs](http://arxiv.org/abs/1806.00914v1) - [pdf](http://arxiv.org/pdf/1806.00914v1)

> Most industrial recommender systems rely on the popular collaborative filtering (CF) technique for providing personalized recommendations to its users. However, the very nature of CF is adversarial to the idea of user privacy, because users need to share their preferences with others in order to be grouped with like-minded people and receive accurate recommendations. While previous privacy preserving approaches have been successful inasmuch as they concealed user preference information to some extent from a centralized recommender system, they have also, nevertheless, incurred significant trade-offs in terms of privacy, scalability, and accuracy. They are also vulnerable to privacy breaches by malicious actors. In light of these observations, we propose a novel selective privacy preserving (SP2) paradigm that allows users to custom define the scope and extent of their individual privacies, by marking their personal ratings as either public (which can be shared) or private (which are never shared and stored only on the user device). Our SP2 framework works in two steps: (i) First, it builds an initial recommendation model based on the sum of all public ratings that have been shared by users and (ii) then, this public model is fine-tuned on each user's device based on the user private ratings, thus eventually learning a more accurate model. Furthermore, in this work, we introduce three different algorithms for implementing an end-to-end SP2 framework that can scale effectively from thousands to hundreds of millions of items. Our user survey shows that an overwhelming fraction of users are likely to rate much more items to improve the overall recommendations when they can control what ratings will be publicly shared with others.

</details>

<details>

<summary>2018-06-04 22:50:47 - Mitigation of Policy Manipulation Attacks on Deep Q-Networks with Parameter-Space Noise</summary>

- *Vahid Behzadan, Arslan Munir*

- `1806.02190v1` - [abs](http://arxiv.org/abs/1806.02190v1) - [pdf](http://arxiv.org/pdf/1806.02190v1)

> Recent developments have established the vulnerability of deep reinforcement learning to policy manipulation attacks via intentionally perturbed inputs, known as adversarial examples. In this work, we propose a technique for mitigation of such attacks based on addition of noise to the parameter space of deep reinforcement learners during training. We experimentally verify the effect of parameter-space noise in reducing the transferability of adversarial examples, and demonstrate the promising performance of this technique in mitigating the impact of whitebox and blackbox attacks at both test and training times.

</details>

<details>

<summary>2018-06-06 15:44:53 - Adversarial Regression with Multiple Learners</summary>

- *Liang Tong, Sixie Yu, Scott Alfeld, Yevgeniy Vorobeychik*

- `1806.02256v1` - [abs](http://arxiv.org/abs/1806.02256v1) - [pdf](http://arxiv.org/pdf/1806.02256v1)

> Despite the considerable success enjoyed by machine learning techniques in practice, numerous studies demonstrated that many approaches are vulnerable to attacks. An important class of such attacks involves adversaries changing features at test time to cause incorrect predictions. Previous investigations of this problem pit a single learner against an adversary. However, in many situations an adversary's decision is aimed at a collection of learners, rather than specifically targeted at each independently. We study the problem of adversarial linear regression with multiple learners. We approximate the resulting game by exhibiting an upper bound on learner loss functions, and show that the resulting game has a unique symmetric equilibrium. We present an algorithm for computing this equilibrium, and show through extensive experiments that equilibrium models are significantly more robust than conventional regularized linear regression.

</details>

<details>

<summary>2018-06-06 18:23:33 - Adversarial Attack on Graph Structured Data</summary>

- *Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, Le Song*

- `1806.02371v1` - [abs](http://arxiv.org/abs/1806.02371v1) - [pdf](http://arxiv.org/pdf/1806.02371v1)

> Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool the model by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. Also, variants of genetic algorithms and gradient methods are presented in the scenario where prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.

</details>

<details>

<summary>2018-06-07 08:57:44 - AI-based Two-Stage Intrusion Detection for Software Defined IoT Networks</summary>

- *Jiaqi Li, Zhifeng Zhao, Rongpeng Li, Honggang Zhang*

- `1806.02566v1` - [abs](http://arxiv.org/abs/1806.02566v1) - [pdf](http://arxiv.org/pdf/1806.02566v1)

> Software Defined Internet of Things (SD-IoT) Networks profits from centralized management and interactive resource sharing which enhances the efficiency and scalability of IoT applications. But with the rapid growth in services and applications, it is vulnerable to possible attacks and faces severe security challenges. Intrusion detection has been widely used to ensure network security, but classical detection means are usually signature-based or explicit-behavior-based and fail to detect unknown attacks intelligently, which are hard to satisfy the requirements of SD-IoT Networks. In this paper, we propose an AI-based two-stage intrusion detection empowered by software defined technology. It flexibly captures network flows with a globle view and detects attacks intelligently through applying AI algorithms. We firstly leverage Bat algorithm with swarm division and Differential Mutation to select typical features. Then, we exploit Random forest through adaptively altering the weights of samples using weighted voting mechanism to classify flows. Evaluation results prove that the modified intelligent algorithms select more important features and achieve superior performance in flow classification. It is also verified that intelligent intrusion detection shows better accuracy with lower overhead comparied with existing solutions.

</details>

<details>

<summary>2018-06-08 12:55:29 - Cryptanalyzing an image encryption algorithm based on autoblocking and electrocardiography</summary>

- *Chengqing Li, Dongdong Lin, Jinhu Lü, Feng Hao*

- `1711.01858v3` - [abs](http://arxiv.org/abs/1711.01858v3) - [pdf](http://arxiv.org/pdf/1711.01858v3)

> This paper analyzes the security of an image encryption algorithm proposed by Ye and Huang [\textit{IEEE MultiMedia}, vol. 23, pp. 64-71, 2016]. The Ye-Huang algorithm uses electrocardiography (ECG) signals to generate the initial key for a chaotic system and applies an autoblocking method to divide a plain image into blocks of certain sizes suitable for subsequent encryption. The designers claimed that the proposed algorithm is "strong and flexible enough for practical applications". In this paper, we perform a thorough analysis of their algorithm from the view point of modern cryptography. We find it is vulnerable to the known plaintext attack: based on one pair of a known plain-image and its corresponding cipher-image, an adversary is able to derive a mask image, which can be used as an equivalent secret key to successfully decrypt other cipher-images encrypted under the same key with a non-negligible probability of 1/256. Using this as a typical counterexample, we summarize security defects in the design of the Ye-Huang algorithm. The lessons are generally applicable to many other image encryption schemes.

</details>

<details>

<summary>2018-06-08 17:26:11 - Badger: Complexity Analysis with Fuzzing and Symbolic Execution</summary>

- *Yannic Noller, Rody Kersten, Corina S. Păsăreanu*

- `1806.03283v1` - [abs](http://arxiv.org/abs/1806.03283v1) - [pdf](http://arxiv.org/pdf/1806.03283v1)

> Hybrid testing approaches that involve fuzz testing and symbolic execution have shown promising results in achieving high code coverage, uncovering subtle errors and vulnerabilities in a variety of software applications. In this paper we describe Badger - a new hybrid approach for complexity analysis, with the goal of discovering vulnerabilities which occur when the worst-case time or space complexity of an application is significantly higher than the average case. Badger uses fuzz testing to generate a diverse set of inputs that aim to increase not only coverage but also a resource-related cost associated with each path. Since fuzzing may fail to execute deep program paths due to its limited knowledge about the conditions that influence these paths, we complement the analysis with a symbolic execution, which is also customized to search for paths that increase the resource-related cost. Symbolic execution is particularly good at generating inputs that satisfy various program conditions but by itself suffers from path explosion. Therefore, Badger uses fuzzing and symbolic execution in tandem, to leverage their benefits and overcome their weaknesses. We implemented our approach for the analysis of Java programs, based on Kelinci and Symbolic PathFinder. We evaluated Badger on Java applications, showing that our approach is significantly faster in generating worst-case executions compared to fuzzing or symbolic execution on their own.

</details>

<details>

<summary>2018-06-10 18:15:04 - A Model-Based Approach to Security Analysis for Cyber-Physical Systems</summary>

- *Georgios Bakirtzis, Bryan T. Carter, Carl R. Elks, Cody H. Fleming*

- `1710.11442v3` - [abs](http://arxiv.org/abs/1710.11442v3) - [pdf](http://arxiv.org/pdf/1710.11442v3)

> Evaluating the security of cyber-physical systems throughout their life cycle is necessary to assure that they can be deployed and operated in safety-critical applications, such as infrastructure, military, and transportation. Most safety and security decisions that can have major effects on mitigation strategy options after deployment are made early in the system's life cycle. To allow for a vulnerability analysis before deployment, a sufficient well-formed model has to be constructed. To construct such a model we produce a taxonomy of attributes; that is, a generalized schema for system attributes. This schema captures the necessary specificity that characterizes a possible real system and can also map to the attack vector space associated with the model's attributes. In this way, we can match possible attack vectors and provide architectural mitigation at the design phase. We present a model of a flight control system encoded in the Systems Modeling Language, commonly known as SysML, but also show agnosticism with respect to the modeling language or tool used.

</details>

<details>

<summary>2018-06-11 03:52:07 - Potential of Augmented Reality for Intelligent Transportation Systems</summary>

- *Adnan Mahmood, Bernard Butler, Brendan Jennings*

- `1806.04724v1` - [abs](http://arxiv.org/abs/1806.04724v1) - [pdf](http://arxiv.org/pdf/1806.04724v1)

> Rapid advances in wireless communication technologies coupled with ongoing massive development in vehicular networking standards and innovations in computing, sensing, and analytics have paved the way for intelligent transportation systems (ITS) to develop rapidly in the near future. ITS provides a complete solution for the efficient and intelligent management of real-time traffic, wherein sensory data is collected from within the vehicles (i.e., via their onboard units) as well as data exchanged between the vehicles, between the vehicles and their supporting roadside infrastructure/network, among the vehicles and vulnerable pedestrians, subsequently paving the way for the realization of the futuristic Internet of Vehicles. The traditional intent of an ITS system is to detect, monitor, control, and subsequently reduce traffic congestion based on a real-time analysis of the data pertinent to certain patterns of the road traffic, including traffic density at a geographical area of interest, precise velocity of vehicles, current and predicted travelling trajectories and times, etc. However, merely relying on an ITS framework is not an optimal solution. In case of dense traffic environments, where communication broadcasts from hundreds of thousands of vehicles could potentially choke the entire network (and so could lead to fatal accidents in the case of autonomous vehicles that depend on reliable communications for their operational safety), a fall back to the traditional decentralized vehicular ad hoc network (VANET) approach becomes necessary. It is therefore of critical importance to enhance the situational awareness of vehicular drivers so as to enable them to make quick but well-founded manual decisions in such safety-critical situations.

</details>

<details>

<summary>2018-06-11 04:49:00 - Greybox fuzzing as a contextual bandits problem</summary>

- *Ketan Patil, Aditya Kanade*

- `1806.03806v1` - [abs](http://arxiv.org/abs/1806.03806v1) - [pdf](http://arxiv.org/pdf/1806.03806v1)

> Greybox fuzzing is one of the most useful and effective techniques for the bug detection in large scale application programs. It uses minimal amount of instrumentation. American Fuzzy Lop (AFL) is a popular coverage based evolutionary greybox fuzzing tool. AFL performs extremely well in fuzz testing large applications and finding critical vulnerabilities, but AFL involves a lot of heuristics while deciding the favored test case(s), skipping test cases during fuzzing, assigning fuzzing iterations to test case(s). In this work, we aim at replacing the heuristics the AFL uses while assigning the fuzzing iterations to a test case during the random fuzzing. We formalize this problem as a `contextual bandit problem' and we propose an algorithm to solve this problem. We have implemented our approach on top of the AFL. We modify the AFL's heuristics with our learned model through the policy gradient method. Our learning algorithm selects the multiplier of the number of fuzzing iterations to be assigned to a test case during random fuzzing, given a fixed length substring of the test case to be fuzzed. We fuzz the substring with this new energy value and continuously updates the policy based upon the interesting test cases it produces on fuzzing.

</details>

<details>

<summary>2018-06-12 02:16:14 - Model-Free Information Extraction in Enriched Nonlinear Phase-Space</summary>

- *Bin Li, Yueheng Lan, Weisi Guo, Chenglin Zhao*

- `1804.05170v2` - [abs](http://arxiv.org/abs/1804.05170v2) - [pdf](http://arxiv.org/pdf/1804.05170v2)

> Detecting anomalies and discovering driving signals is an essential component of scientific research and industrial practice. Often the underlying mechanism is highly complex, involving hidden evolving nonlinear dynamics and noise contamination. When representative physical models and large labeled data sets are unavailable, as is the case with most real-world applications, model-dependent Bayesian approaches would yield misleading results, and most supervised learning machines would also fail to reliably resolve the intricately evolving systems. Here, we propose an unsupervised machine-learning approach that operates in a well-constructed function space, whereby the evolving nonlinear dynamics are captured through a linear functional representation determined by the Koopman operator. This breakthrough leverages on the time-feature embedding and the ensuing reconstruction of a phase-space representation of the dynamics, thereby permitting the reliable identification of critical global signatures from the whole trajectory. This dramatically improves over commonly used static local features, which are vulnerable to unknown transitions or noise. Thanks to its data-driven nature, our method excludes any prior models and training corpus. We benchmark the astonishing accuracy of our method on three diverse and challenging problems in: biology, medicine, and engineering. In all cases, it outperforms existing state-of-the-art methods. As a new unsupervised information processing paradigm, it is suitable for ubiquitous nonlinear dynamical systems or end-users with little expertise, which permits an unbiased excavation of underlying working principles or intrinsic correlations submerged in unlabeled data flows.

</details>

<details>

<summary>2018-06-12 03:59:47 - Sound Patch Generation for Vulnerabilities</summary>

- *Zhen Huang, David Lie*

- `1711.11136v2` - [abs](http://arxiv.org/abs/1711.11136v2) - [pdf](http://arxiv.org/pdf/1711.11136v2)

> Security vulnerabilities are among the most critical software defects in existence. As such, they require patches that are correct and quickly deployed. This motivates an automatic patch generation method that emphasizes both soundness and wide applicability. To address this challenge, we propose Senx, which uses three novel patch generation techniques to create patches for out-of-bounds read/write vulnerabilities. Senx uses symbolic execution to extract expressions from the source code of a target application to synthesize patches. To reduce the runtime overhead of patches, it uses loop cloning and access range analysis to analyze loops involved in these vulnerabilities and elevate patches outside of loops. For vulnerabilities that span multiple functions, Senx uses expression translation to translate expressions and place them in a function scope where all values are available to create the patch. This enables Senx to patch vulnerabilities with complex loops and interprocedural dependencies that previous semantics-based patch generation systems cannot handle.   We have implemented a prototype using this approach. Our evaluation shows that the patches generated by Senx successfully fix 76% of 42 real-world vulnerabilities from 11 applications including various tools or libraries for manipulating graphics/media files, a programming language interpreter, a relational database engine, a collection of programming tools for creating and managing binary programs, and a collection of basic file, shell, and text manipulation tools. All patches that Senx produces are sound, and Senx correctly aborts patch generations in cases where its analysis will fall short.

</details>

<details>

<summary>2018-06-12 06:36:30 - SoK: Sanitizing for Security</summary>

- *Dokyung Song, Julian Lettner, Prabhu Rajasekaran, Yeoul Na, Stijn Volckaert, Per Larsen, Michael Franz*

- `1806.04355v1` - [abs](http://arxiv.org/abs/1806.04355v1) - [pdf](http://arxiv.org/pdf/1806.04355v1)

> The C and C++ programming languages are notoriously insecure yet remain indispensable. Developers therefore resort to a multi-pronged approach to find security issues before adversaries. These include manual, static, and dynamic program analysis. Dynamic bug finding tools --- henceforth "sanitizers" --- can find bugs that elude other types of analysis because they observe the actual execution of a program, and can therefore directly observe incorrect program behavior as it happens.   A vast number of sanitizers have been prototyped by academics and refined by practitioners. We provide a systematic overview of sanitizers with an emphasis on their role in finding security issues. Specifically, we taxonomize the available tools and the security vulnerabilities they cover, describe their performance and compatibility properties, and highlight various trade-offs.

</details>

<details>

<summary>2018-06-12 22:32:05 - ASP:A Fast Adversarial Attack Example Generation Framework based on Adversarial Saliency Prediction</summary>

- *Fuxun Yu, Qide Dong, Xiang Chen*

- `1802.05763v3` - [abs](http://arxiv.org/abs/1802.05763v3) - [pdf](http://arxiv.org/pdf/1802.05763v3)

> With the excellent accuracy and feasibility, the Neural Networks have been widely applied into the novel intelligent applications and systems. However, with the appearance of the Adversarial Attack, the NN based system performance becomes extremely vulnerable:the image classification results can be arbitrarily misled by the adversarial examples, which are crafted images with human unperceivable pixel-level perturbation. As this raised a significant system security issue, we implemented a series of investigations on the adversarial attack in this work: We first identify an image's pixel vulnerability to the adversarial attack based on the adversarial saliency analysis. By comparing the analyzed saliency map and the adversarial perturbation distribution, we proposed a new evaluation scheme to comprehensively assess the adversarial attack precision and efficiency. Then, with a novel adversarial saliency prediction method, a fast adversarial example generation framework, namely "ASP", is proposed with significant attack efficiency improvement and dramatic computation cost reduction. Compared to the previous methods, experiments show that ASP has at most 12 times speed-up for adversarial example generation, 2 times lower perturbation rate, and high attack success rate of 87% on both MNIST and Cifar10. ASP can be also well utilized to support the data-hungry NN adversarial training. By reducing the attack success rate as much as 90%, ASP can quickly and effectively enhance the defense capability of NN based system to the adversarial attacks.

</details>

<details>

<summary>2018-06-13 05:42:31 - SybilBlind: Detecting Fake Users in Online Social Networks without Manual Labels</summary>

- *Binghui Wang, Le Zhang, Neil Zhenqiang Gong*

- `1806.04853v1` - [abs](http://arxiv.org/abs/1806.04853v1) - [pdf](http://arxiv.org/pdf/1806.04853v1)

> Detecting fake users (also called Sybils) in online social networks is a basic security research problem. State-of-the-art approaches rely on a large amount of manually labeled users as a training set. These approaches suffer from three key limitations: 1) it is time-consuming and costly to manually label a large training set, 2) they cannot detect new Sybils in a timely fashion, and 3) they are vulnerable to Sybil attacks that leverage information of the training set. In this work, we propose SybilBlind, a structure-based Sybil detection framework that does not rely on a manually labeled training set. SybilBlind works under the same threat model as state-of-the-art structure-based methods. We demonstrate the effectiveness of SybilBlind using 1) a social network with synthetic Sybils and 2) two Twitter datasets with real Sybils. For instance, SybilBlind achieves an AUC of 0.98 on a Twitter dataset.

</details>

<details>

<summary>2018-06-14 02:00:28 - o-glasses: Visualizing x86 Code from Binary Using a 1d-CNN</summary>

- *Yuhei Otsubo, Akira Otsuka, Mamoru Mimura, Takeshi Sakaki, Atsuhiro Goto*

- `1806.05328v1` - [abs](http://arxiv.org/abs/1806.05328v1) - [pdf](http://arxiv.org/pdf/1806.05328v1)

> Malicious document files used in targeted attacks often contain a small program called shellcode. It is often hard to prepare a runnable environment for dynamic analysis of these document files because they exploit specific vulnerabilities. In these cases, it is necessary to identify the position of the shellcode in each document file to analyze it. If the exploit code uses executable scripts such as JavaScript and Flash, it is not so hard to locate the shellcode. On the other hand, it is sometimes almost impossible to locate the shellcode when it does not contain any JavaScript or Flash but consists of native x86 code only.   Binary fragment classification is often applied to visualize the location of regions of interest, and shellcode must contain at least a small fragment of x86 native code even if most of it is obfuscated, such as, a decoder for the obfuscated body of the shellcode. In this paper, we propose a novel method, o-glasses, to visualize the shellcode by recognizing the x86 native code using a specially designed one-dimensional convolutional neural network (1d-CNN). The fragment size needs to be as small as the minimum size of the x86 native code in the whole shellcode. Our results show that a 16-instruction-sequence (approximately 48 bytes on average) is sufficient for the code fragment visualization. Our method, o-glasses (1d-CNN), outperforms other methods in that it recognizes x86 native code with a surprisingly high F-measure rate (about 99.95%).

</details>

<details>

<summary>2018-06-14 08:52:18 - On the Resilience of RTL NN Accelerators: Fault Characterization and Mitigation</summary>

- *Behzad Salami, Osman Unsal, Adrian Cristal*

- `1806.09679v1` - [abs](http://arxiv.org/abs/1806.09679v1) - [pdf](http://arxiv.org/pdf/1806.09679v1)

> Machine Learning (ML) is making a strong resurgence in tune with the massive generation of unstructured data which in turn requires massive computational resources. Due to the inherently compute- and power-intensive structure of Neural Networks (NNs), hardware accelerators emerge as a promising solution. However, with technology node scaling below 10nm, hardware accelerators become more susceptible to faults, which in turn can impact the NN accuracy. In this paper, we study the resilience aspects of Register-Transfer Level (RTL) model of NN accelerators, in particular, fault characterization and mitigation. By following a High-Level Synthesis (HLS) approach, first, we characterize the vulnerability of various components of RTL NN. We observed that the severity of faults depends on both i) application-level specifications, i.e., NN data (inputs, weights, or intermediate), NN layers, and NN activation functions, and ii) architectural-level specifications, i.e., data representation model and the parallelism degree of the underlying accelerator. Second, motivated by characterization results, we present a low-overhead fault mitigation technique that can efficiently correct bit flips, by 47.3% better than state-of-the-art methods.

</details>

<details>

<summary>2018-06-16 05:13:56 - EdgeChain: An Edge-IoT Framework and Prototype Based on Blockchain and Smart Contracts</summary>

- *Jianli Pan, Jianyu Wang, Austin Hester, Ismail Alqerm, Yuanni Liu, Ying Zhao*

- `1806.06185v1` - [abs](http://arxiv.org/abs/1806.06185v1) - [pdf](http://arxiv.org/pdf/1806.06185v1)

> The emerging Internet of Things (IoT) is facing significant scalability and security challenges. On the one hand, IoT devices are "weak" and need external assistance. Edge computing provides a promising direction addressing the deficiency of centralized cloud computing in scaling massive number of devices. On the other hand, IoT devices are also relatively "vulnerable" facing malicious hackers due to resource constraints. The emerging blockchain and smart contracts technologies bring a series of new security features for IoT and edge computing. In this paper, to address the challenges, we design and prototype an edge-IoT framework named "EdgeChain" based on blockchain and smart contracts. The core idea is to integrate a permissioned blockchain and the internal currency or "coin" system to link the edge cloud resource pool with each IoT device' account and resource usage, and hence behavior of the IoT devices. EdgeChain uses a credit-based resource management system to control how much resource IoT devices can obtain from edge servers, based on pre-defined rules on priority, application types and past behaviors. Smart contracts are used to enforce the rules and policies to regulate the IoT device behavior in a non-deniable and automated manner. All the IoT activities and transactions are recorded into blockchain for secure data logging and auditing. We implement an EdgeChain prototype and conduct extensive experiments to evaluate the ideas. The results show that while gaining the security benefits of blockchain and smart contracts, the cost of integrating them into EdgeChain is within a reasonable and acceptable range.

</details>

<details>

<summary>2018-06-17 23:56:58 - Breaking Transferability of Adversarial Samples with Randomness</summary>

- *Yan Zhou, Murat Kantarcioglu, Bowei Xi*

- `1805.04613v2` - [abs](http://arxiv.org/abs/1805.04613v2) - [pdf](http://arxiv.org/pdf/1805.04613v2)

> We investigate the role of transferability of adversarial attacks in the observed vulnerabilities of Deep Neural Networks (DNNs). We demonstrate that introducing randomness to the DNN models is sufficient to defeat adversarial attacks, given that the adversary does not have an unlimited attack budget. Instead of making one specific DNN model robust to perfect knowledge attacks (a.k.a, white box attacks), creating randomness within an army of DNNs completely eliminates the possibility of perfect knowledge acquisition, resulting in a significantly more robust DNN ensemble against the strongest form of attacks. We also show that when the adversary has an unlimited budget of data perturbation, all defensive techniques would eventually break down as the budget increases. Therefore, it is important to understand the game saddle point where the adversary would not further pursue this endeavor.   Furthermore, we explore the relationship between attack severity and decision boundary robustness in the version space. We empirically demonstrate that by simply adding a small Gaussian random noise to the learned weights, a DNN model can increase its resilience to adversarial attacks by as much as 74.2%. More importantly, we show that by randomly activating/revealing a model from a pool of pre-trained DNNs at each query request, we can put a tremendous strain on the adversary's attack strategies. We compare our randomization techniques to the Ensemble Adversarial Training technique and show that our randomization techniques are superior under different attack budget constraints.

</details>

<details>

<summary>2018-06-18 17:54:01 - Early Stage Malware Prediction Using Recurrent Neural Networks</summary>

- *Matilda Rhode, Pete Burnap, Kevin Jones*

- `1708.03513v3` - [abs](http://arxiv.org/abs/1708.03513v3) - [pdf](http://arxiv.org/pdf/1708.03513v3)

> Static malware analysis is well-suited to endpoint anti-virus systems as it can be conducted quickly by examining the features of an executable piece of code and matching it to previously observed malicious code. However, static code analysis can be vulnerable to code obfuscation techniques. Behavioural data collected during file execution is more difficult to obfuscate, but takes a relatively long time to capture - typically up to 5 minutes, meaning the malicious payload has likely already been delivered by the time it is detected.   In this paper we investigate the possibility of predicting whether or not an executable is malicious based on a short snapshot of behavioural data. We find that an ensemble of recurrent neural networks are able to predict whether an executable is malicious or benign within the first 5 seconds of execution with 94% accuracy. This is the first time general types of malicious file have been predicted to be malicious during execution rather than using a complete activity log file post-execution, and enables cyber security endpoint protection to be advanced to use behavioural data for blocking malicious payloads rather than detecting them post-execution and having to repair the damage.

</details>

<details>

<summary>2018-06-18 19:57:04 - Security Awareness and Affective Feedback: Categorical Behaviour vs. Reported Behaviour</summary>

- *Lynsay A. Shepherd, Jacqueline Archibald*

- `1806.06905v1` - [abs](http://arxiv.org/abs/1806.06905v1) - [pdf](http://arxiv.org/pdf/1806.06905v1)

> A lack of awareness surrounding secure online behaviour can lead to end-users, and their personal details becoming vulnerable to compromise. This paper describes an ongoing research project in the field of usable security, examining the relationship between end-user-security behaviour, and the use of affective feedback to educate end-users. Part of the aforementioned research project considers the link between categorical information users reveal about themselves online, and the information users believe, or report that they have revealed online. The experimental results confirm a disparity between information revealed, and what users think they have revealed, highlighting a deficit in security awareness. Results gained in relation to the affective feedback delivered are mixed, indicating limited short-term impact. Future work seeks to perform a long-term study, with the view that positive behavioural changes may be reflected in the results as end-users become more knowledgeable about security awareness.

</details>

<details>

<summary>2018-06-19 02:00:32 - RF-PUF: Enhancing IoT Security through Authentication of Wireless Nodes using In-situ Machine Learning</summary>

- *Baibhab Chatterjee, Debayan Das, Shovan Maity, Shreyas Sen*

- `1805.01374v3` - [abs](http://arxiv.org/abs/1805.01374v3) - [pdf](http://arxiv.org/pdf/1805.01374v3)

> Traditional authentication in radio-frequency (RF) systems enable secure data communication within a network through techniques such as digital signatures and hash-based message authentication codes (HMAC), which suffer from key recovery attacks. State-of-the-art IoT networks such as Nest also use Open Authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery forgery (CSRF), which shows that these techniques may not prevent an adversary from copying or modeling the secret IDs or encryption keys using invasive, side channel, learning or software attacks. Physical unclonable functions (PUF), on the other hand, can exploit manufacturing process variations to uniquely identify silicon chips which makes a PUF-based system extremely robust and secure at low cost, as it is practically impossible to replicate the same silicon characteristics across dies. Taking inspiration from human communication, which utilizes inherent variations in the voice signatures to identify a certain speaker, we present RF- PUF: a deep neural network-based framework that allows real-time authentication of wireless nodes, using the effects of inherent process variation on RF properties of the wireless transmitters (Tx), detected through in-situ machine learning at the receiver (Rx) end. The proposed method utilizes the already-existing asymmetric RF communication framework and does not require any additional circuitry for PUF generation or feature extraction. Simulation results involving the process variations in a standard 65 nm technology node, and features such as LO offset and I-Q imbalance detected with a neural network having 50 neurons in the hidden layer indicate that the framework can distinguish up to 4800 transmitters with an accuracy of 99.9% (~ 99% for 10,000 transmitters) under varying channel conditions, and without the need for traditional preambles.

</details>

<details>

<summary>2018-06-19 07:16:51 - Sparsity-based Defense against Adversarial Attacks on Linear Classifiers</summary>

- *Zhinus Marzi, Soorya Gopalakrishnan, Upamanyu Madhow, Ramtin Pedarsani*

- `1801.04695v3` - [abs](http://arxiv.org/abs/1801.04695v3) - [pdf](http://arxiv.org/pdf/1801.04695v3)

> Deep neural networks represent the state of the art in machine learning in a growing number of fields, including vision, speech and natural language processing. However, recent work raises important questions about the robustness of such architectures, by showing that it is possible to induce classification errors through tiny, almost imperceptible, perturbations. Vulnerability to such "adversarial attacks", or "adversarial examples", has been conjectured to be due to the excessive linearity of deep networks. In this paper, we study this phenomenon in the setting of a linear classifier, and show that it is possible to exploit sparsity in natural data to combat $\ell_{\infty}$-bounded adversarial perturbations. Specifically, we demonstrate the efficacy of a sparsifying front end via an ensemble averaged analysis, and experimental results for the MNIST handwritten digit database. To the best of our knowledge, this is the first work to show that sparsity provides a theoretically rigorous framework for defense against adversarial attacks.

</details>

<details>

<summary>2018-06-20 13:42:37 - Combinatorial Testing for Deep Learning Systems</summary>

- *Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu, Jianjun Zhao, Yadong Wang*

- `1806.07723v1` - [abs](http://arxiv.org/abs/1806.07723v1) - [pdf](http://arxiv.org/pdf/1806.07723v1)

> Deep learning (DL) has achieved remarkable progress over the past decade and been widely applied to many safety-critical applications. However, the robustness of DL systems recently receives great concerns, such as adversarial examples against computer vision systems, which could potentially result in severe consequences. Adopting testing techniques could help to evaluate the robustness of a DL system and therefore detect vulnerabilities at an early stage. The main challenge of testing such systems is that its runtime state space is too large: if we view each neuron as a runtime state for DL, then a DL system often contains massive states, rendering testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to reduce the testing space while obtaining relatively high defect detection abilities. In this paper, we perform an exploratory study of CT on DL systems. We adapt the concept in CT and propose a set of coverage criteria for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems. We further pose several open questions and interesting directions for combinatorial testing of DL systems.

</details>

<details>

<summary>2018-06-21 03:12:43 - Injected and Delivered: Fabricating Implicit Control over Actuation Systems by Spoofing Inertial Sensors</summary>

- *Yazhou Tu, Zhiqiang Lin, Insup Lee, Xiali Hei*

- `1806.07558v2` - [abs](http://arxiv.org/abs/1806.07558v2) - [pdf](http://arxiv.org/pdf/1806.07558v2)

> Inertial sensors provide crucial feedback for control systems to determine motional status and make timely, automated decisions. Prior efforts tried to control the output of inertial sensors with acoustic signals. However, their approaches did not consider sample rate drifts in analog-to-digital converters as well as many other realistic factors. As a result, few attacks demonstrated effective control over inertial sensors embedded in real systems.   This work studies the out-of-band signal injection methods to deliver adversarial control to embedded MEMS inertial sensors and evaluates consequent vulnerabilities exposed in control systems relying on them. Acoustic signals injected into inertial sensors are out-of-band analog signals. Consequently, slight sample rate drifts could be amplified and cause deviations in the frequency of digital signals. Such deviations result in fluctuating sensor output; nevertheless, we characterize two methods to control the output: digital amplitude adjusting and phase pacing. Based on our analysis, we devise non-invasive attacks to manipulate the sensor output as well as the derived inertial information to deceive control systems. We test 25 devices equipped with MEMS inertial sensors and find that 17 of them could be implicitly controlled by our attacks. Furthermore, we investigate the generalizability of our methods and show the possibility to manipulate the digital output through signals with relatively low frequencies in the sensing channel.

</details>

<details>

<summary>2018-06-22 02:47:53 - DRACO: Byzantine-resilient Distributed Training via Redundant Gradients</summary>

- *Lingjiao Chen, Hongyi Wang, Zachary Charles, Dimitris Papailiopoulos*

- `1803.09877v4` - [abs](http://arxiv.org/abs/1803.09877v4) - [pdf](http://arxiv.org/pdf/1803.09877v4)

> Distributed model training is vulnerable to byzantine system failures and adversarial compute nodes, i.e., nodes that use malicious updates to corrupt the global model stored at a parameter server (PS). To guarantee some form of robustness, recent work suggests using variants of the geometric median as an aggregation rule, in place of gradient averaging. Unfortunately, median-based rules can incur a prohibitive computational overhead in large-scale settings, and their convergence guarantees often require strong assumptions. In this work, we present DRACO, a scalable framework for robust distributed training that uses ideas from coding theory. In DRACO, each compute node evaluates redundant gradients that are used by the parameter server to eliminate the effects of adversarial updates. DRACO comes with problem-independent robustness guarantees, and the model that it trains is identical to the one trained in the adversary-free setup. We provide extensive experiments on real datasets and distributed setups across a variety of large-scale models, where we show that DRACO is several times, to orders of magnitude faster than median-based approaches.

</details>

<details>

<summary>2018-06-23 10:12:38 - A Recursive PLS (Partial Least Squares) based Approach for Enterprise Threat Management</summary>

- *Janardan Misra*

- `1806.08941v1` - [abs](http://arxiv.org/abs/1806.08941v1) - [pdf](http://arxiv.org/pdf/1806.08941v1)

> Most of the existing solutions to enterprise threat management are preventive approaches prescribing means to prevent policy violations with varying degrees of success. In this paper we consider the complementary scenario where a number of security violations have already occurred, or security threats, or vulnerabilities have been reported and a security administrator needs to generate optimal response to these security events. We present a principled approach to study and model the human expertise in responding to the emergent threats owing to these security events. A recursive Partial Least Squares based adaptive learning model is defined using a factorial analysis of the security events together with a method for estimating the effect of global context dependent semantic information used by the security administrators. Presented model is theoretically optimal and operationally recursive in nature to deal with the set of security events being generated continuously. We discuss the underlying challenges and ways in which the model could be operationalized in centralized versus decentralized, and real-time versus batch processing modes.

</details>

<details>

<summary>2018-06-23 20:08:56 - On Adversarial Examples for Character-Level Neural Machine Translation</summary>

- *Javid Ebrahimi, Daniel Lowd, Dejing Dou*

- `1806.09030v1` - [abs](http://arxiv.org/abs/1806.09030v1) - [pdf](http://arxiv.org/pdf/1806.09030v1)

> Evaluating on adversarial examples has become a standard procedure to measure robustness of deep learning models. Due to the difficulty of creating white-box adversarial examples for discrete text input, most analyses of the robustness of NLP models have been done through black-box adversarial examples. We investigate adversarial examples for character-level neural machine translation (NMT), and contrast black-box adversaries with a novel white-box adversary, which employs differentiable string-edit operations to rank adversarial changes. We propose two novel types of attacks which aim to remove or change a word in a translation, rather than simply break the NMT. We demonstrate that white-box adversarial examples are significantly stronger than their black-box counterparts in different attack scenarios, which show more serious vulnerabilities than previously known. In addition, after performing adversarial training, which takes only 3 times longer than regular training, we can improve the model's robustness significantly.

</details>

<details>

<summary>2018-06-23 20:46:06 - Defending Malware Classification Networks Against Adversarial Perturbations with Non-Negative Weight Restrictions</summary>

- *Alex Kouzemtchenko*

- `1806.09035v1` - [abs](http://arxiv.org/abs/1806.09035v1) - [pdf](http://arxiv.org/pdf/1806.09035v1)

> There is a growing body of literature showing that deep neural networks are vulnerable to adversarial input modification. Recently this work has been extended from image classification to malware classification over boolean features. In this paper we present several new methods for training restricted networks in this specific domain that are highly effective at preventing adversarial perturbations. We start with a fully adversarially resistant neural network that has hard non-negative weight restrictions and is equivalent to learning a monotonic boolean function and then attempt to relax the constraints to improve classifier accuracy.

</details>

<details>

<summary>2018-06-24 09:18:34 - WPSE: Fortifying Web Protocols via Browser-Side Security Monitoring</summary>

- *Stefano Calzavara, Riccardo Focardi, Matteo Maffei, Clara Schneidewind, Marco Squarcina, Mauro Tempesta*

- `1806.09111v1` - [abs](http://arxiv.org/abs/1806.09111v1) - [pdf](http://arxiv.org/pdf/1806.09111v1)

> We present WPSE, a browser-side security monitor for web protocols designed to ensure compliance with the intended protocol flow, as well as confidentiality and integrity properties of messages. We formally prove that WPSE is expressive enough to protect web applications from a wide range of protocol implementation bugs and web attacks. We discuss concrete examples of attacks which can be prevented by WPSE on OAuth 2.0 and SAML 2.0, including a novel attack on the Google implementation of SAML 2.0 which we discovered by formalizing the protocol specification in WPSE. Moreover, we use WPSE to carry out an extensive experimental evaluation of OAuth 2.0 in the wild. Out of 90 tested websites, we identify security flaws in 55 websites (61.1%), including new critical vulnerabilities introduced by tracking libraries such as Facebook Pixel, all of which fixable by WPSE. Finally, we show that WPSE works flawlessly on 83 websites (92.2%), with the 7 compatibility issues being caused by custom implementations deviating from the OAuth 2.0 specification, one of which introducing a critical vulnerability.

</details>

<details>

<summary>2018-06-25 09:15:11 - SAQL: A Stream-based Query System for Real-Time Abnormal System Behavior Detection</summary>

- *Peng Gao, Xusheng Xiao, Ding Li, Zhichun Li, Kangkook Jee, Zhenyu Wu, Chung Hwan Kim, Sanjeev R. Kulkarni, Prateek Mittal*

- `1806.09339v1` - [abs](http://arxiv.org/abs/1806.09339v1) - [pdf](http://arxiv.org/pdf/1806.09339v1)

> Recently, advanced cyber attacks, which consist of a sequence of steps that involve many vulnerabilities and hosts, compromise the security of many well-protected businesses. This has led to the solutions that ubiquitously monitor system activities in each host (big data) as a series of events, and search for anomalies (abnormal behaviors) for triaging risky events. Since fighting against these attacks is a time-critical mission to prevent further damage, these solutions face challenges in incorporating expert knowledge to perform timely anomaly detection over the large-scale provenance data.   To address these challenges, we propose a novel stream-based query system that takes as input, a real-time event feed aggregated from multiple hosts in an enterprise, and provides an anomaly query engine that queries the event feed to identify abnormal behaviors based on the specified anomalies. To facilitate the task of expressing anomalies based on expert knowledge, our system provides a domain-specific query language, SAQL, which allows analysts to express models for (1) rule-based anomalies, (2) time-series anomalies, (3) invariant-based anomalies, and (4) outlier-based anomalies. We deployed our system in NEC Labs America comprising 150 hosts and evaluated it using 1.1TB of real system monitoring data (containing 3.3 billion events). Our evaluations on a broad set of attack behaviors and micro-benchmarks show that our system has a low detection latency (<2s) and a high system throughput (110,000 events/s; supporting ~4000 hosts), and is more efficient in memory utilization than the existing stream-based complex event processing systems.

</details>

<details>

<summary>2018-06-25 14:45:21 - A Leak-Resilient Dual Stack Scheme for Backward-Edge Control-Flow Integrity</summary>

- *Philipp Zieris, Julian Horsch*

- `1806.09496v1` - [abs](http://arxiv.org/abs/1806.09496v1) - [pdf](http://arxiv.org/pdf/1806.09496v1)

> Manipulations of return addresses on the stack are the basis for a variety of attacks on programs written in memory unsafe languages. Dual stack schemes for protecting return addresses promise an efficient and effective defense against such attacks. By introducing a second, safe stack to separate return addresses from potentially unsafe stack objects, they prevent attacks that, for example, maliciously modify a return address by overflowing a buffer. However, the security of dual stacks is based on the concealment of the safe stack in memory. Unfortunately, all current dual stack schemes are vulnerable to information disclosure attacks that are able to reveal the safe stack location, and therefore effectively break their promised security properties. In this paper, we present a new, leak-resilient dual stack scheme capable of withstanding sophisticated information disclosure attacks. We carefully study previous dual stack schemes and systematically develop a novel design for stack separation that eliminates flaws leading to the disclosure of safe stacks. We show the feasibility and practicality of our approach by presenting a full integration into the LLVM compiler framework with support for the x86-64 and ARM64 architectures. With an average of 2.7% on x86-64 and 0.0% on ARM64, the performance overhead of our implementation is negligible.

</details>

<details>

<summary>2018-06-27 20:20:33 - Discovering Flaws in Security-Focused Static Analysis Tools for Android using Systematic Mutation</summary>

- *Richard Bonett, Kaushal Kafle, Kevin Moran, Adwait Nadkarni, Denys Poshyvanyk*

- `1806.09761v2` - [abs](http://arxiv.org/abs/1806.09761v2) - [pdf](http://arxiv.org/pdf/1806.09761v2)

> Mobile application security has been one of the major areas of security research in the last decade. Numerous application analysis tools have been proposed in response to malicious, curious, or vulnerable apps. However, existing tools, and specifically, static analysis tools, trade soundness of the analysis for precision and performance, and are hence soundy. Unfortunately, the specific unsound choices or flaws in the design of these tools are often not known or well-documented, leading to a misplaced confidence among researchers, developers, and users. This paper proposes the Mutation-based soundness evaluation ($\mu$SE) framework, which systematically evaluates Android static analysis tools to discover, document, and fix, flaws, by leveraging the well-founded practice of mutation analysis. We implement $\mu$SE as a semi-automated framework, and apply it to a set of prominent Android static analysis tools that detect private data leaks in apps. As the result of an in-depth analysis of one of the major tools, we discover 13 undocumented flaws. More importantly, we discover that all 13 flaws propagate to tools that inherit the flawed tool. We successfully fix one of the flaws in cooperation with the tool developers. Our results motivate the urgent need for systematic discovery and documentation of unsound choices in soundy tools, and demonstrate the opportunities in leveraging mutation testing in achieving this goal.

</details>

<details>

<summary>2018-06-27 22:47:37 - Gradient Similarity: An Explainable Approach to Detect Adversarial Attacks against Deep Learning</summary>

- *Jasjeet Dhaliwal, Saurabh Shintre*

- `1806.10707v1` - [abs](http://arxiv.org/abs/1806.10707v1) - [pdf](http://arxiv.org/pdf/1806.10707v1)

> Deep neural networks are susceptible to small-but-specific adversarial perturbations capable of deceiving the network. This vulnerability can lead to potentially harmful consequences in security-critical applications. To address this vulnerability, we propose a novel metric called \emph{Gradient Similarity} that allows us to capture the influence of training data on test inputs. We show that \emph{Gradient Similarity} behaves differently for normal and adversarial inputs, and enables us to detect a variety of adversarial attacks with a near perfect ROC-AUC of 95-100\%. Even white-box adversaries equipped with perfect knowledge of the system cannot bypass our detector easily. On the MNIST dataset, white-box attacks are either detected with a high ROC-AUC of 87-96\%, or require very high distortion to bypass our detector.

</details>

<details>

<summary>2018-06-28 02:28:18 - Robust Neural Malware Detection Models for Emulation Sequence Learning</summary>

- *Rakshit Agrawal, Jack W. Stokes, Mady Marinescu, Karthik Selvaraj*

- `1806.10741v1` - [abs](http://arxiv.org/abs/1806.10741v1) - [pdf](http://arxiv.org/pdf/1806.10741v1)

> Malicious software, or malware, presents a continuously evolving challenge in computer security. These embedded snippets of code in the form of malicious files or hidden within legitimate files cause a major risk to systems with their ability to run malicious command sequences. Malware authors even use polymorphism to reorder these commands and create several malicious variations. However, if executed in a secure environment, one can perform early malware detection on emulated command sequences.   The models presented in this paper leverage this sequential data derived via emulation in order to perform Neural Malware Detection. These models target the core of the malicious operation by learning the presence and pattern of co-occurrence of malicious event actions from within these sequences. Our models can capture entire event sequences and be trained directly using the known target labels. These end-to-end learning models are powered by two commonly used structures - Long Short-Term Memory (LSTM) Networks and Convolutional Neural Networks (CNNs). Previously proposed sequential malware classification models process no more than 200 events. Attackers can evade detection by delaying any malicious activity beyond the beginning of the file. We present specialized models that can handle extremely long sequences while successfully performing malware detection in an efficient way. We present an implementation of the Convoluted Partitioning of Long Sequences approach in order to tackle this vulnerability and operate on long sequences. We present our results on a large dataset consisting of 634,249 file sequences, with extremely long file sequences.

</details>

<details>

<summary>2018-06-28 03:35:09 - Robust Fuzzy-Learning For Partially Overlapping Channels Allocation In UAV Communication Networks</summary>

- *Chaoqiong Fan, Bin Li, Jia Hou, Yi Wu, Weisi Guo, Chenglin Zhao*

- `1806.10756v1` - [abs](http://arxiv.org/abs/1806.10756v1) - [pdf](http://arxiv.org/pdf/1806.10756v1)

> In this paper, we consider a mesh-structured unmanned aerial vehicle (UAV) networks exploiting partially overlapping channels (POCs). For general data-collection tasks in UAV networks, we aim to optimize the network throughput with constraints on transmission power and quality of service (QoS). As far as the highly mobile and constantly changing UAV networks are concerned, unfortunately, most existing methods rely on definite information which is vulnerable to the dynamic environment, rendering system performance to be less effective. In order to combat dynamic topology and varying interference of UAV networks, a robust and distributed learning scheme is proposed. Rather than the perfect channel state information (CSI), we introduce uncertainties to characterize the dynamic channel gains among UAV nodes, which are then interpreted with fuzzy numbers. Instead of the traditional observation space where the channel capacity is a crisp reward, we implement the learning and decision process in a mapped fuzzy space. This allows the system to achieve a smoother and more robust performance by optimizing in an alternate space. To this end, we design a fuzzy payoffs function (FPF) to describe the fluctuated utility, and the problem of POCs assignment is formulated as a fuzzy payoffs game (FPG). Assisted by an attractive property of fuzzy bi-matrix games, the existence of fuzzy Nash equilibrium (FNE) for our formulated FPG is proved. Our robust fuzzy-learning algorithm could reach the equilibrium solution via a least-deviation method. Finally, numerical simulations are provided to demonstrate the advantages of our new scheme over the existing scheme.

</details>

<details>

<summary>2018-06-29 06:06:15 - Understanding and Mitigating the Security Risks of Voice-Controlled Third-Party Skills on Amazon Alexa and Google Home</summary>

- *Nan Zhang, Xianghang Mi, Xuan Feng, XiaoFeng Wang, Yuan Tian, Feng Qian*

- `1805.01525v2` - [abs](http://arxiv.org/abs/1805.01525v2) - [pdf](http://arxiv.org/pdf/1805.01525v2)

> Virtual personal assistants (VPA) (e.g., Amazon Alexa and Google Assistant) today mostly rely on the voice channel to communicate with their users, which however is known to be vulnerable, lacking proper authentication. The rapid growth of VPA skill markets opens a new attack avenue, potentially allowing a remote adversary to publish attack skills to attack a large number of VPA users through popular IoT devices such as Amazon Echo and Google Home. In this paper, we report a study that concludes such remote, large-scale attacks are indeed realistic. More specifically, we implemented two new attacks: voice squatting in which the adversary exploits the way a skill is invoked (e.g., "open capital one"), using a malicious skill with similarly pronounced name (e.g., "capital won") or paraphrased name (e.g., "capital one please") to hijack the voice command meant for a different skill, and voice masquerading in which a malicious skill impersonates the VPA service or a legitimate skill to steal the user's data or eavesdrop on her conversations. These attacks aim at the way VPAs work or the user's mis-conceptions about their functionalities, and are found to pose a realistic threat by our experiments (including user studies and real-world deployments) on Amazon Echo and Google Home. The significance of our findings have already been acknowledged by Amazon and Google, and further evidenced by the risky skills discovered on Alexa and Google markets by the new detection systems we built. We further developed techniques for automatic detection of these attacks, which already capture real-world skills likely to pose such threats.

</details>


## 2018-07

<details>

<summary>2018-07-01 15:08:52 - Towards Adversarial Training with Moderate Performance Improvement for Neural Network Classification</summary>

- *Xinhan Di, Pengqian Yu, Meng Tian*

- `1807.00340v1` - [abs](http://arxiv.org/abs/1807.00340v1) - [pdf](http://arxiv.org/pdf/1807.00340v1)

> It has been demonstrated that deep neural networks are prone to noisy examples particular adversarial samples during inference process. The gap between robust deep learning systems in real world applications and vulnerable neural networks is still large. Current adversarial training strategies improve the robustness against adversarial samples. However, these methods lead to accuracy reduction when the input examples are clean thus hinders the practicability. In this paper, we investigate an approach that protects the neural network classification from the adversarial samples and improves its accuracy when the input examples are clean. We demonstrate the versatility and effectiveness of our proposed approach on a variety of different networks and datasets.

</details>

<details>

<summary>2018-07-02 02:20:45 - Intrusion Detection Systems for Networked Unmanned Aerial Vehicles: A Survey</summary>

- *Gaurav Choudhary, Vishal Sharma, Ilsun You, Kangbin Yim, Ing-Ray Chen, Jin-Hee Cho*

- `1807.00435v1` - [abs](http://arxiv.org/abs/1807.00435v1) - [pdf](http://arxiv.org/pdf/1807.00435v1)

> Unmanned Aerial Vehicles (UAV)-based civilian or military applications become more critical to serving civilian and/or military missions. The significantly increased attention on UAV applications also has led to security concerns particularly in the context of networked UAVs. Networked UAVs are vulnerable to malicious attacks over open-air radio space and accordingly, intrusion detection systems (IDSs) have been naturally derived to deal with the vulnerabilities and/or attacks. In this paper, we briefly survey the state-of-the-art IDS mechanisms that deal with vulnerabilities and attacks under networked UAV environments. In particular, we classify the existing IDS mechanisms according to information gathering sources, deployment strategies, detection methods, detection states, IDS acknowledgment, and intrusion types. We conclude this paper with research challenges, insights, and future research directions to propose a networked UAV-IDS system which meets required standards of effectiveness and efficiency in terms of the goals of both security and performance.

</details>

<details>

<summary>2018-07-03 08:13:51 - Cooperative Tracking of Cyclists Based on Smart Devices and Infrastructure</summary>

- *Günther Reitberger, Stefan Zernetsch, Maarten Bieshaar, Bernhard Sick, Konrad Doll, Erich Fuchs*

- `1803.02096v2` - [abs](http://arxiv.org/abs/1803.02096v2) - [pdf](http://arxiv.org/pdf/1803.02096v2)

> In future traffic scenarios, vehicles and other traffic participants will be interconnected and equipped with various types of sensors, allowing for cooperation based on data or information exchange. This article presents an approach to cooperative tracking of cyclists using smart devices and infrastructure-based sensors. A smart device is carried by the cyclists and an intersection is equipped with a wide angle stereo camera system. Two tracking models are presented and compared. The first model is based on the stereo camera system detections only, whereas the second model cooperatively combines the camera based detections with velocity and yaw rate data provided by the smart device. Our aim is to overcome limitations of tracking approaches based on single data sources. We show in numerical evaluations on scenes where cyclists are starting or turning right that the cooperation leads to an improvement in both the ability to keep track of a cyclist and the accuracy of the track particularly when it comes to occlusions in the visual system. We, therefore, contribute to the safety of vulnerable road users in future traffic.

</details>

<details>

<summary>2018-07-03 11:46:17 - Usability and Security Effects of Code Examples on Crypto APIs - CryptoExamples: A platform for free, minimal, complete and secure crypto examples</summary>

- *Kai Mindermann, Stefan Wagner*

- `1807.01095v1` - [abs](http://arxiv.org/abs/1807.01095v1) - [pdf](http://arxiv.org/pdf/1807.01095v1)

> Context: Cryptographic APIs are said to be not usable and researchers suggest to add example code to the documentation. Aim: We wanted to create a free platform for cryptographic code examples that improves the usability and security of created applications by non security experts. Method: We created the open-source web platform CryptoExamples and conducted a controlled experiment where 58 students added symmetric encryption to a Java program. We then measured the usability and security. Results: The participants who used the platform were not only significantly more effective (+73 %) but also their code contained significantly less possible security vulnerabilities (-66 %). Conclusions: With CryptoExamples the gap between hard to change API documentation and the need for complete and secure code examples can be closed. Still, the platform needs more code examples.

</details>

<details>

<summary>2018-07-05 04:12:41 - Improving Fuzzing Using Software Complexity Metrics</summary>

- *Maksim Shudrak, Vyacheslav Zolotarev*

- `1807.01838v1` - [abs](http://arxiv.org/abs/1807.01838v1) - [pdf](http://arxiv.org/pdf/1807.01838v1)

> Vulnerable software represents a tremendous threat to modern information systems. Vulnerabilities in widespread applications may be used to spread malware, steal money and conduct target attacks. To address this problem, developers and researchers use different approaches of dynamic and static software analysis; one of these approaches is called fuzzing. Fuzzing is performed by generating and sending potentially malformed data to an application under test. Since first appearance in 1988, fuzzing has evolved a lot, but issues which addressed to effectiveness evaluation have not fully investigated until now. In our research, we propose a novel approach of fuzzing effectiveness evaluation, taking into account semantics of executed code along with a quantitative assessment. For this purpose, we use specific metrics of source code complexity assessment adapted to perform analysis of machine code. We conducted effectiveness evaluation of these metrics on 104 widespread applications with known vulnerabilities. As a result of these experiments, we were able to identify a set of metrics that are more suitable to find bugs. In addition, we conducted separate experiments on 7 applications without known vulnerabilities by using the set of metrics. The experimental results confirmed that proposed approach can be applied to increase performance of the fuzzing. Moreover, the tools helped detect two critical zero day (previously unknown) vulnerabilities in the wide-spread applications.

</details>

<details>

<summary>2018-07-05 17:31:14 - Sequential Attacks on Agents for Long-Term Adversarial Goals</summary>

- *Edgar Tretschk, Seong Joon Oh, Mario Fritz*

- `1805.12487v2` - [abs](http://arxiv.org/abs/1805.12487v2) - [pdf](http://arxiv.org/pdf/1805.12487v2)

> Reinforcement learning (RL) has advanced greatly in the past few years with the employment of effective deep neural networks (DNNs) on the policy networks. With the great effectiveness came serious vulnerability issues with DNNs that small adversarial perturbations on the input can change the output of the network. Several works have pointed out that learned agents with a DNN policy network can be manipulated against achieving the original task through a sequence of small perturbations on the input states. In this paper, we demonstrate furthermore that it is also possible to impose an arbitrary adversarial reward on the victim policy network through a sequence of attacks. Our method involves the latest adversarial attack technique, Adversarial Transformer Network (ATN), that learns to generate the attack and is easy to integrate into the policy network. As a result of our attack, the victim agent is misguided to optimise for the adversarial reward over time. Our results expose serious security threats for RL applications in safety-critical systems including drones, medical analysis, and self-driving cars.

</details>

<details>

<summary>2018-07-06 02:59:54 - Homodyne-detector-blinding attack in continuous-variable quantum key distribution</summary>

- *Hao Qin, Rupesh Kumar, Vadim Makarov, Romain Alléaume*

- `1805.01620v2` - [abs](http://arxiv.org/abs/1805.01620v2) - [pdf](http://arxiv.org/pdf/1805.01620v2)

> We propose an efficient strategy to attack a continuous-variable quantum key distribution (CV-QKD) system, that we call homodyne detector blinding. This attack strategy takes advantage of a generic vulnerability of homodyne receivers: a bright light pulse sent on the signal port can lead to a saturation of the detector electronics. While detector saturation has already been proposed to attack CV-QKD, the attack we study in this paper has the additional advantage of not requiring an eavesdropper to be phase locked with the homodyne receiver. We show that under certain conditions, an attacker can use a simple laser, incoherent with the homodyne receiver, to generate bright pulses and bias the excess noise to arbitrary small values, fully comprising CV-QKD security. These results highlight the feasibility and the impact of the detector blinding attack. We finally discuss how to design countermeasures in order to protect against this attack.

</details>

<details>

<summary>2018-07-06 15:33:23 - A Practical Approach to the Automatic Classification of Security-Relevant Commits</summary>

- *Antonino Sabetta, Michele Bezzi*

- `1807.02458v1` - [abs](http://arxiv.org/abs/1807.02458v1) - [pdf](http://arxiv.org/pdf/1807.02458v1)

> The lack of reliable sources of detailed information on the vulnerabilities of open-source software (OSS) components is a major obstacle to maintaining a secure software supply chain and an effective vulnerability management process. Standard sources of advisories and vulnerability data, such as the National Vulnerability Database (NVD), are known to suffer from poor coverage and inconsistent quality.   To reduce our dependency on these sources, we propose an approach that uses machine-learning to analyze source code repositories and to automatically identify commits that are security-relevant (i.e., that are likely to fix a vulnerability). We treat the source code changes introduced by commits as documents written in natural language, classifying them using standard document classification methods.   Combining independent classifiers that use information from different facets of commits, our method can yield high precision (80%) while ensuring acceptable recall (43%). In particular, the use of information extracted from the source code changes yields a substantial improvement over the best known approach in state of the art, while requiring a significantly smaller amount of training data and employing a simpler architecture.

</details>

<details>

<summary>2018-07-07 00:34:51 - WedgeTail: An Intrusion Prevention System for the Data Plane of Software Defined Networks</summary>

- *Arash Shaghaghi, Mohamed Ali Kaafar, Sanjay Jha*

- `1708.05477v2` - [abs](http://arxiv.org/abs/1708.05477v2) - [pdf](http://arxiv.org/pdf/1708.05477v2)

> Networks are vulnerable to disruptions caused by malicious forwarding devices. The situation is likely to worsen in Software Defined Networks (SDNs) with the incompatibility of existing solutions, use of programmable soft switches and the potential of bringing down an entire network through compromised forwarding devices. In this paper, we present WedgeTail, an Intrusion Prevention System (IPS) designed to secure the SDN data plane. WedgeTail regards forwarding devices as points within a geometric space and stores the path packets take when traversing the network as trajectories. To be efficient, it prioritizes forwarding devices before inspection using an unsupervised trajectory-based sampling mechanism. For each of the forwarding device, WedgeTail computes the expected and actual trajectories of packets and `hunts' for any forwarding device not processing packets as expected. Compared to related work, WedgeTail is also capable of distinguishing between malicious actions such as packet drop and generation. Moreover, WedgeTail employs a radically different methodology that enables detecting threats autonomously. In fact, it has no reliance on pre-defined rules by an administrator and may be easily imported to protect SDN networks with different setups, forwarding devices, and controllers. We have evaluated WedgeTail in simulated environments, and it has been capable of detecting and responding to all implanted malicious forwarding devices within a reasonable time-frame. We report on the design, implementation, and evaluation of WedgeTail in this manuscript.

</details>

<details>

<summary>2018-07-07 02:32:57 - Adversarial Examples: Attacks and Defenses for Deep Learning</summary>

- *Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li*

- `1712.07107v3` - [abs](http://arxiv.org/abs/1712.07107v3) - [pdf](http://arxiv.org/pdf/1712.07107v3)

> With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.

</details>

<details>

<summary>2018-07-10 17:02:16 - Speculative Buffer Overflows: Attacks and Defenses</summary>

- *Vladimir Kiriansky, Carl Waldspurger*

- `1807.03757v1` - [abs](http://arxiv.org/abs/1807.03757v1) - [pdf](http://arxiv.org/pdf/1807.03757v1)

> Practical attacks that exploit speculative execution can leak confidential information via microarchitectural side channels. The recently-demonstrated Spectre attacks leverage speculative loads which circumvent access checks to read memory-resident secrets, transmitting them to an attacker using cache timing or other covert communication channels.   We introduce Spectre1.1, a new Spectre-v1 variant that leverages speculative stores to create speculative buffer overflows. Much like classic buffer overflows, speculative out-of-bounds stores can modify data and code pointers. Data-value attacks can bypass some Spectre-v1 mitigations, either directly or by redirecting control flow. Control-flow attacks enable arbitrary speculative code execution, which can bypass fence instructions and all other software mitigations for previous speculative-execution attacks. It is easy to construct return-oriented-programming (ROP) gadgets that can be used to build alternative attack payloads.   We also present Spectre1.2: on CPUs that do not enforce read/write protections, speculative stores can overwrite read-only data and code pointers to breach sandboxes.   We highlight new risks posed by these vulnerabilities, discuss possible software mitigations, and sketch microarchitectural mechanisms that could serve as hardware defenses. We have not yet evaluated the performance impact of our proposed software and hardware mitigations. We describe the salient vulnerability features and additional hypothetical attack scenarios only to the detail necessary to guide hardware and software vendors in threat analysis and mitigations. We advise users to refer to more user-friendly vendor recommendations for mitigations against speculative buffer overflows or available patches.

</details>

<details>

<summary>2018-07-10 19:00:54 - Thermanator: Thermal Residue-Based Post Factum Attacks On Keyboard Password Entry</summary>

- *Tyler Kaczmarek, Ercan Ozturk, Gene Tsudik*

- `1806.10189v2` - [abs](http://arxiv.org/abs/1806.10189v2) - [pdf](http://arxiv.org/pdf/1806.10189v2)

> As a warm-blooded mammalian species, we humans routinely leave thermal residues on various objects with which we come in contact. This includes common input devices, such as keyboards, that are used for entering (among other things) secret information, such as passwords and PINs. Although thermal residue dissipates over time, there is always a certain time window during which thermal energy readings can be harvested from input devices to recover recently entered, and potentially sensitive, information. To-date, there has been no systematic investigation of thermal profiles of keyboards, and thus no efforts have been made to secure them. This serves as our main motivation for constructing a means for password harvesting from keyboard thermal emanations. Specifically, we introduce Thermanator, a new post factum insider attack based on heat transfer caused by a user typing a password on a typical external keyboard. We conduct and describe a user study that collected thermal residues from 30 users entering 10 unique passwords (both weak and strong) on 4 popular commodity keyboards. Results show that entire sets of key-presses can be recovered by non-expert users as late as 30 seconds after initial password entry, while partial sets can be recovered as late as 1 minute after entry. Furthermore, we find that Hunt-and-Peck typists are particularly vulnerable. We also discuss some Thermanator mitigation strategies. The main take-away of this work is three-fold: (1) using external keyboards to enter (already much-maligned) passwords is even less secure than previously recognized, (2) post factum (planned or impromptu) thermal imaging attacks are realistic, and finally (3) perhaps it is time to either stop using keyboards for password entry, or abandon passwords altogether.

</details>

<details>

<summary>2018-07-11 12:14:20 - The Effect of Noise on Sofware Engineers' Performance</summary>

- *Simone Romano, Giuseppe Scanniello, Davide Fucci, Natalia Juristo, Burak Turhan*

- `1807.04100v1` - [abs](http://arxiv.org/abs/1807.04100v1) - [pdf](http://arxiv.org/pdf/1807.04100v1)

> Background: Noise, defined as an unwanted sound, is one of the commonest factors that could affect people's performance in their daily work activities. The software engineering research community has marginally investigated the effects of noise on software engineers' performance. Aims: We studied if noise affects software engineers' performance in (i) comprehending functional requirements and (ii) fixing faults in the source code. Method: We conducted two experiments with final-year undergraduate students in Computer Science. In the first experiment, we asked 55 students to comprehend functional requirements exposing them or not to noise, while in the second experiment 42 students were asked to fix faults in Java code. Results: The participants in the second experiment, when exposed to noise, had significantly worse performance in fixing faults in the source code. On the other hand, we did not observe any statistically significant difference in the first experiment. Conclusions: Fixing faults in source code seems to be more vulnerable to noise than comprehending functional requirements.

</details>

<details>

<summary>2018-07-11 13:09:25 - ThingPot: an interactive Internet-of-Things honeypot</summary>

- *Meng Wang, Javier Santillan, Fernando Kuipers*

- `1807.04114v1` - [abs](http://arxiv.org/abs/1807.04114v1) - [pdf](http://arxiv.org/pdf/1807.04114v1)

> The Mirai Distributed Denial-of-Service (DDoS) attack exploited security vulnerabilities of Internet-of-Things (IoT) devices and thereby clearly signalled that attackers have IoT on their radar. Securing IoT is therefore imperative, but in order to do so it is crucial to understand the strategies of such attackers. For that purpose, in this paper, a novel IoT honeypot called ThingPot is proposed and deployed. Honeypot technology mimics devices that might be exploited by attackers and logs their behavior to detect and analyze the used attack vectors. ThingPot is the first of its kind, since it focuses not only on the IoT application protocols themselves, but on the whole IoT platform. A Proof-of-Concept is implemented with XMPP and a REST API, to mimic a Philips Hue smart lighting system. ThingPot has been deployed for 1.5 months and through the captured data we have found five types of attacks and attack vectors against smart devices. The ThingPot source code is made available as open source.

</details>

<details>

<summary>2018-07-11 14:54:39 - Explainable Security</summary>

- *Luca Viganò, Daniele Magazzeni*

- `1807.04178v1` - [abs](http://arxiv.org/abs/1807.04178v1) - [pdf](http://arxiv.org/pdf/1807.04178v1)

> The Defense Advanced Research Projects Agency (DARPA) recently launched the Explainable Artificial Intelligence (XAI) program that aims to create a suite of new AI techniques that enable end users to understand, appropriately trust, and effectively manage the emerging generation of AI systems.   In this paper, inspired by DARPA's XAI program, we propose a new paradigm in security research: Explainable Security (XSec). We discuss the ``Six Ws'' of XSec (Who? What? Where? When? Why? and How?) and argue that XSec has unique and complex characteristics: XSec involves several different stakeholders (i.e., the system's developers, analysts, users and attackers) and is multi-faceted by nature (as it requires reasoning about system model, threat model and properties of security, privacy and trust as well as about concrete attacks, vulnerabilities and countermeasures). We define a roadmap for XSec that identifies several possible research directions.

</details>

<details>

<summary>2018-07-12 09:01:50 - Beyond Metadata: Code-centric and Usage-based Analysis of Known Vulnerabilities in Open-source Software</summary>

- *Serena E. Ponta, Henrik Plate, Antonino Sabetta*

- `1806.05893v3` - [abs](http://arxiv.org/abs/1806.05893v3) - [pdf](http://arxiv.org/pdf/1806.05893v3)

> The use of open-source software (OSS) is ever-increasing, and so is the number of open-source vulnerabilities being discovered and publicly disclosed. The gains obtained from the reuse of community-developed libraries may be offset by the cost of detecting, assessing, and mitigating their vulnerabilities in a timely fashion.   In this paper we present a novel method to detect, assess and mitigate OSS vulnerabilities that improves on state-of-the-art approaches, which commonly depend on metadata to identify vulnerable OSS dependencies. Our solution instead is code-centric and combines static and dynamic analysis to determine the reachability of the vulnerable portion of libraries used (directly or transitively) by an application. Taking this usage into account, our approach then supports developers in choosing among the existing non-vulnerable library versions.   VULAS, the tool implementing our code-centric and usage-based approach, is officially recommended by SAP to scan its Java software, and has been successfully used to perform more than 250000 scans of about 500 applications since December 2016. We report on our experience and on the lessons we learned when maturing the tool from a research prototype to an industrial-grade solution.

</details>

<details>

<summary>2018-07-12 16:14:24 - Symbolic Verification of Cache Side-channel Freedom</summary>

- *Sudipta Chattopadhyay, Abhik Roychoudhury*

- `1807.04701v1` - [abs](http://arxiv.org/abs/1807.04701v1) - [pdf](http://arxiv.org/pdf/1807.04701v1)

> Cache timing attacks allow third-party observers to retrieve sensitive information from program executions. But, is it possible to automatically check the vulnerability of a program against cache timing attacks and then, automatically shield program executions against these attacks? For a given program, a cache configuration and an attack model, our CACHEFIX framework either verifies the cache side-channel freedom of the program or synthesizes a series of patches to ensure cache side-channel freedom during program execution. At the core of our framework is a novel symbolic verification technique based on automated abstraction refinement of cache semantics. The power of such a framework is to allow symbolic reasoning over counterexample traces and to combine it with runtime monitoring for eliminating cache side channels during program execution. Our evaluation with routines from OpenSSL, libfixedtimefixedpoint, GDK and FourQlib libraries reveals that our CACHEFIX approach (dis)proves cache sidechannel freedom within an average of 75 seconds. Besides, in all except one case, CACHEFIX synthesizes all patches within 20 minutes to ensure cache side-channel freedom of the respective routines during execution.

</details>

<details>

<summary>2018-07-17 18:10:23 - The Hidden Vulnerability of Distributed Learning in Byzantium</summary>

- *El Mahdi El Mhamdi, Rachid Guerraoui, Sébastien Rouault*

- `1802.07927v2` - [abs](http://arxiv.org/abs/1802.07927v2) - [pdf](http://arxiv.org/pdf/1802.07927v2)

> While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochastic gradient descent (SGD). Recent approaches have been proposed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending poisoned gradients during the training phase. Some of these approaches have been proven Byzantine-resilient: they ensure the convergence of SGD despite the presence of a minority of adversarial workers.   We show in this paper that convergence is not enough. In high dimension $d \gg 1$, an adver\-sary can build on the loss function's non-convexity to make SGD converge to ineffective models. More precisely, we bring to light that existing Byzantine-resilient schemes leave a margin of poisoning of $\Omega\left(f(d)\right)$, where $f(d)$ increases at least like $\sqrt{d~}$. Based on this leeway, we build a simple attack, and experimentally show its strong to utmost effectivity on CIFAR-10 and MNIST.   We introduce Bulyan, and prove it significantly reduces the attackers leeway to a narrow $O( \frac{1}{\sqrt{d~}})$ bound. We empirically show that Bulyan does not suffer the fragility of existing aggregation rules and, at a reasonable cost in terms of required batch size, achieves convergence as if only non-Byzantine gradients had been used to update the model.

</details>

<details>

<summary>2018-07-18 13:58:39 - How Usable are Rust Cryptography APIs?</summary>

- *Kai Mindermann, Philipp Keck, Stefan Wagner*

- `1806.04929v4` - [abs](http://arxiv.org/abs/1806.04929v4) - [pdf](http://arxiv.org/pdf/1806.04929v4)

> Context: Poor usability of cryptographic APIs is a severe source of vulnerabilities. Aim: We wanted to find out what kind of cryptographic libraries are present in Rust and how usable they are. Method: We explored Rust's cryptographic libraries through a systematic search, conducted an exploratory study on the major libraries and a controlled experiment on two of these libraries with 28 student participants. Results: Only half of the major libraries explicitly focus on usability and misuse resistance, which is reflected in their current APIs. We found that participants were more successful using rust-crypto which we considered less usable than ring before the experiment. Conclusion: We discuss API design insights and make recommendations for the design of crypto libraries in Rust regarding the detail and structure of the documentation, higher-level APIs as wrappers for the existing low-level libraries, and selected, good-quality example code to improve the emerging cryptographic libraries of Rust.

</details>

<details>

<summary>2018-07-18 14:19:49 - Quantifying Biases in Online Information Exposure</summary>

- *Dimitar Nikolov, Mounia Lalmas, Alessandro Flammini, Filippo Menczer*

- `1807.06958v1` - [abs](http://arxiv.org/abs/1807.06958v1) - [pdf](http://arxiv.org/pdf/1807.06958v1)

> Our consumption of online information is mediated by filtering, ranking, and recommendation algorithms that introduce unintentional biases as they attempt to deliver relevant and engaging content. It has been suggested that our reliance on online technologies such as search engines and social media may limit exposure to diverse points of view and make us vulnerable to manipulation by disinformation. In this paper, we mine a massive dataset of Web traffic to quantify two kinds of bias: (i) homogeneity bias, which is the tendency to consume content from a narrow set of information sources, and (ii) popularity bias, which is the selective exposure to content from top sites. Our analysis reveals different bias levels across several widely used Web platforms. Search exposes users to a diverse set of sources, while social media traffic tends to exhibit high popularity and homogeneity bias. When we focus our analysis on traffic to news sites, we find higher levels of popularity bias, with smaller differences across applications. Overall, our results quantify the extent to which our choices of online systems confine us inside "social bubbles."

</details>

<details>

<summary>2018-07-19 08:27:23 - Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning</summary>

- *Battista Biggio, Fabio Roli*

- `1712.03141v2` - [abs](http://arxiv.org/abs/1712.03141v2) - [pdf](http://arxiv.org/pdf/1712.03141v2)

> Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.

</details>

<details>

<summary>2018-07-19 15:19:38 - A Microservice-enabled Architecture for Smart Surveillance using Blockchain Technology</summary>

- *Deeraj Nagothu, Ronghua Xu, Seyed Yahya Nikouei, Yu Chen*

- `1807.07487v1` - [abs](http://arxiv.org/abs/1807.07487v1) - [pdf](http://arxiv.org/pdf/1807.07487v1)

> While the smart surveillance system enhanced by the Internet of Things (IoT) technology becomes an essential part of Smart Cities, it also brings new concerns in security of the data. Compared to the traditional surveillance systems that is built following a monolithic architecture to carry out lower level operations, such as monitoring and recording, the modern surveillance systems are expected to support more scalable and decentralized solutions for advanced video stream analysis at the large volumes of distributed edge devices. In addition, the centralized architecture of the conventional surveillance systems is vulnerable to single point of failure and privacy breach owning to the lack of protection to the surveillance feed. This position paper introduces a novel secure smart surveillance system based on microservices architecture and blockchain technology. Encapsulating the video analysis algorithms as various independent microservices not only isolates the video feed from different sectors, but also improve the system availability and robustness by decentralizing the operations. The blockchain technology securely synchronizes the video analysis databases among microservices across surveillance domains, and provides tamper proof of data in the trustless network environment. Smart contract enabled access authorization strategy prevents any unauthorized user from accessing the microservices and offers a scalable, decentralized and fine-grained access control solution for smart surveillance systems.

</details>

<details>

<summary>2018-07-19 16:14:08 - Using Deep Neural Networks to Translate Multi-lingual Threat Intelligence</summary>

- *Priyanka Ranade, Sudip Mittal, Anupam Joshi, Karuna Joshi*

- `1807.07517v1` - [abs](http://arxiv.org/abs/1807.07517v1) - [pdf](http://arxiv.org/pdf/1807.07517v1)

> The multilingual nature of the Internet increases complications in the cybersecurity community's ongoing efforts to strategically mine threat intelligence from OSINT data on the web. OSINT sources such as social media, blogs, and dark web vulnerability markets exist in diverse languages and hinder security analysts, who are unable to draw conclusions from intelligence in languages they don't understand. Although third party translation engines are growing stronger, they are unsuited for private security environments. First, sensitive intelligence is not a permitted input to third party engines due to privacy and confidentiality policies. In addition, third party engines produce generalized translations that tend to lack exclusive cybersecurity terminology. In this paper, we address these issues and describe our system that enables threat intelligence understanding across unfamiliar languages. We create a neural network based system that takes in cybersecurity data in a different language and outputs the respective English translation. The English translation can then be understood by an analyst, and can also serve as input to an AI based cyber-defense system that can take mitigative action. As a proof of concept, we have created a pipeline which takes Russian threats and generates its corresponding English, RDF, and vectorized representations. Our network optimizes translations on specifically, cybersecurity data.

</details>

<details>

<summary>2018-07-20 14:43:32 - Learning Inputs in Greybox Fuzzing</summary>

- *Valentin Wüstholz, Maria Christakis*

- `1807.07875v1` - [abs](http://arxiv.org/abs/1807.07875v1) - [pdf](http://arxiv.org/pdf/1807.07875v1)

> Greybox fuzzing is a lightweight testing approach that effectively detects bugs and security vulnerabilities. However, greybox fuzzers randomly mutate program inputs to exercise new paths; this makes it challenging to cover code that is guarded by complex checks.   In this paper, we present a technique that extends greybox fuzzing with a method for learning new inputs based on already explored program executions. These inputs can be learned such that they guide exploration toward specific executions, for instance, ones that increase path coverage or reveal vulnerabilities. We have evaluated our technique and compared it to traditional greybox fuzzing on 26 real-world benchmarks. In comparison, our technique significantly increases path coverage (by up to 3X) and detects more bugs (up to 38% more), often orders-of-magnitude faster.

</details>

<details>

<summary>2018-07-20 16:56:40 - Spectre Returns! Speculation Attacks using the Return Stack Buffer</summary>

- *Esmaeil Mohammadian Koruyeh, Khaled Khasawneh, Chengyu Song, Nael Abu-Ghazaleh*

- `1807.07940v1` - [abs](http://arxiv.org/abs/1807.07940v1) - [pdf](http://arxiv.org/pdf/1807.07940v1)

> The recent Spectre attacks exploit speculative execution, a pervasively used feature of modern microprocessors, to allow the exfiltration of sensitive data across protection boundaries. In this paper, we introduce a new Spectre-class attack that we call SpectreRSB. In particular, rather than exploiting the branch predictor unit, SpectreRSB exploits the return stack buffer (RSB), a common predictor structure in modern CPUs used to predict return addresses. We show that both local attacks (within the same process such as Spectre 1) and attacks on SGX are possible by constructing proof of concept attacks. We also analyze additional types of the attack on the kernel or across address spaces and show that under some practical and widely used conditions they are possible. Importantly, none of the known defenses including Retpoline and Intel's microcode patches stop all SpectreRSB attacks. We believe that future system developers should be aware of this vulnerability and consider it in developing defenses against speculation attacks. In particular, on Core-i7 Skylake and newer processors (but not on Intel's Xeon processor line), a patch called RSB refilling is used to address a vulnerability when the RSB underfills; this defense interferes with SpectreRSB's ability to launch attacks that switch into the kernel. We recommend that this patch should be used on all machines to protect against SpectreRSB.

</details>

<details>

<summary>2018-07-20 17:59:58 - Decentralized Task Allocation in Multi-Robot Systems via Bipartite Graph Matching Augmented with Fuzzy Clustering</summary>

- *Payam Ghassemi, Souma Chowdhury*

- `1807.07957v1` - [abs](http://arxiv.org/abs/1807.07957v1) - [pdf](http://arxiv.org/pdf/1807.07957v1)

> Robotic systems, working together as a team, are becoming valuable players in different real-world applications, from disaster response to warehouse fulfillment services. Centralized solutions for coordinating multi-robot teams often suffer from poor scalability and vulnerability to communication disruptions. This paper develops a decentralized multi-agent task allocation (Dec-MATA) algorithm for multi-robot applications. The task planning problem is posed as a maximum-weighted matching of a bipartite graph, the solution of which using the blossom algorithm allows each robot to autonomously identify the optimal sequence of tasks it should undertake. The graph weights are determined based on a soft clustering process, which also plays a problem decomposition role seeking to reduce the complexity of the individual-agents' task assignment problems. To evaluate the new Dec-MATA algorithm, a series of case studies (of varying complexity) are performed, with tasks being distributed randomly over an observable 2D environment. A centralized approach, based on a state-of-the-art MILP formulation of the multi-Traveling Salesman problem is used for comparative analysis. While getting within 7-28% of the optimal cost obtained by the centralized algorithm, the Dec-MATA algorithm is found to be 1-3 orders of magnitude faster and minimally sensitive to task-to-robot ratios, unlike the centralized algorithm.

</details>

<details>

<summary>2018-07-20 20:51:32 - TCP SYN Cookie Vulnerability</summary>

- *Dakshil Shah, Varshali Kumar*

- `1807.08026v1` - [abs](http://arxiv.org/abs/1807.08026v1) - [pdf](http://arxiv.org/pdf/1807.08026v1)

> TCP SYN Cookies were implemented to mitigate against DoS attacks. It ensured that the server did not have to store any information for half-open connections. A SYN cookie contains all information required by the server to know the request is valid. However, the usage of these cookies introduces a vulnerability that allows an attacker to guess the initial sequence number and use that to spoof a connection or plant false logs.

</details>

<details>

<summary>2018-07-23 03:40:42 - Adversarial Training Versus Weight Decay</summary>

- *Angus Galloway, Thomas Tanay, Graham W. Taylor*

- `1804.03308v3` - [abs](http://arxiv.org/abs/1804.03308v3) - [pdf](http://arxiv.org/pdf/1804.03308v3)

> Performance-critical machine learning models should be robust to input perturbations not seen during training. Adversarial training is a method for improving a model's robustness to some perturbations by including them in the training process, but this tends to exacerbate other vulnerabilities of the model. The adversarial training framework has the effect of translating the data with respect to the cost function, while weight decay has a scaling effect. Although weight decay could be considered a crude regularization technique, it appears superior to adversarial training as it remains stable over a broader range of regimes and reduces all generalization errors. Equipped with these abstractions, we provide key baseline results and methodology for characterizing robustness. The two approaches can be combined to yield one small model that demonstrates good robustness to several white-box attacks associated with different metrics.

</details>

<details>

<summary>2018-07-23 19:02:38 - Debloating Software through Piece-Wise Compilation and Loading</summary>

- *Anh Quach, Aravind Prakash, Lok Kwong Yan*

- `1802.00759v3` - [abs](http://arxiv.org/abs/1802.00759v3) - [pdf](http://arxiv.org/pdf/1802.00759v3)

> Programs are bloated. Our study shows that only 5% of libc is used on average across the Ubuntu Desktop environment (2016 programs); the heaviest user, vlc media player, only needed 18%.   In this paper: (1) We present a debloating framework built on a compiler toolchain that can successfully debloat programs (shared/static libraries and executables). Our solution can successfully compile and load most libraries on Ubuntu Desktop 16.04. (2) We demonstrate the elimination of over 79% of code from coreutils and 86% of code from SPEC CPU 2006 benchmark programs without affecting functionality. We show that even complex programs such as Firefox and curl can be debloated without a need to recompile. (3) We demonstrate the security impact of debloating by eliminating over 71% of reusable code gadgets from the coreutils suite and show that unused code that contains real-world vulnerabilities can also be successfully eliminated without adverse effects on the program. (4) We incur a low load time overhead.

</details>

<details>

<summary>2018-07-23 19:37:07 - Note on Attacking Object Detectors with Adversarial Stickers</summary>

- *Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Dawn Song, Tadayoshi Kohno, Amir Rahmati, Atul Prakash, Florian Tramer*

- `1712.08062v2` - [abs](http://arxiv.org/abs/1712.08062v2) - [pdf](http://arxiv.org/pdf/1712.08062v2)

> Deep learning has proven to be a powerful tool for computer vision and has seen widespread adoption for numerous tasks. However, deep learning algorithms are known to be vulnerable to adversarial examples. These adversarial inputs are created such that, when provided to a deep learning algorithm, they are very likely to be mislabeled. This can be problematic when deep learning is used to assist in safety critical decisions. Recent research has shown that classifiers can be attacked by physical adversarial examples under various physical conditions. Given the fact that state-of-the-art objection detection algorithms are harder to be fooled by the same set of adversarial examples, here we show that these detectors can also be attacked by physical adversarial examples. In this note, we briefly show both static and dynamic test results. We design an algorithm that produces physical adversarial inputs, which can fool the YOLO object detector and can also attack Faster-RCNN with relatively high success rate based on transferability. Furthermore, our algorithm can compress the size of the adversarial inputs to stickers that, when attached to the targeted object, result in the detector either mislabeling or not detecting the object a high percentage of the time. This note provides a small set of results. Our upcoming paper will contain a thorough evaluation on other object detectors, and will present the algorithm.

</details>

<details>

<summary>2018-07-24 14:47:59 - Automatically Assessing Vulnerabilities Discovered by Compositional Analysis</summary>

- *Saahil Ognawala, Ricardo Nales Amato, Alexander Pretschner, Pooja Kulkarni*

- `1807.09160v1` - [abs](http://arxiv.org/abs/1807.09160v1) - [pdf](http://arxiv.org/pdf/1807.09160v1)

> Testing is the most widely employed method to find vulnerabilities in real-world software programs. Compositional analysis, based on symbolic execution, is an automated testing method to find vulnerabilities in medium- to large-scale programs consisting of many interacting components. However, existing compositional analysis frameworks do not assess the severity of reported vulnerabilities. In this paper, we present a framework to analyze vulnerabilities discovered by an existing compositional analysis tool and assign CVSS3 (Common Vulnerability Scoring System v3.0) scores to them, based on various heuristics such as interaction with related components, ease of reachability, complexity of design and likelihood of accepting unsanitized input. By analyzing vulnerabilities reported with CVSS3 scores in the past, we train simple machine learning models. By presenting our interactive framework to developers of popular open-source software and other security experts, we gather feedback on our trained models and further improve the features to increase the accuracy of our predictions. By providing qualitative (based on community feedback) and quantitative (based on prediction accuracy) evidence from 21 open-source programs, we show that our severity prediction framework can effectively assist developers with assessing vulnerabilities.

</details>

<details>

<summary>2018-07-27 05:20:20 - A Model for Android and iOS Applications Risk Calculation: CVSS Analysis and Enhancement Using Case-Control Studies</summary>

- *Milda Petraityte, Ali Dehghantanha, Gregory Epiphaniou*

- `1807.10435v1` - [abs](http://arxiv.org/abs/1807.10435v1) - [pdf](http://arxiv.org/pdf/1807.10435v1)

> Various researchers have shown that the Common Vulnerability Scoring System (CVSS) has many drawbacks and may not provide a precise view of the risks related to software vulnerabilities. However, many threat intelligence platforms and industry-wide standards are relying on CVSS score to evaluate cybersecurity compliance. This paper suggests several improvements to the calculation of Impact and Exploitability sub-scores within the CVSS, improve its accuracy and help threat intelligence analysts to focus on the key risks associated with their assets. We will apply our suggested improvements against risks associated with several Android and iOS applications and discuss achieved improvements and advantages of our modelling, such as the importance and the impact of time on the overall CVSS score calculation.

</details>

<details>

<summary>2018-07-27 05:52:22 - Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection</summary>

- *Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, Dawn Song*

- `1708.06525v4` - [abs](http://arxiv.org/abs/1708.06525v4) - [pdf](http://arxiv.org/pdf/1708.06525v4)

> The problem of cross-platform binary code similarity detection aims at detecting whether two binary functions coming from different platforms are similar or not. It has many security applications, including plagiarism detection, malware detection, vulnerability search, etc. Existing approaches rely on approximate graph matching algorithms, which are inevitably slow and sometimes inaccurate, and hard to adapt to a new task. To address these issues, in this work, we propose a novel neural network-based approach to compute the embedding, i.e., a numeric vector, based on the control flow graph of each binary function, then the similarity detection can be done efficiently by measuring the distance between the embeddings for two functions. We implement a prototype called Gemini. Our extensive evaluation shows that Gemini outperforms the state-of-the-art approaches by large margins with respect to similarity detection accuracy. Further, Gemini can speed up prior art's embedding generation time by 3 to 4 orders of magnitude and reduce the required training time from more than 1 week down to 30 minutes to 10 hours. Our real world case studies demonstrate that Gemini can identify significantly more vulnerable firmware images than the state-of-the-art, i.e., Genius. Our research showcases a successful application of deep learning on computer security problems.

</details>

<details>

<summary>2018-07-29 08:06:19 - Artificial Impostors for Location Privacy Preservation</summary>

- *Cheng Wang, Zhiyang Xie*

- `1801.06827v3` - [abs](http://arxiv.org/abs/1801.06827v3) - [pdf](http://arxiv.org/pdf/1801.06827v3)

> The progress of location-based services has led to serious concerns on location privacy leakage. For effective and efficient location privacy preservation (LPP), existing methods are still not fully competent. They are often vulnerable under the identification attack with side information, or hard to be implemented due to the high computational complexity. In this paper, we pursue the high protection efficacy and low computational complexity simultaneously. We propose a scalable LPP method based on the paradigm of counterfeiting locations. To make fake locations extremely plausible, we forge them through synthesizing artificial impostors (AIs). The AIs refer to the synthesized traces which have similar semantic features to the actual traces, and do not contain any target location. Two dedicated techniques are devised: the sampling-based synthesis method and population-level semantic model. They play significant roles in two critical steps of synthesizing AIs. We conduct experiments on real datasets in two cities (Shanghai, China and Asturias, Spain) to validate the high efficacy and scalability of the proposed method. In these two datasets, the experimental results show that our method achieves the preservation efficacy of $97.65\%$ and $96.12\%$, and its run time of building the generators is only $230.47$ and $215.92$ seconds, respectively. This study would give the research community new insights into improving the practicality of the state-of-the-art LPP paradigm via counterfeiting locations.

</details>

<details>

<summary>2018-07-29 08:58:38 - A Survey of Machine and Deep Learning Methods for Internet of Things (IoT) Security</summary>

- *Mohammed Ali Al-Garadi, Amr Mohamed, Abdulla Al-Ali, Xiaojiang Du, Mohsen Guizani*

- `1807.11023v1` - [abs](http://arxiv.org/abs/1807.11023v1) - [pdf](http://arxiv.org/pdf/1807.11023v1)

> The Internet of Things (IoT) integrates billions of smart devices that can communicate with one another with minimal human intervention. It is one of the fastest developing fields in the history of computing, with an estimated 50 billion devices by the end of 2020. On the one hand, IoT play a crucial role in enhancing several real-life smart applications that can improve life quality. On the other hand, the crosscutting nature of IoT systems and the multidisciplinary components involved in the deployment of such systems introduced new security challenges. Implementing security measures, such as encryption, authentication, access control, network security and application security, for IoT devices and their inherent vulnerabilities is ineffective. Therefore, existing security methods should be enhanced to secure the IoT system effectively. Machine learning and deep learning (ML/DL) have advanced considerably over the last few years, and machine intelligence has transitioned from laboratory curiosity to practical machinery in several important applications. Consequently, ML/DL methods are important in transforming the security of IoT systems from merely facilitating secure communication between devices to security-based intelligence systems. The goal of this work is to provide a comprehensive survey of ML /DL methods that can be used to develop enhanced security methods for IoT systems. IoT security threats that are related to inherent or newly introduced threats are presented, and various potential IoT system attack surfaces and the possible threats related to each surface are discussed. We then thoroughly review ML/DL methods for IoT security and present the opportunities, advantages and shortcomings of each method. We discuss the opportunities and challenges involved in applying ML/DL to IoT security. These opportunities and challenges can serve as potential future research directions.

</details>

<details>

<summary>2018-07-29 17:22:24 - Mobile Technology in Healthcare Environment: Security Vulnerabilities and Countermeasures</summary>

- *Sajedul Talukder, Shalisha Witherspoon, Kanishk Srivastava, Ryan Thompson*

- `1807.11086v1` - [abs](http://arxiv.org/abs/1807.11086v1) - [pdf](http://arxiv.org/pdf/1807.11086v1)

> Mobile devices and technologies offer a tremendous amount of benefits to users, although it is also understood that it introduces a set of challenges when it comes to security, compliance, and risks. More and more healthcare organizations have been seeking to update their outdated technology, and have considered the adoption of mobile devices to meet these needs. However, introducing mobile devices and technology also introduces new risks and threats to the organization. As a test case, we examine Epic Rover, a mobile application that has been identified as a viable solution to manage the electronic medical system. In this paper, we study the insights that the security team needs to investigate, before the adoption of this mobile technology, as well as provide a thorough examination of the vulnerabilities and threats that the use of mobile devices in the healthcare environment brings, and introduce countermeasures and mitigations to reduce the risk while maintaining regulatory compliance.

</details>

<details>

<summary>2018-07-30 13:48:10 - Experimental Analysis of Subscribers' Privacy Exposure by LTE Paging</summary>

- *Christian Sørseth, Xianyu Shelley Zhou, Stig F. Mjølsnes, Ruxandra F. Olimid*

- `1807.11350v1` - [abs](http://arxiv.org/abs/1807.11350v1) - [pdf](http://arxiv.org/pdf/1807.11350v1)

> Over the last years, considerable attention has been given to the privacy of individuals in wireless environments. Although significantly improved over the previous generations of mobile networks, LTE still exposes vulnerabilities that attackers can exploit. This might be the case of paging messages, wake-up notifications that target specific subscribers, and that are broadcasted in clear over the radio interface. If they are not properly implemented, paging messages can expose the identity of subscribers and furthermore provide information about their location. It is therefore important that mobile network operators comply with the recommendations and implement the appropriate mechanisms to mitigate attacks. In this paper, we verify by experiment that paging messages can be captured and decoded by using minimal technical skills and publicly available tools. Moreover, we present a general experimental method to test privacy exposure by LTE paging messages, and we conduct a case study on three different LTE mobile operators.

</details>

<details>

<summary>2018-07-31 14:37:18 - Open Source Android Vulnerability Detection Tools: A Survey</summary>

- *Keyur Kulkarni, Ahmad Y Javaid*

- `1807.11840v1` - [abs](http://arxiv.org/abs/1807.11840v1) - [pdf](http://arxiv.org/pdf/1807.11840v1)

> Since last decade, smartphones have become an integral part of everyone's life. Having the ability to handle many useful and attractive applications, smartphones sport flawless functionality and small sizes leading to their exponential growth. Additionally, due to the huge user base and a wide range of functionalities, these mobile platforms have become a popular source of information to the public through several Apps provided by the DHS Citizen Application Directory. Such wide audience to this platform is also making it a huge target for cyber- attacks. While Android, the most popular open source mobile platform, has its base set of permissions to protect the device and resources, it does not provide a security framework to defend against any attack. This paper surveys threat, vulnerability and security analysis tools, which are open source in nature, for the Android platform and systemizes the knowledge of Android security mechanisms. Additionally, a comparison of three popular tools is presented.

</details>

<details>

<summary>2018-07-31 15:01:26 - Cyber-attack Mitigation and Impact Analysis for Low-power IoT Devices</summary>

- *Ashutosh Bandekar, Ahmad Y. Javaid*

- `1807.11850v1` - [abs](http://arxiv.org/abs/1807.11850v1) - [pdf](http://arxiv.org/pdf/1807.11850v1)

> Recent years have seen exponential development in wireless sensor devices and their rebirth as IoT, as well as increased popularity in wireless home devices such as bulbs, fans, and microwave. As they can be used in various fields such as medical devices, environmental studies, fire department or military application, etc., security of these low powered devices will always be a concern for all the user and security experts. Users nowadays want to control all these "smart" wireless home devices through their smartphones using an internet connection. Attacks such as distributed attacks on these devices will render the whole system vulnerable as these attacks can record and extract confidential information as well as increase resource (energy) consumption of the entire network. In this paper, we propose a cyber-attack detection algorithm and present an impact analysis of easy-to-launch cyber-attacks on a low-power mote (Z1 Zolertia) as a model IoT device. We also present detailed results of power consumption analysis with and without attack along with when the mitigation algorithm for intrusion detection is implemented.

</details>


## 2018-08

<details>

<summary>2018-08-01 00:44:31 - Towards Robust Neural Networks via Random Self-ensemble</summary>

- *Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh*

- `1712.00673v2` - [abs](http://arxiv.org/abs/1712.00673v2) - [pdf](http://arxiv.org/pdf/1712.00673v2)

> Recent studies have revealed the vulnerability of deep neural networks: A small adversarial perturbation that is imperceptible to human can easily make a well-trained deep neural network misclassify. This makes it unsafe to apply neural networks in security-critical applications. In this paper, we propose a new defense algorithm called Random Self-Ensemble (RSE) by combining two important concepts: {\bf randomness} and {\bf ensemble}. To protect a targeted model, RSE adds random noise layers to the neural network to prevent the strong gradient-based attacks, and ensembles the prediction over random noises to stabilize the performance. We show that our algorithm is equivalent to ensemble an infinite number of noisy models $f_\epsilon$ without any additional memory overhead, and the proposed training procedure based on noisy stochastic gradient descent can ensure the ensemble model has a good predictive capability. Our algorithm significantly outperforms previous defense techniques on real data sets. For instance, on CIFAR-10 with VGG network (which has 92\% accuracy without any attack), under the strong C\&W attack within a certain distortion tolerance, the accuracy of unprotected model drops to less than 10\%, the best previous defense technique has $48\%$ accuracy, while our method still has $86\%$ prediction accuracy under the same level of attack. Finally, our method is simple and easy to integrate into any neural network.

</details>

<details>

<summary>2018-08-01 00:59:08 - EagleEye: Attack-Agnostic Defense against Adversarial Inputs (Technical Report)</summary>

- *Yujie Ji, Xinyang Zhang, Ting Wang*

- `1808.00123v1` - [abs](http://arxiv.org/abs/1808.00123v1) - [pdf](http://arxiv.org/pdf/1808.00123v1)

> Deep neural networks (DNNs) are inherently vulnerable to adversarial inputs: such maliciously crafted samples trigger DNNs to misbehave, leading to detrimental consequences for DNN-powered systems. The fundamental challenges of mitigating adversarial inputs stem from their adaptive and variable nature. Existing solutions attempt to improve DNN resilience against specific attacks; yet, such static defenses can often be circumvented by adaptively engineered inputs or by new attack variants.   Here, we present EagleEye, an attack-agnostic adversarial tampering analysis engine for DNN-powered systems. Our design exploits the {\em minimality principle} underlying many attacks: to maximize the attack's evasiveness, the adversary often seeks the minimum possible distortion to convert genuine inputs to adversarial ones. We show that this practice entails the distinct distributional properties of adversarial inputs in the input space. By leveraging such properties in a principled manner, EagleEye effectively discriminates adversarial inputs and even uncovers their correct classification outputs. Through extensive empirical evaluation using a range of benchmark datasets and DNN models, we validate EagleEye's efficacy. We further investigate the adversary's possible countermeasures, which implies a difficult dilemma for her: to evade EagleEye's detection, excessive distortion is necessary, thereby significantly reducing the attack's evasiveness regarding other detection mechanisms.

</details>

<details>

<summary>2018-08-01 15:38:08 - A Differentially Private Kernel Two-Sample Test</summary>

- *Anant Raj, Ho Chung Leon Law, Dino Sejdinovic, Mijung Park*

- `1808.00380v1` - [abs](http://arxiv.org/abs/1808.00380v1) - [pdf](http://arxiv.org/pdf/1808.00380v1)

> Kernel two-sample testing is a useful statistical tool in determining whether data samples arise from different distributions without imposing any parametric assumptions on those distributions. However, raw data samples can expose sensitive information about individuals who participate in scientific studies, which makes the current tests vulnerable to privacy breaches. Hence, we design a new framework for kernel two-sample testing conforming to differential privacy constraints, in order to guarantee the privacy of subjects in the data. Unlike existing differentially private parametric tests that simply add noise to data, kernel-based testing imposes a challenge due to a complex dependence of test statistics on the raw data, as these statistics correspond to estimators of distances between representations of probability measures in Hilbert spaces. Our approach considers finite dimensional approximations to those representations. As a result, a simple chi-squared test is obtained, where a test statistic depends on a mean and covariance of empirical differences between the samples, which we perturb for a privacy guarantee. We investigate the utility of our framework in two realistic settings and conclude that our method requires only a relatively modest increase in sample size to achieve a similar level of power to the non-private tests in both settings.

</details>

<details>

<summary>2018-08-01 19:08:54 - Implementation and Evaluation of a Cooperative Vehicle-to-Pedestrian Safety Application</summary>

- *Amin Tahmasbi-Sarvestani, Hossein Nourkhiz Mahjoub, Yaser P. Fallah, Ehsan Moradi-Pari, Oubada Abuchaar*

- `1808.06463v1` - [abs](http://arxiv.org/abs/1808.06463v1) - [pdf](http://arxiv.org/pdf/1808.06463v1)

> While the development of Vehicle-to-Vehicle (V2V) safety applications based on Dedicated Short-Range Communications (DSRC) has been extensively undergoing standardization for more than a decade, such applications are extremely missing for Vulnerable Road Users (VRUs). Nonexistence of collaborative systems between VRUs and vehicles was the main reason for this lack of attention. Recent developments in Wi-Fi Direct and DSRC-enabled smartphones are changing this perspective. Leveraging the existing V2V platforms, we propose a new framework using a DSRC-enabled smartphone to extend safety benefits to VRUs. The interoperability of applications between vehicles and portable DSRC enabled devices is achieved through the SAE J2735 Personal Safety Message (PSM). However, considering the fact that VRU movement dynamics, response times, and crash scenarios are fundamentally different from vehicles, a specific framework should be designed for VRU safety applications to study their performance. In this article, we first propose an end-to-end Vehicle-to-Pedestrian (V2P) framework to provide situational awareness and hazard detection based on the most common and injury-prone crash scenarios. The details of our VRU safety module, including target classification and collision detection algorithms, are explained next. Furthermore, we propose and evaluate a mitigating solution for congestion and power consumption issues in such systems. Finally, the whole system is implemented and analyzed for realistic crash scenarios.

</details>

<details>

<summary>2018-08-02 13:27:12 - Automated software vulnerability detection with machine learning</summary>

- *Jacob A. Harer, Louis Y. Kim, Rebecca L. Russell, Onur Ozdemir, Leonard R. Kosta, Akshay Rangamani, Lei H. Hamilton, Gabriel I. Centeno, Jonathan R. Key, Paul M. Ellingwood, Erik Antelman, Alan Mackay, Marc W. McConley, Jeffrey M. Opper, Peter Chin, Tomo Lazovich*

- `1803.04497v2` - [abs](http://arxiv.org/abs/1803.04497v2) - [pdf](http://arxiv.org/pdf/1803.04497v2)

> Thousands of security vulnerabilities are discovered in production software each year, either reported publicly to the Common Vulnerabilities and Exposures database or discovered internally in proprietary code. Vulnerabilities often manifest themselves in subtle ways that are not obvious to code reviewers or the developers themselves. With the wealth of open source code available for analysis, there is an opportunity to learn the patterns of bugs that can lead to security vulnerabilities directly from data. In this paper, we present a data-driven approach to vulnerability detection using machine learning, specifically applied to C and C++ programs. We first compile a large dataset of hundreds of thousands of open-source functions labeled with the outputs of a static analyzer. We then compare methods applied directly to source code with methods applied to artifacts extracted from the build process, finding that source-based models perform better. We also compare the application of deep neural network models with more traditional models such as random forests and find the best performance comes from combining features learned by deep models with tree-based models. Ultimately, our highest performing model achieves an area under the precision-recall curve of 0.49 and an area under the ROC curve of 0.87.

</details>

<details>

<summary>2018-08-02 14:44:08 - Shepherd: Enabling Automatic and Large-Scale Login Security Studies</summary>

- *Hugo Jonker, Jelmer Kalkman, Benjamin Krumnow, Marc Sleegers, Alan Verresen*

- `1808.00840v1` - [abs](http://arxiv.org/abs/1808.00840v1) - [pdf](http://arxiv.org/pdf/1808.00840v1)

> More and more parts of the internet are hidden behind a login field. This poses a barrier to any study predicated on scanning the internet. Moreover, the authentication process itself may be a weak point. To study authentication weaknesses at scale, automated login capabilities are needed. In this work we introduce Shepherd, a scanning framework to automatically log in on websites. The Shepherd framework enables us to perform large-scale scans of post-login aspects of websites. Shepherd scans a website for login fields, attempts to submit credentials and evaluates whether login was successful. We illustrate Shepherd's capabilities by means of a scan for session hijacking susceptibility. In this study, we use a set of unverified website credentials, some of which will be invalid. Using this set, Shepherd is able to fully automatically log in and verify that it is indeed logged in on 6,273 unknown sites, or 12.4% of the test set. We found that from our (biased) test set, 2,579 sites, i.e., 41.4%, are vulnerable to simple session hijacking attacks.

</details>

<details>

<summary>2018-08-02 15:28:08 - Virtualization Technologies and Cloud Security: advantages, issues, and perspectives</summary>

- *Roberto Di Pietro, Flavio Lombardi*

- `1807.11016v2` - [abs](http://arxiv.org/abs/1807.11016v2) - [pdf](http://arxiv.org/pdf/1807.11016v2)

> Virtualization technologies allow multiple tenants to share physical resources with a degree of security and isolation that cannot be guaranteed by mere containerization. Further, virtualization allows protected transparent introspection of Virtual Machine activity and content, thus supporting additional control and monitoring. These features provide an explanation, although partial, of why virtualization has been an enabler for the flourishing of cloud services. Nevertheless, security and privacy issues are still present in virtualization technology and hence in Cloud platforms. As an example, even hardware virtualization protection/isolation is far from being perfect and uncircumventable, as recently discovered vulnerabilities show. The objective of this paper is to shed light on current virtualization technology and its evolution from the point of view of security, having as an objective its applications to the Cloud setting.

</details>

<details>

<summary>2018-08-03 01:03:39 - ContractFuzzer: Fuzzing Smart Contracts for Vulnerability Detection</summary>

- *Bo Jiang, Ye Liu, W. K. Chan*

- `1807.03932v2` - [abs](http://arxiv.org/abs/1807.03932v2) - [pdf](http://arxiv.org/pdf/1807.03932v2)

> Decentralized cryptocurrencies feature the use of blockchain to transfer values among peers on networks without central agency. Smart contracts are programs running on top of the blockchain consensus protocol to enable people make agreements while minimizing trusts. Millions of smart contracts have been deployed in various decentralized applications. The security vulnerabilities within those smart contracts pose significant threats to their applications. Indeed, many critical security vulnerabilities within smart contracts on Ethereum platform have caused huge financial losses to their users. In this work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs based on the ABI specifications of smart contracts, defines test oracles to detect security vulnerabilities, instruments the EVM to log smart contracts runtime behaviors, and analyzes these logs to report security vulnerabilities. Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities with high precision. In particular, our fuzzing tool successfully detects the vulnerability of the DAO contract that leads to USD 60 million loss and the vulnerabilities of Parity Wallet that have led to the loss of $30 million and the freezing of USD 150 million worth of Ether.

</details>

<details>

<summary>2018-08-06 07:26:44 - Gray-box Adversarial Training</summary>

- *Vivek B. S., Konda Reddy Mopuri, R. Venkatesh Babu*

- `1808.01753v1` - [abs](http://arxiv.org/abs/1808.01753v1) - [pdf](http://arxiv.org/pdf/1808.01753v1)

> Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks. In this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed gray-box adversarial attacks" based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial training, named Graybox Adversarial Training" that uses intermediate versions of the models to seed the adversaries. Experimental evaluation demonstrates that the models trained using our method exhibit better robustness compared to both undefended and adversarially trained model

</details>

<details>

<summary>2018-08-06 09:01:41 - Defense Against Adversarial Attacks with Saak Transform</summary>

- *Sibo Song, Yueru Chen, Ngai-Man Cheung, C. -C. Jay Kuo*

- `1808.01785v1` - [abs](http://arxiv.org/abs/1808.01785v1) - [pdf](http://arxiv.org/pdf/1808.01785v1)

> Deep neural networks (DNNs) are known to be vulnerable to adversarial perturbations, which imposes a serious threat to DNN-based decision systems. In this paper, we propose to apply the lossy Saak transform to adversarially perturbed images as a preprocessing tool to defend against adversarial attacks. Saak transform is a recently-proposed state-of-the-art for computing the spatial-spectral representations of input images. Empirically, we observe that outputs of the Saak transform are very discriminative in differentiating adversarial examples from clean ones. Therefore, we propose a Saak transform based preprocessing method with three steps: 1) transforming an input image to a joint spatial-spectral representation via the forward Saak transform, 2) apply filtering to its high-frequency components, and, 3) reconstructing the image via the inverse Saak transform. The processed image is found to be robust against adversarial perturbations. We conduct extensive experiments to investigate various settings of the Saak transform and filtering functions. Without harming the decision performance on clean images, our method outperforms state-of-the-art adversarial defense methods by a substantial margin on both the CIFAR-10 and ImageNet datasets. Importantly, our results suggest that adversarial perturbations can be effectively and efficiently defended using state-of-the-art frequency analysis.

</details>

<details>

<summary>2018-08-06 14:52:26 - Assessing and countering reaction attacks against post-quantum public-key cryptosystems based on QC-LDPC codes</summary>

- *Paolo Santini, Marco Baldi, Franco Chiaraluce*

- `1808.01945v1` - [abs](http://arxiv.org/abs/1808.01945v1) - [pdf](http://arxiv.org/pdf/1808.01945v1)

> Code-based public-key cryptosystems based on QC-LDPC and QC-MDPC codes are promising post-quantum candidates to replace quantum vulnerable classical alternatives. However, a new type of attacks based on Bob's reactions have recently been introduced and appear to significantly reduce the length of the life of any keypair used in these systems. In this paper we estimate the complexity of all known reaction attacks against QC-LDPC and QC-MDPC code-based variants of the McEliece cryptosystem. We also show how the structure of the secret key and, in particular, the secret code rate affect the complexity of these attacks. It follows from our results that QC-LDPC code-based systems can indeed withstand reaction attacks, on condition that some specific decoding algorithms are used and the secret code has a sufficiently high rate.

</details>

<details>

<summary>2018-08-07 03:09:54 - Privacy in Social Media: Identification, Mitigation and Applications</summary>

- *Ghazaleh Beigi, Huan Liu*

- `1808.02191v1` - [abs](http://arxiv.org/abs/1808.02191v1) - [pdf](http://arxiv.org/pdf/1808.02191v1)

> The increasing popularity of social media has attracted a huge number of people to participate in numerous activities on a daily basis. This results in tremendous amounts of rich user-generated data. This data provides opportunities for researchers and service providers to study and better understand users' behaviors and further improve the quality of the personalized services. Publishing user-generated data risks exposing individuals' privacy. Users privacy in social media is an emerging task and has attracted increasing attention in recent years. These works study privacy issues in social media from the two different points of views: identification of vulnerabilities, and mitigation of privacy risks. Recent research has shown the vulnerability of user-generated data against the two general types of attacks, identity disclosure and attribute disclosure. These privacy issues mandate social media data publishers to protect users' privacy by sanitizing user-generated data before publishing it. Consequently, various protection techniques have been proposed to anonymize user-generated social media data. There is a vast literature on privacy of users in social media from many perspectives. In this survey, we review the key achievements of user privacy in social media. In particular, we review and compare the state-of-the-art algorithms in terms of the privacy leakage attacks and anonymization algorithms. We overview the privacy risks from different aspects of social media and categorize the relevant works into five groups 1) graph data anonymization and de-anonymization, 2) author identification, 3) profile attribute disclosure, 4) user location and privacy, and 5) recommender systems and privacy issues. We also discuss open problems and future research directions for user privacy issues in social media.

</details>

<details>

<summary>2018-08-07 04:08:39 - Survey of Automated Vulnerability Detection and Exploit Generation Techniques in Cyber Reasoning Systems</summary>

- *Teresa Nicole Brooks*

- `1702.06162v4` - [abs](http://arxiv.org/abs/1702.06162v4) - [pdf](http://arxiv.org/pdf/1702.06162v4)

> Software is everywhere, from mission critical systems such as industrial power stations, pacemakers and even household appliances. This growing dependence on technology and the increasing complexity software has serious security implications as it means we are potentially surrounded by software that contain exploitable vulnerabilities. These challenges have made binary analysis an important area of research in computer science and has emphasized the need for building automated analysis systems that can operate at scale, speed and efficacy; all while performing with the skill of a human expert. Though great progress has been made in this area of research, there remains limitations and open challenges to be addressed. Recognizing this need, DARPA sponsored the Cyber Grand Challenge (CGC), a competition to showcase the current state of the art in systems that perform; automated vulnerability detection, exploit generation and software patching. This paper is a survey of the vulnerability detection and exploit generation techniques, underlying technologies and related works of two of the winning systems Mayhem and Mechanical Phish.

</details>

<details>

<summary>2018-08-07 18:46:02 - Vehicle Security: Risk Assessment in Transportation</summary>

- *Kaveh Bakhsh Kelarestaghi, Mahsa Foruhandeh, Kevin Heaslip, Ryan Gerdes*

- `1804.07381v2` - [abs](http://arxiv.org/abs/1804.07381v2) - [pdf](http://arxiv.org/pdf/1804.07381v2)

> Intelligent Transportation Systems (ITS) are critical infrastructure that are not immune to both physical and cyber threats. Vehicles are cyber/physical systems which are a core component of ITS, can be either a target or a launching point for an attack on the ITS network. Unknown vehicle security vulnerabilities trigger a race among adversaries to exploit the weaknesses and security experts to mitigate the vulnerability. In this study, we identified opportunities for adversaries to take control of the in-vehicle network, which can compromise the safety, privacy, reliability, efficiency, and security of the transportation system. This study contributes in three ways to the literature of ITS security and resiliency. First, we aggregate individual risks that are associated with hacking the in-vehicle network to determine system-level risk. Second, we employ a risk-based model to conduct a qualitative vulnerability-oriented risk assessment. Third, we identify the consequences of hacking the in-vehicle network through a risk-based approach, using an impact-likelihood matrix. The qualitative assessment communicates risk outcomes for policy analysis. The outcome of this study would be of interest and usefulness to policymakers and engineers concerned with the potential vulnerabilities of the critical infrastructures.

</details>

<details>

<summary>2018-08-08 07:09:44 - Rethinking Misalignment to Raise the Bar for Heap Pointer Corruption</summary>

- *Daehee Jang, Jonghwan Kim, Minjoon Park, Yunjong Jung, Hojoon Lee, Brent Byunghoon Kang*

- `1807.01023v3` - [abs](http://arxiv.org/abs/1807.01023v3) - [pdf](http://arxiv.org/pdf/1807.01023v3)

> Heap layout randomization renders a good portion of heap vulnerabilities unexploitable. However, some remnants of the vulnerabilities are still exploitable even under the randomized layout. According to our analysis, such heap exploits often abuse pointer-width allocation granularity to spray crafted pointers. To address this problem, we explore the efficacy of byte-granularity (the most fine-grained) heap randomization. Heap randomization, in general, has been a well-trodden area; however, the efficacy of byte-granularity randomization has never been fully explored as \emph{misalignment} raises various concerns. This paper unravels the pros and cons of byte-granularity heap randomization by conducting comprehensive analysis in three folds: (i) security effectiveness, (ii) performance impact, and (iii) compatibility analysis to measure deployment cost. Security discussion based on 20 CVE case studies suggests that byte-granularity heap randomization raises the bar against heap exploits more than we initially expected; as pointer spraying approach is becoming prevalent in modern heap exploits. Afterward, to demystify the skeptical concerns regarding misalignment, we conduct cycle-level microbenchmarks and report that the performance cost is highly concentrated to edge cases depending on L1-cache line. Based on such observations, we design and implement an allocator suited to optimize the performance cost of byte-granularity heap randomization; then evaluate the performance with the memory-intensive benchmark (SPEC2006). Finally, we discuss compatibility issues using Coreutils, Nginx, and ChakraCore.

</details>

<details>

<summary>2018-08-08 09:20:13 - Starting Movement Detection of Cyclists Using Smart Devices</summary>

- *Maarten Bieshaar, Malte Depping, Jan Schneegans, Bernhard Sick*

- `1808.04449v1` - [abs](http://arxiv.org/abs/1808.04449v1) - [pdf](http://arxiv.org/pdf/1808.04449v1)

> In near future, vulnerable road users (VRUs) such as cyclists and pedestrians will be equipped with smart devices and wearables which are capable to communicate with intelligent vehicles and other traffic participants. Road users are then able to cooperate on different levels, such as in cooperative intention detection for advanced VRU protection. Smart devices can be used to detect intentions, e.g., an occluded cyclist intending to cross the road, to warn vehicles of VRUs, and prevent potential collisions. This article presents a human activity recognition approach to detect the starting movement of cyclists wearing smart devices. We propose a novel two-stage feature selection procedure using a score specialized for robust starting detection reducing the false positive detections and leading to understandable and interpretable features. The detection is modelled as a classification problem and realized by means of a machine learning classifier. We introduce an auxiliary class, that models starting movements and allows to integrate early movement indicators, i.e., body part movements indicating future behaviour. In this way we improve the robustness and reduce the detection time of the classifier. Our empirical studies with real-world data originating from experiments which involve 49 test subjects and consists of 84 starting motions show that we are able to detect the starting movements early. Our approach reaches an F1-score of 67 % within 0.33 s after the first movement of the bicycle wheel. Investigations concerning the device wearing location show that for devices worn in the trouser pocket the detector has less false detections and detects starting movements faster on average. We found that we can further improve the results when we train distinct classifiers for different wearing locations.

</details>

<details>

<summary>2018-08-09 17:35:52 - Counterfactual Normalization: Proactively Addressing Dataset Shift and Improving Reliability Using Causal Mechanisms</summary>

- *Adarsh Subbaswamy, Suchi Saria*

- `1808.03253v1` - [abs](http://arxiv.org/abs/1808.03253v1) - [pdf](http://arxiv.org/pdf/1808.03253v1)

> Predictive models can fail to generalize from training to deployment environments because of dataset shift, posing a threat to model reliability and the safety of downstream decisions made in practice. Instead of using samples from the target distribution to reactively correct dataset shift, we use graphical knowledge of the causal mechanisms relating variables in a prediction problem to proactively remove relationships that do not generalize across environments, even when these relationships may depend on unobserved variables (violations of the "no unobserved confounders" assumption). To accomplish this, we identify variables with unstable paths of statistical influence and remove them from the model. We also augment the causal graph with latent counterfactual variables that isolate unstable paths of statistical influence, allowing us to retain stable paths that would otherwise be removed. Our experiments demonstrate that models that remove vulnerable variables and use estimates of the latent variables transfer better, often outperforming in the target domain despite some accuracy loss in the training domain.

</details>

<details>

<summary>2018-08-09 23:28:09 - Mining Threat Intelligence about Open-Source Projects and Libraries from Code Repository Issues and Bug Reports</summary>

- *Lorenzo Neil, Sudip Mittal, Anupam Joshi*

- `1808.04673v1` - [abs](http://arxiv.org/abs/1808.04673v1) - [pdf](http://arxiv.org/pdf/1808.04673v1)

> Open-Source Projects and Libraries are being used in software development while also bearing multiple security vulnerabilities. This use of third party ecosystem creates a new kind of attack surface for a product in development. An intelligent attacker can attack a product by exploiting one of the vulnerabilities present in linked projects and libraries.   In this paper, we mine threat intelligence about open source projects and libraries from bugs and issues reported on public code repositories. We also track library and project dependencies for installed software on a client machine. We represent and store this threat intelligence, along with the software dependencies in a security knowledge graph. Security analysts and developers can then query and receive alerts from the knowledge graph if any threat intelligence is found about linked libraries and projects, utilized in their products.

</details>

<details>

<summary>2018-08-10 13:00:45 - Security of GPS/INS based On-road Location Tracking Systems</summary>

- *Sashank Narain, Aanjhan Ranganathan, Guevara Noubir*

- `1808.03515v1` - [abs](http://arxiv.org/abs/1808.03515v1) - [pdf](http://arxiv.org/pdf/1808.03515v1)

> Location information is critical to a wide-variety of navigation and tracking applications. Today, GPS is the de-facto outdoor localization system but has been shown to be vulnerable to signal spoofing attacks. Inertial Navigation Systems (INS) are emerging as a popular complementary system, especially in road transportation systems as they enable improved navigation and tracking as well as offer resilience to wireless signals spoofing, and jamming attacks. In this paper, we evaluate the security guarantees of INS-aided GPS tracking and navigation for road transportation systems. We consider an adversary required to travel from a source location to a destination, and monitored by a INS-aided GPS system. The goal of the adversary is to travel to alternate locations without being detected. We developed and evaluated algorithms that achieve such goal, providing the adversary significant latitude. Our algorithms build a graph model for a given road network and enable us to derive potential destinations an attacker can reach without raising alarms even with the INS-aided GPS tracking and navigation system. The algorithms render the gyroscope and accelerometer sensors useless as they generate road trajectories indistinguishable from plausible paths (both in terms of turn angles and roads curvature). We also designed, built, and demonstrated that the magnetometer can be actively spoofed using a combination of carefully controlled coils. We implemented and evaluated the impact of the attack using both real-world and simulated driving traces in more than 10 cities located around the world. Our evaluations show that it is possible for an attacker to reach destinations that are as far as 30 km away from the true destination without being detected. We also show that it is possible for the adversary to reach almost 60-80% of possible points within the target region in some cities.

</details>

<details>

<summary>2018-08-12 08:26:25 - Adversarial Personalized Ranking for Recommendation</summary>

- *Xiangnan He, Zhankui He, Xiaoyu Du, Tat-Seng Chua*

- `1808.03908v1` - [abs](http://arxiv.org/abs/1808.03908v1) - [pdf](http://arxiv.org/pdf/1808.03908v1)

> Item recommendation is a personalized ranking task. To this end, many recommender systems optimize models with pairwise ranking objectives, such as the Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) --- the most widely used model in recommendation --- as a demonstration, we show that optimizing it with BPR leads to a recommender model that is not robust. In particular, we find that the resultant model is highly vulnerable to adversarial perturbations on its model parameters, which implies the possibly large error in generalization.   To enhance the robustness of a recommender model and thus improve its generalization performance, we propose a new optimization framework, namely Adversarial Personalized Ranking (APR). In short, our APR enhances the pairwise ranking method BPR by performing adversarial training. It can be interpreted as playing a minimax game, where the minimization of the BPR objective function meanwhile defends an adversary, which adds adversarial perturbations on model parameters to maximize the BPR objective function. To illustrate how it works, we implement APR on MF by adding adversarial perturbations on the embedding vectors of users and items. Extensive experiments on three public real-world datasets demonstrate the effectiveness of APR --- by optimizing MF with APR, it outperforms BPR with a relative improvement of 11.2% on average and achieves state-of-the-art performance for item recommendation. Our implementation is available at: https://github.com/hexiangnan/adversarial_personalized_ranking.

</details>

<details>

<summary>2018-08-13 02:27:39 - An Entropy Analysis based Intrusion Detection System for Controller Area Network in Vehicles</summary>

- *Qian Wang, Zhaojun Lu, Gang Qu*

- `1808.04046v1` - [abs](http://arxiv.org/abs/1808.04046v1) - [pdf](http://arxiv.org/pdf/1808.04046v1)

> Dozens of Electronic Control Units (ECUs) can be found on modern vehicles for safety and driving assistance. These ECUs also introduce new security vulnerabilities as recent attacks have been reported by plugging the in-vehicle system or through wireless access. In this paper, we focus on the security of the Controller Area Network (CAN), which is a standard for communication among ECUs. CAN bus by design does not have sufficient security features to protect it from insider or outsider attacks. Intrusion detection system (IDS) is one of the most effective ways to enhance vehicle security on the insecure CAN bus protocol. We propose a new IDS based on the entropy of the identifier bits in CAN messages. The key observation is that all the known CAN message injection attacks need to alter the CAN ID bits and analyzing the entropy of such bits can be an effective way to detect those attacks. We collected real CAN messages from a vehicle (2016 Ford Fusion) and performed simulated message injection attacks. The experimental results showed that our entropy based IDS can successfully detect all the injection attacks without disrupting the communication on CAN.

</details>

<details>

<summary>2018-08-14 23:07:39 - DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems</summary>

- *Lei Ma, Felix Juefei-Xu, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Chunyang Chen, Ting Su, Li Li, Yang Liu, Jianjun Zhao, Yadong Wang*

- `1803.07519v4` - [abs](http://arxiv.org/abs/1803.07519v4) - [pdf](http://arxiv.org/pdf/1803.07519v4)

> Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.

</details>

<details>

<summary>2018-08-16 23:39:55 - Mitigation of Adversarial Attacks through Embedded Feature Selection</summary>

- *Ziyi Bao, Luis Muñoz-González, Emil C. Lupu*

- `1808.05705v1` - [abs](http://arxiv.org/abs/1808.05705v1) - [pdf](http://arxiv.org/pdf/1808.05705v1)

> Machine learning has become one of the main components for task automation in many application domains. Despite the advancements and impressive achievements of machine learning, it has been shown that learning algorithms can be compromised by attackers both at training and test time. Machine learning systems are especially vulnerable to adversarial examples where small perturbations added to the original data points can produce incorrect or unexpected outputs in the learning algorithms at test time. Mitigation of these attacks is hard as adversarial examples are difficult to detect. Existing related work states that the security of machine learning systems against adversarial examples can be weakened when feature selection is applied to reduce the systems' complexity. In this paper, we empirically disprove this idea, showing that the relative distortion that the attacker has to introduce to succeed in the attack is greater when the target is using a reduced set of features. We also show that the minimal adversarial examples differ statistically more strongly from genuine examples with a lower number of features. However, reducing the feature count can negatively impact the system's performance. We illustrate the trade-off between security and accuracy with specific examples. We propose a design methodology to evaluate the security of machine learning classifiers with embedded feature selection against adversarial examples crafted using different attack strategies.

</details>

<details>

<summary>2018-08-19 20:43:46 - SABRE: Protecting Bitcoin against Routing Attacks</summary>

- *Maria Apostolaki, Gian Marti, Jan Müller, Laurent Vanbever*

- `1808.06254v1` - [abs](http://arxiv.org/abs/1808.06254v1) - [pdf](http://arxiv.org/pdf/1808.06254v1)

> Routing attacks remain practically effective in the Internet today as existing countermeasures either fail to provide protection guarantees or are not easily deployable. Blockchain systems are particularly vulnerable to such attacks as they rely on Internet-wide communication to reach consensus. In particular, Bitcoin -the most widely-used cryptocurrency- can be split in half by any AS-level adversary using BGP hijacking. In this paper, we present SABRE, a secure and scalable Bitcoin relay network which relays blocks worldwide through a set of connections that are resilient to routing attacks. SABRE runs alongside the existing peer-to-peer network and is easily deployable. As a critical system, SABRE design is highly resilient and can efficiently handle high bandwidth loads, including Denial of Service attacks. We built SABRE around two key technical insights. First, we leverage fundamental properties of inter-domain routing (BGP) policies to host relay nodes: (i) in locations that are inherently protected against routing attacks; and (ii) on paths that are economically preferred by the majority of Bitcoin clients. These properties are generic and can be used to protect other Blockchain-based systems. Second, we leverage the fact that relaying blocks is communication-heavy, not computation-heavy. This enables us to offload most of the relay operations to programmable network hardware (using the P4 programming language). Thanks to this hardware/software co-design, SABRE nodes operate seamlessly under high load while mitigating the effects of malicious clients. We present a complete implementation of SABRE together with an extensive evaluation. Our results demonstrate that SABRE is effective at securing Bitcoin against routing attacks, even with deployments as small as 6 nodes.

</details>

<details>

<summary>2018-08-20 14:47:05 - Mitigating Branch-Shadowing Attacks on Intel SGX using Control Flow Randomization</summary>

- *Shohreh Hosseinzadeh, Hans Liljestrand, Ville Leppänen, Andrew Paverd*

- `1808.06478v1` - [abs](http://arxiv.org/abs/1808.06478v1) - [pdf](http://arxiv.org/pdf/1808.06478v1)

> Intel Software Guard Extensions (SGX) is a promising hardware-based technology for protecting sensitive computations from potentially compromised system software. However, recent research has shown that SGX is vulnerable to branch-shadowing -- a side channel attack that leaks the fine-grained (branch granularity) control flow of an enclave (SGX protected code), potentially revealing sensitive data to the attacker. The previously-proposed defense mechanism, called Zigzagger, attempted to hide the control flow, but has been shown to be ineffective if the attacker can single-step through the enclave using the recent SGX-Step framework.   Taking into account these stronger attacker capabilities, we propose a new defense against branch-shadowing, based on control flow randomization. Our scheme is inspired by Zigzagger, but provides quantifiable security guarantees with respect to a tunable security parameter. Specifically, we eliminate conditional branches and hide the targets of unconditional branches using a combination of compile-time modifications and run-time code randomization.   We evaluated the performance of our approach by measuring the run-time overhead of ten benchmark programs of SGX-Nbench in SGX environment.

</details>

<details>

<summary>2018-08-20 16:20:34 - The Effect of Security Education and Expertise on Security Assessments: the Case of Software Vulnerabilities</summary>

- *Luca Allodi, Marco Cremonini, Fabio Massacci, Woohyun Shim*

- `1808.06547v1` - [abs](http://arxiv.org/abs/1808.06547v1) - [pdf](http://arxiv.org/pdf/1808.06547v1)

> In spite of the growing importance of software security and the industry demand for more cyber security expertise in the workforce, the effect of security education and experience on the ability to assess complex software security problems has only been recently investigated. As proxy for the full range of software security skills, we considered the problem of assessing the severity of software vulnerabilities by means of a structured analysis methodology widely used in industry (i.e. the Common Vulnerability Scoring System (\CVSS) v3), and designed a study to compare how accurately individuals with background in information technology but different professional experience and education in cyber security are able to assess the severity of software vulnerabilities. Our results provide some structural insights into the complex relationship between education or experience of assessors and the quality of their assessments. In particular we find that individual characteristics matter more than professional experience or formal education; apparently it is the \emph{combination} of skills that one owns (including the actual knowledge of the system under study), rather than the specialization or the years of experience, to influence more the assessment quality. Similarly, we find that the overall advantage given by professional expertise significantly depends on the composition of the individual security skills as well as on the available information.

</details>

<details>

<summary>2018-08-21 16:13:24 - MLPdf: An Effective Machine Learning Based Approach for PDF Malware Detection</summary>

- *Jason Zhang*

- `1808.06991v1` - [abs](http://arxiv.org/abs/1808.06991v1) - [pdf](http://arxiv.org/pdf/1808.06991v1)

> Due to the popularity of portable document format (PDF) and increasing number of vulnerabilities in major PDF viewer applications, malware writers continue to use it to deliver malware via web downloads, email attachments and other methods in both targeted and non-targeted attacks. The topic on how to effectively block malicious PDF documents has received huge research interests in both cyber security industry and academia with no sign of slowing down. In this paper, we propose a novel approach based on a multilayer perceptron (MLP) neural network model, termed MLPdf, for the detection of PDF based malware. More specifically, the MLPdf model uses a backpropagation algorithm with stochastic gradient decent search for model update. A group of high quality features are extracted from two real-world datasets which comprise around 105000 benign and malicious PDF documents. Evaluation results indicate that the proposed MLPdf approach exhibits excellent performance which significantly outperforms all evaluated eight well known commercial anti-virus scanners with a much higher true positive rate of 95.12% achieved while maintaining a very low false positive rate of 0.08%.

</details>

<details>

<summary>2018-08-22 01:19:14 - Optical TEMPEST</summary>

- *Joe Loughry*

- `1808.07175v1` - [abs](http://arxiv.org/abs/1808.07175v1) - [pdf](http://arxiv.org/pdf/1808.07175v1)

> Research on optical TEMPEST has moved forward since 2002 when the first pair of papers on the subject emerged independently and from widely separated locations in the world within a week of each other. Since that time, vulnerabilities have evolved along with systems, and several new threat vectors have consequently appeared. Although the supply chain ecosystem of Ethernet has reduced the vulnerability of billions of devices through use of standardised PHY solutions, other recent trends including the Internet of Things (IoT) in both industrial settings and the general population, High Frequency Trading (HFT) in the financial sector, the European General Data Protection Regulation (GDPR), and inexpensive drones have made it relevant again for consideration in the design of new products for privacy. One of the general principles of security is that vulnerabilities, once fixed, sometimes do not stay that way.

</details>

<details>

<summary>2018-08-22 09:28:14 - Face Flashing: a Secure Liveness Detection Protocol based on Light Reflections</summary>

- *Di Tang, Zhe Zhou, Yinqian Zhang, Kehuan Zhang*

- `1801.01949v2` - [abs](http://arxiv.org/abs/1801.01949v2) - [pdf](http://arxiv.org/pdf/1801.01949v2)

> Face authentication systems are becoming increasingly prevalent, especially with the rapid development of Deep Learning technologies. However, human facial information is easy to be captured and reproduced, which makes face authentication systems vulnerable to various attacks. Liveness detection is an important defense technique to prevent such attacks, but existing solutions did not provide clear and strong security guarantees, especially in terms of time.   To overcome these limitations, we propose a new liveness detection protocol called Face Flashing that significantly increases the bar for launching successful attacks on face authentication systems. By randomly flashing well-designed pictures on a screen and analyzing the reflected light, our protocol has leveraged physical characteristics of human faces: reflection processing at the speed of light, unique textual features, and uneven 3D shapes. Cooperating with working mechanism of the screen and digital cameras, our protocol is able to detect subtle traces left by an attacking process.   To demonstrate the effectiveness of Face Flashing, we implemented a prototype and performed thorough evaluations with large data set collected from real-world scenarios. The results show that our Timing Verification can effectively detect the time gap between legitimate authentications and malicious cases. Our Face Verification can also differentiate 2D plane from 3D objects accurately. The overall accuracy of our liveness detection system is 98.8\%, and its robustness was evaluated in different scenarios. In the worst case, our system's accuracy decreased to a still-high 97.3\%.

</details>

<details>

<summary>2018-08-23 12:12:10 - Adversarial Attacks on Deep-Learning Based Radio Signal Classification</summary>

- *Meysam Sadeghi, Erik G. Larsson*

- `1808.07713v1` - [abs](http://arxiv.org/abs/1808.07713v1) - [pdf](http://arxiv.org/pdf/1808.07713v1)

> Deep learning (DL), despite its enormous success in many computer vision and language processing applications, is exceedingly vulnerable to adversarial attacks. We consider the use of DL for radio signal (modulation) classification tasks, and present practical methods for the crafting of white-box and universal black-box adversarial attacks in that application. We show that these attacks can considerably reduce the classification performance, with extremely small perturbations of the input. In particular, these attacks are significantly more powerful than classical jamming attacks, which raises significant security and robustness concerns in the use of DL-based algorithms for the wireless physical layer.

</details>

<details>

<summary>2018-08-29 00:35:58 - Security and Privacy Analyses of Internet of Things Children's Toys</summary>

- *Gordon Chu, Noah Apthorpe, Nick Feamster*

- `1805.02751v2` - [abs](http://arxiv.org/abs/1805.02751v2) - [pdf](http://arxiv.org/pdf/1805.02751v2)

> This paper investigates the security and privacy of Internet-connected children's smart toys through case studies of three commercially-available products. We conduct network and application vulnerability analyses of each toy using static and dynamic analysis techniques, including application binary decompilation and network monitoring. We discover several publicly undisclosed vulnerabilities that violate the Children's Online Privacy Protection Rule (COPPA) as well as the toys' individual privacy policies. These vulnerabilities, especially security flaws in network communications with first-party servers, are indicative of a disconnect between many IoT toy developers and security and privacy best practices despite increased attention to Internet-connected toy hacking risks.

</details>

<details>

<summary>2018-08-29 12:22:07 - Vulnerable Open Source Dependencies: Counting Those That Matter</summary>

- *Ivan Pashchenko, Henrik Plate, Serena Elisa Ponta, Antonino Sabetta, Fabio Massacci*

- `1808.09753v1` - [abs](http://arxiv.org/abs/1808.09753v1) - [pdf](http://arxiv.org/pdf/1808.09753v1)

> BACKGROUND: Vulnerable dependencies are a known problem in today's open-source software ecosystems because OSS libraries are highly interconnected and developers do not always update their dependencies. AIMS: In this paper we aim to present a precise methodology, that combines the code-based analysis of patches with information on build, test, update dates, and group extracted from the very code repository, and therefore, caters to the needs of industrial practice for correct allocation of development and audit resources. METHOD: To understand the industrial impact of the proposed methodology, we considered the 200 most popular OSS Java libraries used by SAP in its own software. Our analysis included 10905 distinct GAVs (group, artifact, version) when considering all the library versions. RESULTS: We found that about 20% of the dependencies affected by a known vulnerability are not deployed, and therefore, they do not represent a danger to the analyzed library because they cannot be exploited in practice. Developers of the analyzed libraries are able to fix (and actually responsible for) 82% of the deployed vulnerable dependencies. The vast majority (81%) of vulnerable dependencies may be fixed by simply updating to a new version, while 1% of the vulnerable dependencies in our sample are halted, and therefore, potentially require a costly mitigation strategy. CONCLUSIONS: Our case study shows that the correct counting allows software development companies to receive actionable information about their library dependencies, and therefore, correctly allocate costly development and audit resources, which is spent inefficiently in case of distorted measurements.

</details>

<details>

<summary>2018-08-30 14:13:39 - Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation</summary>

- *Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, David Miller*

- `1808.10307v1` - [abs](http://arxiv.org/abs/1808.10307v1) - [pdf](http://arxiv.org/pdf/1808.10307v1)

> Deep learning models have consistently outperformed traditional machine learning models in various classification tasks, including image classification. As such, they have become increasingly prevalent in many real world applications including those where security is of great concern. Such popularity, however, may attract attackers to exploit the vulnerabilities of the deployed deep learning models and launch attacks against security-sensitive applications. In this paper, we focus on a specific type of data poisoning attack, which we refer to as a {\em backdoor injection attack}. The main goal of the adversary performing such attack is to generate and inject a backdoor into a deep learning model that can be triggered to recognize certain embedded patterns with a target label of the attacker's choice. Additionally, a backdoor injection attack should occur in a stealthy manner, without undermining the efficacy of the victim model. Specifically, we propose two approaches for generating a backdoor that is hardly perceptible yet effective in poisoning the model. We consider two attack settings, with backdoor injection carried out either before model training or during model updating. We carry out extensive experimental evaluations under various assumptions on the adversary model, and demonstrate that such attacks can be effective and achieve a high attack success rate (above $90\%$) at a small cost of model accuracy loss (below $1\%$) with a small injection rate (around $1\%$), even under the weakest assumption wherein the adversary has no knowledge either of the original training data or the classifier model.

</details>

<details>

<summary>2018-08-30 20:10:00 - Modified Relay Selection and Circuit Selection for Faster Tor</summary>

- *Mohsen Imani, Mehrdad Amirabadi, Matthew Wright*

- `1608.07343v3` - [abs](http://arxiv.org/abs/1608.07343v3) - [pdf](http://arxiv.org/pdf/1608.07343v3)

> Users of the Tor anonymity system suffer from lessthan- ideal performance, in part because circuit building and selection processes are not tuned for speed. In this paper, we examine both the process of selecting among pre-built circuits and the process of selecting the path of relays for use in building new circuits to improve performance while maintaining anonymity. First, we show that having three pre-built circuits available allows the Tor client to identify fast circuits and improves median time to first byte (TTFB) by 15% over congestion-aware routing, the current state-of-the-art method. Second, we propose a new path selection algorithm that includes broad geographic location information together with bandwidth to reduce delays. In Shadow simulations, we find 20% faster median TTFB and 11% faster median total download times over congestion-aware routing for accessing webpage-sized objects. Our security evaluations show that this approach leads to better or equal security against a generic relay-level adversary compared to Tor, but increased vulnerability to targeted attacks. We explore this trade-off and find settings of our system that offer good performance, modestly better security against a generic adversary, and only slightly more vulnerability to a targeted adversary.

</details>

<details>

<summary>2018-08-31 16:57:46 - Timelines for In-Code Discovery of Zero-Day Vulnerabilities and Supply-Chain Attacks</summary>

- *Andrew J. Lohn*

- `1808.10062v2` - [abs](http://arxiv.org/abs/1808.10062v2) - [pdf](http://arxiv.org/pdf/1808.10062v2)

> Zero-day vulnerabilities can be accidentally or maliciously placed in code and can remain in place for years. In this study, we address an aspect of their longevity by considering the likelihood that they will be discovered in the code across versions. We approximate well-disguised vulnerabilities as only being discoverable if the relevant lines of code are explicitly examined, and obvious vulnerabilities as being discoverable if any part of the relevant file is examined. We analyze the version-to-version changes in three types of open source software (Mozilla Firefox, GNU/Linus, and glibc) to understand the rate at which the various pieces of code are amended and find that much of the revision behavior can be captured with a simple intuitive model. We use that model and the data from over a billion unique lines of code in 87 different versions of software to specify the bounds for in-code discoverability of vulnerabilities - from expertly hidden to obviously observable.

</details>

<details>

<summary>2018-08-31 19:37:20 - Total Recall, Language Processing, and Software Engineering</summary>

- *Zhe Yu, Tim Menzies*

- `1809.00039v1` - [abs](http://arxiv.org/abs/1809.00039v1) - [pdf](http://arxiv.org/pdf/1809.00039v1)

> A broad class of software engineering problems can be generalized as the "total recall problem". This short paper claims that identifying and exploring total recall language processing problems in software engineering is an important task with wide applicability.   To make that case, we show that by applying and adapting the state of the art active learning and text mining, solutions of the total recall problem, can help solve two important software engineering tasks: (a) supporting large literature reviews and (b) identifying software security vulnerabilities. Furthermore, we conjecture that (c) test case prioritization and (d) static warning identification can also be categorized as the total recall problem.   The widespread applicability of "total recall" to software engineering suggests that there exists some underlying framework that encompasses not just natural language processing, but a wide range of important software engineering tasks.

</details>

<details>

<summary>2018-08-31 21:56:00 - Cyber-Security in Smart Grid: Survey and Challenges</summary>

- *Zakaria El Mrabet, Hassan El Ghazi, Naima Kaabouch, Hamid El Ghazi*

- `1809.02609v1` - [abs](http://arxiv.org/abs/1809.02609v1) - [pdf](http://arxiv.org/pdf/1809.02609v1)

> Smart grid uses the power of information technology to intelligently deliver energy to customers by using a two-way communication, and wisely meet the environmental requirements by facilitating the integration of green technologies. Although smart grid addresses several problems of the traditional grid, it faces a number of security challenges. Because communication has been incorporated into the electrical power with its inherent weaknesses, it has exposed the system to numerous risks. Several research papers have discussed these problems. However, most of them classified attacks based on confidentiality, integrity, and availability, and they excluded attacks which compromise other security criteria such as accountability. In addition, the existed security countermeasures focus on countering some specific attacks or protecting some specific components, but there is no global approach which combines these solutions to secure the entire system. The purpose of this paper is to provide a comprehensive overview of the relevant published works. First, we review the security requirements. Then, we investigate in depth a number of important cyber-attacks in smart grid to diagnose the potential vulnerabilities along with their impact. In addition, we proposed a cyber security strategy as a solution to address breaches, counter attacks, and deploy appropriate countermeasures. Finally, we provide some future research directions.

</details>


## 2018-09

<details>

<summary>2018-09-03 20:19:28 - Automatic Heap Layout Manipulation for Exploitation</summary>

- *Sean Heelan, Tom Melham, Daniel Kroening*

- `1804.08470v2` - [abs](http://arxiv.org/abs/1804.08470v2) - [pdf](http://arxiv.org/pdf/1804.08470v2)

> Heap layout manipulation is integral to exploiting heap-based memory corruption vulnerabilities. In this paper we present the first automatic approach to the problem, based on pseudo-random black-box search. Our approach searches for the inputs required to place the source of a heap-based buffer overflow or underflow next to heap-allocated objects that an exploit developer, or automatic exploit generation system, wishes to read or corrupt. We present a framework for benchmarking heap layout manipulation algorithms, and use it to evaluate our approach on several real-world allocators, showing that pseudo-random black box search can be highly effective. We then present SHRIKE, a novel system that can perform automatic heap layout manipulation on the PHP interpreter and can be used in the construction of control-flow hijacking exploits. Starting from PHP's regression tests, SHRIKE discovers fragments of PHP code that interact with the interpreter's heap in useful ways, such as making allocations and deallocations of particular sizes, or allocating objects containing sensitive data, such as pointers. SHRIKE then uses our search algorithm to piece together these fragments into programs, searching for one that achieves a desired heap layout. SHRIKE allows an exploit developer to focus on the higher level concepts in an exploit, and to defer the resolution of heap layout constraints to SHRIKE. We demonstrate this by using SHRIKE in the construction of a control-flow hijacking exploit for the PHP interpreter.

</details>

<details>

<summary>2018-09-04 00:47:50 - HASP: A High-Performance Adaptive Mobile Security Enhancement Against Malicious Speech Recognition</summary>

- *Zirui Xu, Fuxun Yu, Chenchen Liu, Xiang Chen*

- `1809.01697v1` - [abs](http://arxiv.org/abs/1809.01697v1) - [pdf](http://arxiv.org/pdf/1809.01697v1)

> Nowadays, machine learning based Automatic Speech Recognition (ASR) technique has widely spread in smartphones, home devices, and public facilities. As convenient as this technology can be, a considerable security issue also raises -- the users' speech content might be exposed to malicious ASR monitoring and cause severe privacy leakage. In this work, we propose HASP -- a high-performance security enhancement approach to solve this security issue on mobile devices. Leveraging ASR systems' vulnerability to the adversarial examples, HASP is designed to cast human imperceptible adversarial noises to real-time speech and effectively perturb malicious ASR monitoring by increasing the Word Error Rate (WER). To enhance the practical performance on mobile devices, HASP is also optimized for effective adaptation to the human speech characteristics, environmental noises, and mobile computation scenarios. The experiments show that HASP can achieve optimal real-time security enhancement: it can lead an average WER of 84.55% for perturbing the malicious ASR monitoring, and the data processing speed is 15x to 40x faster compared to the state-of-the-art methods. Moreover, HASP can effectively perturb various ASR systems, demonstrating a strong transferability.

</details>

<details>

<summary>2018-09-04 17:59:55 - Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering</summary>

- *Medhini Narasimhan, Alexander G. Schwing*

- `1809.01124v1` - [abs](http://arxiv.org/abs/1809.01124v1) - [pdf](http://arxiv.org/pdf/1809.01124v1)

> Question answering is an important task for autonomous agents and virtual assistants alike and was shown to support the disabled in efficiently navigating an overwhelming environment. Many existing methods focus on observation-based questions, ignoring our ability to seamlessly combine observed content with general knowledge. To understand interactions with a knowledge base, a dataset has been introduced recently and keyword matching techniques were shown to yield compelling results despite being vulnerable to misconceptions due to synonyms and homographs. To address this issue, we develop a learning-based approach which goes straight to the facts via a learned embedding space. We demonstrate state-of-the-art results on the challenging recently introduced fact-based visual question answering dataset, outperforming competing methods by more than 5%.

</details>

<details>

<summary>2018-09-05 05:50:12 - A Threat Modeling Framework for Evaluating Computing Platforms Against Architectural Attacks</summary>

- *Seyyedeh Atefeh Musavi, Mahmoud Reza Hashemi*

- `1809.01335v1` - [abs](http://arxiv.org/abs/1809.01335v1) - [pdf](http://arxiv.org/pdf/1809.01335v1)

> software component misuse a privileged relationship with the hardware to by pass system protections, monitors, or forensic tools. These relationships are often not illegal and exist between system components by design. Hence, even a system with secure hardware and software components, can be architecturally vulnerable. Unfortunately, the existing threat modeling schemes are not applicable for modeling architectural attacks against computing platforms. This is mostly because the existing techniques rely on an abstract representation of a software (.e.g., Data Flow Diagram) as a primary requirement which is not available for a platform as a whole (considering both hardware and software elements). In this paper, we have discussed the necessity of a hardware-software architectural view to system threat modeling. Then, we have proposed Lamellae, a framework adapts threat modeling method to be applicable for untrusted platforms by a holistic approach. Lamellae involves system security architecture for abstract modeling of the platforms. Using the Design structure matrix analysis, Lamellae helps an end-user to identify possible attack vectors against a platform. The framework is a connection point of concepts from system engineering and software security domains. We have applied the framework on a multi-purpose computer with x86-64 architecture as a case-study to show the effectiveness of our framework.

</details>

<details>

<summary>2018-09-05 07:39:23 - Toward Validation of Textual Information Retrieval Techniques for Software Weaknesses</summary>

- *Jukka Ruohonen, Ville Leppänen*

- `1809.01360v1` - [abs](http://arxiv.org/abs/1809.01360v1) - [pdf](http://arxiv.org/pdf/1809.01360v1)

> This paper presents a preliminary validation of common textual information retrieval techniques for mapping unstructured software vulnerability information to distinct software weaknesses. The validation is carried out with a dataset compiled from four software repositories tracked in the Snyk vulnerability database. According to the results, the information retrieval techniques used perform unsatisfactorily compared to regular expression searches. Although the results vary from a repository to another, the preliminary validation presented indicates that explicit referencing of vulnerability and weakness identifiers is preferable for concrete vulnerability tracking. Such referencing allows the use of keyword-based searches, which currently seem to yield more consistent results compared to information retrieval techniques. Further validation work is required for improving the precision of the techniques, however.

</details>

<details>

<summary>2018-09-05 20:16:14 - Bridging machine learning and cryptography in defence against adversarial attacks</summary>

- *Olga Taran, Shideh Rezaeifar, Slava Voloshynovskiy*

- `1809.01715v1` - [abs](http://arxiv.org/abs/1809.01715v1) - [pdf](http://arxiv.org/pdf/1809.01715v1)

> In the last decade, deep learning algorithms have become very popular thanks to the achieved performance in many machine learning and computer vision tasks. However, most of the deep learning architectures are vulnerable to so called adversarial examples. This questions the security of deep neural networks (DNN) for many security- and trust-sensitive domains. The majority of the proposed existing adversarial attacks are based on the differentiability of the DNN cost function.Defence strategies are mostly based on machine learning and signal processing principles that either try to detect-reject or filter out the adversarial perturbations and completely neglect the classical cryptographic component in the defence. In this work, we propose a new defence mechanism based on the second Kerckhoffs's cryptographic principle which states that the defence and classification algorithm are supposed to be known, but not the key. To be compliant with the assumption that the attacker does not have access to the secret key, we will primarily focus on a gray-box scenario and do not address a white-box one. More particularly, we assume that the attacker does not have direct access to the secret block, but (a) he completely knows the system architecture, (b) he has access to the data used for training and testing and (c) he can observe the output of the classifier for each given input. We show empirically that our system is efficient against most famous state-of-the-art attacks in black-box and gray-box scenarios.

</details>

<details>

<summary>2018-09-06 16:27:32 - Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models</summary>

- *Tong Niu, Mohit Bansal*

- `1809.02079v1` - [abs](http://arxiv.org/abs/1809.02079v1) - [pdf](http://arxiv.org/pdf/1809.02079v1)

> We present two categories of model-agnostic adversarial strategies that reveal the weaknesses of several generative, task-oriented dialogue models: Should-Not-Change strategies that evaluate over-sensitivity to small and semantics-preserving edits, as well as Should-Change strategies that test if a model is over-stable against subtle yet semantics-changing modifications. We next perform adversarial training with each strategy, employing a max-margin approach for negative generative examples. This not only makes the target dialogue model more robust to the adversarial inputs, but also helps it perform significantly better on the original inputs. Moreover, training on all strategies combined achieves further improvements, achieving a new state-of-the-art performance on the original task (also verified via human evaluation). In addition to adversarial training, we also address the robustness task at the model-level, by feeding it subword units as both inputs and outputs, and show that the resulting model is equally competitive, requires only 1/4 of the original vocabulary size, and is robust to one of the adversarial strategies (to which the original model is vulnerable) even without adversarial training.

</details>

<details>

<summary>2018-09-07 12:44:19 - Metamorphic Relation Based Adversarial Attacks on Differentiable Neural Computer</summary>

- *Alvin Chan, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Yang Liu, Yew Soon Ong*

- `1809.02444v1` - [abs](http://arxiv.org/abs/1809.02444v1) - [pdf](http://arxiv.org/pdf/1809.02444v1)

> Deep neural networks (DNN), while becoming the driving force of many novel technology and achieving tremendous success in many cutting-edge applications, are still vulnerable to adversarial attacks. Differentiable neural computer (DNC) is a novel computing machine with DNN as its central controller operating on an external memory module for data processing. The unique architecture of DNC contributes to its state-of-the-art performance in tasks which requires the ability to represent variables and data structure as well as to store data over long timescales. However, there still lacks a comprehensive study on how adversarial examples affect DNC in terms of robustness. In this paper, we propose metamorphic relation based adversarial techniques for a range of tasks described in the natural processing language domain. We show that the near-perfect performance of the DNC in bAbI logical question answering tasks can be degraded by adversarially injected sentences. We further perform in-depth study on the role of DNC's memory size in its robustness and analyze the potential reason causing why DNC fails. Our study demonstrates the current challenges and potential opportunities towards constructing more robust DNCs.

</details>

<details>

<summary>2018-09-07 18:31:52 - Pushing the Limits of Encrypted Databases with Secure Hardware</summary>

- *Panagiotis Antonopoulos, Arvind Arasu, Ken Eguro, Joachim Hammer, Raghav Kaushik, Donald Kossmann, Ravi Ramamurthy, Jakub Szymaszek*

- `1809.02631v1` - [abs](http://arxiv.org/abs/1809.02631v1) - [pdf](http://arxiv.org/pdf/1809.02631v1)

> Encrypted databases have been studied for more than 10 years and are quickly emerging as a critical technology for the cloud. The current state of the art is to use property-preserving encrypting techniques (e.g., deterministic encryption) to protect the confidentiality of the data and support query processing at the same time. Unfortunately, these techniques have many limitations. Recently, trusted computing platforms (e.g., Intel SGX) have emerged as an alternative to implement encrypted databases. This paper demonstrates some vulnerabilities and the limitations of this technology, but it also shows how to make best use of it in order to improve on confidentiality, functionality, and performance.

</details>

<details>

<summary>2018-09-07 22:39:49 - Empirical Vulnerability Analysis of Automated Smart Contracts Security Testing on Blockchains</summary>

- *Reza M. Parizi, Ali Dehghantanha, Kim-Kwang Raymond Choo, Amritraj Singh*

- `1809.02702v1` - [abs](http://arxiv.org/abs/1809.02702v1) - [pdf](http://arxiv.org/pdf/1809.02702v1)

> The emerging blockchain technology supports decentralized computing paradigm shift and is a rapidly approaching phenomenon. While blockchain is thought primarily as the basis of Bitcoin, its application has grown far beyond cryptocurrencies due to the introduction of smart contracts. Smart contracts are self-enforcing pieces of software, which reside and run over a hosting blockchain. Using blockchain-based smart contracts for secure and transparent management to govern interactions (authentication, connection, and transaction) in Internet-enabled environments, mostly IoT, is a niche area of research and practice. However, writing trustworthy and safe smart contracts can be tremendously challenging because of the complicated semantics of underlying domain-specific languages and its testability. There have been high-profile incidents that indicate blockchain smart contracts could contain various code-security vulnerabilities, instigating financial harms. When it involves security of smart contracts, developers embracing the ability to write the contracts should be capable of testing their code, for diagnosing security vulnerabilities, before deploying them to the immutable environments on blockchains. However, there are only a handful of security testing tools for smart contracts. This implies that the existing research on automatic smart contracts security testing is not adequate and remains in a very stage of infancy. With a specific goal to more readily realize the application of blockchain smart contracts in security and privacy, we should first understand their vulnerabilities before widespread implementation. Accordingly, the goal of this paper is to carry out a far-reaching experimental assessment of current static smart contracts security testing tools, for the most widely used blockchain, the Ethereum and its domain-specific programming language, Solidity to provide the first...

</details>

<details>

<summary>2018-09-08 19:30:23 - Dynamic Bayesian Games for Adversarial and Defensive Cyber Deception</summary>

- *Linan Huang, Quanyan Zhu*

- `1809.02013v2` - [abs](http://arxiv.org/abs/1809.02013v2) - [pdf](http://arxiv.org/pdf/1809.02013v2)

> Security challenges accompany the efficiency. The pervasive integration of information and communications technologies (ICTs) makes cyber-physical systems vulnerable to targeted attacks that are deceptive, persistent, adaptive and strategic. Attack instances such as Stuxnet, Dyn, and WannaCry ransomware have shown the insufficiency of off-the-shelf defensive methods including the firewall and intrusion detection systems. Hence, it is essential to design up-to-date security mechanisms that can mitigate the risks despite the successful infiltration and the strategic response of sophisticated attackers. In this chapter, we use game theory to model competitive interactions between defenders and attackers. First, we use the static Bayesian game to capture the stealthy and deceptive characteristics of the attacker. A random variable called the \textit{type} characterizes users' essences and objectives, e.g., a legitimate user or an attacker. The realization of the user's type is private information due to the cyber deception. Then, we extend the one-shot simultaneous interaction into the one-shot interaction with asymmetric information structure, i.e., the signaling game. Finally, we investigate the multi-stage transition under a case study of Advanced Persistent Threats (APTs) and Tennessee Eastman (TE) process. Two-Sided incomplete information is introduced because the defender can adopt defensive deception techniques such as honey files and honeypots to create sufficient amount of uncertainties for the attacker. Throughout this chapter, the analysis of the Nash equilibrium (NE), Bayesian Nash equilibrium (BNE), and perfect Bayesian Nash equilibrium (PBNE) enables the policy prediction of the adversary and the design of proactive and strategic defenses to deter attackers and mitigate losses.

</details>

<details>

<summary>2018-09-09 03:49:06 - Towards Query Efficient Black-box Attacks: An Input-free Perspective</summary>

- *Yali Du, Meng Fang, Jinfeng Yi, Jun Cheng, Dacheng Tao*

- `1809.02918v1` - [abs](http://arxiv.org/abs/1809.02918v1) - [pdf](http://arxiv.org/pdf/1809.02918v1)

> Recent studies have highlighted that deep neural networks (DNNs) are vulnerable to adversarial attacks, even in a black-box scenario. However, most of the existing black-box attack algorithms need to make a huge amount of queries to perform attacks, which is not practical in the real world. We note one of the main reasons for the massive queries is that the adversarial example is required to be visually similar to the original image, but in many cases, how adversarial examples look like does not matter much. It inspires us to introduce a new attack called \emph{input-free} attack, under which an adversary can choose an arbitrary image to start with and is allowed to add perceptible perturbations on it. Following this approach, we propose two techniques to significantly reduce the query complexity. First, we initialize an adversarial example with a gray color image on which every pixel has roughly the same importance for the target model. Then we shrink the dimension of the attack space by perturbing a small region and tiling it to cover the input image. To make our algorithm more effective, we stabilize a projected gradient ascent algorithm with momentum, and also propose a heuristic approach for region size selection. Through extensive experiments, we show that with only 1,701 queries on average, we can perturb a gray image to any target class of ImageNet with a 100\% success rate on InceptionV3. Besides, our algorithm has successfully defeated two real-world systems, the Clarifai food detection API and the Baidu Animal Identification API.

</details>

<details>

<summary>2018-09-11 14:18:49 - Detecting Intentions of Vulnerable Road Users Based on Collective Intelligence</summary>

- *Maarten Bieshaar, Günther Reitberger, Stefan Zernetsch, Bernhard Sick, Erich Fuchs, Konrad Doll*

- `1809.03916v1` - [abs](http://arxiv.org/abs/1809.03916v1) - [pdf](http://arxiv.org/pdf/1809.03916v1)

> Vulnerable road users (VRUs, i.e. cyclists and pedestrians) will play an important role in future traffic. To avoid accidents and achieve a highly efficient traffic flow, it is important to detect VRUs and to predict their intentions. In this article a holistic approach for detecting intentions of VRUs by cooperative methods is presented. The intention detection consists of basic movement primitive prediction, e.g. standing, moving, turning, and a forecast of the future trajectory. Vehicles equipped with sensors, data processing systems and communication abilities, referred to as intelligent vehicles, acquire and maintain a local model of their surrounding traffic environment, e.g. crossing cyclists. Heterogeneous, open sets of agents (cooperating and interacting vehicles, infrastructure, e.g. cameras and laser scanners, and VRUs equipped with smart devices and body-worn sensors) exchange information forming a multi-modal sensor system with the goal to reliably and robustly detect VRUs and their intentions under consideration of real time requirements and uncertainties. The resulting model allows to extend the perceptual horizon of the individual agent beyond their own sensory capabilities, enabling a longer forecast horizon. Concealments, implausibilities and inconsistencies are resolved by the collective intelligence of cooperating agents. Novel techniques of signal processing and modelling in combination with analytical and learning based approaches of pattern and activity recognition are used for detection, as well as intention prediction of VRUs. Cooperation, by means of probabilistic sensor and knowledge fusion, takes place on the level of perception and intention recognition. Based on the requirements of the cooperative approach for the communication a new strategy for an ad hoc network is proposed.

</details>

<details>

<summary>2018-09-11 19:50:41 - Poisoning Attacks to Graph-Based Recommender Systems</summary>

- *Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, Jia Liu*

- `1809.04127v1` - [abs](http://arxiv.org/abs/1809.04127v1) - [pdf](http://arxiv.org/pdf/1809.04127v1)

> Recommender system is an important component of many web services to help users locate items that match their interests. Several studies showed that recommender systems are vulnerable to poisoning attacks, in which an attacker injects fake data to a given system such that the system makes recommendations as the attacker desires. However, these poisoning attacks are either agnostic to recommendation algorithms or optimized to recommender systems that are not graph-based. Like association-rule-based and matrix-factorization-based recommender systems, graph-based recommender system is also deployed in practice, e.g., eBay, Huawei App Store. However, how to design optimized poisoning attacks for graph-based recommender systems is still an open problem. In this work, we perform a systematic study on poisoning attacks to graph-based recommender systems. Due to limited resources and to avoid detection, we assume the number of fake users that can be injected into the system is bounded. The key challenge is how to assign rating scores to the fake users such that the target item is recommended to as many normal users as possible. To address the challenge, we formulate the poisoning attacks as an optimization problem, solving which determines the rating scores for the fake users. We also propose techniques to solve the optimization problem. We evaluate our attacks and compare them with existing attacks under white-box (recommendation algorithm and its parameters are known), gray-box (recommendation algorithm is known but its parameters are unknown), and black-box (recommendation algorithm is unknown) settings using two real-world datasets. Our results show that our attack is effective and outperforms existing attacks for graph-based recommender systems. For instance, when 1% fake users are injected, our attack can make a target item recommended to 580 times more normal users in certain scenarios.

</details>

<details>

<summary>2018-09-13 12:35:18 - Query-Efficient Black-Box Attack by Active Learning</summary>

- *Pengcheng Li, Jinfeng Yi, Lijun Zhang*

- `1809.04913v1` - [abs](http://arxiv.org/abs/1809.04913v1) - [pdf](http://arxiv.org/pdf/1809.04913v1)

> Deep neural network (DNN) as a popular machine learning model is found to be vulnerable to adversarial attack. This attack constructs adversarial examples by adding small perturbations to the raw input, while appearing unmodified to human eyes but will be misclassified by a well-trained classifier. In this paper, we focus on the black-box attack setting where attackers have almost no access to the underlying models. To conduct black-box attack, a popular approach aims to train a substitute model based on the information queried from the target DNN. The substitute model can then be attacked using existing white-box attack approaches, and the generated adversarial examples will be used to attack the target DNN. Despite its encouraging results, this approach suffers from poor query efficiency, i.e., attackers usually needs to query a huge amount of times to collect enough information for training an accurate substitute model. To this end, we first utilize state-of-the-art white-box attack methods to generate samples for querying, and then introduce an active learning strategy to significantly reduce the number of queries needed. Besides, we also propose a diversity criterion to avoid the sampling bias. Our extensive experimental results on MNIST and CIFAR-10 show that the proposed method can reduce more than $90\%$ of queries while preserve attacking success rates and obtain an accurate substitute model which is more than $85\%$ similar with the target oracle.

</details>

<details>

<summary>2018-09-13 17:49:46 - A New Secure Network Architecture to Increase Security Among Virtual Machines in Cloud Computing</summary>

- *Zakaria El Mrabet, Hamid El Ghazi, Tayeb Sadiki, Hassan El Ghazi*

- `1809.05528v1` - [abs](http://arxiv.org/abs/1809.05528v1) - [pdf](http://arxiv.org/pdf/1809.05528v1)

> Cloud computing is a new model of computing which provides scalability, flexibility and on-demand service. Virtualization is one of the main components of the cloud, but unfortunately, this technology suffers from many security vulnerabilities. The main purpose of this paper is to present a new secure architecture of Virtual Network machines in order to increase security among virtual machines in a virtualized environment (Xen as a case study). First, we expose the different network modes based on Xen Hypervisor, and then we analyze vulnerabilities and security issues within this kind of environment. Finally, we present in details new secure architecture and demonstrate how it can face the main security network attacks.

</details>

<details>

<summary>2018-09-13 20:26:32 - Defensive Dropout for Hardening Deep Neural Networks under Adversarial Attacks</summary>

- *Siyue Wang, Xiao Wang, Pu Zhao, Wujie Wen, David Kaeli, Peter Chin, Xue Lin*

- `1809.05165v1` - [abs](http://arxiv.org/abs/1809.05165v1) - [pdf](http://arxiv.org/pdf/1809.05165v1)

> Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That is, adversarial examples, obtained by adding delicately crafted distortions onto original legal inputs, can mislead a DNN to classify them as any target labels. This work provides a solution to hardening DNNs under adversarial attacks through defensive dropout. Besides using dropout during training for the best test accuracy, we propose to use dropout also at test time to achieve strong defense effects. We consider the problem of building robust DNNs as an attacker-defender two-player game, where the attacker and the defender know each others' strategies and try to optimize their own strategies towards an equilibrium. Based on the observations of the effect of test dropout rate on test accuracy and attack success rate, we propose a defensive dropout algorithm to determine an optimal test dropout rate given the neural network model and the attacker's strategy for generating adversarial examples.We also investigate the mechanism behind the outstanding defense effects achieved by the proposed defensive dropout. Comparing with stochastic activation pruning (SAP), another defense method through introducing randomness into the DNN model, we find that our defensive dropout achieves much larger variances of the gradients, which is the key for the improved defense effects (much lower attack success rate). For example, our defensive dropout can reduce the attack success rate from 100% to 13.89% under the currently strongest attack i.e., C&W attack on MNIST dataset.

</details>

<details>

<summary>2018-09-14 03:01:48 - S-Mbank: Secure Mobile Banking Authentication Scheme Using Signcryption, Pair Based Text Authentication, and Contactless Smartcard</summary>

- *Dea Saka Kurnia Putra, Mohamad Ali Sadikin, Susila Windarta*

- `1809.05238v1` - [abs](http://arxiv.org/abs/1809.05238v1) - [pdf](http://arxiv.org/pdf/1809.05238v1)

> Nowadays, mobile banking becomes a popular tool which consumers can conduct financial transactions such as shopping, monitoring accounts balance, transferring funds and other payments. Consumers dependency on mobile needs, make people take a little bit more interest in mobile banking. The use of the one-time password which is sent to the user mobile phone by short message service (SMS) is a vulnerability which we want to solve with proposing a new scheme called S-Mbank. We replace the authentication using the one-time password with the contactless smart card to prevent attackers to use the unencrypted message which is sent to the user's mobile phone. Moreover, it deals vulnerability of spoofer to send an SMS pretending as a bank's server. The contactless smart card is proposed because of its flexibility and security which easier to bring in our wallet than the common passcode generators. The replacement of SMS-based authentication with contactless smart card removes the vulnerability of unauthorized users to act as a legitimate user to exploit the mobile banking user's account. Besides that, we use public-private key pair and PIN to provide two factors authentication and mutual authentication. We use signcryption scheme to provide the efficiency of the computation. Pair based text authentication is also proposed for the login process as a solution to shoulder-surfing attack. We use Scyther tool to analyze the security of authentication protocol in S-Mbank scheme. From the proposed scheme, we are able to provide more security protection for mobile banking service.

</details>

<details>

<summary>2018-09-14 21:47:36 - Dynamic Detection of False Data Injection Attack in Smart Grid using Deep Learning</summary>

- *Xiangyu Niu Jiangnan Li, Jinyuan Sun*

- `1808.01094v2` - [abs](http://arxiv.org/abs/1808.01094v2) - [pdf](http://arxiv.org/pdf/1808.01094v2)

> Modern advances in sensor, computing, and communication technologies enable various smart grid applications. The heavy dependence on communication technology has highlighted the vulnerability of the electricity grid to false data injection (FDI) attacks that can bypass bad data detection mechanisms. Existing mitigation in the power system either focus on redundant measurements or protect a set of basic measurements. These methods make specific assumptions about FDI attacks, which are often restrictive and inadequate to deal with modern cyber threats. In the proposed approach, a deep learning based framework is used to detect injected data measurement. Our time-series anomaly detector adopts a Convolutional Neural Network (CNN) and a Long Short Term Memory (LSTM) network. To effectively estimate system variables, our approach observes both data measurements and network level features to jointly learn system states. The proposed system is tested on IEEE 39-bus system. Experimental analysis shows that the deep learning algorithm can identify anomalies which cannot be detected by traditional state estimation bad data detection.

</details>

<details>

<summary>2018-09-16 14:14:36 - Road Detection Technique Using Filters with Application to Autonomous Driving System</summary>

- *Y. O. Agunbiade, J. O. Dehinbo, T. Zuva, A. K. Akanbi*

- `1809.05878v1` - [abs](http://arxiv.org/abs/1809.05878v1) - [pdf](http://arxiv.org/pdf/1809.05878v1)

> Autonomous driving systems are broadly used equipment in the industries and in our daily lives, they assist in production, but are majorly used for exploration in dangerous or unfamiliar locations. Thus, for a successful exploration, navigation plays a significant role. Road detection is an essential factor that assists autonomous robots achieved perfect navigation. Various techniques using camera sensors have been proposed by numerous scholars with inspiring results, but their techniques are still vulnerable to these environmental noises: rain, snow, light intensity and shadow. In addressing these problems, this paper proposed to enhance the road detection system with filtering algorithm to overcome these limitations. Normalized Differences Index (NDI) and morphological operation are the filtering algorithms used to address the effect of shadow and guidance and re-guidance image filtering algorithms are used to address the effect of rain and/or snow, while dark channel image and specular-to-diffuse are the filters used to address light intensity effects. The experimental performance of the road detection system with filtering algorithms was tested qualitatively and quantitatively using the following evaluation schemes: False Negative Rate (FNR) and False Positive Rate (FPR). Comparison results of the road detection system with and without filtering algorithm shows the filtering algorithm's capability to suppress the effect of environmental noises because better road/non-road classification is achieved by the road detection system. with filtering algorithm. This achievement has further improved path planning/region classification for autonomous driving system

</details>

<details>

<summary>2018-09-17 06:54:53 - Attention-Guided Answer Distillation for Machine Reading Comprehension</summary>

- *Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li, Nan Yang, Ming Zhou*

- `1808.07644v4` - [abs](http://arxiv.org/abs/1808.07644v4) - [pdf](http://arxiv.org/pdf/1808.07644v4)

> Despite that current reading comprehension systems have achieved significant advancements, their promising performances are often obtained at the cost of making an ensemble of numerous models. Besides, existing approaches are also vulnerable to adversarial attacks. This paper tackles these problems by leveraging knowledge distillation, which aims to transfer knowledge from an ensemble model to a single model. We first demonstrate that vanilla knowledge distillation applied to answer span prediction is effective for reading comprehension systems. We then propose two novel approaches that not only penalize the prediction on confusing answers but also guide the training with alignment information distilled from the ensemble. Experiments show that our best student model has only a slight drop of 0.4% F1 on the SQuAD test set compared to the ensemble teacher, while running 12x faster during inference. It even outperforms the teacher on adversarial SQuAD datasets and NarrativeQA benchmark.

</details>

<details>

<summary>2018-09-19 05:59:23 - HCIC: Hardware-assisted Control-flow Integrity Checking</summary>

- *Jiliang Zhang, Binhang Qi, Gang Qu*

- `1801.07397v2` - [abs](http://arxiv.org/abs/1801.07397v2) - [pdf](http://arxiv.org/pdf/1801.07397v2)

> Recently, code reuse attacks (CRAs), such as return-oriented programming (ROP) and jump-oriented programming (JOP), have emerged as a new class of ingenious security threatens. Attackers can utilize CRAs to hijack the control flow of programs to perform malicious actions without injecting any codes. Many defenses, classed into software-based and hardware-based, have been proposed. However, software-based methods are difficult to be deployed in practical systems due to high performance overhead. Hardware-based methods can reduce performance overhead but may require extending instruction set architectures (ISAs) and modifying compiler or suffer the vulnerability of key leakage. To tackle these issues, this paper proposes a new hardware-based control flow checking method to resist CRAs with negligible performance overhead without extending ISAs, modifying compiler and leaking the encryption/decryption key. The key technique involves two control flow checking mechanisms. The first one is the encrypted Hamming distances (EHDs) matching between the physical unclonable function (PUF) response and the return addresses, which prevents attackers from returning between gadgets so long as the PUF response is secret, thus resisting ROP attacks. The second one is the liner encryption/decryption operation (XOR) between PUF response and the instructions at target addresses of call and jmp instructions to defeat JOP attacks. Advanced return-based full-function reuse attacks will be prevented with the dynamic key-updating method. Experimental evaluations on benchmarks demonstrate that the proposed method introduces negligible 0.95% run-time overhead and 0.78% binary size overhead on average.

</details>

<details>

<summary>2018-09-19 17:35:03 - A short introduction to secrecy and verifiability for elections</summary>

- *Elizabeth A. Quaglia, Ben Smyth*

- `1702.03168v3` - [abs](http://arxiv.org/abs/1702.03168v3) - [pdf](http://arxiv.org/pdf/1702.03168v3)

> We explore the fundamental properties that are necessary to ensure that election schemes behave as expected. The exploration reveals how our understanding of those expectations has evolved, culminating in the emergence of formal definitions of properties necessary to fulfil expectations. We provide insights into definitions of secrecy and verifiability, allowing us to learn and appreciate the underlying intuition and technical details of these notions.   Equipped with definitions, we can build election schemes that can be proven to behave as expected. And, as an illustrative example, we review a variant of the Helios election system that was built and proven secure, in this way. Furthermore, the definitions can be used to analyse existing election schemes, and vulnerabilities have been uncovered. Indeed, we describe a series of vulnerabilities that were discovered during the analysis of the original Helios system, which advanced our understanding of system behaviour and prompted the design of the aforementioned variant.   Thus, this article contributes to the science of security by sharing valuable insights into elections, and demonstrating the value that formal definitions and analysis have in building schemes guaranteed to behave as expected.

</details>

<details>

<summary>2018-09-21 23:00:39 - Adversarial Recommendation: Attack of the Learned Fake Users</summary>

- *Konstantina Christakopoulou, Arindam Banerjee*

- `1809.08336v1` - [abs](http://arxiv.org/abs/1809.08336v1) - [pdf](http://arxiv.org/pdf/1809.08336v1)

> Can machine learning models for recommendation be easily fooled? While the question has been answered for hand-engineered fake user profiles, it has not been explored for machine learned adversarial attacks. This paper attempts to close this gap.   We propose a framework for generating fake user profiles which, when incorporated in the training of a recommendation system, can achieve an adversarial intent, while remaining indistinguishable from real user profiles. We formulate this procedure as a repeated general-sum game between two players: an oblivious recommendation system $R$ and an adversarial fake user generator $A$ with two goals: (G1) the rating distribution of the fake users needs to be close to the real users, and (G2) some objective $f_A$ encoding the attack intent, such as targeting the top-K recommendation quality of $R$ for a subset of users, needs to be optimized. We propose a learning framework to achieve both goals, and offer extensive experiments considering multiple types of attacks highlighting the vulnerability of recommendation systems.

</details>

<details>

<summary>2018-09-23 19:03:11 - Insufficient properties of image encryption algorithms</summary>

- *Martin Stanek*

- `1809.08661v1` - [abs](http://arxiv.org/abs/1809.08661v1) - [pdf](http://arxiv.org/pdf/1809.08661v1)

> We analyze the security of recently proposed image encryption scheme [1]. We show that the scheme is insecure and the methods used to evaluate its security are insufficient. By designing the Deliberately Weak Cipher, a completely vulnerable cipher with good statistical properties, we illustrate our main point -- a solid analysis cannot be replaced by some selected set of statistical properties.

</details>

<details>

<summary>2018-09-24 02:39:02 - The Sorry State of TLS Security in Enterprise Interception Appliances</summary>

- *Louis Waked, Mohammad Mannan, Amr Youssef*

- `1809.08729v1` - [abs](http://arxiv.org/abs/1809.08729v1) - [pdf](http://arxiv.org/pdf/1809.08729v1)

> Network traffic inspection, including TLS traffic, in enterprise environments is widely practiced. Reasons for doing so are primarily related to improving enterprise security (e.g., malware detection) and meeting legal requirements. To analyze TLS-encrypted data, network appliances implement a Man-in-the-Middle TLS proxy, by acting as the intended web server to a requesting client (e.g., a browser), and acting as the client to the outside web server. As such, the TLS proxy must implement both a TLS client and a server, and handle a large amount of traffic, preferably, in real-time. However, as protocol and implementation layer vulnerabilities in TLS/HTTPS are quite frequent, these proxies must be, at least, as secure as a modern, up-to-date web browser, and a properly configured web server. As opposed to client-end TLS proxies (e.g., as in several anti-virus products), the proxies in network appliances may serve hundreds to thousands of clients, and any vulnerability in their TLS implementations can significantly downgrade enterprise security.   To analyze TLS security of network appliances, we develop a comprehensive framework, by combining and extending tests from existing work on client-end and network-based interception studies. We analyze thirteen representative network appliances over a period of more than a year (including versions before and after notifying affected vendors, a total of 17 versions), and uncover several security issues. For instance, we found that four appliances perform no certificate validation at all, three use pre-generated certificates, and eleven accept certificates signed using MD5, exposing their clients to MITM attacks. Our goal is to highlight the risks introduced by widely-used TLS proxies in enterprise and government environments, potentially affecting many systems hosting security, privacy, and financially sensitive data.

</details>

<details>

<summary>2018-09-24 15:05:01 - On The Utility of Conditional Generation Based Mutual Information for Characterizing Adversarial Subspaces</summary>

- *Chia-Yi Hsu, Pei-Hsuan Lu, Pin-Yu Chen, Chia-Mu Yu*

- `1809.08986v1` - [abs](http://arxiv.org/abs/1809.08986v1) - [pdf](http://arxiv.org/pdf/1809.08986v1)

> Recent studies have found that deep learning systems are vulnerable to adversarial examples; e.g., visually unrecognizable adversarial images can easily be crafted to result in misclassification. The robustness of neural networks has been studied extensively in the context of adversary detection, which compares a metric that exhibits strong discriminate power between natural and adversarial examples. In this paper, we propose to characterize the adversarial subspaces through the lens of mutual information (MI) approximated by conditional generation methods. We use MI as an information-theoretic metric to strengthen existing defenses and improve the performance of adversary detection. Experimental results on MagNet defense demonstrate that our proposed MI detector can strengthen its robustness against powerful adversarial attacks.

</details>

<details>

<summary>2018-09-24 20:29:35 - Generating Natural Language Adversarial Examples</summary>

- *Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang*

- `1804.07998v2` - [abs](http://arxiv.org/abs/1804.07998v2) - [pdf](http://arxiv.org/pdf/1804.07998v2)

> Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations are often virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97% and 70%, respectively. We additionally demonstrate that 92.3% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.

</details>

<details>

<summary>2018-09-25 12:24:00 - Antilizer: Run Time Self-Healing Security for Wireless Sensor Networks</summary>

- *Ivana Tomic, Po Yu Chen, Michael J. Breza, Julie A. McCann*

- `1809.09426v1` - [abs](http://arxiv.org/abs/1809.09426v1) - [pdf](http://arxiv.org/pdf/1809.09426v1)

> Wireless Sensor Network (WSN) applications range from domestic Internet of Things systems like temperature monitoring of homes to the monitoring and control of large-scale critical infrastructures. The greatest risk with the use of WSNs in critical infrastructure is their vulnerability to malicious network level attacks. Their radio communication network can be disrupted, causing them to lose or delay data which will compromise system functionality. This paper presents Antilizer, a lightweight, fully-distributed solution to enable WSNs to detect and recover from common network level attack scenarios. In Antilizer each sensor node builds a self-referenced trust model of its neighbourhood using network overhearing. The node uses the trust model to autonomously adapt its communication decisions. In the case of a network attack, a node can make neighbour collaboration routing decisions to avoid affected regions of the network. Mobile agents further bound the damage caused by attacks. These agents enable a simple notification scheme which propagates collaborative decisions from the nodes to the base station. A filtering mechanism at the base station further validates the authenticity of the information shared by mobile agents. We evaluate Antilizer in simulation against several routing attacks. Our results show that Antilizer reduces data loss down to 1% (4% on average), with operational overheads of less than 1% and provides fast network-wide convergence.

</details>

<details>

<summary>2018-09-25 22:09:15 - Forgetting the Forgotten with Letheia, Concealing Content Deletion from Persistent Observers</summary>

- *Mohsen Minaei, Mainack Mondal, Patrick Loiseau, Krishna Gummadi, Aniket Kate*

- `1710.11271v2` - [abs](http://arxiv.org/abs/1710.11271v2) - [pdf](http://arxiv.org/pdf/1710.11271v2)

> Most social platforms offer mechanisms allowing users to delete their posts, and a significant fraction of users exercise this right to be forgotten. However, ironically, users' attempt to reduce attention to sensitive posts via deletion, in practice, attracts unwanted attention from stalkers specifically to those posts. Thus, deletions may leave users more vulnerable to attacks on their privacy in general. Users hoping to make their posts forgotten face a "damned if I do, damned if I don't" dilemma. Many are shifting towards ephemeral social platform like Snapchat, which will deprive us of important user-data archival. In the form of intermittent withdrawals, we present, Lethe, a novel solution to this problem of forgetting the forgotten. If the next-generation social platforms are willing to give up the uninterrupted availability of non-deleted posts by a very small fraction, Lethe provides privacy to the deleted posts over long durations. In presence of Lethe, an adversarial observer becomes unsure if some posts are permanently deleted or just temporarily withdrawn by Lethe; at the same time, the adversarial observer is overwhelmed by a large number of falsely flagged undeleted posts. To demonstrate the feasibility and performance of Lethe, we analyze large-scale real data about users' deletion over Twitter and thoroughly investigate how to choose time duration distributions for alternating between temporary withdrawals and resurrections of non-deleted posts. We find a favorable trade-off between privacy, availability and adversarial overhead in different settings for users exercising their right to delete. We show that, even against an ultimate adversary with an uninterrupted access to the entire platform, Lethe offers deletion privacy for up to 3 months from the time of deletion, while maintaining content availability as high as 95% and keeping the adversarial precision to 20%.

</details>

<details>

<summary>2018-09-26 10:25:17 - Adversarial Attacks on Cognitive Self-Organizing Networks: The Challenge and the Way Forward</summary>

- *Muhammad Usama, Junaid Qadir, Ala Al-Fuqaha*

- `1810.07242v1` - [abs](http://arxiv.org/abs/1810.07242v1) - [pdf](http://arxiv.org/pdf/1810.07242v1)

> Future communications and data networks are expected to be largely cognitive self-organizing networks (CSON). Such networks will have the essential property of cognitive self-organization, which can be achieved using machine learning techniques (e.g., deep learning). Despite the potential of these techniques, these techniques in their current form are vulnerable to adversarial attacks that can cause cascaded damages with detrimental consequences for the whole network. In this paper, we explore the effect of adversarial attacks on CSON. Our experiments highlight the level of threat that CSON have to deal with in order to meet the challenges of next-generation networks and point out promising directions for future work.

</details>

<details>

<summary>2018-09-27 19:50:11 - SAIL: Machine Learning Guided Structural Analysis Attack on Hardware Obfuscation</summary>

- *Prabuddha Chakraborty, Jonathan Cruz, Swarup Bhunia*

- `1809.10743v1` - [abs](http://arxiv.org/abs/1809.10743v1) - [pdf](http://arxiv.org/pdf/1809.10743v1)

> Obfuscation is a technique for protecting hardware intellectual property (IP) blocks against reverse engineering, piracy, and malicious modifications. Current obfuscation efforts mainly focus on functional locking of a design to prevent black-box usage. They do not directly address hiding design intent through structural transformations, which is an important objective of obfuscation. We note that current obfuscation techniques incorporate only: (1) local, and (2) predictable changes in circuit topology. In this paper, we present SAIL, a structural attack on obfuscation using machine learning (ML) models that exposes a critical vulnerability of these methods. Through this attack, we demonstrate that the gate-level structure of an obfuscated design can be retrieved in most parts through a systematic set of steps. The proposed attack is applicable to all forms of logic obfuscation, and significantly more powerful than existing attacks, e.g., SAT-based attacks, since it does not require the availability of golden functional responses (e.g. an unlocked IC). Evaluation on benchmark circuits show that we can recover an average of around 84% (up to 95%) transformations introduced by obfuscation. We also show that this attack is scalable, flexible, and versatile.

</details>

<details>

<summary>2018-09-28 17:20:54 - Fast Geometrically-Perturbed Adversarial Faces</summary>

- *Ali Dabouei, Sobhan Soleymani, Jeremy Dawson, Nasser M. Nasrabadi*

- `1809.08999v2` - [abs](http://arxiv.org/abs/1809.08999v2) - [pdf](http://arxiv.org/pdf/1809.08999v2)

> The state-of-the-art performance of deep learning algorithms has led to a considerable increase in the utilization of machine learning in security-sensitive and critical applications. However, it has recently been shown that a small and carefully crafted perturbation in the input space can completely fool a deep model. In this study, we explore the extent to which face recognition systems are vulnerable to geometrically-perturbed adversarial faces. We propose a fast landmark manipulation method for generating adversarial faces, which is approximately 200 times faster than the previous geometric attacks and obtains 99.86% success rate on the state-of-the-art face recognition models. To further force the generated samples to be natural, we introduce a second attack constrained on the semantic structure of the face which has the half speed of the first attack with the success rate of 99.96%. Both attacks are extremely robust against the state-of-the-art defense methods with the success rate of equal or greater than 53.59%. Code is available at https://github.com/alldbi/FLM

</details>

<details>

<summary>2018-09-28 20:09:04 - Adversarial Attacks and Defences: A Survey</summary>

- *Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, Debdeep Mukhopadhyay*

- `1810.00069v1` - [abs](http://arxiv.org/abs/1810.00069v1) - [pdf](http://arxiv.org/pdf/1810.00069v1)

> Deep learning has emerged as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. In the last few years, deep learning has advanced radically in such a way that it can surpass human-level performance on a number of tasks. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, security of deep learning systems are vulnerable to crafted adversarial examples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. In this paper, we attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate the efficiency and challenges of recent countermeasures against them.

</details>

<details>

<summary>2018-09-29 20:30:06 - Featurized Bidirectional GAN: Adversarial Defense via Adversarially Learned Semantic Inference</summary>

- *Ruying Bao, Sihang Liang, Qingcan Wang*

- `1805.07862v2` - [abs](http://arxiv.org/abs/1805.07862v2) - [pdf](http://arxiv.org/pdf/1805.07862v2)

> Deep neural networks have been demonstrated to be vulnerable to adversarial attacks, where small perturbations intentionally added to the original inputs can fool the classifier. In this paper, we propose a defense method, Featurized Bidirectional Generative Adversarial Networks (FBGAN), to extract the semantic features of the input and filter the non-semantic perturbation. FBGAN is pre-trained on the clean dataset in an unsupervised manner, adversarially learning a bidirectional mapping between the high-dimensional data space and the low-dimensional semantic space; also mutual information is applied to disentangle the semantically meaningful features. After the bidirectional mapping, the adversarial data can be reconstructed to denoised data, which could be fed into any pre-trained classifier. We empirically show the quality of reconstruction images and the effectiveness of defense.

</details>


## 2018-10

<details>

<summary>2018-10-01 08:00:22 - Stronger Cryptography For Every Device, Everywhere</summary>

- *JV Roig*

- `1810.00567v1` - [abs](http://arxiv.org/abs/1810.00567v1) - [pdf](http://arxiv.org/pdf/1810.00567v1)

> Generating secure random numbers is a central problem in cryptography that needs a reliable source of enough computing entropy. Without enough entropy available - meaning no good source of secure random numbers - a device is susceptible to cryptographic protocol failures such as weak, factorable, or predictable keys, which lead to various security and privacy vulnerabilities. In this paper, the author presents a significant improvement: a reliable way for any CPU-powered device - from the small, simple CPUs in embedded devices, to larger, more complex CPUs in modern servers - to collect virtually unlimited entropy through side channel measurements of trivial CPU operations, making the generation of secure random numbers an easy, safe, and reliable operation.

</details>

<details>

<summary>2018-10-01 12:50:27 - CertLedger: A New PKI Model with Certificate Transparency Based on Blockchain</summary>

- *Murat Yasin Kubilay, Mehmet Sabir Kiraz, Haci Ali Mantar*

- `1806.03914v2` - [abs](http://arxiv.org/abs/1806.03914v2) - [pdf](http://arxiv.org/pdf/1806.03914v2)

> In conventional PKI, CAs are assumed to be fully trusted. However, in practice, CAs' absolute responsibility for providing trustworthiness caused major security and privacy issues. To prevent such issues, Google introduced the concept of Certificate Transparency (CT) in 2013. Later, several new PKI models (e.g., AKI, ARPKI, and DTKI) are proposed to reduce the level of trust to the CAs. However, all of these proposals are still vulnerable to split-world attacks if the adversary is capable of showing different views of the log to the targeted victims. In this paper, we propose a new PKI architecture with certificate transparency based on blockchain, what we called CertLedger, to eliminate the split-world attacks and to provide an ideal certificate/revocation transparency. All TLS certificates, their revocation status, entire revocation process, and trusted CA management are conducted in the CertLedger. CertLedger provides a unique, efficient, and trustworthy certificate validation process eliminating the conventional inadequate and incompatible certificate validation processes implemented by different software vendors. TLS clients in the CertLedger also do not require to make certificate validation and store the trusted CA certificates anymore. We analyze the security and performance of the CertLedger and provide a comparison with the previous proposals.

</details>

<details>

<summary>2018-10-01 23:57:58 - Fighting Against XSS Attacks: A Usability Evaluation of OWASP ESAPI Output Encoding</summary>

- *Chamila Wijayarathna, Nalin Asanka Gamagedara Arachchilage*

- `1810.01017v1` - [abs](http://arxiv.org/abs/1810.01017v1) - [pdf](http://arxiv.org/pdf/1810.01017v1)

> Cross Site Scripting (XSS) is one of the most critical vulnerabilities exist in web applications. XSS can be prevented by encoding untrusted data that are loaded into browser content of web applications. Security Application Programming Interfaces (APIs) such as OWASP ESAPI provide output encoding functionalities for programmers to use to protect their applications from XSS attacks. However, XSS still being ranked as one of the most critical vulnerabilities in web applications suggests that programmers are not effectively using those APIs to encode untrusted data. Therefore, we conducted an experimental study with 10 programmers where they attempted to fix XSS vulnerabilities of a web application using the output encoding functionality of OWASP ESAPI. Results revealed 3 types of mistakes that programmers made which resulted in them failing to fix the application by removing XSS vulnerabilities. We also identified 16 usability issues of OWASP ESAPI. We identified that some of these usability issues as the reason for mistakes that programmers made. Based on these results, we provided suggestions on how the usability of output encoding APIs should be improved to give a better experience to programmers.

</details>

<details>

<summary>2018-10-03 02:59:39 - Distributing and Obfuscating Firewalls via Oblivious Bloom Filter Evaluation</summary>

- *Ken Goss, Wei Jiang*

- `1810.01571v1` - [abs](http://arxiv.org/abs/1810.01571v1) - [pdf](http://arxiv.org/pdf/1810.01571v1)

> Firewalls have long been in use to protect local networks from threats of the larger Internet. Although firewalls are effective in preventing attacks initiated from outside, they are vulnerable to insider threats, e.g., malicious insiders may access and alter firewall configurations, and disable firewall services. In this paper, we develop an innovative distributed architecture to obliviously manage and evaluate firewalls to prevent both insider and external attacks oriented to the firewalls. Our proposed structure alleviates these issues by obfuscating the firewall rules or policies themselves, then distributing the function of evaluating these rules across multiple servers. Thus, both accessing and altering the rules are considerably more difficult thereby providing better protection to the local network as well as greater security for the firewall itself. We achieve this by integrating multiple areas of research such as secret sharing schemes and multi-party computation, as well as Bloom filters and Byzantine agreement protocols. Our resulting solution is an efficient and secure means by which a firewall may be distributed, and obfuscated while maintaining the ability for multiple servers to obliviously evaluate its functionality.

</details>

<details>

<summary>2018-10-03 03:50:41 - Is Ordered Weighted $\ell_1$ Regularized Regression Robust to Adversarial Perturbation? A Case Study on OSCAR</summary>

- *Pin-Yu Chen, Bhanukiran Vinzamuri, Sijia Liu*

- `1809.08706v2` - [abs](http://arxiv.org/abs/1809.08706v2) - [pdf](http://arxiv.org/pdf/1809.08706v2)

> Many state-of-the-art machine learning models such as deep neural networks have recently shown to be vulnerable to adversarial perturbations, especially in classification tasks. Motivated by adversarial machine learning, in this paper we investigate the robustness of sparse regression models with strongly correlated covariates to adversarially designed measurement noises. Specifically, we consider the family of ordered weighted $\ell_1$ (OWL) regularized regression methods and study the case of OSCAR (octagonal shrinkage clustering algorithm for regression) in the adversarial setting. Under a norm-bounded threat model, we formulate the process of finding a maximally disruptive noise for OWL-regularized regression as an optimization problem and illustrate the steps towards finding such a noise in the case of OSCAR. Experimental results demonstrate that the regression performance of grouping strongly correlated features can be severely degraded under our adversarial setting, even when the noise budget is significantly smaller than the ground-truth signals.

</details>

<details>

<summary>2018-10-03 22:43:58 - Controlling Over-generalization and its Effect on Adversarial Examples Generation and Detection</summary>

- *Mahdieh Abbasi, Arezoo Rajabi, Azadeh Sadat Mozafari, Rakesh B. Bobba, Christian Gagne*

- `1808.08282v2` - [abs](http://arxiv.org/abs/1808.08282v2) - [pdf](http://arxiv.org/pdf/1808.08282v2)

> Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.

</details>

<details>

<summary>2018-10-05 11:04:06 - Spatially-weighted Anomaly Detection</summary>

- *Minori Narita, Daiki Kimura, Ryuki Tachibana*

- `1810.02607v1` - [abs](http://arxiv.org/abs/1810.02607v1) - [pdf](http://arxiv.org/pdf/1810.02607v1)

> Many types of anomaly detection methods have been proposed recently, and applied to a wide variety of fields including medical screening and production quality checking. Some methods have utilized images, and, in some cases, a part of the anomaly images is known beforehand. However, this kind of information is dismissed by previous methods, because the methods can only utilize a normal pattern. Moreover, the previous methods suffer a decrease in accuracy due to negative effects from surrounding noises. In this study, we propose a spatially-weighted anomaly detection method (SPADE) that utilizes all of the known patterns and lessens the vulnerability to ambient noises by applying Grad-CAM, which is the visualization method of a CNN. We evaluated our method quantitatively using two datasets, the MNIST dataset with noise and a dataset based on a brief screening test for dementia.

</details>

<details>

<summary>2018-10-05 18:07:23 - Physical Adversarial Examples for Object Detectors</summary>

- *Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash, Tadayoshi Kohno, Dawn Song*

- `1807.07769v2` - [abs](http://arxiv.org/abs/1807.07769v2) - [pdf](http://arxiv.org/pdf/1807.07769v2)

> Deep neural networks (DNNs) are vulnerable to adversarial examples-maliciously crafted inputs that cause DNNs to make incorrect predictions. Recent work has shown that these attacks generalize to the physical domain, to create perturbations on physical objects that fool image classifiers under a variety of real-world conditions. Such attacks pose a risk to deep learning models used in safety-critical cyber-physical systems. In this work, we extend physical attacks to more challenging object detection models, a broader class of deep learning algorithms widely used to detect and label multiple objects within a scene. Improving upon a previous physical attack on image classifiers, we create perturbed physical objects that are either ignored or mislabeled by object detection models. We implement a Disappearance Attack, in which we cause a Stop sign to "disappear" according to the detector-either by covering thesign with an adversarial Stop sign poster, or by adding adversarial stickers onto the sign. In a video recorded in a controlled lab environment, the state-of-the-art YOLOv2 detector failed to recognize these adversarial Stop signs in over 85% of the video frames. In an outdoor experiment, YOLO was fooled by the poster and sticker attacks in 72.5% and 63.5% of the video frames respectively. We also use Faster R-CNN, a different object detection model, to demonstrate the transferability of our adversarial perturbations. The created poster perturbation is able to fool Faster R-CNN in 85.9% of the video frames in a controlled lab environment, and 40.2% of the video frames in an outdoor environment. Finally, we present preliminary results with a new Creation Attack, where in innocuous physical stickers fool a model into detecting nonexistent objects.

</details>

<details>

<summary>2018-10-08 01:37:39 - Interface-Based Side Channel Attack Against Intel SGX</summary>

- *Jinwen Wang, Yueqiang Cheng, Qi Li, Yong Jiang*

- `1811.05378v1` - [abs](http://arxiv.org/abs/1811.05378v1) - [pdf](http://arxiv.org/pdf/1811.05378v1)

> Intel has introduced a trusted computing technology, Intel Software Guard Extension (SGX), which provides an isolated and secure execution environment called enclave for a user program without trusting any privilege software (e.g., an operating system or a hypervisor) or firmware. Nevertheless, SGX is vulnerable to several side channel attacks (e.g. page-fault-based attack and cache-based attack). In this paper, we explore a new, yet critical side channel attack in SGX, interface-based side channel attack, which can infer the information of the enclave input data. The root cause of the interface-based side channel attack is the input dependent interface invocation information (e.g., interface information and invocation patterns) which can be observed by the untrusted privilege software can reveal the control flow in the enclave. We study the methodology which can be used to conduct the interface-based side channel attack. To illustrate the effectiveness of the interface-based side-channel attacks, we use our methodology to infer whether tracked web pages have been processed by the SGX-assisted NFV platforms and achieve the accuracy of 87.6% and recall of 76.6%. We also identify the packets which belong to the tracked web pages, with the accuracy of 67.9%and recall of 71.1%. We finally propose some countermeasures to defense the interface-based side channel attack in SGX-assisted applications.

</details>

<details>

<summary>2018-10-08 23:00:06 - Efficient Two-Step Adversarial Defense for Deep Neural Networks</summary>

- *Ting-Jui Chang, Yukun He, Peng Li*

- `1810.03739v1` - [abs](http://arxiv.org/abs/1810.03739v1) - [pdf](http://arxiv.org/pdf/1810.03739v1)

> In recent years, deep neural networks have demonstrated outstanding performance in many machine learning tasks. However, researchers have discovered that these state-of-the-art models are vulnerable to adversarial examples: legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples during the training process, is a well known defense to improve the robustness of the model against adversarial attacks. However, this robustness is only effective to the same attack method used for adversarial training. Madry et al.(2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularly that projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistance against many other first order attacks. However, the computational cost of the adversarial training with PGD and other multi-step adversarial examples is much higher than that of the adversarial training with other simpler attack techniques. In this paper, we show how strong adversarial examples can be generated only at a cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to that of the adversarial training with multi-step adversarial examples. We empirically demonstrate the effectiveness of the proposed two-step defense approach against different attack methods and its improvements over existing defense strategies.

</details>

<details>

<summary>2018-10-09 08:50:39 - BabelView: Evaluating the Impact of Code Injection Attacks in Mobile Webviews</summary>

- *Claudio Rizzo, Lorenzo Cavallaro, Johannes Kinder*

- `1709.05690v2` - [abs](http://arxiv.org/abs/1709.05690v2) - [pdf](http://arxiv.org/pdf/1709.05690v2)

> A Webview embeds a full-fledged browser in a mobile application and allows the application to expose a custom interface to JavaScript code. This is a popular technique to build so-called hybrid applications, but it circumvents the usual security model of the browser: any malicious JavaScript code injected into the Webview gains access to the interface and can use it to manipulate the device or exfiltrate sensitive data. In this paper, we present an approach to systematically evaluate the possible impact of code injection attacks against Webviews using static information flow analysis. Our key idea is that we can make reasoning about JavaScript semantics unnecessary by instrumenting the application with a model of possible attacker behavior -- the BabelView. We evaluate our approach on 11,648 apps from various Android marketplaces, finding 2,677 vulnerabilities in 1,663 apps. Taken together, the apps reported as vulnerable have over 835 million installations worldwide. We manually validated a random sample of 66 apps and estimate that our fully automated analysis achieves a precision of 90% at a recall of 66%.

</details>

<details>

<summary>2018-10-09 11:13:44 - Analyzing the Noise Robustness of Deep Neural Networks</summary>

- *Mengchen Liu, Shixia Liu, Hang Su, Kelei Cao, Jun Zhu*

- `1810.03913v1` - [abs](http://arxiv.org/abs/1810.03913v1) - [pdf](http://arxiv.org/pdf/1810.03913v1)

> Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.

</details>

<details>

<summary>2018-10-10 11:27:40 - You Shall Not Bypass: Employing data dependencies to prevent Bounds Check Bypass</summary>

- *Oleksii Oleksenko, Bohdan Trach, Tobias Reiher, Mark Silberstein, Christof Fetzer*

- `1805.08506v3` - [abs](http://arxiv.org/abs/1805.08506v3) - [pdf](http://arxiv.org/pdf/1805.08506v3)

> A recent discovery of a new class of microarchitectural attacks called Spectre picked up the attention of the security community as these attacks can circumvent many traditional mechanisms of defense. One of the attacks---Bounds Check Bypass---can neither be efficiently solved on system nor architectural levels and requires changes in the application itself. So far, the proposed mitigations involved serialization, which reduces the usage of CPU resources and causes high overheads. In this report, we explore methods of delaying the vulnerable instructions without complete serialization. We discuss several ways of achieving it and compare them with Speculative Load Hardening, an existing solution based on a similar idea. The solutions of this type cause 60% overhead across Phoenix benchmark suite, which compares favorably to the full serialization causing 440% slowdown.

</details>

<details>

<summary>2018-10-10 14:04:08 - Secure Deep Learning Engineering: A Software Quality Assurance Perspective</summary>

- *Lei Ma, Felix Juefei-Xu, Minhui Xue, Qiang Hu, Sen Chen, Bo Li, Yang Liu, Jianjun Zhao, Jianxiong Yin, Simon See*

- `1810.04538v1` - [abs](http://arxiv.org/abs/1810.04538v1) - [pdf](http://arxiv.org/pdf/1810.04538v1)

> Over the past decades, deep learning (DL) systems have achieved tremendous success and gained great popularity in various applications, such as intelligent machines, image processing, speech processing, and medical diagnostics. Deep neural networks are the key driving force behind its recent success, but still seem to be a magic black box lacking interpretability and understanding. This brings up many open safety and security issues with enormous and urgent demands on rigorous methodologies and engineering practice for quality enhancement. A plethora of studies have shown that the state-of-the-art DL systems suffer from defects and vulnerabilities that can lead to severe loss and tragedies, especially when applied to real-world safety-critical applications. In this paper, we perform a large-scale study and construct a paper repository of 223 relevant works to the quality assurance, security, and interpretation of deep learning. We, from a software quality assurance perspective, pinpoint challenges and future opportunities towards universal secure deep learning engineering. We hope this work and the accompanied paper repository can pave the path for the software engineering community towards addressing the pressing industrial demand of secure intelligent applications.

</details>

<details>

<summary>2018-10-10 21:42:29 - Leveraging Textual Specifications for Grammar-based Fuzzing of Network Protocols</summary>

- *Samuel Jero, Maria Leonor Pacheco, Dan Goldwasser, Cristina Nita-Rotaru*

- `1810.04755v1` - [abs](http://arxiv.org/abs/1810.04755v1) - [pdf](http://arxiv.org/pdf/1810.04755v1)

> Grammar-based fuzzing is a technique used to find software vulnerabilities by injecting well-formed inputs generated following rules that encode application semantics. Most grammar-based fuzzers for network protocols rely on human experts to manually specify these rules. In this work we study automated learning of protocol rules from textual specifications (i.e. RFCs). We evaluate the automatically extracted protocol rules by applying them to a state-of-the-art fuzzer for transport protocols and show that it leads to a smaller number of test cases while finding the same attacks as the system that uses manually specified rules.

</details>

<details>

<summary>2018-10-11 13:00:52 - On the Learning of Deep Local Features for Robust Face Spoofing Detection</summary>

- *Gustavo Botelho de Souza, João Paulo Papa, Aparecido Nilceu Marana*

- `1806.07492v2` - [abs](http://arxiv.org/abs/1806.07492v2) - [pdf](http://arxiv.org/pdf/1806.07492v2)

> Biometrics emerged as a robust solution for security systems. However, given the dissemination of biometric applications, criminals are developing techniques to circumvent them by simulating physical or behavioral traits of legal users (spoofing attacks). Despite face being a promising characteristic due to its universality, acceptability and presence of cameras almost everywhere, face recognition systems are extremely vulnerable to such frauds since they can be easily fooled with common printed facial photographs. State-of-the-art approaches, based on Convolutional Neural Networks (CNNs), present good results in face spoofing detection. However, these methods do not consider the importance of learning deep local features from each facial region, even though it is known from face recognition that each facial region presents different visual aspects, which can also be exploited for face spoofing detection. In this work we propose a novel CNN architecture trained in two steps for such task. Initially, each part of the neural network learns features from a given facial region. Afterwards, the whole model is fine-tuned on the whole facial images. Results show that such pre-training step allows the CNN to learn different local spoofing cues, improving the performance and the convergence speed of the final model, outperforming the state-of-the-art approaches.

</details>

<details>

<summary>2018-10-11 16:04:51 - A methodology to Evaluate the Usability of Security APIs</summary>

- *Chamila Wijayarathna, Nalin Asanka Gamagedara Arachchilage*

- `1810.05100v1` - [abs](http://arxiv.org/abs/1810.05100v1) - [pdf](http://arxiv.org/pdf/1810.05100v1)

> Increasing number of cyber-attacks demotivate people to use Information and Communication Technology (ICT) for industrial as well as day to day work. A main reason for the increasing number of cyber-attacks is mistakes that programmers make while developing software applications that are caused by usability issues exist in security Application Programming Interfaces (APIs). These mistakes make software vulnerable to cyber-attacks. In this paper, we attempt to take a step closer to solve this problem by proposing a methodology to evaluate the usability and identify usability issues exist in security APIs. By conducting a review of previous research, we identified 5 usability evaluation methodologies that have been proposed to evaluate the usability of general APIs and characteristics of those methodologies that would affect when using these methodologies to evaluate security APIs. Based on the findings, we propose a methodology to evaluate the usability of security APIs.

</details>

<details>

<summary>2018-10-11 17:03:44 - Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation</summary>

- *Chaowei Xiao, Ruizhi Deng, Bo Li, Fisher Yu, Mingyan Liu, Dawn Song*

- `1810.05162v1` - [abs](http://arxiv.org/abs/1810.05162v1) - [pdf](http://arxiv.org/pdf/1810.05162v1)

> Deep Neural Networks (DNNs) have been widely applied in various recognition tasks. However, recently DNNs have been shown to be vulnerable against adversarial examples, which can mislead DNNs to make arbitrary incorrect predictions. While adversarial examples are well studied in classification tasks, other learning problems may have different properties. For instance, semantic segmentation requires additional components such as dilated convolutions and multiscale processing. In this paper, we aim to characterize adversarial examples based on spatial context information in semantic segmentation. We observe that spatial consistency information can be potentially leveraged to detect adversarial examples robustly even when a strong adaptive attacker has access to the model and detection strategies. We also show that adversarial examples based on attacks considered within the paper barely transfer among models, even though transferability is common in classification. Our observations shed new light on developing adversarial attacks and defenses to better understand the vulnerabilities of DNNs.

</details>

<details>

<summary>2018-10-14 17:08:54 - Robust Neural Abstractive Summarization Systems and Evaluation against Adversarial Information</summary>

- *Lisa Fan, Dong Yu, Lu Wang*

- `1810.06065v1` - [abs](http://arxiv.org/abs/1810.06065v1) - [pdf](http://arxiv.org/pdf/1810.06065v1)

> Sequence-to-sequence (seq2seq) neural models have been actively investigated for abstractive summarization. Nevertheless, existing neural abstractive systems frequently generate factually incorrect summaries and are vulnerable to adversarial information, suggesting a crucial lack of semantic understanding. In this paper, we propose a novel semantic-aware neural abstractive summarization model that learns to generate high quality summaries through semantic interpretation over salient content. A novel evaluation scheme with adversarial samples is introduced to measure how well a model identifies off-topic information, where our model yields significantly better performance than the popular pointer-generator summarizer. Human evaluation also confirms that our system summaries are uniformly more informative and faithful as well as less redundant than the seq2seq model.

</details>

<details>

<summary>2018-10-16 14:59:55 - ShieldScatter: Improving IoT Security with Backscatter Assistance</summary>

- *Zhiqing Luo, Wei Wang, Jun Qu, Tao Jiang, Qian Zhang*

- `1810.07058v1` - [abs](http://arxiv.org/abs/1810.07058v1) - [pdf](http://arxiv.org/pdf/1810.07058v1)

> The lightweight protocols and low-power radio technologies open up many opportunities to facilitate Internet-of-Things (IoT) into our daily life, while their minimalist design also makes IoT devices vulnerable to many active attacks due to the lack of sophisticated security protocols. Recent advances advocate the use of an antenna array to extract fine-grained physical-layer signatures to mitigate these active attacks. However, it adds burdens in terms of energy consumption and hardware cost that IoT devices cannot afford. To overcome this predicament, we present ShieldScatter, a lightweight system that attaches battery-free backscatter tags to single-antenna devices to shield the system from active attacks. The key insight of ShieldScatter is to intentionally create multi-path propagation signatures with the careful deployment of backscatter tags. These signatures can be used to construct a sensitive profile to identify the location of the signals' arrival, and thus detect the threat. We prototype ShieldScatter with USRPs and ambient backscatter tags to evaluate our system in various environments. The experimental results show that even when the attacker is located only 15 cm away from the legitimate device, ShieldScatter with merely three backscatter tags can mitigate 97% of spoofing attack attempts while at the same time trigger false alarms on just 7% of legitimate traffic.

</details>

<details>

<summary>2018-10-18 21:29:47 - DeepMasterPrints: Generating MasterPrints for Dictionary Attacks via Latent Variable Evolution</summary>

- *Philip Bontrager, Aditi Roy, Julian Togelius, Nasir Memon, Arun Ross*

- `1705.07386v4` - [abs](http://arxiv.org/abs/1705.07386v4) - [pdf](http://arxiv.org/pdf/1705.07386v4)

> Recent research has demonstrated the vulnerability of fingerprint recognition systems to dictionary attacks based on MasterPrints. MasterPrints are real or synthetic fingerprints that can fortuitously match with a large number of fingerprints thereby undermining the security afforded by fingerprint systems. Previous work by Roy et al. generated synthetic MasterPrints at the feature-level. In this work we generate complete image-level MasterPrints known as DeepMasterPrints, whose attack accuracy is found to be much superior than that of previous methods. The proposed method, referred to as Latent Variable Evolution, is based on training a Generative Adversarial Network on a set of real fingerprint images. Stochastic search in the form of the Covariance Matrix Adaptation Evolution Strategy is then used to search for latent input variables to the generator network that can maximize the number of impostor matches as assessed by a fingerprint recognizer. Experiments convey the efficacy of the proposed method in generating DeepMasterPrints. The underlying method is likely to have broad applications in fingerprint security as well as fingerprint synthesis.

</details>

<details>

<summary>2018-10-19 09:14:27 - IoT-KEEPER: Securing IoT Communications in Edge Networks</summary>

- *Ibbad Hafeez, Markku Antikainen, Aaron Yi Ding, Sasu Tarkoma*

- `1810.08415v1` - [abs](http://arxiv.org/abs/1810.08415v1) - [pdf](http://arxiv.org/pdf/1810.08415v1)

> The increased popularity of IoT devices have made them lucrative targets for attackers. Due to insecure product development practices, these devices are often vulnerable even to very trivial attacks and can be easily compromised. Due to the sheer number and heterogeneity of IoT devices, it is not possible to secure the IoT ecosystem using traditional endpoint and network security solutions. To address the challenges and requirements of securing IoT devices in edge networks, we present IoT-Keeper, which is a novel system capable of securing the network against any malicious activity, in real time. The proposed system uses a lightweight anomaly detection technique, to secure both device-to-device and device-to-infrastructure communications, while using limited resources available on the gateway. It uses unlabeled network data to distinguish between benign and malicious traffic patterns observed in the network. A detailed evaluation, done with real world testbed, shows that IoT-Keeper detects any device generating malicious traffic with high accuracy (0.982) and low false positive rate (0.01). The results demonstrate that IoT-Keeper is lightweight, responsive and can effectively handle complex D2D interactions without requiring explicit attack signatures or sophisticated hardware.

</details>

<details>

<summary>2018-10-20 21:21:28 - Triad-NVM: Persistent-Security for Integrity-Protected and Encrypted Non-Volatile Memories (NVMs)</summary>

- *Amro Awad, Laurent Njilla, Mao Ye*

- `1810.09438v1` - [abs](http://arxiv.org/abs/1810.09438v1) - [pdf](http://arxiv.org/pdf/1810.09438v1)

> Emerging Non-Volatile Memories (NVMs) are promising contenders for building future memory systems. On the other side, unlike DRAM systems, NVMs can retain data even after power loss and thus enlarge the attack surface. While data encryption and integrity verification have been proposed earlier for DRAM systems, protecting and recovering secure memories becomes more challenging with persistent memory. Specifically, security metadata, e.g., encryption counters and Merkle Tree data, should be securely persisted and recovered across system reboots and during recovery from crashes. Not persisting updates to security metadata can lead to data inconsistency, in addition to serious security vulnerabilities.   In this paper, we pioneer a new direction that explores persistency of both Merkle Tree and encryption counters to enable secure recovery of data-verifiable and encrypted memory systems. To this end, we coin a new concept that we call Persistent-Security. We discuss the requirements for such persistently secure systems, propose novel optimizations, and evaluate the impact of the proposed relaxation schemes and optimizations on performance, resilience and recovery time. To the best of our knowledge, our paper is the first to discuss the persistence of security metadata in integrity-protected NVM systems and provide corresponding optimizations. We define a set of relaxation schemes that bring trade-offs between performance and recovery time for large capacity NVM systems. Our results show that our proposed design, Triad-NVM, can improve the throughput by an average of ~2x (relative to strict persistence). Moreover, Triad-NVM maintains a recovery time of less than 4 seconds for an 8TB NVM system (30.6 seconds for 64TB), which is ~3648x faster than a system without security metadata persistence.

</details>

<details>

<summary>2018-10-22 12:18:56 - Traceability Decentralization in Supply Chain Management Using Blockchain Technologies</summary>

- *Thomas Sermpinis, Christos Sermpinis*

- `1810.09203v1` - [abs](http://arxiv.org/abs/1810.09203v1) - [pdf](http://arxiv.org/pdf/1810.09203v1)

> With the increase of web users and applications with real time requests, the ability to identify, track and trace elements of a product as it moves in the supply chain is deemed necessary, and for many industries is even mandated by national or international regulations. Traceability presupposes the integrity and transparency of data that is saved and shared. This is a problem for current technologies, as there are many examples with tampered data and database vulnerabilities that resulted in serious implications and data loss. A solution to this problem can be the decentralization of the system, which will remove the central point of failure. To that effect, blockchain or DLT technologies, an emergent technology that enables the decentralization of a network can be used, by implementing a trustless model to achieve it. Blockchains are tamperproof and transparent, which means that by exploiting blockchain characteristics, traceability can be improved. A model that describes the decentralization process of the supply chain traceability part has been developed for this paper and is later evaluated and compared with the traditional system.

</details>

<details>

<summary>2018-10-23 07:05:17 - The Faults in Our Pi Stars: Security Issues and Open Challenges in Deep Reinforcement Learning</summary>

- *Vahid Behzadan, Arslan Munir*

- `1810.10369v1` - [abs](http://arxiv.org/abs/1810.10369v1) - [pdf](http://arxiv.org/pdf/1810.10369v1)

> Since the inception of Deep Reinforcement Learning (DRL) algorithms, there has been a growing interest in both research and industrial communities in the promising potentials of this paradigm. The list of current and envisioned applications of deep RL ranges from autonomous navigation and robotics to control applications in the critical infrastructure, air traffic control, defense technologies, and cybersecurity. While the landscape of opportunities and the advantages of deep RL algorithms are justifiably vast, the security risks and issues in such algorithms remain largely unexplored. To facilitate and motivate further research on these critical challenges, this paper presents a foundational treatment of the security problem in DRL. We formulate the security requirements of DRL, and provide a high-level threat model through the classification and identification of vulnerabilities, attack vectors, and adversarial capabilities. Furthermore, we present a review of current literature on security of deep RL from both offensive and defensive perspectives. Lastly, we enumerate critical research venues and open problems in mitigation and prevention of intentional attacks against deep RL as a roadmap for further research in this area.

</details>

<details>

<summary>2018-10-24 15:54:26 - Preserving Both Privacy and Utility in Network Trace Anonymization</summary>

- *Meisam Mohammady, Lingyu Wang, Yuan Hong, Habib Louafi, Makan Pourzandi, Mourad Debbabi*

- `1810.10464v1` - [abs](http://arxiv.org/abs/1810.10464v1) - [pdf](http://arxiv.org/pdf/1810.10464v1)

> As network security monitoring grows more sophisticated, there is an increasing need for outsourcing such tasks to third-party analysts. However, organizations are usually reluctant to share their network traces due to privacy concerns over sensitive information, e.g., network and system configuration, which may potentially be exploited for attacks. In cases where data owners are convinced to share their network traces, the data are typically subjected to certain anonymization techniques, e.g., CryptoPAn, which replaces real IP addresses with prefix-preserving pseudonyms. However, most such techniques either are vulnerable to adversaries with prior knowledge about some network flows in the traces, or require heavy data sanitization or perturbation, both of which may result in a significant loss of data utility. In this paper, we aim to preserve both privacy and utility through shifting the trade-off from between privacy and utility to between privacy and computational cost. The key idea is for the analysts to generate and analyze multiple anonymized views of the original network traces; those views are designed to be sufficiently indistinguishable even to adversaries armed with prior knowledge, which preserves the privacy, whereas one of the views will yield true analysis results privately retrieved by the data owner, which preserves the utility. We present the general approach and instantiate it based on CryptoPAn. We formally analyze the privacy of our solution and experimentally evaluate it using real network traces provided by a major ISP. The results show that our approach can significantly reduce the level of information leakage (e.g., less than 1\% of the information leaked by CryptoPAn) with comparable utility.

</details>

<details>

<summary>2018-10-25 13:33:53 - Playing With Danger: A Taxonomy and Evaluation of Threats to Smart Toys</summary>

- *Sharon Shasha, Moustafa Mahmoud, Mohammad Mannan, Amr Youssef*

- `1809.05556v2` - [abs](http://arxiv.org/abs/1809.05556v2) - [pdf](http://arxiv.org/pdf/1809.05556v2)

> Smart toys have captured an increasing share of the toy market, and are growing ubiquitous in households with children. Smart toys are a subset of Internet of Things (IoT) devices, containing sensors, actuators, and/or artificial intelligence capabilities. They frequently have internet connectivity, directly or indirectly through companion apps, and collect information about their users and environments. Recent studies have found security flaws in many smart toys that have led to serious privacy leaks, or allowed tracking a child's physical location. Some well-publicized discoveries of this nature have prompted actions from governments around the world to ban some of these toys. Compared to other IoT devices, smart toys pose unique risks because of their easily-vulnerable user base, and our work is intended to define these risks and assess a subset of toys against them. We provide a classification of threats specific to smart toys in order to unite and complement existing adhoc analyses, and help comprehensive evaluation of other smart toys. Our threat classification framework addresses the potential security and privacy flaws that can lead to leakage of private information or allow an adversary to control the toy to lure, harm, or distress a child. Using this framework, we perform a thorough experimental analysis of eleven smart toys and their companion apps. Our systematic analysis has uncovered that several current toys still expose children to multiple threats for attackers with physical, nearby, or remote access to the toy.

</details>

<details>

<summary>2018-10-26 03:37:27 - LRCoin: Leakage-resilient Cryptocurrency Based on Bitcoin for Data Trading in IoT</summary>

- *Yong Yu, Yujie Ding, Yanqi Zhao, Yannan Li, Xiaojiang Du, Mohsen Guizani*

- `1810.11175v1` - [abs](http://arxiv.org/abs/1810.11175v1) - [pdf](http://arxiv.org/pdf/1810.11175v1)

> Currently, the number of Internet of Thing (IoT) devices making up the IoT is more than 11 billion and this number has been continuously increasing. The prevalence of these devices leads to an emerging IoT business model called Device-as-a-service(DaaS), which enables sensor devices to collect data disseminated to all interested devices. The devices sharing data with other devices could receive some financial reward such as Bitcoin. However, side-channel attacks, which aim to exploit some information leaked from the IoT devices during data trade execution, are possible since most of the IoT devices are vulnerable to be hacked or compromised. Thus, it is challenging to securely realize data trading in IoT environment due to the information leakage such as leaking the private key for signing a Bitcoin transaction in Bitcoin system. In this paper, we propose LRCoin, a kind of leakage-resilient cryptocurrency based on bitcoin in which the signature algorithm used for authenticating bitcoin transactions is leakage-resilient. LRCoin is suitable for the scenarios where information leakage is inevitable such as IoT applications. Our core contribution is proposing an efficient bilinear-based continual-leakage-resilient ECDSA signature. We prove the proposed signature algorithm is unforgeable against adaptively chosen messages attack in the generic bilinear group model under the continual leakage setting. Both the theoretical analysis and the implementation demonstrate the practicability of the proposed scheme.

</details>

<details>

<summary>2018-10-26 05:36:51 - Simultaneous Edge Alignment and Learning</summary>

- *Zhiding Yu, Weiyang Liu, Yang Zou, Chen Feng, Srikumar Ramalingam, B. V. K. Vijaya Kumar, Jan Kautz*

- `1808.01992v3` - [abs](http://arxiv.org/abs/1808.01992v3) - [pdf](http://arxiv.org/pdf/1808.01992v3)

> Edge detection is among the most fundamental vision problems for its role in perceptual grouping and its wide applications. Recent advances in representation learning have led to considerable improvements in this area. Many state of the art edge detection models are learned with fully convolutional networks (FCNs). However, FCN-based edge learning tends to be vulnerable to misaligned labels due to the delicate structure of edges. While such problem was considered in evaluation benchmarks, similar issue has not been explicitly addressed in general edge learning. In this paper, we show that label misalignment can cause considerably degraded edge learning quality, and address this issue by proposing a simultaneous edge alignment and learning framework. To this end, we formulate a probabilistic model where edge alignment is treated as latent variable optimization, and is learned end-to-end during network training. Experiments show several applications of this work, including improved edge detection with state of the art performance, and automatic refinement of noisy annotations.

</details>

<details>

<summary>2018-10-27 14:27:57 - IoTSan: Fortifying the Safety of IoT Systems</summary>

- *Dang Tu Nguyen, Chengyu Song, Zhiyun Qian, Srikanth V. Krishnamurthy, Edward J. M. Colbert, Patrick McDaniel*

- `1810.09551v2` - [abs](http://arxiv.org/abs/1810.09551v2) - [pdf](http://arxiv.org/pdf/1810.09551v2)

> Today's IoT systems include event-driven smart applications (apps) that interact with sensors and actuators. A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states. Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. In this paper, we design IoTSan, a novel practical system that uses model checking as a building block to reveal "interaction-level" flaws by identifying events that can lead the system to unsafe states. In building IoTSan, we design novel techniques tailored to IoT systems, to alleviate the state explosion associated with model checking. IoTSan also automatically translates IoT apps into a format amenable to model checking. Finally, to understand the root cause of a detected vulnerability, we design an attribution mechanism to identify problematic and potentially malicious apps. We evaluate IoTSan on the Samsung SmartThings platform. From 76 manually configured systems, IoTSan detects 147 vulnerabilities. We also evaluate IoTSan with malicious SmartThings apps from a previous effort. IoTSan detects the potential safety violations and also effectively attributes these apps as malicious.

</details>

<details>

<summary>2018-10-28 18:22:18 - Learning to Repair Software Vulnerabilities with Generative Adversarial Networks</summary>

- *Jacob Harer, Onur Ozdemir, Tomo Lazovich, Christopher P. Reale, Rebecca L. Russell, Louis Y. Kim, Peter Chin*

- `1805.07475v3` - [abs](http://arxiv.org/abs/1805.07475v3) - [pdf](http://arxiv.org/pdf/1805.07475v3)

> Motivated by the problem of automated repair of software vulnerabilities, we propose an adversarial learning approach that maps from one discrete source domain to another target domain without requiring paired labeled examples or source and target domains to be bijections. We demonstrate that the proposed adversarial learning approach is an effective technique for repairing software vulnerabilities, performing close to seq2seq approaches that require labeled pairs. The proposed Generative Adversarial Network approach is application-agnostic in that it can be applied to other problems similar to code repair, such as grammar correction or sentiment translation.

</details>

<details>

<summary>2018-10-29 05:41:55 - SD-WAN Internet Census</summary>

- *Sergey Gordeychik, Denis Kolegov, Antony Nikolaev*

- `1808.09027v2` - [abs](http://arxiv.org/abs/1808.09027v2) - [pdf](http://arxiv.org/pdf/1808.09027v2)

> The concept of software defined wide area network (SD-WAN or SDWAN) is central to modern computer networking, particularly in enterprise networks. By definition, these systems form network perimeter and connect Internet, WAN, extranet, and branches that makes them crucial from cybersecurity point of view. The goal of this paper is to provide the results of passive and active fingerprinting for SD-WAN systems using a common threat intelligence approach. We explore Internet-based and cloud-based publicly available SD-WAN systems using well-known Shodan and Censys search engines and custom developed automation tools and show that most of the SD-WAN systems have known vulnerabilities related to outdated software and insecure configuration.

</details>

<details>

<summary>2018-10-29 16:19:35 - Explaining Black-box Android Malware Detection</summary>

- *Marco Melis, Davide Maiorca, Battista Biggio, Giorgio Giacinto, Fabio Roli*

- `1803.03544v2` - [abs](http://arxiv.org/abs/1803.03544v2) - [pdf](http://arxiv.org/pdf/1803.03544v2)

> Machine-learning models have been recently used for detecting malicious Android applications, reporting impressive performances on benchmark datasets, even when trained only on features statically extracted from the application, such as system calls and permissions. However, recent findings have highlighted the fragility of such in-vitro evaluations with benchmark datasets, showing that very few changes to the content of Android malware may suffice to evade detection. How can we thus trust that a malware detector performing well on benchmark data will continue to do so when deployed in an operating environment? To mitigate this issue, the most popular Android malware detectors use linear, explainable machine-learning models to easily identify the most influential features contributing to each decision. In this work, we generalize this approach to any black-box machine- learning model, by leveraging a gradient-based approach to identify the most influential local features. This enables using nonlinear models to potentially increase accuracy without sacrificing interpretability of decisions. Our approach also highlights the global characteristics learned by the model to discriminate between benign and malware applications. Finally, as shown by our empirical analysis on a popular Android malware detection task, it also helps identifying potential vulnerabilities of linear and nonlinear models against adversarial manipulations.

</details>

<details>

<summary>2018-10-30 02:21:46 - DARKMENTION: A Deployed System to Predict Enterprise-Targeted External Cyberattacks</summary>

- *Mohammed Almukaynizi, Ericsson Marin, Eric Nunes, Paulo Shakarian, Gerardo I. Simari, Dipsy Kapoor, Timothy Siedlecki*

- `1810.12492v1` - [abs](http://arxiv.org/abs/1810.12492v1) - [pdf](http://arxiv.org/pdf/1810.12492v1)

> Recent incidents of data breaches call for organizations to proactively identify cyber attacks on their systems. Darkweb/Deepweb (D2web) forums and marketplaces provide environments where hackers anonymously discuss existing vulnerabilities and commercialize malicious software to exploit those vulnerabilities. These platforms offer security practitioners a threat intelligence environment that allows to mine for patterns related to organization-targeted cyber attacks. In this paper, we describe a system (called DARKMENTION) that learns association rules correlating indicators of attacks from D2web to real-world cyber incidents. Using the learned rules, DARKMENTION generates and submits warnings to a Security Operations Center (SOC) prior to attacks. Our goal was to design a system that automatically generates enterprise-targeted warnings that are timely, actionable, accurate, and transparent. We show that DARKMENTION meets our goal. In particular, we show that it outperforms baseline systems that attempt to generate warnings of cyber attacks related to two enterprises with an average increase in F1 score of about 45% and 57%. Additionally, DARKMENTION was deployed as part of a larger system that is built under a contract with the IARPA Cyber-attack Automated Unconventional Sensor Environment (CAUSE) program. It is actively producing warnings that precede attacks by an average of 3 days.

</details>

<details>

<summary>2018-10-30 12:02:07 - Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding</summary>

- *Lea Schönherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, Dorothea Kolossa*

- `1808.05665v2` - [abs](http://arxiv.org/abs/1808.05665v2) - [pdf](http://arxiv.org/pdf/1808.05665v2)

> Voice interfaces are becoming accepted widely as input methods for a diverse set of devices. This development is driven by rapid improvements in automatic speech recognition (ASR), which now performs on par with human listening in many tasks. These improvements base on an ongoing evolution of DNNs as the computational core of ASR. However, recent research results show that DNNs are vulnerable to adversarial perturbations, which allow attackers to force the transcription into a malicious output.   In this paper, we introduce a new type of adversarial examples based on psychoacoustic hiding. Our attack exploits the characteristics of DNN-based ASR systems, where we extend the original analysis procedure by an additional backpropagation step. We use this backpropagation to learn the degrees of freedom for the adversarial perturbation of the input signal, i.e., we apply a psychoacoustic model and manipulate the acoustic signal below the thresholds of human perception. To further minimize the perceptibility of the perturbations, we use forced alignment to find the best fitting temporal alignment between the original audio sample and the malicious target transcription. These extensions allow us to embed an arbitrary audio input with a malicious voice command that is then transcribed by the ASR system, with the audio signal remaining barely distinguishable from the original signal. In an experimental evaluation, we attack the state-of-the-art speech recognition system Kaldi and determine the best performing parameter and analysis setup for different types of input. Our results show that we are successful in up to 98% of cases with a computational effort of fewer than two minutes for a ten-second audio file. Based on user studies, we found that none of our target transcriptions were audible to human listeners, who still understand the original speech content with unchanged accuracy.

</details>

<details>

<summary>2018-10-30 19:41:56 - Automated Remote Patient Monitoring: Data Sharing and Privacy Using Blockchain</summary>

- *Gautam Srivastava, Ashutosh Dhar Dwivedi, Rajani Singh*

- `1811.03417v1` - [abs](http://arxiv.org/abs/1811.03417v1) - [pdf](http://arxiv.org/pdf/1811.03417v1)

> The revolution of Internet of Things (IoT) devices and wearable technology has opened up great possibilities in remote patient monitoring. To streamline the diagnosis and treatment process, healthcare professionals are now adopting the wearable technology. However, these technologies also pose grave privacy risks and security concerns about the transfer and the logging of data transactions. One solution to protect privacy in healthcare is the use of blockchain technology. However, one of the primary problems with blockchain is its highly limited scalability. In this work here, we propose the utilization of a blockchain based protocol to provide secure management and analysis of data. In this paper we use recently introduced PoW based protocol GHOSTDAG, that generalizes Satoshi's blockchain to a direct acyclic graph of blocks (blockDAG) and provides high throughput while also avoiding the security-scalability problem. We use two blockchains based on the original GHOSTDAG protocol, one that is private and one that is public. Using a private blockchain, we create a system where we use smart contracts to analyze patient health data. If the smart contract for any reason issues an alert for an abnormal reading then the system makes the record of that event to the public blockchain. This would resolve the privacy and security vulnerabilities associated with remote patient monitoring and also the limited scalability problem of Satoshi's original blockchain.

</details>


## 2018-11

<details>

<summary>2018-11-01 22:06:02 - Adaptive MTD Security using Markov Game Modeling</summary>

- *Ankur Chowdhary, Sailik Sengupta, Adel Alshamrani, Dijiang Huang, Abdulhakim Sabur*

- `1811.00651v1` - [abs](http://arxiv.org/abs/1811.00651v1) - [pdf](http://arxiv.org/pdf/1811.00651v1)

> Large scale cloud networks consist of distributed networking and computing elements that process critical information and thus security is a key requirement for any environment. Unfortunately, assessing the security state of such networks is a challenging task and the tools used in the past by security experts such as packet filtering, firewall, Intrusion Detection Systems (IDS) etc., provide a reactive security mechanism. In this paper, we introduce a Moving Target Defense (MTD) based proactive security framework for monitoring attacks which lets us identify and reason about multi-stage attacks that target software vulnerabilities present in a cloud network. We formulate the multi-stage attack scenario as a two-player zero-sum Markov Game (between the attacker and the network administrator) on attack graphs. The rewards and transition probabilities are obtained by leveraging the expert knowledge present in the Common Vulnerability Scoring System (CVSS). Our framework identifies an attacker's optimal policy and places countermeasures to ensure that this attack policy is always detected, thus forcing the attacker to use a sub-optimal policy with higher cost.

</details>

<details>

<summary>2018-11-03 00:25:50 - A Marauder's Map of Security and Privacy in Machine Learning</summary>

- *Nicolas Papernot*

- `1811.01134v1` - [abs](http://arxiv.org/abs/1811.01134v1) - [pdf](http://arxiv.org/pdf/1811.01134v1)

> There is growing recognition that machine learning (ML) exposes new security and privacy vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited but expanding. In this talk, we explore the threat model space of ML algorithms through the lens of Saltzer and Schroeder's principles for the design of secure computer systems. This characterization of the threat space prompts an investigation of current and future research directions. We structure our discussion around three of these directions, which we believe are likely to lead to significant progress. The first encompasses a spectrum of approaches to verification and admission control, which is a prerequisite to enable fail-safe defaults in machine learning systems. The second seeks to design mechanisms for assembling reliable records of compromise that would help understand the degree to which vulnerabilities are exploited by adversaries, as well as favor psychological acceptability of machine learning applications. The third pursues formal frameworks for security and privacy in machine learning, which we argue should strive to align machine learning goals such as generalization with security and privacy desiderata like robustness or privacy. Key insights resulting from these three directions pursued both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by systematizing best practices in our community.

</details>

<details>

<summary>2018-11-03 00:34:06 - Securing IoT Apps with Fine-grained Control of Information Flows</summary>

- *Davino Mauro Junior, Kiev Gama, Atul Prakash*

- `1810.13367v3` - [abs](http://arxiv.org/abs/1810.13367v3) - [pdf](http://arxiv.org/pdf/1810.13367v3)

> Internet of Things is growing rapidly, with many connected devices now available to consumers. With this growth, the IoT apps that manage the devices from smartphones raise significant security concerns. Typically, these apps are secured via sensitive credentials such as email and password that need to be validated through specific servers, thus requiring permissions to access the Internet. Unfortunately, even when developers are well-intentioned, such apps can be non-trivial to secure so as to guarantee that user's credentials do not leak to unauthorized servers on the Internet. For example, if the app relies on third-party libraries, as many do, those libraries can potentially capture and leak sensitive credentials. Bugs in the applications can also result in exploitable vulnerabilities that leak credentials. This paper presents our work in-progress on a prototype that enables developers to control how information flows within the app from sensitive UI data to specific servers. We extend FlowFence to enforce fine-grained information flow policies on sensitive UI data.

</details>

<details>

<summary>2018-11-04 11:16:23 - Hardening Deep Neural Networks via Adversarial Model Cascades</summary>

- *Deepak Vijaykeerthy, Anshuman Suri, Sameep Mehta, Ponnurangam Kumaraguru*

- `1802.01448v4` - [abs](http://arxiv.org/abs/1802.01448v4) - [pdf](http://arxiv.org/pdf/1802.01448v4)

> Deep neural networks (DNNs) are vulnerable to malicious inputs crafted by an adversary to produce erroneous outputs. Works on securing neural networks against adversarial examples achieve high empirical robustness on simple datasets such as MNIST. However, these techniques are inadequate when empirically tested on complex data sets such as CIFAR-10 and SVHN. Further, existing techniques are designed to target specific attacks and fail to generalize across attacks. We propose the Adversarial Model Cascades (AMC) as a way to tackle the above inadequacies. Our approach trains a cascade of models sequentially where each model is optimized to be robust towards a mixture of multiple attacks. Ultimately, it yields a single model which is secure against a wide range of attacks; namely FGSM, Elastic, Virtual Adversarial Perturbations and Madry. On an average, AMC increases the model's empirical robustness against various attacks simultaneously, by a significant margin (of 6.225% for MNIST, 5.075% for SVHN and 2.65% for CIFAR10). At the same time, the model's performance on non-adversarial inputs is comparable to the state-of-the-art models.

</details>

<details>

<summary>2018-11-05 00:30:21 - Security for Machine Learning-based Systems: Attacks and Challenges during Training and Inference</summary>

- *Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, Muhammad Shafique*

- `1811.01463v1` - [abs](http://arxiv.org/abs/1811.01463v1) - [pdf](http://arxiv.org/pdf/1811.01463v1)

> The exponential increase in dependencies between the cyber and physical world leads to an enormous amount of data which must be efficiently processed and stored. Therefore, computing paradigms are evolving towards machine learning (ML)-based systems because of their ability to efficiently and accurately process the enormous amount of data. Although ML-based solutions address the efficient computing requirements of big data, they introduce (new) security vulnerabilities into the systems, which cannot be addressed by traditional monitoring-based security measures. Therefore, this paper first presents a brief overview of various security threats in machine learning, their respective threat models and associated research challenges to develop robust security measures. To illustrate the security vulnerabilities of ML during training, inferencing and hardware implementation, we demonstrate some key security threats on ML using LeNet and VGGNet for MNIST and German Traffic Sign Recognition Benchmarks (GTSRB), respectively. Moreover, based on the security analysis of ML-training, we also propose an attack that has a very less impact on the inference accuracy. Towards the end, we highlight the associated research challenges in developing security measures and provide a brief overview of the techniques used to mitigate such security threats.

</details>

<details>

<summary>2018-11-05 11:52:47 - On the Transferability of Adversarial Examples Against CNN-Based Image Forensics</summary>

- *Mauro Barni, Kassem Kallas, Ehsan Nowroozi, Benedetta Tondi*

- `1811.01629v1` - [abs](http://arxiv.org/abs/1811.01629v1) - [pdf](http://arxiv.org/pdf/1811.01629v1)

> Recent studies have shown that Convolutional Neural Networks (CNN) are relatively easy to attack through the generation of so-called adversarial examples. Such vulnerability also affects CNN-based image forensic tools. Research in deep learning has shown that adversarial examples exhibit a certain degree of transferability, i.e., they maintain part of their effectiveness even against CNN models other than the one targeted by the attack. This is a very strong property undermining the usability of CNN's in security-oriented applications. In this paper, we investigate if attack transferability also holds in image forensics applications. With specific reference to the case of manipulation detection, we analyse the results of several experiments considering different sources of mismatch between the CNN used to build the adversarial examples and the one adopted by the forensic analyst. The analysis ranges from cases in which the mismatch involves only the training dataset, to cases in which the attacker and the forensic analyst adopt different architectures. The results of our experiments show that, in the majority of the cases, the attacks are not transferable, thus easing the design of proper countermeasures at least when the attacker does not have a perfect knowledge of the target detector.

</details>

<details>

<summary>2018-11-06 04:31:04 - The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure</summary>

- *Saeed Mahloujifar, Dimitrios I. Diochnos, Mohammad Mahmoody*

- `1809.03063v2` - [abs](http://arxiv.org/abs/1809.03063v2) - [pdf](http://arxiv.org/pdf/1809.03063v2)

> Many modern machine learning classifiers are shown to be vulnerable to adversarial perturbations of the instances. Despite a massive amount of work focusing on making classifiers robust, the task seems quite challenging. In this work, through a theoretical study, we investigate the adversarial risk and robustness of classifiers and draw a connection to the well-known phenomenon of concentration of measure in metric measure spaces. We show that if the metric probability space of the test instance is concentrated, any classifier with some initial constant error is inherently vulnerable to adversarial perturbations.   One class of concentrated metric probability spaces are the so-called Levy families that include many natural distributions. In this special case, our attacks only need to perturb the test instance by at most $O(\sqrt n)$ to make it misclassified, where $n$ is the data dimension. Using our general result about Levy instance spaces, we first recover as special case some of the previously proved results about the existence of adversarial examples. However, many more Levy families are known (e.g., product distribution under the Hamming distance) for which we immediately obtain new attacks that find adversarial examples of distance $O(\sqrt n)$.   Finally, we show that concentration of measure for product spaces implies the existence of forms of "poisoning" attacks in which the adversary tampers with the training data with the goal of degrading the classifier. In particular, we show that for any learning algorithm that uses $m$ training examples, there is an adversary who can increase the probability of any "bad property" (e.g., failing on a particular test instance) that initially happens with non-negligible probability to $\approx 1$ by substituting only $\tilde{O}(\sqrt m)$ of the examples with other (still correctly labeled) examples.

</details>

<details>

<summary>2018-11-06 13:25:14 - A Roadmap Towards Resilient Internet of Things for Cyber-Physical Systems</summary>

- *Denise Ratasich, Faiq Khalid, Florian Geissler, Radu Grosu, Muhammad Shafique, Ezio Bartocci*

- `1810.06870v2` - [abs](http://arxiv.org/abs/1810.06870v2) - [pdf](http://arxiv.org/pdf/1810.06870v2)

> The Internet of Things (IoT) is a ubiquitous system connecting many different devices - the things - which can be accessed from the distance. The cyber-physical systems (CPS) monitor and control the things from the distance. As a result, the concepts of dependability and security get deeply intertwined. The increasing level of dynamicity, heterogeneity, and complexity adds to the system's vulnerability, and challenges its ability to react to faults. This paper summarizes state-of-the-art of existing work on anomaly detection, fault-tolerance and self-healing, and adds a number of other methods applicable to achieve resilience in an IoT. We particularly focus on non-intrusive methods ensuring data integrity in the network. Furthermore, this paper presents the main challenges in building a resilient IoT for CPS which is crucial in the era of smart CPS with enhanced connectivity (an excellent example of such a system is connected autonomous vehicles). It further summarizes our solutions, work-in-progress and future work to this topic to enable "Trustworthy IoT for CPS". Finally, this framework is illustrated on a selected use case: A smart sensor infrastructure in the transport domain.

</details>

<details>

<summary>2018-11-07 07:36:55 - CAAD 2018: Iterative Ensemble Adversarial Attack</summary>

- *Jiayang Liu, Weiming Zhang, Nenghai Yu*

- `1811.03456v1` - [abs](http://arxiv.org/abs/1811.03456v1) - [pdf](http://arxiv.org/pdf/1811.03456v1)

> Deep Neural Networks (DNNs) have recently led to significant improvements in many fields. However, DNNs are vulnerable to adversarial examples which are samples with imperceptible perturbations while dramatically misleading the DNNs. Adversarial attacks can be used to evaluate the robustness of deep learning models before they are deployed. Unfortunately, most of existing adversarial attacks can only fool a black-box model with a low success rate. To improve the success rates for black-box adversarial attacks, we proposed an iterated adversarial attack against an ensemble of image classifiers. With this method, we won the 5th place in CAAD 2018 Targeted Adversarial Attack competition.

</details>

<details>

<summary>2018-11-07 11:03:23 - Towards Robust Detection of Adversarial Examples</summary>

- *Tianyu Pang, Chao Du, Yinpeng Dong, Jun Zhu*

- `1706.00633v4` - [abs](http://arxiv.org/abs/1706.00633v4) - [pdf](http://arxiv.org/pdf/1706.00633v4)

> Although the recent progress is substantial, deep learning methods can be vulnerable to the maliciously generated adversarial examples. In this paper, we present a novel training procedure and a thresholding test strategy, towards robust detection of adversarial examples. In training, we propose to minimize the reverse cross-entropy (RCE), which encourages a deep network to learn latent representations that better distinguish adversarial examples from normal ones. In testing, we propose to use a thresholding strategy as the detector to filter out adversarial examples for reliable predictions. Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization. We apply our method to defend various attacking methods on the widely used MNIST and CIFAR-10 datasets, and achieve significant improvements on robust predictions under all the threat models in the adversarial setting.

</details>

<details>

<summary>2018-11-07 15:08:26 - Adversarial Binaries for Authorship Identification</summary>

- *Xiaozhu Meng, Barton P. Miller, Somesh Jha*

- `1809.08316v2` - [abs](http://arxiv.org/abs/1809.08316v2) - [pdf](http://arxiv.org/pdf/1809.08316v2)

> Binary code authorship identification determines authors of a binary program. Existing techniques have used supervised machine learning for this task. In this paper, we look this problem from an attacker's perspective. We aim to modify a test binary, such that it not only causes misprediction but also maintains the functionality of the original input binary. Attacks against binary code are intrinsically more difficult than attacks against domains such as computer vision, where attackers can change each pixel of the input image independently and still maintain a valid image. For binary code, even flipping one bit of a binary may cause the binary to be invalid, to crash at the run-time, or to lose the original functionality. We investigate two types of attacks: untargeted attacks, causing misprediction to any of the incorrect authors, and targeted attacks, causing misprediction to a specific one among the incorrect authors. We develop two key attack capabilities: feature vector modification, generating an adversarial feature vector that both corresponds to a real binary and causes the required misprediction, and input binary modification, modifying the input binary to match the adversarial feature vector while maintaining the functionality of the input binary. We evaluated our attack against classifiers trained with a state-of-the-art method for authorship attribution. The classifiers for authorship identification have 91% accuracy on average. Our untargeted attack has a 96% success rate on average, showing that we can effectively suppress authorship signal. Our targeted attack has a 46% success rate on average, showing that it is possible, but significantly more difficult to impersonate a specific programmer's style. Our attack reveals that existing binary code authorship identification techniques rely on code features that are easy to modify, and thus are vulnerable to attacks.

</details>

<details>

<summary>2018-11-07 16:26:32 - Explaining Deep Learning Models - A Bayesian Non-parametric Approach</summary>

- *Wenbo Guo, Sui Huang, Yunzhe Tao, Xinyu Xing, Lin Lin*

- `1811.03422v1` - [abs](http://arxiv.org/abs/1811.03422v1) - [pdf](http://arxiv.org/pdf/1811.03422v1)

> Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.

</details>

<details>

<summary>2018-11-08 09:26:34 - Web Security Investigation through Penetration Tests: A Case study of an Educational Institution Portal</summary>

- *Daniel Omeiza, Jemima Owusu-Tweneboah*

- `1811.01388v2` - [abs](http://arxiv.org/abs/1811.01388v2) - [pdf](http://arxiv.org/pdf/1811.01388v2)

> Web security has become an important subject; many companies and organizations are becoming more security conscious as they build web applications to render online services and increase web presence. Unfortunately, many of these web applications are still susceptible to threats as they lack strong immunity to malicious attacks. This poses potential danger to the users of the sites and could also affect operations of the organizations or companies concerned. Educational institutions are not left out, their portals and websites hold vital information whose integrity is of utmost importance. Taking Carnegie Mellon University Africa's internship portal as case study, we carried out penetration tests to investigate web vulnerabilities and proffered possible remedies to the discovered vulnerabilities. Our result will inform educational institutions on better website security practices, especially in the African domain.

</details>

<details>

<summary>2018-11-09 02:09:31 - Securing Behavior-based Opinion Spam Detection</summary>

- *Shuaijun Ge, Guixiang Ma, Sihong Xie, Philip S. Yu*

- `1811.03739v1` - [abs](http://arxiv.org/abs/1811.03739v1) - [pdf](http://arxiv.org/pdf/1811.03739v1)

> Reviews spams are prevalent in e-commerce to manipulate product ranking and customers decisions maliciously. While spams generated based on simple spamming strategy can be detected effectively, hardened spammers can evade regular detectors via more advanced spamming strategies. Previous work gave more attention to evasion against text and graph-based detectors, but evasions against behavior-based detectors are largely ignored, leading to vulnerabilities in spam detection systems. Since real evasion data are scarce, we first propose EMERAL (Evasion via Maximum Entropy and Rating sAmpLing) to generate evasive spams to certain existing detectors. EMERAL can simulate spammers with different goals and levels of knowledge about the detectors, targeting at different stages of the life cycle of target products. We show that in the evasion-defense dynamic, only a few evasion types are meaningful to the spammers, and any spammer will not be able to evade too many detection signals at the same time. We reveal that some evasions are quite insidious and can fail all detection signals. We then propose DETER (Defense via Evasion generaTion using EmeRal), based on model re-training on diverse evasive samples generated by EMERAL. Experiments confirm that DETER is more accurate in detecting both suspicious time window and individual spamming reviews. In terms of security, DETER is versatile enough to be vaccinated against diverse and unexpected evasions, is agnostic about evasion strategy and can be released without privacy concern.

</details>

<details>

<summary>2018-11-09 14:46:40 - RadIoT: Radio Communications Intrusion Detection for IoT - A Protocol Independent Approach</summary>

- *Jonathan Roux, Eric Alata, Guillaume Auriol, Mohamed Kaâniche, Vincent Nicomette, Romain Cayre*

- `1811.03934v1` - [abs](http://arxiv.org/abs/1811.03934v1) - [pdf](http://arxiv.org/pdf/1811.03934v1)

> Internet-of-Things (IoT) devices are nowadays massively integrated in daily life: homes, factories, or public places. This technology offers attractive services to improve the quality of life as well as new economic markets through the exploitation of the collected data. However, these connected objects have also become attractive targets for attackers because their current security design is often weak or flawed, as illustrated by several vulnerabilities such as Mirai, Blueborne, etc. This paper presents a novel approach for detecting intrusions in smart spaces such as smarthomes, or smartfactories, that is based on the monitoring and profiling of radio communications at the physical layer using machine learning techniques. The approach is designed to be independent of the large and heterogeneous set of wireless communication protocols typically implemented by connected objects such as WiFi, Bluetooth, Zigbee, Bluetooth-Low-Energy (BLE) or proprietary communication protocols. The main concepts of the proposed approach are presented together with an experimental case study illustrating its feasibility based on data collected during the deployment of the intrusion detection approach in a smart home under real-life conditions.

</details>

<details>

<summary>2018-11-12 06:45:15 - SD-WAN Threat Landscape</summary>

- *Sergey Gordeychik, Denis Kolegov*

- `1811.04583v1` - [abs](http://arxiv.org/abs/1811.04583v1) - [pdf](http://arxiv.org/pdf/1811.04583v1)

> Software Defined Wide Area Network (SD-WAN or SDWAN) is a modern conception and an attractive trend in network technologies. SD-WAN is defined as a specific application of software-defined networking (SDN) to WAN connections. There is growing recognition that SDN and SD-WAN technologies not only expand features, but also expose new vulnerabilities. Unfortunately, at the present time, most vendors say that SD-WAN are perfectly safe, hardened, and fully protected. The goal of this paper is to understand SD-WAN threats using practical approach. We describe basic SD-WAN features and components, investigate an attack surface, explore various vendor features and their security, explain threats and vulnerabilities found in SD-WAN products. We also extend existing SDN threat models by describing new potential threats and attack vectors, provide examples, and consider high-level approaches for their mitigations. The provided results may be used by SD-WAN developers as a part of Secure Software Development Life Cycle (SSDLC), security researchers for penetration testing and vulnerability assessment, system integrators for secure design of SD-WAN solutions, and finally customers for secure deployment operations and configurations of SD-WAN enabled network. The main idea of this work is that SD-WAN threat model involves all traditional network and SDN threats, as well as new product-specific threats, appended by vendors which reinvent or introduce proprietary technologies immature from a security perspective.

</details>

<details>

<summary>2018-11-12 15:34:09 - The SFS Summer Research Study at UMBC: Project-Based Learning Inspires Cybersecurity Students</summary>

- *Alan Sherman, Enis Golaszewski, Edward LaFemina, Ethan Goldschen, Mohammed Khan, Lauren Mundy, Mykah Rather, Bryan Solis, Wubnyonga Tete, Edwin Valdez, Brian Weber, Damian Doyle, Casey O'Brien, Linda Oliva, Joseph Roundy, Jack Suess*

- `1811.04794v1` - [abs](http://arxiv.org/abs/1811.04794v1) - [pdf](http://arxiv.org/pdf/1811.04794v1)

> May 30-June 2, 2017, Scholarship for Service (SFS) scholars at the University of Maryland, Baltimore County (UMBC) analyzed the security of a targeted aspect of the UMBC computer systems. During this hands-on study, with complete access to source code, students identified vulnerabilities, devised and implemented exploits, and suggested mitigations. As part of a pioneering program at UMBC to extend SFS scholarships to community colleges, the study helped initiate six students from two nearby community colleges, who transferred to UMBC in fall 2017 to complete their four-year degrees in computer science and information systems.   The study examined the security of a set of "NetAdmin" custom scripts that enable UMBC faculty and staff to open the UMBC firewall to allow external access to machines they control for research purposes. Students discovered vulnerabilities stemming from weak architectural design, record overflow, and failure to sanitize inputs properly. For example, they implemented a record-overflow and code-injection exploit that exfiltrated the vital API key of the UMBC firewall.   This report summarizes student activities and findings, and reflects on lessons learned for students, educators, and system administrators. Our students found the collaborative experience inspirational, students and educators appreciated the authentic case study, and IT administrators gained access to future employees and received free recommendations for improving the security of their systems. We hope that other universities can benefit from our motivational and educational strategy of teaming educators and system administrators to engage students in active project-based learning centering on focused questions about their university computer systems.

</details>

<details>

<summary>2018-11-13 13:26:55 - Unsupervised Features Extraction for Binary Similarity Using Graph Embedding Neural Networks</summary>

- *Roberto Baldoni, Giuseppe Antonio Di Luna, Luca Massarelli, Fabio Petroni, Leonardo Querzoni*

- `1810.09683v2` - [abs](http://arxiv.org/abs/1810.09683v2) - [pdf](http://arxiv.org/pdf/1810.09683v2)

> In this paper we consider the binary similarity problem that consists in determining if two binary functions are similar only considering their compiled form. This problem is know to be crucial in several application scenarios, such as copyright disputes, malware analysis, vulnerability detection, etc. The current state-of-the-art solutions in this field work by creating an embedding model that maps binary functions into vectors in $\mathbb{R}^{n}$. Such embedding model captures syntactic and semantic similarity between binaries, i.e., similar binary functions are mapped to points that are close in the vector space. This strategy has many advantages, one of them is the possibility to precompute embeddings of several binary functions, and then compare them with simple geometric operations (e.g., dot product). In [32] functions are first transformed in Annotated Control Flow Graphs (ACFGs) constituted by manually engineered features and then graphs are embedded into vectors using a deep neural network architecture. In this paper we propose and test several ways to compute annotated control flow graphs that use unsupervised approaches for feature learning, without incurring a human bias. Our methods are inspired after techniques used in the natural language processing community (e.g., we use word2vec to encode assembly instructions). We show that our approach is indeed successful, and it leads to better performance than previous state-of-the-art solutions. Furthermore, we report on a qualitative analysis of functions embeddings. We found interesting cases in which embeddings are clustered according to the semantic of the original binary function.

</details>

<details>

<summary>2018-11-13 15:56:52 - Universal Decision-Based Black-Box Perturbations: Breaking Security-Through-Obscurity Defenses</summary>

- *Thomas A. Hogan, Bhavya Kailkhura*

- `1811.03733v2` - [abs](http://arxiv.org/abs/1811.03733v2) - [pdf](http://arxiv.org/pdf/1811.03733v2)

> We study the problem of finding a universal (image-agnostic) perturbation to fool machine learning (ML) classifiers (e.g., neural nets, decision tress) in the hard-label black-box setting. Recent work in adversarial ML in the white-box setting (model parameters are known) has shown that many state-of-the-art image classifiers are vulnerable to universal adversarial perturbations: a fixed human-imperceptible perturbation that, when added to any image, causes it to be misclassified with high probability Kurakin et al. [2016], Szegedy et al. [2013], Chen et al. [2017a], Carlini and Wagner [2017]. This paper considers a more practical and challenging problem of finding such universal perturbations in an obscure (or black-box) setting. More specifically, we use zeroth order optimization algorithms to find such a universal adversarial perturbation when no model information is revealed-except that the attacker can make queries to probe the classifier. We further relax the assumption that the output of a query is continuous valued confidence scores for all the classes and consider the case where the output is a hard-label decision. Surprisingly, we found that even in these extremely obscure regimes, state-of-the-art ML classifiers can be fooled with a very high probability just by adding a single human-imperceptible image perturbation to any natural image. The surprising existence of universal perturbations in a hard-label black-box setting raises serious security concerns with the existence of a universal noise vector that adversaries can possibly exploit to break a classifier on most natural images.

</details>

<details>

<summary>2018-11-13 20:23:37 - Deep Q learning for fooling neural networks</summary>

- *Mandar Kulkarni*

- `1811.05521v1` - [abs](http://arxiv.org/abs/1811.05521v1) - [pdf](http://arxiv.org/pdf/1811.05521v1)

> Deep learning models are vulnerable to external attacks. In this paper, we propose a Reinforcement Learning (RL) based approach to generate adversarial examples for the pre-trained (target) models. We assume a semi black-box setting where the only access an adversary has to the target model is the class probabilities obtained for the input queries. We train a Deep Q Network (DQN) agent which, with experience, learns to attack only a small portion of image pixels to generate non-targeted adversarial images. Initially, an agent explores an environment by sequentially modifying random sets of image pixels and observes its effect on the class probabilities. At the end of an episode, it receives a positive (negative) reward if it succeeds (fails) to alter the label of the image. Experimental results with MNIST, CIFAR-10 and Imagenet datasets demonstrate that our RL framework is able to learn an effective attack policy.

</details>

<details>

<summary>2018-11-15 04:15:34 - Plan Interdiction Games</summary>

- *Yevgeniy Vorobeychik, Michael Pritchard*

- `1811.06162v1` - [abs](http://arxiv.org/abs/1811.06162v1) - [pdf](http://arxiv.org/pdf/1811.06162v1)

> We propose a framework for cyber risk assessment and mitigation which models attackers as formal planners and defenders as interdicting such plans. We illustrate the value of plan interdiction problems by first modeling network cyber risk through the use of formal planning, and subsequently formalizing an important question of prioritizing vulnerabilities for patching in the plan interdiction framework. In particular, we show that selectively patching relatively few vulnerabilities allows a network administrator to significantly reduce exposure to cyber risk. More broadly, we have developed a number of scalable approaches for plan interdiction problems, making especially significant advances when attack plans involve uncertainty about system dynamics. However, important open problems remain, including how to effectively capture information asymmetry between the attacker and defender, how to best model dynamics in the attacker-defender interaction, and how to develop scalable algorithms for solving associated plan interdiction games.

</details>

<details>

<summary>2018-11-15 14:13:16 - iSTRICT: An Interdependent Strategic Trust Mechanism for the Cloud-Enabled Internet of Controlled Things</summary>

- *Jeffrey Pawlick, Juntao Chen, Quanyan Zhu*

- `1805.00403v2` - [abs](http://arxiv.org/abs/1805.00403v2) - [pdf](http://arxiv.org/pdf/1805.00403v2)

> The cloud-enabled Internet of controlled things (IoCT) envisions a network of sensors, controllers, and actuators connected through a local cloud in order to intelligently control physical devices. Because cloud services are vulnerable to advanced persistent threats (APTs), each device in the IoCT must strategically decide whether to trust cloud services that may be compromised. In this paper, we present iSTRICT, an interdependent strategic trust mechanism for the cloud-enabled IoCT. iSTRICT is composed of three interdependent layers. In the cloud layer, iSTRICT uses FlipIt games to conceptualize APTs. In the communication layer, it captures the interaction between devices and the cloud using signaling games. In the physical layer, iSTRICT uses optimal control to quantify the utilities in the higher level games. Best response dynamics link the three layers in an overall "game-of-games," for which the outcome is captured by a concept called Gestalt Nash equilibrium (GNE). We prove the existence of a GNE under a set of natural assumptions and develop an adaptive algorithm to iteratively compute the equilibrium. Finally, we apply iSTRICT to trust management for autonomous vehicles that rely on measurements from remote sources. We show that strategic trust in the communication layer achieves a worst-case probability of compromise for any attack and defense costs in the cyber layer.

</details>

<details>

<summary>2018-11-15 14:25:54 - Mayall: A Framework for Desktop JavaScript Auditing and Post-Exploitation Analysis</summary>

- *Adam Rapley, Xavier Bellekens, Lynsay A. Shepherd, Colin McLean*

- `1811.05945v2` - [abs](http://arxiv.org/abs/1811.05945v2) - [pdf](http://arxiv.org/pdf/1811.05945v2)

> Writing desktop applications in JavaScript offers developers the opportunity to write cross-platform applications with cutting edge capabilities. However in doing so, they are potentially submitting their code to a number of unsanctioned modifications from malicious actors. Electron is one such JavaScript application framework which facilitates this multi-platform out-the-box paradigm and is based upon the Node.js JavaScript runtime --- an increasingly popular server-side technology. In bringing this technology to the client-side environment, previously unrealized risks are exposed to users due to the powerful system programming interface that Node.js exposes. In a concerted effort to highlight previously unexposed risks in these rapidly expanding frameworks, this paper presents the Mayall Framework, an extensible toolkit aimed at JavaScript security auditing and post-exploitation analysis. The paper also exposes fifteen highly popular Electron applications and demonstrates that two thirds of applications were found to be using known vulnerable elements with high CVSS scores. Moreover, this paper discloses a wide-reaching and overlooked vulnerability within the Electron Framework which is a direct byproduct of shipping the runtime unaltered with each application, allowing malicious actors to modify source code and inject covert malware inside verified and signed applications without restriction. Finally, a number of injection vectors are explored and appropriate remediations are proposed.

</details>

<details>

<summary>2018-11-15 16:08:05 - Adversarial Resilience Learning - Towards Systemic Vulnerability Analysis for Large and Complex Systems</summary>

- *Lars Fischer, Jan-Menno Memmen, Eric MSP Veith, Martin Tröschel*

- `1811.06447v1` - [abs](http://arxiv.org/abs/1811.06447v1) - [pdf](http://arxiv.org/pdf/1811.06447v1)

> This paper introduces Adversarial Resilience Learning (ARL), a concept to model, train, and analyze artificial neural networks as representations of competitive agents in highly complex systems. In our examples, the agents normally take the roles of attackers or defenders that aim at worsening or improving-or keeping, respectively-defined performance indicators of the system. Our concept provides adaptive, repeatable, actor-based testing with a chance of detecting previously unknown attack vectors. We provide the constitutive nomenclature of ARL and, based on it, the description of experimental setups and results of a preliminary implementation of ARL in simulated power systems.

</details>

<details>

<summary>2018-11-16 05:25:54 - An Empirical Analysis of Vulnerabilities in Python Packages for Web Applications</summary>

- *Jukka Ruohonen*

- `1810.13310v2` - [abs](http://arxiv.org/abs/1810.13310v2) - [pdf](http://arxiv.org/pdf/1810.13310v2)

> This paper examines software vulnerabilities in common Python packages used particularly for web development. The empirical dataset is based on the PyPI package repository and the so-called Safety DB used to track vulnerabilities in selected packages within the repository. The methodological approach builds on a release-based time series analysis of the conditional probabilities for the releases of the packages to be vulnerable. According to the results, many of the Python vulnerabilities observed seem to be only modestly severe; input validation and cross-site scripting have been the most typical vulnerabilities. In terms of the time series analysis based on the release histories, only the recent past is observed to be relevant for statistical predictions; the classical Markov property holds.

</details>

<details>

<summary>2018-11-16 11:02:56 - All roads lead to Rome: Many ways to double spend your cryptocurrency</summary>

- *Zhiniang Peng, Yuki Chen*

- `1811.06751v1` - [abs](http://arxiv.org/abs/1811.06751v1) - [pdf](http://arxiv.org/pdf/1811.06751v1)

> In 2008, Satoshi Nakamoto proposed an electronic cash system (bitcoin) that is completely realized by peer-to-peer technology. The core value of this scheme is that it proposes a solution based on Proof-of Work, so that the cash system can run in a peer-to-peer environment and be able to prevent double-spend attacks. Bitcoin has been developed for ten years, and since then countless digital currencies have been created. But the discussion of double-spend attacks seems to still concentrate on 51% Attacks. In fact, our research has found that there are many other way to achieve double-spend attacks. In this paper, by introducing a number of double-spend attack vulnerabilities that we have found in EOS, NEO and other large blockchain platforms, we summarized various reasons for causing double-spend attacks, and propose an efficient mitigation measure against them.

</details>

<details>

<summary>2018-11-16 20:13:25 - Protecting Voice Controlled Systems Using Sound Source Identification Based on Acoustic Cues</summary>

- *Yuan Gong, Christian Poellabauer*

- `1811.07018v1` - [abs](http://arxiv.org/abs/1811.07018v1) - [pdf](http://arxiv.org/pdf/1811.07018v1)

> Over the last few years, a rapidly increasing number of Internet-of-Things (IoT) systems that adopt voice as the primary user input have emerged. These systems have been shown to be vulnerable to various types of voice spoofing attacks. Existing defense techniques can usually only protect from a specific type of attack or require an additional authentication step that involves another device. Such defense strategies are either not strong enough or lower the usability of the system. Based on the fact that legitimate voice commands should only come from humans rather than a playback device, we propose a novel defense strategy that is able to detect the sound source of a voice command based on its acoustic features. The proposed defense strategy does not require any information other than the voice command itself and can protect a system from multiple types of spoofing attacks. Our proof-of-concept experiments verify the feasibility and effectiveness of this defense strategy.

</details>

<details>

<summary>2018-11-18 05:19:54 - libmpk: Software Abstraction for Intel Memory Protection Keys</summary>

- *Soyeon Park, Sangho Lee, Wen Xu, Hyungon Moon, Taesoo Kim*

- `1811.07276v1` - [abs](http://arxiv.org/abs/1811.07276v1) - [pdf](http://arxiv.org/pdf/1811.07276v1)

> Intel memory protection keys (MPK) is a new hardware feature to support thread-local permission control on groups of pages without requiring modification of page tables. Unfortunately, its current hardware implementation and software supports suffer from security, scalability, and semantic-gap problems: (1) MPK is vulnerable to protection-key-use-after-free and protection-key corruption; (2) MPK does not scale due to hardware limitations; and (3) MPK is not perfectly compatible with mprotect() because it does not support permission synchronization across threads.   In this paper, we propose libmpk, a software abstraction for MPK. libmpk virtualizes protection keys to eliminate the protection-key-use-after-free and protection-key corruption problems while supporting a tremendous number of memory page groups. libmpk also prevents unauthorized writes to its metadata and supports inter-thread key synchronization. We apply libmpk to three real-world applications: OpenSSL, JavaScript JIT compiler, and Memcached for memory protection and isolation. An evaluation shows that libmpk introduces negligible performance overhead (<1%) compared with insecure versions, and improves their performance by 8.1x over secure equivalents using mprotect(). The source code of libmpk will be publicly available and maintained as an open source project.

</details>

<details>

<summary>2018-11-20 19:40:24 - Intermediate Level Adversarial Attack for Enhanced Transferability</summary>

- *Qian Huang, Zeqi Gu, Isay Katsman, Horace He, Pian Pawakapan, Zhiqiu Lin, Serge Belongie, Ser-Nam Lim*

- `1811.08458v1` - [abs](http://arxiv.org/abs/1811.08458v1) - [pdf](http://arxiv.org/pdf/1811.08458v1)

> Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples may be overfit to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. This leads us to introduce the Intermediate Level Attack (ILA), which attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model. We show that our method can effectively achieve this goal and that we can decide a nearly-optimal layer of the source model to perturb without any knowledge of the target models.

</details>

<details>

<summary>2018-11-21 02:29:02 - Encryption is Futile: Delay Attacks on High-Precision Clock Synchronization</summary>

- *Robert Annessi, Joachim Fabini, Felix Iglesias, Tanja Zseby*

- `1811.08569v1` - [abs](http://arxiv.org/abs/1811.08569v1) - [pdf](http://arxiv.org/pdf/1811.08569v1)

> Clock synchronization has become essential to modern societies since many critical infrastructures depend on a precise notion of time. This paper analyzes security aspects of high-precision clock synchronization protocols, particularly their alleged protection against delay attacks when clock synchronization traffic is encrypted using standard network security protocols such as IPsec, MACsec, or TLS. We use the Precision Time Protocol (PTP), the most widely used protocol for high-precision clock synchronization, to demonstrate that statistical traffic analysis can identify properties that support selective message delay attacks even for encrypted traffic. We furthermore identify a fundamental conflict in secure clock synchronization between the need of deterministic traffic to improve precision and the need to obfuscate traffic in order to mitigate delay attacks.   A theoretical analysis of clock synchronization protocols isolates the characteristics that make these protocols vulnerable to delay attacks and argues that such attacks cannot be prevented entirely but only be mitigated. Knowledge of the underlying communication network in terms of one-way delays and knowledge on physical constraints of these networks can help to compute guaranteed maximum bounds for slave clock offsets. These bounds are essential for detecting delay attacks and minimizing their impact. In the general case, however, the precision that can be guaranteed in adversarial settings is orders of magnitude lower than required for high-precision clock synchronization in critical infrastructures, which, therefore, must not rely on a precise notion of time when using untrusted networks.

</details>

<details>

<summary>2018-11-21 08:32:51 - CAAD 2018: Generating Transferable Adversarial Examples</summary>

- *Yash Sharma, Tien-Dung Le, Moustafa Alzantot*

- `1810.01268v2` - [abs](http://arxiv.org/abs/1810.01268v2) - [pdf](http://arxiv.org/pdf/1810.01268v2)

> Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations carefully crafted to fool the targeted DNN, in both the non-targeted and targeted case. In the non-targeted case, the attacker simply aims to induce misclassification. In the targeted case, the attacker aims to induce classification to a specified target class. In addition, it has been observed that strong adversarial examples can transfer to unknown models, yielding a serious security concern. The NIPS 2017 competition was organized to accelerate research in adversarial attacks and defenses, taking place in the realistic setting where submitted adversarial attacks attempt to transfer to submitted defenses. The CAAD 2018 competition took place with nearly identical rules to the NIPS 2017 one. Given the requirement that the NIPS 2017 submissions were to be open-sourced, participants in the CAAD 2018 competition were able to directly build upon previous solutions, and thus improve the state-of-the-art in this setting. Our team participated in the CAAD 2018 competition, and won 1st place in both attack subtracks, non-targeted and targeted adversarial attacks, and 3rd place in defense. We outline our solutions and development results in this article. We hope our results can inform researchers in both generating and defending against adversarial examples.

</details>

<details>

<summary>2018-11-21 23:34:57 - Improving Grey-Box Fuzzing by Modeling Program Behavior</summary>

- *Siddharth Karamcheti, Gideon Mann, David Rosenberg*

- `1811.08973v1` - [abs](http://arxiv.org/abs/1811.08973v1) - [pdf](http://arxiv.org/pdf/1811.08973v1)

> Grey-box fuzzers such as American Fuzzy Lop (AFL) are popular tools for finding bugs and potential vulnerabilities in programs. While these fuzzers have been able to find vulnerabilities in many widely used programs, they are not efficient; of the millions of inputs executed by AFL in a typical fuzzing run, only a handful discover unseen behavior or trigger a crash. The remaining inputs are redundant, exhibiting behavior that has already been observed. Here, we present an approach to increase the efficiency of fuzzers like AFL by applying machine learning to directly model how programs behave. We learn a forward prediction model that maps program inputs to execution traces, training on the thousands of inputs collected during standard fuzzing. This learned model guides exploration by focusing on fuzzing inputs on which our model is the most uncertain (measured via the entropy of the predicted execution trace distribution). By focusing on executing inputs our learned model is unsure about, and ignoring any input whose behavior our model is certain about, we show that we can significantly limit wasteful execution. Through testing our approach on a set of binaries released as part of the DARPA Cyber Grand Challenge, we show that our approach is able to find a set of inputs that result in more code coverage and discovered crashes than baseline fuzzers with significantly fewer executions.

</details>

<details>

<summary>2018-11-22 20:32:58 - Strength in Numbers: Trading-off Robustness and Computation via Adversarially-Trained Ensembles</summary>

- *Edward Grefenstette, Robert Stanforth, Brendan O'Donoghue, Jonathan Uesato, Grzegorz Swirszcz, Pushmeet Kohli*

- `1811.09300v1` - [abs](http://arxiv.org/abs/1811.09300v1) - [pdf](http://arxiv.org/pdf/1811.09300v1)

> While deep learning has led to remarkable results on a number of challenging problems, researchers have discovered a vulnerability of neural networks in adversarial settings, where small but carefully chosen perturbations to the input can make the models produce extremely inaccurate outputs. This makes these models particularly unsuitable for safety-critical application domains (e.g. self-driving cars) where robustness is extremely important. Recent work has shown that augmenting training with adversarially generated data provides some degree of robustness against test-time attacks. In this paper we investigate how this approach scales as we increase the computational budget given to the defender. We show that increasing the number of parameters in adversarially-trained models increases their robustness, and in particular that ensembling smaller models while adversarially training the entire ensemble as a single model is a more efficient way of spending said budget than simply using a larger single model. Crucially, we show that it is the adversarial training of the ensemble, rather than the ensembling of adversarially trained models, which provides robustness.

</details>

<details>

<summary>2018-11-22 21:10:52 - Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial Attack</summary>

- *Adnan Siraj Rakin, Zhezhi He, Deliang Fan*

- `1811.09310v1` - [abs](http://arxiv.org/abs/1811.09310v1) - [pdf](http://arxiv.org/pdf/1811.09310v1)

> Recent development in the field of Deep Learning have exposed the underlying vulnerability of Deep Neural Network (DNN) against adversarial examples. In image classification, an adversarial example is a carefully modified image that is visually imperceptible to the original image but can cause DNN model to misclassify it. Training the network with Gaussian noise is an effective technique to perform model regularization, thus improving model robustness against input variation. Inspired by this classical method, we explore to utilize the regularization characteristic of noise injection to improve DNN's robustness against adversarial attack. In this work, we propose Parametric-Noise-Injection (PNI) which involves trainable Gaussian noise injection at each layer on either activation or weights through solving the min-max optimization problem, embedded with adversarial training. These parameters are trained explicitly to achieve improved robustness. To the best of our knowledge, this is the first work that uses trainable noise injection to improve network robustness against adversarial attacks, rather than manually configuring the injected noise level through cross-validation. The extensive results show that our proposed PNI technique effectively improves the robustness against a variety of powerful white-box and black-box attacks such as PGD, C & W, FGSM, transferable attack and ZOO attack. Last but not the least, PNI method improves both clean- and perturbed-data accuracy in comparison to the state-of-the-art defense methods, which outperforms current unbroken PGD defense by 1.1 % and 6.8 % on clean test data and perturbed test data respectively using Resnet-20 architecture.

</details>

<details>

<summary>2018-11-23 12:14:34 - Smart Greybox Fuzzing</summary>

- *Van-Thuan Pham, Marcel Böhme, Andrew E. Santosa, Alexandru Răzvan Căciulescu, Abhik Roychoudhury*

- `1811.09447v1` - [abs](http://arxiv.org/abs/1811.09447v1) - [pdf](http://arxiv.org/pdf/1811.09447v1)

> Coverage-based greybox fuzzing (CGF) is one of the most successful methods for automated vulnerability detection. Given a seed file (as a sequence of bits), CGF randomly flips, deletes or bits to generate new files. CGF iteratively constructs (and fuzzes) a seed corpus by retaining those generated files which enhance coverage. However, random bitflips are unlikely to produce valid files (or valid chunks in files), for applications processing complex file formats.   In this work, we introduce smart greybox fuzzing (SGF) which leverages a high-level structural representation of the seed file to generate new files. We define innovative mutation operators that work on the virtual file structure rather than on the bit level which allows SGF to explore completely new input domains while maintaining file validity. We introduce a novel validity-based power schedule that enables SGF to spend more time generating files that are more likely to pass the parsing stage of the program, which can expose vulnerabilities much deeper in the processing logic.   Our evaluation demonstrates the effectiveness of SGF. On several libraries that parse structurally complex files, our tool AFLSmart explores substantially more paths (up to 200%) and exposes more vulnerabilities than baseline AFL. Our tool AFLSmart has discovered 42 zero-day vulnerabilities in widely-used, well-tested tools and libraries; so far 17 CVEs were assigned.

</details>

<details>

<summary>2018-11-23 19:07:57 - Evaluating and Understanding the Robustness of Adversarial Logit Pairing</summary>

- *Logan Engstrom, Andrew Ilyas, Anish Athalye*

- `1807.10272v2` - [abs](http://arxiv.org/abs/1807.10272v2) - [pdf](http://arxiv.org/pdf/1807.10272v2)

> We evaluate the robustness of Adversarial Logit Pairing, a recently proposed defense against adversarial examples. We find that a network trained with Adversarial Logit Pairing achieves 0.6% accuracy in the threat model in which the defense is considered. We provide a brief overview of the defense and the threat models/claims considered, as well as a discussion of the methodology and results of our attack, which may offer insights into the reasons underlying the vulnerability of ALP to adversarial attack.

</details>

<details>

<summary>2018-11-23 22:03:40 - Robustness via curvature regularization, and vice versa</summary>

- *Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, Pascal Frossard*

- `1811.09716v1` - [abs](http://arxiv.org/abs/1811.09716v1) - [pdf](http://arxiv.org/pdf/1811.09716v1)

> State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more "linear" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.

</details>

<details>

<summary>2018-11-25 04:55:31 - Towards Blockchain-Driven, Secure and Transparent Audit Logs</summary>

- *Ashar Ahmad, Muhammad Saad, Mostafa Bassiouni, Aziz Mohaisen*

- `1811.09944v1` - [abs](http://arxiv.org/abs/1811.09944v1) - [pdf](http://arxiv.org/pdf/1811.09944v1)

> Audit logs serve as a critical component in the enterprise business systems that are used for auditing, storing, and tracking changes made to the data. However, audit logs are vulnerable to a series of attacks, which enable adversaries to tamper data and corresponding audit logs. In this paper, we present BlockAudit: a scalable and tamper-proof system that leverages the design properties of audit logs and security guarantees of blockchains to enable secure and trustworthy audit logs. Towards that, we construct the design schema of BlockAudit, and outline its operational procedures. We implement our design on Hyperledger and evaluate its performance in terms of latency, network size, and payload size. Our results show that conventional audit logs can seamlessly transition into BlockAudit to achieve higher security, integrity, and fault tolerance.

</details>

<details>

<summary>2018-11-25 10:31:53 - Poisoning Behavioral Malware Clustering</summary>

- *Battista Biggio, Konrad Rieck, Davide Ariu, Christian Wressnegger, Igino Corona, Giorgio Giacinto, Fabio Roli*

- `1811.09985v1` - [abs](http://arxiv.org/abs/1811.09985v1) - [pdf](http://arxiv.org/pdf/1811.09985v1)

> Clustering algorithms have become a popular tool in computer security to analyze the behavior of malware variants, identify novel malware families, and generate signatures for antivirus systems. However, the suitability of clustering algorithms for security-sensitive settings has been recently questioned by showing that they can be significantly compromised if an attacker can exercise some control over the input data. In this paper, we revisit this problem by focusing on behavioral malware clustering approaches, and investigate whether and to what extent an attacker may be able to subvert these approaches through a careful injection of samples with poisoning behavior. To this end, we present a case study on Malheur, an open-source tool for behavioral malware clustering. Our experiments not only demonstrate that this tool is vulnerable to poisoning attacks, but also that it can be significantly compromised even if the attacker can only inject a very small percentage of attacks into the input data. As a remedy, we discuss possible countermeasures and highlight the need for more secure clustering algorithms.

</details>

<details>

<summary>2018-11-27 07:43:47 - How a simple bug in ML compiler could be exploited for backdoors?</summary>

- *Baptiste David*

- `1811.10851v1` - [abs](http://arxiv.org/abs/1811.10851v1) - [pdf](http://arxiv.org/pdf/1811.10851v1)

> Whenever a bug occurs in a program, software developers assume that the code is flawed, not the compiler. In fact, if compilers should be correct, they are just normal software with their own bugs. Hard to find, errors in them have significant impact, since it could result to vulnerabilities, especially when they silently miscompile a critical application. Using assembly language to write such software is quite common, especially when time constraint is involved in such program.   This paper exposes a bug found in Microsoft Macro Assembler (ml for short) compiler, developed by Microsoft since 1981. This assembly has the characteristics to get high level-like constructs and high level-like records which help the developer to write assembly code. It is in the management of one of this level-like construct the bug has been found.   This study aims to show how a compiler-bug can be audited and possibly corrected. For application developers, it shows that even old and mature compilers can present bugs. For security researcher, it shows possibilities to hide some unexpected behavior in software with a clear and officially non-bogus code. It highlights opportunities for including stealth backdoors even in open-source software.

</details>

<details>

<summary>2018-11-27 08:27:49 - Sapiens Chain: A Blockchain-based Cybersecurity Framework</summary>

- *Yu Han, Zhongru Wang, Qiang Ruan, Binxing Fang*

- `1811.10868v1` - [abs](http://arxiv.org/abs/1811.10868v1) - [pdf](http://arxiv.org/pdf/1811.10868v1)

> Recently, cybersecurity becomes more and more important due to the rapid development of Internet. However, existing methods are in reality highly sensitive to attacks and are far more vulnerable than expected, as they are lack of trustable measures. In this paper, to address the aforementioned problems, we propose a blockchain-based cybersecurity framework, termed as Sapiens Chain, which can protect the privacy of the anonymous users and ensure that the transactions are immutable by providing decentralized and trustable services. Integrating semantic analysis, symbolic execution, and routing learning methods into intelligent auditing, this framework can achieve good accuracy for detecting hidden vulnerabilities. In addition, a revenue incentive mechanism, which aims to donate participants, is built. The practical results demonstrate the effectiveness of the proposed framework.

</details>

<details>

<summary>2018-11-28 00:27:12 - Automated Vulnerability Detection in Source Code Using Deep Representation Learning</summary>

- *Rebecca L. Russell, Louis Kim, Lei H. Hamilton, Tomo Lazovich, Jacob A. Harer, Onur Ozdemir, Paul M. Ellingwood, Marc W. McConley*

- `1807.04320v2` - [abs](http://arxiv.org/abs/1807.04320v2) - [pdf](http://arxiv.org/pdf/1807.04320v2)

> Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a large-scale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. The labeled dataset is available at: https://osf.io/d45bw/. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.

</details>

<details>

<summary>2018-11-28 03:19:09 - Relational dynamic memory networks</summary>

- *Trang Pham, Truyen Tran, Svetha Venkatesh*

- `1808.04247v3` - [abs](http://arxiv.org/abs/1808.04247v3) - [pdf](http://arxiv.org/pdf/1808.04247v3)

> Neural networks excel in detecting regular patterns but are less successful in representing and manipulating complex data structures, possibly due to the lack of an external memory. This has led to the recent development of a new line of architectures known as Memory-Augmented Neural Networks (MANNs), each of which consists of a neural network that interacts with an external memory matrix. However, this RAM-like memory matrix is unstructured and thus does not naturally encode structured objects. Here we design a new MANN dubbed Relational Dynamic Memory Network (RMDN) to bridge the gap. Like existing MANNs, RMDN has a neural controller but its memory is structured as multi-relational graphs. RMDN uses the memory to represent and manipulate graph-structured data in response to query; and as a neural network, RMDN is trainable from labeled data. Thus RMDN learns to answer queries about a set of graph-structured objects without explicit programming. We evaluate the capability of RMDN on several important prediction problems, including software vulnerability, molecular bioactivity and chemical-chemical interaction. Results demonstrate the efficacy of the proposed model.

</details>

<details>

<summary>2018-11-28 11:03:26 - A randomized gradient-free attack on ReLU networks</summary>

- *Francesco Croce, Matthias Hein*

- `1811.11493v1` - [abs](http://arxiv.org/abs/1811.11493v1) - [pdf](http://arxiv.org/pdf/1811.11493v1)

> It has recently been shown that neural networks but also other classifiers are vulnerable to so called adversarial attacks e.g. in object recognition an almost non-perceivable change of the image changes the decision of the classifier. Relatively fast heuristics have been proposed to produce these adversarial inputs but the problem of finding the optimal adversarial input, that is with the minimal change of the input, is NP-hard. While methods based on mixed-integer optimization which find the optimal adversarial input have been developed, they do not scale to large networks. Currently, the attack scheme proposed by Carlini and Wagner is considered to produce the best adversarial inputs. In this paper we propose a new attack scheme for the class of ReLU networks based on a direct optimization on the resulting linear regions. In our experimental validation we improve in all except one experiment out of 18 over the Carlini-Wagner attack with a relative improvement of up to 9\%. As our approach is based on the geometrical structure of ReLU networks, it is less susceptible to defences targeting their functional properties.

</details>

<details>

<summary>2018-11-28 12:35:03 - Towards Decentralization of Social Media</summary>

- *Sarang Mahajan, Amey Kasar*

- `1811.11522v1` - [abs](http://arxiv.org/abs/1811.11522v1) - [pdf](http://arxiv.org/pdf/1811.11522v1)

> Facebook uses Artificial Intelligence for targeting users with advertisements based on the events in which they engage like sharing, liking, making comments, posts by a friend, a group creation, etcetera. Each user interacts with these events in different ways, thus receiving different recommendations curated by Facebook's intelligent systems. Facebook segregates its users into chambers, fragmenting them into communities. The technology has completely changed the marketing domain. It is however caught in a race for our finite attention with a motive to make more and more money. Facebook is not a neutral product. It is programmed to get users addicted to it with a goal of gaining added information about the users and optimizing the recommendations provided to the users according to his or her preferences. This paper delineates how Facebook's recommendation system works and presents three methods to safeguard human vulnerabilities exploited by Facebook and other corporations.

</details>

<details>

<summary>2018-11-28 17:48:11 - An Adversarial Approach for Explainable AI in Intrusion Detection Systems</summary>

- *Daniel L. Marino, Chathurika S. Wickramasinghe, Milos Manic*

- `1811.11705v1` - [abs](http://arxiv.org/abs/1811.11705v1) - [pdf](http://arxiv.org/pdf/1811.11705v1)

> Despite the growing popularity of modern machine learning techniques (e.g. Deep Neural Networks) in cyber-security applications, most of these models are perceived as a black-box for the user. Adversarial machine learning offers an approach to increase our understanding of these models. In this paper we present an approach to generate explanations for incorrect classifications made by data-driven Intrusion Detection Systems (IDSs). An adversarial approach is used to find the minimum modifications (of the input features) required to correctly classify a given set of misclassified samples. The magnitude of such modifications is used to visualize the most relevant features that explain the reason for the misclassification. The presented methodology generated satisfactory explanations that describe the reasoning behind the mis-classifications, with descriptions that match expert knowledge. The advantages of the presented methodology are: 1) applicable to any classifier with defined gradients. 2) does not require any modification of the classifier model. 3) can be extended to perform further diagnosis (e.g. vulnerability assessment) and gain further understanding of the system. Experimental evaluation was conducted on the NSL-KDD99 benchmark dataset using Linear and Multilayer perceptron classifiers. The results are shown using intuitive visualizations in order to improve the interpretability of the results.

</details>

<details>

<summary>2018-11-28 22:10:44 - Security and Protocol Exploit Analysis of the 5G Specifications</summary>

- *Roger Piqueras Jover, Vuk Marojevic*

- `1809.06925v4` - [abs](http://arxiv.org/abs/1809.06925v4) - [pdf](http://arxiv.org/pdf/1809.06925v4)

> The Third Generation Partnership Project (3GPP) released its first 5G security specifications in March 2018. This paper reviews the 5G security architecture, requirements and main processes and evaluates them in the context of known and new protocol exploits. Although the security has been enhanced when compared to previous generations to tackle known protocol exploits, our analysis identifies some potentially unrealistic system assumptions that are critical for security as well as a number protocol edge cases that could render 5G systems vulnerable to adversarial attacks. For example, null encryption and null authentication are supported and can be used in valid system configurations, and certain key security functions are still left outside of the scope of the specifications. Moreover, the prevention of pre-authentcation message exploits appears to rely on the implicit assumption of impractical carrier and roaming agreements and the management of public keys from all global operators. In parallel, existing threats such as International Mobile Subscriber Identity (IMSI) catchers are prevented only if the serving network enforces optional security features and if the UE knows the public key of the home network operator. The comparison with 4G LTE protocol exploits reveals that the 5G security specifications, as of Release 15, do not fully address the user privacy and network availability concerns, where one edge case can compromise the privacy, security and availability of 5G users and services.

</details>

<details>

<summary>2018-11-28 23:54:47 - Pay attention! - Robustifying a Deep Visuomotor Policy through Task-Focused Attention</summary>

- *Pooya Abolghasemi, Amir Mazaheri, Mubarak Shah, Ladislau Bölöni*

- `1809.10093v2` - [abs](http://arxiv.org/abs/1809.10093v2) - [pdf](http://arxiv.org/pdf/1809.10093v2)

> Several recent studies have demonstrated the promise of deep visuomotor policies for robot manipulator control. Despite impressive progress, these systems are known to be vulnerable to physical disturbances, such as accidental or adversarial bumps that make them drop the manipulated object. They also tend to be distracted by visual disturbances such as objects moving in the robot's field of view, even if the disturbance does not physically prevent the execution of the task. In this paper, we propose an approach for augmenting a deep visuomotor policy trained through demonstrations with Task Focused visual Attention (TFA). The manipulation task is specified with a natural language text such as `move the red bowl to the left'. This allows the visual attention component to concentrate on the current object that the robot needs to manipulate. We show that even in benign environments, the TFA allows the policy to consistently outperform a variant with no attention mechanism. More importantly, the new policy is significantly more robust: it regularly recovers from severe physical disturbances (such as bumps causing it to drop the object) from which the baseline policy, i.e. with no visual attention, almost never recovers. In addition, we show that the proposed policy performs correctly in the presence of a wide class of visual disturbances, exhibiting a behavior reminiscent of human selective visual attention experiments. Our proposed approach consists of a VAE-GAN network which encodes the visual input and feeds it to a Motor network that moves the robot joints. Also, our approach benefits from a teacher network for the TFA that leverages textual input command to robustify the visual encoder against various types of disturbances.

</details>

<details>

<summary>2018-11-29 14:51:37 - The Untold Secrets of Operational Wi-Fi Calling Services: Vulnerabilities, Attacks, and Countermeasures</summary>

- *Tian Xie, Guan-Hua Tu, Bangjie Yin, Chi-Yu Li, Chunyi Peng, Mi Zhang, Hui Liu, Xiaoming Liu*

- `1811.11274v2` - [abs](http://arxiv.org/abs/1811.11274v2) - [pdf](http://arxiv.org/pdf/1811.11274v2)

> Since 2016, all of four major U.S. operators have rolled out nationwide Wi-Fi calling services. They are projected to surpass VoLTE (Voice over LTE) and other VoIP services in terms of mobile IP voice usage minutes in 2018. They enable mobile users to place cellular calls over Wi-Fi networks based on the 3GPP IMS (IP Multimedia Subsystem) technology. Compared with conventional cellular voice solutions, the major difference lies in that their traffic traverses untrustful Wi-Fi networks and the Internet. This exposure to insecure networks may cause the Wi-Fi calling users to suffer from security threats. Its security mechanisms are similar to the VoLTE, because both of them are supported by the IMS. They include SIM-based security, 3GPP AKA (Authentication and Key Agreement), IPSec (Internet Protocol Security), etc. However, are they sufficient to secure Wi-Fi calling services? Unfortunately, our study yields a negative answer. We conduct the first study of exploring security issues of the operational Wi-Fi calling services in three major U.S. operators' networks using commodity devices. We disclose that current Wi-Fi calling security is not bullet-proof and uncover four vulnerabilities which stem from improper standard designs, device implementation issues and network operation slips. By exploiting the vulnerabilities, together with several state-of-the-art computer visual recognition technologies, we devise two proof-of-concept attacks: user privacy leakage and telephony harassment or denial of voice service (THDoS); both of them can bypass the security defenses deployed on mobile devices and the network infrastructure. We have confirmed their feasibility and simplicity using real-world experiments, as well as assessed their potential damages and proposed recommended solutions.

</details>

<details>

<summary>2018-11-29 18:57:43 - CNN-Cert: An Efficient Framework for Certifying Robustness of Convolutional Neural Networks</summary>

- *Akhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel*

- `1811.12395v1` - [abs](http://arxiv.org/abs/1811.12395v1) - [pdf](http://arxiv.org/pdf/1811.12395v1)

> Verifying robustness of neural network classifiers has attracted great interests and attention due to the success of deep neural networks and their unexpected vulnerability to adversarial perturbations. Although finding minimum adversarial distortion of neural networks (with ReLU activations) has been shown to be an NP-complete problem, obtaining a non-trivial lower bound of minimum distortion as a provable robustness guarantee is possible. However, most previous works only focused on simple fully-connected layers (multilayer perceptrons) and were limited to ReLU activations. This motivates us to propose a general and efficient framework, CNN-Cert, that is capable of certifying robustness on general convolutional neural networks. Our framework is general -- we can handle various architectures including convolutional layers, max-pooling layers, batch normalization layer, residual blocks, as well as general activation functions; our approach is efficient -- by exploiting the special structure of convolutional layers, we achieve up to 17 and 11 times of speed-up compared to the state-of-the-art certification algorithms (e.g. Fast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach while our algorithm obtains similar or even better verification bounds. In addition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and CROWN. We demonstrate by extensive experiments that our method outperforms state-of-the-art lower-bound-based certification algorithms in terms of both bound quality and speed.

</details>

<details>

<summary>2018-11-29 20:46:35 - Security, Privacy and Safety Risk Assessment for Virtual Reality Learning Environment Applications</summary>

- *Aniket Gulhane, Akhil Vyas, Reshmi Mitra, Roland Oruche, Gabriela Hoefer, Samaikya Valluripally, Prasad Calyam, Khaza Anuarul Hoque*

- `1811.12476v1` - [abs](http://arxiv.org/abs/1811.12476v1) - [pdf](http://arxiv.org/pdf/1811.12476v1)

> Social Virtual Reality based Learning Environments (VRLEs) such as vSocial render instructional content in a three-dimensional immersive computer experience for training youth with learning impediments. There are limited prior works that explored attack vulnerability in VR technology, and hence there is a need for systematic frameworks to quantify risks corresponding to security, privacy, and safety (SPS) threats. The SPS threats can adversely impact the educational user experience and hinder delivery of VRLE content. In this paper, we propose a novel risk assessment framework that utilizes attack trees to calculate a risk score for varied VRLE threats with rate and duration of threats as inputs. We compare the impact of a well-constructed attack tree with an adhoc attack tree to study the trade-offs between overheads in managing attack trees, and the cost of risk mitigation when vulnerabilities are identified. We use a vSocial VRLE testbed in a case study to showcase the effectiveness of our framework and demonstrate how a suitable attack tree formalism can result in a more safer, privacy-preserving and secure VRLE system.

</details>

<details>

<summary>2018-11-29 22:55:38 - KASR: A Reliable and Practical Approach to Attack Surface Reduction of Commodity OS Kernels</summary>

- *Zhi Zhang, Yueqiang Cheng, Surya Nepal, Dongxi Liu, Qingni Shen, Fethi Rabhi*

- `1802.07062v2` - [abs](http://arxiv.org/abs/1802.07062v2) - [pdf](http://arxiv.org/pdf/1802.07062v2)

> Commodity OS kernels have broad attack surfaces due to the large code base and the numerous features such as device drivers. For a real-world use case (e.g., an Apache Server), many kernel services are unused and only a small amount of kernel code is used. Within the used code, a certain part is invoked only at runtime while the rest are executed at startup and/or shutdown phases in the kernel's lifetime run. In this paper, we propose a reliable and practical system, named KASR, which transparently reduces attack surfaces of commodity OS kernels at runtime without requiring their source code. The KASR system, residing in a trusted hypervisor, achieves the attack surface reduction through a two-step approach: (1) reliably depriving unused code of executable permissions, and (2) transparently segmenting used code and selectively activating them. We implement a prototype of KASR on Xen-4.8.2 hypervisor and evaluate its security effectiveness on Linux kernel-4.4.0-87-generic. Our evaluation shows that KASR reduces the kernel attack surface by 64% and trims off 40% of CVE vulnerabilities. Besides, KASR successfully detects and blocks all 6 real-world kernel rootkits. We measure its performance overhead with three benchmark tools (i.e., SPECINT, httperf and bonnie++). The experimental results indicate that KASR imposes less than 1% performance overhead (compared to an unmodified Xen hypervisor) on all the benchmarks.

</details>

<details>

<summary>2018-11-30 10:38:37 - Mapping Informal Settlements in Developing Countries with Multi-resolution, Multi-spectral Data</summary>

- *Patrick Helber, Bradley Gram-Hansen, Indhu Varatharajan, Faiza Azam, Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski*

- `1812.00812v1` - [abs](http://arxiv.org/abs/1812.00812v1) - [pdf](http://arxiv.org/pdf/1812.00812v1)

> Detecting and mapping informal settlements encompasses several of the United Nations sustainable development goals. This is because informal settlements are home to the most socially and economically vulnerable people on the planet. Thus, understanding where these settlements are is of paramount importance to both government and non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), who can use this information to deliver effective social and economic aid. We propose two effective methods for detecting and mapping the locations of informal settlements. One uses only low-resolution (LR), freely available, Sentinel-2 multispectral satellite imagery with noisy annotations, whilst the other is a deep learning approach that uses only costly very-high-resolution (VHR) satellite imagery. To our knowledge, we are the first to map informal settlements successfully with low-resolution satellite imagery. We extensively evaluate and compare the proposed methods. Please find additional material at https://frontierdevelopmentlab.github.io/informal-settlements/.

</details>

<details>

<summary>2018-11-30 14:14:44 - Generative Models for Simulating Mobility Trajectories</summary>

- *Vaibhav Kulkarni, Natasa Tagasovska, Thibault Vatter, Benoit Garbinato*

- `1811.12801v1` - [abs](http://arxiv.org/abs/1811.12801v1) - [pdf](http://arxiv.org/pdf/1811.12801v1)

> Mobility datasets are fundamental for evaluating algorithms pertaining to geographic information systems and facilitating experimental reproducibility. But privacy implications restrict sharing such datasets, as even aggregated location-data is vulnerable to membership inference attacks. Current synthetic mobility dataset generators attempt to superficially match a priori modeled mobility characteristics which do not accurately reflect the real-world characteristics. Modeling human mobility to generate synthetic yet semantically and statistically realistic trajectories is therefore crucial for publishing trajectory datasets having satisfactory utility level while preserving user privacy. Specifically, long-range dependencies inherent to human mobility are challenging to capture with both discriminative and generative models. In this paper, we benchmark the performance of recurrent neural architectures (RNNs), generative adversarial networks (GANs) and nonparametric copulas to generate synthetic mobility traces. We evaluate the generated trajectories with respect to their geographic and semantic similarity, circadian rhythms, long-range dependencies, training and generation time. We also include two sample tests to assess statistical similarity between the observed and simulated distributions, and we analyze the privacy tradeoffs with respect to membership inference and location-sequence attacks.

</details>

<details>

<summary>2018-11-30 15:44:52 - Adversarial vulnerability for any classifier</summary>

- *Alhussein Fawzi, Hamza Fawzi, Omar Fawzi*

- `1802.08686v2` - [abs](http://arxiv.org/abs/1802.08686v2) - [pdf](http://arxiv.org/pdf/1802.08686v2)

> Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations. This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets.

</details>

<details>

<summary>2018-11-30 16:24:04 - On The Relation Between Outdated Docker Containers, Severity Vulnerabilities and Bugs</summary>

- *Ahmed Zerouali, Tom Mens, Gregorio Robles, Jesus Gonzalez-Barahona*

- `1811.12874v1` - [abs](http://arxiv.org/abs/1811.12874v1) - [pdf](http://arxiv.org/pdf/1811.12874v1)

> Packaging software into containers is becoming a common practice when deploying services in cloud and other environments. Docker images are one of the most popular container technologies for building and deploying containers. A container image usually includes a collection of software packages, that can have bugs and security vulnerabilities that affect the container health. Our goal is to support container deployers by analysing the relation between outdated containers and vulnerable and buggy packages installed in them. We use the concept of technical lag of a container as the difference between a given container and the most up-to-date container that is possible with the most recent releases of the same collection of packages. For 7,380 official and community Docker images that are based on the Debian Linux distribution, we identify which software packages are installed in them and measure their technical lag in terms of version updates, security vulnerabilities and bugs. We have found, among others, that no release is devoid of vulnerabilities, so deployers cannot avoid vulnerabilities even if they deploy the most recent packages. We offer some lessons learned for container developers in regard to the strategies they can follow to minimize the number of vulnerabilities. We argue that Docker container scan and security management tools should improve their platforms by adding data about other kinds of bugs and include the measurement of technical lag to offer deployers information of when to update.

</details>


## 2018-12

<details>

<summary>2018-12-01 12:19:48 - When a Patch is Not Enough - HardFails: Software-Exploitable Hardware Bugs</summary>

- *Ghada Dessouky, David Gens, Patrick Haney, Garrett Persyn, Arun Kanuparthi, Hareesh Khattri, Jason M. Fung, Ahmad-Reza Sadeghi, Jeyavijayan Rajendran*

- `1812.00197v1` - [abs](http://arxiv.org/abs/1812.00197v1) - [pdf](http://arxiv.org/pdf/1812.00197v1)

> In this paper, we take a deep dive into microarchitectural security from a hardware designer's perspective by reviewing the existing approaches to detect hardware vulnerabilities during the design phase. We show that a protection gap currently exists in practice that leaves chip designs vulnerable to software-based attacks. In particular, existing verification approaches fail to detect specific classes of vulnerabilities, which we call HardFails: these bugs evade detection by current verification techniques while being exploitable from software. We demonstrate such vulnerabilities in real-world SoCs using RISC-V to showcase and analyze concrete instantiations of HardFails. Patching these hardware bugs may not always be possible and can potentially result in a product recall. We base our findings on two extensive case studies: the recent Hack@DAC 2018 hardware security competition, where 54 independent teams of researchers competed world-wide over a period of 12 weeks to catch inserted security bugs in SoC RTL designs, and an in-depth systematic evaluation of state-of-the-art verification approaches. Our findings indicate that even combinations of techniques will miss high-impact bugs due to the large number of modules with complex interdependencies and fundamental limitations of current detection approaches. We also craft a real-world software attack that exploits one of the RTL bugs from Hack@DAC that evaded detection and discuss novel approaches to mitigate the growing problem of cross-layer bugs at design time.

</details>

<details>

<summary>2018-12-01 12:58:56 - FineFool: Fine Object Contour Attack via Attention</summary>

- *Jinyin Chen, Haibin Zheng, Hui Xiong, Mengmeng Su*

- `1812.01713v1` - [abs](http://arxiv.org/abs/1812.01713v1) - [pdf](http://arxiv.org/pdf/1812.01713v1)

> Machine learning models have been shown vulnerable to adversarial attacks launched by adversarial examples which are carefully crafted by attacker to defeat classifiers. Deep learning models cannot escape the attack either. Most of adversarial attack methods are focused on success rate or perturbations size, while we are more interested in the relationship between adversarial perturbation and the image itself. In this paper, we put forward a novel adversarial attack based on contour, named FineFool. Finefool not only has better attack performance compared with other state-of-art white-box attacks in aspect of higher attack success rate and smaller perturbation, but also capable of visualization the optimal adversarial perturbation via attention on object contour. To the best of our knowledge, Finefool is for the first time combines the critical feature of the original clean image with the optimal perturbations in a visible manner. Inspired by the correlations between adversarial perturbations and object contour, slighter perturbations is produced via focusing on object contour features, which is more imperceptible and difficult to be defended, especially network add-on defense methods with the trade-off between perturbations filtering and contour feature loss. Compared with existing state-of-art attacks, extensive experiments are conducted to show that Finefool is capable of efficient attack against defensive deep models.

</details>

<details>

<summary>2018-12-03 17:23:49 - An Historical Analysis of the SEAndroid Policy Evolution</summary>

- *Bumjin Im, Ang Chen, Dan Wallach*

- `1812.00920v1` - [abs](http://arxiv.org/abs/1812.00920v1) - [pdf](http://arxiv.org/pdf/1812.00920v1)

> Android adopted SELinux's mandatory access control (MAC) mechanisms in 2013. Since then, billions of Android devices have benefited from mandatory access control security policies. These policies are expressed in a variety of rules, maintained by Google and extended by Android OEMs. Over the years, the rules have grown to be quite complex, making it challenging to properly understand or configure these policies.   In this paper, we perform a measurement study on the SEAndroid repository to understand the evolution of these policies. We propose a new metric to measure the complexity of the policy by expanding policy rules, with their abstraction features such as macros and groups, into primitive "boxes", which we then use to show that the complexity of the SEAndroid policies has been growing exponentially over time. By analyzing the Git commits, snapshot by snapshot, we are also able to analyze the "age" of policy rules, the trend of changes, and the contributor composition. We also look at hallmark events in Android's history, such as the "Stagefright" vulnerability in Android's media facilities, pointing out how these events led to changes in the MAC policies. The growing complexity of Android's mandatory policies suggests that we will eventually hit the limits of our ability to understand these policies, requiring new tools and techniques.

</details>

<details>

<summary>2018-12-04 23:00:04 - Exploiting Data Sensitivity on Partitioned Data</summary>

- *Sharad Mehrotra, Kerim Yasin Oktay, Shantanu Sharma*

- `1812.01741v1` - [abs](http://arxiv.org/abs/1812.01741v1) - [pdf](http://arxiv.org/pdf/1812.01741v1)

> Several researchers have proposed solutions for secure data outsourcing on the public clouds based on encryption, secret-sharing, and trusted hardware. Existing approaches, however, exhibit many limitations including high computational complexity, imperfect security, and information leakage. This chapter describes an emerging trend in secure data processing that recognizes that an entire dataset may not be sensitive, and hence, non-sensitivity of data can be exploited to overcome some of the limitations of existing encryption-based approaches. In particular, data and computation can be partitioned into sensitive or non-sensitive datasets - sensitive data can either be encrypted prior to outsourcing or stored/processed locally on trusted servers. The non-sensitive dataset, on the other hand, can be outsourced and processed in the cleartext. While partitioned computing can bring new efficiencies since it does not incur (expensive) encrypted data processing costs on non-sensitive data, it can lead to information leakage. We study partitioned computing in two contexts - first, in the context of the hybrid cloud where local resources are integrated with public cloud resources to form an effective and secure storage and computational platform for enterprise data. In the hybrid cloud, sensitive data is stored on the private cloud to prevent leakage and a computation is partitioned between private and public clouds. Care must be taken that the public cloud cannot infer any information about sensitive data from inter-cloud data access during query processing. We then consider partitioned computing in a public cloud only setting, where sensitive data is encrypted before outsourcing. We formally define a partitioned security criterion that any approach to partitioned computing on public clouds must ensure in order to not introduce any new vulnerabilities to the existing secure solution.

</details>

<details>

<summary>2018-12-05 05:32:00 - Regularized Ensembles and Transferability in Adversarial Learning</summary>

- *Yifan Chen, Yevgeniy Vorobeychik*

- `1812.01821v1` - [abs](http://arxiv.org/abs/1812.01821v1) - [pdf](http://arxiv.org/pdf/1812.01821v1)

> Despite the considerable success of convolutional neural networks in a broad array of domains, recent research has shown these to be vulnerable to small adversarial perturbations, commonly known as adversarial examples. Moreover, such examples have shown to be remarkably portable, or transferable, from one model to another, enabling highly successful black-box attacks. We explore this issue of transferability and robustness from two dimensions: first, considering the impact of conventional $l_p$ regularization as well as replacing the top layer with a linear support vector machine (SVM), and second, the value of combining regularized models into an ensemble. We show that models trained with different regularizers present barriers to transferability, as does partial information about the models comprising the ensemble.

</details>

<details>

<summary>2018-12-05 17:29:11 - Processor Hardware Security Vulnerabilities and their Detection by Unique Program Execution Checking</summary>

- *Mohammad Rahmani Fadiheh, Dominik Stoffel, Clark Barrett, Subhasish Mitra, Wolfgang Kunz*

- `1812.04975v1` - [abs](http://arxiv.org/abs/1812.04975v1) - [pdf](http://arxiv.org/pdf/1812.04975v1)

> Recent discovery of security attacks in advanced processors, known as Spectre and Meltdown, has resulted in high public alertness about security of hardware. The root cause of these attacks is information leakage across "covert channels" that reveal secret data without any explicit information flow between the secret and the attacker. Many sources believe that such covert channels are intrinsic to highly advanced processor architectures based on speculation and out-of-order execution, suggesting that such security risks can be avoided by staying away from high-end processors. This paper, however, shows that the problem is of wider scope: we present new classes of covert channel attacks which are possible in average-complexity processors with in-order pipelining, as they are mainstream in applications ranging from Internet-of-Things to Autonomous Systems.   We present a new approach as a foundation for remedy against covert channels: while all previous attacks were found by clever thinking of human attackers, this paper presents an automated and exhaustive method called "Unique Program Execution Checking" which detects and locates vulnerabilities to covert channels systematically, including those to covert channels unknown so far.

</details>

<details>

<summary>2018-12-06 03:12:16 - On Configurable Defense against Adversarial Example Attacks</summary>

- *Bo Luo, Min Li, Yu Li, Qiang Xu*

- `1812.02737v1` - [abs](http://arxiv.org/abs/1812.02737v1) - [pdf](http://arxiv.org/pdf/1812.02737v1)

> Machine learning systems based on deep neural networks (DNNs) have gained mainstream adoption in many applications. Recently, however, DNNs are shown to be vulnerable to adversarial example attacks with slight perturbations on the inputs. Existing defense mechanisms against such attacks try to improve the overall robustness of the system, but they do not differentiate different targeted attacks even though the corresponding impacts may vary significantly. To tackle this problem, we propose a novel configurable defense mechanism in this work, wherein we are able to flexibly tune the robustness of the system against different targeted attacks to satisfy application requirements. This is achieved by refining the DNN loss function with an attack sensitive matrix to represent the impacts of different targeted attacks. Experimental results on CIFAR-10 and GTSRB data sets demonstrate the efficacy of the proposed solution.

</details>

<details>

<summary>2018-12-06 06:00:48 - Trustworthy Smart Band: Security Requirement Analysis with Threat Modeling</summary>

- *Suin Kang, Hye Min Kim, Huy Kang Kim*

- `1812.02361v1` - [abs](http://arxiv.org/abs/1812.02361v1) - [pdf](http://arxiv.org/pdf/1812.02361v1)

> As smart bands make life more convenient and provide a positive lifestyle, many people are now using them. Since smart bands deal with private information, security design and implementation for smart band system become necessary. To make a trustworthy smart band, we must derive the security requirements of the system first, and then design the system satisfying the security requirements. In this paper, we apply threat modeling techniques such as Data Flow Diagram, STRIDE, and Attack Tree to the smart band system to identify threats and derive security requirements accordingly. Through threat modeling, we found the vulnerabilities of the smart band system and successfully exploited smart bands with them. To defend against these threats, we propose security measures and verify that they are secure by using Scyther which is a tool for automatic verification of security protocol.

</details>

<details>

<summary>2018-12-06 14:59:29 - Prior Networks for Detection of Adversarial Attacks</summary>

- *Andrey Malinin, Mark Gales*

- `1812.02575v1` - [abs](http://arxiv.org/abs/1812.02575v1) - [pdf](http://arxiv.org/pdf/1812.02575v1)

> Adversarial examples are considered a serious issue for safety critical applications of AI, such as finance, autonomous vehicle control and medicinal applications. Though significant work has resulted in increased robustness of systems to these attacks, systems are still vulnerable to well-crafted attacks. To address this problem, several adversarial attack detection methods have been proposed. However, a system can still be vulnerable to adversarial samples that are designed to specifically evade these detection methods. One recent detection scheme that has shown good performance is based on uncertainty estimates derived from Monte-Carlo dropout ensembles. Prior Networks, a new method of estimating predictive uncertainty, has been shown to outperform Monte-Carlo dropout on a range of tasks. One of the advantages of this approach is that the behaviour of a Prior Network can be explicitly tuned to, for example, predict high uncertainty in regions where there are no training data samples. In this work, Prior Networks are applied to adversarial attack detection using measures of uncertainty in a similar fashion to Monte-Carlo Dropout. Detection based on measures of uncertainty derived from DNNs and Monte-Carlo dropout ensembles are used as a baseline. Prior Networks are shown to significantly out-perform these baseline approaches over a range of adversarial attacks in both detection of whitebox and blackbox configurations. Even when the adversarial attacks are constructed with full knowledge of the detection mechanism, it is shown to be highly challenging to successfully generate an adversarial sample.

</details>

<details>

<summary>2018-12-07 09:23:08 - Research on the Security of Blockchain Data: A Survey</summary>

- *Liehuang Zhu, Baokun Zheng, Meng Shen, Shui Yu, Feng Gao, Hongyu Li, Kexin Shi, Keke Gai*

- `1812.02009v2` - [abs](http://arxiv.org/abs/1812.02009v2) - [pdf](http://arxiv.org/pdf/1812.02009v2)

> With the more and more extensive application of blockchain, blockchain security has been widely concerned by the society and deeply studied by scholars. Moreover, the security of blockchain data directly affects the security of various applications of blockchain. In this survey, we perform a comprehensive classification and summary of the security of blockchain data. First, we present classification of blockchain data attacks. Subsequently, we present the attacks and defenses of blockchain data in terms of privacy, availability, integrity and controllability. Data privacy attacks present data leakage or data obtained by attackers through analysis. Data availability attacks present abnormal or incorrect access to blockchain data. Data integrity attacks present blockchain data being tampered. Data controllability attacks present blockchain data accidentally manipulated by smart contract vulnerability. Finally, we present several important open research directions to identify follow-up studies in this area.

</details>

<details>

<summary>2018-12-07 16:21:24 - Combatting Adversarial Attacks through Denoising and Dimensionality Reduction: A Cascaded Autoencoder Approach</summary>

- *Rajeev Sahay, Rehana Mahfuz, Aly El Gamal*

- `1812.03087v1` - [abs](http://arxiv.org/abs/1812.03087v1) - [pdf](http://arxiv.org/pdf/1812.03087v1)

> Machine Learning models are vulnerable to adversarial attacks that rely on perturbing the input data. This work proposes a novel strategy using Autoencoder Deep Neural Networks to defend a machine learning model against two gradient-based attacks: The Fast Gradient Sign attack and Fast Gradient attack. First we use an autoencoder to denoise the test data, which is trained with both clean and corrupted data. Then, we reduce the dimension of the denoised data using the hidden layer representation of another autoencoder. We perform this experiment for multiple values of the bound of adversarial perturbations, and consider different numbers of reduced dimensions. When the test data is preprocessed using this cascaded pipeline, the tested deep neural network classifier yields a much higher accuracy, thus mitigating the effect of the adversarial perturbation.

</details>

<details>

<summary>2018-12-07 19:25:52 - Deep-RBF Networks Revisited: Robust Classification with Rejection</summary>

- *Pourya Habib Zadeh, Reshad Hosseini, Suvrit Sra*

- `1812.03190v1` - [abs](http://arxiv.org/abs/1812.03190v1) - [pdf](http://arxiv.org/pdf/1812.03190v1)

> One of the main drawbacks of deep neural networks, like many other classifiers, is their vulnerability to adversarial attacks. An important reason for their vulnerability is assigning high confidence to regions with few or even no feature points. By feature points, we mean a nonlinear transformation of the input space extracting a meaningful representation of the input data. On the other hand, deep-RBF networks assign high confidence only to the regions containing enough feature points, but they have been discounted due to the widely-held belief that they have the vanishing gradient problem. In this paper, we revisit the deep-RBF networks by first giving a general formulation for them, and then proposing a family of cost functions thereof inspired by metric learning. In the proposed deep-RBF learning algorithm, the vanishing gradient problem does not occur. We make these networks robust to adversarial attack by adding the reject option to their output layer. Through several experiments on the MNIST dataset, we demonstrate that our proposed method not only achieves significant classification accuracy but is also very resistant to various adversarial attacks.

</details>

<details>

<summary>2018-12-07 22:23:13 - Reaching Data Confidentiality and Model Accountability on the CalTrain</summary>

- *Zhongshu Gu, Hani Jamjoom, Dong Su, Heqing Huang, Jialong Zhang, Tengfei Ma, Dimitrios Pendarakis, Ian Molloy*

- `1812.03230v1` - [abs](http://arxiv.org/abs/1812.03230v1) - [pdf](http://arxiv.org/pdf/1812.03230v1)

> Distributed collaborative learning (DCL) paradigms enable building joint machine learning models from distrusting multi-party participants. Data confidentiality is guaranteed by retaining private training data on each participant's local infrastructure. However, this approach to achieving data confidentiality makes today's DCL designs fundamentally vulnerable to data poisoning and backdoor attacks. It also limits DCL's model accountability, which is key to backtracking the responsible "bad" training data instances/contributors. In this paper, we introduce CALTRAIN, a Trusted Execution Environment (TEE) based centralized multi-party collaborative learning system that simultaneously achieves data confidentiality and model accountability. CALTRAIN enforces isolated computation on centrally aggregated training data to guarantee data confidentiality. To support building accountable learning models, we securely maintain the links between training instances and their corresponding contributors. Our evaluation shows that the models generated from CALTRAIN can achieve the same prediction accuracy when compared to the models trained in non-protected environments. We also demonstrate that when malicious training participants tend to implant backdoors during model training, CALTRAIN can accurately and precisely discover the poisoned and mislabeled training data that lead to the runtime mispredictions.

</details>

<details>

<summary>2018-12-08 03:52:47 - Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks</summary>

- *Yongshuai Liu, Jiyu Chen, Hao Chen*

- `1801.02850v2` - [abs](http://arxiv.org/abs/1801.02850v2) - [pdf](http://arxiv.org/pdf/1801.02850v2)

> Deep neural networks are vulnerable to adversarial examples. Prior defenses attempted to make deep networks more robust by either changing the network architecture or augmenting the training set with adversarial examples, but both have inherent limitations. Motivated by recent research that shows outliers in the training set have a high negative influence on the trained model, we studied the relationship between model robustness and the quality of the training set. We first show that outliers give the model better generalization ability but weaker robustness. Next, we propose an adversarial example detection framework, in which we design two methods for removing outliers from training set to obtain the sanitized model and then detect adversarial example by calculating the difference of outputs between the original and the sanitized model. We evaluated the framework on both MNIST and SVHN. Based on the difference measured by Kullback-Leibler divergence, we could detect adversarial examples with accuracy between 94.67% to 99.89%.

</details>

<details>

<summary>2018-12-08 11:52:43 - Detecting Adversarial Examples in Convolutional Neural Networks</summary>

- *Stefanos Pertigkiozoglou, Petros Maragos*

- `1812.03303v1` - [abs](http://arxiv.org/abs/1812.03303v1) - [pdf](http://arxiv.org/pdf/1812.03303v1)

> The great success of convolutional neural networks has caused a massive spread of the use of such models in a large variety of Computer Vision applications. However, these models are vulnerable to certain inputs, the adversarial examples, which although are not easily perceived by humans, they can lead a neural network to produce faulty results. This paper focuses on the detection of adversarial examples, which are created for convolutional neural networks that perform image classification. We propose three methods for detecting possible adversarial examples and after we analyze and compare their performance, we combine their best aspects to develop an even more robust approach. The first proposed method is based on the regularization of the feature vector that the neural network produces as output. The second method detects adversarial examples by using histograms, which are created from the outputs of the hidden layers of the neural network. These histograms create a feature vector which is used as the input of an SVM classifier, which classifies the original input either as an adversarial or as a real input. Finally, for the third method we introduce the concept of the residual image, which contains information about the parts of the input pattern that are ignored by the neural network. This method aims at the detection of possible adversarial examples, by using the residual image and reinforcing the parts of the input pattern that are ignored by the neural network. Each one of these methods has some novelties and by combining them we can further improve the detection results. For the proposed methods and their combination, we present the results of detecting adversarial examples on the MNIST dataset. The combination of the proposed methods offers some improvements over similar state of the art approaches.

</details>

<details>

<summary>2018-12-09 14:30:34 - Security Vulnerability of FDD Massive MIMO Systems in Downlink Training Phase</summary>

- *Mohammad Amin Sheikhi, S. Mohammad Razavizadeh*

- `1812.03492v1` - [abs](http://arxiv.org/abs/1812.03492v1) - [pdf](http://arxiv.org/pdf/1812.03492v1)

> We consider downlink channel training of a frequency division duplex (FDD) massive multiple-input-multiple-output (MIMO) system when a multi-antenna jammer is present in the network. The jammer intends to degrade mean square error (MSE) of the downlink channel training by designing an attack based on second-order statistics of its channel. The channels are assumed to be spatially correlated. First, a closed-form expression for the channel estimation MSE is derived and then the jammer determines the conditions under which the MSE is maximized. Numerical results demonstrate that the proposed jamming can severely increase the estimation MSE even if the optimal training signals with a large number of pilot symbols are used by the legitimate system.

</details>

<details>

<summary>2018-12-09 20:25:47 - A Precision Environment-Wide Association Study of Hypertension via Supervised Cadre Models</summary>

- *Alexander New, Kristin P. Bennett*

- `1808.04880v2` - [abs](http://arxiv.org/abs/1808.04880v2) - [pdf](http://arxiv.org/pdf/1808.04880v2)

> We consider the problem in precision health of grouping people into subpopulations based on their degree of vulnerability to a risk factor. These subpopulations cannot be discovered with traditional clustering techniques because their quality is evaluated with a supervised metric: the ease of modeling a response variable over observations within them. Instead, we apply the supervised cadre model (SCM), which does use this metric. We extend the SCM formalism so that it may be applied to multivariate regression and binary classification problems. We also develop a way to use conditional entropy to assess the confidence in the process by which a subject is assigned their cadre. Using the SCM, we generalize the environment-wide association study (EWAS) workflow to be able to model heterogeneity in population risk. In our EWAS, we consider more than two hundred environmental exposure factors and find their association with diastolic blood pressure, systolic blood pressure, and hypertension. This requires adapting the SCM to be applicable to data generated by a complex survey design. After correcting for false positives, we found 25 exposure variables that had a significant association with at least one of our response variables. Eight of these were significant for a discovered subpopulation but not for the overall population. Some of these associations have been identified by previous researchers, while others appear to be novel. We examine several discovered subpopulations in detail, and we find that they are interpretable and that they suggest further research questions.

</details>

<details>

<summary>2018-12-10 09:30:57 - Provenance-enabled Packet Path Tracing in the RPL-based Internet of Things</summary>

- *Sabah Suhail, Mohammad Abdellatif, Shashi Raj Pandey, Abid Khan, Choong Seon Hong*

- `1811.06143v3` - [abs](http://arxiv.org/abs/1811.06143v3) - [pdf](http://arxiv.org/pdf/1811.06143v3)

> The interconnection of resource-constrained and globally accessible things with untrusted and unreliable Internet make them vulnerable to attacks including data forging, false data injection, and packet drop that affects applications with critical decision-making processes. For data trustworthiness, reliance on provenance is considered to be an effective mechanism that tracks both data acquisition and data transmission. However, provenance management for sensor networks introduces several challenges, such as low energy, bandwidth consumption, and efficient storage. This paper attempts to identify packet drop (either maliciously or due to network disruptions) and detect faulty or misbehaving nodes in the Routing Protocol for Low-Power and Lossy Networks (RPL) by following a bi-fold provenance-enabled packed path tracing (PPPT) approach. Firstly, a system-level ordered-provenance information encapsulates the data generating nodes and the forwarding nodes in the data packet. Secondly, to closely monitor the dropped packets, a node-level provenance in the form of the packet sequence number is enclosed as a routing entry in the routing table of each participating node. Lossless in nature, both approaches conserve the provenance size satisfying processing and storage requirements of IoT devices. Finally, we evaluate the efficacy of the proposed scheme with respect to provenance size, provenance generation time, and energy consumption.

</details>

<details>

<summary>2018-12-10 10:38:30 - Security Code Smells in Android ICC</summary>

- *Pascal Gadient, Mohammad Ghafari, Patrick Frischknecht, Oscar Nierstrasz*

- `1811.12713v2` - [abs](http://arxiv.org/abs/1811.12713v2) - [pdf](http://arxiv.org/pdf/1811.12713v2)

> Android Inter-Component Communication (ICC) is complex, largely unconstrained, and hard for developers to understand. As a consequence, ICC is a common source of security vulnerability in Android apps. To promote secure programming practices, we have reviewed related research, and identified avoidable ICC vulnerabilities in Android-run devices and the security code smells that indicate their presence. We explain the vulnerabilities and their corresponding smells, and we discuss how they can be eliminated or mitigated during development. We present a lightweight static analysis tool on top of Android Lint that analyzes the code under development and provides just-in-time feedback within the IDE about the presence of such smells in the code. Moreover, with the help of this tool we study the prevalence of security code smells in more than 700 open-source apps, and manually inspect around 15% of the apps to assess the extent to which identifying such smells uncovers ICC security vulnerabilities.

</details>

<details>

<summary>2018-12-10 10:57:00 - TxProbe: Discovering Bitcoin's Network Topology Using Orphan Transactions</summary>

- *Sergi Delgado-Segura, Surya Bakshi, Cristina Pérez-Solà, James Litton, Andrew Pachulski, Andrew Miller, Bobby Bhattacharjee*

- `1812.00942v2` - [abs](http://arxiv.org/abs/1812.00942v2) - [pdf](http://arxiv.org/pdf/1812.00942v2)

> Bitcoin relies on a peer-to-peer overlay network to broadcast transactions and blocks. From the viewpoint of network measurement, we would like to observe this topology so we can characterize its performance, fairness and robustness. However, this is difficult because Bitcoin is deliberately designed to hide its topology from onlookers. Knowledge of the topology is not in itself a vulnerability, although it could conceivably help an attacker performing targeted eclipse attacks or to deanonymize transaction senders.   In this paper we present TxProbe, a novel technique for reconstructing the Bitcoin network topology. TxProbe makes use of peculiarities in how Bitcoin processes out of order, or "orphaned" transactions. We conducted experiments on Bitcoin testnet that suggest our technique reconstructs topology with precision and recall surpassing 90%. We also used TxProbe to take a snapshot of the Bitcoin testnet in just a few hours. TxProbe may be useful for future measurement campaigns of Bitcoin or other cryptocurrency networks.

</details>

<details>

<summary>2018-12-10 16:31:52 - Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders</summary>

- *Partha Ghosh, Arpan Losalka, Michael J Black*

- `1806.00081v2` - [abs](http://arxiv.org/abs/1806.00081v2) - [pdf](http://arxiv.org/pdf/1806.00081v2)

> Susceptibility of deep neural networks to adversarial attacks poses a major theoretical and practical challenge. All efforts to harden classifiers against such attacks have seen limited success. Two distinct categories of samples to which deep networks are vulnerable, "adversarial samples" and "fooling samples", have been tackled separately so far due to the difficulty posed when considered together. In this work, we show how one can address them both under one unified framework. We tie a discriminative model with a generative model, rendering the adversarial objective to entail a conflict. Our model has the form of a variational autoencoder, with a Gaussian mixture prior on the latent vector. Each mixture component of the prior distribution corresponds to one of the classes in the data. This enables us to perform selective classification, leading to the rejection of adversarial samples instead of misclassification. Our method inherently provides a way of learning a selective classifier in a semi-supervised scenario as well, which can resist adversarial attacks. We also show how one can reclassify the rejected adversarial samples.

</details>

<details>

<summary>2018-12-11 02:30:27 - Code-less Patching for Heap Vulnerabilities Using Targeted Calling Context Encoding</summary>

- *Qiang Zeng, Golam Kayas, Emil Mohammed, Lannan Luo, Xiaojiang Du, Junghwan Rhee*

- `1812.04191v1` - [abs](http://arxiv.org/abs/1812.04191v1) - [pdf](http://arxiv.org/pdf/1812.04191v1)

> Exploitation of heap vulnerabilities has been on the rise, leading to many devastating attacks. Conventional heap patch generation is a lengthy procedure, requiring intensive manual efforts. Worse, fresh patches tend to harm system dependability, hence deterring users from deploying them. We propose a heap patching system that simultaneously has the following prominent advantages: (1) generating patches without manual efforts; (2) installing patches without altering the code (so called code-less patching); (3) handling various heap vulnerability types; (4) imposing a very low overhead; and (5) no dependency on specific heap allocators. As a separate contribution, we propose targeted calling context encoding, which is a suite of algorithms for optimizing calling context encoding, an important technique with applications in many areas. The system properly combines heavyweight offline attack analysis with lightweight online defense generation, and provides a new countermeasure against heap attacks. The evaluation shows that the system is effective and efficient.

</details>

<details>

<summary>2018-12-11 04:35:02 - Privacy-preserving data aggregation in resource-constrained sensor nodes in Internet of Things: A review</summary>

- *Inayat Ali, Sonia Sabir, Eraj Khan*

- `1812.04216v1` - [abs](http://arxiv.org/abs/1812.04216v1) - [pdf](http://arxiv.org/pdf/1812.04216v1)

> Privacy problems are lethal and getting more attention than any other issue with the notion of the Internet of Things (IoT). Since IoT has many application areas including smart home, smart grids, smart healthcare system, smart and intelligent transportation and many more. Most of these applications are fueled by the resource-constrained sensor network, such as Smart healthcare system is powered by Wireless Body Area Network (WBAN) and Smart home and weather monitoring systems are fueled by Wireless Sensor Networks (WSN). In the mentioned application areas sensor node life is a very important aspect of these technologies as it explicitly effects the network life and performance. Data aggregation techniques are used to increase sensor node life by decreasing communication overhead. However, when the data is aggregated at intermediate nodes to reduce communication overhead, data privacy problems becomes more vulnerable. Different Privacy-Preserving Data Aggregation (PPDA) techniques have been proposed to ensure data privacy during data aggregation in resource-constrained sensor nodes. We provide a review and comparative analysis of the state of the art PPDA techniques in this paper. The comparative analysis is based on Computation Cost, Communication overhead, Privacy Level, resistance against malicious aggregator, sensor node life and energy consumption by the sensor node. We have studied the most recent techniques and provide in-depth analysis of the minute steps involved in these techniques. To the best of our knowledge, this survey is the most recent and comprehensive study of PPDA techniques.

</details>

<details>

<summary>2018-12-11 06:48:59 - Intelligence-based Cybersecurity Awareness Training- an Exploratory Project</summary>

- *Tam n. Nguyen, Lydia Sbityakov, Samantha Scoggins*

- `1812.04234v1` - [abs](http://arxiv.org/abs/1812.04234v1) - [pdf](http://arxiv.org/pdf/1812.04234v1)

> Cybersecurity training should be adaptable to evolving the cyber threat landscape, cost effective and integrated well with other enterprise management components. Unfortunately, very few cybersecurity training platforms can satisfy such requirements. This paper proposes a new and novel model for conducting cybersecurity training with three main objectives: (i) training should be initiated by emerging relevant threats and delivered first to the most vulnerable members (ii) the process has to be agile (iii) training results must be able to provide actionable intelligence. For the first time, this paper establishes a type system (ontology and associated relationships) that links the domain of cybersecurity awareness training with that of cyber threat intelligence. Powered by IBM Watson Knowledge Studio platform, the proposed method was found to be practical and scalable. Main contributions such as exports of the type system, the manually annotated corpus of 100 threat reports and 127 cybersecurity assessment results, the dictionaries for pre-annotation, etc were made publicly available.

</details>

<details>

<summary>2018-12-11 19:27:14 - Information Security Risks Assessment: A Case Study</summary>

- *Samuel Cris Ayo, Bonaventure Ngala, Olasunkanmi Amzat, Robin Lal Khoshi, Samarappulige Isuru Madusanka*

- `1812.04659v1` - [abs](http://arxiv.org/abs/1812.04659v1) - [pdf](http://arxiv.org/pdf/1812.04659v1)

> Owing to recorded incidents of Information technology inclined organisations failing to respond effectively to threat incidents, this project outlines the benefits of conducting a comprehensive risk assessment which would aid proficiency in responding to potential threats. The ultimate goal is primarily to identify, quantify and control the key threats that are detrimental to achieving business objectives. This project carries out a detailed risk assessment for a case study organisation. It includes a comprehensive literature review analysing several professional views on pressing issues in Information security. In the risk register, five prominent assets were identified in respect to their owners. The work is followed by a qualitative analysis methodology to determine the magnitude of the potential threats and vulnerabilities. Collating these parameters enabled the valuation of individual risk per asset, per threat and vulnerability. Evaluating a risk appetite aided in prioritising and determining acceptable risks. From the analysis, it was deduced that human being posed the greatest Information security risk through intentional/ unintentional human error. In conclusion, effective control techniques based on defence in-depth were devised to mitigate the impact of the identified risks from risk register.

</details>

<details>

<summary>2018-12-12 08:54:29 - Recurrent Neural Networks for Fuzz Testing Web Browsers</summary>

- *Martin Sablotny, Bjørn Sand Jensen, Chris W. Johnson*

- `1812.04852v1` - [abs](http://arxiv.org/abs/1812.04852v1) - [pdf](http://arxiv.org/pdf/1812.04852v1)

> Generation-based fuzzing is a software testing approach which is able to discover different types of bugs and vulnerabilities in software. It is, however, known to be very time consuming to design and fine tune classical fuzzers to achieve acceptable coverage, even for small-scale software systems. To address this issue, we investigate a machine learning-based approach to fuzz testing in which we outline a family of test-case generators based on Recurrent Neural Networks (RNNs) and train those on readily available datasets with a minimum of human fine tuning. The proposed generators do, in contrast to previous work, not rely on heuristic sampling strategies but principled sampling from the predictive distributions. We provide a detailed analysis to demonstrate the characteristics and efficacy of the proposed generators in a challenging web browser testing scenario. The empirical results show that the RNN-based generators are able to provide better coverage than a mutation based method and are able to discover paths not discovered by a classical fuzzer. Our results supplement findings in other domains suggesting that generation based fuzzing with RNNs is a viable route to better software quality conditioned on the use of a suitable model selection/analysis procedure.

</details>

<details>

<summary>2018-12-13 05:32:43 - TextBugger: Generating Adversarial Text Against Real-world Applications</summary>

- *Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, Ting Wang*

- `1812.05271v1` - [abs](http://arxiv.org/abs/1812.05271v1) - [pdf](http://arxiv.org/pdf/1812.05271v1)

> Deep Learning-based Text Understanding (DLTU) is the backbone technique behind various applications, including question answering, machine translation, and text classification. Despite its tremendous popularity, the security vulnerabilities of DLTU are still largely unknown, which is highly concerning given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection. In this paper, we show that DLTU is inherently vulnerable to adversarial text attacks, in which maliciously crafted texts trigger target DLTU systems and services to misbehave. Specifically, we present TextBugger, a general attack framework for generating adversarial texts. In contrast to prior works, TextBugger differs in significant ways: (i) effective -- it outperforms state-of-the-art attacks in terms of attack success rate; (ii) evasive -- it preserves the utility of benign text, with 94.9\% of the adversarial text correctly recognized by human readers; and (iii) efficient -- it generates adversarial text with computational complexity sub-linear to the text length. We empirically evaluate TextBugger on a set of real-world DLTU systems and services used for sentiment analysis and toxic content detection, demonstrating its effectiveness, evasiveness, and efficiency. For instance, TextBugger achieves 100\% success rate on the IMDB dataset based on Amazon AWS Comprehend within 4.61 seconds and preserves 97\% semantic similarity. We further discuss possible defense mechanisms to mitigate such attack and the adversary's potential countermeasures, which leads to promising directions for further research.

</details>

<details>

<summary>2018-12-13 09:50:18 - Breaking the borders: an investigation of cross-ecosystem software packages</summary>

- *Eleni Constantinou, Alexandre Decan, Tom Mens*

- `1812.04868v2` - [abs](http://arxiv.org/abs/1812.04868v2) - [pdf](http://arxiv.org/pdf/1812.04868v2)

> Software ecosystems are collections of projects that are developed and evolve together in the same environment. Existing literature investigates software ecosystems as isolated entities whose boundaries do not overlap and assumes they are self-contained. However, a number of software projects are distributed in more than one ecosystem. As different aspects, e.g., success, security vulnerabilities, bugs, etc., of such cross-ecosystem packages can affect multiple ecosystems, we investigate the presence and characteristics of these cross-ecosystem packages in 12 large software distributions. We found a small number of packages distributed in multiple packaging ecosystems and that such packages are usually distributed in two ecosystems. These packages tend to better support with new releases certain ecosystems, while their evolution can impact a multitude of packages in other ecosystems. Finally, such packages appear to be popular with large developer communities.

</details>

<details>

<summary>2018-12-13 12:23:45 - Use Dimensionality Reduction and SVM Methods to Increase the Penetration Rate of Computer Networks</summary>

- *Amir Moradibaad, Ramin Jalilian Mashhoud*

- `1812.03173v2` - [abs](http://arxiv.org/abs/1812.03173v2) - [pdf](http://arxiv.org/pdf/1812.03173v2)

> In the world today computer networks have a very important position and most of the urban and national infrastructure as well as organizations are managed by computer networks, therefore, the security of these systems against the planned attacks is of great importance. Therefore, researchers have been trying to find these vulnerabilities so that after identifying ways to penetrate the system, they will provide system protection through preventive or countermeasures. SVM is one of the major algorithms for intrusion detection. In this research, we studied a variety of malware and methods of intrusion detection, provide an efficient method for detecting attacks and utilizing dimension reduction.Thus, we will be able to detect attacks by carefully combining these two algorithms and pre-processes that are performed before the two on the input data. The main question raised is how we can identify attacks on computer networks with the above-mentioned method. In anomalies diagnostic method, by identifying behavior as a normal behavior for the user, the host, or the whole system, any deviation from this behavior is considered as an abnormal behavior, which can be a potential occurrence of an attack. The network intrusion detection system is used by anomaly detection method that uses the SVM algorithm for classification and SVD to reduce the size. Steps of the proposed method include pre-processing of the data set, feature selection, support vector machine, and evaluation.The NSL-KDD data set has been used to teach and test the proposed model. In this study, we inferred the intrusion detection using the SVM algorithm for classification and SVD for diminishing dimensions with no classification algorithm.Also the KNN algorithm has been compared in situations with and without diminishing dimensions,the results have shown that the proposed method has a better performance than comparable methods.

</details>

<details>

<summary>2018-12-13 16:41:41 - Defending Against Machine Learning Model Stealing Attacks Using Deceptive Perturbations</summary>

- *Taesung Lee, Benjamin Edwards, Ian Molloy, Dong Su*

- `1806.00054v4` - [abs](http://arxiv.org/abs/1806.00054v4) - [pdf](http://arxiv.org/pdf/1806.00054v4)

> Machine learning models are vulnerable to simple model stealing attacks if the adversary can obtain output labels for chosen inputs. To protect against these attacks, it has been proposed to limit the information provided to the adversary by omitting probability scores, significantly impacting the utility of the provided service. In this work, we illustrate how a service provider can still provide useful, albeit misleading, class probability information, while significantly limiting the success of the attack. Our defense forces the adversary to discard the class probabilities, requiring significantly more queries before they can train a model with comparable performance. We evaluate several attack strategies, model architectures, and hyperparameters under varying adversarial models, and evaluate the efficacy of our defense against the strongest adversary. Finally, we quantify the amount of noise injected into the class probabilities to mesure the loss in utility, e.g., adding 1.26 nats per query on CIFAR-10 and 3.27 on MNIST. Our evaluation shows our defense can degrade the accuracy of the stolen model at least 20%, or require up to 64 times more queries while keeping the accuracy of the protected model almost intact.

</details>

<details>

<summary>2018-12-14 13:54:34 - Sereum: Protecting Existing Smart Contracts Against Re-Entrancy Attacks</summary>

- *Michael Rodler, Wenting Li, Ghassan O. Karame, Lucas Davi*

- `1812.05934v1` - [abs](http://arxiv.org/abs/1812.05934v1) - [pdf](http://arxiv.org/pdf/1812.05934v1)

> Recently, a number of existing blockchain systems have witnessed major bugs and vulnerabilities within smart contracts. Although the literature features a number of proposals for securing smart contracts, these proposals mostly focus on proving the correctness or absence of a certain type of vulnerability within a contract, but cannot protect deployed (legacy) contracts from being exploited. In this paper, we address this problem in the context of re-entrancy exploits and propose a novel smart contract security technology, dubbed Sereum (Secure Ethereum), which protects existing, deployed contracts against re-entrancy attacks in a backwards compatible way based on run-time monitoring and validation. Sereum does neither require any modification nor any semantic knowledge of existing contracts. By means of implementation and evaluation using the Ethereum blockchain, we show that Sereum covers the actual execution flow of a smart contract to accurately detect and prevent attacks with a false positive rate as small as 0.06% and with negligible run-time overhead. As a by-product, we develop three advanced re-entrancy attacks to demonstrate the limitations of existing offline vulnerability analysis tools.

</details>

<details>

<summary>2018-12-15 03:06:45 - A Survey of Privacy Infrastructures and Their Vulnerabilities</summary>

- *Tian Yunfan, Zhang Xiang*

- `1812.06226v1` - [abs](http://arxiv.org/abs/1812.06226v1) - [pdf](http://arxiv.org/pdf/1812.06226v1)

> Over the last two decades, the scale and complexity of Anonymous networks and its associated technologies grows exponentially as privacy has become a major concern of individuals. Also, some cyber attackers make use of privacy infrastructures including botnets and Tor to do illegal activities like drug, contraband or DDoS attack. However, anonymous networks are not perfect, there are some methods could exploit the vulnerabilities and track user information. In this paper, we analyze few of privacy infrastructures and their vulnerabilities.

</details>

<details>

<summary>2018-12-16 00:52:18 - Trust Region Based Adversarial Attack on Neural Networks</summary>

- *Zhewei Yao, Amir Gholami, Peng Xu, Kurt Keutzer, Michael Mahoney*

- `1812.06371v1` - [abs](http://arxiv.org/abs/1812.06371v1) - [pdf](http://arxiv.org/pdf/1812.06371v1)

> Deep Neural Networks are quite vulnerable to adversarial perturbations. Current state-of-the-art adversarial attack methods typically require very time consuming hyper-parameter tuning, or require many iterations to solve an optimization based adversarial attack. To address this problem, we present a new family of trust region based adversarial attacks, with the goal of computing adversarial perturbations efficiently. We propose several attacks based on variants of the trust region optimization method. We test the proposed methods on Cifar-10 and ImageNet datasets using several different models including AlexNet, ResNet-50, VGG-16, and DenseNet-121 models. Our methods achieve comparable results with the Carlini-Wagner (CW) attack, but with significant speed up of up to $37\times$, for the VGG-16 model on a Titan Xp GPU. For the case of ResNet-50 on ImageNet, we can bring down its classification accuracy to less than 0.1\% with at most $1.5\%$ relative $L_\infty$ (or $L_2$) perturbation requiring only $1.02$ seconds as compared to $27.04$ seconds for the CW attack. We have open sourced our method which can be accessed at [1].

</details>

<details>

<summary>2018-12-16 21:50:13 - Neural Machine Translation Inspired Binary Code Similarity Comparison beyond Function Pairs</summary>

- *Fei Zuo, Xiaopeng Li, Patrick Young, Lannan Luo, Qiang Zeng, Zhexin Zhang*

- `1808.04706v2` - [abs](http://arxiv.org/abs/1808.04706v2) - [pdf](http://arxiv.org/pdf/1808.04706v2)

> Binary code analysis allows analyzing binary code without having access to the corresponding source code. A binary, after disassembly, is expressed in an assembly language. This inspires us to approach binary analysis by leveraging ideas and techniques from Natural Language Processing (NLP), a rich area focused on processing text of various natural languages. We notice that binary code analysis and NLP share a lot of analogical topics, such as semantics extraction, summarization, and classification. This work utilizes these ideas to address two important code similarity comparison problems. (I) Given a pair of basic blocks for different instruction set architectures (ISAs), determining whether their semantics is similar or not; and (II) given a piece of code of interest, determining if it is contained in another piece of assembly code for a different ISA. The solutions to these two problems have many applications, such as cross-architecture vulnerability discovery and code plagiarism detection. We implement a prototype system INNEREYE and perform a comprehensive evaluation. A comparison between our approach and existing approaches to Problem I shows that our system outperforms them in terms of accuracy, efficiency and scalability. And the case studies utilizing the system demonstrate that our solution to Problem II is effective. Moreover, this research showcases how to apply ideas and techniques from NLP to large-scale binary code analysis.

</details>

<details>

<summary>2018-12-17 08:15:00 - Countering Selfish Mining in Blockchains</summary>

- *Muhammad Saad, Laurent Njilla, Charles Kamhoua, Aziz Mohaisen*

- `1811.09943v2` - [abs](http://arxiv.org/abs/1811.09943v2) - [pdf](http://arxiv.org/pdf/1811.09943v2)

> Selfish mining is a well known vulnerability in blockchains exploited by miners to steal block rewards. In this paper, we explore a new form of selfish mining attack that guarantees high rewards with low cost. We show the feasibility of this attack facilitated by recent developments in blockchain technology opening new attack avenues. By outlining the limitations of existing countermeasures, we highlight a need for new defense strategies to counter this attack, and leverage key system parameters in blockchain applications to propose an algorithm that enforces fair mining. We use the expected transaction confirmation height and block publishing height to detect selfish mining behavior and develop a network-wide defense mechanism to disincentivize selfish miners. Our design involves a simple modifications to transactions' data structure in order to obtain a "truth state" used to catch the selfish miners and prevent honest miners from losing block rewards.

</details>

<details>

<summary>2018-12-17 14:55:41 - Spartan Networks: Self-Feature-Squeezing Neural Networks for increased robustness in adversarial settings</summary>

- *François Menet, Paul Berthier, José M. Fernandez, Michel Gagnon*

- `1812.06815v1` - [abs](http://arxiv.org/abs/1812.06815v1) - [pdf](http://arxiv.org/pdf/1812.06815v1)

> Deep learning models are vulnerable to adversarial examples which are input samples modified in order to maximize the error on the system. We introduce Spartan Networks, resistant deep neural networks that do not require input preprocessing nor adversarial training. These networks have an adversarial layer designed to discard some information of the network, thus forcing the system to focus on relevant input. This is done using a new activation function to discard data. The added layer trains the neural network to filter-out usually-irrelevant parts of its input. Our performance evaluation shows that Spartan Networks have a slightly lower precision but report a higher robustness under attack when compared to unprotected models. Results of this study of Adversarial AI as a new attack vector are based on tests conducted on the MNIST dataset.

</details>

<details>

<summary>2018-12-18 18:59:19 - Security and Privacy Issues for Connected Vehicles</summary>

- *Wenjun Xiong, Robert Lagerström*

- `1812.04967v2` - [abs](http://arxiv.org/abs/1812.04967v2) - [pdf](http://arxiv.org/pdf/1812.04967v2)

> Modern vehicles contain more than a hundred Electronic Control Units (ECUs) that communicate over different in-vehicle networks, and they are often connected to the Internet, which makes them vulnerable to various cyber-attacks. Besides, data collected by the connected vehicles is directly connected to the vehicular network. Thus, big vehicular data are collected, which are valuable and generate insights into driver behavior. Previously, a probabilistic modeling and simulation language named vehicleLang is presented to analyze the security of connected vehicles. However, the privacy issues of vehicular data have not been addressed. To fill in the gap, this work present a privacy specification for vehicles based on vehicleLang, which uses the Meta Attack Language (MAL) to assess the security of connected vehicles in a formal way, with a special focus on the privacy aspect. To evaluate this work, test cases are also presented.

</details>

<details>

<summary>2018-12-19 11:57:33 - AnFlo: Detecting Anomalous Sensitive Information Flows in Android Apps</summary>

- *Biniam Fisseha Demissie, Mariano Ceccato, Lwin Khin Shar*

- `1812.07894v1` - [abs](http://arxiv.org/abs/1812.07894v1) - [pdf](http://arxiv.org/pdf/1812.07894v1)

> Smartphone apps usually have access to sensitive user data such as contacts, geo-location, and account credentials and they might share such data to external entities through the Internet or with other apps. Confidentiality of user data could be breached if there are anomalies in the way sensitive data is handled by an app which is vulnerable or malicious. Existing approaches that detect anomalous sensitive data flows have limitations in terms of accuracy because the definition of anomalous flows may differ for different apps with different functionalities; it is normal for "Health" apps to share heart rate information through the Internet but is anomalous for "Travel" apps.   In this paper, we propose a novel approach to detect anomalous sensitive data flows in Android apps, with improved accuracy. To achieve this objective, we first group trusted apps according to the topics inferred from their functional descriptions. We then learn sensitive information flows with respect to each group of trusted apps. For a given app under analysis, anomalies are identified by comparing sensitive information flows in the app against those flows learned from trusted apps grouped under the same topic. In the evaluation, information flow is learned from 11,796 trusted apps. We then checked for anomalies in 596 new (benign) apps and identified 2 previously-unknown vulnerable apps related to anomalous flows. We also analyzed 18 malware apps and found anomalies in 6 of them.

</details>

<details>

<summary>2018-12-20 01:53:51 - Deep Defense: Training DNNs with Improved Adversarial Robustness</summary>

- *Ziang Yan, Yiwen Guo, Changshui Zhang*

- `1803.00404v3` - [abs](http://arxiv.org/abs/1803.00404v3) - [pdf](http://arxiv.org/pdf/1803.00404v3)

> Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named "deep defense". Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results are available at https://github.com/ZiangYan/deepdefense.pytorch

</details>

<details>

<summary>2018-12-24 08:25:50 - Detection based Defense against Adversarial Examples from the Steganalysis Point of View</summary>

- *Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu, Hongyue Zha, Nenghai Yu*

- `1806.09186v3` - [abs](http://arxiv.org/abs/1806.09186v3) - [pdf](http://arxiv.org/pdf/1806.09186v3)

> Deep Neural Networks (DNNs) have recently led to significant improvements in many fields. However, DNNs are vulnerable to adversarial examples which are samples with imperceptible perturbations while dramatically misleading the DNNs. Moreover, adversarial examples can be used to perform an attack on various kinds of DNN based systems, even if the adversary has no access to the underlying model. Many defense methods have been proposed, such as obfuscating gradients of the networks or detecting adversarial examples. However it is proved out that these defense methods are not effective or cannot resist secondary adversarial attacks. In this paper, we point out that steganalysis can be applied to adversarial examples detection, and propose a method to enhance steganalysis features by estimating the probability of modifications caused by adversarial attacks. Experimental results show that the proposed method can accurately detect adversarial examples. Moreover, secondary adversarial attacks cannot be directly performed to our method because our method is not based on a neural network but based on high-dimensional artificial features and FLD (Fisher Linear Discriminant) ensemble.

</details>

<details>

<summary>2018-12-31 16:34:54 - Variational Bayesian Inference for Robust Streaming Tensor Factorization and Completion</summary>

- *Cole Hawkins, Zheng Zhang*

- `1809.02153v2` - [abs](http://arxiv.org/abs/1809.02153v2) - [pdf](http://arxiv.org/pdf/1809.02153v2)

> Streaming tensor factorization is a powerful tool for processing high-volume and multi-way temporal data in Internet networks, recommender systems and image/video data analysis. Existing streaming tensor factorization algorithms rely on least-squares data fitting and they do not possess a mechanism for tensor rank determination. This leaves them susceptible to outliers and vulnerable to over-fitting. This paper presents a Bayesian robust streaming tensor factorization model to identify sparse outliers, automatically determine the underlying tensor rank and accurately fit low-rank structure. We implement our model in Matlab and compare it with existing algorithms on tensor datasets generated from dynamic MRI and Internet traffic.

</details>

<details>

<summary>2018-12-31 16:35:08 - RF Jamming Classification using Relative Speed Estimation in Vehicular Wireless Networks</summary>

- *Dimitrios Kosmanos, Dimitrios Karagiannis, Antonios Argyriou, Spyros Lalis, Leandros Maglaras*

- `1812.11886v1` - [abs](http://arxiv.org/abs/1812.11886v1) - [pdf](http://arxiv.org/pdf/1812.11886v1)

> Wireless communications are vulnerable against radio frequency (RF) jamming which might be caused either intentionally or unintentionally. A particular subset of wireless networks, vehicular ad-hoc networks (VANET) which incorporate a series of safety-critical applications, may be a potential target of RF jamming with detrimental safety effects. To ensure secure communication and defend it against this type of attacks, an accurate detection scheme must be adopted. In this paper we introduce a detection scheme that is based on supervised learning. The machine-learning algorithms, KNearest Neighbors (KNN) and Random Forests (RF), utilize a series of features among which is the metric of the variations of relative speed (VRS) between the jammer and the receiver that is passively estimated from the combined value of the useful and the jamming signal at the receiver. To the best of our knowledge, this metric has never been utilized before in a machine-learning detection scheme in the literature. Through offline training and the proposed KNN-VRS, RF-VRS classification algorithms, we are able to efficiently detect various cases of Denial of Service Attacks (DoS) jamming attacks, differentiate them from cases of interference as well as foresee a potential danger successfully and act accordingly.

</details>

