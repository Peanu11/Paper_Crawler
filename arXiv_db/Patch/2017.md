# 2017

## TOC

- [2017-03](#2017-03)
- [2017-04](#2017-04)
- [2017-05](#2017-05)
- [2017-06](#2017-06)
- [2017-07](#2017-07)
- [2017-08](#2017-08)
- [2017-09](#2017-09)
- [2017-10](#2017-10)
- [2017-11](#2017-11)
- [2017-12](#2017-12)

## 2017-03

<details>

<summary>2017-03-29 05:58:21 - LabelBank: Revisiting Global Perspectives for Semantic Segmentation</summary>

- *Hexiang Hu, Zhiwei Deng, Guang-Tong Zhou, Fei Sha, Greg Mori*

- `1703.09891v1` - [abs](http://arxiv.org/abs/1703.09891v1) - [pdf](http://arxiv.org/pdf/1703.09891v1)

> Semantic segmentation requires a detailed labeling of image pixels by object category. Information derived from local image patches is necessary to describe the detailed shape of individual objects. However, this information is ambiguous and can result in noisy labels. Global inference of image content can instead capture the general semantic concepts present. We advocate that holistic inference of image concepts provides valuable information for detailed pixel labeling. We propose a generic framework to leverage holistic information in the form of a LabelBank for pixel-level segmentation.   We show the ability of our framework to improve semantic segmentation performance in a variety of settings. We learn models for extracting a holistic LabelBank from visual cues, attributes, and/or textual descriptions. We demonstrate improvements in semantic segmentation accuracy on standard datasets across a range of state-of-the-art segmentation architectures and holistic inference approaches.

</details>


## 2017-04

<details>

<summary>2017-04-11 13:34:17 - Harmonic Networks: Deep Translation and Rotation Equivariance</summary>

- *Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, Gabriel J. Brostow*

- `1612.04642v2` - [abs](http://arxiv.org/abs/1612.04642v2) - [pdf](http://arxiv.org/pdf/1612.04642v2)

> Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch.   H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.

</details>

<details>

<summary>2017-04-27 16:15:30 - Learning Correspondence Structures for Person Re-identification</summary>

- *Weiyao Lin, Yang Shen, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong Wang, Ke Lu*

- `1703.06931v3` - [abs](http://arxiv.org/abs/1703.06931v3) - [pdf](http://arxiv.org/pdf/1703.06931v3)

> This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global constraint-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Finally, we also extend our approach by introducing a multi-structure scheme, which learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, so as to handle the spatial misalignments between individual images in a more precise way. Experimental results on various datasets demonstrate the effectiveness of our approach.

</details>

<details>

<summary>2017-04-29 05:01:08 - Evaluating Security and Availability of Multiple Redundancy Designs when Applying Security Patches</summary>

- *Mengmeng Ge, Huy Kang Kim, Dong Seong Kim*

- `1705.00128v1` - [abs](http://arxiv.org/abs/1705.00128v1) - [pdf](http://arxiv.org/pdf/1705.00128v1)

> In most of modern enterprise systems, redundancy configuration is often considered to provide availability during the part of such systems is being patched. However, the redundancy may increase the attack surface of the system. In this paper, we model and assess the security and capacity oriented availability of multiple server redundancy designs when applying security patches to the servers. We construct (1) a graphical security model to evaluate the security under potential attacks before and after applying patches, (2) a stochastic reward net model to assess the capacity oriented availability of the system with a patch schedule. We present our approach based on case study and model-based evaluation for multiple design choices. The results show redundancy designs increase capacity oriented availability but decrease security when applying security patches. We define functions that compare values of security metrics and capacity oriented availability with the chosen upper/lower bounds to find design choices that satisfy both security and availability requirements.

</details>


## 2017-05

<details>

<summary>2017-05-02 06:17:31 - ACDC: Altering Control Dependence Chains for Automated Patch Generation</summary>

- *Rawad Abou Assi, Chadi Trad, Wes Masri*

- `1705.00811v1` - [abs](http://arxiv.org/abs/1705.00811v1) - [pdf](http://arxiv.org/pdf/1705.00811v1)

> Once a failure is observed, the primary concern of the developer is to identify what caused it in order to repair the code that induced the incorrect behavior. Until a permanent repair is afforded, code repair patches are invaluable. The aim of this work is to devise an automated patch generation technique that proceeds as follows: Step1) It identifies a set of failure-causing control dependence chains that are minimal in terms of number and length. Step2) It identifies a set of predicates within the chains along with associated execution instances, such that negating the predicates at the given instances would exhibit correct behavior. Step3) For each candidate predicate, it creates a classifier that dictates when the predicate should be negated to yield correct program behavior. Step4) Prior to each candidate predicate, the faulty program is injected with a call to its corresponding classifier passing it the program state and getting a return value predictively indicating whether to negate the predicate or not. The role of the classifiers is to ensure that: 1) the predicates are not negated during passing runs; and 2) the predicates are negated at the appropriate instances within failing runs. We implemented our patch generation approach for the Java platform and evaluated our toolset using 148 defects from the Introclass and Siemens benchmarks. The toolset identified 56 full patches and another 46 partial patches, and the classification accuracy averaged 84%.

</details>

<details>

<summary>2017-05-12 02:13:26 - Can defects be fixed with weak test suites? An analysis of 50 defects from Defects4J</summary>

- *Jiajun Jiang, Yingfei Xiong*

- `1705.04149v2` - [abs](http://arxiv.org/abs/1705.04149v2) - [pdf](http://arxiv.org/pdf/1705.04149v2)

> Automated program repair techniques, which target to generating correct patches for real world defects automatically, have gained a lot of attention in the last decade. Many different techniques and tools have been proposed and developed. However, even the most sophisticated program repair techniques can only repair a small portion of defects while producing a lot of incorrect patches. A possible reason for this low performance is that the test suites of real world programs are usually too weak to guarantee the behavior of the program. To understand to what extent defects can be fixed with weak test suites, we analyzed 50 real world defects from Defects4J, in which we found that up to 84% of them could be correctly fixed. This result suggests that there is plenty of space for current automated program repair techniques to improve. Furthermore, we summarized seven fault localization strategies and seven patch generation strategies that were useful in localizing and fixing these defects, and compared those strategies with current repair techniques. The results indicate potential directions to improve automatic program repair in the future research.

</details>

<details>

<summary>2017-05-19 09:30:32 - Optimized Certificate Revocation List Distribution for Secure V2X Communications</summary>

- *Giovanni Rigazzi, Andrea Tassi, Robert J. Piechocki, Theo Tryfonas, Andrew Nix*

- `1705.06903v1` - [abs](http://arxiv.org/abs/1705.06903v1) - [pdf](http://arxiv.org/pdf/1705.06903v1)

> The successful deployment of safe and trustworthy Connected and Autonomous Vehicles (CAVs) will highly depend on the ability to devise robust and effective security solutions to resist sophisticated cyber attacks and patch up critical vulnerabilities. Pseudonym Public Key Infrastructure (PPKI) is a promising approach to secure vehicular networks as well as ensure data and location privacy, concealing the vehicles' real identities. Nevertheless, pseudonym distribution and management affect PPKI scalability due to the significant number of digital certificates required by a single vehicle. In this paper, we focus on the certificate revocation process and propose a versatile and low-complexity framework to facilitate the distribution of the Certificate Revocation Lists (CRL) issued by the Certification Authority (CA). CRL compression is achieved through optimized Bloom filters, which guarantee a considerable overhead reduction with a configurable rate of false positives. Our results show that the distribution of compressed CRLs can significantly enhance the system scalability without increasing the complexity of the revocation process.

</details>

<details>

<summary>2017-05-23 22:16:20 - Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification</summary>

- *Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie*

- `1705.08550v1` - [abs](http://arxiv.org/abs/1705.08550v1) - [pdf](http://arxiv.org/pdf/1705.08550v1)

> Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods rely on regions of interest (ROIs) which require great efforts to annotate. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning (MIL) for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned ROIs. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed networks compared to previous work using segmentation and detection annotations.

</details>

<details>

<summary>2017-05-27 12:40:24 - Global hard thresholding algorithms for joint sparse image representation and denoising</summary>

- *Reza Borhani, Jeremy Watt, Aggelos Katsaggelos*

- `1705.09816v1` - [abs](http://arxiv.org/abs/1705.09816v1) - [pdf](http://arxiv.org/pdf/1705.09816v1)

> Sparse coding of images is traditionally done by cutting them into small patches and representing each patch individually over some dictionary given a pre-determined number of nonzero coefficients to use for each patch. In lack of a way to effectively distribute a total number (or global budget) of nonzero coefficients across all patches, current sparse recovery algorithms distribute the global budget equally across all patches despite the wide range of differences in structural complexity among them. In this work we propose a new framework for joint sparse representation and recovery of all image patches simultaneously. We also present two novel global hard thresholding algorithms, based on the notion of variable splitting, for solving the joint sparse model. Experimentation using both synthetic and real data shows effectiveness of the proposed framework for sparse image representation and denoising tasks. Additionally, time complexity analysis of the proposed algorithms indicate high scalability of both algorithms, making them favorable to use on large megapixel images.

</details>


## 2017-06

<details>

<summary>2017-06-07 17:05:16 - An Unsupervised Algorithm For Learning Lie Group Transformations</summary>

- *Jascha Sohl-Dickstein, Ching Ming Wang, Bruno A. Olshausen*

- `1001.1027v5` - [abs](http://arxiv.org/abs/1001.1027v5) - [pdf](http://arxiv.org/pdf/1001.1027v5)

> We present several theoretical contributions which allow Lie groups to be fit to high dimensional datasets. Transformation operators are represented in their eigen-basis, reducing the computational complexity of parameter estimation to that of training a linear transformation model. A transformation specific "blurring" operator is introduced that allows inference to escape local minima via a smoothing of the transformation space. A penalty on traversed manifold distance is added which encourages the discovery of sparse, minimal distance, transformations between states. Both learning and inference are demonstrated using these methods for the full set of affine transformations on natural image patches. Transformation operators are then trained on natural video sequences. It is shown that the learned video transformations provide a better description of inter-frame differences than the standard motion model based on rigid translation.

</details>

<details>

<summary>2017-06-08 00:10:04 - Dense Transformer Networks</summary>

- *Jun Li, Yongjun Chen, Lei Cai, Ian Davidson, Shuiwang Ji*

- `1705.08881v2` - [abs](http://arxiv.org/abs/1705.08881v2) - [pdf](http://arxiv.org/pdf/1705.08881v2)

> The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.

</details>

<details>

<summary>2017-06-16 17:27:41 - Local Feature Descriptor Learning with Adaptive Siamese Network</summary>

- *Chong Huang, Qiong Liu, Yan-Ying Chen, Kwang-Ting, Cheng*

- `1706.05358v1` - [abs](http://arxiv.org/abs/1706.05358v1) - [pdf](http://arxiv.org/pdf/1706.05358v1)

> Although the recent progress in the deep neural network has led to the development of learnable local feature descriptors, there is no explicit answer for estimation of the necessary size of a neural network. Specifically, the local feature is represented in a low dimensional space, so the neural network should have more compact structure. The small networks required for local feature descriptor learning may be sensitive to initial conditions and learning parameters and more likely to become trapped in local minima. In order to address the above problem, we introduce an adaptive pruning Siamese Architecture based on neuron activation to learn local feature descriptors, making the network more computationally efficient with an improved recognition rate over more complex networks. Our experiments demonstrate that our learned local feature descriptors outperform the state-of-art methods in patch matching.

</details>

<details>

<summary>2017-06-22 04:37:31 - Curvature-aware Manifold Learning</summary>

- *Yangyang Li*

- `1706.07167v1` - [abs](http://arxiv.org/abs/1706.07167v1) - [pdf](http://arxiv.org/pdf/1706.07167v1)

> Traditional manifold learning algorithms assumed that the embedded manifold is globally or locally isometric to Euclidean space. Under this assumption, they divided manifold into a set of overlapping local patches which are locally isometric to linear subsets of Euclidean space. By analyzing the global or local isometry assumptions it can be shown that the learnt manifold is a flat manifold with zero Riemannian curvature tensor. In general, manifolds may not satisfy these hypotheses. One major limitation of traditional manifold learning is that it does not consider the curvature information of manifold. In order to remove these limitations, we present our curvature-aware manifold learning algorithm called CAML. The purpose of our algorithm is to break the local isometry assumption and to reduce the dimension of the general manifold which is not isometric to Euclidean space. Thus, our method adds the curvature information to the process of manifold learning. The experiments have shown that our method CAML is more stable than other manifold learning algorithms by comparing the neighborhood preserving ratios.

</details>

<details>

<summary>2017-06-28 17:59:33 - Recovery of Missing Samples Using Sparse Approximation via a Convex Similarity Measure</summary>

- *Amirhossein Javaheri, Hadi Zayyani, Farokh Marvasti*

- `1706.09395v1` - [abs](http://arxiv.org/abs/1706.09395v1) - [pdf](http://arxiv.org/pdf/1706.09395v1)

> In this paper, we study the missing sample recovery problem using methods based on sparse approximation. In this regard, we investigate the algorithms used for solving the inverse problem associated with the restoration of missed samples of image signal. This problem is also known as inpainting in the context of image processing and for this purpose, we suggest an iterative sparse recovery algorithm based on constrained $l_1$-norm minimization with a new fidelity metric. The proposed metric called Convex SIMilarity (CSIM) index, is a simplified version of the Structural SIMilarity (SSIM) index, which is convex and error-sensitive. The optimization problem incorporating this criterion, is then solved via Alternating Direction Method of Multipliers (ADMM). Simulation results show the efficiency of the proposed method for missing sample recovery of 1D patch vectors and inpainting of 2D image signals.

</details>


## 2017-07

<details>

<summary>2017-07-15 05:55:44 - Partitioning Patches into Test-equivalence Classes for Scaling Program Repair</summary>

- *Sergey Mechtaev, Xiang Gao, Shin Hwei Tan, Abhik Roychoudhury*

- `1707.03139v2` - [abs](http://arxiv.org/abs/1707.03139v2) - [pdf](http://arxiv.org/pdf/1707.03139v2)

> Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading assignments and patching security vulnerabilities. A common challenge faced by all existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider.   The correctness criterion for program repair is often given as a suite of tests, since a formal specification of the intended program behavior may not be available. Current repair techniques do not scale due to the large number of test executions performed by the underlying search algorithms. We address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are "test-equivalent" for a given test, they produce indistinguishable results on this test). We propose two test-equivalence relations based on runtime values and dependencies respectively and present an algorithm that performs on-the-fly partitioning of patches into test-equivalence classes.   Our experiments on real-world programs reveal that the proposed methodology drastically reduces the number of test executions and therefore provides an order of magnitude efficiency improvement over existing repair techniques, without sacrificing patch quality.

</details>

<details>

<summary>2017-07-17 14:13:30 - An Empirical Analysis of the Influence of Fault Space on Search-Based Automated Program Repair</summary>

- *Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, Shing-Chi Cheung*

- `1707.05172v1` - [abs](http://arxiv.org/abs/1707.05172v1) - [pdf](http://arxiv.org/pdf/1707.05172v1)

> Automated program repair (APR) has attracted great research attention, and various techniques have been proposed. Search-based APR is one of the most important categories among these techniques. Existing researches focus on the design of effective mutation operators and searching algorithms to better find the correct patch. Despite various efforts, the effectiveness of these techniques are still limited by the search space explosion problem. One of the key factors attribute to this problem is the quality of fault spaces as reported by existing studies. This motivates us to study the importance of the fault space to the success of finding a correct patch. Our empirical study aims to answer three questions. Does the fault space significantly correlate with the performance of search-based APR? If so, are there any indicative measurements to approximate the accuracy of the fault space before applying expensive APR techniques? Are there any automatic methods that can improve the accuracy of the fault space? We observe that the accuracy of the fault space affects the effectiveness and efficiency of search-based APR techniques, e.g., the failure rate of GenProg could be as high as $60\%$ when the real fix location is ranked lower than 10 even though the correct patch is in the search space. Besides, GenProg is able to find more correct patches and with fewer trials when given a fault space with a higher accuracy. We also find that the negative mutation coverage, which is designed in this study to measure the capability of a test suite to kill the mutants created on the statements executed by failing tests, is the most indicative measurement to estimate the efficiency of search-based APR. Finally, we confirm that automated generated test cases can help improve the accuracy of fault spaces, and further improve the performance of search-based APR techniques.

</details>

<details>

<summary>2017-07-18 05:17:14 - Downgrade Attack on TrustZone</summary>

- *Yue Chen, Yulong Zhang, Zhi Wang, Tao Wei*

- `1707.05082v2` - [abs](http://arxiv.org/abs/1707.05082v2) - [pdf](http://arxiv.org/pdf/1707.05082v2)

> Security-critical tasks require proper isolation from untrusted software. Chip manufacturers design and include trusted execution environments (TEEs) in their processors to secure these tasks. The integrity and security of the software in the trusted environment depend on the verification process of the system.   We find a form of attack that can be performed on the current implementations of the widely deployed ARM TrustZone technology. The attack exploits the fact that the trustlet (TA) or TrustZone OS loading verification procedure may use the same verification key and may lack proper rollback prevention across versions. If an exploit works on an out-of-date version, but the vulnerability is patched on the latest version, an attacker can still use the same exploit to compromise the latest system by downgrading the software to an older and exploitable version.   We did experiments on popular devices on the market including those from Google, Samsung and Huawei, and found that all of them have the risk of being attacked. Also, we show a real-world example to exploit Qualcomm's QSEE.   In addition, in order to find out which device images share the same verification key, pattern matching schemes for different vendors are analyzed and summarized.

</details>

<details>

<summary>2017-07-19 15:15:36 - Self-paced Convolutional Neural Network for Computer Aided Detection in Medical Imaging Analysis</summary>

- *Xiang Li, Aoxiao Zhong, Ming Lin, Ning Guo, Mu Sun, Arkadiusz Sitek, Jieping Ye, James Thrall, Quanzheng Li*

- `1707.06145v1` - [abs](http://arxiv.org/abs/1707.06145v1) - [pdf](http://arxiv.org/pdf/1707.06145v1)

> Tissue characterization has long been an important component of Computer Aided Diagnosis (CAD) systems for automatic lesion detection and further clinical planning. Motivated by the superior performance of deep learning methods on various computer vision problems, there has been increasing work applying deep learning to medical image analysis. However, the development of a robust and reliable deep learning model for computer-aided diagnosis is still highly challenging due to the combination of the high heterogeneity in the medical images and the relative lack of training samples. Specifically, annotation and labeling of the medical images is much more expensive and time-consuming than other applications and often involves manual labor from multiple domain experts. In this work, we propose a multi-stage, self-paced learning framework utilizing a convolutional neural network (CNN) to classify Computed Tomography (CT) image patches. The key contribution of this approach is that we augment the size of training samples by refining the unlabeled instances with a self-paced learning CNN. By implementing the framework on high performance computing servers including the NVIDIA DGX1 machine, we obtained the experimental result, showing that the self-pace boosted network consistently outperformed the original network even with very scarce manual labels. The performance gain indicates that applications with limited training samples such as medical image analysis can benefit from using the proposed framework.

</details>

<details>

<summary>2017-07-20 14:03:16 - Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data</summary>

- *Gil Keren, Björn Schuller*

- `1602.05875v3` - [abs](http://arxiv.org/abs/1602.05875v3) - [pdf](http://arxiv.org/pdf/1602.05875v3)

> Traditional convolutional layers extract features from patches of data by applying a non-linearity on an affine function of the input. We propose a model that enhances this feature extraction process for the case of sequential data, by feeding patches of the data into a recurrent neural network and using the outputs or hidden states of the recurrent units to compute the extracted features. By doing so, we exploit the fact that a window containing a few frames of the sequential data is a sequence itself and this additional structure might encapsulate valuable information. In addition, we allow for more steps of computation in the feature extraction process, which is potentially beneficial as an affine function followed by a non-linearity can result in too simple features. Using our convolutional recurrent layers we obtain an improvement in performance in two audio classification tasks, compared to traditional convolutional layers. Tensorflow code for the convolutional recurrent layers is publicly available in https://github.com/cruvadom/Convolutional-RNN.

</details>

<details>

<summary>2017-07-24 14:09:47 - Automatic breast cancer grading in lymph nodes using a deep neural network</summary>

- *Thomas Wollmann, Karl Rohr*

- `1707.07565v1` - [abs](http://arxiv.org/abs/1707.07565v1) - [pdf](http://arxiv.org/pdf/1707.07565v1)

> The progression of breast cancer can be quantified in lymph node whole-slide images (WSIs). We describe a novel method for effectively performing classification of whole-slide images and patient level breast cancer grading. Our method utilises a deep neural network. The method performs classification on small patches and uses model averaging for boosting. In the first step, region of interest patches are determined and cropped automatically by color thresholding and then classified by the deep neural network. The classification results are used to determine a slide level class and for further aggregation to predict a patient level grade. Fast processing speed of our method enables high throughput image analysis.

</details>

<details>

<summary>2017-07-25 04:01:25 - SAR Target Recognition Using the Multi-aspect-aware Bidirectional LSTM Recurrent Neural Networks</summary>

- *Fan Zhang, Chen Hu, Qiang Yin, Wei Li, Hengchao Li, Wen Hong*

- `1707.09875v1` - [abs](http://arxiv.org/abs/1707.09875v1) - [pdf](http://arxiv.org/pdf/1707.09875v1)

> The outstanding pattern recognition performance of deep learning brings new vitality to the synthetic aperture radar (SAR) automatic target recognition (ATR). However, there is a limitation in current deep learning based ATR solution that each learning process only handle one SAR image, namely learning the static scattering information, while missing the space-varying information. It is obvious that multi-aspect joint recognition introduced space-varying scattering information should improve the classification accuracy and robustness. In this paper, a novel multi-aspect-aware method is proposed to achieve this idea through the bidirectional Long Short-Term Memory (LSTM) recurrent neural networks based space-varying scattering information learning. Specifically, we first select different aspect images to generate the multi-aspect space-varying image sequences. Then, the Gabor filter and three-patch local binary pattern (TPLBP) are progressively implemented to extract a comprehensive spatial features, followed by dimensionality reduction with the Multi-layer Perceptron (MLP) network. Finally, we design a bidirectional LSTM recurrent neural network to learn the multi-aspect features with further integrating the softmax classifier to achieve target recognition. Experimental results demonstrate that the proposed method can achieve 99.9% accuracy for 10-class recognition. Besides, its anti-noise and anti-confusion performance are also better than the conventional deep learning based methods.

</details>

<details>

<summary>2017-07-25 14:35:49 - Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors</summary>

- *Mengyu Chu, Nils Thuerey*

- `1705.01425v2` - [abs](http://arxiv.org/abs/1705.01425v2) - [pdf](http://arxiv.org/pdf/1705.01425v2)

> We present a novel data-driven algorithm to synthesize high-resolution flow simulations with reusable repositories of space-time flow data. In our work, we employ a descriptor learning approach to encode the similarity between fluid regions with differences in resolution and numerical viscosity. We use convolutional neural networks to generate the descriptors from fluid data such as smoke density and flow velocity. At the same time, we present a deformation limiting patch advection method which allows us to robustly track deformable fluid regions. With the help of this patch advection, we generate stable space-time data sets from detailed fluids for our repositories. We can then use our learned descriptors to quickly localize a suitable data set when running a new simulation. This makes our approach very efficient, and resolution independent. We will demonstrate with several examples that our method yields volumes with very high effective resolutions, and non-dissipative small scale details that naturally integrate into the motions of the underlying flow.

</details>

<details>

<summary>2017-07-25 14:40:18 - Predicting Exploitation of Disclosed Software Vulnerabilities Using Open-source Data</summary>

- *Benjamin L. Bullough, Anna K. Yanchenko, Christopher L. Smith, Joseph R. Zipkin*

- `1707.08015v1` - [abs](http://arxiv.org/abs/1707.08015v1) - [pdf](http://arxiv.org/pdf/1707.08015v1)

> Each year, thousands of software vulnerabilities are discovered and reported to the public. Unpatched known vulnerabilities are a significant security risk. It is imperative that software vendors quickly provide patches once vulnerabilities are known and users quickly install those patches as soon as they are available. However, most vulnerabilities are never actually exploited. Since writing, testing, and installing software patches can involve considerable resources, it would be desirable to prioritize the remediation of vulnerabilities that are likely to be exploited. Several published research studies have reported moderate success in applying machine learning techniques to the task of predicting whether a vulnerability will be exploited. These approaches typically use features derived from vulnerability databases (such as the summary text describing the vulnerability) or social media posts that mention the vulnerability by name. However, these prior studies share multiple methodological shortcomings that inflate predictive power of these approaches. We replicate key portions of the prior work, compare their approaches, and show how selection of training and test data critically affect the estimated performance of predictive models. The results of this study point to important methodological considerations that should be taken into account so that results reflect real-world utility.

</details>


## 2017-08

<details>

<summary>2017-08-01 09:46:54 - Fast Preprocessing for Robust Face Sketch Synthesis</summary>

- *Yibing Song, Jiawei Zhang, Linchao Bao, Qingxiong Yang*

- `1708.00224v1` - [abs](http://arxiv.org/abs/1708.00224v1) - [pdf](http://arxiv.org/pdf/1708.00224v1)

> Exemplar-based face sketch synthesis methods usually meet the challenging problem that input photos are captured in different lighting conditions from training photos. The critical step causing the failure is the search of similar patch candidates for an input photo patch. Conventional illumination invariant patch distances are adopted rather than directly relying on pixel intensity difference, but they will fail when local contrast within a patch changes. In this paper, we propose a fast preprocessing method named Bidirectional Luminance Remapping (BLR), which interactively adjust the lighting of training and input photos. Our method can be directly integrated into state-of-the-art exemplar-based methods to improve their robustness with ignorable computational cost.

</details>

<details>

<summary>2017-08-03 10:38:51 - Using the SLEUTH urban growth model to simulate the impacts of future policy scenarios on urban land use in the Tehran metropolitan area in Iran</summary>

- *Shaghayegh Kargozar Nahavandya, Lalit Kumar, Pedram Ghamisi*

- `1708.01089v1` - [abs](http://arxiv.org/abs/1708.01089v1) - [pdf](http://arxiv.org/pdf/1708.01089v1)

> The SLEUTH model, based on the Cellular Automata (CA), can be applied to city development simulation in metropolitan areas. In this study the SLEUTH model was used to model the urban expansion and predict the future possible behavior of the urban growth in Tehran. The fundamental data were five Landsat TM and ETM images of 1988, 1992, 1998, 2001 and 2010. Three scenarios were designed to simulate the spatial pattern. The first scenario assumed historical urbanization mode would persist and the only limitations for development were height and slope. The second one was a compact scenario which makes the growth mostly internal and limited the expansion of suburban areas. The last scenario proposed a polycentric urban structure which let the little patches grow without any limitation and would not consider the areas beyond the specific buffer zone from the larger patches for development. Results showed that the urban growth rate was greater in the first scenario in comparison with the other two scenarios. Also it was shown that the third scenario was more suitable for Tehran since it could avoid undesirable effects such as congestion and pollution and was more in accordance with the conditions of Tehran city.

</details>

<details>

<summary>2017-08-21 13:10:39 - Learned Multi-Patch Similarity</summary>

- *Wilfried Hartmann, Silvano Galliani, Michal Havlena, Luc Van Gool, Konrad Schindler*

- `1703.08836v2` - [abs](http://arxiv.org/abs/1703.08836v2) - [pdf](http://arxiv.org/pdf/1703.08836v2)

> Estimating a depth map from multiple views of a scene is a fundamental task in computer vision. As soon as more than two viewpoints are available, one faces the very basic question how to measure similarity across >2 image patches. Surprisingly, no direct solution exists, instead it is common to fall back to more or less robust averaging of two-view similarities. Encouraged by the success of machine learning, and in particular convolutional neural networks, we propose to learn a matching function which directly maps multiple image patches to a scalar similarity score. Experiments on several multi-view datasets demonstrate that this approach has advantages over methods based on pairwise patch similarity.

</details>

<details>

<summary>2017-08-22 16:08:18 - Herding Vulnerable Cats: A Statistical Approach to Disentangle Joint Responsibility for Web Security in Shared Hosting</summary>

- *Samaneh Tajalizadehkhoob, Tom van Goethem, Maciej Korczyński, Arman Noroozian, Rainer Böhme, Tyler Moore, Wouter Joosen, Michel van Eeten*

- `1708.06693v1` - [abs](http://arxiv.org/abs/1708.06693v1) - [pdf](http://arxiv.org/pdf/1708.06693v1)

> Hosting providers play a key role in fighting web compromise, but their ability to prevent abuse is constrained by the security practices of their own customers. {\em Shared} hosting, offers a unique perspective since customers operate under restricted privileges and providers retain more control over configurations. We present the first empirical analysis of the distribution of web security features and software patching practices in shared hosting providers, the influence of providers on these security practices, and their impact on web compromise rates. We construct provider-level features on the global market for shared hosting -- containing 1,259 providers -- by gathering indicators from 442,684 domains. Exploratory factor analysis of 15 indicators identifies four main latent factors that capture security efforts: content security, webmaster security, web infrastructure security and web application security. We confirm, via a fixed-effect regression model, that providers exert significant influence over the latter two factors, which are both related to the software stack in their hosting environment. Finally, by means of GLM regression analysis of these factors on phishing and malware abuse, we show that the four security and software patching factors explain between 10\% and 19\% of the variance in abuse at providers, after controlling for size. For web-application security for instance, we found that when a provider moves from the bottom 10\% to the best-performing 10\%, it would experience 4 times fewer phishing incidents. We show that providers have influence over patch levels--even higher in the stack, where CMSes can run as client-side software--and that this influence is tied to a substantial reduction in abuse levels.

</details>

<details>

<summary>2017-08-28 16:51:15 - Viden: Attacker Identification on In-Vehicle Networks</summary>

- *Kyong-Tak Cho, Kang Shin*

- `1708.08414v1` - [abs](http://arxiv.org/abs/1708.08414v1) - [pdf](http://arxiv.org/pdf/1708.08414v1)

> Various defense schemes --- which determine the presence of an attack on the in-vehicle network --- have recently been proposed. However, they fail to identify which Electronic Control Unit (ECU) actually mounted the attack. Clearly, pinpointing the attacker ECU is essential for fast/efficient forensic, isolation, security patch, etc. To meet this need, we propose a novel scheme, called Viden (Voltage-based attacker identification), which can identify the attacker ECU by measuring and utilizing voltages on the in-vehicle network. The first phase of Viden, called ACK learning, determines whether or not the measured voltage signals really originate from the genuine message transmitter. Viden then exploits the voltage measurements to construct and update the transmitter ECUs' voltage profiles as their fingerprints. It finally uses the voltage profiles to identify the attacker ECU. Since Viden adapts its profiles to changes inside/outside of the vehicle, it can pinpoint the attacker ECU under various conditions. Moreover, its efficiency and design-compliance with modern in-vehicle network implementations make Viden practical and easily deployable. Our extensive experimental evaluations on both a CAN bus prototype and two real vehicles have shown that Viden can accurately fingerprint ECUs based solely on voltage measurements and thus identify the attacker ECU with a low false identification rate of 0.2%.

</details>

<details>

<summary>2017-08-29 16:59:19 - Visual Cues to Improve Myoelectric Control of Upper Limb Prostheses</summary>

- *Andrea Gigli, Arjan Gijsberts, Valentina Gregori, Matteo Cognolato, Manfredo Atzori, Barbara Caputo*

- `1709.02236v1` - [abs](http://arxiv.org/abs/1709.02236v1) - [pdf](http://arxiv.org/pdf/1709.02236v1)

> The instability of myoelectric signals over time complicates their use to control highly articulated prostheses. To address this problem, studies have tried to combine surface electromyography with modalities that are less affected by the amputation and environment, such as accelerometry or gaze information. In the latter case, the hypothesis is that a subject looks at the object he or she intends to manipulate and that knowing this object's affordances allows to constrain the set of possible grasps. In this paper, we develop an automated way to detect stable fixations and show that gaze information is indeed helpful in predicting hand movements. In our multimodal approach, we automatically detect stable gazes and segment an object of interest around the subject's fixation in the visual frame. The patch extracted around this object is subsequently fed through an off-the-shelf deep convolutional neural network to obtain a high level feature representation, which is then combined with traditional surface electromyography in the classification stage. Tests have been performed on a dataset acquired from five intact subjects who performed ten types of grasps on various objects as well as in a functional setting. They show that the addition of gaze information increases the classification accuracy considerably. Further analysis demonstrates that this improvement is consistent for all grasps and concentrated during the movement onset and offset.

</details>


## 2017-09

<details>

<summary>2017-09-05 18:51:03 - Did we learn from LLC Side Channel Attacks? A Cache Leakage Detection Tool for Crypto Libraries</summary>

- *Gorka Irazoqui, Kai Cong, Xiaofei Guo, Hareesh Khattri, Arun Kanuparthi, Thomas Eisenbarth, Berk Sunar*

- `1709.01552v1` - [abs](http://arxiv.org/abs/1709.01552v1) - [pdf](http://arxiv.org/pdf/1709.01552v1)

> This work presents a new tool to verify the correctness of cryptographic implementations with respect to cache attacks. Our methodology discovers vulnerabilities that are hard to find with other techniques, observed as exploitable leakage. The methodology works by identifying secret dependent memory and introducing forced evictions inside potentially vulnerable code to obtain cache traces that are analyzed using Mutual Information. If dependence is observed, the cryptographic implementation is classified as to leak information.   We demonstrate the viability of our technique in the design of the three main cryptographic primitives, i.e., AES, RSA and ECC, in eight popular up to date cryptographic libraries, including OpenSSL, Libgcrypt, Intel IPP and NSS. Our results show that cryptographic code designers are far away from incorporating the appropriate countermeasures to avoid cache leakages, as we found that 50% of the default implementations analyzed leaked information that lead to key extraction. We responsibly notified the designers of all the leakages found and suggested patches to solve these vulnerabilities.

</details>

<details>

<summary>2017-09-06 17:53:44 - An Efficient Method for Robust Projection Matrix Design</summary>

- *Tao Hong, Zhihui Zhu*

- `1609.08281v3` - [abs](http://arxiv.org/abs/1609.08281v3) - [pdf](http://arxiv.org/pdf/1609.08281v3)

> Our objective is to efficiently design a robust projection matrix $\Phi$ for the Compressive Sensing (CS) systems when applied to the signals that are not exactly sparse. The optimal projection matrix is obtained by mainly minimizing the average coherence of the equivalent dictionary. In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix. Without requiring of training data, we can efficiently design the robust projection matrix and apply it for most of CS systems, like a CS system for image processing with a conventional wavelet dictionary in which the SRE matrix is generally not available. Simulation results demonstrate the efficiency and effectiveness of the proposed approach compared with the state-of-the-art methods. In addition, we experimentally demonstrate with natural images that under similar compression rate, a CS system with a learned dictionary in high dimensions outperforms the one in low dimensions in terms of reconstruction accuracy. This together with the fact that our proposed method can efficiently work in high dimension suggests that a CS system can be potentially implemented beyond the small patches in sparsity-based image processing.

</details>

<details>

<summary>2017-09-14 05:59:50 - Do Developers Update Their Library Dependencies? An Empirical Study on the Impact of Security Advisories on Library Migration</summary>

- *Raula Gaikovina Kula, Daniel M. German, Ali Ouni, Takashi Ishio, Katsuro Inoue*

- `1709.04621v1` - [abs](http://arxiv.org/abs/1709.04621v1) - [pdf](http://arxiv.org/pdf/1709.04621v1)

> Third-party library reuse has become common practice in contemporary software development, as it includes several benefits for developers. Library dependencies are constantly evolving, with newly added features and patches that fix bugs in older versions. To take full advantage of third-party reuse, developers should always keep up to date with the latest versions of their library dependencies. In this paper, we investigate the extent of which developers update their library dependencies. Specifically, we conducted an empirical study on library migration that covers over 4,600 GitHub software projects and 2,700 library dependencies. Results show that although many of these systems rely heavily on dependencies, 81.5% of the studied systems still keep their outdated dependencies. In the case of updating a vulnerable dependency, the study reveals that affected developers are not likely to respond to a security advisory. Surveying these developers, we find that 69% of the interviewees claim that they were unaware of their vulnerable dependencies. Furthermore, developers are not likely to prioritize library updates, citing it as extra effort and added responsibility. This study concludes that even though third-party reuse is commonplace, the practice of updating a dependency is not as common for many developers.

</details>

<details>

<summary>2017-09-15 09:24:57 - Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning</summary>

- *Natalia Neverova, Christian Wolf, Florian Nebout, Graham Taylor*

- `1511.06728v4` - [abs](http://arxiv.org/abs/1511.06728v4) - [pdf](http://arxiv.org/pdf/1511.06728v4)

> We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth data is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi/weakly-supervised way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled ground truth (pixelwise segmentations); and (ii) a dataset with real images for which ground truth joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise restoration process, which aligns tentative segmentation maps with a large dictionary of synthetic poses. The underlying premise is that the domain shift between synthetic and real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw input domain. Experiments on the NYU dataset show that the proposed training method decreases error on joints over direct regression of joints from depth data by 15.7%.

</details>

<details>

<summary>2017-09-17 14:44:07 - Neural Affine Grayscale Image Denoising</summary>

- *Sungmin Cha, Taesup Moon*

- `1709.05672v1` - [abs](http://arxiv.org/abs/1709.05672v1) - [pdf](http://arxiv.org/pdf/1709.05672v1)

> We propose a new grayscale image denoiser, dubbed as Neural Affine Image Denoiser (Neural AIDE), which utilizes neural network in a novel way. Unlike other neural network based image denoising methods, which typically apply simple supervised learning to learn a mapping from a noisy patch to a clean patch, we formulate to train a neural network to learn an \emph{affine} mapping that gets applied to a noisy pixel, based on its context. Our formulation enables both supervised training of the network from the labeled training dataset and adaptive fine-tuning of the network parameters using the given noisy image subject to denoising. The key tool for devising Neural AIDE is to devise an estimated loss function of the MSE of the affine mapping, solely based on the noisy data. As a result, our algorithm can outperform most of the recent state-of-the-art methods in the standard benchmark datasets. Moreover, our fine-tuning method can nicely overcome one of the drawbacks of the patch-level supervised learning methods in image denoising; namely, a supervised trained model with a mismatched noise variance can be mostly corrected as long as we have the matched noise variance during the fine-tuning step.

</details>

<details>

<summary>2017-09-26 06:14:49 - Is It Safe to Uplift This Patch? An Empirical Study on Mozilla Firefox</summary>

- *Marco Castelluccio, Le An, Foutse Khomh*

- `1709.08852v1` - [abs](http://arxiv.org/abs/1709.08852v1) - [pdf](http://arxiv.org/pdf/1709.08852v1)

> In rapid release development processes, patches that fix critical issues, or implement high-value features are often promoted directly from the development channel to a stabilization channel, potentially skipping one or more stabilization channels. This practice is called patch uplift. Patch uplift is risky, because patches that are rushed through the stabilization phase can end up introducing regressions in the code. This paper examines patch uplift operations at Mozilla, with the aim to identify the characteristics of uplifted patches that introduce regressions. Through statistical and manual analyses, we quantitatively and qualitatively investigate the reasons behind patch uplift decisions and the characteristics of uplifted patches that introduced regressions. Additionally, we interviewed three Mozilla release managers to understand organizational factors that affect patch uplift decisions and outcomes. Results show that most patches are uplifted because of a wrong functionality or a crash. Uplifted patches that lead to faults tend to have larger patch size, and most of the faults are due to semantic or memory errors in the patches. Also, release managers are more inclined to accept patch uplift requests that concern certain specific components, and-or that are submitted by certain specific developers.

</details>


## 2017-10

<details>

<summary>2017-10-14 18:17:30 - Mental Sampling in Multimodal Representations</summary>

- *Jian-Qiao Zhu, Adam N. Sanborn, Nick Chater*

- `1710.05219v1` - [abs](http://arxiv.org/abs/1710.05219v1) - [pdf](http://arxiv.org/pdf/1710.05219v1)

> Both resources in the natural environment and concepts in a semantic space are distributed "patchily", with large gaps in between the patches. To describe people's internal and external foraging behavior, various random walk models have been proposed. In particular, internal foraging has been modeled as sampling: in order to gather relevant information for making a decision, people draw samples from a mental representation using random-walk algorithms such as Markov chain Monte Carlo (MCMC). However, two common empirical observations argue against simple sampling algorithms such as MCMC. First, the spatial structure is often best described by a L\'evy flight distribution: the probability of the distance between two successive locations follows a power-law on the distances. Second, the temporal structure of the sampling that humans and other animals produce have long-range, slowly decaying serial correlations characterized as $1/f$-like fluctuations. We propose that mental sampling is not done by simple MCMC, but is instead adapted to multimodal representations and is implemented by Metropolis-coupled Markov chain Monte Carlo (MC$^3$), one of the first algorithms developed for sampling from multimodal distributions. MC$^3$ involves running multiple Markov chains in parallel but with target distributions of different temperatures, and it swaps the states of the chains whenever a better location is found. Heated chains more readily traverse valleys in the probability landscape to propose moves to far-away peaks, while the colder chains make the local steps that explore the current peak or patch. We show that MC$^3$ generates distances between successive samples that follow a L\'evy flight distribution and $1/f$-like serial correlations, providing a single mechanistic account of these two puzzling empirical phenomena.

</details>

<details>

<summary>2017-10-17 15:17:59 - A Convex Similarity Index for Sparse Recovery of Missing Image Samples</summary>

- *Amirhossein Javaheri, Hadi Zayyani, Farokh Marvasti*

- `1701.07422v3` - [abs](http://arxiv.org/abs/1701.07422v3) - [pdf](http://arxiv.org/pdf/1701.07422v3)

> This paper investigates the problem of recovering missing samples using methods based on sparse representation adapted especially for image signals. Instead of $l_2$-norm or Mean Square Error (MSE), a new perceptual quality measure is used as the similarity criterion between the original and the reconstructed images. The proposed criterion called Convex SIMilarity (CSIM) index is a modified version of the Structural SIMilarity (SSIM) index, which despite its predecessor, is convex and uni-modal. We derive mathematical properties for the proposed index and show how to optimally choose the parameters of the proposed criterion, investigating the Restricted Isometry (RIP) and error-sensitivity properties. We also propose an iterative sparse recovery method based on a constrained $l_1$-norm minimization problem, incorporating CSIM as the fidelity criterion. The resulting convex optimization problem is solved via an algorithm based on Alternating Direction Method of Multipliers (ADMM). Taking advantage of the convexity of the CSIM index, we also prove the convergence of the algorithm to the globally optimal solution of the proposed optimization problem, starting from any arbitrary point. Simulation results confirm the performance of the new similarity index as well as the proposed algorithm for missing sample recovery of image patch signals.

</details>

<details>

<summary>2017-10-23 13:01:05 - Generic 3D Representation via Pose Estimation and Matching</summary>

- *Amir R. Zamir, Tilman Wekel, Pulkit Argrawal, Colin Weil, Jitendra Malik, Silvio Savarese*

- `1710.08247v1` - [abs](http://arxiv.org/abs/1710.08247v1) - [pdf](http://arxiv.org/pdf/1710.08247v1)

> Though a large body of computer vision research has investigated developing generic semantic representations, efforts towards developing a similar representation for 3D has been limited. In this paper, we learn a generic 3D representation through solving a set of foundational proxy 3D tasks: object-centric camera pose estimation and wide baseline feature matching. Our method is based upon the premise that by providing supervision over a set of carefully selected foundational tasks, generalization to novel tasks and abstraction capabilities can be achieved. We empirically show that the internal representation of a multi-task ConvNet trained to solve the above core problems generalizes to novel 3D tasks (e.g., scene layout estimation, object pose estimation, surface normal estimation) without the need for fine-tuning and shows traits of abstraction abilities (e.g., cross-modality pose estimation). In the context of the core supervised tasks, we demonstrate our representation achieves state-of-the-art wide baseline feature matching results without requiring apriori rectification (unlike SIFT and the majority of learned features). We also show 6DOF camera pose estimation given a pair local image patches. The accuracy of both supervised tasks come comparable to humans. Finally, we contribute a large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information, and conclude with a discussion on the learned representation and open research questions.

</details>

<details>

<summary>2017-10-30 09:24:27 - Stochastic Subsampling for Factorizing Huge Matrices</summary>

- *Arthur Mensch, Julien Mairal, Bertrand Thirion, Gael Varoquaux*

- `1701.05363v3` - [abs](http://arxiv.org/abs/1701.05363v3) - [pdf](http://arxiv.org/pdf/1701.05363v3)

> We present a matrix-factorization algorithm that scales to input matrices with both huge number of rows and columns. Learned factors may be sparse or dense and/or non-negative, which makes our algorithm suitable for dictionary learning, sparse component analysis, and non-negative matrix factorization. Our algorithm streams matrix columns while subsampling them to iteratively learn the matrix factors. At each iteration, the row dimension of a new sample is reduced by subsampling, resulting in lower time complexity compared to a simple streaming algorithm. Our method comes with convergence guarantees to reach a stationary point of the matrix-factorization problem. We demonstrate its efficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on patches extracted from hyperspectral images (103 GB). For both problems, which involve different penalties on rows and columns, we obtain significant speed-ups compared to state-of-the-art algorithms.

</details>


## 2017-11

<details>

<summary>2017-11-02 16:10:14 - Talos: Neutralizing Vulnerabilities with Security Workarounds for Rapid Response</summary>

- *Zhen Huang, Mariana D'Angelo, Dhaval Miyani, David Lie*

- `1711.00795v1` - [abs](http://arxiv.org/abs/1711.00795v1) - [pdf](http://arxiv.org/pdf/1711.00795v1)

> Considerable delays often exist between the discovery of a vulnerability and the issue of a patch. One way to mitigate this window of vulnerability is to use a configuration workaround, which prevents the vulnerable code from being executed at the cost of some lost functionality -- but only if one is available. Since program configurations are not specifically designed to mitigate software vulnerabilities, we find that they only cover 25.2% of vulnerabilities.   To minimize patch delay vulnerabilities and address the limitations of configuration workarounds, we propose Security Workarounds for Rapid Response (SWRRs), which are designed to neutralize security vulnerabilities in a timely, secure, and unobtrusive manner. Similar to configuration workarounds, SWRRs neutralize vulnerabilities by preventing vulnerable code from being executed at the cost of some lost functionality. However, the key difference is that SWRRs use existing error-handling code within programs, which enables them to be mechanically inserted with minimal knowledge of the program and minimal developer effort. This allows SWRRs to achieve high coverage while still being fast and easy to deploy.   We have designed and implemented Talos, a system that mechanically instruments SWRRs into a given program, and evaluate it on five popular Linux server programs. We run exploits against 11 real-world software vulnerabilities and show that SWRRs neutralize the vulnerabilities in all cases. Quantitative measurements on 320 SWRRs indicate that SWRRs instrumented by Talos can neutralize 75.1% of all potential vulnerabilities and incur a loss of functionality similar to configuration workarounds in 71.3% of those cases. Our overall conclusion is that automatically generated SWRRs can safely mitigate 2.1x more vulnerabilities, while only incurring a loss of functionality comparable to that of traditional configuration workarounds.

</details>

<details>

<summary>2017-11-05 18:30:59 - Modeling flow in porous media with double porosity/permeability: A stabilized mixed formulation, error analysis, and numerical solutions</summary>

- *S. H. S. Joodat, K. B. Nakshatrala, R. Ballarini*

- `1705.08883v3` - [abs](http://arxiv.org/abs/1705.08883v3) - [pdf](http://arxiv.org/pdf/1705.08883v3)

> The flow of incompressible fluids through porous media plays a crucial role in many technological applications such as enhanced oil recovery and geological carbon-dioxide sequestration. The flow within numerous natural and synthetic porous materials that contain multiple scales of pores cannot be adequately described by the classical Darcy equations. It is for this reason that mathematical models for fluid flow in media with multiple scales of pores have been proposed in the literature. However, these models are analytically intractable for realistic problems. In this paper, a stabilized mixed four-field finite element formulation is presented to study the flow of an incompressible fluid in porous media exhibiting double porosity/permeability. The stabilization terms and the stabilization parameters are derived in a mathematically and thermodynamically consistent manner, and the computationally convenient equal-order interpolation of all the field variables is shown to be stable. A systematic error analysis is performed on the resulting stabilized weak formulation. Representative problems, patch tests and numerical convergence analyses are performed to illustrate the performance and convergence behavior of the proposed mixed formulation in the discrete setting. The accuracy of numerical solutions is assessed using the mathematical properties satisfied by the solutions of this double porosity/permeability model. Moreover, it is shown that the proposed framework can perform well under transient conditions and that it can capture well-known instabilities such as viscous fingering.

</details>

<details>

<summary>2017-11-06 18:58:44 - SAIC: Identifying Configuration Files for System Configuration Management</summary>

- *Zhen Huang, David Lie*

- `1711.03397v1` - [abs](http://arxiv.org/abs/1711.03397v1) - [pdf](http://arxiv.org/pdf/1711.03397v1)

> Systems can become misconfigured for a variety of reasons such as operator errors or buggy patches. When a misconfiguration is discovered, usually the first order of business is to restore availability, often by undoing the misconfiguration. To simplify this task, we propose the Statistical Analysis for Identifying Configuration Files (SAIC), which analyzes how the contents of a file changes over time to automatically determine which files contain configuration state. In this way, SAIC reduces the number of files a user must manually examine during recovery and allows versioning file systems to make more efficient use of their versioning storage.   The two key insights that enable SAIC to identify configuration files are that configuration state must persist across executions of an application and that configuration state changes at a slower rate than other types of application state. SAIC applies these insights through a set of filters, which eliminate non-persistent files from consideration, and a novel similarity metric, which measures how similar a file's versions are to each other. Together, these two mechanisms enable SAIC to identify all 72 configuration files out of 2363 versioned files from 6 common applications in two user traces, while mistaking only 33 non-configuration files as configuration files, which allows a versioning file system to eliminate roughly 66% of non-configuration file versions from its logs, thus reducing the number of file versions that a user must try to recover from a misconfiguration.

</details>

<details>

<summary>2017-11-12 15:45:34 - A machine learning approach for efficient uncertainty quantification using multiscale methods</summary>

- *Shing Chan, Ahmed H. Elsheikh*

- `1711.04315v1` - [abs](http://arxiv.org/abs/1711.04315v1) - [pdf](http://arxiv.org/pdf/1711.04315v1)

> Several multiscale methods account for sub-grid scale features using coarse scale basis functions. For example, in the Multiscale Finite Volume method the coarse scale basis functions are obtained by solving a set of local problems over dual-grid cells. We introduce a data-driven approach for the estimation of these coarse scale basis functions. Specifically, we employ a neural network predictor fitted using a set of solution samples from which it learns to generate subsequent basis functions at a lower computational cost than solving the local problems. The computational advantage of this approach is realized for uncertainty quantification tasks where a large number of realizations has to be evaluated. We attribute the ability to learn these basis functions to the modularity of the local problems and the redundancy of the permeability patches between samples. The proposed method is evaluated on elliptic problems yielding very promising results.

</details>


## 2017-12

<details>

<summary>2017-12-01 22:58:12 - A new shell formulation for graphene structures based on existing ab-initio data</summary>

- *Reza Ghaffari, Thang X. Duong, Roger A. Sauer*

- `1612.08965v2` - [abs](http://arxiv.org/abs/1612.08965v2) - [pdf](http://arxiv.org/pdf/1612.08965v2)

> An existing hyperelastic membrane model for graphene calibrated from ab-initio data (Kumar and Parks, 2014) is adapted to curvilinear coordinates and extended to a rotation-free shell formulation based on isogeometric finite elements. Therefore, the membrane model is extended by a hyperelastic bending model that reflects the ab-inito data of Kudin et al. (2001). The proposed formulation can be implemented straight-forwardly into an existing finite element package, since it does not require the description of molecular interactions. It thus circumvents the use of interatomic potentials that tend to be less accurate than ab-initio data. The proposed shell formulation is verified and analyzed by a set of simple test cases. The results are in agreement to analytical solutions and satisfy the FE patch test. The performance of the shell formulation for graphene structures is illustrated by several numerical examples. The considered examples are indentation and peeling of graphene and torsion, bending and axial stretch of carbon nanotubes. Adhesive substrates are modeled by the Lennard-Jones potential and a coarse grained contact model. In principle, the proposed formulation can be extended to other 2D materials.

</details>

<details>

<summary>2017-12-03 06:02:23 - Spatial PixelCNN: Generating Images from Patches</summary>

- *Nader Akoury, Anh Nguyen*

- `1712.00714v1` - [abs](http://arxiv.org/abs/1712.00714v1) - [pdf](http://arxiv.org/pdf/1712.00714v1)

> In this paper we propose Spatial PixelCNN, a conditional autoregressive model that generates images from small patches. By conditioning on a grid of pixel coordinates and global features extracted from a Variational Autoencoder (VAE), we are able to train on patches of images, and reproduce the full-sized image. We show that it not only allows for generating high quality samples at the same resolution as the underlying dataset, but is also capable of upscaling images to arbitrary resolutions (tested at resolutions up to $50\times$) on the MNIST dataset. Compared to a PixelCNN++ baseline, Spatial PixelCNN quantitatively and qualitatively achieves similar performance on the MNIST dataset.

</details>

<details>

<summary>2017-12-05 11:29:27 - Deep Learning with Permutation-invariant Operator for Multi-instance Histopathology Classification</summary>

- *Jakub M. Tomczak, Maximilian Ilse, Max Welling*

- `1712.00310v2` - [abs](http://arxiv.org/abs/1712.00310v2) - [pdf](http://arxiv.org/pdf/1712.00310v2)

> The computer-aided analysis of medical scans is a longstanding goal in the medical imaging field. Currently, deep learning has became a dominant methodology for supporting pathologists and radiologist. Deep learning algorithms have been successfully applied to digital pathology and radiology, nevertheless, there are still practical issues that prevent these tools to be widely used in practice. The main obstacles are low number of available cases and large size of images (a.k.a. the small n, large p problem in machine learning), and a very limited access to annotation at a pixel level that can lead to severe overfitting and large computational requirements. We propose to handle these issues by introducing a framework that processes a medical image as a collection of small patches using a single, shared neural network. The final diagnosis is provided by combining scores of individual patches using a permutation-invariant operator (combination). In machine learning community such approach is called a multi-instance learning (MIL).

</details>

<details>

<summary>2017-12-21 06:06:07 - ARJA: Automated Repair of Java Programs via Multi-Objective Genetic Programming</summary>

- *Yuan Yuan, Wolfgang Banzhaf*

- `1712.07804v1` - [abs](http://arxiv.org/abs/1712.07804v1) - [pdf](http://arxiv.org/pdf/1712.07804v1)

> Recent empirical studies show that the performance of GenProg is not satisfactory, particularly for Java. In this paper, we propose ARJA, a new GP based repair approach for automated repair of Java programs. To be specific, we present a novel lower-granularity patch representation that properly decouples the search subspaces of likely-buggy locations, operation types and potential fix ingredients, enabling GP to explore the search space more effectively. Based on this new representation, we formulate automated program repair as a multi-objective search problem and use NSGA-II to look for simpler repairs. To reduce the computational effort and search space, we introduce a test filtering procedure that can speed up the fitness evaluation of GP and three types of rules that can be applied to avoid unnecessary manipulations of the code. Moreover, we also propose a type matching strategy that can create new potential fix ingredients by exploiting the syntactic patterns of the existing statements. We conduct a large-scale empirical evaluation of ARJA along with its variants on both seeded bugs and real-world bugs in comparison with several state-of-the-art repair approaches. Our results verify the effectiveness and efficiency of the search mechanisms employed in ARJA and also show its superiority over the other approaches. In particular, compared to jGenProg (an implementation of GenProg for Java), an ARJA version fully following the redundancy assumption can generate a test-suite adequate patch for more than twice the number of bugs (from 27 to 59), and a correct patch for nearly four times of the number (from 5 to 18), on 224 real-world bugs considered in Defects4J. Furthermore, ARJA is able to correctly fix several real multi-location bugs that are hard to be repaired by most of the existing repair approaches.

</details>

