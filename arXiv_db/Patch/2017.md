# 2017

## TOC

- [2017-01](#2017-01)
- [2017-02](#2017-02)
- [2017-03](#2017-03)
- [2017-04](#2017-04)
- [2017-05](#2017-05)
- [2017-06](#2017-06)
- [2017-07](#2017-07)
- [2017-08](#2017-08)
- [2017-09](#2017-09)
- [2017-10](#2017-10)
- [2017-11](#2017-11)
- [2017-12](#2017-12)

## 2017-01

<details>

<summary>2017-01-06 13:38:07 - MatlabCompat.jl: helping Julia understand Your Matlab/Octave Code</summary>

- *Vardan Andriasyan, Yauhen Yakimovich, Artur Yakimovich*

- `1701.02220v1` - [abs](http://arxiv.org/abs/1701.02220v1) - [pdf](http://arxiv.org/pdf/1701.02220v1)

> Scientific legacy code in MATLAB/Octave not compatible with modernization of research workflows is vastly abundant throughout academic community. Performance of non-vectorized code written in MATLAB/Octave represents a major burden. A new programming language for technical computing Julia, promises to address these issues. Although Julia syntax is similar to MATLAB/Octave, porting code to Julia may be cumbersome for researchers. Here we present MatlabCompat.jl - a library aimed at simplifying the conversion of your MATLAB/Octave code to Julia. We show using a simplistic image analysis use case that MATLAB/Octave code can be easily ported to high performant Julia using MatlabCompat.jl.

</details>

<details>

<summary>2017-01-08 13:55:35 - Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation</summary>

- *Konstantinos Kamnitsas, Christian Ledig, Virginia F. J. Newcombe, Joanna P. Simpson, Andrew D. Kane, David K. Menon, Daniel Rueckert, Ben Glocker*

- `1603.05959v3` - [abs](http://arxiv.org/abs/1603.05959v3) - [pdf](http://arxiv.org/pdf/1603.05959v3)

> We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumors, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.

</details>

<details>

<summary>2017-01-09 10:27:05 - Low-rank and Adaptive Sparse Signal (LASSI) Models for Highly Accelerated Dynamic Imaging</summary>

- *Saiprasad Ravishankar, Brian E. Moore, Raj Rao Nadakuditi, Jeffrey A. Fessler*

- `1611.04069v2` - [abs](http://arxiv.org/abs/1611.04069v2) - [pdf](http://arxiv.org/pdf/1611.04069v2)

> Sparsity-based approaches have been popular in many applications in image processing and imaging. Compressed sensing exploits the sparsity of images in a transform domain or dictionary to improve image recovery from undersampled measurements. In the context of inverse problems in dynamic imaging, recent research has demonstrated the promise of sparsity and low-rank techniques. For example, the patches of the underlying data are modeled as sparse in an adaptive dictionary domain, and the resulting image and dictionary estimation from undersampled measurements is called dictionary-blind compressed sensing, or the dynamic image sequence is modeled as a sum of low-rank and sparse (in some transform domain) components (L+S model) that are estimated from limited measurements. In this work, we investigate a data-adaptive extension of the L+S model, dubbed LASSI, where the temporal image sequence is decomposed into a low-rank component and a component whose spatiotemporal (3D) patches are sparse in some adaptive dictionary domain. We investigate various formulations and efficient methods for jointly estimating the underlying dynamic signal components and the spatiotemporal dictionary from limited measurements. We also obtain efficient sparsity penalized dictionary-blind compressed sensing methods as special cases of our LASSI approaches. Our numerical experiments demonstrate the promising performance of LASSI schemes for dynamic magnetic resonance image reconstruction from limited k-t space data compared to recent methods such as k-t SLR and L+S, and compared to the proposed dictionary-blind compressed sensing method.

</details>

<details>

<summary>2017-01-12 03:35:13 - SIPHON: Towards Scalable High-Interaction Physical Honeypots</summary>

- *Juan Guarnizo, Amit Tambe, Suman Sankar Bhunia, Martín Ochoa, Nils Tippenhauer, Asaf Shabtai, Yuval Elovici*

- `1701.02446v2` - [abs](http://arxiv.org/abs/1701.02446v2) - [pdf](http://arxiv.org/pdf/1701.02446v2)

> In recent years, the emerging Internet-of-Things (IoT) has led to rising concerns about the security of networked embedded devices. In this work, we focus on the adaptation of Honeypots for improving the security of IoTs. Low-interaction honeypots are used so far in the context of IoT. Such honeypots are limited and easily detectable, and thus, there is a need to find ways how to develop high-interaction, reliable, IoT honeypots that will attract skilled attackers. In this work, we propose the SIPHON architecture - a Scalable high-Interaction Honeypot platform for IoT devices. Our architecture leverages IoT devices that are physically at one location and are connected to the Internet through so-called wormholes distributed around the world. The resulting architecture allows exposing few physical devices over a large number of geographically distributed IP addresses. We demonstrate the proposed architecture in a large scale experiment with 39 wormhole instances in 16 cities in 9 countries. Based on this setup, six physical IP cameras, one NVR and one IP printer are presented as 85 real IoT devices on the Internet, attracting a daily traffic of 700MB for a period of two months. A preliminary analysis of the collected traffic indicates that devices in some cities attracted significantly more traffic than others (ranging from 600 000 incoming TCP connections for the most popular destination to less than 50000 for the least popular). We recorded over 400 brute-force login attempts to the web-interface of our devices using a total of 1826 distinct credentials, from which 11 attempts were successful. Moreover, we noted login attempts to Telnet and SSH ports some of which used credentials found in the recently disclosed Mirai malware.

</details>


## 2017-02

<details>

<summary>2017-02-02 10:52:03 - Optical Flow Requires Multiple Strategies (but only one network)</summary>

- *Tal Schuster, Lior Wolf, David Gadot*

- `1611.05607v3` - [abs](http://arxiv.org/abs/1611.05607v3) - [pdf](http://arxiv.org/pdf/1611.05607v3)

> We show that the matching problem that underlies optical flow requires multiple strategies, depending on the amount of image motion and other factors. We then study the implications of this observation on training a deep neural network for representing image patches in the context of descriptor based optical flow. We propose a metric learning method, which selects suitable negative samples based on the nature of the true match. This type of training produces a network that displays multiple strategies depending on the input and leads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.

</details>

<details>

<summary>2017-02-27 03:26:56 - Stochastic Patching Process</summary>

- *Xuhui Fan, Bin Li, Yi Wang, Yang Wang, Fang Chen*

- `1605.06886v2` - [abs](http://arxiv.org/abs/1605.06886v2) - [pdf](http://arxiv.org/pdf/1605.06886v2)

> Stochastic partition models tailor a product space into a number of rectangular regions such that the data within each region exhibit certain types of homogeneity. Due to constraints of partition strategy, existing models may cause unnecessary dissections in sparse regions when fitting data in dense regions. To alleviate this limitation, we propose a parsimonious partition model, named Stochastic Patching Process (SPP), to deal with multi-dimensional arrays. SPP adopts an "enclosing" strategy to attach rectangular patches to dense regions. SPP is self-consistent such that it can be extended to infinite arrays. We apply SPP to relational modeling and the experimental results validate its merit compared to the state-of-the-arts.

</details>

<details>

<summary>2017-02-27 08:53:31 - DeepNAT: Deep Convolutional Neural Network for Segmenting Neuroanatomy</summary>

- *Christian Wachinger, Martin Reuter, Tassilo Klein*

- `1702.08192v1` - [abs](http://arxiv.org/abs/1702.08192v1) - [pdf](http://arxiv.org/pdf/1702.08192v1)

> We introduce DeepNAT, a 3D Deep convolutional neural network for the automatic segmentation of NeuroAnaTomy in T1-weighted magnetic resonance images. DeepNAT is an end-to-end learning-based approach to brain segmentation that jointly learns an abstract feature representation and a multi-class classification. We propose a 3D patch-based approach, where we do not only predict the center voxel of the patch but also neighbors, which is formulated as multi-task learning. To address a class imbalance problem, we arrange two networks hierarchically, where the first one separates foreground from background, and the second one identifies 25 brain structures on the foreground. Since patches lack spatial context, we augment them with coordinates. To this end, we introduce a novel intrinsic parameterization of the brain volume, formed by eigenfunctions of the Laplace-Beltrami operator. As network architecture, we use three convolutional layers with pooling, batch normalization, and non-linearities, followed by fully connected layers with dropout. The final segmentation is inferred from the probabilistic output of the network with a 3D fully connected conditional random field, which ensures label agreement between close voxels. The roughly 2.7 million parameters in the network are learned with stochastic gradient descent. Our results show that DeepNAT compares favorably to state-of-the-art methods. Finally, the purely learning-based method may have a high potential for the adaptation to young, old, or diseased brains by fine-tuning the pre-trained network with a small training sample on the target application, where the availability of larger datasets with manual annotations may boost the overall segmentation accuracy in the future.

</details>

<details>

<summary>2017-02-28 14:17:16 - Discriminative Neural Topic Models</summary>

- *Gaurav Pandey, Ambedkar Dukkipati*

- `1701.06796v2` - [abs](http://arxiv.org/abs/1701.06796v2) - [pdf](http://arxiv.org/pdf/1701.06796v2)

> We propose a neural network based approach for learning topics from text and image datasets. The model makes no assumptions about the conditional distribution of the observed features given the latent topics. This allows us to perform topic modelling efficiently using sentences of documents and patches of images as observed features, rather than limiting ourselves to words. Moreover, the proposed approach is online, and hence can be used for streaming data. Furthermore, since the approach utilizes neural networks, it can be implemented on GPU with ease, and hence it is very scalable.

</details>


## 2017-03

<details>

<summary>2017-03-01 09:55:30 - Test Case Generation for Program Repair: A Study of Feasibility and Effectiveness</summary>

- *Zhongxing Yu, Matias Martinez, Benjamin Danglot, Thomas Durieux, Martin Monperrus*

- `1703.00198v1` - [abs](http://arxiv.org/abs/1703.00198v1) - [pdf](http://arxiv.org/pdf/1703.00198v1)

> Among the many different kinds of program repair techniques, one widely studied family of techniques is called test suite based repair. Test-suites are in essence input-output specifications and are therefore typically inadequate for completely specifying the expected behavior of the program under repair. Consequently, the patches generated by test suite based program repair techniques pass the test suite, yet may be incorrect. Patches that are overly specific to the used test suite and fail to generalize to other test cases are called overfitting patches. In this paper, we investigate the feasibility and effectiveness of test case generation in alleviating the overfitting issue. We propose two approaches for using test case generation to improve test suite based repair, and perform an extensive evaluation of the effectiveness of the proposed approaches in enabling better test suite based repair on 224 bugs of the Defects4J repository. The results indicate that test case generation can change the resulting patch, but is not effective at turning incorrect patches into correct ones. We identify the problems related with the ineffectiveness, and anticipate that our results and findings will lead to future research to build test-case generation techniques that are tailored to automatic repair systems.

</details>

<details>

<summary>2017-03-14 19:15:39 - HYDRA: HYbrid Design for Remote Attestation (Using a Formally Verified Microkernel)</summary>

- *Karim ElDefrawy, Norrathep Rattanavipanon, Gene Tsudik*

- `1703.02688v2` - [abs](http://arxiv.org/abs/1703.02688v2) - [pdf](http://arxiv.org/pdf/1703.02688v2)

> Remote Attestation (RA) allows a trusted entity (verifier) to securely measure internal state of a remote untrusted hardware platform (prover). RA can be used to establish a static or dynamic root of trust in embedded and cyber-physical systems. It can also be used as a building block for other security services and primitives, such as software updates and patches, verifiable deletion and memory resetting. There are three major classes of RA designs: hardware-based, software-based, and hybrid, each with its own set of benefits and drawbacks. This paper presents the first hybrid RA design, called HYDRA, that builds upon formally verified software components that ensure memory isolation and protection, as well as enforce access control to memory and other resources. HYDRA obtains these properties by using the formally verified seL4 microkernel. (Until now, this was only attainable with purely hardware-based designs.) Using seL4 requires fewer hardware modifications to the underlying microprocessor. Building upon a formally verified software component increases confidence in security of the overall design of HYDRA and its implementation. We instantiate HYDRA on two commodity hardware platforms and assess the performance and overhead of performing RA on such platforms via experimentation; we show that HYDRA can attest 10MB of memory in less than 500msec when using a Speck-based message authentication code (MAC) to compute a cryptographic checksum over the memory to be attested.

</details>

<details>

<summary>2017-03-15 21:59:05 - Traffic-aware Patching for Cyber Security in Mobile IoT</summary>

- *Shin-Ming Cheng, Pin-Yu Chen, Ching-Chao Lin, Hsu-Chun Hsiao*

- `1703.05400v1` - [abs](http://arxiv.org/abs/1703.05400v1) - [pdf](http://arxiv.org/pdf/1703.05400v1)

> The various types of communication technologies and mobility features in Internet of Things (IoT) on the one hand enable fruitful and attractive applications, but on the other hand facilitates malware propagation, thereby raising new challenges on handling IoT-empowered malware for cyber security. Comparing with the malware propagation control scheme in traditional wireless networks where nodes can be directly repaired and secured, in IoT, compromised end devices are difficult to be patched. Alternatively, blocking malware via patching intermediate nodes turns out to be a more feasible and practical solution. Specifically, patching intermediate nodes can effectively prevent the proliferation of malware propagation by securing infrastructure links and limiting malware propagation to local device-to-device dissemination. This article proposes a novel traffic-aware patching scheme to select important intermediate nodes to patch, which applies to the IoT system with limited patching resources and response time constraint. Experiments on real-world trace datasets in IoT networks are conducted to demonstrate the advantage of the proposed traffic-aware patching scheme in alleviating malware propagation.

</details>

<details>

<summary>2017-03-16 01:46:37 - Semantic Change Detection with Hypermaps</summary>

- *Teppei Suzuki, Soma Shirakabe, Yudai Miyashita, Akio Nakamura, Yutaka Satoh, Hirokatsu Kataoka*

- `1604.07513v2` - [abs](http://arxiv.org/abs/1604.07513v2) - [pdf](http://arxiv.org/pdf/1604.07513v2)

> Change detection is the study of detecting changes between two different images of a scene taken at different times. By the detected change areas, however, a human cannot understand how different the two images. Therefore, a semantic understanding is required in the change detection research such as disaster investigation. The paper proposes the concept of semantic change detection, which involves intuitively inserting semantic meaning into detected change areas. We mainly focus on the novel semantic segmentation in addition to a conventional change detection approach. In order to solve this problem and obtain a high-level of performance, we propose an improvement to the hypercolumns representation, hereafter known as hypermaps, which effectively uses convolutional maps obtained from convolutional neural networks (CNNs). We also employ multi-scale feature representation captured by different image patches. We applied our method to the TSUNAMI Panoramic Change Detection dataset, and re-annotated the changed areas of the dataset via semantic classes. The results show that our multi-scale hypermaps provided outstanding performance on the re-annotated TSUNAMI dataset.

</details>

<details>

<summary>2017-03-17 08:27:05 - Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</summary>

- *Thomas Schlegl, Philipp Seeböck, Sebastian M. Waldstein, Ursula Schmidt-Erfurth, Georg Langs*

- `1703.05921v1` - [abs](http://arxiv.org/abs/1703.05921v1) - [pdf](http://arxiv.org/pdf/1703.05921v1)

> Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.

</details>

<details>

<summary>2017-03-22 16:28:17 - On the Probe Complexity of Local Computation Algorithms</summary>

- *Uriel Feige, Boaz Patt-Shamir, Shai Vardi*

- `1703.07734v1` - [abs](http://arxiv.org/abs/1703.07734v1) - [pdf](http://arxiv.org/pdf/1703.07734v1)

> The Local Computation Algorithms (LCA) model is a computational model aimed at problem instances with huge inputs and output. For graph problems, the input graph is accessed using probes: strong probes (SP) specify a vertex $v$ and receive as a reply a list of $v$'s neighbors, and weak probes (WP) specify a vertex $v$ and a port number $i$ and receive as a reply $v$'s $i^{th}$ neighbor. Given a local query (e.g., "is a certain vertex in the vertex cover of the input graph?"), an LCA should compute the corresponding local output (e.g., "yes" or "no") while making only a small number of probes, with the requirement that all local outputs form a single global solution (e.g., a legal vertex cover). We study the probe complexity of LCAs that are required to work on graphs that may have arbitrarily large degrees. In particular, such LCAs are expected to probe the graph a number of times that is significantly smaller than the maximum, average, or even minimum degree.   For weak probes, we focus on the weak coloring problem. Among our results we show a separation between weak 3-coloring and weak 2-coloring for deterministic LCAs: $\log^* n + O(1)$ weak probes suffice for weak 3-coloring, but $\Omega\left(\frac{\log n}{\log\log n}\right)$ weak probes are required for weak 2-coloring.   For strong probes, we consider randomized LCAs for vertex cover and maximal/maximum matching. Our negative results include showing that there are graphs for which finding a \emph{maximal} matching requires $\Omega(\sqrt{n})$ strong probes. On the positive side, we design a randomized LCA that finds a $(1-\epsilon)$ approximation to \emph{maximum} matching in regular graphs, and uses $\frac{1}{\epsilon }^{O\left( \frac{1}{\epsilon ^2}\right)}$ probes, independently of the number of vertices and of their degrees.

</details>

<details>

<summary>2017-03-29 05:58:21 - LabelBank: Revisiting Global Perspectives for Semantic Segmentation</summary>

- *Hexiang Hu, Zhiwei Deng, Guang-Tong Zhou, Fei Sha, Greg Mori*

- `1703.09891v1` - [abs](http://arxiv.org/abs/1703.09891v1) - [pdf](http://arxiv.org/pdf/1703.09891v1)

> Semantic segmentation requires a detailed labeling of image pixels by object category. Information derived from local image patches is necessary to describe the detailed shape of individual objects. However, this information is ambiguous and can result in noisy labels. Global inference of image content can instead capture the general semantic concepts present. We advocate that holistic inference of image concepts provides valuable information for detailed pixel labeling. We propose a generic framework to leverage holistic information in the form of a LabelBank for pixel-level segmentation.   We show the ability of our framework to improve semantic segmentation performance in a variety of settings. We learn models for extracting a holistic LabelBank from visual cues, attributes, and/or textual descriptions. We demonstrate improvements in semantic segmentation accuracy on standard datasets across a range of state-of-the-art segmentation architectures and holistic inference approaches.

</details>


## 2017-04

<details>

<summary>2017-04-05 15:25:11 - CHAOS: an SDN-based Moving Target Defense System</summary>

- *Juan Wang, Feng Xiao, Jianwei Huang, Daochen Zha, Hongxin Hu, Huanguo Zhan*

- `1704.01482v1` - [abs](http://arxiv.org/abs/1704.01482v1) - [pdf](http://arxiv.org/pdf/1704.01482v1)

> The static nature of current cyber systems has made them easy to be attacked and compromised. By constantly changing a system, Moving Target Defense (MTD) has provided a promising way to reduce or move the attack surface that is available for exploitation by an adversary. However, the current network- based MTD obfuscates networks indiscriminately that makes some networks key services, such as web and DNS services, unavailable, because many information of these services has to be opened to the outside and remain real without compromising their usability. Moreover, the indiscriminate obfuscation also severely reduces the performance of networks. In this paper, we propose CHAOS, an SDN (Software-defined networking)-based MTD system, which discriminately obfuscates hosts with different security levels in a network. In CHAOS, we introduce a Chaos Tower Obfuscation (CTO) method, which uses a Chaos Tower Structure (CTS) to depict the hierarchy of all the hosts in an intranet and provides a more unpredictable and flexible obfuscation method. We also present the design of CHAOS, which leverages SDN features to obfuscate the attack surface including IP obfuscation, ports obfuscation, and fingerprint obfuscation thereby enhancing the unpredictability of the networking environment. We develop fast CTO algorithms to achieve a different degree of obfuscation for the hosts in each layer. Our experimental results show that a network protected by CHAOS is capable of decreasing the percentage of information disclosure effectively to guarantee the normal flow of traffic.

</details>

<details>

<summary>2017-04-11 13:34:17 - Harmonic Networks: Deep Translation and Rotation Equivariance</summary>

- *Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, Gabriel J. Brostow*

- `1612.04642v2` - [abs](http://arxiv.org/abs/1612.04642v2) - [pdf](http://arxiv.org/pdf/1612.04642v2)

> Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch.   H-Nets use a rich, parameter-efficient and low computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.

</details>

<details>

<summary>2017-04-14 10:16:50 - Enabling Embedded Inference Engine with ARM Compute Library: A Case Study</summary>

- *Dawei Sun, Shaoshan Liu, Jean-Luc Gaudiot*

- `1704.03751v3` - [abs](http://arxiv.org/abs/1704.03751v3) - [pdf](http://arxiv.org/pdf/1704.03751v3)

> When you need to enable deep learning on low-cost embedded SoCs, is it better to port an existing deep learning framework or should you build one from scratch? In this paper, we share our practical experiences of building an embedded inference engine using ARM Compute Library (ACL). The results show that, contradictory to conventional wisdoms, for simple models, it takes much less development time to build an inference engine from scratch compared to porting existing frameworks. In addition, by utilizing ACL, we managed to build an inference engine that outperforms TensorFlow by 25%. Our conclusion is that, on embedded devices, we most likely will use very simple deep learning models for inference, and with well-developed building blocks such as ACL, it may be better in both performance and development time to build the engine from scratch.

</details>

<details>

<summary>2017-04-25 15:09:51 - DeepAM: Migrate APIs with Multi-modal Sequence to Sequence Learning</summary>

- *Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim*

- `1704.07734v1` - [abs](http://arxiv.org/abs/1704.07734v1) - [pdf](http://arxiv.org/pdf/1704.07734v1)

> Computer programs written in one language are often required to be ported to other languages to support multiple devices and environments. When programs use language specific APIs (Application Programming Interfaces), it is very challenging to migrate these APIs to the corresponding APIs written in other languages. Existing approaches mine API mappings from projects that have corresponding versions in two languages. They rely on the sparse availability of bilingual projects, thus producing a limited number of API mappings. In this paper, we propose an intelligent system called DeepAM for automatically mining API mappings from a large-scale code corpus without bilingual projects. The key component of DeepAM is based on the multimodal sequence to sequence learning architecture that aims to learn joint semantic representations of bilingual API sequences from big source code data. Experimental results indicate that DeepAM significantly increases the accuracy of API mappings as well as the number of API mappings, when compared with the state-of-the-art approaches.

</details>

<details>

<summary>2017-04-27 16:15:30 - Learning Correspondence Structures for Person Re-identification</summary>

- *Weiyao Lin, Yang Shen, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong Wang, Ke Lu*

- `1703.06931v3` - [abs](http://arxiv.org/abs/1703.06931v3) - [pdf](http://arxiv.org/pdf/1703.06931v3)

> This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global constraint-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Finally, we also extend our approach by introducing a multi-structure scheme, which learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, so as to handle the spatial misalignments between individual images in a more precise way. Experimental results on various datasets demonstrate the effectiveness of our approach.

</details>

<details>

<summary>2017-04-29 05:01:08 - Evaluating Security and Availability of Multiple Redundancy Designs when Applying Security Patches</summary>

- *Mengmeng Ge, Huy Kang Kim, Dong Seong Kim*

- `1705.00128v1` - [abs](http://arxiv.org/abs/1705.00128v1) - [pdf](http://arxiv.org/pdf/1705.00128v1)

> In most of modern enterprise systems, redundancy configuration is often considered to provide availability during the part of such systems is being patched. However, the redundancy may increase the attack surface of the system. In this paper, we model and assess the security and capacity oriented availability of multiple server redundancy designs when applying security patches to the servers. We construct (1) a graphical security model to evaluate the security under potential attacks before and after applying patches, (2) a stochastic reward net model to assess the capacity oriented availability of the system with a patch schedule. We present our approach based on case study and model-based evaluation for multiple design choices. The results show redundancy designs increase capacity oriented availability but decrease security when applying security patches. We define functions that compare values of security metrics and capacity oriented availability with the chosen upper/lower bounds to find design choices that satisfy both security and availability requirements.

</details>


## 2017-05

<details>

<summary>2017-05-02 06:17:31 - ACDC: Altering Control Dependence Chains for Automated Patch Generation</summary>

- *Rawad Abou Assi, Chadi Trad, Wes Masri*

- `1705.00811v1` - [abs](http://arxiv.org/abs/1705.00811v1) - [pdf](http://arxiv.org/pdf/1705.00811v1)

> Once a failure is observed, the primary concern of the developer is to identify what caused it in order to repair the code that induced the incorrect behavior. Until a permanent repair is afforded, code repair patches are invaluable. The aim of this work is to devise an automated patch generation technique that proceeds as follows: Step1) It identifies a set of failure-causing control dependence chains that are minimal in terms of number and length. Step2) It identifies a set of predicates within the chains along with associated execution instances, such that negating the predicates at the given instances would exhibit correct behavior. Step3) For each candidate predicate, it creates a classifier that dictates when the predicate should be negated to yield correct program behavior. Step4) Prior to each candidate predicate, the faulty program is injected with a call to its corresponding classifier passing it the program state and getting a return value predictively indicating whether to negate the predicate or not. The role of the classifiers is to ensure that: 1) the predicates are not negated during passing runs; and 2) the predicates are negated at the appropriate instances within failing runs. We implemented our patch generation approach for the Java platform and evaluated our toolset using 148 defects from the Introclass and Siemens benchmarks. The toolset identified 56 full patches and another 46 partial patches, and the classification accuracy averaged 84%.

</details>

<details>

<summary>2017-05-12 02:13:26 - Can defects be fixed with weak test suites? An analysis of 50 defects from Defects4J</summary>

- *Jiajun Jiang, Yingfei Xiong*

- `1705.04149v2` - [abs](http://arxiv.org/abs/1705.04149v2) - [pdf](http://arxiv.org/pdf/1705.04149v2)

> Automated program repair techniques, which target to generating correct patches for real world defects automatically, have gained a lot of attention in the last decade. Many different techniques and tools have been proposed and developed. However, even the most sophisticated program repair techniques can only repair a small portion of defects while producing a lot of incorrect patches. A possible reason for this low performance is that the test suites of real world programs are usually too weak to guarantee the behavior of the program. To understand to what extent defects can be fixed with weak test suites, we analyzed 50 real world defects from Defects4J, in which we found that up to 84% of them could be correctly fixed. This result suggests that there is plenty of space for current automated program repair techniques to improve. Furthermore, we summarized seven fault localization strategies and seven patch generation strategies that were useful in localizing and fixing these defects, and compared those strategies with current repair techniques. The results indicate potential directions to improve automatic program repair in the future research.

</details>

<details>

<summary>2017-05-19 09:30:32 - Optimized Certificate Revocation List Distribution for Secure V2X Communications</summary>

- *Giovanni Rigazzi, Andrea Tassi, Robert J. Piechocki, Theo Tryfonas, Andrew Nix*

- `1705.06903v1` - [abs](http://arxiv.org/abs/1705.06903v1) - [pdf](http://arxiv.org/pdf/1705.06903v1)

> The successful deployment of safe and trustworthy Connected and Autonomous Vehicles (CAVs) will highly depend on the ability to devise robust and effective security solutions to resist sophisticated cyber attacks and patch up critical vulnerabilities. Pseudonym Public Key Infrastructure (PPKI) is a promising approach to secure vehicular networks as well as ensure data and location privacy, concealing the vehicles' real identities. Nevertheless, pseudonym distribution and management affect PPKI scalability due to the significant number of digital certificates required by a single vehicle. In this paper, we focus on the certificate revocation process and propose a versatile and low-complexity framework to facilitate the distribution of the Certificate Revocation Lists (CRL) issued by the Certification Authority (CA). CRL compression is achieved through optimized Bloom filters, which guarantee a considerable overhead reduction with a configurable rate of false positives. Our results show that the distribution of compressed CRLs can significantly enhance the system scalability without increasing the complexity of the revocation process.

</details>

<details>

<summary>2017-05-20 15:04:34 - Formalized Lambek Calculus in Higher Order Logic (HOL4)</summary>

- *Chun Tian*

- `1705.07318v1` - [abs](http://arxiv.org/abs/1705.07318v1) - [pdf](http://arxiv.org/pdf/1705.07318v1)

> In this project, a rather complete proof-theoretical formalization of Lambek Calculus (non-associative with arbitrary extensions) has been ported from Coq proof assistent to HOL4 theorem prover, with some improvements and new theorems.   Three deduction systems (Syntactic Calculus, Natural Deduction and Sequent Calculus) of Lambek Calculus are defined with many related theorems proved. The equivalance between these systems are formally proved. Finally, a formalization of Sequent Calculus proofs (where Coq has built-in supports) has been designed and implemented in HOL4. Some basic results including the sub-formula properties of the so-called "cut-free" proofs are formally proved.   This work can be considered as the preliminary work towards a language parser based on category grammars which is not multimodal but still has ability to support context-sensitive languages through customized extensions.

</details>

<details>

<summary>2017-05-23 16:45:20 - Parallel Matrix-Free Implementation of Frequency-Domain Finite Difference Methods for Cluster Computing</summary>

- *Amir Geranmayeh*

- `1705.08849v1` - [abs](http://arxiv.org/abs/1705.08849v1) - [pdf](http://arxiv.org/pdf/1705.08849v1)

> Full-wave 3D electromagnetic simulations of complex planar devices, multilayer interconnects, and chip packages are presented for wide-band frequency-domain analysis using the finite difference integration technique developed in the PETSc software package. Initial reordering of the index assignment to the unknowns makes the resulting system matrix diagonally dominant. The rearrangement also facilitates the decomposition of large domain into slices for passing the mesh information to different machines. Matrix-free methods are then exploited to minimize the number of element-wise multiplications and memory requirements in the construction of the system of linear equations. Besides, the recipes provide extreme ease of modifications in the kernel of the code. The applicability of different Krylov subspace solvers is investigated. The accuracy is checked through comparisons with CST MICROWAVE STUDIO transient solver results. The parallel execution of the compiled code on specific number of processors in multi-core distributed-memory architectures demonstrate high scalability of the computational algorithm.

</details>

<details>

<summary>2017-05-23 22:16:20 - Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification</summary>

- *Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie*

- `1705.08550v1` - [abs](http://arxiv.org/abs/1705.08550v1) - [pdf](http://arxiv.org/pdf/1705.08550v1)

> Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods rely on regions of interest (ROIs) which require great efforts to annotate. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning (MIL) for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned ROIs. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed networks compared to previous work using segmentation and detection annotations.

</details>

<details>

<summary>2017-05-27 12:40:24 - Global hard thresholding algorithms for joint sparse image representation and denoising</summary>

- *Reza Borhani, Jeremy Watt, Aggelos Katsaggelos*

- `1705.09816v1` - [abs](http://arxiv.org/abs/1705.09816v1) - [pdf](http://arxiv.org/pdf/1705.09816v1)

> Sparse coding of images is traditionally done by cutting them into small patches and representing each patch individually over some dictionary given a pre-determined number of nonzero coefficients to use for each patch. In lack of a way to effectively distribute a total number (or global budget) of nonzero coefficients across all patches, current sparse recovery algorithms distribute the global budget equally across all patches despite the wide range of differences in structural complexity among them. In this work we propose a new framework for joint sparse representation and recovery of all image patches simultaneously. We also present two novel global hard thresholding algorithms, based on the notion of variable splitting, for solving the joint sparse model. Experimentation using both synthetic and real data shows effectiveness of the proposed framework for sparse image representation and denoising tasks. Additionally, time complexity analysis of the proposed algorithms indicate high scalability of both algorithms, making them favorable to use on large megapixel images.

</details>


## 2017-06

<details>

<summary>2017-06-02 19:28:19 - CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases</summary>

- *Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Tarek F. Abdelzaher, Jiawei Han*

- `1610.08763v2` - [abs](http://arxiv.org/abs/1610.08763v2) - [pdf](http://arxiv.org/pdf/1610.08763v2)

> Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domain-independent framework, called CoType, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. CoType, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object "translation" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of CoType across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method.

</details>

<details>

<summary>2017-06-03 00:48:12 - MobiRNN: Efficient Recurrent Neural Network Execution on Mobile GPU</summary>

- *Qingqing Cao, Niranjan Balasubramanian, Aruna Balasubramanian*

- `1706.00878v1` - [abs](http://arxiv.org/abs/1706.00878v1) - [pdf](http://arxiv.org/pdf/1706.00878v1)

> In this paper, we explore optimizations to run Recurrent Neural Network (RNN) models locally on mobile devices. RNN models are widely used for Natural Language Processing, Machine Translation, and other tasks. However, existing mobile applications that use RNN models do so on the cloud. To address privacy and efficiency concerns, we show how RNN models can be run locally on mobile devices. Existing work on porting deep learning models to mobile devices focus on Convolution Neural Networks (CNNs) and cannot be applied directly to RNN models. In response, we present MobiRNN, a mobile-specific optimization framework that implements GPU offloading specifically for mobile GPUs. Evaluations using an RNN model for activity recognition shows that MobiRNN does significantly decrease the latency of running RNN models on phones.

</details>

<details>

<summary>2017-06-07 17:05:16 - An Unsupervised Algorithm For Learning Lie Group Transformations</summary>

- *Jascha Sohl-Dickstein, Ching Ming Wang, Bruno A. Olshausen*

- `1001.1027v5` - [abs](http://arxiv.org/abs/1001.1027v5) - [pdf](http://arxiv.org/pdf/1001.1027v5)

> We present several theoretical contributions which allow Lie groups to be fit to high dimensional datasets. Transformation operators are represented in their eigen-basis, reducing the computational complexity of parameter estimation to that of training a linear transformation model. A transformation specific "blurring" operator is introduced that allows inference to escape local minima via a smoothing of the transformation space. A penalty on traversed manifold distance is added which encourages the discovery of sparse, minimal distance, transformations between states. Both learning and inference are demonstrated using these methods for the full set of affine transformations on natural image patches. Transformation operators are then trained on natural video sequences. It is shown that the learned video transformations provide a better description of inter-frame differences than the standard motion model based on rigid translation.

</details>

<details>

<summary>2017-06-08 00:10:04 - Dense Transformer Networks</summary>

- *Jun Li, Yongjun Chen, Lei Cai, Ian Davidson, Shuiwang Ji*

- `1705.08881v2` - [abs](http://arxiv.org/abs/1705.08881v2) - [pdf](http://arxiv.org/pdf/1705.08881v2)

> The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.

</details>

<details>

<summary>2017-06-16 17:27:41 - Local Feature Descriptor Learning with Adaptive Siamese Network</summary>

- *Chong Huang, Qiong Liu, Yan-Ying Chen, Kwang-Ting, Cheng*

- `1706.05358v1` - [abs](http://arxiv.org/abs/1706.05358v1) - [pdf](http://arxiv.org/pdf/1706.05358v1)

> Although the recent progress in the deep neural network has led to the development of learnable local feature descriptors, there is no explicit answer for estimation of the necessary size of a neural network. Specifically, the local feature is represented in a low dimensional space, so the neural network should have more compact structure. The small networks required for local feature descriptor learning may be sensitive to initial conditions and learning parameters and more likely to become trapped in local minima. In order to address the above problem, we introduce an adaptive pruning Siamese Architecture based on neuron activation to learn local feature descriptors, making the network more computationally efficient with an improved recognition rate over more complex networks. Our experiments demonstrate that our learned local feature descriptors outperform the state-of-art methods in patch matching.

</details>

<details>

<summary>2017-06-19 04:37:15 - Hey, you, keep away from my device: remotely implanting a virus expeller to defeat Mirai on IoT devices</summary>

- *Chen Cao, Le Guan, Peng Liu, Neng Gao, Jingqiang Lin, Ji Xiang*

- `1706.05779v1` - [abs](http://arxiv.org/abs/1706.05779v1) - [pdf](http://arxiv.org/pdf/1706.05779v1)

> Mirai is botnet which targets out-of-date Internet-of-Things (IoT) devices. The disruptive Distributed Denial of Service (DDoS) attack last year has hit major Internet companies, causing intermittent service for millions of Internet users. Since the affected devices typically do not support firmware update, it becomes challenging to expel these vulnerable devices in the wild.   Both industry and academia have made great efforts in amending the situation. However, none of these efforts is simple to deploy, and at the same time effective in solving the problem. In this work, we design a collaborative defense strategy to tackle Mirai. Our key idea is to take advantage of human involvement in the least aggressive way. In particular, at a negotiated time slot, a customer is required to reboot the compromised device, then a "white" Mirai operated by the manufacturer breaks into the clean-state IoT devices immediately. The "white" Mirai expels other malicious Mirai variants, blocks vulnerable ports, and keeps a heart-beat connection with the server operated by the manufacturer. Once the heart-beat is lost, the server re-implants the "white" Mirai instantly. We have implemented a full prototype of the designed system, and the results show that our system can evade Mirai attacks effectively.

</details>

<details>

<summary>2017-06-21 06:11:59 - Cross-language Learning with Adversarial Neural Networks: Application to Community Question Answering</summary>

- *Shafiq Joty, Preslav Nakov, Lluís Màrquez, Israa Jaradat*

- `1706.06749v1` - [abs](http://arxiv.org/abs/1706.06749v1) - [pdf](http://arxiv.org/pdf/1706.06749v1)

> We address the problem of cross-language adaptation for question-question similarity reranking in community question answering, with the objective to port a system trained on one input language to another input language given labeled training data for the first language and only unlabeled data for the second language. In particular, we propose to use adversarial training of neural networks to learn high-level features that are discriminative for the main learning task, and at the same time are invariant across the input languages. The evaluation results show sizable improvements for our cross-language adversarial neural network (CLANN) model over a strong non-adversarial system.

</details>

<details>

<summary>2017-06-22 04:37:31 - Curvature-aware Manifold Learning</summary>

- *Yangyang Li*

- `1706.07167v1` - [abs](http://arxiv.org/abs/1706.07167v1) - [pdf](http://arxiv.org/pdf/1706.07167v1)

> Traditional manifold learning algorithms assumed that the embedded manifold is globally or locally isometric to Euclidean space. Under this assumption, they divided manifold into a set of overlapping local patches which are locally isometric to linear subsets of Euclidean space. By analyzing the global or local isometry assumptions it can be shown that the learnt manifold is a flat manifold with zero Riemannian curvature tensor. In general, manifolds may not satisfy these hypotheses. One major limitation of traditional manifold learning is that it does not consider the curvature information of manifold. In order to remove these limitations, we present our curvature-aware manifold learning algorithm called CAML. The purpose of our algorithm is to break the local isometry assumption and to reduce the dimension of the general manifold which is not isometric to Euclidean space. Thus, our method adds the curvature information to the process of manifold learning. The experiments have shown that our method CAML is more stable than other manifold learning algorithms by comparing the neighborhood preserving ratios.

</details>

<details>

<summary>2017-06-28 17:59:33 - Recovery of Missing Samples Using Sparse Approximation via a Convex Similarity Measure</summary>

- *Amirhossein Javaheri, Hadi Zayyani, Farokh Marvasti*

- `1706.09395v1` - [abs](http://arxiv.org/abs/1706.09395v1) - [pdf](http://arxiv.org/pdf/1706.09395v1)

> In this paper, we study the missing sample recovery problem using methods based on sparse approximation. In this regard, we investigate the algorithms used for solving the inverse problem associated with the restoration of missed samples of image signal. This problem is also known as inpainting in the context of image processing and for this purpose, we suggest an iterative sparse recovery algorithm based on constrained $l_1$-norm minimization with a new fidelity metric. The proposed metric called Convex SIMilarity (CSIM) index, is a simplified version of the Structural SIMilarity (SSIM) index, which is convex and error-sensitive. The optimization problem incorporating this criterion, is then solved via Alternating Direction Method of Multipliers (ADMM). Simulation results show the efficiency of the proposed method for missing sample recovery of 1D patch vectors and inpainting of 2D image signals.

</details>


## 2017-07

<details>

<summary>2017-07-15 05:55:44 - Partitioning Patches into Test-equivalence Classes for Scaling Program Repair</summary>

- *Sergey Mechtaev, Xiang Gao, Shin Hwei Tan, Abhik Roychoudhury*

- `1707.03139v2` - [abs](http://arxiv.org/abs/1707.03139v2) - [pdf](http://arxiv.org/pdf/1707.03139v2)

> Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading assignments and patching security vulnerabilities. A common challenge faced by all existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider.   The correctness criterion for program repair is often given as a suite of tests, since a formal specification of the intended program behavior may not be available. Current repair techniques do not scale due to the large number of test executions performed by the underlying search algorithms. We address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are "test-equivalent" for a given test, they produce indistinguishable results on this test). We propose two test-equivalence relations based on runtime values and dependencies respectively and present an algorithm that performs on-the-fly partitioning of patches into test-equivalence classes.   Our experiments on real-world programs reveal that the proposed methodology drastically reduces the number of test executions and therefore provides an order of magnitude efficiency improvement over existing repair techniques, without sacrificing patch quality.

</details>

<details>

<summary>2017-07-17 14:13:30 - An Empirical Analysis of the Influence of Fault Space on Search-Based Automated Program Repair</summary>

- *Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, Shing-Chi Cheung*

- `1707.05172v1` - [abs](http://arxiv.org/abs/1707.05172v1) - [pdf](http://arxiv.org/pdf/1707.05172v1)

> Automated program repair (APR) has attracted great research attention, and various techniques have been proposed. Search-based APR is one of the most important categories among these techniques. Existing researches focus on the design of effective mutation operators and searching algorithms to better find the correct patch. Despite various efforts, the effectiveness of these techniques are still limited by the search space explosion problem. One of the key factors attribute to this problem is the quality of fault spaces as reported by existing studies. This motivates us to study the importance of the fault space to the success of finding a correct patch. Our empirical study aims to answer three questions. Does the fault space significantly correlate with the performance of search-based APR? If so, are there any indicative measurements to approximate the accuracy of the fault space before applying expensive APR techniques? Are there any automatic methods that can improve the accuracy of the fault space? We observe that the accuracy of the fault space affects the effectiveness and efficiency of search-based APR techniques, e.g., the failure rate of GenProg could be as high as $60\%$ when the real fix location is ranked lower than 10 even though the correct patch is in the search space. Besides, GenProg is able to find more correct patches and with fewer trials when given a fault space with a higher accuracy. We also find that the negative mutation coverage, which is designed in this study to measure the capability of a test suite to kill the mutants created on the statements executed by failing tests, is the most indicative measurement to estimate the efficiency of search-based APR. Finally, we confirm that automated generated test cases can help improve the accuracy of fault spaces, and further improve the performance of search-based APR techniques.

</details>

<details>

<summary>2017-07-18 05:17:14 - Downgrade Attack on TrustZone</summary>

- *Yue Chen, Yulong Zhang, Zhi Wang, Tao Wei*

- `1707.05082v2` - [abs](http://arxiv.org/abs/1707.05082v2) - [pdf](http://arxiv.org/pdf/1707.05082v2)

> Security-critical tasks require proper isolation from untrusted software. Chip manufacturers design and include trusted execution environments (TEEs) in their processors to secure these tasks. The integrity and security of the software in the trusted environment depend on the verification process of the system.   We find a form of attack that can be performed on the current implementations of the widely deployed ARM TrustZone technology. The attack exploits the fact that the trustlet (TA) or TrustZone OS loading verification procedure may use the same verification key and may lack proper rollback prevention across versions. If an exploit works on an out-of-date version, but the vulnerability is patched on the latest version, an attacker can still use the same exploit to compromise the latest system by downgrading the software to an older and exploitable version.   We did experiments on popular devices on the market including those from Google, Samsung and Huawei, and found that all of them have the risk of being attacked. Also, we show a real-world example to exploit Qualcomm's QSEE.   In addition, in order to find out which device images share the same verification key, pattern matching schemes for different vendors are analyzed and summarized.

</details>

<details>

<summary>2017-07-19 15:15:36 - Self-paced Convolutional Neural Network for Computer Aided Detection in Medical Imaging Analysis</summary>

- *Xiang Li, Aoxiao Zhong, Ming Lin, Ning Guo, Mu Sun, Arkadiusz Sitek, Jieping Ye, James Thrall, Quanzheng Li*

- `1707.06145v1` - [abs](http://arxiv.org/abs/1707.06145v1) - [pdf](http://arxiv.org/pdf/1707.06145v1)

> Tissue characterization has long been an important component of Computer Aided Diagnosis (CAD) systems for automatic lesion detection and further clinical planning. Motivated by the superior performance of deep learning methods on various computer vision problems, there has been increasing work applying deep learning to medical image analysis. However, the development of a robust and reliable deep learning model for computer-aided diagnosis is still highly challenging due to the combination of the high heterogeneity in the medical images and the relative lack of training samples. Specifically, annotation and labeling of the medical images is much more expensive and time-consuming than other applications and often involves manual labor from multiple domain experts. In this work, we propose a multi-stage, self-paced learning framework utilizing a convolutional neural network (CNN) to classify Computed Tomography (CT) image patches. The key contribution of this approach is that we augment the size of training samples by refining the unlabeled instances with a self-paced learning CNN. By implementing the framework on high performance computing servers including the NVIDIA DGX1 machine, we obtained the experimental result, showing that the self-pace boosted network consistently outperformed the original network even with very scarce manual labels. The performance gain indicates that applications with limited training samples such as medical image analysis can benefit from using the proposed framework.

</details>

<details>

<summary>2017-07-20 14:03:16 - Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data</summary>

- *Gil Keren, Björn Schuller*

- `1602.05875v3` - [abs](http://arxiv.org/abs/1602.05875v3) - [pdf](http://arxiv.org/pdf/1602.05875v3)

> Traditional convolutional layers extract features from patches of data by applying a non-linearity on an affine function of the input. We propose a model that enhances this feature extraction process for the case of sequential data, by feeding patches of the data into a recurrent neural network and using the outputs or hidden states of the recurrent units to compute the extracted features. By doing so, we exploit the fact that a window containing a few frames of the sequential data is a sequence itself and this additional structure might encapsulate valuable information. In addition, we allow for more steps of computation in the feature extraction process, which is potentially beneficial as an affine function followed by a non-linearity can result in too simple features. Using our convolutional recurrent layers we obtain an improvement in performance in two audio classification tasks, compared to traditional convolutional layers. Tensorflow code for the convolutional recurrent layers is publicly available in https://github.com/cruvadom/Convolutional-RNN.

</details>

<details>

<summary>2017-07-24 14:09:47 - Automatic breast cancer grading in lymph nodes using a deep neural network</summary>

- *Thomas Wollmann, Karl Rohr*

- `1707.07565v1` - [abs](http://arxiv.org/abs/1707.07565v1) - [pdf](http://arxiv.org/pdf/1707.07565v1)

> The progression of breast cancer can be quantified in lymph node whole-slide images (WSIs). We describe a novel method for effectively performing classification of whole-slide images and patient level breast cancer grading. Our method utilises a deep neural network. The method performs classification on small patches and uses model averaging for boosting. In the first step, region of interest patches are determined and cropped automatically by color thresholding and then classified by the deep neural network. The classification results are used to determine a slide level class and for further aggregation to predict a patient level grade. Fast processing speed of our method enables high throughput image analysis.

</details>

<details>

<summary>2017-07-25 04:01:25 - SAR Target Recognition Using the Multi-aspect-aware Bidirectional LSTM Recurrent Neural Networks</summary>

- *Fan Zhang, Chen Hu, Qiang Yin, Wei Li, Hengchao Li, Wen Hong*

- `1707.09875v1` - [abs](http://arxiv.org/abs/1707.09875v1) - [pdf](http://arxiv.org/pdf/1707.09875v1)

> The outstanding pattern recognition performance of deep learning brings new vitality to the synthetic aperture radar (SAR) automatic target recognition (ATR). However, there is a limitation in current deep learning based ATR solution that each learning process only handle one SAR image, namely learning the static scattering information, while missing the space-varying information. It is obvious that multi-aspect joint recognition introduced space-varying scattering information should improve the classification accuracy and robustness. In this paper, a novel multi-aspect-aware method is proposed to achieve this idea through the bidirectional Long Short-Term Memory (LSTM) recurrent neural networks based space-varying scattering information learning. Specifically, we first select different aspect images to generate the multi-aspect space-varying image sequences. Then, the Gabor filter and three-patch local binary pattern (TPLBP) are progressively implemented to extract a comprehensive spatial features, followed by dimensionality reduction with the Multi-layer Perceptron (MLP) network. Finally, we design a bidirectional LSTM recurrent neural network to learn the multi-aspect features with further integrating the softmax classifier to achieve target recognition. Experimental results demonstrate that the proposed method can achieve 99.9% accuracy for 10-class recognition. Besides, its anti-noise and anti-confusion performance are also better than the conventional deep learning based methods.

</details>

<details>

<summary>2017-07-25 14:35:49 - Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors</summary>

- *Mengyu Chu, Nils Thuerey*

- `1705.01425v2` - [abs](http://arxiv.org/abs/1705.01425v2) - [pdf](http://arxiv.org/pdf/1705.01425v2)

> We present a novel data-driven algorithm to synthesize high-resolution flow simulations with reusable repositories of space-time flow data. In our work, we employ a descriptor learning approach to encode the similarity between fluid regions with differences in resolution and numerical viscosity. We use convolutional neural networks to generate the descriptors from fluid data such as smoke density and flow velocity. At the same time, we present a deformation limiting patch advection method which allows us to robustly track deformable fluid regions. With the help of this patch advection, we generate stable space-time data sets from detailed fluids for our repositories. We can then use our learned descriptors to quickly localize a suitable data set when running a new simulation. This makes our approach very efficient, and resolution independent. We will demonstrate with several examples that our method yields volumes with very high effective resolutions, and non-dissipative small scale details that naturally integrate into the motions of the underlying flow.

</details>

<details>

<summary>2017-07-25 14:40:18 - Predicting Exploitation of Disclosed Software Vulnerabilities Using Open-source Data</summary>

- *Benjamin L. Bullough, Anna K. Yanchenko, Christopher L. Smith, Joseph R. Zipkin*

- `1707.08015v1` - [abs](http://arxiv.org/abs/1707.08015v1) - [pdf](http://arxiv.org/pdf/1707.08015v1)

> Each year, thousands of software vulnerabilities are discovered and reported to the public. Unpatched known vulnerabilities are a significant security risk. It is imperative that software vendors quickly provide patches once vulnerabilities are known and users quickly install those patches as soon as they are available. However, most vulnerabilities are never actually exploited. Since writing, testing, and installing software patches can involve considerable resources, it would be desirable to prioritize the remediation of vulnerabilities that are likely to be exploited. Several published research studies have reported moderate success in applying machine learning techniques to the task of predicting whether a vulnerability will be exploited. These approaches typically use features derived from vulnerability databases (such as the summary text describing the vulnerability) or social media posts that mention the vulnerability by name. However, these prior studies share multiple methodological shortcomings that inflate predictive power of these approaches. We replicate key portions of the prior work, compare their approaches, and show how selection of training and test data critically affect the estimated performance of predictive models. The results of this study point to important methodological considerations that should be taken into account so that results reflect real-world utility.

</details>


## 2017-08

<details>

<summary>2017-08-01 09:46:54 - Fast Preprocessing for Robust Face Sketch Synthesis</summary>

- *Yibing Song, Jiawei Zhang, Linchao Bao, Qingxiong Yang*

- `1708.00224v1` - [abs](http://arxiv.org/abs/1708.00224v1) - [pdf](http://arxiv.org/pdf/1708.00224v1)

> Exemplar-based face sketch synthesis methods usually meet the challenging problem that input photos are captured in different lighting conditions from training photos. The critical step causing the failure is the search of similar patch candidates for an input photo patch. Conventional illumination invariant patch distances are adopted rather than directly relying on pixel intensity difference, but they will fail when local contrast within a patch changes. In this paper, we propose a fast preprocessing method named Bidirectional Luminance Remapping (BLR), which interactively adjust the lighting of training and input photos. Our method can be directly integrated into state-of-the-art exemplar-based methods to improve their robustness with ignorable computational cost.

</details>

<details>

<summary>2017-08-03 10:38:51 - Using the SLEUTH urban growth model to simulate the impacts of future policy scenarios on urban land use in the Tehran metropolitan area in Iran</summary>

- *Shaghayegh Kargozar Nahavandya, Lalit Kumar, Pedram Ghamisi*

- `1708.01089v1` - [abs](http://arxiv.org/abs/1708.01089v1) - [pdf](http://arxiv.org/pdf/1708.01089v1)

> The SLEUTH model, based on the Cellular Automata (CA), can be applied to city development simulation in metropolitan areas. In this study the SLEUTH model was used to model the urban expansion and predict the future possible behavior of the urban growth in Tehran. The fundamental data were five Landsat TM and ETM images of 1988, 1992, 1998, 2001 and 2010. Three scenarios were designed to simulate the spatial pattern. The first scenario assumed historical urbanization mode would persist and the only limitations for development were height and slope. The second one was a compact scenario which makes the growth mostly internal and limited the expansion of suburban areas. The last scenario proposed a polycentric urban structure which let the little patches grow without any limitation and would not consider the areas beyond the specific buffer zone from the larger patches for development. Results showed that the urban growth rate was greater in the first scenario in comparison with the other two scenarios. Also it was shown that the third scenario was more suitable for Tehran since it could avoid undesirable effects such as congestion and pollution and was more in accordance with the conditions of Tehran city.

</details>

<details>

<summary>2017-08-21 13:10:39 - Learned Multi-Patch Similarity</summary>

- *Wilfried Hartmann, Silvano Galliani, Michal Havlena, Luc Van Gool, Konrad Schindler*

- `1703.08836v2` - [abs](http://arxiv.org/abs/1703.08836v2) - [pdf](http://arxiv.org/pdf/1703.08836v2)

> Estimating a depth map from multiple views of a scene is a fundamental task in computer vision. As soon as more than two viewpoints are available, one faces the very basic question how to measure similarity across >2 image patches. Surprisingly, no direct solution exists, instead it is common to fall back to more or less robust averaging of two-view similarities. Encouraged by the success of machine learning, and in particular convolutional neural networks, we propose to learn a matching function which directly maps multiple image patches to a scalar similarity score. Experiments on several multi-view datasets demonstrate that this approach has advantages over methods based on pairwise patch similarity.

</details>

<details>

<summary>2017-08-22 16:08:18 - Herding Vulnerable Cats: A Statistical Approach to Disentangle Joint Responsibility for Web Security in Shared Hosting</summary>

- *Samaneh Tajalizadehkhoob, Tom van Goethem, Maciej Korczyński, Arman Noroozian, Rainer Böhme, Tyler Moore, Wouter Joosen, Michel van Eeten*

- `1708.06693v1` - [abs](http://arxiv.org/abs/1708.06693v1) - [pdf](http://arxiv.org/pdf/1708.06693v1)

> Hosting providers play a key role in fighting web compromise, but their ability to prevent abuse is constrained by the security practices of their own customers. {\em Shared} hosting, offers a unique perspective since customers operate under restricted privileges and providers retain more control over configurations. We present the first empirical analysis of the distribution of web security features and software patching practices in shared hosting providers, the influence of providers on these security practices, and their impact on web compromise rates. We construct provider-level features on the global market for shared hosting -- containing 1,259 providers -- by gathering indicators from 442,684 domains. Exploratory factor analysis of 15 indicators identifies four main latent factors that capture security efforts: content security, webmaster security, web infrastructure security and web application security. We confirm, via a fixed-effect regression model, that providers exert significant influence over the latter two factors, which are both related to the software stack in their hosting environment. Finally, by means of GLM regression analysis of these factors on phishing and malware abuse, we show that the four security and software patching factors explain between 10\% and 19\% of the variance in abuse at providers, after controlling for size. For web-application security for instance, we found that when a provider moves from the bottom 10\% to the best-performing 10\%, it would experience 4 times fewer phishing incidents. We show that providers have influence over patch levels--even higher in the stack, where CMSes can run as client-side software--and that this influence is tied to a substantial reduction in abuse levels.

</details>

<details>

<summary>2017-08-28 16:51:15 - Viden: Attacker Identification on In-Vehicle Networks</summary>

- *Kyong-Tak Cho, Kang Shin*

- `1708.08414v1` - [abs](http://arxiv.org/abs/1708.08414v1) - [pdf](http://arxiv.org/pdf/1708.08414v1)

> Various defense schemes --- which determine the presence of an attack on the in-vehicle network --- have recently been proposed. However, they fail to identify which Electronic Control Unit (ECU) actually mounted the attack. Clearly, pinpointing the attacker ECU is essential for fast/efficient forensic, isolation, security patch, etc. To meet this need, we propose a novel scheme, called Viden (Voltage-based attacker identification), which can identify the attacker ECU by measuring and utilizing voltages on the in-vehicle network. The first phase of Viden, called ACK learning, determines whether or not the measured voltage signals really originate from the genuine message transmitter. Viden then exploits the voltage measurements to construct and update the transmitter ECUs' voltage profiles as their fingerprints. It finally uses the voltage profiles to identify the attacker ECU. Since Viden adapts its profiles to changes inside/outside of the vehicle, it can pinpoint the attacker ECU under various conditions. Moreover, its efficiency and design-compliance with modern in-vehicle network implementations make Viden practical and easily deployable. Our extensive experimental evaluations on both a CAN bus prototype and two real vehicles have shown that Viden can accurately fingerprint ECUs based solely on voltage measurements and thus identify the attacker ECU with a low false identification rate of 0.2%.

</details>

<details>

<summary>2017-08-29 16:59:19 - Visual Cues to Improve Myoelectric Control of Upper Limb Prostheses</summary>

- *Andrea Gigli, Arjan Gijsberts, Valentina Gregori, Matteo Cognolato, Manfredo Atzori, Barbara Caputo*

- `1709.02236v1` - [abs](http://arxiv.org/abs/1709.02236v1) - [pdf](http://arxiv.org/pdf/1709.02236v1)

> The instability of myoelectric signals over time complicates their use to control highly articulated prostheses. To address this problem, studies have tried to combine surface electromyography with modalities that are less affected by the amputation and environment, such as accelerometry or gaze information. In the latter case, the hypothesis is that a subject looks at the object he or she intends to manipulate and that knowing this object's affordances allows to constrain the set of possible grasps. In this paper, we develop an automated way to detect stable fixations and show that gaze information is indeed helpful in predicting hand movements. In our multimodal approach, we automatically detect stable gazes and segment an object of interest around the subject's fixation in the visual frame. The patch extracted around this object is subsequently fed through an off-the-shelf deep convolutional neural network to obtain a high level feature representation, which is then combined with traditional surface electromyography in the classification stage. Tests have been performed on a dataset acquired from five intact subjects who performed ten types of grasps on various objects as well as in a functional setting. They show that the addition of gaze information increases the classification accuracy considerably. Further analysis demonstrates that this improvement is consistent for all grasps and concentrated during the movement onset and offset.

</details>


## 2017-09

<details>

<summary>2017-09-05 18:51:03 - Did we learn from LLC Side Channel Attacks? A Cache Leakage Detection Tool for Crypto Libraries</summary>

- *Gorka Irazoqui, Kai Cong, Xiaofei Guo, Hareesh Khattri, Arun Kanuparthi, Thomas Eisenbarth, Berk Sunar*

- `1709.01552v1` - [abs](http://arxiv.org/abs/1709.01552v1) - [pdf](http://arxiv.org/pdf/1709.01552v1)

> This work presents a new tool to verify the correctness of cryptographic implementations with respect to cache attacks. Our methodology discovers vulnerabilities that are hard to find with other techniques, observed as exploitable leakage. The methodology works by identifying secret dependent memory and introducing forced evictions inside potentially vulnerable code to obtain cache traces that are analyzed using Mutual Information. If dependence is observed, the cryptographic implementation is classified as to leak information.   We demonstrate the viability of our technique in the design of the three main cryptographic primitives, i.e., AES, RSA and ECC, in eight popular up to date cryptographic libraries, including OpenSSL, Libgcrypt, Intel IPP and NSS. Our results show that cryptographic code designers are far away from incorporating the appropriate countermeasures to avoid cache leakages, as we found that 50% of the default implementations analyzed leaked information that lead to key extraction. We responsibly notified the designers of all the leakages found and suggested patches to solve these vulnerabilities.

</details>

<details>

<summary>2017-09-06 17:53:44 - An Efficient Method for Robust Projection Matrix Design</summary>

- *Tao Hong, Zhihui Zhu*

- `1609.08281v3` - [abs](http://arxiv.org/abs/1609.08281v3) - [pdf](http://arxiv.org/pdf/1609.08281v3)

> Our objective is to efficiently design a robust projection matrix $\Phi$ for the Compressive Sensing (CS) systems when applied to the signals that are not exactly sparse. The optimal projection matrix is obtained by mainly minimizing the average coherence of the equivalent dictionary. In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix. Without requiring of training data, we can efficiently design the robust projection matrix and apply it for most of CS systems, like a CS system for image processing with a conventional wavelet dictionary in which the SRE matrix is generally not available. Simulation results demonstrate the efficiency and effectiveness of the proposed approach compared with the state-of-the-art methods. In addition, we experimentally demonstrate with natural images that under similar compression rate, a CS system with a learned dictionary in high dimensions outperforms the one in low dimensions in terms of reconstruction accuracy. This together with the fact that our proposed method can efficiently work in high dimension suggests that a CS system can be potentially implemented beyond the small patches in sparsity-based image processing.

</details>

<details>

<summary>2017-09-14 05:59:50 - Do Developers Update Their Library Dependencies? An Empirical Study on the Impact of Security Advisories on Library Migration</summary>

- *Raula Gaikovina Kula, Daniel M. German, Ali Ouni, Takashi Ishio, Katsuro Inoue*

- `1709.04621v1` - [abs](http://arxiv.org/abs/1709.04621v1) - [pdf](http://arxiv.org/pdf/1709.04621v1)

> Third-party library reuse has become common practice in contemporary software development, as it includes several benefits for developers. Library dependencies are constantly evolving, with newly added features and patches that fix bugs in older versions. To take full advantage of third-party reuse, developers should always keep up to date with the latest versions of their library dependencies. In this paper, we investigate the extent of which developers update their library dependencies. Specifically, we conducted an empirical study on library migration that covers over 4,600 GitHub software projects and 2,700 library dependencies. Results show that although many of these systems rely heavily on dependencies, 81.5% of the studied systems still keep their outdated dependencies. In the case of updating a vulnerable dependency, the study reveals that affected developers are not likely to respond to a security advisory. Surveying these developers, we find that 69% of the interviewees claim that they were unaware of their vulnerable dependencies. Furthermore, developers are not likely to prioritize library updates, citing it as extra effort and added responsibility. This study concludes that even though third-party reuse is commonplace, the practice of updating a dependency is not as common for many developers.

</details>

<details>

<summary>2017-09-15 09:24:57 - Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning</summary>

- *Natalia Neverova, Christian Wolf, Florian Nebout, Graham Taylor*

- `1511.06728v4` - [abs](http://arxiv.org/abs/1511.06728v4) - [pdf](http://arxiv.org/pdf/1511.06728v4)

> We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth data is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi/weakly-supervised way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled ground truth (pixelwise segmentations); and (ii) a dataset with real images for which ground truth joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise restoration process, which aligns tentative segmentation maps with a large dictionary of synthetic poses. The underlying premise is that the domain shift between synthetic and real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw input domain. Experiments on the NYU dataset show that the proposed training method decreases error on joints over direct regression of joints from depth data by 15.7%.

</details>

<details>

<summary>2017-09-17 14:44:07 - Neural Affine Grayscale Image Denoising</summary>

- *Sungmin Cha, Taesup Moon*

- `1709.05672v1` - [abs](http://arxiv.org/abs/1709.05672v1) - [pdf](http://arxiv.org/pdf/1709.05672v1)

> We propose a new grayscale image denoiser, dubbed as Neural Affine Image Denoiser (Neural AIDE), which utilizes neural network in a novel way. Unlike other neural network based image denoising methods, which typically apply simple supervised learning to learn a mapping from a noisy patch to a clean patch, we formulate to train a neural network to learn an \emph{affine} mapping that gets applied to a noisy pixel, based on its context. Our formulation enables both supervised training of the network from the labeled training dataset and adaptive fine-tuning of the network parameters using the given noisy image subject to denoising. The key tool for devising Neural AIDE is to devise an estimated loss function of the MSE of the affine mapping, solely based on the noisy data. As a result, our algorithm can outperform most of the recent state-of-the-art methods in the standard benchmark datasets. Moreover, our fine-tuning method can nicely overcome one of the drawbacks of the patch-level supervised learning methods in image denoising; namely, a supervised trained model with a mismatched noise variance can be mostly corrected as long as we have the matched noise variance during the fine-tuning step.

</details>

<details>

<summary>2017-09-22 08:06:38 - BreathRNNet: Breathing Based Authentication on Resource-Constrained IoT Devices using RNNs</summary>

- *Jagmohan Chauhan, Suranga Seneviratne, Yining Hu, Archan Misra, Aruna Seneviratne, Youngki Lee*

- `1709.07626v1` - [abs](http://arxiv.org/abs/1709.07626v1) - [pdf](http://arxiv.org/pdf/1709.07626v1)

> Recurrent neural networks (RNNs) have shown promising results in audio and speech processing applications due to their strong capabilities in modelling sequential data. In many applications, RNNs tend to outperform conventional models based on GMM/UBMs and i-vectors. Increasing popularity of IoT devices makes a strong case for implementing RNN based inferences for applications such as acoustics based authentication, voice commands, and edge analytics for smart homes. Nonetheless, the feasibility and performance of RNN based inferences on resources-constrained IoT devices remain largely unexplored. In this paper, we investigate the feasibility of using RNNs for an end-to-end authentication system based on breathing acoustics. We evaluate the performance of RNN models on three types of devices; smartphone, smartwatch, and Raspberry Pi and show that unlike CNN models, RNN models can be easily ported onto resource-constrained devices without a significant loss in accuracy.

</details>

<details>

<summary>2017-09-26 06:14:49 - Is It Safe to Uplift This Patch? An Empirical Study on Mozilla Firefox</summary>

- *Marco Castelluccio, Le An, Foutse Khomh*

- `1709.08852v1` - [abs](http://arxiv.org/abs/1709.08852v1) - [pdf](http://arxiv.org/pdf/1709.08852v1)

> In rapid release development processes, patches that fix critical issues, or implement high-value features are often promoted directly from the development channel to a stabilization channel, potentially skipping one or more stabilization channels. This practice is called patch uplift. Patch uplift is risky, because patches that are rushed through the stabilization phase can end up introducing regressions in the code. This paper examines patch uplift operations at Mozilla, with the aim to identify the characteristics of uplifted patches that introduce regressions. Through statistical and manual analyses, we quantitatively and qualitatively investigate the reasons behind patch uplift decisions and the characteristics of uplifted patches that introduced regressions. Additionally, we interviewed three Mozilla release managers to understand organizational factors that affect patch uplift decisions and outcomes. Results show that most patches are uplifted because of a wrong functionality or a crash. Uplifted patches that lead to faults tend to have larger patch size, and most of the faults are due to semantic or memory errors in the patches. Also, release managers are more inclined to accept patch uplift requests that concern certain specific components, and-or that are submitted by certain specific developers.

</details>


## 2017-10

<details>

<summary>2017-10-14 18:17:30 - Mental Sampling in Multimodal Representations</summary>

- *Jian-Qiao Zhu, Adam N. Sanborn, Nick Chater*

- `1710.05219v1` - [abs](http://arxiv.org/abs/1710.05219v1) - [pdf](http://arxiv.org/pdf/1710.05219v1)

> Both resources in the natural environment and concepts in a semantic space are distributed "patchily", with large gaps in between the patches. To describe people's internal and external foraging behavior, various random walk models have been proposed. In particular, internal foraging has been modeled as sampling: in order to gather relevant information for making a decision, people draw samples from a mental representation using random-walk algorithms such as Markov chain Monte Carlo (MCMC). However, two common empirical observations argue against simple sampling algorithms such as MCMC. First, the spatial structure is often best described by a L\'evy flight distribution: the probability of the distance between two successive locations follows a power-law on the distances. Second, the temporal structure of the sampling that humans and other animals produce have long-range, slowly decaying serial correlations characterized as $1/f$-like fluctuations. We propose that mental sampling is not done by simple MCMC, but is instead adapted to multimodal representations and is implemented by Metropolis-coupled Markov chain Monte Carlo (MC$^3$), one of the first algorithms developed for sampling from multimodal distributions. MC$^3$ involves running multiple Markov chains in parallel but with target distributions of different temperatures, and it swaps the states of the chains whenever a better location is found. Heated chains more readily traverse valleys in the probability landscape to propose moves to far-away peaks, while the colder chains make the local steps that explore the current peak or patch. We show that MC$^3$ generates distances between successive samples that follow a L\'evy flight distribution and $1/f$-like serial correlations, providing a single mechanistic account of these two puzzling empirical phenomena.

</details>

<details>

<summary>2017-10-17 09:27:39 - MAG: A Multilingual, Knowledge-base Agnostic and Deterministic Entity Linking Approach</summary>

- *Diego Moussallem, Ricardo Usbeck, Michael Röder, Axel-Cyrille Ngonga Ngomo*

- `1707.05288v3` - [abs](http://arxiv.org/abs/1707.05288v3) - [pdf](http://arxiv.org/pdf/1707.05288v3)

> Entity linking has recently been the subject of a significant body of research. Currently, the best performing approaches rely on trained mono-lingual models. Porting these approaches to other languages is consequently a difficult endeavor as it requires corresponding training data and retraining of the models. We address this drawback by presenting a novel multilingual, knowledge-based agnostic and deterministic approach to entity linking, dubbed MAG. MAG is based on a combination of context-based retrieval on structured knowledge bases and graph algorithms. We evaluate MAG on 23 data sets and in 7 languages. Our results show that the best approach trained on English datasets (PBOH) achieves a micro F-measure that is up to 4 times worse on datasets in other languages. MAG, on the other hand, achieves state-of-the-art performance on English datasets and reaches a micro F-measure that is up to 0.6 higher than that of PBOH on non-English languages.

</details>

<details>

<summary>2017-10-17 15:17:59 - A Convex Similarity Index for Sparse Recovery of Missing Image Samples</summary>

- *Amirhossein Javaheri, Hadi Zayyani, Farokh Marvasti*

- `1701.07422v3` - [abs](http://arxiv.org/abs/1701.07422v3) - [pdf](http://arxiv.org/pdf/1701.07422v3)

> This paper investigates the problem of recovering missing samples using methods based on sparse representation adapted especially for image signals. Instead of $l_2$-norm or Mean Square Error (MSE), a new perceptual quality measure is used as the similarity criterion between the original and the reconstructed images. The proposed criterion called Convex SIMilarity (CSIM) index is a modified version of the Structural SIMilarity (SSIM) index, which despite its predecessor, is convex and uni-modal. We derive mathematical properties for the proposed index and show how to optimally choose the parameters of the proposed criterion, investigating the Restricted Isometry (RIP) and error-sensitivity properties. We also propose an iterative sparse recovery method based on a constrained $l_1$-norm minimization problem, incorporating CSIM as the fidelity criterion. The resulting convex optimization problem is solved via an algorithm based on Alternating Direction Method of Multipliers (ADMM). Taking advantage of the convexity of the CSIM index, we also prove the convergence of the algorithm to the globally optimal solution of the proposed optimization problem, starting from any arbitrary point. Simulation results confirm the performance of the new similarity index as well as the proposed algorithm for missing sample recovery of image patch signals.

</details>

<details>

<summary>2017-10-23 13:01:05 - Generic 3D Representation via Pose Estimation and Matching</summary>

- *Amir R. Zamir, Tilman Wekel, Pulkit Argrawal, Colin Weil, Jitendra Malik, Silvio Savarese*

- `1710.08247v1` - [abs](http://arxiv.org/abs/1710.08247v1) - [pdf](http://arxiv.org/pdf/1710.08247v1)

> Though a large body of computer vision research has investigated developing generic semantic representations, efforts towards developing a similar representation for 3D has been limited. In this paper, we learn a generic 3D representation through solving a set of foundational proxy 3D tasks: object-centric camera pose estimation and wide baseline feature matching. Our method is based upon the premise that by providing supervision over a set of carefully selected foundational tasks, generalization to novel tasks and abstraction capabilities can be achieved. We empirically show that the internal representation of a multi-task ConvNet trained to solve the above core problems generalizes to novel 3D tasks (e.g., scene layout estimation, object pose estimation, surface normal estimation) without the need for fine-tuning and shows traits of abstraction abilities (e.g., cross-modality pose estimation). In the context of the core supervised tasks, we demonstrate our representation achieves state-of-the-art wide baseline feature matching results without requiring apriori rectification (unlike SIFT and the majority of learned features). We also show 6DOF camera pose estimation given a pair local image patches. The accuracy of both supervised tasks come comparable to humans. Finally, we contribute a large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information, and conclude with a discussion on the learned representation and open research questions.

</details>

<details>

<summary>2017-10-23 22:01:39 - "Birds in the Clouds": Adventures in Data Engineering</summary>

- *N. Cherel, J. Reesman, A. Sahuguet, T. Auer, D. Fink*

- `1710.08521v1` - [abs](http://arxiv.org/abs/1710.08521v1) - [pdf](http://arxiv.org/pdf/1710.08521v1)

> Leveraging their eBird crowdsourcing project, the Cornell Lab of Ornithology generates sophisticated Spatio-Temporal Exploratory Model (STEM) maps of bird migrations. Such maps are highly relevant for both scientific and educational purposes, but creating them requires advanced modeling techniques that rely on long and potentially expensive computations.   In this paper, we share our experience porting the eBird STEM data pipeline from a physical cluster to the cloud, providing a seamless deployment at a lower cost. Using open source tools and cloud "marketplaces", we managed to divide the operating costs by a factor of 6, making it possible to scale our pipeline on a research budget.

</details>

<details>

<summary>2017-10-30 09:24:27 - Stochastic Subsampling for Factorizing Huge Matrices</summary>

- *Arthur Mensch, Julien Mairal, Bertrand Thirion, Gael Varoquaux*

- `1701.05363v3` - [abs](http://arxiv.org/abs/1701.05363v3) - [pdf](http://arxiv.org/pdf/1701.05363v3)

> We present a matrix-factorization algorithm that scales to input matrices with both huge number of rows and columns. Learned factors may be sparse or dense and/or non-negative, which makes our algorithm suitable for dictionary learning, sparse component analysis, and non-negative matrix factorization. Our algorithm streams matrix columns while subsampling them to iteratively learn the matrix factors. At each iteration, the row dimension of a new sample is reduced by subsampling, resulting in lower time complexity compared to a simple streaming algorithm. Our method comes with convergence guarantees to reach a stationary point of the matrix-factorization problem. We demonstrate its efficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on patches extracted from hyperspectral images (103 GB). For both problems, which involve different penalties on rows and columns, we obtain significant speed-ups compared to state-of-the-art algorithms.

</details>


## 2017-11

<details>

<summary>2017-11-02 16:10:14 - Talos: Neutralizing Vulnerabilities with Security Workarounds for Rapid Response</summary>

- *Zhen Huang, Mariana D'Angelo, Dhaval Miyani, David Lie*

- `1711.00795v1` - [abs](http://arxiv.org/abs/1711.00795v1) - [pdf](http://arxiv.org/pdf/1711.00795v1)

> Considerable delays often exist between the discovery of a vulnerability and the issue of a patch. One way to mitigate this window of vulnerability is to use a configuration workaround, which prevents the vulnerable code from being executed at the cost of some lost functionality -- but only if one is available. Since program configurations are not specifically designed to mitigate software vulnerabilities, we find that they only cover 25.2% of vulnerabilities.   To minimize patch delay vulnerabilities and address the limitations of configuration workarounds, we propose Security Workarounds for Rapid Response (SWRRs), which are designed to neutralize security vulnerabilities in a timely, secure, and unobtrusive manner. Similar to configuration workarounds, SWRRs neutralize vulnerabilities by preventing vulnerable code from being executed at the cost of some lost functionality. However, the key difference is that SWRRs use existing error-handling code within programs, which enables them to be mechanically inserted with minimal knowledge of the program and minimal developer effort. This allows SWRRs to achieve high coverage while still being fast and easy to deploy.   We have designed and implemented Talos, a system that mechanically instruments SWRRs into a given program, and evaluate it on five popular Linux server programs. We run exploits against 11 real-world software vulnerabilities and show that SWRRs neutralize the vulnerabilities in all cases. Quantitative measurements on 320 SWRRs indicate that SWRRs instrumented by Talos can neutralize 75.1% of all potential vulnerabilities and incur a loss of functionality similar to configuration workarounds in 71.3% of those cases. Our overall conclusion is that automatically generated SWRRs can safely mitigate 2.1x more vulnerabilities, while only incurring a loss of functionality comparable to that of traditional configuration workarounds.

</details>

<details>

<summary>2017-11-05 18:30:59 - Modeling flow in porous media with double porosity/permeability: A stabilized mixed formulation, error analysis, and numerical solutions</summary>

- *S. H. S. Joodat, K. B. Nakshatrala, R. Ballarini*

- `1705.08883v3` - [abs](http://arxiv.org/abs/1705.08883v3) - [pdf](http://arxiv.org/pdf/1705.08883v3)

> The flow of incompressible fluids through porous media plays a crucial role in many technological applications such as enhanced oil recovery and geological carbon-dioxide sequestration. The flow within numerous natural and synthetic porous materials that contain multiple scales of pores cannot be adequately described by the classical Darcy equations. It is for this reason that mathematical models for fluid flow in media with multiple scales of pores have been proposed in the literature. However, these models are analytically intractable for realistic problems. In this paper, a stabilized mixed four-field finite element formulation is presented to study the flow of an incompressible fluid in porous media exhibiting double porosity/permeability. The stabilization terms and the stabilization parameters are derived in a mathematically and thermodynamically consistent manner, and the computationally convenient equal-order interpolation of all the field variables is shown to be stable. A systematic error analysis is performed on the resulting stabilized weak formulation. Representative problems, patch tests and numerical convergence analyses are performed to illustrate the performance and convergence behavior of the proposed mixed formulation in the discrete setting. The accuracy of numerical solutions is assessed using the mathematical properties satisfied by the solutions of this double porosity/permeability model. Moreover, it is shown that the proposed framework can perform well under transient conditions and that it can capture well-known instabilities such as viscous fingering.

</details>

<details>

<summary>2017-11-06 18:58:44 - SAIC: Identifying Configuration Files for System Configuration Management</summary>

- *Zhen Huang, David Lie*

- `1711.03397v1` - [abs](http://arxiv.org/abs/1711.03397v1) - [pdf](http://arxiv.org/pdf/1711.03397v1)

> Systems can become misconfigured for a variety of reasons such as operator errors or buggy patches. When a misconfiguration is discovered, usually the first order of business is to restore availability, often by undoing the misconfiguration. To simplify this task, we propose the Statistical Analysis for Identifying Configuration Files (SAIC), which analyzes how the contents of a file changes over time to automatically determine which files contain configuration state. In this way, SAIC reduces the number of files a user must manually examine during recovery and allows versioning file systems to make more efficient use of their versioning storage.   The two key insights that enable SAIC to identify configuration files are that configuration state must persist across executions of an application and that configuration state changes at a slower rate than other types of application state. SAIC applies these insights through a set of filters, which eliminate non-persistent files from consideration, and a novel similarity metric, which measures how similar a file's versions are to each other. Together, these two mechanisms enable SAIC to identify all 72 configuration files out of 2363 versioned files from 6 common applications in two user traces, while mistaking only 33 non-configuration files as configuration files, which allows a versioning file system to eliminate roughly 66% of non-configuration file versions from its logs, thus reducing the number of file versions that a user must try to recover from a misconfiguration.

</details>

<details>

<summary>2017-11-12 15:45:34 - A machine learning approach for efficient uncertainty quantification using multiscale methods</summary>

- *Shing Chan, Ahmed H. Elsheikh*

- `1711.04315v1` - [abs](http://arxiv.org/abs/1711.04315v1) - [pdf](http://arxiv.org/pdf/1711.04315v1)

> Several multiscale methods account for sub-grid scale features using coarse scale basis functions. For example, in the Multiscale Finite Volume method the coarse scale basis functions are obtained by solving a set of local problems over dual-grid cells. We introduce a data-driven approach for the estimation of these coarse scale basis functions. Specifically, we employ a neural network predictor fitted using a set of solution samples from which it learns to generate subsequent basis functions at a lower computational cost than solving the local problems. The computational advantage of this approach is realized for uncertainty quantification tasks where a large number of realizations has to be evaluated. We attribute the ability to learn these basis functions to the modularity of the local problems and the redundancy of the permeability patches between samples. The proposed method is evaluated on elliptic problems yielding very promising results.

</details>

<details>

<summary>2017-11-13 20:43:02 - Bi-National Delay Pattern Analysis For Commercial and Passenger Vehicles at Niagara Frontier Border</summary>

- *Zhenhua Zhang, Lei Lin, Lei Zhu, Anuj Sharma*

- `1711.09723v1` - [abs](http://arxiv.org/abs/1711.09723v1) - [pdf](http://arxiv.org/pdf/1711.09723v1)

> Border crossing delays between New York State and Southern Ontario cause problems like enormous economic loss and massive environmental pollutions. In this area, there are three border-crossing ports: Peace Bridge (PB), Rainbow Bridge (RB) and Lewiston-Queenston Bridge (LQ) at Niagara Frontier border. The goals of this paper are to figure out whether the distributions of bi-national wait times for commercial and passenger vehicles are evenly distributed among the three ports and uncover the hidden significant influential factors that result in the possible insufficient utilization. The historical border wait time data from 7:00 to 21:00 between 08/22/2016 and 06/20/2017 are archived, as well as the corresponding temporal and weather data. For each vehicle type towards each direction, a Decision Tree is built to identify the various border delay patterns over the three bridges. We find that for the passenger vehicles to the USA, the convenient connections between the Canada freeways with USA I-190 by LQ and PB may cause these two bridges more congested than RB, especially when it is a holiday in Canada. For the passenger vehicles in the other bound, RB is much more congested than LQ and PB in some cases, and the visitors to Niagara Falls in the USA in summer may be a reason. For the commercial trucks to the USA, the various delay patterns show PB is always more congested than LQ. Hour interval and weekend are the most significant factors appearing in all the four Decision Trees. These Decision Trees can help the authorities to make specific routing suggestions when the corresponding conditions are satisfied.

</details>

<details>

<summary>2017-11-14 19:46:16 - Practical Whole-System Provenance Capture</summary>

- *Thomas Pasquier, Xueyuan Han, Mark Goldstein, Thomas Moyer, David Eyers, Margo Seltzer, Jean Bacon*

- `1711.05296v1` - [abs](http://arxiv.org/abs/1711.05296v1) - [pdf](http://arxiv.org/pdf/1711.05296v1)

> Data provenance describes how data came to be in its present form. It includes data sources and the transformations that have been applied to them. Data provenance has many uses, from forensics and security to aiding the reproducibility of scientific experiments. We present CamFlow, a whole-system provenance capture mechanism that integrates easily into a PaaS offering. While there have been several prior whole-system provenance systems that captured a comprehensive, systemic and ubiquitous record of a system's behavior, none have been widely adopted. They either A) impose too much overhead, B) are designed for long-outdated kernel releases and are hard to port to current systems, C) generate too much data, or D) are designed for a single system. CamFlow addresses these shortcoming by: 1) leveraging the latest kernel design advances to achieve efficiency; 2) using a self-contained, easily maintainable implementation relying on a Linux Security Module, NetFilter, and other existing kernel facilities; 3) providing a mechanism to tailor the captured provenance data to the needs of the application; and 4) making it easy to integrate provenance across distributed systems. The provenance we capture is streamed and consumed by tenant-built auditor applications. We illustrate the usability of our implementation by describing three such applications: demonstrating compliance with data regulations; performing fault/intrusion detection; and implementing data loss prevention. We also show how CamFlow can be leveraged to capture meaningful provenance without modifying existing applications.

</details>

<details>

<summary>2017-11-23 16:27:18 - DeepSign: Deep Learning for Automatic Malware Signature Generation and Classification</summary>

- *Eli David, Nathan S. Netanyahu*

- `1711.08336v2` - [abs](http://arxiv.org/abs/1711.08336v2) - [pdf](http://arxiv.org/pdf/1711.08336v2)

> This paper presents a novel deep learning based method for automatic malware signature generation and classification. The method uses a deep belief network (DBN), implemented with a deep stack of denoising autoencoders, generating an invariant compact representation of the malware behavior. While conventional signature and token based methods for malware detection do not detect a majority of new variants for existing malware, the results presented in this paper show that signatures generated by the DBN allow for an accurate classification of new malware variants. Using a dataset containing hundreds of variants for several major malware families, our method achieves 98.6% classification accuracy using the signatures generated by the DBN. The presented method is completely agnostic to the type of malware behavior that is logged (e.g., API calls and their parameters, registry entries, websites and ports accessed, etc.), and can use any raw input from a sandbox to successfully train the deep neural network which is used to generate malware signatures.

</details>


## 2017-12

<details>

<summary>2017-12-01 13:15:49 - An LLVM Instrumentation Plug-in for Score-P</summary>

- *Ronny Tschüter, Johannes Ziegenbalg, Bert Wesarg, Matthias Weber, Christian Herold, Sebastian Döbel, Ronny Brendel*

- `1712.01718v1` - [abs](http://arxiv.org/abs/1712.01718v1) - [pdf](http://arxiv.org/pdf/1712.01718v1)

> Reducing application runtime, scaling parallel applications to higher numbers of processes/threads, and porting applications to new hardware architectures are tasks necessary in the software development process. Therefore, developers have to investigate and understand application runtime behavior. Tools such as monitoring infrastructures that capture performance relevant data during application execution assist in this task. The measured data forms the basis for identifying bottlenecks and optimizing the code. Monitoring infrastructures need mechanisms to record application activities in order to conduct measurements. Automatic instrumentation of the source code is the preferred method in most application scenarios. We introduce a plug-in for the LLVM infrastructure that enables automatic source code instrumentation at compile-time. In contrast to available instrumentation mechanisms in LLVM/Clang, our plug-in can selectively include/exclude individual application functions. This enables developers to fine-tune the measurement to the required level of detail while avoiding large runtime overheads due to excessive instrumentation.

</details>

<details>

<summary>2017-12-01 22:58:12 - A new shell formulation for graphene structures based on existing ab-initio data</summary>

- *Reza Ghaffari, Thang X. Duong, Roger A. Sauer*

- `1612.08965v2` - [abs](http://arxiv.org/abs/1612.08965v2) - [pdf](http://arxiv.org/pdf/1612.08965v2)

> An existing hyperelastic membrane model for graphene calibrated from ab-initio data (Kumar and Parks, 2014) is adapted to curvilinear coordinates and extended to a rotation-free shell formulation based on isogeometric finite elements. Therefore, the membrane model is extended by a hyperelastic bending model that reflects the ab-inito data of Kudin et al. (2001). The proposed formulation can be implemented straight-forwardly into an existing finite element package, since it does not require the description of molecular interactions. It thus circumvents the use of interatomic potentials that tend to be less accurate than ab-initio data. The proposed shell formulation is verified and analyzed by a set of simple test cases. The results are in agreement to analytical solutions and satisfy the FE patch test. The performance of the shell formulation for graphene structures is illustrated by several numerical examples. The considered examples are indentation and peeling of graphene and torsion, bending and axial stretch of carbon nanotubes. Adhesive substrates are modeled by the Lennard-Jones potential and a coarse grained contact model. In principle, the proposed formulation can be extended to other 2D materials.

</details>

<details>

<summary>2017-12-03 06:02:23 - Spatial PixelCNN: Generating Images from Patches</summary>

- *Nader Akoury, Anh Nguyen*

- `1712.00714v1` - [abs](http://arxiv.org/abs/1712.00714v1) - [pdf](http://arxiv.org/pdf/1712.00714v1)

> In this paper we propose Spatial PixelCNN, a conditional autoregressive model that generates images from small patches. By conditioning on a grid of pixel coordinates and global features extracted from a Variational Autoencoder (VAE), we are able to train on patches of images, and reproduce the full-sized image. We show that it not only allows for generating high quality samples at the same resolution as the underlying dataset, but is also capable of upscaling images to arbitrary resolutions (tested at resolutions up to $50\times$) on the MNIST dataset. Compared to a PixelCNN++ baseline, Spatial PixelCNN quantitatively and qualitatively achieves similar performance on the MNIST dataset.

</details>

<details>

<summary>2017-12-05 11:29:27 - Deep Learning with Permutation-invariant Operator for Multi-instance Histopathology Classification</summary>

- *Jakub M. Tomczak, Maximilian Ilse, Max Welling*

- `1712.00310v2` - [abs](http://arxiv.org/abs/1712.00310v2) - [pdf](http://arxiv.org/pdf/1712.00310v2)

> The computer-aided analysis of medical scans is a longstanding goal in the medical imaging field. Currently, deep learning has became a dominant methodology for supporting pathologists and radiologist. Deep learning algorithms have been successfully applied to digital pathology and radiology, nevertheless, there are still practical issues that prevent these tools to be widely used in practice. The main obstacles are low number of available cases and large size of images (a.k.a. the small n, large p problem in machine learning), and a very limited access to annotation at a pixel level that can lead to severe overfitting and large computational requirements. We propose to handle these issues by introducing a framework that processes a medical image as a collection of small patches using a single, shared neural network. The final diagnosis is provided by combining scores of individual patches using a permutation-invariant operator (combination). In machine learning community such approach is called a multi-instance learning (MIL).

</details>

<details>

<summary>2017-12-14 15:56:46 - Constraint and Mathematical Programming Models for Integrated Port Container Terminal Operations</summary>

- *Damla Kizilay, Deniz T. Eliiyi, Pascal Van Hentenryck*

- `1712.05302v1` - [abs](http://arxiv.org/abs/1712.05302v1) - [pdf](http://arxiv.org/pdf/1712.05302v1)

> This paper considers the integrated problem of quay crane assignment, quay crane scheduling, yard location assignment, and vehicle dispatching operations at a container terminal. The main objective is to minimize vessel turnover times and maximize the terminal throughput, which are key economic drivers in terminal operations. Due to their computational complexities, these problems are not optimized jointly in existing work. This paper revisits this limitation and proposes Mixed Integer Programming (MIP) and Constraint Programming (CP) models for the integrated problem, under some realistic assumptions. Experimental results show that the MIP formulation can only solve small instances, while the CP model finds optimal solutions in reasonable times for realistic instances derived from actual container terminal operations.

</details>

<details>

<summary>2017-12-19 20:58:08 - Accelerating the computation of FLAPW methods on heterogeneous architectures</summary>

- *Davor Davidović, Diego Fabregat-Traver, Markus Höhnerbach, Edoardo di Napoli*

- `1712.07206v1` - [abs](http://arxiv.org/abs/1712.07206v1) - [pdf](http://arxiv.org/pdf/1712.07206v1)

> Legacy codes in computational science and engineering have been very successful in providing essential functionality to researchers. However, they are not capable of exploiting the massive parallelism provided by emerging heterogeneous architectures. The lack of portable performance and scalability puts them at high risk: either they evolve or they are doomed to disappear. One example of legacy code which would heavily benefit from a modern design is FLEUR, a software for electronic structure calculations. In previous work, the computational bottleneck of FLEUR was partially re-engineered to have a modular design that relies on standard building blocks, namely BLAS and LAPACK. In this paper, we demonstrate how the initial redesign enables the portability to heterogeneous architectures. More specifically, we study different approaches to port the code to architectures consisting of multi-core CPUs equipped with one or more coprocessors such as Nvidia GPUs and Intel Xeon Phis. Our final code attains over 70\% of the architectures' peak performance, and outperforms Nvidia's and Intel's libraries. Finally, on JURECA, the supercomputer where FLEUR is often executed, the code takes advantage of the full power of the computing nodes, attaining $5\times$ speedup over the sole use of the CPUs.

</details>

<details>

<summary>2017-12-21 06:06:07 - ARJA: Automated Repair of Java Programs via Multi-Objective Genetic Programming</summary>

- *Yuan Yuan, Wolfgang Banzhaf*

- `1712.07804v1` - [abs](http://arxiv.org/abs/1712.07804v1) - [pdf](http://arxiv.org/pdf/1712.07804v1)

> Recent empirical studies show that the performance of GenProg is not satisfactory, particularly for Java. In this paper, we propose ARJA, a new GP based repair approach for automated repair of Java programs. To be specific, we present a novel lower-granularity patch representation that properly decouples the search subspaces of likely-buggy locations, operation types and potential fix ingredients, enabling GP to explore the search space more effectively. Based on this new representation, we formulate automated program repair as a multi-objective search problem and use NSGA-II to look for simpler repairs. To reduce the computational effort and search space, we introduce a test filtering procedure that can speed up the fitness evaluation of GP and three types of rules that can be applied to avoid unnecessary manipulations of the code. Moreover, we also propose a type matching strategy that can create new potential fix ingredients by exploiting the syntactic patterns of the existing statements. We conduct a large-scale empirical evaluation of ARJA along with its variants on both seeded bugs and real-world bugs in comparison with several state-of-the-art repair approaches. Our results verify the effectiveness and efficiency of the search mechanisms employed in ARJA and also show its superiority over the other approaches. In particular, compared to jGenProg (an implementation of GenProg for Java), an ARJA version fully following the redundancy assumption can generate a test-suite adequate patch for more than twice the number of bugs (from 27 to 59), and a correct patch for nearly four times of the number (from 5 to 18), on 224 real-world bugs considered in Defects4J. Furthermore, ARJA is able to correctly fix several real multi-location bugs that are hard to be repaired by most of the existing repair approaches.

</details>

<details>

<summary>2017-12-22 15:32:29 - The Heisenberg Defense: Proactively Defending SGX Enclaves against Page-Table-Based Side-Channel Attacks</summary>

- *Raoul Strackx, Frank Piessens*

- `1712.08519v1` - [abs](http://arxiv.org/abs/1712.08519v1) - [pdf](http://arxiv.org/pdf/1712.08519v1)

> Protected-module architectures (PMAs) have been proposed to provide strong isolation guarantees, even on top of a compromised system. Unfortunately, Intel SGX -- the only publicly available high-end PMA -- has been shown to only provide limited isolation. An attacker controlling the untrusted page tables, can learn enclave secrets by observing its page access patterns.   Fortifying existing protected-module architectures in a real-world setting against side-channel attacks is an extremely difficult task as system software (hypervisor, operating system, ...) needs to remain in full control over the underlying hardware. Most state-of-the-art solutions propose a reactive defense that monitors for signs of an attack. Such approaches unfortunately cannot detect the most novel attacks, suffer from false-positives, and place an extraordinary heavy burden on enclave-developers when an attack is detected.   We present Heisenberg, a proactive defense that provides complete protection against page table based side channels. We guarantee that any attack will either be prevented or detected automatically before {\em any} sensitive information leaks. Consequently, Heisenberg can always securely resume enclave execution -- even when the attacker is still present in the system.   We present two implementations. Heisenberg-HW relies on very limited hardware features to defend against page-table-based attacks. We use the x86/SGX platform as an example, but the same approach can be applied when protected-module architectures are ported to different platforms as well.   Heisenberg-SW avoids these hardware modifications and can readily be applied. Unfortunately, it's reliance on Intel Transactional Synchronization Extensions (TSX) may lead to significant performance overhead under real-life conditions.

</details>

