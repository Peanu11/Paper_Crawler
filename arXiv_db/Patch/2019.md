# 2019

## TOC

- [2019-01](#2019-01)
- [2019-02](#2019-02)
- [2019-03](#2019-03)
- [2019-04](#2019-04)
- [2019-05](#2019-05)
- [2019-06](#2019-06)
- [2019-07](#2019-07)
- [2019-08](#2019-08)
- [2019-09](#2019-09)
- [2019-10](#2019-10)
- [2019-11](#2019-11)
- [2019-12](#2019-12)

## 2019-01

<details>

<summary>2019-01-01 00:15:45 - Improving Tree-LSTM with Tree Attention</summary>

- *Mahtab Ahmed, Muhammad Rifayat Samee, Robert E. Mercer*

- `1901.00066v1` - [abs](http://arxiv.org/abs/1901.00066v1) - [pdf](http://arxiv.org/pdf/1901.00066v1)

> In Natural Language Processing (NLP), we often need to extract information from tree topology. Sentence structure can be represented via a dependency tree or a constituency tree structure. For this reason, a variant of LSTMs, named Tree-LSTM, was proposed to work on tree topology. In this paper, we design a generalized attention framework for both dependency and constituency trees by encoding variants of decomposable attention inside a Tree-LSTM cell. We evaluated our models on a semantic relatedness task and achieved notable results compared to Tree-LSTM based methods with no attention as well as other neural and non-neural methods and good results compared to Tree-LSTM based methods with attention.

</details>

<details>

<summary>2019-01-01 08:43:56 - Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition</summary>

- *Xiaohang Zhan, Ziwei Liu, Junjie Yan, Dahua Lin, Chen Change Loy*

- `1809.01407v2` - [abs](http://arxiv.org/abs/1809.01407v2) - [pdf](http://arxiv.org/pdf/1809.01407v2)

> Face recognition has witnessed great progress in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environments and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationships by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two modules, the "committee" and the "mediator", which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling accuracy of 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all labels are employed.

</details>

<details>

<summary>2019-01-02 13:08:19 - Rate-Accuracy Trade-Off In Video Classification With Deep Convolutional Neural Networks</summary>

- *Mohammad Jubran, Alhabib Abbas, Aaron Chadha, Yiannis Andreopoulos*

- `1810.03964v2` - [abs](http://arxiv.org/abs/1810.03964v2) - [pdf](http://arxiv.org/pdf/1810.03964v2)

> Advanced video classification systems decode video frames to derive the necessary texture and motion representations for ingestion and analysis by spatio-temporal deep convolutional neural networks (CNNs). However, when considering visual Internet-of-Things applications, surveillance systems and semantic crawlers of large video repositories, the video capture and the CNN-based semantic analysis parts do not tend to be co-located. This necessitates the transport of compressed video over networks and incurs significant overhead in bandwidth and energy consumption, thereby significantly undermining the deployment potential of such systems. In this paper, we investigate the trade-off between the encoding bitrate and the achievable accuracy of CNN-based video classification models that directly ingest AVC/H.264 and HEVC encoded videos. Instead of retaining entire compressed video bitstreams and applying complex optical flow calculations prior to CNN processing, we only retain motion vector and select texture information at significantly-reduced bitrates and apply no additional processing prior to CNN ingestion. Based on three CNN architectures and two action recognition datasets, we achieve 11%-94% saving in bitrate with marginal effect on classification accuracy. A model-based selection between multiple CNNs increases these savings further, to the point where, if up to 7% loss of accuracy can be tolerated, video classification can take place with as little as 3 kbps for the transport of the required compressed video information to the system implementing the CNN models.

</details>

<details>

<summary>2019-01-02 16:41:09 - Game Semantics and Linear Logic in the Cognition Process</summary>

- *Dmitry Maximov*

- `1812.11969v2` - [abs](http://arxiv.org/abs/1812.11969v2) - [pdf](http://arxiv.org/pdf/1812.11969v2)

> A description of the environment cognition process by intelligent systems with a fixed set of system goals is suggested. Such a system is represented by the set of its goals only without any models of the system elements or the environment. The set has a lattice structure and a monoid structure; thus, the structure of linear logic is defined on the set. The cognition process of some environment by the system is described on this basis. The environment is represented as a configuration space of possible system positions which are estimated by an information amount (by corresponding sets). This information is supplied to the system by the environment. Thus, it is possible to define the category of Conway games with a payoff on the configuration space and to choose an optimal system's play (i.e., a trajectory). The choice is determined by the requirement of maximal information increasing and takes into account the structure of the system goal set: the linear logic on the set is used to determine the priority of possible different parallel processes. The survey may be useful to describe the behavior of robots and simple biological systems, e.g., ants.

</details>

<details>

<summary>2019-01-03 11:18:52 - Generating Multiple Objects at Spatially Distinct Locations</summary>

- *Tobias Hinz, Stefan Heinrich, Stefan Wermter*

- `1901.00686v1` - [abs](http://arxiv.org/abs/1901.00686v1) - [pdf](http://arxiv.org/pdf/1901.00686v1)

> Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. Furthermore, conditional GANs allow us to control the image generation process through labels or even natural language descriptions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. This is especially true for images that should contain multiple distinct objects at different spatial locations. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. We perform experiments on the Multi-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations. We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background.

</details>

<details>

<summary>2019-01-04 14:56:30 - Page Cache Attacks</summary>

- *Daniel Gruss, Erik Kraft, Trishita Tiwari, Michael Schwarz, Ari Trachtenberg, Jason Hennessey, Alex Ionescu, Anders Fogh*

- `1901.01161v1` - [abs](http://arxiv.org/abs/1901.01161v1) - [pdf](http://arxiv.org/pdf/1901.01161v1)

> We present a new hardware-agnostic side-channel attack that targets one of the most fundamental software caches in modern computer systems: the operating system page cache. The page cache is a pure software cache that contains all disk-backed pages, including program binaries, shared libraries, and other files, and our attacks thus work across cores and CPUs. Our side-channel permits unprivileged monitoring of some memory accesses of other processes, with a spatial resolution of 4KB and a temporal resolution of 2 microseconds on Linux (restricted to 6.7 measurements per second) and 466 nanoseconds on Windows (restricted to 223 measurements per second); this is roughly the same order of magnitude as the current state-of-the-art cache attacks. We systematically analyze our side channel by demonstrating different local attacks, including a sandbox bypassing high-speed covert channel, timed user-interface redressing attacks, and an attack recovering automatically generated temporary passwords. We further show that we can trade off the side channel's hardware agnostic property for remote exploitability. We demonstrate this via a low profile remote covert channel that uses this page-cache side-channel to exfiltrate information from a malicious sender process through innocuous server requests. Finally, we propose mitigations for some of our attacks, which have been acknowledged by operating system vendors and slated for future security patches.

</details>

<details>

<summary>2019-01-07 01:00:42 - Multi-Location Program Repair Strategies Learned from Past Successful Experience</summary>

- *Shangwen Wang, Xiaoguang Mao, Nan Niu, Xin Yi, Anbang Guo*

- `1810.12556v2` - [abs](http://arxiv.org/abs/1810.12556v2) - [pdf](http://arxiv.org/pdf/1810.12556v2)

> Automated program repair (APR) has great potential to reduce the effort and time-consumption in software maintenance and becomes a hot topic in software engineering recently with many approaches being proposed. Multi-location program repair has always been a challenge in this field since its complexity in logic and structure. While some approaches do not claim to have the features for solving multi-location bugs, they generate correct patches for these defects in practice. In this paper, we first make an observation on multi-location bugs in Defects4J and divide them into two categories (i.e., similar and relevant multi-location bugs) based on the repair actions in their patches. We then summarize the situation of multi-location bugs in Defects4J fixed by current tools. We analyze the twenty-two patches generated by current tools and propose two feasible strategies for fixing multi-location bugs, illustrating them through two detailed case studies. At last, the experimental results prove the feasibility of our methods with the repair of two bugs that have never been fixed before. By learning from successful experience in the past, this paper points out possible ways ahead for multi-location program repair.

</details>

<details>

<summary>2019-01-07 08:03:35 - Vector representations of text data in deep learning</summary>

- *Karol Grzegorczyk*

- `1901.01695v1` - [abs](http://arxiv.org/abs/1901.01695v1) - [pdf](http://arxiv.org/pdf/1901.01695v1)

> In this dissertation we report results of our research on dense distributed representations of text data. We propose two novel neural models for learning such representations. The first model learns representations at the document level, while the second model learns word-level representations.   For document-level representations we propose Binary Paragraph Vector: a neural network models for learning binary representations of text documents, which can be used for fast document retrieval. We provide a thorough evaluation of these models and demonstrate that they outperform the seminal method in the field in the information retrieval task. We also report strong results in transfer learning settings, where our models are trained on a generic text corpus and then used to infer codes for documents from a domain-specific dataset. In contrast to previously proposed approaches, Binary Paragraph Vector models learn embeddings directly from raw text data.   For word-level representations we propose Disambiguated Skip-gram: a neural network model for learning multi-sense word embeddings. Representations learned by this model can be used in downstream tasks, like part-of-speech tagging or identification of semantic relations. In the word sense induction task Disambiguated Skip-gram outperforms state-of-the-art models on three out of four benchmarks datasets. Our model has an elegant probabilistic interpretation. Furthermore, unlike previous models of this kind, it is differentiable with respect to all its parameters and can be trained with backpropagation. In addition to quantitative results, we present qualitative evaluation of Disambiguated Skip-gram, including two-dimensional visualisations of selected word-sense embeddings.

</details>

<details>

<summary>2019-01-07 13:54:49 - MicroWalk: A Framework for Finding Side Channels in Binaries</summary>

- *Jan Wichelmann, Ahmad Moghimi, Thomas Eisenbarth, Berk Sunar*

- `1808.05575v2` - [abs](http://arxiv.org/abs/1808.05575v2) - [pdf](http://arxiv.org/pdf/1808.05575v2)

> Microarchitectural side channels expose unprotected software to information leakage attacks where a software adversary is able to track runtime behavior of a benign process and steal secrets such as cryptographic keys. As suggested by incremental software patches for the RSA algorithm against variants of side-channel attacks within different versions of cryptographic libraries, protecting security-critical algorithms against side channels is an intricate task. Software protections avoid leakages by operating in constant time with a uniform resource usage pattern independent of the processed secret. In this respect, automated testing and verification of software binaries for leakage-free behavior is of importance, particularly when the source code is not available. In this work, we propose a novel technique based on Dynamic Binary Instrumentation and Mutual Information Analysis to efficiently locate and quantify memory based and control-flow based microarchitectural leakages. We develop a software framework named \tool~for side-channel analysis of binaries which can be extended to support new classes of leakage. For the first time, by utilizing \tool, we perform rigorous leakage analysis of two widely-used closed-source cryptographic libraries: \emph{Intel IPP} and \emph{Microsoft CNG}. We analyze $15$ different cryptographic implementations consisting of $112$ million instructions in about $105$ minutes of CPU time. By locating previously unknown leakages in hardened implementations, our results suggest that \tool~can efficiently find microarchitectural leakages in software binaries.

</details>

<details>

<summary>2019-01-07 19:56:19 - Spherical CNNs on Unstructured Grids</summary>

- *Chiyu "Max" Jiang, Jingwei Huang, Karthik Kashinath, Prabhat, Philip Marcus, Matthias Niessner*

- `1901.02039v1` - [abs](http://arxiv.org/abs/1901.02039v1) - [pdf](http://arxiv.org/pdf/1901.02039v1)

> We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly lower number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we present (1) a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and (2) we show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.

</details>

<details>

<summary>2019-01-08 07:37:27 - StaBL - State Based Language for Specification of Web Applications</summary>

- *Karthika Venkatesan, Sujit Kumar Chakrabarti*

- `1901.02188v1` - [abs](http://arxiv.org/abs/1901.02188v1) - [pdf](http://arxiv.org/pdf/1901.02188v1)

> Context and motivation: Usage of Formal Specification languages is scarce in web application development as compared to safety critical/hardware systems. Question/problem: An apt formal specification language should provide the following features: Firstly, it should have well-defined semantics, so that specifications written in it can not be inherently ambiguous. Secondly, it should have tool support for automatic detection of specification bugs. Additionally, for domains like web development, it is important that specification formalisms build over familiar notations, as the benefits of learning highly mathematical notations in such domains are perceived to be low. Principal ideas/results: This work presents a State Based Language inspired by Statecharts called StaBL for specification of web applications, and how StaBL can be used for writing such specifications. We also present modifications to the language w.r.t Statechart which facilitate writing modular and scalable specification. Contribution: In particular, we present the feature of locally scoped variables with inter-state data-flow. We summarise our experience of developing specifications with StaBL, which shows that StaBL specifications, on the one hand, are able to capture most essential elements of the functional aspects of a web application while foregoing much of the verbosity of a regular programming language.

</details>

<details>

<summary>2019-01-08 10:34:48 - Modeling Taxi Drivers' Behaviour for the Next Destination Prediction</summary>

- *Alberto Rossi, Gianni Barlacchi, Monica Bianchini, Bruno Lepri*

- `1807.08173v2` - [abs](http://arxiv.org/abs/1807.08173v2) - [pdf](http://arxiv.org/pdf/1807.08173v2)

> In this paper, we study how to model taxi drivers' behaviour and geographical information for an interesting and challenging task: the next destination prediction in a taxi journey. Predicting the next location is a well studied problem in human mobility, which finds several applications in real-world scenarios, from optimizing the efficiency of electronic dispatching systems to predicting and reducing the traffic jam. This task is normally modeled as a multiclass classification problem, where the goal is to select, among a set of already known locations, the next taxi destination. We present a Recurrent Neural Network (RNN) approach that models the taxi drivers' behaviour and encodes the semantics of visited locations by using geographical information from Location-Based Social Networks (LBSNs). In particular, RNNs are trained to predict the exact coordinates of the next destination, overcoming the problem of producing, in output, a limited set of locations, seen during the training phase. The proposed approach was tested on the ECML/PKDD Discovery Challenge 2015 dataset - based on the city of Porto -, obtaining better results with respect to the competition winner, whilst using less information, and on Manhattan and San Francisco datasets.

</details>

<details>

<summary>2019-01-08 11:06:42 - DEMN: Distilled-Exposition Enhanced Matching Network for Story Comprehension</summary>

- *Chunhua Liu, Haiou Zhang, Shan Jiang, Dong Yu*

- `1901.02252v1` - [abs](http://arxiv.org/abs/1901.02252v1) - [pdf](http://arxiv.org/pdf/1901.02252v1)

> This paper proposes a Distilled-Exposition Enhanced Matching Network (DEMN) for story-cloze test, which is still a challenging task in story comprehension. We divide a complete story into three narrative segments: an \textit{exposition}, a \textit{climax}, and an \textit{ending}. The model consists of three modules: input module, matching module, and distillation module. The input module provides semantic representations for the three segments and then feeds them into the other two modules. The matching module collects interaction features between the ending and the climax. The distillation module distills the crucial semantic information in the exposition and infuses it into the matching module in two different ways. We evaluate our single and ensemble model on ROCStories Corpus \cite{Mostafazadeh2016ACA}, achieving an accuracy of 80.1\% and 81.2\% on the test set respectively. The experimental results demonstrate that our DEMN model achieves a state-of-the-art performance.

</details>

<details>

<summary>2019-01-08 20:45:49 - Multi-stream CNN based Video Semantic Segmentation for Automated Driving</summary>

- *Ganesh Sistu, Sumanth Chennupati, Senthil Yogamani*

- `1901.02511v1` - [abs](http://arxiv.org/abs/1901.02511v1) - [pdf](http://arxiv.org/pdf/1901.02511v1)

> Majority of semantic segmentation algorithms operate on a single frame even in the case of videos. In this work, the goal is to exploit temporal information within the algorithm model for leveraging motion cues and temporal consistency. We propose two simple high-level architectures based on Recurrent FCN (RFCN) and Multi-Stream FCN (MSFCN) networks. In case of RFCN, a recurrent network namely LSTM is inserted between the encoder and decoder. MSFCN combines the encoders of different frames into a fused encoder via 1x1 channel-wise convolution. We use a ResNet50 network as the baseline encoder and construct three networks namely MSFCN of order 2 & 3 and RFCN of order 2. MSFCN-3 produces the best results with an accuracy improvement of 9% and 15% for Highway and New York-like city scenarios in the SYNTHIA-CVPR'16 dataset using mean IoU metric. MSFCN-3 also produced 11% and 6% for SegTrack V2 and DAVIS datasets over the baseline FCN network. We also designed an efficient version of MSFCN-2 and RFCN-2 using weight sharing among the two encoders. The efficient MSFCN-2 provided an improvement of 11% and 5% for KITTI and SYNTHIA with negligible increase in computational complexity compared to the baseline version.

</details>

<details>

<summary>2019-01-09 09:19:28 - What do Language Representations Really Represent?</summary>

- *Johannes Bjerva, Robert Östling, Maria Han Veiga, Jörg Tiedemann, Isabelle Augenstein*

- `1901.02646v1` - [abs](http://arxiv.org/abs/1901.02646v1) - [pdf](http://arxiv.org/pdf/1901.02646v1)

> A neural language model trained on a text corpus can be used to induce distributed representations of words, such that similar words end up with similar representations. If the corpus is multilingual, the same model can be used to learn distributed representations of languages, such that similar languages end up with similar representations. We show that this holds even when the multilingual corpus has been translated into English, by picking up the faint signal left by the source languages. However, just like it is a thorny problem to separate semantic from syntactic similarity in word representations, it is not obvious what type of similarity is captured by language representations. We investigate correlations and causal relationships between language representations learned from translations on one hand, and genetic, geographical, and several levels of structural similarity between languages on the other. Of these, structural similarity is found to correlate most strongly with language representation similarity, while genetic relationships---a convenient benchmark used for evaluation in previous work---appears to be a confounding factor. Apart from implications about translation effects, we see this more generally as a case where NLP and linguistic typology can interact and benefit one another.

</details>

<details>

<summary>2019-01-09 22:46:44 - A Compact Embedding for Facial Expression Similarity</summary>

- *Raviteja Vemulapalli, Aseem Agarwala*

- `1811.11283v2` - [abs](http://arxiv.org/abs/1811.11283v2) - [pdf](http://arxiv.org/pdf/1811.11283v2)

> Most of the existing work on automatic facial expression analysis focuses on discrete emotion recognition, or facial action unit detection. However, facial expressions do not always fall neatly into pre-defined semantic categories. Also, the similarity between expressions measured in the action unit space need not correspond to how humans perceive expression similarity. Different from previous work, our goal is to describe facial expressions in a continuous fashion using a compact embedding space that mimics human visual preferences. To achieve this goal, we collect a large-scale faces-in-the-wild dataset with human annotations in the form: Expressions A and B are visually more similar when compared to expression C, and use this dataset to train a neural network that produces a compact (16-dimensional) expression embedding. We experimentally demonstrate that the learned embedding can be successfully used for various applications such as expression retrieval, photo album summarization, and emotion recognition. We also show that the embedding learned using the proposed dataset performs better than several other embeddings learned using existing emotion or action unit datasets.

</details>

<details>

<summary>2019-01-10 02:23:18 - Sentence Rewriting for Semantic Parsing</summary>

- *Bo Chen, Le Sun, Xianpei Han, Bo An*

- `1901.02998v1` - [abs](http://arxiv.org/abs/1901.02998v1) - [pdf](http://arxiv.org/pdf/1901.02998v1)

> A major challenge of semantic parsing is the vocabulary mismatch problem between natural language and target ontology. In this paper, we propose a sentence rewriting based semantic parsing method, which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form. Specifically, we propose two sentence-rewriting methods for two common types of mismatch: a dictionary-based method for 1-N mismatch and a template-based method for N-1 mismatch. We evaluate our entence rewriting based semantic parser on the benchmark semantic parsing dataset -- WEBQUESTIONS. Experimental results show that our system outperforms the base system with a 3.4% gain in F1, and generates logical forms more accurately and parses sentences more robustly.

</details>

<details>

<summary>2019-01-10 02:32:26 - PFML-based Semantic BCI Agent for Game of Go Learning and Prediction</summary>

- *Chang-Shing Lee, Mei-Hui Wang, Li-Wei Ko, Bo-Yu Tsai, Yi-Lin Tsai, Sheng-Chi Yang, Lu-An Lin, Yi-Hsiu Lee, Hirofumi Ohashi, Naoyuki Kubota, Nan Shuo*

- `1901.02999v1` - [abs](http://arxiv.org/abs/1901.02999v1) - [pdf](http://arxiv.org/pdf/1901.02999v1)

> This paper presents a semantic brain computer interface (BCI) agent with particle swarm optimization (PSO) based on a Fuzzy Markup Language (FML) for Go learning and prediction applications. Additionally, we also establish an Open Go Darkforest (OGD) cloud platform with Facebook AI research (FAIR) open source Darkforest and ELF OpenGo AI bots. The Japanese robot Palro will simultaneously predict the move advantage in the board game Go to the Go players for reference or learning. The proposed semantic BCI agent operates efficiently by the human-based BCI data from their brain waves and machine-based game data from the prediction of the OGD cloud platform for optimizing the parameters between humans and machines. Experimental results show that the proposed human and smart machine co-learning mechanism performs favorably. We hope to provide students with a better online learning environment, combining different kinds of handheld devices, robots, or computer equipment, to achieve a desired and intellectual learning goal in the future.

</details>

<details>

<summary>2019-01-10 09:05:22 - Out-of-Distribution Detection using Multiple Semantic Label Representations</summary>

- *Gabi Shalev, Yossi Adi, Joseph Keshet*

- `1808.06664v3` - [abs](http://arxiv.org/abs/1808.06664v3) - [pdf](http://arxiv.org/pdf/1808.06664v3)

> Deep Neural Networks are powerful models that attained remarkable results on a variety of tasks. These models are shown to be extremely efficient when training and test data are drawn from the same distribution. However, it is not clear how a network will act when it is fed with an out-of-distribution example. In this work, we consider the problem of out-of-distribution detection in neural networks. We propose to use multiple semantic dense representations instead of sparse representation as the target label. Specifically, we propose to use several word representations obtained from different corpora or architectures as target labels. We evaluated the proposed model on computer vision, and speech commands detection tasks and compared it to previous methods. Results suggest that our method compares favorably with previous work. Besides, we present the efficiency of our approach for detecting wrongly classified and adversarial examples.

</details>

<details>

<summary>2019-01-10 16:16:33 - Attribute Evaluation on Attack Trees with Incomplete Information</summary>

- *Ahto Buldas, Olga Gadyatskaya, Aleksandr Lenin, Sjouke Mauw, Rolando Trujillo-Rasua*

- `1812.10754v2` - [abs](http://arxiv.org/abs/1812.10754v2) - [pdf](http://arxiv.org/pdf/1812.10754v2)

> Attack trees are considered a useful tool for security modelling because they support qualitative as well as quantitative analysis. The quantitative approach is based on values associated to each node in the tree, expressing, for instance, the minimal cost or probability of an attack. Current quantitative methods for attack trees allow the analyst to, based on an initial assignment of values to the leaf nodes, derive the values of the higher nodes in the tree. In practice, however, it shows to be very difficult to obtain reliable values for all leaf nodes. The main reasons are that data is only available for some of the nodes, that data is available for intermediate nodes rather than for the leaf nodes, or even that the available data is inconsistent. We address these problems by developing a generalisation of the standard bottom-up calculation method in three ways. First, we allow initial attributions of non-leaf nodes. Second, we admit additional relations between attack steps beyond those provided by the underlying attack tree semantics. Third, we support the calculation of an approximative solution in case of inconsistencies. We illustrate our method, which is based on constraint programming, by a comprehensive case study.

</details>

<details>

<summary>2019-01-11 02:28:38 - From Plots to Endings: A Reinforced Pointer Generator for Story Ending Generation</summary>

- *Yan Zhao, Lu Liu, Chunhua Liu, Ruoyao Yang, Dong Yu*

- `1901.03459v1` - [abs](http://arxiv.org/abs/1901.03459v1) - [pdf](http://arxiv.org/pdf/1901.03459v1)

> We introduce a new task named Story Ending Generation (SEG), whic-h aims at generating a coherent story ending from a sequence of story plot. Wepropose a framework consisting of a Generator and a Reward Manager for thistask. The Generator follows the pointer-generator network with coverage mech-anism to deal with out-of-vocabulary (OOV) and repetitive words. Moreover, amixed loss method is introduced to enable the Generator to produce story endingsof high semantic relevance with story plots. In the Reward Manager, the rewardis computed to fine-tune the Generator with policy-gradient reinforcement learn-ing (PGRL). We conduct experiments on the recently-introduced ROCStoriesCorpus. We evaluate our model in both automatic evaluation and human evalua-tion. Experimental results show that our model exceeds the sequence-to-sequencebaseline model by 15.75% and 13.57% in terms of CIDEr and consistency scorerespectively.

</details>

<details>

<summary>2019-01-11 09:17:22 - Hierarchy Neighborhood Discriminative Hashing for An Unified View of Single-Label and Multi-Label Image retrieval</summary>

- *Lei Ma, Hongliang Li, Qingbo Wu, Fanman Meng, King Ngi Ngan*

- `1901.03060v2` - [abs](http://arxiv.org/abs/1901.03060v2) - [pdf](http://arxiv.org/pdf/1901.03060v2)

> Recently, deep supervised hashing methods have become popular for large-scale image retrieval task. To preserve the semantic similarity notion between examples, they typically utilize the pairwise supervision or the triplet supervised information for hash learning. However, these methods usually ignore the semantic class information which can help the improvement of the semantic discriminative ability of hash codes. In this paper, we propose a novel hierarchy neighborhood discriminative hashing method. Specifically, we construct a bipartite graph to build coarse semantic neighbourhood relationship between the sub-class feature centers and the embeddings features. Moreover, we utilize the pairwise supervised information to construct the fined semantic neighbourhood relationship between embeddings features. Finally, we propose a hierarchy neighborhood discriminative hashing loss to unify the single-label and multilabel image retrieval problem with a one-stream deep neural network architecture. Experimental results on two largescale datasets demonstrate that the proposed method can outperform the state-of-the-art hashing methods.

</details>

<details>

<summary>2019-01-11 10:23:02 - Understanding Rowhammer Attacks through the Lens of a Unified Reference Framework</summary>

- *Xiaoxuan Lou, Fan Zhang, Zheng Leong Chua, Zhenkai Liang, Yueqiang Cheng, Yajin Zhou*

- `1901.03538v1` - [abs](http://arxiv.org/abs/1901.03538v1) - [pdf](http://arxiv.org/pdf/1901.03538v1)

> Rowhammer is a hardware-based bug that allows the attacker to modify the data in the memory without accessing it, just repeatedly and frequently accessing (or hammering) physically adjacent memory rows. So that it can break the memory isolation between processes, which is seen as the cornerstone of modern system security, exposing the sensitive data to unauthorized and imperceptible corruption. A number of previous works have leveraged the rowhammer bug to achieve various critical attacks.   In this work, we propose a unified reference framework for analyzing the rowhammer attacks, indicating three necessary factors in a practical rowhammer attack: the attack origin, the intended implication and the methodology. Each factor includes multiple primitives, the attacker can select primitives from three factors to constitute an effective attack. In particular, the methodology further summarizes all existing attack techniques, that are used to achieve its three primitives: Location Preparation (LP), Rapid Hammering (RH), and Exploit Verification (EV). Based on the reference framework, we analyze all previous rowhammer attacks and corresponding countermeasures. Our analysis shows that how primitives in different factors are combined and used in previous attacks, and thus points out new possibility of rowhammer attacks, enabling proactive prevention before it causes harm. Under the framework, we propose a novel expressive rowhammer attack that is capable of accumulating injected memory changes and achieving rich attack semantics. We conclude by outlining future research directions.

</details>

<details>

<summary>2019-01-11 10:52:54 - Feature Fusion for Robust Patch Matching With Compact Binary Descriptors</summary>

- *Andrea Migliorati, Attilio Fiandrotti, Gianluca Francini, Skjalg Lepsoy, Riccardo Leonardi*

- `1901.03547v1` - [abs](http://arxiv.org/abs/1901.03547v1) - [pdf](http://arxiv.org/pdf/1901.03547v1)

> This work addresses the problem of learning compact yet discriminative patch descriptors within a deep learning framework. We observe that features extracted by convolutional layers in the pixel domain are largely complementary to features extracted in a transformed domain. We propose a convolutional network framework for learning binary patch descriptors where pixel domain features are fused with features extracted from the transformed domain. In our framework, while convolutional and transformed features are distinctly extracted, they are fused and provided to a single classifier which thus jointly operates on convolutional and transformed features. We experiment at matching patches from three different datasets, showing that our feature fusion approach outperforms multiple state-of-the-art approaches in terms of accuracy, rate, and complexity.

</details>

<details>

<summary>2019-01-11 12:57:32 - Multi-Task Learning as Multi-Objective Optimization</summary>

- *Ozan Sener, Vladlen Koltun*

- `1810.04650v2` - [abs](http://arxiv.org/abs/1810.04650v2) - [pdf](http://arxiv.org/pdf/1810.04650v2)

> In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.

</details>

<details>

<summary>2019-01-11 16:42:43 - ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-constrained Neural Machine Translation</summary>

- *J. Edward Hu, Rachel Rudinger, Matt Post, Benjamin Van Durme*

- `1901.03644v1` - [abs](http://arxiv.org/abs/1901.03644v1) - [pdf](http://arxiv.org/pdf/1901.03644v1)

> We present ParaBank, a large-scale English paraphrase dataset that surpasses prior work in both quantity and quality. Following the approach of ParaNMT, we train a Czech-English neural machine translation (NMT) system to generate novel paraphrases of English reference sentences. By adding lexical constraints to the NMT decoding procedure, however, we are able to produce multiple high-quality sentential paraphrases per source sentence, yielding an English paraphrase resource with more than 4 billion generated tokens and exhibiting greater lexical diversity. Using human judgments, we also demonstrate that ParaBank's paraphrases improve over ParaNMT on both semantic similarity and fluency. Finally, we use ParaBank to train a monolingual NMT model with the same support for lexically-constrained decoding for sentence rewriting tasks.

</details>

<details>

<summary>2019-01-11 17:27:16 - Optimizing Answer Set Computation via Heuristic-Based Decomposition</summary>

- *Francesco Calimeri, Simona Perri, Jessica Zangari*

- `1812.09718v2` - [abs](http://arxiv.org/abs/1812.09718v2) - [pdf](http://arxiv.org/pdf/1812.09718v2)

> Answer Set Programming (ASP) is a purely declarative formalism developed in the field of logic programming and nonmonotonic reasoning: computational problems are encoded by logic programs whose answer sets, corresponding to solutions, are computed by an ASP system. Different, semantically equivalent, programs can be defined for the same problem; however, performance of systems evaluating them might significantly vary. We propose an approach for automatically transforming an input logic program into an equivalent one that can be evaluated more efficiently. One can make use of existing tree-decomposition techniques for rewriting selected rules into a set of multiple ones; the idea is to guide and adaptively apply them on the basis of proper new heuristics, to obtain a smart rewriting algorithm to be integrated into an ASP system. The method is rather general: it can be adapted to any system and implement different preference policies. Furthermore, we define a set of new heuristics tailored at optimizing grounding, one of the main phases of the ASP computation; we use them in order to implement the approach into the ASP system DLV, in particular into its grounding subsystem I-DLV, and carry out an extensive experimental activity for assessing the impact of the proposal. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2019-01-11 18:40:42 - Exploring Deep Spiking Neural Networks for Automated Driving Applications</summary>

- *Sambit Mohapatra, Heinrich Gotzig, Senthil Yogamani, Stefan Milz, Raoul Zollner*

- `1903.02080v1` - [abs](http://arxiv.org/abs/1903.02080v1) - [pdf](http://arxiv.org/pdf/1903.02080v1)

> Neural networks have become the standard model for various computer vision tasks in automated driving including semantic segmentation, moving object detection, depth estimation, visual odometry, etc. The main flavors of neural networks which are used commonly are convolutional (CNN) and recurrent (RNN). In spite of rapid progress in embedded processors, power consumption and cost is still a bottleneck. Spiking Neural Networks (SNNs) are gradually progressing to achieve low-power event-driven hardware architecture which has a potential for high efficiency. In this paper, we explore the role of deep spiking neural networks (SNN) for automated driving applications. We provide an overview of progress on SNN and argue how it can be a good fit for automated driving applications.

</details>

<details>

<summary>2019-01-11 19:10:46 - Optical Flow augmented Semantic Segmentation networks for Automated Driving</summary>

- *Hazem Rashed, Senthil Yogamani, Ahmad El-Sallab, Pavel Krizek, Mohamed El-Helw*

- `1901.07355v1` - [abs](http://arxiv.org/abs/1901.07355v1) - [pdf](http://arxiv.org/pdf/1901.07355v1)

> Motion is a dominant cue in automated driving systems. Optical flow is typically computed to detect moving objects and to estimate depth using triangulation. In this paper, our motivation is to leverage the existing dense optical flow to improve the performance of semantic segmentation. To provide a systematic study, we construct four different architectures which use RGB only, flow only, RGBF concatenated and two-stream RGB + flow. We evaluate these networks on two automotive datasets namely Virtual KITTI and Cityscapes using the state-of-the-art flow estimator FlowNet v2. We also make use of the ground truth optical flow in Virtual KITTI to serve as an ideal estimator and a standard Farneback optical flow algorithm to study the effect of noise. Using the flow ground truth in Virtual KITTI, two-stream architecture achieves the best results with an improvement of 4% IoU. As expected, there is a large improvement for moving objects like trucks, vans and cars with 38%, 28% and 6% increase in IoU. FlowNet produces an improvement of 2.4% in average IoU with larger improvement in the moving objects corresponding to 26%, 11% and 5% in trucks, vans and cars. In Cityscapes, flow augmentation provided an improvement for moving objects like motorcycle and train with an increase of 17% and 7% in IoU.

</details>

<details>

<summary>2019-01-12 02:45:32 - Automatic classification of geologic units in seismic images using partially interpreted examples</summary>

- *Bas Peters, Justin Granek, Eldad Haber*

- `1901.03786v1` - [abs](http://arxiv.org/abs/1901.03786v1) - [pdf](http://arxiv.org/pdf/1901.03786v1)

> Geologic interpretation of large seismic stacked or migrated seismic images can be a time-consuming task for seismic interpreters. Neural network based semantic segmentation provides fast and automatic interpretations, provided a sufficient number of example interpretations are available. Networks that map from image-to-image emerged recently as powerful tools for automatic segmentation, but standard implementations require fully interpreted examples. Generating training labels for large images manually is time consuming. We introduce a partial loss-function and labeling strategies such that networks can learn from partially interpreted seismic images. This strategy requires only a small number of annotated pixels per seismic image. Tests on seismic images and interpretation information from the Sea of Ireland show that we obtain high-quality predicted interpretations from a small number of large seismic images. The combination of a partial-loss function, a multi-resolution network that explicitly takes small and large-scale geological features into account, and new labeling strategies make neural networks a more practical tool for automatic seismic interpretation.

</details>

<details>

<summary>2019-01-12 22:25:06 - Real-time Joint Object Detection and Semantic Segmentation Network for Automated Driving</summary>

- *Ganesh Sistu, Isabelle Leang, Senthil Yogamani*

- `1901.03912v1` - [abs](http://arxiv.org/abs/1901.03912v1) - [pdf](http://arxiv.org/pdf/1901.03912v1)

> Convolutional Neural Networks (CNN) are successfully used for various visual perception tasks including bounding box object detection, semantic segmentation, optical flow, depth estimation and visual SLAM. Generally these tasks are independently explored and modeled. In this paper, we present a joint multi-task network design for learning object detection and semantic segmentation simultaneously. The main motivation is to achieve real-time performance on a low power embedded SOC by sharing of encoder for both the tasks. We construct an efficient architecture using a small ResNet10 like encoder which is shared for both decoders. Object detection uses YOLO v2 like decoder and semantic segmentation uses FCN8 like decoder. We evaluate the proposed network in two public datasets (KITTI, Cityscapes) and in our private fisheye camera dataset, and demonstrate that joint network provides the same accuracy as that of separate networks. We further optimize the network to achieve 30 fps for 1280x384 resolution image.

</details>

<details>

<summary>2019-01-13 02:03:10 - Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing</summary>

- *Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc Le, Ni Lao*

- `1807.02322v5` - [abs](http://arxiv.org/abs/1807.02322v5) - [pdf](http://arxiv.org/pdf/1807.02322v5)

> We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks. We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer. To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at https://github.com/crazydonkey200/neural-symbolic-machines

</details>

<details>

<summary>2019-01-13 15:46:24 - An Advanced Conceptual Diagnostic Healthcare Framework for Diabetes and Cardiovascular Disorders</summary>

- *M. Sharma, G. Singh, R. Singh*

- `1901.10530v1` - [abs](http://arxiv.org/abs/1901.10530v1) - [pdf](http://arxiv.org/pdf/1901.10530v1)

> The data mining along with emerging computing techniques have astonishingly influenced the healthcare industry. Researchers have used different Data Mining and Internet of Things (IoT) for enrooting a programmed solution for diabetes and heart patients. However, still, more advanced and united solution is needed that can offer a therapeutic opinion to individual diabetic and cardio patients. Therefore, here, a smart data mining and IoT (SMDIoT) based advanced healthcare system for proficient diabetes and cardiovascular diseases have been proposed. The hybridization of data mining and IoT with other emerging computing techniques is supposed to give an effective and economical solution to diabetes and cardio patients. SMDIoT hybridized the ideas of data mining, Internet of Things, chatbots, contextual entity search (CES), bio-sensors, semantic analysis and granular computing (GC). The bio-sensors of the proposed system assist in getting the current and precise status of the concerned patients so that in case of an emergency, the needful medical assistance can be provided. The novelty lies in the hybrid framework and the adequate support of chatbots, granular computing, context entity search and semantic analysis. The practical implementation of this system is very challenging and costly. However, it appears to be more operative and economical solution for diabetes and cardio patients.

</details>

<details>

<summary>2019-01-14 16:42:26 - Diffusion Maps for Textual Network Embedding</summary>

- *Xinyuan Zhang, Yitong Li, Dinghan Shen, Lawrence Carin*

- `1805.09906v2` - [abs](http://arxiv.org/abs/1805.09906v2) - [pdf](http://arxiv.org/pdf/1805.09906v2)

> Textual network embedding leverages rich text information associated with the network to learn low-dimensional vectorial representations of vertices. Rather than using typical natural language processing (NLP) approaches, recent research exploits the relationship of texts on the same edge to graphically embed text. However, these models neglect to measure the complete level of connectivity between any two texts in the graph. We present diffusion maps for textual network embedding (DMTE), integrating global structural information of the graph to capture the semantic relatedness between texts, with a diffusion-convolution operation applied on the text inputs. In addition, a new objective function is designed to efficiently preserve the high-order proximity using the graph diffusion. Experimental results show that the proposed approach outperforms state-of-the-art methods on the vertex-classification and link-prediction tasks.

</details>

<details>

<summary>2019-01-15 08:25:00 - DeepCF: A Unified Framework of Representation Learning and Matching Function Learning in Recommender System</summary>

- *Zhi-Hong Deng, Ling Huang, Chang-Dong Wang, Jian-Huang Lai, Philip S. Yu*

- `1901.04704v1` - [abs](http://arxiv.org/abs/1901.04704v1) - [pdf](http://arxiv.org/pdf/1901.04704v1)

> In general, recommendation can be viewed as a matching problem, i.e., match proper items for proper users. However, due to the huge semantic gap between users and items, it's almost impossible to directly match users and items in their initial representation spaces. To solve this problem, many methods have been studied, which can be generally categorized into two types, i.e., representation learning-based CF methods and matching function learning-based CF methods. Representation learning-based CF methods try to map users and items into a common representation space. In this case, the higher similarity between a user and an item in that space implies they match better. Matching function learning-based CF methods try to directly learn the complex matching function that maps user-item pairs to matching scores. Although both methods are well developed, they suffer from two fundamental flaws, i.e., the limited expressiveness of dot product and the weakness in capturing low-rank relations respectively. To this end, we propose a general framework named DeepCF, short for Deep Collaborative Filtering, to combine the strengths of the two types of methods and overcome such flaws. Extensive experiments on four publicly available datasets demonstrate the effectiveness of the proposed DeepCF framework.

</details>

<details>

<summary>2019-01-15 17:26:43 - Automatic Surface Area and Volume Prediction on Ellipsoidal Ham using Deep Learning</summary>

- *Y. S. Gan, Sze-Teng Liong, Yen-Chang Huang*

- `1901.04947v1` - [abs](http://arxiv.org/abs/1901.04947v1) - [pdf](http://arxiv.org/pdf/1901.04947v1)

> This paper presents novel methods to predict the surface and volume of the ham through a camera. This implies that the conventional weight measurement to obtain in the object's volume can be neglected and hence it is economically effective. Both of the measurements are obtained in the following two ways: manually and automatically. The former is assume as the true or exact measurement and the latter is through a computer vision technique with some geometrical analysis that includes mathematical derived functions. For the automatic implementation, most of the existing approaches extract the features of the food material based on handcrafted features and to the best of our knowledge this is the first attempt to estimate the surface area and volume on ham with deep learning features. We address the estimation task with a Mask Region-based CNN (Mask R-CNN) approach, which well performs the ham detection and semantic segmentation from a video. The experimental results demonstrate that the algorithm proposed is robust as promising surface area and volume estimation are obtained for two angles of the ellipsoidal ham (i.e., horizontal and vertical positions). Specifically, in the vertical ham point of view, it achieves an overall accuracy up to 95% whereas the horizontal ham reaches 80% of accuracy.

</details>

<details>

<summary>2019-01-16 03:16:28 - Memory Augmented Deep Generative models for Forecasting the Next Shot Location in Tennis</summary>

- *Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes*

- `1901.05123v1` - [abs](http://arxiv.org/abs/1901.05123v1) - [pdf](http://arxiv.org/pdf/1901.05123v1)

> This paper presents a novel framework for predicting shot location and type in tennis. Inspired by recent neuroscience discoveries we incorporate neural memory modules to model the episodic and semantic memory components of a tennis player. We propose a Semi Supervised Generative Adversarial Network architecture that couples these memory models with the automatic feature learning power of deep neural networks and demonstrate methodologies for learning player level behavioural patterns with the proposed framework. We evaluate the effectiveness of the proposed model on tennis tracking data from the 2012 Australian Tennis open and exhibit applications of the proposed method in discovering how players adapt their style depending on the match context.

</details>

<details>

<summary>2019-01-16 08:47:47 - Formal models of Structure Building in Music, Language and Animal Songs</summary>

- *Willem Zuidema, Dieuwke Hupkes, Geraint Wiggins, Constance Scharff, Martin Rohrmeier*

- `1901.05180v1` - [abs](http://arxiv.org/abs/1901.05180v1) - [pdf](http://arxiv.org/pdf/1901.05180v1)

> Human language, music and a variety of animal vocalisations constitute ways of sonic communication that exhibit remarkable structural complexity. While the complexities of language and possible parallels in animal communication have been discussed intensively, reflections on the complexity of music and animal song, and their comparisons are underrepresented. In some ways, music and animal songs are more comparable to each other than to language, as propositional semantics cannot be used as as indicator of communicative success or well-formedness, and notions of grammaticality are less easily defined. This review brings together accounts of the principles of structure building in language, music and animal song, relating them to the corresponding models in formal language theory, with a special focus on evaluating the benefits of using the Chomsky hierarchy (CH). We further discuss common misunderstandings and shortcomings concerning the CH, as well as extensions or augmentations of it that address some of these issues, and suggest ways to move beyond.

</details>

<details>

<summary>2019-01-16 10:02:04 - Towards Neural Network Patching: Evaluating Engagement-Layers and Patch-Architectures</summary>

- *Sebastian Kauschke, David Hermann Lehmann*

- `1812.03468v2` - [abs](http://arxiv.org/abs/1812.03468v2) - [pdf](http://arxiv.org/pdf/1812.03468v2)

> In this report we investigate fundamental requirements for the application of classifier patching on neural networks. Neural network patching is an approach for adapting neural network models to handle concept drift in nonstationary environments. Instead of creating or updating the existing network to accommodate concept drift, neural network patching leverages the inner layers of the network as well as its output to learn a patch that enhances the classification and corrects errors caused by the drift. It learns (i) a predictor that estimates whether the original network will misclassify an instance, and (ii) a patching network that fixes the misclassification. Neural network patching is based on the idea that the original network can still classify a majority of instances well, and that the inner feature representations encoded in the deep network aid the classifier to cope with unseen or changed inputs. In order to apply this kind of patching, we evaluate different engagement layers and patch architectures in this report, and find a set of generally applicable heuristics, which aid in parametrizing the patching procedure.

</details>

<details>

<summary>2019-01-16 10:40:18 - Sentence transition matrix: An efficient approach that preserves sentence semantics</summary>

- *Myeongjun Jang, Pilsung Kang*

- `1901.05219v1` - [abs](http://arxiv.org/abs/1901.05219v1) - [pdf](http://arxiv.org/pdf/1901.05219v1)

> Sentence embedding is a significant research topic in the field of natural language processing (NLP). Generating sentence embedding vectors reflecting the intrinsic meaning of a sentence is a key factor to achieve an enhanced performance in various NLP tasks such as sentence classification and document summarization. Therefore, various sentence embedding models based on supervised and unsupervised learning have been proposed after the advent of researches regarding the distributed representation of words. They were evaluated through semantic textual similarity (STS) tasks, which measure the degree of semantic preservation of a sentence and neural network-based supervised embedding models generally yielded state-of-the-art performance. However, these models have a limitation in that they have multiple parameters to update, thereby requiring a tremendous amount of labeled training data. In this study, we propose an efficient approach that learns a transition matrix that refines a sentence embedding vector to reflect the latent semantic meaning of a sentence. The proposed method has two practical advantages; (1) it can be applied to any sentence embedding method, and (2) it can achieve robust performance in STS tasks irrespective of the number of training examples.

</details>

<details>

<summary>2019-01-16 10:41:35 - Feature Maps: A Comprehensible Software Representation for Design Pattern Detection</summary>

- *Hannes Thaller, Lukas Linsbauer, Alexander Egyed*

- `1812.09873v2` - [abs](http://arxiv.org/abs/1812.09873v2) - [pdf](http://arxiv.org/pdf/1812.09873v2)

> Design patterns are elegant and well-tested solutions to recurrent software development problems. They are the result of software developers dealing with problems that frequently occur, solving them in the same or a slightly adapted way. A pattern's semantics provide the intent, motivation, and applicability, describing what it does, why it is needed, and where it is useful. Consequently, design patterns encode a well of information. Developers weave this information into their systems whenever they use design patterns to solve problems. This work presents Feature Maps, a flexible human- and machine-comprehensible software representation based on micro-structures. Our algorithm, the Feature-Role Normalization, presses the high-dimensional, inhomogeneous vector space of micro-structures into a feature map. We apply these concepts to the problem of detecting instances of design patterns in source code. We evaluate our methodology on four design patterns, a wide range of balanced and imbalanced labeled training data, and compare classical machine learning (Random Forests) with modern deep learning approaches (Convolutional Neural Networks). Feature maps yield robust classifiers even under challenging settings of strongly imbalanced data distributions without sacrificing human comprehensibility. Results suggest that feature maps are an excellent addition in the software analysis toolbox that can reveal useful information hidden in the source code.

</details>

<details>

<summary>2019-01-16 13:48:53 - Dependency or Span, End-to-End Uniform Semantic Role Labeling</summary>

- *Zuchao Li, Shexia He, Hai Zhao, Yiqing Zhang, Zhuosheng Zhang, Xi Zhou, Xiang Zhou*

- `1901.05280v1` - [abs](http://arxiv.org/abs/1901.05280v1) - [pdf](http://arxiv.org/pdf/1901.05280v1)

> Semantic role labeling (SRL) aims to discover the predicateargument structure of a sentence. End-to-end SRL without syntactic input has received great attention. However, most of them focus on either span-based or dependency-based semantic representation form and only show specific model optimization respectively. Meanwhile, handling these two SRL tasks uniformly was less successful. This paper presents an end-to-end model for both dependency and span SRL with a unified argument representation to deal with two different types of argument annotations in a uniform fashion. Furthermore, we jointly predict all predicates and arguments, especially including long-term ignored predicate identification subtask. Our single model achieves new state-of-the-art results on both span (CoNLL 2005, 2012) and dependency (CoNLL 2008, 2009) SRL benchmarks.

</details>

<details>

<summary>2019-01-16 16:56:14 - Location, Occupation, and Semantics based Socioeconomic Status Inference on Twitter</summary>

- *Jacobo Levy Abitbol, Márton Karsai, Eric Fleury*

- `1901.05389v1` - [abs](http://arxiv.org/abs/1901.05389v1) - [pdf](http://arxiv.org/pdf/1901.05389v1)

> The socioeconomic status of people depends on a combination of individual characteristics and environmental variables, thus its inference from online behavioral data is a difficult task. Attributes like user semantics in communication, habitat, occupation, or social network are all known to be determinant predictors of this feature. In this paper we propose three different data collection and combination methods to first estimate and, in turn, infer the socioeconomic status of French Twitter users from their online semantics. Our methods are based on open census data, crawled professional profiles, and remotely sensed, expert annotated information on living environment. Our inference models reach similar performance of earlier results with the advantage of relying on broadly available datasets and of providing a generalizable framework to estimate socioeconomic status of large numbers of Twitter users. These results may contribute to the scientific discussion on social stratification and inequalities, and may fuel several applications.

</details>

<details>

<summary>2019-01-17 12:25:47 - Multilevel Coupled Model Transformations for Precise and Reusable Definition of Model Behaviour</summary>

- *Fernando Macías, Uwe Wolter, Adrian Rutle, Francisco Durán, Roberto Rodriguez-Echeverria*

- `1901.05754v1` - [abs](http://arxiv.org/abs/1901.05754v1) - [pdf](http://arxiv.org/pdf/1901.05754v1)

> The use of Domain-Specific Languages (DSLs) is a promising field for the development of tools tailored to specific problem spaces, effectively diminishing the complexity of hand-made software. With the goal of making models as precise, simple and reusable as possible, we augment DSLs with concepts from multilevel modelling, where the number of abstraction levels are not limited. This is particularly useful for DSL definitions with behaviour, whose concepts inherently belong to different levels of abstraction. Here, models can represent the state of the modelled system and evolve using model transformations. These transformations can benefit from a multilevel setting, becoming a precise and reusable definition of the semantics for behavioural modelling languages. We present in this paper the concept of Multilevel Coupled Model Transformations, together with examples, formal definitions and tools to assess their conceptual soundness and practical value.

</details>

<details>

<summary>2019-01-17 14:32:06 - AuxNet: Auxiliary tasks enhanced Semantic Segmentation for Automated Driving</summary>

- *Sumanth Chennupati, Ganesh Sistu, Senthil Yogamani, Samir Rawashdeh*

- `1901.05808v1` - [abs](http://arxiv.org/abs/1901.05808v1) - [pdf](http://arxiv.org/pdf/1901.05808v1)

> Decision making in automated driving is highly specific to the environment and thus semantic segmentation plays a key role in recognizing the objects in the environment around the car. Pixel level classification once considered a challenging task which is now becoming mature to be productized in a car. However, semantic annotation is time consuming and quite expensive. Synthetic datasets with domain adaptation techniques have been used to alleviate the lack of large annotated datasets. In this work, we explore an alternate approach of leveraging the annotations of other tasks to improve semantic segmentation. Recently, multi-task learning became a popular paradigm in automated driving which demonstrates joint learning of multiple tasks improves overall performance of each tasks. Motivated by this, we use auxiliary tasks like depth estimation to improve the performance of semantic segmentation task. We propose adaptive task loss weighting techniques to address scale issues in multi-task loss functions which become more crucial in auxiliary tasks. We experimented on automotive datasets including SYNTHIA and KITTI and obtained 3% and 5% improvement in accuracy respectively.

</details>

<details>

<summary>2019-01-17 22:46:47 - Bears: An Extensible Java Bug Benchmark for Automatic Program Repair Studies</summary>

- *Fernanda Madeiral, Simon Urli, Marcelo Maia, Martin Monperrus*

- `1901.06024v1` - [abs](http://arxiv.org/abs/1901.06024v1) - [pdf](http://arxiv.org/pdf/1901.06024v1)

> Benchmarks of bugs are essential to empirically evaluate automatic program repair tools. In this paper, we present Bears, a project for collecting and storing bugs into an extensible bug benchmark for automatic repair studies in Java. The collection of bugs relies on commit building state from Continuous Integration (CI) to find potential pairs of buggy and patched program versions from open-source projects hosted on GitHub. Each pair of program versions passes through a pipeline where an attempt of reproducing a bug and its patch is performed. The core step of the reproduction pipeline is the execution of the test suite of the program on both program versions. If a test failure is found in the buggy program version candidate and no test failure is found in its patched program version candidate, a bug and its patch were successfully reproduced. The uniqueness of Bears is the usage of CI (builds) to identify buggy and patched program version candidates, which has been widely adopted in the last years in open-source projects. This approach allows us to collect bugs from a diversity of projects beyond mature projects that use bug tracking systems. Moreover, Bears was designed to be publicly available and to be easily extensible by the research community through automatic creation of branches with bugs in a given GitHub repository, which can be used for pull requests in the Bears repository. We present in this paper the approach employed by Bears, and we deliver the version 1.0 of Bears, which contains 251 reproducible bugs collected from 72 projects that use the Travis CI and Maven build environment.

</details>

<details>

<summary>2019-01-18 07:58:43 - Hierarchically Structured Reinforcement Learning for Topically Coherent Visual Story Generation</summary>

- *Qiuyuan Huang, Zhe Gan, Asli Celikyilmaz, Dapeng Wu, Jianfeng Wang, Xiaodong He*

- `1805.08191v3` - [abs](http://arxiv.org/abs/1805.08191v3) - [pdf](http://arxiv.org/pdf/1805.08191v3)

> We propose a hierarchically structured reinforcement learning approach to address the challenges of planning for generating coherent multi-sentence stories for the visual storytelling task. Within our framework, the task of generating a story given a sequence of images is divided across a two-level hierarchical decoder. The high-level decoder constructs a plan by generating a semantic concept (i.e., topic) for each image in sequence. The low-level decoder generates a sentence for each image using a semantic compositional network, which effectively grounds the sentence generation conditioned on the topic. The two decoders are jointly trained end-to-end using reinforcement learning. We evaluate our model on the visual storytelling (VIST) dataset. Empirical results from both automatic and human evaluations demonstrate that the proposed hierarchically structured reinforced training achieves significantly better performance compared to a strong flat deep reinforcement learning baseline.

</details>

<details>

<summary>2019-01-18 14:52:34 - Improving Sequence-to-Sequence Learning via Optimal Transport</summary>

- *Liqun Chen, Yizhe Zhang, Ruiyi Zhang, Chenyang Tao, Zhe Gan, Haichao Zhang, Bai Li, Dinghan Shen, Changyou Chen, Lawrence Carin*

- `1901.06283v1` - [abs](http://arxiv.org/abs/1901.06283v1) - [pdf](http://arxiv.org/pdf/1901.06283v1)

> Sequence-to-sequence models are commonly trained via maximum likelihood estimation (MLE). However, standard MLE training considers a word-level objective, predicting the next word given the previous ground-truth partial sentence. This procedure focuses on modeling local syntactic patterns, and may fail to capture long-range semantic structure. We present a novel solution to alleviate these issues. Our approach imposes global sequence-level guidance via new supervision based on optimal transport, enabling the overall characterization and preservation of semantic features. We further show that this method can be understood as a Wasserstein gradient flow trying to match our model to the ground truth sequence distribution. Extensive experiments are conducted to validate the utility of the proposed approach, showing consistent improvements over a wide variety of NLP tasks, including machine translation, abstractive text summarization, and image captioning.

</details>

<details>

<summary>2019-01-18 18:44:26 - Block Argumentation</summary>

- *Ryuta Arisaka, Stefano Bistarelli, Francesco Santini*

- `1901.06378v1` - [abs](http://arxiv.org/abs/1901.06378v1) - [pdf](http://arxiv.org/pdf/1901.06378v1)

> We contemplate a higher-level bipolar abstract argumentation for non-elementary arguments such as: X argues against Ys sincerity with the fact that Y has presented his argument to draw a conclusion C, by omitting other facts which would not have validated C. Argumentation involving such arguments requires us to potentially consider an argument as a coherent block of argumentation, i.e. an argument may itself be an argumentation. In this work, we formulate block argumentation as a specific instance of Dung-style bipolar abstract argumentation with the dual nature of arguments. We consider internal consistency of an argument(ation) under a set of constraints, of graphical (syntactic) and of semantic nature, and formulate acceptability semantics in relation to them. We discover that classical acceptability semantics do not in general hold good with the constraints. In particular, acceptability of unattacked arguments is not always warranted. Further, there may not be a unique minimal member in complete semantics, thus sceptic (grounded) semantics may not be its subset. To retain set-theoretically minimal semantics as a subset of complete semantics, we define semi-grounded semantics. Through comparisons, we show how the concept of block argumentation may further generalise structured argumentation.

</details>

<details>

<summary>2019-01-19 06:27:32 - Phonetic-and-Semantic Embedding of Spoken Words with Applications in Spoken Content Retrieval</summary>

- *Yi-Chen Chen, Sung-Feng Huang, Chia-Hao Shen, Hung-yi Lee, Lin-shan Lee*

- `1807.08089v4` - [abs](http://arxiv.org/abs/1807.08089v4) - [pdf](http://arxiv.org/pdf/1807.08089v4)

> Word embedding or Word2Vec has been successful in offering semantics for text words learned from the context of words. Audio Word2Vec was shown to offer phonetic structures for spoken words (signal segments for words) learned from signals within spoken words. This paper proposes a two-stage framework to perform phonetic-and-semantic embedding on spoken words considering the context of the spoken words. Stage 1 performs phonetic embedding with speaker characteristics disentangled. Stage 2 then performs semantic embedding in addition. We further propose to evaluate the phonetic-and-semantic nature of the audio embeddings obtained in Stage 2 by parallelizing with text embeddings. In general, phonetic structure and semantics inevitably disturb each other. For example the words "brother" and "sister" are close in semantics but very different in phonetic structure, while the words "brother" and "bother" are in the other way around. But phonetic-and-semantic embedding is attractive, as shown in the initial experiments on spoken document retrieval. Not only spoken documents including the spoken query can be retrieved based on the phonetic structures, but spoken documents semantically related to the query but not including the query can also be retrieved based on the semantics.

</details>

<details>

<summary>2019-01-19 12:49:56 - The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions</summary>

- *Alberto Garcia-Garcia, Pablo Martinez-Gonzalez, Sergiu Oprea, John Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez, Alvaro Jover-Alvarez*

- `1901.06514v1` - [abs](http://arxiv.org/abs/1901.06514v1) - [pdf](http://arxiv.org/pdf/1901.06514v1)

> Enter the RobotriX, an extremely photorealistic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems. The RobotriX consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth labels. By taking this approach, we were able to generate a dataset of 38 semantic classes totaling 8M stills recorded at +60 frames per second with full HD resolution. For each frame, RGB-D and 3D information is provided with full annotations in both spaces. Thanks to the high quality and quantity of both raw information and annotations, the RobotriX will serve as a new milestone for investigating 2D and 3D robotic vision tasks with large-scale data-driven techniques.

</details>

<details>

<summary>2019-01-19 19:51:31 - Design of Real-time Semantic Segmentation Decoder for Automated Driving</summary>

- *Arindam Das, Saranya Kandan, Senthil Yogamani, Pavel Krizek*

- `1901.06580v1` - [abs](http://arxiv.org/abs/1901.06580v1) - [pdf](http://arxiv.org/pdf/1901.06580v1)

> Semantic segmentation remains a computationally intensive algorithm for embedded deployment even with the rapid growth of computation power. Thus efficient network design is a critical aspect especially for applications like automated driving which requires real-time performance. Recently, there has been a lot of research on designing efficient encoders that are mostly task agnostic. Unlike image classification and bounding box object detection tasks, decoders are computationally expensive as well for semantic segmentation task. In this work, we focus on efficient design of the segmentation decoder and assume that an efficient encoder is already designed to provide shared features for a multi-task learning system. We design a novel efficient non-bottleneck layer and a family of decoders which fit into a small run-time budget using VGG10 as efficient encoder. We demonstrate in our dataset that experimentation with various design choices led to an improvement of 10\% from a baseline performance.

</details>

<details>

<summary>2019-01-20 18:46:21 - Visualizing Semantic Structures of Sequential Data by Learning Temporal Dependencies</summary>

- *Kyoung-Woon On, Eun-Sol Kim, Yu-Jung Heo, Byoung-Tak Zhang*

- `1901.09066v1` - [abs](http://arxiv.org/abs/1901.09066v1) - [pdf](http://arxiv.org/pdf/1901.09066v1)

> While conventional methods for sequential learning focus on interaction between consecutive inputs, we suggest a new method which captures composite semantic flows with variable-length dependencies. In addition, the semantic structures within given sequential data can be interpreted by visualizing temporal dependencies learned from the method. The proposed method, called Temporal Dependency Network (TDN), represents a video as a temporal graph whose node represents a frame of the video and whose edge represents the temporal dependency between two frames of a variable distance. The temporal dependency structure of semantic is discovered by learning parameterized kernels of graph convolutional methods. We evaluate the proposed method on the large-scale video dataset, Youtube-8M. By visualizing the temporal dependency structures as experimental results, we show that the suggested method can find the temporal dependency structures of video semantic.

</details>

<details>

<summary>2019-01-20 18:51:30 - A tensorized logic programming language for large-scale data</summary>

- *Ryosuke Kojima, Taisuke Sato*

- `1901.08548v1` - [abs](http://arxiv.org/abs/1901.08548v1) - [pdf](http://arxiv.org/pdf/1901.08548v1)

> We introduce a new logic programming language T-PRISM based on tensor embeddings. Our embedding scheme is a modification of the distribution semantics in PRISM, one of the state-of-the-art probabilistic logic programming languages, by replacing distribution functions with multidimensional arrays, i.e., tensors. T-PRISM consists of two parts: logic programming part and numerical computation part. The former provides flexible and interpretable modeling at the level of first order logic, and the latter part provides scalable computation utilizing parallelization and hardware acceleration with GPUs. Combing these two parts provides a remarkably wide range of high-level declarative modeling from symbolic reasoning to deep learning. To embody this programming language, we also introduce a new semantics, termed tensorized semantics, which combines the traditional least model semantics in logic programming with the embeddings of tensors. In T-PRISM, we first derive a set of equations related to tensors from a given program using logical inference, i.e., Prolog execution in a symbolic space and then solve the derived equations in a continuous space by TensorFlow. Using our preliminary implementation of T-PRISM, we have successfully dealt with a wide range of modeling. We have succeeded in dealing with real large-scale data in the declarative modeling. This paper presents a DistMult model for knowledge graphs using the FB15k and WN18 datasets.

</details>

<details>

<summary>2019-01-21 02:08:40 - VeriSolid: Correct-by-Design Smart Contracts for Ethereum</summary>

- *Anastasia Mavridou, Aron Laszka, Emmanouela Stachtiari, Abhishek Dubey*

- `1901.01292v2` - [abs](http://arxiv.org/abs/1901.01292v2) - [pdf](http://arxiv.org/pdf/1901.01292v2)

> The adoption of blockchain based distributed ledgers is growing fast due to their ability to provide reliability, integrity, and auditability without trusted entities. One of the key capabilities of these emerging platforms is the ability to create self-enforcing smart contracts. However, the development of smart contracts has proven to be error-prone in practice, and as a result, contracts deployed on public platforms are often riddled with security vulnerabilities. This issue is exacerbated by the design of these platforms, which forbids updating contract code and rolling back malicious transactions. In light of this, it is crucial to ensure that a smart contract is secure before deploying it and trusting it with significant amounts of cryptocurrency. To this end, we introduce the VeriSolid framework for the formal verification of contracts that are specified using a transition-system based model with rigorous operational semantics. Our model-based approach allows developers to reason about and verify contract behavior at a high level of abstraction. VeriSolid allows the generation of Solidity code from the verified models, which enables the correct-by-design development of smart contracts.

</details>

<details>

<summary>2019-01-22 00:29:27 - An Adversarial Approach to High-Quality, Sentiment-Controlled Neural Dialogue Generation</summary>

- *Xiang Kong, Bohan Li, Graham Neubig, Eduard Hovy, Yiming Yang*

- `1901.07129v1` - [abs](http://arxiv.org/abs/1901.07129v1) - [pdf](http://arxiv.org/pdf/1901.07129v1)

> In this work, we propose a method for neural dialogue response generation that allows not only generating semantically reasonable responses according to the dialogue history, but also explicitly controlling the sentiment of the response via sentiment labels. Our proposed model is based on the paradigm of conditional adversarial learning; the training of a sentiment-controlled dialogue generator is assisted by an adversarial discriminator which assesses the fluency and feasibility of the response generating from the dialogue history and a given sentiment label. Because of the flexibility of our framework, the generator could be a standard sequence-to-sequence (SEQ2SEQ) model or a more complicated one such as a conditional variational autoencoder-based SEQ2SEQ model. Experimental results using automatic and human evaluation both demonstrate that our proposed framework is able to generate both semantically reasonable and sentiment-controlled dialogue responses.

</details>

<details>

<summary>2019-01-22 05:31:54 - Enhancing Semantic Word Representations by Embedding Deeper Word Relationships</summary>

- *Anupiya Nugaliyadde, Kok Wai Wong, Ferdous Sohel, Hong Xie*

- `1901.07176v1` - [abs](http://arxiv.org/abs/1901.07176v1) - [pdf](http://arxiv.org/pdf/1901.07176v1)

> Word representations are created using analogy context-based statistics and lexical relations on words. Word representations are inputs for the learning models in Natural Language Understanding (NLU) tasks. However, to understand language, knowing only the context is not sufficient. Reading between the lines is a key component of NLU. Embedding deeper word relationships which are not represented in the context enhances the word representation. This paper presents a word embedding which combines an analogy, context-based statistics using Word2Vec, and deeper word relationships using Conceptnet, to create an expanded word representation. In order to fine-tune the word representation, Self-Organizing Map is used to optimize it. The proposed word representation is compared with semantic word representations using Simlex 999. Furthermore, the use of 3D visual representations has shown to be capable of representing the similarity and association between words. The proposed word representation shows a Spearman correlation score of 0.886 and provided the best results when compared to the current state-of-the-art methods, and exceed the human performance of 0.78.

</details>

<details>

<summary>2019-01-22 17:14:34 - Repair-Based Degrees of Database Inconsistency: Computation and Complexity</summary>

- *Leopoldo Bertossi*

- `1809.10286v3` - [abs](http://arxiv.org/abs/1809.10286v3) - [pdf](http://arxiv.org/pdf/1809.10286v3)

> We propose a generic numerical measure of the inconsistency of a database with respect to a set of integrity constraints. It is based on an abstract repair semantics. In particular, an inconsistency measure associated to cardinality-repairs is investigated in detail. More specifically, it is shown that it can be computed via answer-set programs, but sometimes its computation can be intractable in data complexity. However, polynomial-time deterministic and randomized approximations are exhibited. The behavior of this measure under small updates is analyzed, obtaining fixed-parameter tractability results. Furthermore, alternative inconsistency measures are proposed and discussed.

</details>

<details>

<summary>2019-01-22 17:17:02 - Debugging Frame Semantic Role Labeling</summary>

- *Alexandre Kabbach*

- `1901.07475v1` - [abs](http://arxiv.org/abs/1901.07475v1) - [pdf](http://arxiv.org/pdf/1901.07475v1)

> We propose a quantitative and qualitative analysis of the performances of statistical models for frame semantic structure extraction. We report on a replication study on FrameNet 1.7 data and show that preprocessing toolkits play a major role in argument identification performances, observing gains similar in their order of magnitude to those reported by recent models for frame semantic parsing. We report on the robustness of a recent statistical classifier for frame semantic parsing to lexical configurations of predicate-argument structures, relying on an artificially augmented dataset generated using a rule-based algorithm combining valence pattern matching and lexical substitution. We prove that syntactic pre-processing plays a major role in the performances of statistical classifiers to argument identification, and discuss the core reasons of syntactic mismatch between dependency parsers output and FrameNet syntactic formalism. Finally, we suggest new leads for improving statistical models for frame semantic parsing, including joint syntax-semantic parsing relying on FrameNet syntactic formalism, latent classes inference via split-and-merge algorithms and neural network architectures relying on rich input representations of words.

</details>

<details>

<summary>2019-01-22 18:55:34 - MONet: Unsupervised Scene Decomposition and Representation</summary>

- *Christopher P. Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, Alexander Lerchner*

- `1901.11390v1` - [abs](http://arxiv.org/abs/1901.11390v1) - [pdf](http://arxiv.org/pdf/1901.11390v1)

> The ability to decompose scenes in terms of abstract building blocks is crucial for general intelligence. Where those basic building blocks share meaningful properties, interactions and other regularities across scenes, such decompositions can simplify reasoning and facilitate imagination of novel scenarios. In particular, representing perceptual observations in terms of entities should improve data efficiency and transfer performance on a wide range of tasks. Thus we need models capable of discovering useful decompositions of scenes by identifying units with such regularities and representing them in a common format. To address this problem, we have developed the Multi-Object Network (MONet). In this model, a VAE is trained end-to-end together with a recurrent attention network -- in a purely unsupervised manner -- to provide attention masks around, and reconstructions of, regions of images. We show that this model is capable of learning to decompose and represent challenging 3D scenes into semantically meaningful components, such as objects and background elements.

</details>

<details>

<summary>2019-01-22 23:54:56 - Expansional Retrofitting for Word Vector Enrichment</summary>

- *Hwiyeol Jo*

- `1808.07337v3` - [abs](http://arxiv.org/abs/1808.07337v3) - [pdf](http://arxiv.org/pdf/1808.07337v3)

> Retrofitting techniques, which inject external resources into word representations, have compensated the weakness of distributed representations in semantic and relational knowledge between words. Implicitly retrofitting word vectors by expansional technique outperforms retrofitting in word similarity tasks with word vector generalization. In this paper, we propose unsupervised extrofitting: expansional retrofitting (extrofitting) without external semantic lexicons. We also propose deep extrofitting: in-depth stacking of extrofitting and further combinations of extrofitting with retrofitting. When experimenting with GloVe, we show that our methods outperform the previous methods on most of word similarity tasks while requiring only synonyms as an external resource. Lastly, we show the effect of word vector enrichment on text classification task, as a downstream task.

</details>

<details>

<summary>2019-01-23 00:25:48 - Attenuating Bias in Word Vectors</summary>

- *Sunipa Dev, Jeff Phillips*

- `1901.07656v1` - [abs](http://arxiv.org/abs/1901.07656v1) - [pdf](http://arxiv.org/pdf/1901.07656v1)

> Word vector representations are well developed tools for various NLP and Machine Learning tasks and are known to retain significant semantic and syntactic structure of languages. But they are prone to carrying and amplifying bias which can perpetrate discrimination in various applications. In this work, we explore new simple ways to detect the most stereotypically gendered words in an embedding and remove the bias from them. We verify how names are masked carriers of gender bias and then use that as a tool to attenuate bias in embeddings. Further, we extend this property of names to show how names can be used to detect other types of bias in the embeddings such as bias based on race, ethnicity, and age.

</details>

<details>

<summary>2019-01-23 00:52:39 - Keeping Context In Mind: Automating Mobile App Access Control with User Interface Inspection</summary>

- *Hao Fu, Zizhan Zheng, Sencun Zhu, Pransant Mohapatra*

- `1709.06654v2` - [abs](http://arxiv.org/abs/1709.06654v2) - [pdf](http://arxiv.org/pdf/1709.06654v2)

> Recent studies observe that app foreground is the most striking component that influences the access control decisions in mobile platform, as users tend to deny permission requests lacking visible evidence. However, none of the existing permission models provides a systematic approach that can automatically answer the question: Is the resource access indicated by app foreground? In this work, we present the design, implementation, and evaluation of COSMOS, a context-aware mediation system that bridges the semantic gap between foreground interaction and background access, in order to protect system integrity and user privacy. Specifically, COSMOS learns from a large set of apps with similar functionalities and user interfaces to construct generic models that detect the outliers at runtime. It can be further customized to satisfy specific user privacy preference by continuously evolving with user decisions. Experiments show that COSMOS achieves both high precision and high recall in detecting malicious requests. We also demonstrate the effectiveness of COSMOS in capturing specific user preferences using the decisions collected from 24 users and illustrate that COSMOS can be easily deployed on smartphones as a real-time guard with a very low performance overhead.

</details>

<details>

<summary>2019-01-23 07:28:54 - Astor: Exploring the Design Space of Generate-and-Validate Program Repair beyond GenProg</summary>

- *Matias Martinez, Martin Monperrus*

- `1802.03365v3` - [abs](http://arxiv.org/abs/1802.03365v3) - [pdf](http://arxiv.org/pdf/1802.03365v3)

> During last years, researches have proposed novel repair approaches that automatically generate patches for repairing software bugs. Repair approaches can be loosely characterized along the main design philosophy such generate- and-validate or synthesis-based. Each of those repair approaches is a point in the design space of program repair. Our goal is to facilitate the design, development and evaluation of repair approaches by providing a framework that: a) contains components commonly present in approaches implementations thus new approaches can be built over them, b) provides built-in implementations of existing repair approach. This paper presents a framework named Astor that encores the design space of generate-and-validate repair approaches. Astor provides extension points that form the explicit decision space of program repair. Over those extension points, researchers can reuse existing components or implements new ones. Astor includes 6 Java implementation of repair approaches, including one of the pioneer: GenProg. Researcher have been already defining new approaches over Astor, proposing improvements of those built-in approaches by using the extension points, and executing approaches implementations from Astor in their evaluations. The implementations of the repair approaches built over Astor are capable of repair, in total, 98 real bugs from 5 large Java programs.

</details>

<details>

<summary>2019-01-23 13:23:10 - Context based Analysis of Lexical Semantics for Hindi Language</summary>

- *Mohd Zeeshan Ansari, Lubna Khan*

- `1901.07867v1` - [abs](http://arxiv.org/abs/1901.07867v1) - [pdf](http://arxiv.org/pdf/1901.07867v1)

> A word having multiple senses in a text introduces the lexical semantic task to find out which particular sense is appropriate for the given context. One such task is Word sense disambiguation which refers to the identification of the most appropriate meaning of the polysemous word in a given context using computational algorithms. The language processing research in Hindi, the official language of India, and other Indian languages is restricted by unavailability of the standard corpus. For Hindi word sense disambiguation also, the large corpus is not available. In this work, we prepared the text containing new senses of certain words leading to the enrichment of the sense-tagged Hindi corpus of sixty polysemous words. Furthermore, we analyzed two novel lexical associations for Hindi word sense disambiguation based on the contextual features of the polysemous word. The evaluation of these methods is carried out over learning algorithms and favorable results are achieved.

</details>

<details>

<summary>2019-01-23 13:42:02 - "Is this an example image?" -- Predicting the Relative Abstractness Level of Image and Text</summary>

- *Christian Otto, Sebastian Holzki, Ralph Ewerth*

- `1901.07878v1` - [abs](http://arxiv.org/abs/1901.07878v1) - [pdf](http://arxiv.org/pdf/1901.07878v1)

> Successful multimodal search and retrieval requires the automatic understanding of semantic cross-modal relations, which, however, is still an open research problem. Previous work has suggested the metrics cross-modal mutual information and semantic correlation to model and predict cross-modal semantic relations of image and text. In this paper, we present an approach to predict the (cross-modal) relative abstractness level of a given image-text pair, that is whether the image is an abstraction of the text or vice versa. For this purpose, we introduce a new metric that captures this specific relationship between image and text at the Abstractness Level (ABS). We present a deep learning approach to predict this metric, which relies on an autoencoder architecture that allows us to significantly reduce the required amount of labeled training data. A comprehensive set of publicly available scientific documents has been gathered. Experimental results on a challenging test set demonstrate the feasibility of the approach.

</details>

<details>

<summary>2019-01-23 23:19:45 - Semantic Relation Classification via Bidirectional LSTM Networks with Entity-aware Attention using Latent Entity Typing</summary>

- *Joohong Lee, Sangwoo Seo, Yong Suk Choi*

- `1901.08163v1` - [abs](http://arxiv.org/abs/1901.08163v1) - [pdf](http://arxiv.org/pdf/1901.08163v1)

> Classifying semantic relations between entity pairs in sentences is an important task in Natural Language Processing (NLP). Most previous models for relation classification rely on the high-level lexical and syntactic features obtained by NLP tools such as WordNet, dependency parser, part-of-speech (POS) tagger, and named entity recognizers (NER). In addition, state-of-the-art neural models based on attention mechanisms do not fully utilize information of entity that may be the most crucial features for relation classification. To address these issues, we propose a novel end-to-end recurrent neural model which incorporates an entity-aware attention mechanism with a latent entity typing (LET) method. Our model not only utilizes entities and their latent types as features effectively but also is more interpretable by visualizing attention mechanisms applied to our model and results of LET. Experimental results on the SemEval-2010 Task 8, one of the most popular relation classification task, demonstrate that our model outperforms existing state-of-the-art models without any high-level features.

</details>

<details>

<summary>2019-01-24 07:14:07 - Generative Adversarial Network with Multi-Branch Discriminator for Cross-Species Image-to-Image Translation</summary>

- *Ziqiang Zheng, Zhibin Yu, Haiyong Zheng, Yang Wu, Bing Zheng, Ping Lin*

- `1901.10895v1` - [abs](http://arxiv.org/abs/1901.10895v1) - [pdf](http://arxiv.org/pdf/1901.10895v1)

> Current approaches have made great progress on image-to-image translation tasks benefiting from the success of image synthesis methods especially generative adversarial networks (GANs). However, existing methods are limited to handling translation tasks between two species while keeping the content matching on the semantic level. A more challenging task would be the translation among more than two species. To explore this new area, we propose a simple yet effective structure of a multi-branch discriminator for enhancing an arbitrary generative adversarial architecture (GAN), named GAN-MBD. It takes advantage of the boosting strategy to break a common discriminator into several smaller ones with fewer parameters, which can enhance the generation and synthesis abilities of GANs efficiently and effectively. Comprehensive experiments show that the proposed multi-branch discriminator can dramatically improve the performance of popular GANs on cross-species image-to-image translation tasks while reducing the number of parameters for computation. The code and some datasets are attached as supplementary materials for reference.

</details>

<details>

<summary>2019-01-24 13:20:25 - Application of Decision Rules for Handling Class Imbalance in Semantic Segmentation</summary>

- *Robin Chan, Matthias Rottmann, Fabian Hüger, Peter Schlicht, Hanno Gottschalk*

- `1901.08394v1` - [abs](http://arxiv.org/abs/1901.08394v1) - [pdf](http://arxiv.org/pdf/1901.08394v1)

> As part of autonomous car driving systems, semantic segmentation is an essential component to obtain a full understanding of the car's environment. One difficulty, that occurs while training neural networks for this purpose, is class imbalance of training data. Consequently, a neural network trained on unbalanced data in combination with maximum a-posteriori classification may easily ignore classes that are rare in terms of their frequency in the dataset. However, these classes are often of highest interest. We approach such potential misclassifications by weighting the posterior class probabilities with the prior class probabilities which in our case are the inverse frequencies of the corresponding classes in the training dataset. More precisely, we adopt a localized method by computing the priors pixel-wise such that the impact can be analyzed at pixel level as well. In our experiments, we train one network from scratch using a proprietary dataset containing 20,000 annotated frames of video sequences recorded from street scenes. The evaluation on our test set shows an increase of average recall with regard to instances of pedestrians and info signs by $25\%$ and $23.4\%$, respectively. In addition, we significantly reduce the non-detection rate for instances of the same classes by $61\%$ and $38\%$.

</details>

<details>

<summary>2019-01-24 15:31:11 - Semantic Classification of Tabular Datasets via Character-Level Convolutional Neural Networks</summary>

- *Paul Azunre, Craig Corcoran, Numa Dhamani, Jeffrey Gleason, Garrett Honke, David Sullivan, Rebecca Ruppel, Sandeep Verma, Jonathon Morgan*

- `1901.08456v1` - [abs](http://arxiv.org/abs/1901.08456v1) - [pdf](http://arxiv.org/pdf/1901.08456v1)

> A character-level convolutional neural network (CNN) motivated by applications in "automated machine learning" (AutoML) is proposed to semantically classify columns in tabular data. Simulated data containing a set of base classes is first used to learn an initial set of weights. Hand-labeled data from the CKAN repository is then used in a transfer-learning paradigm to adapt the initial weights to a more sophisticated representation of the problem (e.g., including more classes). In doing so, realistic data imperfections are learned and the set of classes handled can be expanded from the base set with reduced labeled data and computing power requirements. Results show the effectiveness and flexibility of this approach in three diverse domains: semantic classification of tabular data, age prediction from social media posts, and email spam classification. In addition to providing further evidence of the effectiveness of transfer learning in natural language processing (NLP), our experiments suggest that analyzing the semantic structure of language at the character level without additional metadata---i.e., network structure, headers, etc.---can produce competitive accuracy for type classification, spam classification, and social media age prediction. We present our open-source toolkit SIMON, an acronym for Semantic Inference for the Modeling of ONtologies, which implements this approach in a user-friendly and scalable/parallelizable fashion.

</details>

<details>

<summary>2019-01-25 06:19:21 - Teaching machines to understand data science code by semantic enrichment of dataflow graphs</summary>

- *Evan Patterson, Ioana Baldini, Aleksandra Mojsilovic, Kush R. Varshney*

- `1807.05691v2` - [abs](http://arxiv.org/abs/1807.05691v2) - [pdf](http://arxiv.org/pdf/1807.05691v2)

> Your computer is continuously executing programs, but does it really understand them? Not in any meaningful sense. That burden falls upon human knowledge workers, who are increasingly asked to write and understand code. They deserve to have intelligent tools that reveal the connections between code and its subject matter. Towards this prospect, we develop an AI system that forms semantic representations of computer programs, using techniques from knowledge representation and program analysis. To create the representations, we introduce an algorithm for enriching dataflow graphs with semantic information. The semantic enrichment algorithm is undergirded by a new ontology language for modeling computer programs and a new ontology about data science, written in this language. Throughout the paper, we focus on code written by data scientists and we locate our work within a larger movement towards collaborative, open, and reproducible science.

</details>

<details>

<summary>2019-01-25 15:04:02 - Comparing of Term Clustering Frameworks for Modular Ontology Learning</summary>

- *Ziwei Xu, Mounira Harzallah, Fabrice Guillet*

- `1901.09037v1` - [abs](http://arxiv.org/abs/1901.09037v1) - [pdf](http://arxiv.org/pdf/1901.09037v1)

> This paper aims to use term clustering to build a modular ontology according to core ontology from domain-specific text. The acquisition of semantic knowledge focuses on noun phrase appearing with the same syntactic roles in relation to a verb or its preposition combination in a sentence. The construction of this co-occurrence matrix from context helps to build feature space of noun phrases, which is then transformed to several encoding representations including feature selection and dimensionality reduction. In addition, the content has also been presented with the construction of word vectors. These representations are clustered respectively with K-Means and Affinity Propagation (AP) methods, which differentiate into the term clustering frameworks. Due to the randomness of K-Means, iteration efforts are adopted to find the optimal parameter. The frameworks are evaluated extensively where AP shows dominant effectiveness for co-occurred terms and NMF encoding technique is salient by its promising facilities in feature compression.

</details>

<details>

<summary>2019-01-25 16:39:37 - ToyBox: Better Atari Environments for Testing Reinforcement Learning Agents</summary>

- *John Foley, Emma Tosch, Kaleigh Clary, David Jensen*

- `1812.02850v3` - [abs](http://arxiv.org/abs/1812.02850v3) - [pdf](http://arxiv.org/pdf/1812.02850v3)

> It is a widely accepted principle that software without tests has bugs. Testing reinforcement learning agents is especially difficult because of the stochastic nature of both agents and environments, the complexity of state-of-the-art models, and the sequential nature of their predictions. Recently, the Arcade Learning Environment (ALE) has become one of the most widely used benchmark suites for deep learning research, and state-of-the-art Reinforcement Learning (RL) agents have been shown to routinely equal or exceed human performance on many ALE tasks. Since ALE is based on emulation of original Atari games, the environment does not provide semantically meaningful representations of internal game state. This means that ALE has limited utility as an environment for supporting testing or model introspection. We propose ToyBox, a collection of reimplementations of these games that solves this critical problem and enables robust testing of RL agents.

</details>

<details>

<summary>2019-01-26 00:16:08 - Keeping CALM: When Distributed Consistency is Easy</summary>

- *Joseph M. Hellerstein, Peter Alvaro*

- `1901.01930v2` - [abs](http://arxiv.org/abs/1901.01930v2) - [pdf](http://arxiv.org/pdf/1901.01930v2)

> A key concern in modern distributed systems is to avoid the cost of coordination while maintaining consistent semantics. Until recently, there was no answer to the question of when coordination is actually required. In this paper we present an informal introduction to the CALM Theorem, which answers this question precisely by moving up from traditional storage consistency to consider properties of programs.   CALM is an acronym for "consistency as logical monotonicity". The CALM Theorem shows that the programs that have consistent, coordination-free distributed implementations are exactly the programs that can be expressed in monotonic logic. This theoretical result has practical implications for developers of distributed applications. We show how CALM provides a constructive application-level counterpart to conventional "systems" wisdom, such as the apparently negative results of the CAP Theorem. We also discuss ways that monotonic thinking can influence distributed systems design, and how new programming language designs and tools can help developers write consistent, coordination-free code.

</details>

<details>

<summary>2019-01-26 00:24:22 - The informal semantics of Answer Set Programming: A Tarskian perspective</summary>

- *Marc Denecker, Yuliya Lierler, Miroslaw truszczynski, Joost Vennekens*

- `1901.09125v1` - [abs](http://arxiv.org/abs/1901.09125v1) - [pdf](http://arxiv.org/pdf/1901.09125v1)

> In Knowledge Representation, it is crucial that knowledge engineers have a good understanding of the formal expressions that they write. What formal expressions state intuitively about the domain of discourse is studied in the theory of the informal semantics of a logic. In this paper we study the informal semantics of Answer Set Programming. The roots of answer set programming lie in the language of Extended Logic Programming, which was introduced initially as an epistemic logic for default and autoepistemic reasoning. In 1999, the seminal papers on answer set programming proposed to use this logic for a different purpose, namely, to model and solve search problems. Currently, the language is used primarily in this new role. However, the original epistemic intuitions lose their explanatory relevance in this new context. How answer set programs are connected to the specifications of problems they model is more easily explained in a classical Tarskian semantics, in which models correspond to possible worlds, rather than to belief states of an epistemic agent. In this paper, we develop a new theory of the informal semantics of answer set programming, which is formulated in the Tarskian setting and based on Frege's compositionality principle. It differs substantially from the earlier epistemic theory of informal semantics, providing a different view on the meaning of the connectives in answer set programming and on its relation to other logics, in particular classical logic.

</details>

<details>

<summary>2019-01-26 15:48:38 - Distributed Convolutional Dictionary Learning (DiCoDiLe): Pattern Discovery in Large Images and Signals</summary>

- *Thomas Moreau, Alexandre Gramfort*

- `1901.09235v1` - [abs](http://arxiv.org/abs/1901.09235v1) - [pdf](http://arxiv.org/pdf/1901.09235v1)

> Convolutional dictionary learning (CDL) estimates shift invariant basis adapted to multidimensional data. CDL has proven useful for image denoising or inpainting, as well as for pattern discovery on multivariate signals. As estimated patterns can be positioned anywhere in signals or images, optimization techniques face the difficulty of working in extremely high dimensions with millions of pixels or time samples, contrarily to standard patch-based dictionary learning. To address this optimization problem, this work proposes a distributed and asynchronous algorithm, employing locally greedy coordinate descent and an asynchronous locking mechanism that does not require a central server. This algorithm can be used to distribute the computation on a number of workers which scales linearly with the encoded signal's size. Experiments confirm the scaling properties which allows us to learn patterns on large scales images from the Hubble Space Telescope.

</details>

<details>

<summary>2019-01-26 22:40:41 - Putting a bug in ML: The moth olfactory network learns to read MNIST</summary>

- *Charles B. Delahunt, J. Nathan Kutz*

- `1802.05405v3` - [abs](http://arxiv.org/abs/1802.05405v3) - [pdf](http://arxiv.org/pdf/1802.05405v3)

> We seek to (i) characterize the learning architectures exploited in biological neural networks for training on very few samples, and (ii) port these algorithmic structures to a machine learning context. The Moth Olfactory Network is among the simplest biological neural systems that can learn, and its architecture includes key structural elements and mechanisms widespread in biological neural nets, such as cascaded networks, competitive inhibition, high intrinsic noise, sparsity, reward mechanisms, and Hebbian plasticity. These structural biological elements, in combination, enable rapid learning.   MothNet is a computational model of the Moth Olfactory Network, closely aligned with the moth's known biophysics and with in vivo electrode data collected from moths learning new odors. We assign this model the task of learning to read the MNIST digits. We show that MothNet successfully learns to read given very few training samples (1 to 10 samples per class). In this few-samples regime, it outperforms standard machine learning methods such as nearest-neighbors, support-vector machines, and neural networks (NNs), and matches specialized one-shot transfer-learning methods but without the need for pre-training. The MothNet architecture illustrates how algorithmic structures derived from biological brains can be used to build alternative NNs that may avoid some of the learning rate limitations of current engineered NNs.

</details>

<details>

<summary>2019-01-27 22:36:16 - Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting</summary>

- *Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Adam Tauman Kalai*

- `1901.09451v1` - [abs](http://arxiv.org/abs/1901.09451v1) - [pdf](http://arxiv.org/pdf/1901.09451v1)

> We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are "scrubbed," and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.

</details>

<details>

<summary>2019-01-28 00:54:59 - The CM Algorithm for the Maximum Mutual Information Classifications of Unseen Instances</summary>

- *Chenguang Lu*

- `1901.09902v1` - [abs](http://arxiv.org/abs/1901.09902v1) - [pdf](http://arxiv.org/pdf/1901.09902v1)

> The Maximum Mutual Information (MMI) criterion is different from the Least Error Rate (LER) criterion. It can reduce failing to report small probability events. This paper introduces the Channels Matching (CM) algorithm for the MMI classifications of unseen instances. It also introduces some semantic information methods, which base the CM algorithm. In the CM algorithm, label learning is to let the semantic channel match the Shannon channel (Matching I) whereas classifying is to let the Shannon channel match the semantic channel (Matching II). We can achieve the MMI classifications by repeating Matching I and II. For low-dimensional feature spaces, we only use parameters to construct n likelihood functions for n different classes (rather than to construct partitioning boundaries as gradient descent) and expresses the boundaries by numerical values. Without searching in parameter spaces, the computation of the CM algorithm for low-dimensional feature spaces is very simple and fast. Using a two-dimensional example, we test the speed and reliability of the CM algorithm by different initial partitions. For most initial partitions, two iterations can make the mutual information surpass 99% of the convergent MMI. The analysis indicates that for high-dimensional feature spaces, we may combine the CM algorithm with neural networks to improve the MMI classifications for faster and more reliable convergence.

</details>

<details>

<summary>2019-01-28 05:04:28 - Syntactico-Semantic Reasoning using PCFG, MEBN & PP Attachment Ambiguity</summary>

- *Shrinivasan R Patnaik Patnaikuni, Dr. Sachin R Gengaje*

- `1809.07607v2` - [abs](http://arxiv.org/abs/1809.07607v2) - [pdf](http://arxiv.org/pdf/1809.07607v2)

> Probabilistic context free grammars (PCFG) have been the core of the probabilistic reasoning based parsers for several years especially in the context of the NLP. Multi entity bayesian networks (MEBN) a First Order Logic probabilistic reasoning methodology is widely adopted and used method for uncertainty reasoning. Further upper ontology like Probabilistic Ontology Web Language (PR-OWL) built using MEBN takes care of probabilistic ontologies which model and capture the uncertainties inherent in the domain's semantic information. The paper attempts to establish a link between probabilistic reasoning in PCFG and MEBN by proposing a formal description of PCFG driven by MEBN leading to usage of PR-OWL modeled ontologies in PCFG parsers. Furthermore, the paper outlines an approach to resolve prepositional phrase (PP) attachment ambiguity using the proposed mapping between PCFG and MEBN.

</details>

<details>

<summary>2019-01-28 12:38:21 - QA4IE: A Question Answering based Framework for Information Extraction</summary>

- *Lin Qiu, Hao Zhou, Yanru Qu, Weinan Zhang, Suoheng Li, Shu Rong, Dongyu Ru, Lihua Qian, Kewei Tu, Yong Yu*

- `1804.03396v2` - [abs](http://arxiv.org/abs/1804.03396v2) - [pdf](http://arxiv.org/pdf/1804.03396v2)

> Information Extraction (IE) refers to automatically extracting structured relation tuples from unstructured texts. Common IE solutions, including Relation Extraction (RE) and open IE systems, can hardly handle cross-sentence tuples, and are severely restricted by limited relation types as well as informal relation specifications (e.g., free-text based relation tuples). In order to overcome these weaknesses, we propose a novel IE framework named QA4IE, which leverages the flexible question answering (QA) approaches to produce high quality relation triples across sentences. Based on the framework, we develop a large IE benchmark with high quality human evaluation. This benchmark contains 293K documents, 2M golden relation triples, and 636 relation types. We compare our system with some IE baselines on our benchmark and the results show that our system achieves great improvements.

</details>

<details>

<summary>2019-01-28 21:39:23 - Diseño de un espacio semántico sobre la base de la Wikipedia. Una propuesta de análisis de la semántica latente para el idioma español</summary>

- *Dalina Aidee Villa, Igor Barahona, Luis Javier Álvarez*

- `1902.02173v1` - [abs](http://arxiv.org/abs/1902.02173v1) - [pdf](http://arxiv.org/pdf/1902.02173v1)

> Latent Semantic Analysis (LSA) was initially conceived by the cognitive psychology at the 90s decade. Since its emergence, the LSA has been used to model cognitive processes, pointing out academic texts, compare literature works and analyse political speeches, among other applications. Taking as starting point multivariate method for dimensionality reduction, this paper propose a semantic space for Spanish language. Out results include a document text matrix with dimensions 1.3 x10^6 and 5.9x10^6, which later is decomposed into singular values. Those singular values are used to semantically words or text.

</details>

<details>

<summary>2019-01-29 06:02:45 - Adversarial Adaptation of Scene Graph Models for Understanding Civic Issues</summary>

- *Shanu Kumar, Shubham Atreja, Anjali Singh, Mohit Jain*

- `1901.10124v1` - [abs](http://arxiv.org/abs/1901.10124v1) - [pdf](http://arxiv.org/pdf/1901.10124v1)

> Citizen engagement and technology usage are two emerging trends driven by smart city initiatives. Governments around the world are adopting technology for faster resolution of civic issues. Typically, citizens report issues, such as broken roads, garbage dumps, etc. through web portals and mobile apps, in order for the government authorities to take appropriate actions. Several mediums -- text, image, audio, video -- are used to report these issues. Through a user study with 13 citizens and 3 authorities, we found that image is the most preferred medium to report civic issues. However, analyzing civic issue related images is challenging for the authorities as it requires manual effort. Moreover, previous works have been limited to identifying a specific set of issues from images. In this work, given an image, we propose to generate a Civic Issue Graph consisting of a set of objects and the semantic relations between them, which are representative of the underlying civic issue. We also release two multi-modal (text and images) datasets, that can help in further analysis of civic issues from images. We present a novel approach for adversarial training of existing scene graph models that enables the use of scene graphs for new applications in the absence of any labelled training data. We conduct several experiments to analyze the efficacy of our approach, and using human evaluation, we establish the appropriateness of our model at representing different civic issues.

</details>

<details>

<summary>2019-01-29 18:26:47 - A Probabilistic U-Net for Segmentation of Ambiguous Images</summary>

- *Simon A. A. Kohl, Bernardino Romera-Paredes, Clemens Meyer, Jeffrey De Fauw, Joseph R. Ledsam, Klaus H. Maier-Hein, S. M. Ali Eslami, Danilo Jimenez Rezende, Olaf Ronneberger*

- `1806.05034v4` - [abs](http://arxiv.org/abs/1806.05034v4) - [pdf](http://arxiv.org/pdf/1806.05034v4)

> Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.

</details>

<details>

<summary>2019-01-29 18:27:37 - Semantic Redundancies in Image-Classification Datasets: The 10% You Don't Need</summary>

- *Vighnesh Birodkar, Hossein Mobahi, Samy Bengio*

- `1901.11409v1` - [abs](http://arxiv.org/abs/1901.11409v1) - [pdf](http://arxiv.org/pdf/1901.11409v1)

> Large datasets have been crucial to the success of deep learning models in the recent years, which keep performing better as they are trained with more labelled data. While there have been sustained efforts to make these models more data-efficient, the potential benefit of understanding the data itself, is largely untapped. Specifically, focusing on object recognition tasks, we wonder if for common benchmark datasets we can do better than random subsets of the data and find a subset that can generalize on par with the full dataset when trained on. To our knowledge, this is the first result that can find notable redundancies in CIFAR-10 and ImageNet datasets (at least 10%). Interestingly, we observe semantic correlations between required and redundant images. We hope that our findings can motivate further research into identifying additional redundancies and exploiting them for more efficient training or data-collection.

</details>

<details>

<summary>2019-01-30 05:49:17 - Effective weakly supervised semantic frame induction using expression sharing in hierarchical hidden Markov models</summary>

- *Janneke van de Loo, Jort F. Gemmeke, Guy De Pauw, Bart Ons, Walter Daelemans, Hugo Van hamme*

- `1901.10680v1` - [abs](http://arxiv.org/abs/1901.10680v1) - [pdf](http://arxiv.org/pdf/1901.10680v1)

> We present a framework for the induction of semantic frames from utterances in the context of an adaptive command-and-control interface. The system is trained on an individual user's utterances and the corresponding semantic frames representing controls. During training, no prior information on the alignment between utterance segments and frame slots and values is available. In addition, semantic frames in the training data can contain information that is not expressed in the utterances. To tackle this weakly supervised classification task, we propose a framework based on Hidden Markov Models (HMMs). Structural modifications, resulting in a hierarchical HMM, and an extension called expression sharing are introduced to minimize the amount of training time and effort required for the user.   The dataset used for the present study is PATCOR, which contains commands uttered in the context of a vocally guided card game, Patience. Experiments were carried out on orthographic and phonetic transcriptions of commands, segmented on different levels of n-gram granularity. The experimental results show positive effects of all the studied system extensions, with some effect differences between the different input representations. Moreover, evaluation experiments on held-out data with the optimal system configuration show that the extended system is able to achieve high accuracies with relatively small amounts of training data.

</details>

<details>

<summary>2019-01-30 08:14:56 - Automatic White-Box Testing of First-Order Logic Ontologies</summary>

- *Javier Álvez, Montserrat Hermo, Paqui Lucio, German Rigau*

- `1705.10219v3` - [abs](http://arxiv.org/abs/1705.10219v3) - [pdf](http://arxiv.org/pdf/1705.10219v3)

> Formal ontologies are axiomatizations in a logic-based formalism. The development of formal ontologies, and their important role in the Semantic Web area, is generating considerable research on the use of automated reasoning techniques and tools that help in ontology engineering. One of the main aims is to refine and to improve axiomatizations for enabling automated reasoning tools to efficiently infer reliable information. Defects in the axiomatization can not only cause wrong inferences, but can also hinder the inference of expected information, either by increasing the computational cost of, or even preventing, the inference. In this paper, we introduce a novel, fully automatic white-box testing framework for first-order logic ontologies. Our methodology is based on the detection of inference-based redundancies in the given axiomatization. The application of the proposed testing method is fully automatic since a) the automated generation of tests is guided only by the syntax of axioms and b) the evaluation of tests is performed by automated theorem provers. Our proposal enables the detection of defects and serves to certify the grade of suitability --for reasoning purposes-- of every axiom. We formally define the set of tests that are generated from any axiom and prove that every test is logically related to redundancies in the axiom from which the test has been generated. We have implemented our method and used this implementation to automatically detect several non-trivial defects that were hidden in various first-order logic ontologies. Throughout the paper we provide illustrative examples of these defects, explain how they were found, and how each proof --given by an automated theorem-prover-- provides useful hints on the nature of each defect. Additionally, by correcting all the detected defects, we have obtained an improved version of one of the tested ontologies: Adimen-SUMO.

</details>

<details>

<summary>2019-01-30 09:32:51 - Compositionality for Recursive Neural Networks</summary>

- *Martha Lewis*

- `1901.10723v1` - [abs](http://arxiv.org/abs/1901.10723v1) - [pdf](http://arxiv.org/pdf/1901.10723v1)

> Modelling compositionality has been a longstanding area of research in the field of vector space semantics. The categorical approach to compositionality maps grammar onto vector spaces in a principled way, but comes under fire for requiring the formation of very high-dimensional matrices and tensors, and therefore being computationally infeasible. In this paper I show how a linear simplification of recursive neural tensor network models can be mapped directly onto the categorical approach, giving a way of computing the required matrices and tensors. This mapping suggests a number of lines of research for both categorical compositional vector space models of meaning and for recursive neural network models of compositionality.

</details>

<details>

<summary>2019-01-30 18:33:05 - GILE: A Generalized Input-Label Embedding for Text Classification</summary>

- *Nikolaos Pappas, James Henderson*

- `1806.06219v3` - [abs](http://arxiv.org/abs/1806.06219v3) - [pdf](http://arxiv.org/pdf/1806.06219v3)

> Neural text classification models typically treat output labels as categorical variables which lack description and semantics. This forces their parametrization to be dependent on the label set size, and, hence, they are unable to scale to large label sets and generalize to unseen ones. Existing joint input-label text models overcome these issues by exploiting label descriptions, but they are unable to capture complex label relationships, have rigid parametrization, and their gains on unseen labels happen often at the expense of weak performance on the labels seen during training. In this paper, we propose a new input-label model which generalizes over previous such models, addresses their limitations and does not compromise performance on seen labels. The model consists of a joint non-linear input-label embedding with controllable capacity and a joint-space-dependent classification unit which is trained with cross-entropy loss to optimize classification performance. We evaluate models on full-resource and low- or zero-resource text classification of multilingual news and biomedical text with a large label set. Our model outperforms monolingual and multilingual models which do not leverage label semantics and previous joint input-label space models in both scenarios.

</details>

<details>

<summary>2019-01-30 19:19:02 - Software solutions for form-based collection of data and the semantic enrichment of form data</summary>

- *Markus D. Steinberg*

- `1901.11053v1` - [abs](http://arxiv.org/abs/1901.11053v1) - [pdf](http://arxiv.org/pdf/1901.11053v1)

> Data collection is an important part of many citizen science projects as well as other fields of research, particularly in life sciences. Mobile applications with form-based surveys are increasingly used to support this, due to the large number of mobile devices and their growing number of built-in sensors. Since the composition of form-based surveys from scratch can be a tedious task, multiple tools have been published that can help with their design and distribution as well as the data collection via mobile devices and the data storage. Some even support simple data analysis. With this increasing number of software options project leaders will often face the question, which tool is most suitable for their current use case.   With that in mind, this project pursues two main objectives:   1. To present an overview of a selection of survey design tools and their capabilities in order to provide a clear foundation for such a decision.   2. To examine if any tool provides the capability to collect and export data in a way that can easily be used and interpreted by other applications or persons. This aspect includes the supply of metadata about the data collection process and the data itself, information about the meaning of the data as well as an export format that can easily be processed.

</details>

<details>

<summary>2019-01-30 19:59:33 - A Convolutional Neural Network for the Automatic Diagnosis of Collagen VI related Muscular Dystrophies</summary>

- *Adrián Bazaga, Mònica Roldán, Carmen Badosa, Cecilia Jiménez-Mallebrera, Josep M. Porta*

- `1901.11074v1` - [abs](http://arxiv.org/abs/1901.11074v1) - [pdf](http://arxiv.org/pdf/1901.11074v1)

> The development of machine learning systems for the diagnosis of rare diseases is challenging mainly due the lack of data to study them. Despite this challenge, this paper proposes a system for the Computer Aided Diagnosis (CAD) of low-prevalence, congenital muscular dystrophies from confocal microscopy images. The proposed CAD system relies on a Convolutional Neural Network (CNN) which performs an independent classification for non-overlapping patches tiling the input image, and generates an overall decision summarizing the individual decisions for the patches on the query image. This decision scheme points to the possibly problematic areas in the input images and provides a global quantitative evaluation of the state of the patients, which is fundamental for diagnosis and to monitor the efficiency of therapies.

</details>

<details>

<summary>2019-01-30 20:39:54 - Exceptionally Monadic Error Handling</summary>

- *Jan Malakhovski*

- `1810.13430v3` - [abs](http://arxiv.org/abs/1810.13430v3) - [pdf](http://arxiv.org/pdf/1810.13430v3)

> We notice that the type of catch :: c a -> (e -> c a) -> c a operator is a special case of monadic bind operator (>>=) :: m a -> (a -> m b) -> m b, the semantics (surprisingly) matches, and this observation has many interesting consequences.   For instance, the reader is probably aware that the monadic essence of the (>>=) operator of the error monad $\lambda A.E \lor A$ is to behave like identity monad for "normal" values and to stop on "errors". The unappreciated fact is that handling of said "errors" with a catch operator of the "flipped" "conjoined" error monad $\lambda E.E \lor A$ is, too, a monadic computation that treats still unhandled "errors" as "normal" values and stops when an "error" is finally handled.   We show that for an appropriately indexed type of computations such a "conjoined" structure naturally follows from the conventional operational semantics of throw and catch operators. Consequently, we show that this structure uniformly generalizes all conventional monadic error handling mechanisms we are aware of. We also demonstrate several more interesting instances of this structure of which at least bi-indexed monadic parser combinators and conventional exceptions implemented via continuations have immediate practical applications. Finally, we notice that these observations provide surprising perspectives on error handling in general and point to a largely unexplored trail in programming language design space.

</details>

<details>

<summary>2019-01-31 01:46:35 - A Generalized Language Model in Tensor Space</summary>

- *Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, Dawei Song*

- `1901.11167v1` - [abs](http://arxiv.org/abs/1901.11167v1) - [pdf](http://arxiv.org/pdf/1901.11167v1)

> In the literature, tensors have been effectively used for capturing the context information in language models. However, the existing methods usually adopt relatively-low order tensors, which have limited expressive power in modeling language. Developing a higher-order tensor representation is challenging, in terms of deriving an effective solution and showing its generality. In this paper, we propose a language model named Tensor Space Language Model (TSLM), by utilizing tensor networks and tensor decomposition. In TSLM, we build a high-dimensional semantic space constructed by the tensor product of word vectors. Theoretically, we prove that such tensor representation is a generalization of the n-gram language model. We further show that this high-order tensor representation can be decomposed to a recursive calculation of conditional probability for language modeling. The experimental results on Penn Tree Bank (PTB) dataset and WikiText benchmark demonstrate the effectiveness of TSLM.

</details>

<details>

<summary>2019-01-31 14:29:35 - Learning and Evaluating General Linguistic Intelligence</summary>

- *Dani Yogatama, Cyprien de Masson d'Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, Phil Blunsom*

- `1901.11373v1` - [abs](http://arxiv.org/abs/1901.11373v1) - [pdf](http://arxiv.org/pdf/1901.11373v1)

> We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.

</details>

<details>

<summary>2019-01-31 17:18:42 - Learning Taxonomies of Concepts and not Words using Contextualized Word Representations: A Position Paper</summary>

- *Lukas Schmelzeisen, Steffen Staab*

- `1902.02169v1` - [abs](http://arxiv.org/abs/1902.02169v1) - [pdf](http://arxiv.org/pdf/1902.02169v1)

> Taxonomies are semantic hierarchies of concepts. One limitation of current taxonomy learning systems is that they define concepts as single words. This position paper argues that contextualized word representations, which recently achieved state-of-the-art results on many competitive NLP tasks, are a promising method to address this limitation. We outline a novel approach for taxonomy learning that (1) defines concepts as synsets, (2) learns density-based approximations of contextualized word representations, and (3) can measure similarity and hypernymy among them.

</details>


## 2019-02

<details>

<summary>2019-02-02 13:53:29 - Enhancing Interpretability of Black-box Soft-margin SVM by Integrating Data-based Priors</summary>

- *Shaohan Chen, Chuanhou Gao, Ping Zhang*

- `1710.02924v2` - [abs](http://arxiv.org/abs/1710.02924v2) - [pdf](http://arxiv.org/pdf/1710.02924v2)

> The lack of interpretability often makes black-box models difficult to be applied to many practical domains. For this reason, the current work, from the black-box model input port, proposes to incorporate data-based prior information into the black-box soft-margin SVM model to enhance its interpretability. The concept and incorporation mechanism of data-based prior information are successively developed, based on which the interpretable or partly interpretable SVM optimization model is designed and then solved through handily rewriting the optimization problem as a nonlinear quadratic programming problem. An algorithm for mining data-based linear prior information from data set is also proposed, which generates a linear expression with respect to two appropriate inputs identified from all inputs of system. At last, the proposed interpretability enhancement strategy is applied to eight benchmark examples for effectiveness exhibition.

</details>

<details>

<summary>2019-02-02 15:59:29 - Distinction Graphs and Graphtropy: A Formalized Phenomenological Layer Underlying Classical and Quantum Entropy, Observational Semantics and Cognitive Computation</summary>

- *Ben Goertzel*

- `1902.00741v1` - [abs](http://arxiv.org/abs/1902.00741v1) - [pdf](http://arxiv.org/pdf/1902.00741v1)

> A new conceptual foundation for the notion of "information" is proposed, based on the concept of a "distinction graph": a graph in which two nodes are connected iff they cannot be distinguished by a particular observer. The "graphtropy" of a distinction graph is defined as the average connection probability of two nodes; in the case where the distinction graph is a composed of disconnected components that are fully connected subgraphs, this is equivalent to Ellerman's logical entropy, which has straightforward relationships to Shannon entropy. Probabilistic distinction graphs and probabilistic graphtropy are also considered, as well as connections between graphtropy and thermodynamic and quantum entropy. The semantics of the Second Law of Thermodynamics and the Maximum Entropy Production Principle are unfolded in a novel way, via analysis of the cognitive processes underlying the making of distinction graphs This evokes an interpretation in which complex intelligence is seen to correspond to states of consciousness with intermediate graphtropy, which are associated with memory imperfections that violate the assumptions leading to derivation of the Second Law. In the case where nodes of a distinction graph are labeled by computable entities, graphtropy is shown to be monotonically related to the average algorithmic information of the nodes (relative to to the algorithmic information of the observer). A quantum-mechanical version of distinction graphs is considered, in which distinctions can exist in a superposed state; this yields to graphtropy as a measure of the impurity of a mixed state, and to a concept of "quangraphtropy." Finally, a novel computational model called Dynamic Distinction Graphs (DDGs) is formulated, via enhancing distinction graphs with additional links expressing causal implications, enabling a distinction-based model of "observers."

</details>

<details>

<summary>2019-02-02 17:04:14 - Word Embeddings for Sentiment Analysis: A Comprehensive Empirical Survey</summary>

- *Erion Çano, Maurizio Morisio*

- `1902.00753v1` - [abs](http://arxiv.org/abs/1902.00753v1) - [pdf](http://arxiv.org/pdf/1902.00753v1)

> This work investigates the role of factors like training method, training corpus size and thematic relevance of texts in the performance of word embedding features on sentiment analysis of tweets, song lyrics, movie reviews and item reviews. We also explore specific training or post-processing methods that can be used to enhance the performance of word embeddings in certain tasks or domains. Our empirical observations indicate that models trained with multithematic texts that are large and rich in vocabulary are the best in answering syntactic and semantic word analogy questions. We further observe that influence of thematic relevance is stronger on movie and phone reviews, but weaker on tweets and lyrics. These two later domains are more sensitive to corpus size and training method, with Glove outperforming Word2vec. "Injecting" extra intelligence from lexicons or generating sentiment specific word embeddings are two prominent alternatives for increasing performance of word embedding features.

</details>

<details>

<summary>2019-02-02 23:53:18 - Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task</summary>

- *Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, Dragomir Radev*

- `1809.08887v5` - [abs](http://arxiv.org/abs/1809.08887v5) - [pdf](http://arxiv.org/pdf/1809.08887v5)

> We present Spider, a large-scale, complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task where different complex SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and the exact same programs in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 12.4% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task are publicly available at https://yale-lily.github.io/spider

</details>

<details>

<summary>2019-02-03 10:29:22 - Stabilized MorteX method for mesh tying along embedded interfaces</summary>

- *Basava Raju Akula, Julien Vignollet, Vladislav A. Yastrebov*

- `1902.04003v1` - [abs](http://arxiv.org/abs/1902.04003v1) - [pdf](http://arxiv.org/pdf/1902.04003v1)

> We present a unified framework to tie overlapping meshes in solid mechanics applications. This framework is a combination of the X-FEM method and the mortar method, which uses Lagrange multipliers to fulfill the tying constraints. As known, mixed formulations are prone to mesh locking which manifests itself by the emergence of spurious oscillations in the vicinity of the tying interface. To overcome this inherent difficulty, we suggest a new coarse-grained interpolation of Lagrange multipliers. This technique consists in selective assignment of Lagrange multipliers on nodes of the mortar side and in non-local interpolation of the associated traction field. The optimal choice of the coarse-graining spacing is guided solely by the mesh-density contrast between the mesh of the mortar side and the number of blending elements of the host mesh. The method is tested on two patch tests (compression and bending) for different interpolations and element types as well as for different material and mesh contrasts. The optimal mesh convergence and removal of spurious oscillations is also demonstrated on the Eshelby inclusion problem for high contrasts of inclusion/matrix materials. Few additional examples confirm the performance of the elaborated framework.

</details>

<details>

<summary>2019-02-03 11:02:20 - MorteX method for contact along real and embedded surfaces: coupling X-FEM with the Mortar method</summary>

- *Basava Raju Akula, Julien Vignollet, Vladislav A. Yastrebov*

- `1902.04000v1` - [abs](http://arxiv.org/abs/1902.04000v1) - [pdf](http://arxiv.org/pdf/1902.04000v1)

> A method to treat frictional contact problems along embedded surfaces in the finite element framework is developed. Arbitrarily shaped embedded surfaces, cutting through finite element meshes, are handled by the X-FEM. The frictional contact problem is solved using the monolithic augmented Lagrangian method within the mortar framework which was adapted for handling embedded surfaces. We report that the resulting mixed formulation is prone to mesh locking in case of high elastic and mesh density contrasts across the contact interface. The mesh locking manifests itself in spurious stress oscillations in the vicinity of the contact interface. We demonstrate that in the classical patch test, these oscillations can be removed simply by using triangular blending elements. In a more general case, the triangulation is shown inefficient, therefore stabilization of the problem is achieved by adopting a recently proposed coarse-graining interpolation of Lagrange multipliers. Moreover, we demonstrate that the coarse-graining is also beneficial for the classical mortar method to avoid spurious oscillations for contact interfaces with high elastic contrast. The performance of this novel method, called MorteX, is demonstrated on several examples which show as accurate treatment of frictional contact along embedded surfaces as the classical mortar method along boundary fitted surfaces.

</details>

<details>

<summary>2019-02-03 15:41:09 - Study, representation and applications of hypergraph minimal transversals</summary>

- *M. Nidhal Jelassi*

- `1902.00911v1` - [abs](http://arxiv.org/abs/1902.00911v1) - [pdf](http://arxiv.org/pdf/1902.00911v1)

> This work is part of the field of the hypergraph theory and focuses on hypergraph minimal transversal. The problem of extracting the minimal transversals from a hypergraph received the interest of many researchers as shown the number of algorithms proposed in the literature, and this is mainly due to the solutions offered by the minimal transversal in various application areas such as databases, artificial intelligence, e-commerce, semantic web, etc. In view of the wide range of fields of minimal transversal application and the interest they generate, the objective of this thesis is to explore new application paths of minimal transversal by proposing methods to optimize the extraction. This has led to three proposed contributions in this thesis. The first approach takes advantage of the emergence of Web 2.0 and, therefore, social networks using minimal transversal for the detection of important actors within these networks. The second part of research in this thesis has focused on reducing the number of hypergraph minimal transversal. A concise and accurate representation of minimal transversal was proposed and is based on the construction of an irredundant hypergraph, hence are calculated the irredundant minimal transversal of the initial hypergraph. An application of this representation to the dependency inference problem is presented to illustrate the usefulness of this approach. The last approach includes the hypergraph decomposition into partial hypergraph the local minimal transversal are calculated and their Cartesian product can generate all the hypergraph transversal sets. Different experimental studies have shown the value of these proposed approaches.

</details>

<details>

<summary>2019-02-03 16:13:53 - Discovering Implicational Knowledge in Wikidata</summary>

- *Tom Hanika, Maximilian Marx, Gerd Stumme*

- `1902.00916v1` - [abs](http://arxiv.org/abs/1902.00916v1) - [pdf](http://arxiv.org/pdf/1902.00916v1)

> Knowledge graphs have recently become the state-of-the-art tool for representing the diverse and complex knowledge of the world. Examples include the proprietary knowledge graphs of companies such as Google, Facebook, IBM, or Microsoft, but also freely available ones such as YAGO, DBpedia, and Wikidata. A distinguishing feature of Wikidata is that the knowledge is collaboratively edited and curated. While this greatly enhances the scope of Wikidata, it also makes it impossible for a single individual to grasp complex connections between properties or understand the global impact of edits in the graph. We apply Formal Concept Analysis to efficiently identify comprehensible implications that are implicitly present in the data. Although the complex structure of data modelling in Wikidata is not amenable to a direct approach, we overcome this limitation by extracting contextual representations of parts of Wikidata in a systematic fashion. We demonstrate the practical feasibility of our approach through several experiments and show that the results may lead to the discovery of interesting implicational knowledge. Besides providing a method for obtaining large real-world data sets for FCA, we sketch potential applications in offering semantic assistance for editing and curating Wikidata.

</details>

<details>

<summary>2019-02-04 18:53:14 - Embodied Multimodal Multitask Learning</summary>

- *Devendra Singh Chaplot, Lisa Lee, Ruslan Salakhutdinov, Devi Parikh, Dhruv Batra*

- `1902.01385v1` - [abs](http://arxiv.org/abs/1902.01385v1) - [pdf](http://arxiv.org/pdf/1902.01385v1)

> Recent efforts on training visual navigation agents conditioned on language using deep reinforcement learning have been successful in learning policies for different multimodal tasks, such as semantic goal navigation and embodied question answering. In this paper, we propose a multitask model capable of jointly learning these multimodal tasks, and transferring knowledge of words and their grounding in visual objects across the tasks. The proposed model uses a novel Dual-Attention unit to disentangle the knowledge of words in the textual representations and visual concepts in the visual representations, and align them with each other. This disentangled task-invariant alignment of representations facilitates grounding and knowledge transfer across both tasks. We show that the proposed model outperforms a range of baselines on both tasks in simulated 3D environments. We also show that this disentanglement of representations makes our model modular, interpretable, and allows for transfer to instructions containing new words by leveraging object detectors.

</details>

<details>

<summary>2019-02-05 11:44:54 - Relative Entailment Among Probabilistic Implications</summary>

- *Albert Atserias, José L. Balcázar, Marie Ely Piceno*

- `1501.04826v5` - [abs](http://arxiv.org/abs/1501.04826v5) - [pdf](http://arxiv.org/pdf/1501.04826v5)

> We study a natural variant of the implicational fragment of propositional logic. Its formulas are pairs of conjunctions of positive literals, related together by an implicational-like connective; the semantics of this sort of implication is defined in terms of a threshold on a conditional probability of the consequent, given the antecedent: we are dealing with what the data analysis community calls confidence of partial implications or association rules. Existing studies of redundancy among these partial implications have characterized so far only entailment from one premise and entailment from two premises, both in the stand-alone case and in the case of presence of additional classical implications (this is what we call "relative entailment"). By exploiting a previously noted alternative view of the entailment in terms of linear programming duality, we characterize exactly the cases of entailment from arbitrary numbers of premises, again both in the stand-alone case and in the case of presence of additional classical implications. As a result, we obtain decision algorithms of better complexity; additionally, for each potential case of entailment, we identify a critical confidence threshold and show that it is, actually, intrinsic to each set of premises and antecedent of the conclusion.

</details>

<details>

<summary>2019-02-05 15:49:36 - Generalised Differential Privacy for Text Document Processing</summary>

- *Natasha Fernandes, Mark Dras, Annabelle McIver*

- `1811.10256v2` - [abs](http://arxiv.org/abs/1811.10256v2) - [pdf](http://arxiv.org/pdf/1811.10256v2)

> We address the problem of how to "obfuscate" texts by removing stylistic clues which can identify authorship, whilst preserving (as much as possible) the content of the text. In this paper we combine ideas from "generalised differential privacy" and machine learning techniques for text processing to model privacy for text documents. We define a privacy mechanism that operates at the level of text documents represented as "bags-of-words" - these representations are typical in machine learning and contain sufficient information to carry out many kinds of classification tasks including topic identification and authorship attribution (of the original documents). We show that our mechanism satisfies privacy with respect to a metric for semantic similarity, thereby providing a balance between utility, defined by the semantic content of texts, with the obfuscation of stylistic clues. We demonstrate our implementation on a "fan fiction" dataset, confirming that it is indeed possible to disguise writing style effectively whilst preserving enough information and variation for accurate content classification tasks.

</details>

<details>

<summary>2019-02-05 19:19:10 - A Surveillance Infrastructure for Malaria Analytics: Provisioning Data Access and Preservation of Interoperability</summary>

- *Mohammad Sadnan Al Manir, Jon Haël Brenas, Christopher JO Baker, Arash Shaban-Nejad*

- `1902.01877v1` - [abs](http://arxiv.org/abs/1902.01877v1) - [pdf](http://arxiv.org/pdf/1902.01877v1)

> We propose the Semantics, Interoperability, and Evolution for Malaria Analytics (SIEMA) platform for use in malaria surveillance based on semantic data federation. Using this approach, it is possible to access distributed data, extend and preserve interoperability between multiple dynamic distributed malaria sources, and facilitate detection of system changes that can interrupt mission-critical global surveillance activities. We used Semantic Automated Discovery and Integration (SADI) Semantic Web Services to enable data access and improve interoperability, and the graphical user interface-enabled semantic query engine HYDRA to implement the target queries typical of malaria programs. We implemented a custom algorithm to detect changes to community-developed terminologies, data sources, and services that are core to SIEMA. This algorithm reports to a dashboard. Valet SADI is used to mitigate the impact of changes by rebuilding affected services. We developed a prototype surveillance and change management platform from a combination of third-party tools, community-developed terminologies, and custom algorithms. We illustrated a methodology and core infrastructure to facilitate interoperable access to distributed data sources using SADI Semantic Web services. This degree of access makes it possible to implement complex queries needed by our user community with minimal technical skill. We implemented a dashboard that reports on terminology changes that can render the services inactive, jeopardizing system interoperability. Using this information, end users can control and reactively rebuild services to preserve interoperability and minimize service downtime.

</details>

<details>

<summary>2019-02-05 19:49:56 - Situational Grounding within Multimodal Simulations</summary>

- *James Pustejovsky, Nikhil Krishnaswamy*

- `1902.01886v1` - [abs](http://arxiv.org/abs/1902.01886v1) - [pdf](http://arxiv.org/pdf/1902.01886v1)

> In this paper, we argue that simulation platforms enable a novel type of embodied spatial reasoning, one facilitated by a formal model of object and event semantics that renders the continuous quantitative search space of an open-world, real-time environment tractable. We provide examples for how a semantically-informed AI system can exploit the precise, numerical information provided by a game engine to perform qualitative reasoning about objects and events, facilitate learning novel concepts from data, and communicate with a human to improve its models and demonstrate its understanding. We argue that simulation environments, and game engines in particular, bring together many different notions of "simulation" and many different technologies to provide a highly-effective platform for developing both AI systems and tools to experiment in both machine and human intelligence.

</details>

<details>

<summary>2019-02-05 21:33:04 - Adversarial Learning of Semantic Relevance in Text to Image Synthesis</summary>

- *Miriam Cha, Youngjune L. Gwon, H. T. Kung*

- `1812.05083v2` - [abs](http://arxiv.org/abs/1812.05083v2) - [pdf](http://arxiv.org/pdf/1812.05083v2)

> We describe a new approach that improves the training of generative adversarial nets (GANs) for synthesizing diverse images from a text input. Our approach is based on the conditional version of GANs and expands on previous work leveraging an auxiliary task in the discriminator. Our generated images are not limited to certain classes and do not suffer from mode collapse while semantically matching the text input. A key to our training methods is how to form positive and negative training examples with respect to the class label of a given image. Instead of selecting random training examples, we perform negative sampling based on the semantic distance from a positive example in the class. We evaluate our approach using the Oxford-102 flower dataset, adopting the inception score and multi-scale structural similarity index (MS-SSIM) metrics to assess discriminability and diversity of the generated images. The empirical results indicate greater diversity in the generated images, especially when we gradually select more negative training examples closer to a positive example in the semantic space.

</details>

<details>

<summary>2019-02-06 08:07:37 - DeepIrisNet2: Learning Deep-IrisCodes from Scratch for Segmentation-Robust Visible Wavelength and Near Infrared Iris Recognition</summary>

- *Abhishek Gangwar, Akanksha Joshi, Padmaja Joshi, R. Raghavendra*

- `1902.05390v1` - [abs](http://arxiv.org/abs/1902.05390v1) - [pdf](http://arxiv.org/pdf/1902.05390v1)

> We first, introduce a deep learning based framework named as DeepIrisNet2 for visible spectrum and NIR Iris representation. The framework can work without classical iris normalization step or very accurate iris segmentation; allowing to work under non-ideal situation. The framework contains spatial transformer layers to handle deformation and supervision branches after certain intermediate layers to mitigate overfitting. In addition, we present a dual CNN iris segmentation pipeline comprising of a iris/pupil bounding boxes detection network and a semantic pixel-wise segmentation network. Furthermore, to get compact templates, we present a strategy to generate binary iris codes using DeepIrisNet2. Since, no ground truth dataset are available for CNN training for iris segmentation, We build large scale hand labeled datasets and make them public; i) iris, pupil bounding boxes, ii) labeled iris texture. The networks are evaluated on challenging ND-IRIS-0405, UBIRIS.v2, MICHE-I, and CASIA v4 Interval datasets. Proposed approach significantly improves the state-of-the-art and achieve outstanding performance surpassing all previous methods.

</details>

<details>

<summary>2019-02-06 12:42:58 - Multi-Label Zero-Shot Human Action Recognition via Joint Latent Ranking Embedding</summary>

- *Qian Wang, Ke Chen*

- `1709.05107v3` - [abs](http://arxiv.org/abs/1709.05107v3) - [pdf](http://arxiv.org/pdf/1709.05107v3)

> Human action recognition refers to automatic recognizing human actions from a video clip. In reality, there often exist multiple human actions in a video stream. Such a video stream is often weakly-annotated with a set of relevant human action labels at a global level rather than assigning each label to a specific video episode corresponding to a single action, which leads to a multi-label learning problem. Furthermore, there are many meaningful human actions in reality but it would be extremely difficult to collect/annotate video clips regarding all of various human actions, which leads to a zero-shot learning scenario. To the best of our knowledge, there is no work that has addressed all the above issues together in human action recognition. In this paper, we formulate a real-world human action recognition task as a multi-label zero-shot learning problem and propose a framework to tackle this problem in a holistic way. Our framework holistically tackles the issue of unknown temporal boundaries between different actions for multi-label learning and exploits the side information regarding the semantic relationship between different human actions for knowledge transfer. Consequently, our framework leads to a joint latent ranking embedding for multi-label zero-shot human action recognition. A novel neural architecture of two component models and an alternate learning algorithm are proposed to carry out the joint latent ranking embedding learning. Thus, multi-label zero-shot recognition is done by measuring relatedness scores of action labels to a test video clip in the joint latent visual and semantic embedding spaces. We evaluate our framework with different settings, including a novel data split scheme designed especially for evaluating multi-label zero-shot learning, on two datasets: Breakfast and Charades. The experimental results demonstrate the effectiveness of our framework.

</details>

<details>

<summary>2019-02-06 15:57:39 - Generative Image Translation for Data Augmentation of Bone Lesion Pathology</summary>

- *Anant Gupta, Srivas Venkatesh, Sumit Chopra, Christian Ledig*

- `1902.02248v1` - [abs](http://arxiv.org/abs/1902.02248v1) - [pdf](http://arxiv.org/pdf/1902.02248v1)

> Insufficient training data and severe class imbalance are often limiting factors when developing machine learning models for the classification of rare diseases. In this work, we address the problem of classifying bone lesions from X-ray images by increasing the small number of positive samples in the training set. We propose a generative data augmentation approach based on a cycle-consistent generative adversarial network that synthesizes bone lesions on images without pathology. We pose the generative task as an image-patch translation problem that we optimize specifically for distinct bones (humerus, tibia, femur). In experimental results, we confirm that the described method mitigates the class imbalance problem in the binary classification task of bone lesion detection. We show that the augmented training sets enable the training of superior classifiers achieving better performance on a held-out test set. Additionally, we demonstrate the feasibility of transfer learning and apply a generative model that was trained on one body part to another.

</details>

<details>

<summary>2019-02-06 18:42:18 - Extending a model for ontology-based Arabic-English machine translation</summary>

- *Neama Abdulaziz Dahan, Fadl Mutaher Ba-Alwi*

- `1902.02326v1` - [abs](http://arxiv.org/abs/1902.02326v1) - [pdf](http://arxiv.org/pdf/1902.02326v1)

> The acceleration in telecommunication needs leads to many groups of research, especially in communication facilitating and Machine Translation fields. While people contact with others having different languages and cultures, they need to have instant translations. However, the available instant translators are still providing somewhat bad Arabic-English Translations, for instance when translating books or articles, the meaning is not totally accurate. Therefore, using the semantic web techniques to deal with the homographs and homonyms semantically, the aim of this research is to extend a model for the ontology-based Arabic-English Machine Translation, named NAN, which simulate the human way in translation. The experimental results show that NAN translation is approximately more similar to the Human Translation than the other instant translators. The resulted translation will help getting the translated texts in the target language somewhat correctly and semantically more similar to human translations for the Non-Arabic Natives and the Non-English natives.

</details>

<details>

<summary>2019-02-06 18:53:39 - Semi-Supervised Learning by Label Gradient Alignment</summary>

- *Jacob Jackson, John Schulman*

- `1902.02336v1` - [abs](http://arxiv.org/abs/1902.02336v1) - [pdf](http://arxiv.org/pdf/1902.02336v1)

> We present label gradient alignment, a novel algorithm for semi-supervised learning which imputes labels for the unlabeled data and trains on the imputed labels. We define a semantically meaningful distance metric on the input space by mapping a point (x, y) to the gradient of the model at (x, y). We then formulate an optimization problem whose objective is to minimize the distance between the labeled and the unlabeled data in this space, and we solve it by gradient descent on the imputed labels. We evaluate label gradient alignment using the standardized architecture introduced by Oliver et al. (2018) and demonstrate state-of-the-art accuracy in semi-supervised CIFAR-10 classification.

</details>

<details>

<summary>2019-02-07 18:54:58 - Cognitive Mapping and Planning for Visual Navigation</summary>

- *Saurabh Gupta, Varun Tolani, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik*

- `1702.03920v3` - [abs](http://arxiv.org/abs/1702.03920v3) - [pdf](http://arxiv.org/pdf/1702.03920v3)

> We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as 'going to a chair'. We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.

</details>

<details>

<summary>2019-02-08 07:18:07 - Systimator: A Design Space Exploration Methodology for Systolic Array based CNNs Acceleration on the FPGA-based Edge Nodes</summary>

- *Hazoor Ahmad, Muhammad Tanvir, Muhammad Abdullah Hanif, Muhammad Usama Javed, Rehan Hafiz, Muhammad Shafique*

- `1901.04986v2` - [abs](http://arxiv.org/abs/1901.04986v2) - [pdf](http://arxiv.org/pdf/1901.04986v2)

> The evolution of IoT based smart applications demand porting of artificial intelligence algorithms to the edge computing devices. CNNs form a large part of these AI algorithms. Systolic array based CNN acceleration is being widely advocated due its ability to allow scalable architectures. However, CNNs are inherently memory and compute intensive algorithms, and hence pose significant challenges to be implemented on the resource-constrained edge computing devices. Memory-constrained low-cost FPGA based devices form a substantial fraction of these edge computing devices. Thus, when porting to such edge-computing devices, the designer is left unguided as to how to select a suitable systolic array configuration that could fit in the available hardware resources. In this paper we propose Systimator, a design space exploration based methodology that provides a set of design points that can be mapped within the memory bounds of the target FPGA device. The methodology is based upon an analytical model that is formulated to estimate the required resources for systolic arrays, assuming multiple data reuse patterns. The methodology further provides the performance estimates for each of the candidate design points. We show that Systimator provides an in-depth analysis of resource-requirement of systolic array based CNNs. We provide our resource estimation results for porting of convolutional layers of TINY YOLO, a CNN based object detector, on a Xilinx ARTIX 7 FPGA.

</details>

<details>

<summary>2019-02-08 13:42:51 - Knowledge Graph Fact Prediction via Knowledge-Enriched Tensor Factorization</summary>

- *Ankur Padia, Kostantinos Kalpakis, Francis Ferraro, Tim Finin*

- `1902.03077v1` - [abs](http://arxiv.org/abs/1902.03077v1) - [pdf](http://arxiv.org/pdf/1902.03077v1)

> We present a family of novel methods for embedding knowledge graphs into real-valued tensors. These tensor-based embeddings capture the ordered relations that are typical in the knowledge graphs represented by semantic web languages like RDF. Unlike many previous models, our methods can easily use prior background knowledge provided by users or extracted automatically from existing knowledge graphs. In addition to providing more robust methods for knowledge graph embedding, we provide a provably-convergent, linear tensor factorization algorithm. We demonstrate the efficacy of our models for the task of predicting new facts across eight different knowledge graphs, achieving between 5% and 50% relative improvement over existing state-of-the-art knowledge graph embedding techniques. Our empirical evaluation shows that all of the tensor decomposition models perform well when the average degree of an entity in a graph is high, with constraint-based models doing better on graphs with a small number of highly similar relations and regularization-based models dominating for graphs with relations of varying degrees of similarity.

</details>

<details>

<summary>2019-02-08 15:27:24 - The List is the Process: Reliable Pre-Integration Tracking of Commits on Mailing Lists</summary>

- *Ralf Ramsauer, Daniel Lohmann, Wolfgang Mauerer*

- `1902.03147v1` - [abs](http://arxiv.org/abs/1902.03147v1) - [pdf](http://arxiv.org/pdf/1902.03147v1)

> A considerable corpus of research on software evolution focuses on mining changes in software repositories, but omits their pre-integration history.   We present a novel method for tracking this otherwise invisible evolution of software changes on mailing lists by connecting all early revisions of changes to their final version in repositories. Since artefact modifications on mailing lists are communicated by updates to fragments (i.e., patches) only, identifying semantically similar changes is a non-trivial task that our approach solves in a language-independent way. We evaluate our method on high-profile open source software (OSS) projects like the Linux kernel, and validate its high accuracy using an elaborately created ground truth.   Our approach can be used to quantify properties of OSS development processes, which is an essential requirement for using OSS in reliable or safety-critical industrial products, where certifiability and conformance to processes are crucial. The high accuracy of our technique allows, to the best of our knowledge, for the first time to quantitatively determine if an open development process effectively aligns with given formal process requirements.

</details>

<details>

<summary>2019-02-08 22:35:58 - A sequential guiding network with attention for image captioning</summary>

- *Daouda Sow, Zengchang Qin, Mouhamed Niasse, Tao Wan*

- `1811.00228v3` - [abs](http://arxiv.org/abs/1811.00228v3) - [pdf](http://arxiv.org/pdf/1811.00228v3)

> The recent advances of deep learning in both computer vision (CV) and natural language processing (NLP) provide us a new way of understanding semantics, by which we can deal with more challenging tasks such as automatic description generation from natural images. In this challenge, the encoder-decoder framework has achieved promising performance when a convolutional neural network (CNN) is used as image encoder and a recurrent neural network (RNN) as decoder. In this paper, we introduce a sequential guiding network that guides the decoder during word generation. The new model is an extension of the encoder-decoder framework with attention that has an additional guiding long short-term memory (LSTM) and can be trained in an end-to-end manner by using image/descriptions pairs. We validate our approach by conducting extensive experiments on a benchmark dataset, i.e., MS COCO Captions. The proposed model achieves significant improvement comparing to the other state-of-the-art deep learning models.

</details>

<details>

<summary>2019-02-09 18:20:54 - Classical linear logic, cobordisms and categorical semantics of categorial grammars</summary>

- *Sergey Slavnov*

- `1810.02047v6` - [abs](http://arxiv.org/abs/1810.02047v6) - [pdf](http://arxiv.org/pdf/1810.02047v6)

> We propose a categorial grammar based on classical multiplicative linear logic.   This can be seen as an extension of abstract categorial grammars (ACG) and is at least as expressive. However, constituents of {\it linear logic grammars (LLG)} are not abstract ${\lambda}$-terms, but simply tuples of words with labeled endpoints, we call them {\it multiwords}. At least, this gives a concrete and intuitive representation of ACG.   A key observation is that the class of multiwords has a fundamental algebraic structure. Namely, multiwords can be organized in a category, very similar to the category of topological cobordisms. This category is symmetric monoidal closed and compact closed and thus is a model of linear $\lambda$-calculus and classical linear logic. We think that this category is interesting on its own right. In particular, it might provide categorical representation for other formalisms.   On the other hand, many models of language semantics are based on commutative logic or, more generally, on symmetric monoidal closed categories. But the category of {\it word cobordisms} is a category of language elements, which is itself symmetric monoidal closed and independent of any grammar. Thus, it might prove useful in understanding language semantics as well.

</details>

<details>

<summary>2019-02-09 21:47:05 - Multilingual Neural Machine Translation With Soft Decoupled Encoding</summary>

- *Xinyi Wang, Hieu Pham, Philip Arthur, Graham Neubig*

- `1902.03499v1` - [abs](http://arxiv.org/abs/1902.03499v1) - [pdf](http://arxiv.org/pdf/1902.03499v1)

> Multilingual training of neural machine translation (NMT) systems has led to impressive accuracy improvements on low-resource languages. However, there are still significant challenges in efficiently learning word representations in the face of paucity of data. In this paper, we propose Soft Decoupled Encoding (SDE), a multilingual lexicon encoding framework specifically designed to share lexical-level information intelligently without requiring heuristic preprocessing such as pre-segmenting the data. SDE represents a word by its spelling through a character encoding, and its semantic meaning through a latent embedding space shared by all languages. Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs.

</details>

<details>

<summary>2019-02-10 06:27:25 - Task2Vec: Task Embedding for Meta-Learning</summary>

- *Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Stefano Soatto, Pietro Perona*

- `1902.03545v1` - [abs](http://arxiv.org/abs/1902.03545v1) - [pdf](http://arxiv.org/pdf/1902.03545v1)

> We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a "probe network" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar) We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.

</details>

<details>

<summary>2019-02-10 19:46:24 - Neural embeddings for metaphor detection in a corpus of Greek texts</summary>

- *Eirini Florou, Konstantinos Perifanos, Dionysis Goutsos*

- `1902.03659v1` - [abs](http://arxiv.org/abs/1902.03659v1) - [pdf](http://arxiv.org/pdf/1902.03659v1)

> One of the major challenges that NLP faces is metaphor detection, especially by automatic means, a task that becomes even more difficult for languages lacking in linguistic resources and tools. Our purpose is the automatic differentiation between literal and metaphorical meaning in authentic non-annotated phrases from the Corpus of Greek Texts by means of computational methods of machine learning. For this purpose the theoretical background of distributional semantics is discussed and employed. Distributional Semantics Theory develops concepts and methods for the quantification and classification of semantic similarities displayed by linguistic elements in large amounts of linguistic data according to their distributional properties. In accordance with this model, the approach followed in the thesis takes into account the linguistic context for the computation of the distributional representation of phrases in geometrical space, as well as for their comparison with the distributional representations of other phrases, whose function in speech is already "known" with the objective to reach conclusions about their literal or metaphorical function in the specific linguistic context. This procedure aims at dealing with the lack of linguistic resources for the Greek language, as the almost impossible up to now semantic comparison between "phrases", takes the form of an arithmetical comparison of their distributional representations in geometrical space.

</details>

<details>

<summary>2019-02-11 03:34:08 - Blockchain based Privacy-Preserving Software Updates with Proof-of-Delivery for Internet of Things</summary>

- *Yanqi Zhao, Yiming Liu, Yong Yu, Yannan Li*

- `1902.03712v1` - [abs](http://arxiv.org/abs/1902.03712v1) - [pdf](http://arxiv.org/pdf/1902.03712v1)

> A large number of IoT devices are connected via the Internet. However, most of these IoT devices are generally not perfect-by-design even have security weaknesses or vulnerabilities. Thus, it is essential to update these IoT devices securely, patching their vulnerabilities and protecting the safety of the involved users. Existing studies deliver secure and reliable updates based on blockchain network which serves as the transmission network. However, these approaches could compromise users privacy when updating the IoT devices.   In this paper, we propose a new blockchain based privacy-preserving software updates protocol, which delivers secure and reliable updates with an incentive mechanism, as well protects the privacy of involved users. The vendor delivers the updates and it makes a commitment by using a smart contract to provide financial incentive to the transmission nodes who deliver the updates to the IoT devices. A transmission node gets financial incentive by providing a proof-of-delivery. The transmission node uses double authentication preventing signature (DAPS) to carry out the fair exchange to obtain the proof-of-delivery. Specifically, the transmission node exchanges an attribute-based signature from a IoT device by using DAPS. Then, it uses the attribute-based signature as a proof-of-delivery to receive financial incentives. Generally, the IoT device has to execute complex computation for an attribute-based signature (ABS). It is intolerable for resource limited devices. We propose a concrete outsourced attribute-based signature (OABS) scheme to resist the weakness. Then, we prove the security of the proposed OABS and the protocol as well. Finally, we implement smart contract in Solidity to demonstrate the validity of the proposed protocol.

</details>

<details>

<summary>2019-02-11 08:14:34 - Latent Space Reinforcement Learning for Steering Angle Prediction</summary>

- *Qadeer Khan, Torsten Schön, Patrick Wenzel*

- `1902.03765v1` - [abs](http://arxiv.org/abs/1902.03765v1) - [pdf](http://arxiv.org/pdf/1902.03765v1)

> Model-free reinforcement learning has recently been shown to successfully learn navigation policies from raw sensor data. In this work, we address the problem of learning driving policies for an autonomous agent in a high-fidelity simulator. Building upon recent research that applies deep reinforcement learning to navigation problems, we present a modular deep reinforcement learning approach to predict the steering angle of the car from raw images. The first module extracts a low-dimensional latent semantic representation of the image. The control module trained with reinforcement learning takes the latent vector as input to predict the correct steering angle. The experimental results have showed that our method is capable of learning to maneuver the car without any human control signals.

</details>

<details>

<summary>2019-02-11 08:38:26 - Semantic Label Reduction Techniques for Autonomous Driving</summary>

- *Qadeer Khan, Torsten Schön, Patrick Wenzel*

- `1902.03777v1` - [abs](http://arxiv.org/abs/1902.03777v1) - [pdf](http://arxiv.org/pdf/1902.03777v1)

> Semantic segmentation maps can be used as input to models for maneuvering the controls of a car. However, not all labels may be necessary for making the control decision. One would expect that certain labels such as road lanes or sidewalks would be more critical in comparison with labels for vegetation or buildings which may not have a direct influence on the car's driving decision. In this appendix, we evaluate and quantify how sensitive and important the different semantic labels are for controlling the car. Labels that do not influence the driving decision are remapped to other classes, thereby simplifying the task by reducing to only labels critical for driving of the vehicle.

</details>

<details>

<summary>2019-02-11 14:39:38 - Achieving Secure and Efficient Cloud Search Services: Cross-Lingual Multi-Keyword Rank Search over Encrypted Cloud Data</summary>

- *Xueyan Liu, Zhitao Guan, Longfei Wu, Zain Ul Abedin, Mohsen Guizani*

- `1902.03902v1` - [abs](http://arxiv.org/abs/1902.03902v1) - [pdf](http://arxiv.org/pdf/1902.03902v1)

> Multi-user multi-keyword ranked search scheme in arbitrary language is a novel multi-keyword rank searchable encryption (MRSE) framework based on Paillier Cryptosystem with Threshold Decryption (PCTD). Compared to previous MRSE schemes constructed based on the k-nearest neighbor searcha-ble encryption (KNN-SE) algorithm, it can mitigate some draw-backs and achieve better performance in terms of functionality and efficiency. Additionally, it does not require a predefined keyword set and support keywords in arbitrary languages. However, due to the pattern of exact matching of keywords in the new MRSE scheme, multilingual search is limited to each language and cannot be searched across languages. In this pa-per, we propose a cross-lingual multi-keyword rank search (CLRSE) scheme which eliminates the barrier of languages and achieves semantic extension with using the Open Multilingual Wordnet. Our CLRSE scheme also realizes intelligent and per-sonalized search through flexible keyword and language prefer-ence settings. We evaluate the performance of our scheme in terms of security, functionality, precision and efficiency, via extensive experiments.

</details>

<details>

<summary>2019-02-12 12:40:43 - Representation Learning for Heterogeneous Information Networks via Embedding Events</summary>

- *Guoji Fu, Bo Yuan, Qiqi Duan, Xin Yao*

- `1901.10234v2` - [abs](http://arxiv.org/abs/1901.10234v2) - [pdf](http://arxiv.org/pdf/1901.10234v2)

> Network representation learning (NRL) has been widely used to help analyze large-scale networks through mapping original networks into a low-dimensional vector space. However, existing NRL methods ignore the impact of properties of relations on the object relevance in heterogeneous information networks (HINs). To tackle this issue, this paper proposes a new NRL framework, called Event2vec, for HINs to consider both quantities and properties of relations during the representation learning process. Specifically, an event (i.e., a complete semantic unit) is used to represent the relation among multiple objects, and both event-driven first-order and second-order proximities are defined to measure the object relevance according to the quantities and properties of relations. We theoretically prove how event-driven proximities can be preserved in the embedding space by Event2vec, which utilizes event embeddings to facilitate learning the object embeddings. Experimental studies demonstrate the advantages of Event2vec over state-of-the-art algorithms on four real-world datasets and three network analysis tasks (including network reconstruction, link prediction, and node classification).

</details>

<details>

<summary>2019-02-12 16:16:36 - Datalog: Bag Semantics via Set Semantics</summary>

- *Leopoldo Bertossi, Georg Gottlob, Reinhard Pichler*

- `1803.06445v3` - [abs](http://arxiv.org/abs/1803.06445v3) - [pdf](http://arxiv.org/pdf/1803.06445v3)

> Duplicates in data management are common and problematic. In this work, we present a translation of Datalog under bag semantics into a well-behaved extension of Datalog, the so-called {\em warded Datalog}$^\pm$, under set semantics. From a theoretical point of view, this allows us to reason on bag semantics by making use of the well-established theoretical foundations of set semantics. From a practical point of view, this allows us to handle the bag semantics of Datalog by powerful, existing query engines for the required extension of Datalog. This use of Datalog$^\pm$ is extended to give a set semantics to duplicates in Datalog$^\pm$ itself. We investigate the properties of the resulting Datalog$^\pm$ programs, the problem of deciding multiplicities, and expressibility of some bag operations. Moreover, the proposed translation has the potential for interesting applications such as to Multiset Relational Algebra and the semantic web query language SPARQL with bag semantics.

</details>

<details>

<summary>2019-02-13 16:43:32 - Wasserstein Barycenter Model Ensembling</summary>

- *Pierre Dognin, Igor Melnyk, Youssef Mroueh, Jerret Ross, Cicero Dos Santos, Tom Sercu*

- `1902.04999v1` - [abs](http://arxiv.org/abs/1902.04999v1) - [pdf](http://arxiv.org/pdf/1902.04999v1)

> In this paper we propose to perform model ensembling in a multiclass or a multilabel learning setting using Wasserstein (W.) barycenters. Optimal transport metrics, such as the Wasserstein distance, allow incorporating semantic side information such as word embeddings. Using W. barycenters to find the consensus between models allows us to balance confidence and semantics in finding the agreement between the models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results show that the W. ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling.

</details>

<details>

<summary>2019-02-14 01:20:52 - Spectre is here to stay: An analysis of side-channels and speculative execution</summary>

- *Ross Mcilroy, Jaroslav Sevcik, Tobias Tebbi, Ben L. Titzer, Toon Verwaest*

- `1902.05178v1` - [abs](http://arxiv.org/abs/1902.05178v1) - [pdf](http://arxiv.org/pdf/1902.05178v1)

> The recent discovery of the Spectre and Meltdown attacks represents a watershed moment not just for the field of Computer Security, but also of Programming Languages. This paper explores speculative side-channel attacks and their implications for programming languages. These attacks leak information through micro-architectural side-channels which we show are not mere bugs, but in fact lie at the foundation of optimization. We identify three open problems, (1) finding side-channels, (2) understanding speculative vulnerabilities, and (3) mitigating them. For (1) we introduce a mathematical meta-model that clarifies the source of side-channels in simulations and CPUs. For (2) we introduce an architectural model with speculative semantics to study recently-discovered vulnerabilities. For (3) we explore and evaluate software mitigations and prove one correct for this model. Our analysis is informed by extensive offensive research and defensive implementation work for V8, the production JavaScript virtual machine in Chrome. Straightforward extensions to model real hardware suggest these vulnerabilities present formidable challenges for effective, efficient mitigation. As a result of our work, we now believe that speculative vulnerabilities on today's hardware defeat all language-enforced confidentiality with no known comprehensive software mitigations, as we have discovered that untrusted code can construct a universal read gadget to read all memory in the same address space through side-channels. In the face of this reality, we have shifted the security model of the Chrome web browser and V8 to process isolation.

</details>

<details>

<summary>2019-02-14 04:55:28 - Graph-RISE: Graph-Regularized Image Semantic Embedding</summary>

- *Da-Cheng Juan, Chun-Ta Lu, Zhen Li, Futang Peng, Aleksei Timofeev, Yi-Ting Chen, Yaxi Gao, Tom Duerig, Andrew Tomkins, Sujith Ravi*

- `1902.10814v1` - [abs](http://arxiv.org/abs/1902.10814v1) - [pdf](http://arxiv.org/pdf/1902.10814v1)

> Learning image representations to capture fine-grained semantics has been a challenging and important task enabling many applications such as image search and clustering. In this paper, we present Graph-Regularized Image Semantic Embedding (Graph-RISE), a large-scale neural graph learning framework that allows us to train embeddings to discriminate an unprecedented O(40M) ultra-fine-grained semantic labels. Graph-RISE outperforms state-of-the-art image embedding algorithms on several evaluation tasks, including image classification and triplet ranking. We provide case studies to demonstrate that, qualitatively, image retrieval based on Graph-RISE effectively captures semantics and, compared to the state-of-the-art, differentiates nuances at levels that are closer to human-perception.

</details>

<details>

<summary>2019-02-14 20:00:30 - Author Profiling for Hate Speech Detection</summary>

- *Pushkar Mishra, Marco Del Tredici, Helen Yannakoudakis, Ekaterina Shutova*

- `1902.06734v1` - [abs](http://arxiv.org/abs/1902.06734v1) - [pdf](http://arxiv.org/pdf/1902.06734v1)

> The rapid growth of social media in recent years has fed into some highly undesirable phenomena such as proliferation of abusive and offensive language on the Internet. Previous research suggests that such hateful content tends to come from users who share a set of common stereotypes and form communities around them. The current state-of-the-art approaches to hate speech detection are oblivious to user and community information and rely entirely on textual (i.e., lexical and semantic) cues. In this paper, we propose a novel approach to this problem that incorporates community-based profiling features of Twitter users. Experimenting with a dataset of 16k tweets, we show that our methods significantly outperform the current state of the art in hate speech detection. Further, we conduct a qualitative analysis of model characteristics. We release our code, pre-trained models and all the resources used in the public domain.

</details>

<details>

<summary>2019-02-14 20:23:43 - Variability Abstraction and Refinement for Game-based Lifted Model Checking of full CTL (Extended Version)</summary>

- *Aleksandar S. Dimovski, Axel Legay, Andrzej Wasowski*

- `1902.05594v1` - [abs](http://arxiv.org/abs/1902.05594v1) - [pdf](http://arxiv.org/pdf/1902.05594v1)

> Variability models allow effective building of many custom model variants for various configurations. Lifted model checking for a variability model is capable of verifying all its variants simultaneously in a single run by exploiting the similarities between the variants. The computational cost of lifted model checking still greatly depends on the number of variants (the size of configuration space), which is often huge.   One of the most promising approaches to fighting the configuration space explosion problem in lifted model checking are variability abstractions. In this work, we define a novel game-based approach for variability-specific abstraction and refinement for lifted model checking of the full CTL, interpreted over 3-valued semantics. We propose a direct algorithm for solving a 3-valued (abstract) lifted model checking game. In case the result of model checking an abstract variability model is indefinite, we suggest a new notion of refinement, which eliminates indefinite results. This provides an iterative incremental variability-specific abstraction and refinement framework, where refinement is applied only where indefinite results exist and definite results from previous iterations are reused.

</details>

<details>

<summary>2019-02-15 09:36:18 - Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space</summary>

- *Berta Bescos, José Neira, Roland Siegwart, Cesar Cadena*

- `1809.10239v2` - [abs](http://arxiv.org/abs/1809.10239v2) - [pdf](http://arxiv.org/pdf/1809.10239v2)

> In this paper we present an end-to-end deep learning framework to turn images that show dynamic content, such as vehicles or pedestrians, into realistic static frames. This objective encounters two main challenges: detecting all the dynamic objects, and inpainting the static occluded background with plausible imagery. The second problem is approached with a conditional generative adversarial model that, taking as input the original dynamic image and its dynamic/static binary mask, is capable of generating the final static image. The former challenge is addressed by the use of a convolutional network that learns a multi-class semantic segmentation of the image.   These generated images can be used for applications such as augmented reality or vision-based robot localization purposes. To validate our approach, we show both qualitative and quantitative comparisons against other state-of-the-art inpainting methods by removing the dynamic objects and hallucinating the static structure behind them. Furthermore, to demonstrate the potential of our results, we carry out pilot experiments that show the benefits of our proposal for visual place recognition.

</details>

<details>

<summary>2019-02-15 11:24:37 - A Somewhat Homomorphic Encryption Scheme based on Multivariate Polynomial Evaluation</summary>

- *Uddipana Dowerah, Srinivasan Krishnaswamy*

- `1902.05771v1` - [abs](http://arxiv.org/abs/1902.05771v1) - [pdf](http://arxiv.org/pdf/1902.05771v1)

> We propose a symmetric key homomorphic encryption scheme based on the evaluation of multivariate polynomials over a finite field. The proposed scheme is somewhat homomorphic with respect to addition and multiplication. Further, we define a generalization of the Learning with Errors problem called the Hidden Subspace Membership problem and show that the semantic security of the proposed scheme can be reduced to the hardness of this problem.

</details>

<details>

<summary>2019-02-15 15:59:50 - AVATAR : Fixing Semantic Bugs with Fix Patterns of Static Analysis Violations</summary>

- *Kui Liu, Anil Koyuncu, Dongsun Kim, Tegawendé F. Bisyandé*

- `1812.07270v3` - [abs](http://arxiv.org/abs/1812.07270v3) - [pdf](http://arxiv.org/pdf/1812.07270v3)

> Fix pattern-based patch generation is a promising direction in Automated Program Repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this paper, we propose to investigate the possibility in an APR scenario of leveraging code changes that address violations by static bug detection tools. To that end, we build the AVATAR APR system, which exploits fix patterns of static analysis violations as ingredients for patch generation. Evaluated on the Defects4J benchmark, we show that, assuming a perfect localization of faults, AVATAR can generate correct patches to fix 34/39 bugs. We further find that AVATAR yields performance metrics that are comparable to that of the closely-related approaches in the literature. While AVATAR outperforms many of the state-of-the-art pattern-based APR systems, it is mostly complementary to current approaches. Overall, our study highlights the relevance of static bug finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases.

</details>

<details>

<summary>2019-02-15 16:25:36 - You Shall Know the Most Frequent Sense by the Company it Keeps</summary>

- *Bradley Hauer, Yixing Luan, Grzegorz Kondrak*

- `1808.06729v3` - [abs](http://arxiv.org/abs/1808.06729v3) - [pdf](http://arxiv.org/pdf/1808.06729v3)

> Identification of the most frequent sense of a polysemous word is an important semantic task. We introduce two concepts that can benefit MFS detection: companions, which are the most frequently co-occurring words, and the most frequent translation in a bitext. We present two novel methods that incorporate these new concepts, and show that they advance the state of the art on MFS detection.

</details>

<details>

<summary>2019-02-15 22:54:32 - Improving Semantic Parsing for Task Oriented Dialog</summary>

- *Arash Einolghozati, Panupong Pasupat, Sonal Gupta, Rushin Shah, Mrinal Mohit, Mike Lewis, Luke Zettlemoyer*

- `1902.06000v1` - [abs](http://arxiv.org/abs/1902.06000v1) - [pdf](http://arxiv.org/pdf/1902.06000v1)

> Semantic parsing using hierarchical representations has recently been proposed for task oriented dialog with promising results [Gupta et al 2018]. In this paper, we present three different improvements to the model: contextualized embeddings, ensembling, and pairwise re-ranking based on a language model. We taxonomize the errors possible for the hierarchical representation, such as wrong top intent, missing spans or split spans, and show that the three approaches correct different kinds of errors. The best model combines the three techniques and gives 6.4% better exact match accuracy than the state-of-the-art, with an error reduction of 33%, resulting in a new state-of-the-art result on the Task Oriented Parsing (TOP) dataset.

</details>

<details>

<summary>2019-02-16 19:51:31 - Compiled Obfuscation for Data Structures in Encrypted Computing</summary>

- *Peter T. Breuer*

- `1902.06146v1` - [abs](http://arxiv.org/abs/1902.06146v1) - [pdf](http://arxiv.org/pdf/1902.06146v1)

> Encrypted computing is an emerging technology based on a processor that `works encrypted', taking encrypted inputs to encrypted outputs while data remains in encrypted form throughout. It aims to secure user data against possible insider attacks by the operator and operating system (who do not know the user's encryption key and cannot access it in the processor). Formally `obfuscating' compilation for encrypted computing is such that on each recompilation of the source code, machine code of the same structure is emitted for which runtime traces also all have the same structure but each word beneath the encryption differs from nominal with maximal possible entropy across recompilations. That generates classic cryptographic semantic security for data, relative to the security of the encryption, but it guarantees only single words and an adversary has more than that on which to base decryption attempts. This paper extends the existing integer-based technology to doubles, floats, arrays, structs and unions as data structures, covering ANSI C. A single principle drives compiler design and improves the existing security theory to quantitative results: every arithmetic instruction that writes must vary to the maximal extent possible.

</details>

<details>

<summary>2019-02-17 03:17:03 - An Automated Testing Framework for Conversational Agents</summary>

- *Soodeh Atefi, Mohammad Amin Alipour*

- `1902.06193v1` - [abs](http://arxiv.org/abs/1902.06193v1) - [pdf](http://arxiv.org/pdf/1902.06193v1)

> Conversational agents are systems with a conversational interface that afford interaction in spoken language. These systems are becoming prevalent and are preferred in various contexts and for many users. Despite their increasing success, the automated testing infrastructure to support the effective and efficient development of such systems compared to traditional software systems is still limited. Automated testing framework for conversational systems can improve the quality of these systems by assisting developers to write, execute, and maintain test cases. In this paper, we introduce our work-in-progress automated testing framework, and its realization in the Python programming language. We discuss some research problems in the development of such an automated testing framework for conversational agents. In particular, we point out the problems of the specification of the expected behavior, known as test oracles, and semantic comparison of utterances.

</details>

<details>

<summary>2019-02-18 01:22:02 - Evolutionary Multitasking for Semantic Web Service Composition</summary>

- *Chen Wang, Hui Ma, Gang Chen, Sven Hartmann*

- `1902.06370v1` - [abs](http://arxiv.org/abs/1902.06370v1) - [pdf](http://arxiv.org/pdf/1902.06370v1)

> Web services are basic functions of a software system to support the concept of service-oriented architecture. They are often composed together to provide added values, known as web service composition. Researchers often employ Evolutionary Computation techniques to efficiently construct composite services with near-optimized functional quality (i.e., Quality of Semantic Matchmaking) or non-functional quality (i.e., Quality of Service) or both due to the complexity of this problem. With a significant increase in service composition requests, many composition requests have similar input and output requirements but may vary due to different preferences from different user segments. This problem is often treated as a multi-objective service composition so as to cope with different preferences from different user segments simultaneously. Without taking a multi-objective approach that gives rise to a solution selection challenge, we perceive multiple similar service composition requests as jointly forming an evolutionary multi-tasking problem in this work. We propose an effective permutation-based evolutionary multi-tasking approach that can simultaneously generate a set of solutions, with one for each service request. We also introduce a neighborhood structure over multiple tasks to allow newly evolved solutions to be evaluated on related tasks. Our proposed method can perform better at the cost of only a fraction of time, compared to one state-of-art single-tasking EC-based method. We also found that the use of the proper neighborhood structure can enhance the effectiveness of our approach.

</details>

<details>

<summary>2019-02-18 13:53:00 - Binary Debloating for Security via Demand Driven Loading</summary>

- *Girish Mururu, Chris Porter, Prithayan Barua, Santosh Pande*

- `1902.06570v1` - [abs](http://arxiv.org/abs/1902.06570v1) - [pdf](http://arxiv.org/pdf/1902.06570v1)

> Modern software systems heavily use C/C++ based libraries. Because of the weak memory model of C/C++, libraries may suffer from vulnerabilities which can expose the applications to potential attacks. For example, a very large number of return oriented programming gadgets exist in glibc that allow stitching together semantically valid but malicious Turing-complete programs. In spite of significant advances in attack detection and mitigation, full defense is unrealistic against an ever-growing set of possibilities for generating such malicious programs.   In this work, we create a defense mechanism by debloating libraries to reduce the dynamic functions linked so that the possibilities of constructing malicious programs diminishes significantly. The key idea is to locate each library call site within an application, and in each case to load only the set of library functions that will be used at that call site. This approach of demand-driven loading relies on an input-aware oracle that predicts a near-exact set of library functions needed at a given call site during the execution. The predicted functions are loaded just in time, and the complete call chain (of function bodies) inside the library is purged after returning from the library call back into the application. We present a decision-tree based predictor, which acts as an oracle, and an optimized runtime system, which works directly with library binaries like GNU libc and libstdc++. We show that on average, the proposed scheme cuts the exposed code surface of libraries by 97.2%, reduces ROP gadgets present in linked libraries by 97.9%, achieves a prediction accuracy in most cases of at least 97%, and adds a small runtime overhead of 18% on all libraries (16% for glibc, 2% for others) across all benchmarks of SPEC 2006, suggesting this scheme is practical.

</details>

<details>

<summary>2019-02-18 16:13:22 - Appendix for: Cut-free Calculi and Relational Semantics for Temporal STIT logics</summary>

- *Kees van Berkel, Tim Lyon*

- `1902.06632v1` - [abs](http://arxiv.org/abs/1902.06632v1) - [pdf](http://arxiv.org/pdf/1902.06632v1)

> This paper is an appendix to the paper "Cut-free Calculi and Relational Semantics for Temporal STIT logics" by Berkel and Lyon, 2019. It provides the completeness proof for the basic STIT logic Ldm (relative to irreflexive, temporal Kripke STIT frames) as well as gives the derivation of the independence of agents axiom for the logic Xstit.

</details>

<details>

<summary>2019-02-18 18:57:05 - DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching</summary>

- *Fereshteh Sadeghi*

- `1902.05947v1` - [abs](http://arxiv.org/abs/1902.05947v1) - [pdf](http://arxiv.org/pdf/1902.05947v1)

> Robots should understand both semantics and physics to be functional in the real world. While robot platforms provide means for interacting with the physical world they cannot autonomously acquire object-level semantics without needing human. In this paper, we investigate how to minimize human effort and intervention to teach robots perform real world tasks that incorporate semantics. We study this question in the context of visual servoing of mobile robots and propose DIViS, a Domain Invariant policy learning approach for collision free Visual Servoing. DIViS incorporates high level semantics from previously collected static human-labeled datasets and learns collision free servoing entirely in simulation and without any real robot data. However, DIViS can directly be deployed on a real robot and is capable of servoing to the user-specified object categories while avoiding collisions in the real world. DIViS is not constrained to be queried by the final view of goal but rather is robust to servo to image goals taken from initial robot view with high occlusions without this impairing its ability to maintain a collision free path. We show the generalization capability of DIViS on real mobile robots in more than 90 real world test scenarios with various unseen object goals in unstructured environments. DIViS is compared to prior approaches via real world experiments and rigorous tests in simulation. For supplementary videos, see: \href{https://fsadeghi.github.io/DIViS}{https://fsadeghi.github.io/DIViS}

</details>

<details>

<summary>2019-02-18 21:32:32 - Detecting Standard Violation Errors in Smart Contracts</summary>

- *Ao Li, Fan Long*

- `1812.07702v2` - [abs](http://arxiv.org/abs/1812.07702v2) - [pdf](http://arxiv.org/pdf/1812.07702v2)

> We present SOLAR, a new analysis tool for automatically detecting standard violation errors in Ethereum smart contracts.Given the Ethereum Virtual Machine (EVM) bytecode of a smart contract and a user specified constraint or invariant derived from a technical standard such as ERC-20,SOLAR symbolically executes the contract, explores all possible execution paths, and checks whether it is possible to initiate a sequence of malicious transactions to violate the specified constraint or invariant. Our experimental results highlight the effectiveness of SOLAR in finding new errors in smart con-tracts. Out of the evaluated 779 ERC-20 and 310 ERC-721smart contracts, SOLAR found 255 standard violation errors in 197 vulnerable contracts with only three false positives.237 out of the 255 errors are zero-day errors that are not re-ported before. Our results sound the alarm on the prevalence of standard violation errors in critical smart contracts that manipulate publicly traded digital assets

</details>

<details>

<summary>2019-02-19 07:57:27 - Triple Trustworthiness Measurement for Knowledge Graph</summary>

- *Shengbin Jia, Yang Xiang, Xiaojun Chen*

- `1809.09414v3` - [abs](http://arxiv.org/abs/1809.09414v3) - [pdf](http://arxiv.org/pdf/1809.09414v3)

> The Knowledge graph (KG) uses the triples to describe the facts in the real world. It has been widely used in intelligent analysis and applications. However, possible noises and conflicts are inevitably introduced in the process of constructing. And the KG based tasks or applications assume that the knowledge in the KG is completely correct and inevitably bring about potential deviations. In this paper, we establish a knowledge graph triple trustworthiness measurement model that quantify their semantic correctness and the true degree of the facts expressed. The model is a crisscrossing neural network structure. It synthesizes the internal semantic information in the triples and the global inference information of the KG to achieve the trustworthiness measurement and fusion in the three levels of entity level, relationship level, and KG global level. We analyzed the validity of the model output confidence values, and conducted experiments in the real-world dataset FB15K (from Freebase) for the knowledge graph error detection task. The experimental results showed that compared with other models, our model achieved significant and consistent improvements.

</details>

<details>

<summary>2019-02-19 08:33:04 - Efficient Memory Management for GPU-based Deep Learning Systems</summary>

- *Junzhe Zhang, Sai Ho Yeung, Yao Shu, Bingsheng He, Wei Wang*

- `1903.06631v1` - [abs](http://arxiv.org/abs/1903.06631v1) - [pdf](http://arxiv.org/pdf/1903.06631v1)

> GPU (graphics processing unit) has been used for many data-intensive applications. Among them, deep learning systems are one of the most important consumer systems for GPU nowadays. As deep learning applications impose deeper and larger models in order to achieve higher accuracy, memory management becomes an important research topic for deep learning systems, given that GPU has limited memory size. Many approaches have been proposed towards this issue, e.g., model compression and memory swapping. However, they either degrade the model accuracy or require a lot of manual intervention. In this paper, we propose two orthogonal approaches to reduce the memory cost from the system perspective. Our approaches are transparent to the models, and thus do not affect the model accuracy. They are achieved by exploiting the iterative nature of the training algorithm of deep learning to derive the lifetime and read/write order of all variables. With the lifetime semantics, we are able to implement a memory pool with minimal fragments. However, the optimization problem is NP-complete. We propose a heuristic algorithm that reduces up to 13.3% of memory compared with Nvidia's default memory pool with equal time complexity. With the read/write semantics, the variables that are not in use can be swapped out from GPU to CPU to reduce the memory footprint. We propose multiple swapping strategies to automatically decide which variable to swap and when to swap out (in), which reduces the memory cost by up to 34.2% without communication overhead.

</details>

<details>

<summary>2019-02-19 21:03:35 - Semantic Neural Machine Translation using AMR</summary>

- *Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, Jinsong Su*

- `1902.07282v1` - [abs](http://arxiv.org/abs/1902.07282v1) - [pdf](http://arxiv.org/pdf/1902.07282v1)

> It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (short for abstract meaning representation) on NMT. Experiments on a standard English-to-German dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based sequence-to-sequence neural translation model.

</details>

<details>

<summary>2019-02-19 22:32:29 - Multimodal Grounding for Sequence-to-Sequence Speech Recognition</summary>

- *Ozan Caglayan, Ramon Sanabria, Shruti Palaskar, Loïc Barrault, Florian Metze*

- `1811.03865v2` - [abs](http://arxiv.org/abs/1811.03865v2) - [pdf](http://arxiv.org/pdf/1811.03865v2)

> Humans are capable of processing speech by making use of multiple sensory modalities. For example, the environment where a conversation takes place generally provides semantic and/or acoustic context that helps us to resolve ambiguities or to recall named entities. Motivated by this, there have been many works studying the integration of visual information into the speech recognition pipeline. Specifically, in our previous work, we propose a multistep visual adaptive training approach which improves the accuracy of an audio-based Automatic Speech Recognition (ASR) system. This approach, however, is not end-to-end as it requires fine-tuning the whole model with an adaptation layer. In this paper, we propose novel end-to-end multimodal ASR systems and compare them to the adaptive approach by using a range of visual representations obtained from state-of-the-art convolutional neural networks. We show that adaptive training is effective for S2S models leading to an absolute improvement of 1.4% in word error rate. As for the end-to-end systems, although they perform better than baseline, the improvements are slightly less than adaptive training, 0.8 absolute WER reduction in single-best models. Using ensemble decoding, end-to-end models reach a WER of 15% which is the lowest score among all systems.

</details>

<details>

<summary>2019-02-20 19:20:44 - Founded World Views with Autoepistemic Equilibrium Logic</summary>

- *Pedro Cabalar, Jorge Fandinno, Luis Fariñas*

- `1902.07741v1` - [abs](http://arxiv.org/abs/1902.07741v1) - [pdf](http://arxiv.org/pdf/1902.07741v1)

> Defined by Gelfond in 1991 (G91), epistemic specifications (or programs) are an extension of logic programming under stable models semantics that introducessubjective literals. A subjective literal al-lows checking whether some regular literal is true in all (or in some of) the stable models of the program, being those models collected in a setcalledworld view. One epistemic program may yield several world views but, under the original G91 semantics, some of them resulted from self-supported derivations. During the last eight years, several alternative approaches have been proposed to get rid of these self-supported worldviews. Unfortunately, their success could only be measured by studying their behaviour on a set of common examples in the literature, since no formal property of "self-supportedness" had been defined. To fill this gap, we extend in this paper the idea of unfounded set from standard logic programming to the epistemic case. We define when a world view is founded with respect to some program and propose the foundedness property for any semantics whose world views are always founded. Using counterexamples, we explain that the previous approaches violate foundedness, and proceed to propose a new semantics based on a combination of Moore's Autoepistemic Logic and Pearce's Equilibrium Logic. The main result proves that this new semantics precisely captures the set of founded G91 world views.

</details>

<details>

<summary>2019-02-20 23:59:26 - Supporting Very Large Models using Automatic Dataflow Graph Partitioning</summary>

- *Minjie Wang, Chien-chin Huang, Jinyang Li*

- `1807.08887v2` - [abs](http://arxiv.org/abs/1807.08887v2) - [pdf](http://arxiv.org/pdf/1807.08887v2)

> This paper presents Tofu, a system that partitions very large DNN models across multiple GPU devices to reduce per-GPU memory footprint. Tofu is designed to partition a dataflow graph of fine-grained tensor operators in order to work transparently with a general-purpose deep learning platform like MXNet. In order to automatically partition each operator, we propose to describe the semantics of an operator in a simple language which represents tensors as lambda functions mapping from tensor coordinates to values. To optimally partition different operators in a dataflow graph, Tofu uses a recursive search algorithm that minimizes the total communication cost. Our experiments on an 8-GPU machine show that Tofu enables the training of very large CNN and RNN models. It also achieves 25% - 400% speedup over alternative approaches to train very large models.

</details>

<details>

<summary>2019-02-21 01:12:07 - Predicting ConceptNet Path Quality Using Crowdsourced Assessments of Naturalness</summary>

- *Yilun Zhou, Steven Schockaert, Julie A. Shah*

- `1902.07831v1` - [abs](http://arxiv.org/abs/1902.07831v1) - [pdf](http://arxiv.org/pdf/1902.07831v1)

> In many applications, it is important to characterize the way in which two concepts are semantically related. Knowledge graphs such as ConceptNet provide a rich source of information for such characterizations by encoding relations between concepts as edges in a graph. When two concepts are not directly connected by an edge, their relationship can still be described in terms of the paths that connect them. Unfortunately, many of these paths are uninformative and noisy, which means that the success of applications that use such path features crucially relies on their ability to select high-quality paths. In existing applications, this path selection process is based on relatively simple heuristics. In this paper we instead propose to learn to predict path quality from crowdsourced human assessments. Since we are interested in a generic task-independent notion of quality, we simply ask human participants to rank paths according to their subjective assessment of the paths' naturalness, without attempting to define naturalness or steering the participants towards particular indicators of quality. We show that a neural network model trained on these assessments is able to predict human judgments on unseen paths with near optimal performance. Most notably, we find that the resulting path selection method is substantially better than the current heuristic approaches at identifying meaningful paths.

</details>

<details>

<summary>2019-02-21 02:02:13 - Large-Scale Visual Active Learning with Deep Probabilistic Ensembles</summary>

- *Kashyap Chitta, Jose M. Alvarez, Adam Lesnikowski*

- `1811.03575v3` - [abs](http://arxiv.org/abs/1811.03575v3) - [pdf](http://arxiv.org/pdf/1811.03575v3)

> Annotating the right data for training deep neural networks is an important challenge. Active learning using uncertainty estimates from Bayesian Neural Networks (BNNs) could provide an effective solution to this. Despite being theoretically principled, BNNs require approximations to be applied to large-scale problems, where both performance and uncertainty estimation are crucial. In this paper, we introduce Deep Probabilistic Ensembles (DPEs), a scalable technique that uses a regularized ensemble to approximate a deep BNN. We conduct a series of large-scale visual active learning experiments to evaluate DPEs on classification with the CIFAR-10, CIFAR-100 and ImageNet datasets, and semantic segmentation with the BDD100k dataset. Our models require significantly less training data to achieve competitive performances, and steadily improve upon strong active learning baselines as the annotation budget is increased.

</details>

<details>

<summary>2019-02-21 11:09:18 - Deep Discriminative Representation Learning with Attention Map for Scene Classification</summary>

- *Jun Li, Daoyu Lin, Yang Wang, Guangluan Xu, Chibiao Ding*

- `1902.07967v1` - [abs](http://arxiv.org/abs/1902.07967v1) - [pdf](http://arxiv.org/pdf/1902.07967v1)

> Learning powerful discriminative features for remote sensing image scene classification is a challenging computer vision problem. In the past, most classification approaches were based on handcrafted features. However, most recent approaches to remote sensing scene classification are based on Convolutional Neural Networks (CNNs). The de facto practice when learning these CNN models is only to use original RGB patches as input with training performed on large amounts of labeled data (ImageNet). In this paper, we show class activation map (CAM) encoded CNN models, codenamed DDRL-AM, trained using original RGB patches and attention map based class information provide complementary information to the standard RGB deep models. To the best of our knowledge, we are the first to investigate attention information encoded CNNs. Additionally, to enhance the discriminability, we further employ a recently developed object function called "center loss," which has proved to be very useful in face recognition. Finally, our framework provides attention guidance to the model in an end-to-end fashion. Extensive experiments on two benchmark datasets show that our approach matches or exceeds the performance of other methods.

</details>

<details>

<summary>2019-02-21 13:25:00 - A complete formalized knowledge representation model for advanced digital forensics timeline analysis</summary>

- *Yoan Chabot, Aurélie Bertaux, Christophe Nicollea, Tahar Kechadi*

- `1903.01396v1` - [abs](http://arxiv.org/abs/1903.01396v1) - [pdf](http://arxiv.org/pdf/1903.01396v1)

> Having a clear view of events that occurred over time is a difficult objective to achieve in digital investigations (DI). Event reconstruction, which allows investigators to understand the timeline of a crime, is one of the most important step of a DI process. This complex task requires exploration of a large amount of events due to the pervasiveness of new technologies nowadays. Any evidence produced at the end of the investigative process must also meet the requirements of the courts, such as reproducibility, verifiability, validation, etc. For this purpose, we propose a new methodology, supported by theoretical concepts, that can assist investigators through the whole process including the construction and the interpretation of the events describing the case. The proposed approach is based on a model which integrates knowledge of experts from the fields of digital forensics and software development to allow a semantically rich representation of events related to the incident. The main purpose of this model is to allow the analysis of these events in an automatic and efficient way. This paper describes the approach and then focuses on the main conceptual and formal aspects: a formal incident modelization and operators for timeline reconstruction and analysis.

</details>

<details>

<summary>2019-02-21 13:50:56 - Deep Short Text Classification with Knowledge Powered Attention</summary>

- *Jindong Chen, Yizhou Hu, Jingping Liu, Yanghua Xiao, Haiyun Jiang*

- `1902.08050v1` - [abs](http://arxiv.org/abs/1902.08050v1) - [pdf](http://arxiv.org/pdf/1902.08050v1)

> Short text classification is one of important tasks in Natural Language Processing (NLP). Unlike paragraphs or documents, short texts are more ambiguous since they have not enough contextual information, which poses a great challenge for classification. In this paper, we retrieve knowledge from external knowledge source to enhance the semantic representation of short texts. We take conceptual information as a kind of knowledge and incorporate it into deep neural networks. For the purpose of measuring the importance of knowledge, we introduce attention mechanisms and propose deep Short Text Classification with Knowledge powered Attention (STCKA). We utilize Concept towards Short Text (C- ST) attention and Concept towards Concept Set (C-CS) attention to acquire the weight of concepts from two aspects. And we classify a short text with the help of conceptual information. Unlike traditional approaches, our model acts like a human being who has intrinsic ability to make decisions based on observation (i.e., training data for machines) and pays more attention to important knowledge. We also conduct extensive experiments on four public datasets for different tasks. The experimental results and case studies show that our model outperforms the state-of-the-art methods, justifying the effectiveness of knowledge powered attention.

</details>

<details>

<summary>2019-02-21 19:00:30 - Towards Visually Grounded Sub-Word Speech Unit Discovery</summary>

- *David Harwath, James Glass*

- `1902.08213v1` - [abs](http://arxiv.org/abs/1902.08213v1) - [pdf](http://arxiv.org/pdf/1902.08213v1)

> In this paper, we investigate the manner in which interpretable sub-word speech units emerge within a convolutional neural network model trained to associate raw speech waveforms with semantically related natural image scenes. We show how diphone boundaries can be superficially extracted from the activation patterns of intermediate layers of the model, suggesting that the model may be leveraging these events for the purpose of word recognition. We present a series of experiments investigating the information encoded by these events.

</details>

<details>

<summary>2019-02-21 20:54:51 - Latent Translation: Crossing Modalities by Bridging Generative Models</summary>

- *Yingtao Tian, Jesse Engel*

- `1902.08261v1` - [abs](http://arxiv.org/abs/1902.08261v1) - [pdf](http://arxiv.org/pdf/1902.08261v1)

> End-to-end optimization has achieved state-of-the-art performance on many specific problems, but there is no straight-forward way to combine pretrained models for new problems. Here, we explore improving modularity by learning a post-hoc interface between two existing models to solve a new task. Specifically, we take inspiration from neural machine translation, and cast the challenging problem of cross-modal domain transfer as unsupervised translation between the latent spaces of pretrained deep generative models. By abstracting away the data representation, we demonstrate that it is possible to transfer across different modalities (e.g., image-to-audio) and even different types of generative models (e.g., VAE-to-GAN). We compare to state-of-the-art techniques and find that a straight-forward variational autoencoder is able to best bridge the two generative models through learning a shared latent space. We can further impose supervised alignment of attributes in both domains with a classifier in the shared latent space. Through qualitative and quantitative evaluations, we demonstrate that locality and semantic alignment are preserved through the transfer process, as indicated by high transfer accuracies and smooth interpolations within a class. Finally, we show this modular structure speeds up training of new interface models by several orders of magnitude by decoupling it from expensive retraining of base generative models.

</details>

<details>

<summary>2019-02-22 02:03:17 - On the Sensitivity of Adversarial Robustness to Input Data Distributions</summary>

- *Gavin Weiguang Ding, Kry Yik Chau Lui, Xiaomeng Jin, Luyu Wang, Ruitong Huang*

- `1902.08336v1` - [abs](http://arxiv.org/abs/1902.08336v1) - [pdf](http://arxiv.org/pdf/1902.08336v1)

> Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about the most popular robust training method in the literature, adversarial training: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Empirical investigations further confirm our finding. We construct semantically-identical variants for MNIST and CIFAR10 respectively, and show that standardly trained models achieve comparable clean accuracies on them, but adversarially trained models achieve significantly different robustness accuracies. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon.

</details>

<details>

<summary>2019-02-22 05:16:03 - Learning to Represent Edits</summary>

- *Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt, Alexander L. Gaunt*

- `1810.13337v2` - [abs](http://arxiv.org/abs/1810.13337v2) - [pdf](http://arxiv.org/pdf/1810.13337v2)

> We introduce the problem of learning distributed representations of edits. By combining a "neural editor" with an "edit encoder", our models learn to represent the salient information of an edit and can be used to apply edits to new inputs. We experiment on natural language and source code edit data. Our evaluation yields promising results that suggest that our neural network models learn to capture the structure and semantics of edits. We hope that this interesting task and data source will inspire other researchers to work further on this problem.

</details>

<details>

<summary>2019-02-22 05:40:13 - Generating Multi-Agent Trajectories using Programmatic Weak Supervision</summary>

- *Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, Patrick Lucey*

- `1803.07612v6` - [abs](http://arxiv.org/abs/1803.07612v6) - [pdf](http://arxiv.org/pdf/1803.07612v6)

> We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts.

</details>

<details>

<summary>2019-02-22 06:28:36 - Learning to Learn Semantic Parsers from Natural Language Supervision</summary>

- *Igor Labutov, Bishan Yang, Tom Mitchell*

- `1902.08373v1` - [abs](http://arxiv.org/abs/1902.08373v1) - [pdf](http://arxiv.org/pdf/1902.08373v1)

> As humans, we often rely on language to learn language. For example, when corrected in a conversation, we may learn from that correction, over time improving our language fluency. Inspired by this observation, we propose a learning algorithm for training semantic parsers from supervision (feedback) expressed in natural language. Our algorithm learns a semantic parser from users' corrections such as "no, what I really meant was before his job, not after", by also simultaneously learning to parse this natural language feedback in order to leverage it as a form of supervision. Unlike supervision with gold-standard logical forms, our method does not require the user to be familiar with the underlying logical formalism, and unlike supervision from denotation, it does not require the user to know the correct answer to their query. This makes our learning algorithm naturally scalable in settings where existing conversational logs are available and can be leveraged as training data. We construct a novel dataset of natural language feedback in a conversational setting, and show that our method is effective at learning a semantic parser from such natural language supervision.

</details>

<details>

<summary>2019-02-22 20:39:43 - Learning Dual Retrieval Module for Semi-supervised Relation Extraction</summary>

- *Hongtao Lin, Jun Yan, Meng Qu, Xiang Ren*

- `1902.07814v2` - [abs](http://arxiv.org/abs/1902.07814v2) - [pdf](http://arxiv.org/pdf/1902.07814v2)

> Relation extraction is an important task in structuring content of text data, and becomes especially challenging when learning with weak supervision---where only a limited number of labeled sentences are given and a large number of unlabeled sentences are available. Most existing work exploits unlabeled data based on the ideas of self-training (i.e., bootstrapping a model) and multi-view learning (e.g., ensembling multiple model variants). However, these methods either suffer from the issue of semantic drift, or do not fully capture the problem characteristics of relation extraction. In this paper, we leverage a key insight that retrieving sentences expressing a relation is a dual task of predicting relation label for a given sentence---two tasks are complementary to each other and can be optimized jointly for mutual enhancement. To model this intuition, we propose DualRE, a principled framework that introduces a retrieval module which is jointly trained with the original relation prediction module. In this way, high-quality samples selected by retrieval module from unlabeled data can be used to improve prediction module, and vice versa. Experimental results\footnote{\small Code and data can be found at \url{https://github.com/INK-USC/DualRE}.} on two public datasets as well as case studies demonstrate the effectiveness of the DualRE approach.

</details>

<details>

<summary>2019-02-23 11:00:31 - Classification of Questions and Learning Outcome Statements (LOS) Into Blooms Taxonomy (BT) By Similarity Measurements Towards Extracting Of Learning Outcome from Learning Material</summary>

- *Shadi Diab, Badie Sartawi*

- `1706.03191v2` - [abs](http://arxiv.org/abs/1706.03191v2) - [pdf](http://arxiv.org/pdf/1706.03191v2)

> Blooms Taxonomy (BT) have been used to classify the objectives of learning outcome by dividing the learning into three different domains; the cognitive domain, the effective domain and the psychomotor domain. In this paper, we are introducing a new approach to classify the questions and learning outcome statements (LOS) into Blooms taxonomy (BT) and to verify BT verb lists, which are being cited and used by academicians to write questions and (LOS). An experiment was designed to investigate the semantic relationship between the action verbs used in both questions and LOS to obtain more accurate classification of the levels of BT. A sample of 775 different action verbs collected from different universities allows us to measure an accurate and clear-cut cognitive level for the action verb. It is worth mentioning that natural language processing techniques were used to develop our rules as to induce the questions into chunks in order to extract the action verbs. Our proposed solution was able to classify the action verb into a precise level of the cognitive domain. We, on our side, have tested and evaluated our proposed solution using confusion matrix. The results of evaluation tests yielded 97% for the macro average of precision and 90% for F1. Thus, the outcome of the research suggests that it is crucial to analyse and verify the action verbs cited and used by academicians to write LOS and classify their questions based on blooms taxonomy in order to obtain a definite and more accurate classification.

</details>

<details>

<summary>2019-02-23 14:14:05 - textTOvec: Deep Contextualized Neural Autoregressive Topic Models of Language with Distributed Compositional Prior</summary>

- *Pankaj Gupta, Yatin Chaudhary, Florian Buettner, Hinrich Schütze*

- `1810.03947v4` - [abs](http://arxiv.org/abs/1810.03947v4) - [pdf](http://arxiv.org/pdf/1810.03947v4)

> We address two challenges of probabilistic topic modelling in order to better estimate the probability of a word in a given context, i.e., P(word|context): (1) No Language Structure in Context: Probabilistic topic models ignore word order by summarizing a given context as a "bag-of-word" and consequently the semantics of words in the context is lost. The LSTM-LM learns a vector-space representation of each word by accounting for word order in local collocation patterns and models complex characteristics of language (e.g., syntax and semantics), while the TM simultaneously learns a latent representation from the entire document and discovers the underlying thematic structure. We unite two complementary paradigms of learning the meaning of word occurrences by combining a TM (e.g., DocNADE) and a LM in a unified probabilistic framework, named as ctx-DocNADE. (2) Limited Context and/or Smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging. We address this challenge by incorporating external knowledge into neural autoregressive topic models via a language modelling approach: we use word embeddings as input of a LSTM-LM with the aim to improve the word-topic mapping on a smaller and/or short-text corpus. The proposed DocNADE extension is named as ctx-DocNADEe.   We present novel neural autoregressive topic model variants coupled with neural LMs and embeddings priors that consistently outperform state-of-the-art generative TMs in terms of generalization (perplexity), interpretability (topic coherence) and applicability (retrieval and classification) over 6 long-text and 8 short-text datasets from diverse domains.

</details>

<details>

<summary>2019-02-23 17:04:54 - Augmenting Neural Machine Translation with Knowledge Graphs</summary>

- *Diego Moussallem, Mihael Arčan, Axel-Cyrille Ngonga Ngomo, Paul Buitelaar*

- `1902.08816v1` - [abs](http://arxiv.org/abs/1902.08816v1) - [pdf](http://arxiv.org/pdf/1902.08816v1)

> While neural networks have been used extensively to make substantial progress in the machine translation task, they are known for being heavily dependent on the availability of large amounts of training data. Recent efforts have tried to alleviate the data sparsity problem by augmenting the training data using different strategies, such as back-translation. Along with the data scarcity, the out-of-vocabulary words, mostly entities and terminological expressions, pose a difficult challenge to Neural Machine Translation systems. In this paper, we hypothesize that knowledge graphs enhance the semantic feature extraction of neural models, thus optimizing the translation of entities and terminological expressions in texts and consequently leading to a better translation quality. We hence investigate two different strategies for incorporating knowledge graphs into neural models without modifying the neural network architectures. We also examine the effectiveness of our augmentation method to recurrent and non-recurrent (self-attentional) neural architectures. Our knowledge graph augmented neural translation model, dubbed KG-NMT, achieves significant and consistent improvements of +3 BLEU, METEOR and chrF3 on average on the newstest datasets between 2014 and 2018 for WMT English-German translation task.

</details>

<details>

<summary>2019-02-23 22:38:27 - Probabilistic Semantic Inpainting with Pixel Constrained CNNs</summary>

- *Emilien Dupont, Suhas Suresha*

- `1810.03728v2` - [abs](http://arxiv.org/abs/1810.03728v2) - [pdf](http://arxiv.org/pdf/1810.03728v2)

> Semantic inpainting is the task of inferring missing pixels in an image given surrounding pixels and high level image semantics. Most semantic inpainting algorithms are deterministic: given an image with missing regions, a single inpainted image is generated. However, there are often several plausible inpaintings for a given missing region. In this paper, we propose a method to perform probabilistic semantic inpainting by building a model, based on PixelCNNs, that learns a distribution of images conditioned on a subset of visible pixels. Experiments on the MNIST and CelebA datasets show that our method produces diverse and realistic inpaintings.

</details>

<details>

<summary>2019-02-24 01:40:58 - Active Inductive Logic Programming for Code Search</summary>

- *Aishwarya Sivaraman, Tianyi Zhang, Guy Van den Broeck, Miryung Kim*

- `1812.05265v2` - [abs](http://arxiv.org/abs/1812.05265v2) - [pdf](http://arxiv.org/pdf/1812.05265v2)

> Modern search techniques either cannot efficiently incorporate human feedback to refine search results or to express structural or semantic properties of desired code. The key insight of our interactive code search technique ALICE is that user feedback could be actively incorporated to allow users to easily express and refine search queries. We design a query language to model the structure and semantics of code as logic facts. Given a code example with user annotations, ALICE automatically extracts a logic query from features that are tagged as important. Users can refine the search query by labeling one or more examples as desired (positive) or irrelevant (negative). ALICE then infers a new logic query that separates the positives from negative examples via active inductive logic programming. Our comprehensive and systematic simulation experiment shows that ALICE removes a large number of false positives quickly by actively incorporating user feedback. Its search algorithm is also robust to noise and user labeling mistakes. Our choice of leveraging both positive and negative examples and the nested containment structure of selected code is effective in refining search queries. Compared with an existing technique, Critics, ALICE does not require a user to manually construct a search pattern and yet achieves comparable precision and recall with fewer search iterations on average. A case study with users shows that ALICE is easy to use and helps express complex code patterns.

</details>

<details>

<summary>2019-02-24 02:03:00 - K for the Price of 1: Parameter-efficient Multi-task and Transfer Learning</summary>

- *Pramod Kaushik Mudrakarta, Mark Sandler, Andrey Zhmoginov, Andrew Howard*

- `1810.10703v2` - [abs](http://arxiv.org/abs/1810.10703v2) - [pdf](http://arxiv.org/pdf/1810.10703v2)

> We introduce a novel method that enables parameter-efficient transfer and multi-task learning with deep neural networks. The basic approach is to learn a model patch - a small set of parameters - that will specialize to each task, instead of fine-tuning the last layer or the entire network. For instance, we show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (e.g. converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image classification model while reusing 98% of parameters of the SSD feature extractor). Similarly, we show that re-learning existing low-parameter layers (such as depth-wise convolutions) while keeping the rest of the network frozen also improves transfer-learning accuracy significantly. Our approach allows both simultaneous (multi-task) as well as sequential transfer learning. In several multi-task learning problems, despite using much fewer parameters than traditional logits-only fine-tuning, we match single-task performance.

</details>

<details>

<summary>2019-02-24 04:41:07 - Fishy Cyber Attack Detection in Industrial Control Systems</summary>

- *Manikanta Reddy Dornala*

- `1812.03409v2` - [abs](http://arxiv.org/abs/1812.03409v2) - [pdf](http://arxiv.org/pdf/1812.03409v2)

> Cyber attacks have become serious threats to Industrial Control systems as well. It becomes important to develop a serious threat defense system against such vulnerabilities. For such process control systems, safety should also be assured apart from security. As unearthing vulnerabilities and patching them is not a feasible solution, these critical infrastructures need safeguards to prevent accidents, both natural and artificial, that could potentially be hazardous. Morita proposed an effective Zone division, capable of evaluating remote and concealed attacks on the system, coupled with Principal Component Analysis. But the need to analyze the node that has been compromised and stopping any further damages, requires an automated technique. Illustrating the basic ideas we'll simulate a simple Water plant. We propose a new automated approach based on Long Short Term Memory networks capable of detecting attacks and pin point the location of the breach.

</details>

<details>

<summary>2019-02-25 12:32:15 - A Theoretical Analysis of Contrastive Unsupervised Representation Learning</summary>

- *Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, Nikunj Saunshi*

- `1902.09229v1` - [abs](http://arxiv.org/abs/1902.09229v1) - [pdf](http://arxiv.org/pdf/1902.09229v1)

> Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically "similar" data points and "negative samples," the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.

</details>

<details>

<summary>2019-02-25 14:35:31 - Multi-Label Network Classification via Weighted Personalized Factorizations</summary>

- *Ahmed Rashed, Josif Grabocka, Lars Schmidt-Thieme*

- `1902.09294v1` - [abs](http://arxiv.org/abs/1902.09294v1) - [pdf](http://arxiv.org/pdf/1902.09294v1)

> Multi-label network classification is a well-known task that is being used in a wide variety of web-based and non-web-based domains. It can be formalized as a multi-relational learning task for predicting nodes labels based on their relations within the network. In sparse networks, this prediction task can be very challenging when only implicit feedback information is available such as in predicting user interests in social networks. Current approaches rely on learning per-node latent representations by utilizing the network structure, however, implicit feedback relations are naturally sparse and contain only positive observed feedbacks which mean that these approaches will treat all observed relations as equally important. This is not necessarily the case in real-world scenarios as implicit relations might have semantic weights which reflect the strength of those relations. If those weights can be approximated, the models can be trained to differentiate between strong and weak relations. In this paper, we propose a weighted personalized two-stage multi-relational matrix factorization model with Bayesian personalized ranking loss for network classification that utilizes basic transitive node similarity function for weighting implicit feedback relations. Experiments show that the proposed model significantly outperforms the state-of-art models on three different real-world web-based datasets and a biology-based dataset.

</details>

<details>

<summary>2019-02-25 20:04:13 - Verification of Non-Linear Specifications for Neural Networks</summary>

- *Chongli Qin, Krishnamurthy, Dvijotham, Brendan O'Donoghue, Rudy Bunel, Robert Stanforth, Sven Gowal, Jonathan Uesato, Grzegorz Swirszcz, Pushmeet Kohli*

- `1902.09592v1` - [abs](http://arxiv.org/abs/1902.09592v1) - [pdf](http://arxiv.org/pdf/1902.09592v1)

> Prior work on neural network verification has focused on specifications that are linear functions of the output of the network, e.g., invariance of the classifier output under adversarial perturbations of the input. In this paper, we extend verification algorithms to be able to certify richer properties of neural networks. To do this we introduce the class of convex-relaxable specifications, which constitute nonlinear specifications that can be verified using a convex relaxation. We show that a number of important properties of interest can be modeled within this class, including conservation of energy in a learned dynamics model of a physical system; semantic consistency of a classifier's output labels under adversarial perturbations and bounding errors in a system that predicts the summation of handwritten digits. Our experimental evaluation shows that our method is able to effectively verify these specifications. Moreover, our evaluation exposes the failure modes in models which cannot be verified to satisfy these specifications. Thus, emphasizing the importance of training models not just to fit training data but also to be consistent with specifications.

</details>

<details>

<summary>2019-02-26 00:59:42 - Embedding Uncertain Knowledge Graphs</summary>

- *Xuelu Chen, Muhao Chen, Weijia Shi, Yizhou Sun, Carlo Zaniolo*

- `1811.10667v2` - [abs](http://arxiv.org/abs/1811.10667v2) - [pdf](http://arxiv.org/pdf/1811.10667v2)

> Embedding models for deterministic Knowledge Graphs (KG) have been extensively studied, with the purpose of capturing latent semantic relations between entities and incorporating the structured knowledge into machine learning. However, there are many KGs that model uncertain knowledge, which typically model the inherent uncertainty of relations facts with a confidence score, and embedding such uncertain knowledge represents an unresolved challenge. The capturing of uncertain knowledge will benefit many knowledge-driven applications such as question answering and semantic search by providing more natural characterization of the knowledge. In this paper, we propose a novel uncertain KG embedding model UKGE, which aims to preserve both structural and uncertainty information of relation facts in the embedding space. Unlike previous models that characterize relation facts with binary classification techniques, UKGE learns embeddings according to the confidence scores of uncertain relation facts. To further enhance the precision of UKGE, we also introduce probabilistic soft logic to infer confidence scores for unseen relation facts during training. We propose and evaluate two variants of UKGE based on different learning objectives. Experiments are conducted on three real-world uncertain KGs via three tasks, i.e. confidence prediction, relation fact ranking, and relation fact classification. UKGE shows effectiveness in capturing uncertain knowledge by achieving promising results on these tasks, and consistently outperforms baselines on these tasks.

</details>

<details>

<summary>2019-02-26 08:46:15 - Semantic Hilbert Space for Text Representation Learning</summary>

- *Benyou Wang, Qiuchi Li, Massimo Melucci, Dawei Song*

- `1902.09802v1` - [abs](http://arxiv.org/abs/1902.09802v1) - [pdf](http://arxiv.org/pdf/1902.09802v1)

> Capturing the meaning of sentences has long been a challenging task. Current models tend to apply linear combinations of word features to conduct semantic composition for bigger-granularity units e.g. phrases, sentences, and documents. However, the semantic linearity does not always hold in human language. For instance, the meaning of the phrase `ivory tower' can not be deduced by linearly combining the meanings of `ivory' and `tower'. To address this issue, we propose a new framework that models different levels of semantic units (e.g. sememe, word, sentence, and semantic abstraction) on a single \textit{Semantic Hilbert Space}, which naturally admits a non-linear semantic composition by means of a complex-valued vector word representation. An end-to-end neural network~\footnote{https://github.com/wabyking/qnn} is proposed to implement the framework in the text classification task, and evaluation results on six benchmarking text classification datasets demonstrate the effectiveness, robustness and self-explanation power of the proposed model. Furthermore, intuitive case studies are conducted to help end users to understand how the framework works.

</details>

<details>

<summary>2019-02-26 11:14:57 - Text Analysis in Adversarial Settings: Does Deception Leave a Stylistic Trace?</summary>

- *Tommi Gröndahl, N. Asokan*

- `1902.08939v2` - [abs](http://arxiv.org/abs/1902.08939v2) - [pdf](http://arxiv.org/pdf/1902.08939v2)

> Textual deception constitutes a major problem for online security. Many studies have argued that deceptiveness leaves traces in writing style, which could be detected using text classification techniques. By conducting an extensive literature review of existing empirical work, we demonstrate that while certain linguistic features have been indicative of deception in certain corpora, they fail to generalize across divergent semantic domains. We suggest that deceptiveness as such leaves no content-invariant stylistic trace, and textual similarity measures provide superior means of classifying texts as potentially deceptive. Additionally, we discuss forms of deception beyond semantic content, focusing on hiding author identity by writing style obfuscation. Surveying the literature on both author identification and obfuscation techniques, we conclude that current style transformation methods fail to achieve reliable obfuscation while simultaneously ensuring semantic faithfulness to the original text. We propose that future work in style transformation should pay particular attention to disallowing semantically drastic changes.

</details>

<details>

<summary>2019-02-26 13:24:22 - Void Filling of Digital Elevation Models with Deep Generative Models</summary>

- *Konstantinos Gavriil, Georg Muntingh, Oliver J. D. Barrowclough*

- `1811.12693v2` - [abs](http://arxiv.org/abs/1811.12693v2) - [pdf](http://arxiv.org/pdf/1811.12693v2)

> In recent years, advances in machine learning algorithms, cheap computational resources, and the availability of big data have spurred the deep learning revolution in various application domains. In particular, supervised learning techniques in image analysis have led to superhuman performance in various tasks, such as classification, localization, and segmentation, while unsupervised learning techniques based on increasingly advanced generative models have been applied to generate high-resolution synthetic images indistinguishable from real images.   In this paper we consider a state-of-the-art machine learning model for image inpainting, namely a Wasserstein Generative Adversarial Network based on a fully convolutional architecture with a contextual attention mechanism. We show that this model can successfully be transferred to the setting of digital elevation models (DEMs) for the purpose of generating semantically plausible data for filling voids. Training, testing and experimentation is done on GeoTIFF data from various regions in Norway, made openly available by the Norwegian Mapping Authority.

</details>

<details>

<summary>2019-02-26 14:17:59 - Machine Reading Comprehension for Answer Re-Ranking in Customer Support Chatbots</summary>

- *Momchil Hardalov, Ivan Koychev, Preslav Nakov*

- `1902.04574v2` - [abs](http://arxiv.org/abs/1902.04574v2) - [pdf](http://arxiv.org/pdf/1902.04574v2)

> Recent advances in deep neural networks, language modeling and language generation have introduced new ideas to the field of conversational agents. As a result, deep neural models such as sequence-to-sequence, Memory Networks, and the Transformer have become key ingredients of state-of-the-art dialog systems. While those models are able to generate meaningful responses even in unseen situation, they need a lot of training data to build a reliable model. Thus, most real-world systems stuck to traditional approaches based on information retrieval and even hand-crafted rules, due to their robustness and effectiveness, especially for narrow-focused conversations. Here, we present a method that adapts a deep neural architecture from the domain of machine reading comprehension to re-rank the suggested answers from different models using the question as context. We train our model using negative sampling based on question-answer pairs from the Twitter Customer Support Dataset.The experimental results show that our re-ranking framework can improve the performance in terms of word overlap and semantics both for individual models as well as for model combinations.

</details>

<details>

<summary>2019-02-26 16:22:15 - A framework for information extraction from tables in biomedical literature</summary>

- *Nikola Milosevic, Cassie Gregson, Robert Hernandez, Goran Nenadic*

- `1902.10031v1` - [abs](http://arxiv.org/abs/1902.10031v1) - [pdf](http://arxiv.org/pdf/1902.10031v1)

> The scientific literature is growing exponentially, and professionals are no more able to cope with the current amount of publications. Text mining provided in the past methods to retrieve and extract information from text; however, most of these approaches ignored tables and figures. The research done in mining table data still does not have an integrated approach for mining that would consider all complexities and challenges of a table. Our research is examining the methods for extracting numerical (number of patients, age, gender distribution) and textual (adverse reactions) information from tables in the clinical literature. We present a requirement analysis template and an integral methodology for information extraction from tables in clinical domain that contains 7 steps: (1) table detection, (2) functional processing, (3) structural processing, (4) semantic tagging, (5) pragmatic processing, (6) cell selection and (7) syntactic processing and extraction. Our approach performed with the F-measure ranged between 82 and 92%, depending on the variable, task and its complexity.

</details>

<details>

<summary>2019-02-26 23:02:40 - Exploiting Population Activity Dynamics to Predict Urban Epidemiological Incidence</summary>

- *Gergana Todorova, Anastasios Noulas*

- `1902.10260v1` - [abs](http://arxiv.org/abs/1902.10260v1) - [pdf](http://arxiv.org/pdf/1902.10260v1)

> Ambulance services worldwide are of vital importance to population health. Timely responding to incidents by dispatching an ambulance vehicle to the location a call came from can offer significant benefits to patient care across a number of medical conditions. Moreover, identifying the reasons that drive ambulance activity at an area not only can improve the operational capacity of emergency services, but can lead to better policy design in healthcare. In this work, we analyse the temporal dynamics of 5.6 million ambulance calls across a region of 7 million residents in the UK. We identify characteristic temporal patterns featuring diurnal and weekly cycles in ambulance call activity. These patterns are stable over time and across geographies. Using a dataset sourced from location intelligence platform Foursquare, we establish a link between the spatio-temporal dynamics of mobile users engaging with urban activities locally and emergency incidents. We use this information to build a novel metric that assesses the health risk of a geographic area in terms of its propensity to yield ambulance calls. Formulating then an online classification task where the goal becomes to identify which regions will need an ambulance at a given time, we demonstrate how semantic information about real world places crowdsourced through online platforms, can become a useful source of information in understanding and predicting regional epidemiological trends.

</details>

<details>

<summary>2019-02-27 09:10:11 - Social Credibility Incorporating Semantic Analysis and Machine Learning: A Survey of the State-of-the-Art and Future Research Directions</summary>

- *Bilal Abu-Salih, Bushra Bremie, Pornpit Wongthongtham, Kevin Duan, Tomayess Issa, Kit Yan Chan, Mohammad Alhabashneh, Teshreen Albtoush, Sulaiman Alqahtani, Abdullah Alqahtani, Muteeb Alahmari, Naser Alshareef, Abdulaziz Albahlal*

- `1902.10402v1` - [abs](http://arxiv.org/abs/1902.10402v1) - [pdf](http://arxiv.org/pdf/1902.10402v1)

> The wealth of Social Big Data (SBD) represents a unique opportunity for organisations to obtain the excessive use of such data abundance to increase their revenues. Hence, there is an imperative need to capture, load, store, process, analyse, transform, interpret, and visualise such manifold social datasets to develop meaningful insights that are specific to an application domain. This paper lays the theoretical background by introducing the state-of-the-art literature review of the research topic. This is associated with a critical evaluation of the current approaches, and fortified with certain recommendations indicated to bridge the research gap.

</details>

<details>

<summary>2019-02-27 13:04:44 - EL Embeddings: Geometric construction of models for the Description Logic EL ++</summary>

- *Maxat Kulmanov, Wang Liu-Wei, Yuan Yan, Robert Hoehndorf*

- `1902.10499v1` - [abs](http://arxiv.org/abs/1902.10499v1) - [pdf](http://arxiv.org/pdf/1902.10499v1)

> An embedding is a function that maps entities from one algebraic structure into another while preserving certain characteristics. Embeddings are being used successfully for mapping relational data or text into vector spaces where they can be used for machine learning, similarity search, or similar tasks. We address the problem of finding vector space embeddings for theories in the Description Logic $\mathcal{EL}^{++}$ that are also models of the TBox. To find such embeddings, we define an optimization problem that characterizes the model-theoretic semantics of the operators in $\mathcal{EL}^{++}$ within $\Re^n$, thereby solving the problem of finding an interpretation function for an $\mathcal{EL}^{++}$ theory given a particular domain $\Delta$. Our approach is mainly relevant to large $\mathcal{EL}^{++}$ theories and knowledge bases such as the ontologies and knowledge graphs used in the life sciences. We demonstrate that our method can be used for improved prediction of protein--protein interactions when compared to semantic similarity measures or knowledge graph embedding

</details>

<details>

<summary>2019-02-27 14:29:34 - Technical report of "Empirical Study on Human Evaluation of Complex Argumentation Frameworks"</summary>

- *Marcos Cramer, Mathieu Guillaume*

- `1902.10552v1` - [abs](http://arxiv.org/abs/1902.10552v1) - [pdf](http://arxiv.org/pdf/1902.10552v1)

> In abstract argumentation, multiple argumentation semantics have been proposed that allow to select sets of jointly acceptable arguments from a given argumentation framework, i.e. based only on the attack relation between arguments. The existence of multiple argumentation semantics raises the question which of these semantics predicts best how humans evaluate arguments. Previous empirical cognitive studies that have tested how humans evaluate sets of arguments depending on the attack relation between them have been limited to a small set of very simple argumentation frameworks, so that some semantics studied in the literature could not be meaningfully distinguished by these studies. In this paper we report on an empirical cognitive study that overcomes these limitations by taking into consideration twelve argumentation frameworks of three to eight arguments each. These argumentation frameworks were mostly more complex than the argumentation frameworks considered in previous studies. All twelve argumentation framework were systematically instantiated with natural language arguments based on a certain fictional scenario, and participants were shown both the natural language arguments and a graphical depiction of the attack relation between them. Our data shows that grounded and CF2 semantics were the best predictors of human argument evaluation. A detailed analysis revealed that part of the participants chose a cognitively simpler strategy that is predicted very well by grounded semantics, while another part of the participants chose a cognitively more demanding strategy that is mostly predicted well by CF2 semantics.

</details>

<details>

<summary>2019-02-27 15:19:36 - Multiresolution Graph Attention Networks for Relevance Matching</summary>

- *Ting Zhang, Bang Liu, Di Niu, Kunfeng Lai, Yu Xu*

- `1902.10580v1` - [abs](http://arxiv.org/abs/1902.10580v1) - [pdf](http://arxiv.org/pdf/1902.10580v1)

> A large number of deep learning models have been proposed for the text matching problem, which is at the core of various typical natural language processing (NLP) tasks. However, existing deep models are mainly designed for the semantic matching between a pair of short texts, such as paraphrase identification and question answering, and do not perform well on the task of relevance matching between short-long text pairs. This is partially due to the fact that the essential characteristics of short-long text matching have not been well considered in these deep models. More specifically, these methods fail to handle extreme length discrepancy between text pieces and neither can they fully characterize the underlying structural information in long text documents. In this paper, we are especially interested in relevance matching between a piece of short text and a long document, which is critical to problems like query-document matching in information retrieval and web searching. To extract the structural information of documents, an undirected graph is constructed, with each vertex representing a keyword and the weight of an edge indicating the degree of interaction between keywords. Based on the keyword graph, we further propose a Multiresolution Graph Attention Network to learn multi-layered representations of vertices through a Graph Convolutional Network (GCN), and then match the short text snippet with the graphical representation of the document with the attention mechanisms applied over each layer of the GCN. Experimental results on two datasets demonstrate that our graph approach outperforms other state-of-the-art deep matching models.

</details>

<details>

<summary>2019-02-28 02:30:40 - TensorFlow.js: Machine Learning for the Web and Beyond</summary>

- *Daniel Smilkov, Nikhil Thorat, Yannick Assogba, Ann Yuan, Nick Kreeger, Ping Yu, Kangyi Zhang, Shanqing Cai, Eric Nielsen, David Soergel, Stan Bileschi, Michael Terry, Charles Nicholson, Sandeep N. Gupta, Sarah Sirajuddin, D. Sculley, Rajat Monga, Greg Corrado, Fernanda B. Viégas, Martin Wattenberg*

- `1901.05350v2` - [abs](http://arxiv.org/abs/1901.05350v2) - [pdf](http://arxiv.org/pdf/1901.05350v2)

> TensorFlow.js is a library for building and executing machine learning algorithms in JavaScript. TensorFlow.js models run in a web browser and in the Node.js environment. The library is part of the TensorFlow ecosystem, providing a set of APIs that are compatible with those in Python, allowing models to be ported between the Python and JavaScript ecosystems. TensorFlow.js has empowered a new set of developers from the extensive JavaScript community to build and deploy machine learning models and enabled new classes of on-device computation. This paper describes the design, API, and implementation of TensorFlow.js, and highlights some of the impactful use cases.

</details>

<details>

<summary>2019-02-28 05:54:16 - BERT for Joint Intent Classification and Slot Filling</summary>

- *Qian Chen, Zhu Zhuo, Wen Wang*

- `1902.10909v1` - [abs](http://arxiv.org/abs/1902.10909v1) - [pdf](http://arxiv.org/pdf/1902.10909v1)

> Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.

</details>

<details>

<summary>2019-02-28 16:17:33 - Semantics of higher-order probabilistic programs with conditioning</summary>

- *Fredrik Dahlqvist, Dexter Kozen*

- `1902.11189v1` - [abs](http://arxiv.org/abs/1902.11189v1) - [pdf](http://arxiv.org/pdf/1902.11189v1)

> We present a denotational semantics for higher-order probabilistic programs in terms of linear operators between Banach spaces. Our semantics is rooted in the classical theory of Banach spaces and their tensor products, but bears similarities with the well-known Scott semantics of higher-order programs through the use ordered Banach spaces which allow definitions in terms of fixed points. Being based on a monoidal rather than cartesian closed structure, our semantics effectively treats randomness as a resource.

</details>


## 2019-03

<details>

<summary>2019-03-01 07:09:30 - Liability, Ethics, and Culture-Aware Behavior Specification using Rulebooks</summary>

- *Andrea Censi, Konstantin Slutsky, Tichakorn Wongpiromsarn, Dmitry Yershov, Scott Pendleton, James Fu, Emilio Frazzoli*

- `1902.09355v2` - [abs](http://arxiv.org/abs/1902.09355v2) - [pdf](http://arxiv.org/pdf/1902.09355v2)

> The behavior of self-driving cars must be compatible with an enormous set of conflicting and ambiguous objectives, from law, from ethics, from the local culture, and so on. This paper describes a new way to conveniently define the desired behavior for autonomous agents, which we use on the self-driving cars developed at nuTonomy. We define a "rulebook" as a pre-ordered set of "rules", each akin to a violation metric on the possible outcomes ("realizations"). The rules are partially ordered by priority. The semantics of a rulebook imposes a pre-order on the set of realizations. We study the compositional properties of the rulebooks, and we derive which operations we can allow on the rulebooks to preserve previously-introduced constraints. While we demonstrate the application of these techniques in the self-driving domain, the methods are domain-independent.

</details>

<details>

<summary>2019-03-01 11:51:47 - A Deep DUAL-PATH Network for Improved Mammogram Image Processing</summary>

- *Heyi Li, Dongdong Chen, William H. Nailon, Mike E. Davies, Dave Laurenson*

- `1903.00001v1` - [abs](http://arxiv.org/abs/1903.00001v1) - [pdf](http://arxiv.org/pdf/1903.00001v1)

> We present, for the first time, a novel deep neural network architecture called \dcn with a dual-path connection between the input image and output class label for mammogram image processing. This architecture is built upon U-Net, which non-linearly maps the input data into a deep latent space. One path of the \dcnn, the locality preserving learner, is devoted to hierarchically extracting and exploiting intrinsic features of the input, while the other path, called the conditional graph learner, focuses on modeling the input-mask correlations. The learned mask is further used to improve classification results, and the two learning paths complement each other. By integrating the two learners our new architecture provides a simple but effective way to jointly learn the segmentation and predict the class label. Benefiting from the powerful expressive capacity of deep neural networks a more discriminative representation can be learned, in which both the semantics and structure are well preserved. Experimental results show that \dcn achieves the best mammography segmentation and classification simultaneously, outperforming recent state-of-the-art models.

</details>

<details>

<summary>2019-03-01 15:17:02 - Extending Modular Semantics for Bipolar Weighted Argumentation (Technical Report)</summary>

- *Nico Potyka*

- `1809.07133v2` - [abs](http://arxiv.org/abs/1809.07133v2) - [pdf](http://arxiv.org/pdf/1809.07133v2)

> Weighted bipolar argumentation frameworks offer a tool for decision support and social media analysis. Arguments are evaluated by an iterative procedure that takes initial weights and attack and support relations into account. Until recently, convergence of these iterative procedures was not very well understood in cyclic graphs. Mossakowski and Neuhaus recently introduced a unification of different approaches and proved first convergence and divergence results. We build up on this work, simplify and generalize convergence results and complement them with runtime guarantees. As it turns out, there is a tradeoff between semantics' convergence guarantees and their ability to move strength values away from the initial weights. We demonstrate that divergence problems can be avoided without this tradeoff by continuizing semantics. Semantically, we extend the framework with a Duality property that assures a symmetric impact of attack and support relations. We also present a Java implementation of modular semantics and explain the practical usefulness of the theoretical ideas.

</details>

<details>

<summary>2019-03-02 00:42:03 - Learning Robust Representations by Projecting Superficial Statistics Out</summary>

- *Haohan Wang, Zexue He, Zachary C. Lipton, Eric P. Xing*

- `1903.06256v1` - [abs](http://arxiv.org/abs/1903.06256v1) - [pdf](http://arxiv.org/pdf/1903.06256v1)

> Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks' out-of-sample performance. The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's. We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.

</details>

<details>

<summary>2019-03-02 10:56:53 - Automatically Generating Engaging Presentation Slide Decks</summary>

- *Thomas Winters, Kory W. Mathewson*

- `1903.09308v1` - [abs](http://arxiv.org/abs/1903.09308v1) - [pdf](http://arxiv.org/pdf/1903.09308v1)

> Talented public speakers have thousands of hours of practice. One means of improving public speaking skills is practice through improvisation, e.g. presenting an improvised presentation using an unseen slide deck. We present TEDRIC, a novel system capable of generating coherent slide decks based on a single topic suggestion. It combines semantic word webs with text and image data sources to create an engaging slide deck with an overarching theme. We found that audience members perceived the quality of improvised presentations using these generated slide decks to be on par with presentations using human created slide decks for the Improvised TED Talk performance format. TEDRIC is thus a valuable new creative tool for improvisers to perform with, and for anyone looking to improve their presentation skills.

</details>

<details>

<summary>2019-03-02 12:53:25 - Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated Temporal Fully-Convolutional Networks</summary>

- *Noshaba Cheema, Somayeh Hosseini, Janis Sprenger, Erik Herrmann, Han Du, Klaus Fischer, Philipp Slusallek*

- `1903.00695v1` - [abs](http://arxiv.org/abs/1903.00695v1) - [pdf](http://arxiv.org/pdf/1903.00695v1)

> Human motion capture data has been widely used in data-driven character animation. In order to generate realistic, natural-looking motions, most data-driven approaches require considerable efforts of pre-processing, including motion segmentation and annotation. Existing (semi-) automatic solutions either require hand-crafted features for motion segmentation or do not produce the semantic annotations required for motion synthesis and building large-scale motion databases. In addition, human labeled annotation data suffers from inter- and intra-labeler inconsistencies by design. We propose a semi-automatic framework for semantic segmentation of motion capture data based on supervised machine learning techniques. It first transforms a motion capture sequence into a ``motion image'' and applies a convolutional neural network for image segmentation. Dilated temporal convolutions enable the extraction of temporal information from a large receptive field. Our model outperforms two state-of-the-art models for action segmentation, as well as a popular network for sequence modeling. Most of all, our method is very robust under noisy and inaccurate training labels and thus can handle human errors during the labeling process.

</details>

<details>

<summary>2019-03-02 15:32:39 - Predicting and interpreting embeddings for out of vocabulary words in downstream tasks</summary>

- *Nicolas Garneau, Jean-Samuel Leboeuf, Luc Lamontagne*

- `1903.00724v1` - [abs](http://arxiv.org/abs/1903.00724v1) - [pdf](http://arxiv.org/pdf/1903.00724v1)

> We propose a novel way to handle out of vocabulary (OOV) words in downstream natural language processing (NLP) tasks. We implement a network that predicts useful embeddings for OOV words based on their morphology and on the context in which they appear. Our model also incorporates an attention mechanism indicating the focus allocated to the left context words, the right context words or the word's characters, hence making the prediction more interpretable. The model is a ``drop-in'' module that is jointly trained with the downstream task's neural network, thus producing embeddings specialized for the task at hand. When the task is mostly syntactical, we observe that our model aims most of its attention on surface form characters. On the other hand, for tasks more semantical, the network allocates more attention to the surrounding words. In all our tests, the module helps the network to achieve better performances in comparison to the use of simple random embeddings.

</details>

<details>

<summary>2019-03-04 14:36:21 - Using Word Embeddings for Visual Data Exploration with Ontodia and Wikidata</summary>

- *Gerhard Wohlgenannt, Nikolay Klimov, Dmitry Mouromtsev, Daniil Razdyakonov, Dmitry Pavlov, Yury Emelyanov*

- `1903.01275v1` - [abs](http://arxiv.org/abs/1903.01275v1) - [pdf](http://arxiv.org/pdf/1903.01275v1)

> One of the big challenges in Linked Data consumption is to create visual and natural language interfaces to the data usable for non-technical users. Ontodia provides support for diagrammatic data exploration, showcased in this publication in combination with the Wikidata dataset. We present improvements to the natural language interface regarding exploring and querying Linked Data entities. The method uses models of distributional semantics to find and rank entity properties related to user input in Ontodia. Various word embedding types and model settings are evaluated, and the results show that user experience in visual data exploration benefits from the proposed approach.

</details>

<details>

<summary>2019-03-04 15:32:39 - Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks</summary>

- *Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang, Huajun Chen*

- `1903.01306v1` - [abs](http://arxiv.org/abs/1903.01306v1) - [pdf](http://arxiv.org/pdf/1903.01306v1)

> We propose a distance supervised relation extraction approach for long-tailed, imbalanced data which is prevalent in real-world settings. Here, the challenge is to learn accurate "few-shot" models for classes existing at the tail of the class distribution, for which little data is available. Inspired by the rich semantic correlations between classes at the long tail and those at the head, we take advantage of the knowledge from data-rich classes at the head of the distribution to boost the performance of the data-poor classes at the tail. First, we propose to leverage implicit relational knowledge among class labels from knowledge graph embeddings and learn explicit relational knowledge using graph convolution networks. Second, we integrate that relational knowledge into relation extraction model by coarse-to-fine knowledge-aware attention mechanism. We demonstrate our results for a large-scale benchmark dataset which show that our approach significantly outperforms other baselines, especially for long-tail relations.

</details>

<details>

<summary>2019-03-04 17:07:31 - CBinfer: Exploiting Frame-to-Frame Locality for Faster Convolutional Network Inference on Video Streams</summary>

- *Lukas Cavigelli, Luca Benini*

- `1808.05488v2` - [abs](http://arxiv.org/abs/1808.05488v2) - [pdf](http://arxiv.org/pdf/1808.05488v2)

> The last few years have brought advances in computer vision at an amazing pace, grounded on new findings in deep neural network construction and training as well as the availability of large labeled datasets. Applying these networks to images demands a high computational effort and pushes the use of state-of-the-art networks on real-time video data out of reach of embedded platforms. Many recent works focus on reducing network complexity for real-time inference on embedded computing platforms. We adopt an orthogonal viewpoint and propose a novel algorithm exploiting the spatio-temporal sparsity of pixel changes. This optimized inference procedure resulted in an average speed-up of 9.1x over cuDNN on the Tegra X2 platform at a negligible accuracy loss of <0.1% and no retraining of the network for a semantic segmentation application. Similarly, an average speed-up of 7.0x has been achieved for a pose detection DNN and a reduction of 5x of the number of arithmetic operations to be performed for object detection on static camera video surveillance data. These throughput gains combined with a lower power consumption result in an energy efficiency of 511 GOp/s/W compared to 70 GOp/s/W for the baseline.

</details>

<details>

<summary>2019-03-04 19:56:05 - Model Checking Clinical Decision Support Systems Using SMT</summary>

- *Mohammad Hekmatnejad, Andrew M. Simms, Georgios Fainekos*

- `1901.04545v2` - [abs](http://arxiv.org/abs/1901.04545v2) - [pdf](http://arxiv.org/pdf/1901.04545v2)

> Individual clinical Knowledge Artifacts (KA) are designed to be used in Clinical Decision Support (CDS) systems at the point of care for delivery of safe, evidence-based care in modern healthcare systems. For formal authoring of a KA, syntax verification and validation is guaranteed by the grammar. However, there are no methods for semantic verification. Any semantic fallacy may lead to rejection of the outcomes by care providers. As a first step toward solving this problem, we present a framework for translating the logical segments of KAs into Satisfiability Modulo Theory (SMT) models. We present the effectiveness and efficiency of our work by automatically translating the logic fragment of publicly available KAs and verifying them using Z3 SMT solver.

</details>

<details>

<summary>2019-03-05 18:15:37 - Complexity Results and Algorithms for Bipolar Argumentation</summary>

- *Amin Karamlou, Kristijonas Čyras, Francesca Toni*

- `1903.01964v1` - [abs](http://arxiv.org/abs/1903.01964v1) - [pdf](http://arxiv.org/pdf/1903.01964v1)

> Bipolar Argumentation Frameworks (BAFs) admit several interpretations of the support relation and diverging definitions of semantics. Recently, several classes of BAFs have been captured as instances of bipolar Assumption-Based Argumentation, a class of Assumption-Based Argumentation (ABA). In this paper, we establish the complexity of bipolar ABA, and consequently of several classes of BAFs. In addition to the standard five complexity problems, we analyse the rarely-addressed extension enumeration problem too. We also advance backtracking-driven algorithms for enumerating extensions of bipolar ABA frameworks, and consequently of BAFs under several interpretations. We prove soundness and completeness of our algorithms, describe their implementation and provide a scalability evaluation. We thus contribute to the study of the as yet uninvestigated complexity problems of (variously interpreted) BAFs as well as of bipolar ABA, and provide the lacking implementations thereof.

</details>

<details>

<summary>2019-03-06 09:17:54 - Differentially Private Generative Adversarial Networks for Time Series, Continuous, and Discrete Open Data</summary>

- *Lorenzo Frigerio, Anderson Santana de Oliveira, Laurent Gomez, Patrick Duverger*

- `1901.02477v2` - [abs](http://arxiv.org/abs/1901.02477v2) - [pdf](http://arxiv.org/pdf/1901.02477v2)

> Open data plays a fundamental role in the 21th century by stimulating economic growth and by enabling more transparent and inclusive societies. However, it is always difficult to create new high-quality datasets with the required privacy guarantees for many use cases. This paper aims at creating a framework for releasing new open data while protecting the individuality of the users through a strict definition of privacy called differential privacy. Unlike previous work, this paper provides a framework for privacy preserving data publishing that can be easily adapted to different use cases, from the generation of time-series to continuous data, and discrete data; no previous work has focused on the later class. Indeed, many use cases expose discrete data or at least a combination between categorical and numerical values. Thanks to the latest developments in deep learning and generative models, it is now possible to model rich-semantic data maintaining both the original distribution of the features and the correlations between them. The output of this framework is a deep network, namely a generator, able to create new data on demand. We demonstrate the efficiency of our approach on real datasets from the French public administration and classic benchmark datasets.

</details>

<details>

<summary>2019-03-07 00:54:02 - Creation and Evaluation of Datasets for Distributional Semantics Tasks in the Digital Humanities Domain</summary>

- *Gerhard Wohlgenannt, Ariadna Barinova, Dmitry Ilvovsky, Ekaterina Chernyak*

- `1903.02671v1` - [abs](http://arxiv.org/abs/1903.02671v1) - [pdf](http://arxiv.org/pdf/1903.02671v1)

> Word embeddings are already well studied in the general domain, usually trained on large text corpora, and have been evaluated for example on word similarity and analogy tasks, but also as an input to downstream NLP processes. In contrast, in this work we explore the suitability of word embedding technologies in the specialized digital humanities domain. After training embedding models of various types on two popular fantasy novel book series, we evaluate their performance on two task types: term analogies, and word intrusion. To this end, we manually construct test datasets with domain experts. Among the contributions are the evaluation of various word embedding techniques on the different task types, with the findings that even embeddings trained on small corpora perform well for example on the word intrusion task. Furthermore, we provide extensive and high-quality datasets in digital humanities for further investigation, as well as the implementation to easily reproduce or extend the experiments.

</details>

<details>

<summary>2019-03-07 06:28:44 - RAVEN: A Dataset for Relational and Analogical Visual rEasoNing</summary>

- *Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, Song-Chun Zhu*

- `1903.02741v1` - [abs](http://arxiv.org/abs/1903.02741v1) - [pdf](http://arxiv.org/pdf/1903.02741v1)

> Dramatic progress has been witnessed in basic vision tasks involving low-level perception, such as object recognition, detection, and tracking. Unfortunately, there is still an enormous performance gap between artificial vision systems and human intelligence in terms of higher-level vision problems, especially ones involving reasoning. Earlier attempts in equipping machines with high-level reasoning have hovered around Visual Question Answering (VQA), one typical task associating vision and language understanding. In this work, we propose a new dataset, built in the context of Raven's Progressive Matrices (RPM) and aimed at lifting machine intelligence by associating vision with structural, relational, and analogical reasoning in a hierarchical representation. Unlike previous works in measuring abstract reasoning using RPM, we establish a semantic link between vision and reasoning by providing structure representation. This addition enables a new type of abstract reasoning by jointly operating on the structure representation. Machine reasoning ability using modern computer vision is evaluated in this newly proposed dataset. Additionally, we also provide human performance as a reference. Finally, we show consistent improvement across all models by incorporating a simple neural module that combines visual understanding and structure reasoning.

</details>

<details>

<summary>2019-03-07 11:12:16 - Only Connect, Securely</summary>

- *Chandrika Bhardwaj, Sanjiva Prasad*

- `1903.02835v1` - [abs](http://arxiv.org/abs/1903.02835v1) - [pdf](http://arxiv.org/pdf/1903.02835v1)

> The lattice model proposed by Denning in her seminal work provided secure information flow analyses with an intuitive and uniform mathematical foundation. Different organisations, however, may employ quite different security lattices. In this paper, we propose a connection framework that permits different organisations to exchange information while maintaining both security of information flows as well as their autonomy in formulating and maintaining security policy. Our prescriptive framework is based on the rigorous mathematical framework of Lagois connections given by Melton, together with a simple operational model for transferring object data between domains. The merit of this formulation is that it is simple, minimal, adaptable and intuitive, and provides a formal framework for establishing secure information flow across autonomous interacting organisations. We show that our framework is semantically sound, by proving that the connections proposed preserve standard correctness notions such as non-interference.

</details>

<details>

<summary>2019-03-07 16:51:05 - Constrained Graph Variational Autoencoders for Molecule Design</summary>

- *Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, Alexander L. Gaunt*

- `1805.09076v2` - [abs](http://arxiv.org/abs/1805.09076v2) - [pdf](http://arxiv.org/pdf/1805.09076v2)

> Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on the use of graphs to represent chemical molecules, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is more successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.

</details>

<details>

<summary>2019-03-07 18:19:55 - Quantum Latent Semantic Analysis</summary>

- *Fabio A. González, Juan C. Caicedo*

- `1903.03082v1` - [abs](http://arxiv.org/abs/1903.03082v1) - [pdf](http://arxiv.org/pdf/1903.03082v1)

> The main goal of this paper is to explore latent topic analysis (LTA), in the context of quantum information retrieval. LTA is a valuable technique for document analysis and representation, which has been extensively used in information retrieval and machine learning. Different LTA techniques have been proposed, some based on geometrical modeling (such as latent semantic analysis, LSA) and others based on a strong statistical foundation. However, these two different approaches are not usually mixed. Quantum information retrieval has the remarkable virtue of combining both geometry and probability in a common principled framework. We built on this quantum framework to propose a new LTA method, which has a clear geometrical motivation but also supports a well-founded probabilistic interpretation. An initial exploratory experimentation was performed on three standard data sets. The results show that the proposed method outperforms LSA on two of the three datasets. These results suggests that the quantum-motivated representation is an alternative for geometrical latent topic modeling worthy of further exploration.

</details>

<details>

<summary>2019-03-08 02:32:55 - An Algorithm for the Visualization of Relevant Patterns in Astronomical Light Curves</summary>

- *Christian Pieringer, Karim Pichara, Márcio Catelán, Pavlos Protopapas*

- `1903.03254v1` - [abs](http://arxiv.org/abs/1903.03254v1) - [pdf](http://arxiv.org/pdf/1903.03254v1)

> Within the last years, the classification of variable stars with Machine Learning has become a mainstream area of research. Recently, visualization of time series is attracting more attention in data science as a tool to visually help scientists to recognize significant patterns in complex dynamics. Within the Machine Learning literature, dictionary-based methods have been widely used to encode relevant parts of image data. These methods intrinsically assign a degree of importance to patches in pictures, according to their contribution in the image reconstruction. Inspired by dictionary-based techniques, we present an approach that naturally provides the visualization of salient parts in astronomical light curves, making the analogy between image patches and relevant pieces in time series. Our approach encodes the most meaningful patterns such that we can approximately reconstruct light curves by just using the encoded information. We test our method in light curves from the OGLE-III and StarLight databases. Our results show that the proposed model delivers an automatic and intuitive visualization of relevant light curve parts, such as local peaks and drops in magnitude.

</details>

<details>

<summary>2019-03-08 12:45:11 - Generating Automated and Online Test Oracles for Simulink Models with Continuous and Uncertain Behaviors</summary>

- *Claudio Menghi, Shiva Nejati, Khouloud Gaaloul, Lionel Briand*

- `1903.03399v1` - [abs](http://arxiv.org/abs/1903.03399v1) - [pdf](http://arxiv.org/pdf/1903.03399v1)

> Test automation requires automated oracles to assess test outputs. For cyber physical systems (CPS), oracles, in addition to be automated, should ensure some key objectives: (i) they should check test outputs in an online manner to stop expensive test executions as soon as a failure is detected; (ii) they should handle time- and magnitude-continuous CPS behaviors; (iii) they should provide a quantitative degree of satisfaction or failure measure instead of binary pass/fail outputs; and (iv) they should be able to handle uncertainties due to CPS interactions with the environment. We propose an automated approach to translate CPS requirements specified in a logic-based language into test oracles specified in Simulink -- a widely-used development and simulation language for CPS. Our approach achieves the objectives noted above through the identification of a fragment of Signal First Order logic (SFOL) to specify requirements, the definition of a quantitative semantics for this fragment and a sound translation of the fragment into Simulink. The results from applying our approach on 11 industrial case studies show that: (i) our requirements language can express all the 98 requirements of our case studies; (ii) the time and effort required by our approach are acceptable, showing potentials for the adoption of our work in practice, and (iii) for large models, our approach can dramatically reduce the test execution time compared to when test outputs are checked in an offline manner.

</details>

<details>

<summary>2019-03-08 15:10:37 - Set CRDT com Múltiplas Políticas de Resolução de Conflitos</summary>

- *André Rijo, Carla Ferreira, Nuno Preguiça*

- `1903.03487v1` - [abs](http://arxiv.org/abs/1903.03487v1) - [pdf](http://arxiv.org/pdf/1903.03487v1)

> Um CRDT \'e um tipo de dados que pode ser replicado e modificado concorrentemente sem coordena\c{c}\~ao, garantindo-se a converg\^encia das r\'eplicas atrav\'es da resolu\c{c}\~ao autom\'atica de conflitos. Cada CRDT implementa uma pol\'itica espec\'ifica para resolver conflitos. Por exemplo, um conjunto CRDT add-wins d\'a prioridade ao "add" aquando da execu\c{c}\~ao concorrente de um "add" e "rem" do mesmo elemento. Em algumas aplica\c{c}\~oes pode ser necess\'ario usar diferentes pol\'iticas para diferentes execu\c{c}\~oes de uma opera\c{c}\~ao -- por exemplo, uma aplica\c{c}\~ao que utilize um conjunto CRDT add-wins pode querer que alguns "removes" ganhem sobre "adds" concorrentes. Neste artigo \'e apresentado e avaliado o desenho dum conjunto CRDT que implementa as sem\^anticas referidas.   ---   Conflict-Free Replicated Data Types (CRDTs) allow objects to be replicated and concurrently modified without coordination. CRDTs solve conflicts automatically and provide eventual consistency. Typically each CRDT uses a specific policy for solving conflicts. For example, in an add-wins set CRDT, when an element is concurrently add and removed in different replicas, priority is given to add, i.e., the element stays in the set. Unfortunately, this may be inadequate for some applications - it may be desired to overrule the default policy for some operation executions. For example, an application using an add-wins set may want some removes to win over concurrent adds. This paper present the design of a set CRDT that implements such semantics.

</details>

<details>

<summary>2019-03-08 23:12:12 - Image Privacy Prediction Using Deep Neural Networks</summary>

- *Ashwini Tonge, Cornelia Caragea*

- `1903.03695v1` - [abs](http://arxiv.org/abs/1903.03695v1) - [pdf](http://arxiv.org/pdf/1903.03695v1)

> Images today are increasingly shared online on social networking sites such as Facebook, Flickr, Foursquare, and Instagram. Despite that current social networking sites allow users to change their privacy preferences, this is often a cumbersome task for the vast majority of users on the Web, who face difficulties in assigning and managing privacy settings. Thus, automatically predicting images' privacy to warn users about private or sensitive content before uploading these images on social networking sites has become a necessity in our current interconnected world.   In this paper, we explore learning models to automatically predict appropriate images' privacy as private or public using carefully identified image-specific features. We study deep visual semantic features that are derived from various layers of Convolutional Neural Networks (CNNs) as well as textual features such as user tags and deep tags generated from deep CNNs. Particularly, we extract deep (visual and tag) features from four pre-trained CNN architectures for object recognition, i.e., AlexNet, GoogLeNet, VGG-16, and ResNet, and compare their performance for image privacy prediction. Results of our experiments on a Flickr dataset of over thirty thousand images show that the learning models trained on features extracted from ResNet outperform the state-of-the-art models for image privacy prediction. We further investigate the combination of user tags and deep tags derived from CNN architectures using two settings: (1) SVM on the bag-of-tags features; and (2) text-based CNN. Our results show that even though the models trained on the visual features perform better than those trained on the tag features, the combination of deep visual features with image tags shows improvements in performance over the individual feature sets.

</details>

<details>

<summary>2019-03-09 08:24:15 - Mutual Clustering on Comparative Texts via Heterogeneous Information Networks</summary>

- *Jianping Cao, Senzhang Wang, Danyan Wen, Zhaohui Peng, Philip S. Yu, Fei-yue Wang*

- `1903.03762v1` - [abs](http://arxiv.org/abs/1903.03762v1) - [pdf](http://arxiv.org/pdf/1903.03762v1)

> Currently, many intelligence systems contain the texts from multi-sources, e.g., bulletin board system (BBS) posts, tweets and news. These texts can be ``comparative'' since they may be semantically correlated and thus provide us with different perspectives toward the same topics or events. To better organize the multi-sourced texts and obtain more comprehensive knowledge, we propose to study the novel problem of Mutual Clustering on Comparative Texts (MCCT), which aims to cluster the comparative texts simultaneously and collaboratively. The MCCT problem is difficult to address because 1) comparative texts usually present different data formats and structures and thus they are hard to organize, and 2) there lacks an effective method to connect the semantically correlated comparative texts to facilitate clustering them in an unified way. To this aim, in this paper we propose a Heterogeneous Information Network-based Text clustering framework HINT. HINT first models multi-sourced texts (e.g. news and tweets) as heterogeneous information networks by introducing the shared ``anchor texts'' to connect the comparative texts. Next, two similarity matrices based on HINT as well as a transition matrix for cross-text-source knowledge transfer are constructed. Comparative texts clustering are then conducted by utilizing the constructed matrices. Finally, a mutual clustering algorithm is also proposed to further unify the separate clustering results of the comparative texts by introducing a clustering consistency constraint. We conduct extensive experimental on three tweets-news datasets, and the results demonstrate the effectiveness and robustness of the proposed method in addressing the MCCT problem.

</details>

<details>

<summary>2019-03-09 10:01:12 - Logic Rules Powered Knowledge Graph Embedding</summary>

- *Pengwei Wang, Dejing Dou, Fangzhao Wu, Nisansa de Silva, Lianwen Jin*

- `1903.03772v1` - [abs](http://arxiv.org/abs/1903.03772v1) - [pdf](http://arxiv.org/pdf/1903.03772v1)

> Large scale knowledge graph embedding has attracted much attention from both academia and industry in the field of Artificial Intelligence. However, most existing methods concentrate solely on fact triples contained in the given knowledge graph. Inspired by the fact that logic rules can provide a flexible and declarative language for expressing rich background knowledge, it is natural to integrate logic rules into knowledge graph embedding, to transfer human knowledge to entity and relation embedding, and strengthen the learning process. In this paper, we propose a novel logic rule-enhanced method which can be easily integrated with any translation based knowledge graph embedding model, such as TransE . We first introduce a method to automatically mine the logic rules and corresponding confidences from the triples. And then, to put both triples and mined logic rules within the same semantic space, all triples in the knowledge graph are represented as first-order logic. Finally, we define several operations on the first-order logic and minimize a global loss over both of the mined logic rules and the transformed first-order logics. We conduct extensive experiments for link prediction and triple classification on three datasets: WN18, FB166, and FB15K. Experiments show that the rule-enhanced method can significantly improve the performance of several baselines. The highlight of our model is that the filtered Hits@1, which is a pivotal evaluation in the knowledge inference task, has a significant improvement (up to 700% improvement).

</details>

<details>

<summary>2019-03-09 13:47:05 - Program Classification Using Gated Graph Attention Neural Network for Online Programming Service</summary>

- *Mingming Lu, Dingwu Tan, Naixue Xiong, Zailiang Chen, Haifeng Li*

- `1903.03804v1` - [abs](http://arxiv.org/abs/1903.03804v1) - [pdf](http://arxiv.org/pdf/1903.03804v1)

> The online programing services, such as Github,TopCoder, and EduCoder, have promoted a lot of social interactions among the service users. However, the existing social interactions is rather limited and inefficient due to the rapid increasing of source-code repositories, which is difficult to explore manually. The emergence of source-code mining provides a promising way to analyze those source codes, so that those source codes can be relatively easy to understand and share among those service users. Among all the source-code mining attempts,program classification lays a foundation for various tasks related to source-code understanding, because it is impossible for a machine to understand a computer program if it cannot classify the program correctly. Although numerous machine learning models, such as the Natural Language Processing (NLP) based models and the Abstract Syntax Tree (AST) based models, have been proposed to classify computer programs based on their corresponding source codes, the existing works cannot fully characterize the source codes from the perspective of both the syntax and semantic information. To address this problem, we proposed a Graph Neural Network (GNN) based model, which integrates data flow and function call information to the AST,and applies an improved GNN model to the integrated graph, so as to achieve the state-of-art program classification accuracy. The experiment results have shown that the proposed work can classify programs with accuracy over 97%.

</details>

<details>

<summary>2019-03-10 07:38:41 - Predicting Good Configurations for GitHub and Stack Overflow Topic Models</summary>

- *Christoph Treude, Markus Wagner*

- `1804.04749v3` - [abs](http://arxiv.org/abs/1804.04749v3) - [pdf](http://arxiv.org/pdf/1804.04749v3)

> Software repositories contain large amounts of textual data, ranging from source code comments and issue descriptions to questions, answers, and comments on Stack Overflow. To make sense of this textual data, topic modelling is frequently used as a text-mining tool for the discovery of hidden semantic structures in text bodies. Latent Dirichlet allocation (LDA) is a commonly used topic model that aims to explain the structure of a corpus by grouping texts. LDA requires multiple parameters to work well, and there are only rough and sometimes conflicting guidelines available on how these parameters should be set. In this paper, we contribute (i) a broad study of parameters to arrive at good local optima for GitHub and Stack Overflow text corpora, (ii) an a-posteriori characterisation of text corpora related to eight programming languages, and (iii) an analysis of corpus feature importance via per-corpus LDA configuration. We find that (1) popular rules of thumb for topic modelling parameter configuration are not applicable to the corpora used in our experiments, (2) corpora sampled from GitHub and Stack Overflow have different characteristics and require different configurations to achieve good model fit, and (3) we can predict good configurations for unseen corpora reliably. These findings support researchers and practitioners in efficiently determining suitable configurations for topic modelling when analysing textual data contained in software repositories.

</details>

<details>

<summary>2019-03-10 21:56:45 - Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation</summary>

- *Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, Daniel Ulbricht*

- `1903.04064v1` - [abs](http://arxiv.org/abs/1903.04064v1) - [pdf](http://arxiv.org/pdf/1903.04064v1)

> In this work, we connect two distinct concepts for unsupervised domain adaptation: feature distribution alignment between domains by utilizing the task-specific decision boundary and the Wasserstein metric. Our proposed sliced Wasserstein discrepancy (SWD) is designed to capture the natural notion of dissimilarity between the outputs of task-specific classifiers. It provides a geometrically meaningful guidance to detect target samples that are far from the support of the source and enables efficient distribution alignment in an end-to-end trainable fashion. In the experiments, we validate the effectiveness and genericness of our method on digit and sign recognition, image classification, semantic segmentation, and object detection.

</details>

<details>

<summary>2019-03-11 00:43:13 - Exploring OpenStreetMap Availability for Driving Environment Understanding</summary>

- *Yang Zheng, Izzat H. Izzat, John H. L. Hansen*

- `1903.04084v1` - [abs](http://arxiv.org/abs/1903.04084v1) - [pdf](http://arxiv.org/pdf/1903.04084v1)

> With the great achievement of artificial intelligence, vehicle technologies have advanced significantly from human centric driving towards fully automated driving. An intelligent vehicle should be able to understand the driver's perception of the environment as well as controlling behavior of the vehicle. Since high digital map information has been available to provide rich environmental context about static roads, buildings and traffic infrastructures, it would be worthwhile to explore map data capability for driving task understanding. Alternative to commercial used maps, the OpenStreetMap (OSM) data is a free open dataset, which makes it unique for the exploration research. This study is focused on two tasks that leverage OSM for driving environment understanding. First, driving scenario attributes are retrieved from OSM elements, which are combined with vehicle dynamic signals for the driving event recognition. Utilizing steering angle changes and based on a Bi-directional Recurrent Neural Network (Bi-RNN), a driving sequence is segmented and classified as lane-keeping, lane-change-left, lane-change-right, turn-left, and turn-right events. Second, for autonomous driving perception, OSM data can be used to render virtual street views, represented as prior knowledge to fuse with vision/laser systems for road semantic segmentation. Five different types of road masks are generated from OSM, images, and Lidar points, and fused to characterize the drivable space at the driver's perspective. An alternative data-driven approach is based on a Fully Convolutional Network (FCN), OSM availability for deep learning methods are discussed to reveal potential usage on compensating street view images and automatic road semantic annotation.

</details>

<details>

<summary>2019-03-11 09:09:06 - Graph Data on the Web: extend the pivot, don't reinvent the wheel</summary>

- *Fabien Gandon, Franck Michel, Olivier Corby, Michel Buffa, Andrea Tettamanzi, Catherine Faron Zucker, Elena Cabrio, Serena Villata*

- `1903.04181v1` - [abs](http://arxiv.org/abs/1903.04181v1) - [pdf](http://arxiv.org/pdf/1903.04181v1)

> This article is a collective position paper from the Wimmics research team, expressing our vision of how Web graph data technologies should evolve in the future in order to ensure a high-level of interoperability between the many types of applications that produce and consume graph data. Wimmics stands for Web-Instrumented Man-Machine Interactions, Communities, and Semantics. We are a joint research team between INRIA Sophia Antipolis-M{\'e}diterran{\'e}e and I3S (CNRS and Universit{\'e} C{\^o}te d'Azur). Our challenge is to bridge formal semantics and social semantics on the web. Our research areas are graph-oriented knowledge representation, reasoning and operationalization to model and support actors, actions and interactions in web-based epistemic communities. The application of our research is supporting and fostering interactions in online communities and management of their resources. In this position paper, we emphasize the need to extend the semantic Web standard stack to address and fulfill new graph data needs, as well as the importance of remaining compatible with existing recommendations, in particular the RDF stack, to avoid the painful duplication of models, languages, frameworks, etc. The following sections group motivations for different directions of work and collect reasons for the creation of a working group on RDF 2.0 and other recommendations of the RDF family.

</details>

<details>

<summary>2019-03-11 19:57:56 - An Energy-Efficient Configurable Lattice Cryptography Processor for the Quantum-Secure Internet of Things</summary>

- *Utsav Banerjee, Abhishek Pathak, Anantha P. Chandrakasan*

- `1903.04570v1` - [abs](http://arxiv.org/abs/1903.04570v1) - [pdf](http://arxiv.org/pdf/1903.04570v1)

> This paper presents a configurable lattice cryptography processor which enables quantum-resistant security protocols for IoT. Efficient sampling architectures, coupled with a low-power SHA-3 core, provide two orders of magnitude energy savings over software. A single-port RAM-based NTT architecture is proposed, which provides ~124k-gate area savings. This is the first ASIC implementation which demonstrates multiple lattice-based protocols proposed for NIST post-quantum standardization.

</details>

<details>

<summary>2019-03-11 20:21:47 - Revisiting ssFix for Better Program Repair</summary>

- *Qi Xin, Steven P. Reiss*

- `1903.04583v1` - [abs](http://arxiv.org/abs/1903.04583v1) - [pdf](http://arxiv.org/pdf/1903.04583v1)

> A branch of automated program repair (APR) techniques look at finding and reusing existing code for bug repair. ssFix is one of such techniques that is syntactic search-based: it searches a code database for code fragments that are syntactically similar to the bug context and reuses such retrieved code fragments to produce patches. Using such a syntactic approach, ssFix is relatively lightweight and was shown to outperform many other APR techniques. In this paper, to investigate the true effectiveness of ssFix, we conducted multiple experiments to validate ssFix's built-upon assumption (i.e., to see whether it is often possible to reuse existing code for bug repair) and evaluate its code search and code reuse approaches. Our results show that while the basic idea of ssFix, i.e., reusing existing code for bug repair, is promising, the approaches ssFix uses are not the best and can be significantly improved. We proposed a new repair technique sharpFix which follows ssFix's basic idea but differs in the code search and reuse approaches used. We evaluated sharpFix and ssFix on two bug datasets: Defects4J and Bugs.jar-ELIXIR. The results confirm that sharpFix is an improvement over ssFix. For the Defects4J dataset, sharpFix successfully repaired a total of 36 bugs and outperformed many existing repair techniques in repairing more bugs. For the Bugs.jar-ELIXIR dataset, we compared sharpFix, ssFix, and four other APR techniques, and found that sharpFix has the best repair performance. In essence, the paper shows how effective a syntactic search-based approach can be and what techniques should be used for such an approach.

</details>

<details>

<summary>2019-03-12 09:10:46 - Are cracked applications really free? An empirical analysis on Android devices</summary>

- *Konstantinos-Panagiotis Grammatikakis, Angela Ioannou, Stavros Shiaeles, Nicholas Kolokotronis*

- `1903.04793v1` - [abs](http://arxiv.org/abs/1903.04793v1) - [pdf](http://arxiv.org/pdf/1903.04793v1)

> Android is among the popular platforms running on millions of smart devices, like smartphones and tablets, whose widespread adoption is seen as an opportunity for spreading malware. Adding malicious payloads to cracked applications, often popular ones, downloaded from untrusted third markets is a prevalent way for achieving the aforementioned goal. In this paper, we compare 25 applications from the official and third-party application stores delivering cracked applications. The behavioral analysis of applications is carried out on three real devices equipped with different Android versions by using five indicators: requested permissions, CPU usage, RAM usage and the number of opened ports for TCP and HTTP. Based on these indicators, we compute an application intention score and classify cracked applications as malicious or benign. The experimental results show that cracked applications utilize on average more resources and request access to more (dangerous) permissions than their official counterparts.

</details>

<details>

<summary>2019-03-12 19:57:34 - Topological Analysis of Syntactic Structures</summary>

- *Alexander Port, Taelin Karidi, Matilde Marcolli*

- `1903.05181v1` - [abs](http://arxiv.org/abs/1903.05181v1) - [pdf](http://arxiv.org/pdf/1903.05181v1)

> We use the persistent homology method of topological data analysis and dimensional analysis techniques to study data of syntactic structures of world languages. We analyze relations between syntactic parameters in terms of dimensionality, of hierarchical clustering structures, and of non-trivial loops. We show there are relations that hold across language families and additional relations that are family-specific. We then analyze the trees describing the merging structure of persistent connected components for languages in different language families and we show that they partly correlate to historical phylogenetic trees but with significant differences. We also show the existence of interesting non-trivial persistent first homology groups in various language families. We give examples where explicit generators for the persistent first homology can be identified, some of which appear to correspond to homoplasy phenomena, while others may have an explanation in terms of historical linguistics, corresponding to known cases of syntactic borrowing across different language subfamilies.

</details>

<details>

<summary>2019-03-12 21:31:04 - Multiple-Kernel Dictionary Learning for Reconstruction and Clustering of Unseen Multivariate Time-series</summary>

- *Babak Hosseini, Barbara Hammer*

- `1903.01867v3` - [abs](http://arxiv.org/abs/1903.01867v3) - [pdf](http://arxiv.org/pdf/1903.01867v3)

> There exist many approaches for description and recognition of unseen classes in datasets. Nevertheless, it becomes a challenging problem when we deal with multivariate time-series (MTS) (e.g., motion data), where we cannot apply the vectorial algorithms directly to the inputs. In this work, we propose a novel multiple-kernel dictionary learning (MKD) which learns semantic attributes based on specific combinations of MTS dimensions in the feature space. Hence, MKD can fully/partially reconstructs the unseen classes based on the training data (seen classes). Furthermore, we obtain sparse encodings for unseen classes based on the learned MKD attributes, and upon which we propose a simple but effective incremental clustering algorithm to categorize the unseen MTS classes in an unsupervised way. According to the empirical evaluation of our MKD framework on real benchmarks, it provides an interpretable reconstruction of unseen MTS data as well as a high performance regarding their online clustering.

</details>

<details>

<summary>2019-03-12 23:46:30 - Hardware/Software Security Patches for Internet of Trillions of Things</summary>

- *John A. Stankovic, Tu Le, Abdeltawab Hendawi, Yuan Tian*

- `1903.05266v1` - [abs](http://arxiv.org/abs/1903.05266v1) - [pdf](http://arxiv.org/pdf/1903.05266v1)

> With the rapid development of the Internet of Things, there are many interacting devices and applications. One crucial challenge is how to provide security. Our proposal for a new direction is to create "smart buttons" and collections of them called "smart blankets" as hardware/software security patches rather than software-only patches.

</details>

<details>

<summary>2019-03-13 08:14:49 - Survey of Computational Approaches to Lexical Semantic Change</summary>

- *Nina Tahmasebi, Lars Borin, Adam Jatowt*

- `1811.06278v2` - [abs](http://arxiv.org/abs/1811.06278v2) - [pdf](http://arxiv.org/pdf/1811.06278v2)

> Our languages are in constant flux driven by external factors such as cultural, societal and technological changes, as well as by only partially understood internal motivations. Words acquire new meanings and lose old senses, new words are coined or borrowed from other languages and obsolete words slide into obscurity. Understanding the characteristics of shifts in the meaning and in the use of words is useful for those who work with the content of historical texts, the interested general public, but also in and of itself. The findings from automatic lexical semantic change detection, and the models of diachronic conceptual change are currently being incorporated in approaches for measuring document across-time similarity, information retrieval from long-term document archives, the design of OCR algorithms, and so on. In recent years we have seen a surge in interest in the academic community in computational methods and tools supporting inquiry into diachronic conceptual change and lexical replacement. This article is an extract of a survey of recent computational techniques to tackle lexical semantic change currently under review. In this article we focus on diachronic conceptual change as an extension of semantic change.

</details>

<details>

<summary>2019-03-13 09:24:55 - Lost Silence: An emergency response early detection service through continuous processing of telecommunication data streams</summary>

- *Qianru Zhou, Stephen McLaughlin, Alasdair J. G. Gray, Shangbin Wu, Chengxiang Wang*

- `1903.05372v1` - [abs](http://arxiv.org/abs/1903.05372v1) - [pdf](http://arxiv.org/pdf/1903.05372v1)

> Early detection of significant traumatic events, e.g. a terrorist attack or a ship capsizing, is important to ensure that a prompt emergency response can occur. In the modern world telecommunication systems could play a key role in ensuring a successful emergency response by detecting such incidents through significant changes in calls and access to the networks. In this paper a methodology is illustrated to detect such incidents immediately (with the delay in the order of milliseconds), by processing semantically annotated streams of data in cellular telecommunication systems. In our methodology, live information about the position and status of phones are encoded as RDF streams. We propose an algorithm that processes streams of RDF annotated telecommunication data to detect abnormality. Our approach is exemplified in the context of a passenger cruise ship capsizing. However, the approach is readily translatable to other incidents. Our evaluation results show that with a properly chosen window size, such incidents can be detected efficiently and effectively.

</details>

<details>

<summary>2019-03-13 09:30:38 - Weakly-supervised Semantic Parsing with Abstract Examples</summary>

- *Omer Goldman, Veronica Latcinnik, Udi Naveh, Amir Globerson, Jonathan Berant*

- `1711.05240v5` - [abs](http://arxiv.org/abs/1711.05240v5) - [pdf](http://arxiv.org/pdf/1711.05240v5)

> Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential programs needs to be explored at training time to find a correct program. Second, spurious programs that accidentally lead to a correct denotation add noise to training. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far.

</details>

<details>

<summary>2019-03-13 17:06:22 - A Study on Passage Re-ranking in Embedding based Unsupervised Semantic Search</summary>

- *Md Faisal Mahbub Chowdhury, Vijil Chenthamarakshan, Rishav Chakravarti, Alfio M. Gliozzo*

- `1804.08057v4` - [abs](http://arxiv.org/abs/1804.08057v4) - [pdf](http://arxiv.org/pdf/1804.08057v4)

> State of the art approaches for (embedding based) unsupervised semantic search exploits either compositional similarity (of a query and a passage) or pair-wise word (or term) similarity (from the query and the passage). By design, word based approaches do not incorporate similarity in the larger context (query/passage), while compositional similarity based approaches are usually unable to take advantage of the most important cues in the context. In this paper we propose a new compositional similarity based approach, called variable centroid vector (VCVB), that tries to address both of these limitations. We also presents results using a different type of compositional similarity based approach by exploiting universal sentence embedding. We provide empirical evaluation on two different benchmarks.

</details>

<details>

<summary>2019-03-13 19:22:22 - CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning</summary>

- *Ziyu Yao, Jayavardhan Reddy Peddamail, Huan Sun*

- `1904.00720v1` - [abs](http://arxiv.org/abs/1904.00720v1) - [pdf](http://arxiv.org/pdf/1904.00720v1)

> To accelerate software development, much research has been performed to help people understand and reuse the huge amount of available code resources. Two important tasks have been widely studied: code retrieval, which aims to retrieve code snippets relevant to a given natural language query from a code base, and code annotation, where the goal is to annotate a code snippet with a natural language description. Despite their advancement in recent years, the two tasks are mostly explored separately. In this work, we investigate a novel perspective of Code annotation for Code retrieval (hence called `CoaCor'), where a code annotation model is trained to generate a natural language annotation that can represent the semantic meaning of a given code snippet and can be leveraged by a code retrieval model to better distinguish relevant code snippets from others. To this end, we propose an effective framework based on reinforcement learning, which explicitly encourages the code annotation model to generate annotations that can be used for the retrieval task. Through extensive experiments, we show that code annotations generated by our framework are much more detailed and more useful for code retrieval, and they can further improve the performance of existing code retrieval models significantly.

</details>

<details>

<summary>2019-03-13 19:56:04 - ALOHA: Auxiliary Loss Optimization for Hypothesis Augmentation</summary>

- *Ethan M. Rudd, Felipe N. Ducau, Cody Wild, Konstantin Berlin, Richard Harang*

- `1903.05700v1` - [abs](http://arxiv.org/abs/1903.05700v1) - [pdf](http://arxiv.org/pdf/1903.05700v1)

> Malware detection is a popular application of Machine Learning for Information Security (ML-Sec), in which an ML classifier is trained to predict whether a given file is malware or benignware. Parameters of this classifier are typically optimized such that outputs from the model over a set of input samples most closely match the samples' true malicious/benign (1/0) target labels. However, there are often a number of other sources of contextual metadata for each malware sample, beyond an aggregate malicious/benign label, including multiple labeling sources and malware type information (e.g., ransomware, trojan, etc.), which we can feed to the classifier as auxiliary prediction targets. In this work, we fit deep neural networks to multiple additional targets derived from metadata in a threat intelligence feed for Portable Executable (PE) malware and benignware, including a multi-source malicious/benign loss, a count loss on multi-source detections, and a semantic malware attribute tag loss. We find that incorporating multiple auxiliary loss terms yields a marked improvement in performance on the main detection task. We also demonstrate that these gains likely stem from a more informed neural network representation and are not due to a regularization artifact of multi-target learning. Our auxiliary loss architecture yields a significant reduction in detection error rate (false negatives) of 42.6% at a false positive rate (FPR) of $10^{-3}$ when compared to a similar model with only one target, and a decrease of 53.8% at $10^{-5}$ FPR.

</details>

<details>

<summary>2019-03-14 01:16:30 - A Novel Re-Targetable Application Development Platform for Healthcare Mobile Applications</summary>

- *Chae Ho Cho, Fatemehsadat Tabei, Tra Nguyen Phan, Yeesock Kim, Jo Woon Chong*

- `1903.05783v1` - [abs](http://arxiv.org/abs/1903.05783v1) - [pdf](http://arxiv.org/pdf/1903.05783v1)

> The rapid enhancement of central power unit CPU performance enables the development of computationally-intensive healthcare mobile applications for smartphones and wearable devices. However, computationally intensive mobile applications require significant application development time during the application porting procedure when the number of considering target devices operating systems OSs is large. In this paper, we propose a novel retargetable application development platform for healthcare mobile applications, which reduces application development time with maintaining the performance of the algorithm. Although the number of applications target OSs increases, the amount of time required for the code conversion step in the application porting procedure remains constant in the proposed retargetable platform. Experimental results show that our proposed retargetable platform gives reduced application development time compared to the conventional platform with maintaining the performance of the mobile application.

</details>

<details>

<summary>2019-03-14 08:31:05 - MirrorGAN: Learning Text-to-image Generation by Redescription</summary>

- *Tingting Qiao, Jing Zhang, Duanqing Xu, Dacheng Tao*

- `1903.05854v1` - [abs](http://arxiv.org/abs/1903.05854v1) - [pdf](http://arxiv.org/pdf/1903.05854v1)

> Generating an image from a given text description has two goals: visual realism and semantic consistency. Although significant progress has been made in generating high-quality and visually realistic images using generative adversarial networks, guaranteeing semantic consistency between the text description and visual content remains very challenging. In this paper, we address this problem by proposing a novel global-local attentive and semantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN exploits the idea of learning text-to-image generation by redescription and consists of three modules: a semantic text embedding module (STEM), a global-local collaborative attentive module for cascaded image generation (GLAM), and a semantic text regeneration and alignment module (STREAM). STEM generates word- and sentence-level embeddings. GLAM has a cascaded architecture for generating target images from coarse to fine scales, leveraging both local word attention and global sentence attention to progressively enhance the diversity and semantic consistency of the generated images. STREAM seeks to regenerate the text description from the generated image, which semantically aligns with the given text description. Thorough experiments on two public benchmark datasets demonstrate the superiority of MirrorGAN over other representative state-of-the-art methods.

</details>

<details>

<summary>2019-03-14 09:37:53 - Interactive Concept Mining on Personal Data -- Bootstrapping Semantic Services</summary>

- *Markus Schröder, Christian Jilek, Andreas Dengel*

- `1903.05872v1` - [abs](http://arxiv.org/abs/1903.05872v1) - [pdf](http://arxiv.org/pdf/1903.05872v1)

> Semantic services (e.g. Semantic Desktops) are still afflicted by a cold start problem: in the beginning, the user's personal information sphere, i.e. files, mails, bookmarks, etc., is not represented by the system. Information extraction tools used to kick-start the system typically create 1:1 representations of the different information items. Higher level concepts, for example found in file names, mail subjects or in the content body of these items, are not extracted. Leaving these concepts out may lead to underperformance, having to many of them (e.g. by making every found term a concept) will clutter the arising knowledge graph with non-helpful relations. In this paper, we present an interactive concept mining approach proposing concept candidates gathered by exploiting given schemata of usual personal information management applications and analysing the personal information sphere using various metrics. To heed the subjective view of the user, a graphical user interface allows to easily rank and give feedback on proposed concept candidates, thus keeping only those actually considered relevant. A prototypical implementation demonstrates major steps of our approach.

</details>

<details>

<summary>2019-03-14 19:14:10 - Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator</summary>

- *Badri N. Patro, Vinod K. Kurmi, Sandeep Kumar, Vinay P. Namboodiri*

- `1806.00807v5` - [abs](http://arxiv.org/abs/1806.00807v5) - [pdf](http://arxiv.org/pdf/1806.00807v5)

> In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant.

</details>

<details>

<summary>2019-03-14 20:36:13 - A Formal Verification Technique for Architecture-based Embedded Systems in EAST-ADL</summary>

- *Eun-Young Kang*

- `1903.06241v1` - [abs](http://arxiv.org/abs/1903.06241v1) - [pdf](http://arxiv.org/pdf/1903.06241v1)

> Development of quality assured software-intensive systems, such as automotive embedded systems, is an increasing challenge as the complexity of these systems significantly increases. EAST-ADL is an architecture description language developed to specify automotive embedded system architectures at multiple abstraction levels in the development of safety-critical automotive products. In this paper, we propose an architecture-based verification technique which enhances the model-based development process supported by EAST-ADL by adapting model-checking to EAST-ADL specifications. We employ UPPAAL as a verification tool to ensure that predicted function behaviors of the models in EAST-ADL satisfy functional and real-time requirements. The criteria for this architecture-based verification is presented and the transformation rules which comply with this criteria are derived. This enables us to extract the relevant information from EAST-ADL specifications and to generate analyzable UPPAAL models. The formal semantics of EAST-ADL is defined which is essential to automate the verification of EAST-ADL specifications. Our approach is demonstrated by verifying the safety of the steering truck system units.

</details>

<details>

<summary>2019-03-15 08:37:29 - Improving the Generalization of Adversarial Training with Domain Adaptation</summary>

- *Chuanbiao Song, Kun He, Liwei Wang, John E. Hopcroft*

- `1810.00740v7` - [abs](http://arxiv.org/abs/1810.00740v7) - [pdf](http://arxiv.org/pdf/1810.00740v7)

> By injecting adversarial examples into training data, adversarial training is promising for improving the robustness of deep learning models. However, most existing adversarial training approaches are based on a specific type of adversarial attack. It may not provide sufficiently representative samples from the adversarial domain, leading to a weak generalization ability on adversarial examples from other attacks. Moreover, during the adversarial training, adversarial perturbations on inputs are usually crafted by fast single-step adversaries so as to scale to large datasets. This work is mainly focused on the adversarial training yet efficient FGSM adversary. In this scenario, it is difficult to train a model with great generalization due to the lack of representative adversarial samples, aka the samples are unable to accurately reflect the adversarial domain. To alleviate this problem, we propose a novel Adversarial Training with Domain Adaptation (ATDA) method. Our intuition is to regard the adversarial training on FGSM adversary as a domain adaption task with limited number of target domain samples. The main idea is to learn a representation that is semantically meaningful and domain invariant on the clean domain as well as the adversarial domain. Empirical evaluations on Fashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can greatly improve the generalization of adversarial training and the smoothness of the learned models, and outperforms state-of-the-art methods on standard benchmark datasets. To show the transfer ability of our method, we also extend ATDA to the adversarial training on iterative attacks such as PGD-Adversial Training (PAT) and the defense performance is improved considerably.

</details>

<details>

<summary>2019-03-15 18:02:58 - CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</summary>

- *Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant*

- `1811.00937v2` - [abs](http://arxiv.org/abs/1811.00937v2) - [pdf](http://arxiv.org/pdf/1811.00937v2)

> When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.

</details>

<details>

<summary>2019-03-15 21:20:48 - Image classification and retrieval with random depthwise signed convolutional neural networks</summary>

- *Yunzhe Xue, Usman Roshan*

- `1806.05789v3` - [abs](http://arxiv.org/abs/1806.05789v3) - [pdf](http://arxiv.org/pdf/1806.05789v3)

> We propose a random convolutional neural network to generate a feature space in which we study image classification and retrieval performance. Put briefly we apply random convolutional blocks followed by global average pooling to generate a new feature, and we repeat this k times to produce a k-dimensional feature space. This can be interpreted as partitioning the space of image patches with random hyperplanes which we formalize as a random depthwise convolutional neural network. In the network's final layer we perform image classification and retrieval with the linear support vector machine and k-nearest neighbor classifiers and study other empirical properties. We show that the ratio of image pixel distribution similarity across classes to within classes is higher in our network's final layer compared to the input space. When we apply the linear support vector machine for image classification we see that the accuracy is higher than if we were to train just the final layer of VGG16, ResNet18, and DenseNet40 with random weights. In the same setting we compare it to an unsupervised feature learning method and find our accuracy to be comparable on CIFAR10 but higher on CIFAR100 and STL10. We see that the accuracy is not far behind that of trained networks, particularly in the top-k setting. For example the top-2 accuracy of our network is near 90% on both CIFAR10 and a 10-class mini ImageNet, and 85% on STL10. We find that k-nearest neighbor gives a comparable precision on the Corel Princeton Image Similarity Benchmark than if we were to use the final layer of trained networks. As with other networks we find that our network fails to a black box attack even though we lack a gradient and use the sign activation. We highlight sensitivity of our network to background as a potential pitfall and an advantage. Overall our work pushes the boundary of what can be achieved with random weights.

</details>

<details>

<summary>2019-03-15 22:58:13 - Are My Invariants Valid? A Learning Approach</summary>

- *Vincent J. Hellendoorn, Premkumar T. Devanbu, Oleksandr Polozov, Mark Marron*

- `1903.06089v2` - [abs](http://arxiv.org/abs/1903.06089v2) - [pdf](http://arxiv.org/pdf/1903.06089v2)

> Ensuring that a program operates correctly is a difficult task in large, complex systems. Enshrining invariants -- desired properties of correct execution -- in code or comments can support maintainability and help sustain correctness. Tools that can automatically infer and recommend invariants can thus be very beneficial. However, current invariant-suggesting tools, such as Daikon, suffer from high rates of false positives, in part because they only leverage traced program values from available test cases, rather than directly exploiting knowledge of the source code per se. We propose a machine-learning approach to judging the validity of invariants, specifically of method pre- and post-conditions, based directly on a method's source code. We introduce a new, scalable approach to creating labeled invariants: using programs with large test-suites, we generate Daikon invariants using traces from subsets of these test-suites, and then label these as valid/invalid by cross-validating them with held-out tests. This process induces a large set of labels that provide a form of noisy supervision, which is then used to train a deep neural model, based on gated graph neural networks. Our model learns to map the lexical, syntactic, and semantic structure of a given method's body into a probability that a candidate pre- or post-condition on that method's body is correct and is able to accurately label invariants based on the noisy signal, even in cross-project settings. Most importantly, it performs well on a hand-curated dataset of invariants.

</details>

<details>

<summary>2019-03-16 07:44:10 - Concatenated Feature Pyramid Network for Instance Segmentation</summary>

- *Yongqing Sun, Pranav Shenoy K P, Jun Shimamura, Atsushi Sagata*

- `1904.00768v1` - [abs](http://arxiv.org/abs/1904.00768v1) - [pdf](http://arxiv.org/pdf/1904.00768v1)

> Low level features like edges and textures play an important role in accurately localizing instances in neural networks. In this paper, we propose an architecture which improves feature pyramid networks commonly used instance segmentation networks by incorporating low level features in all layers of the pyramid in an optimal and efficient way. Specifically, we introduce a new layer which learns new correlations from feature maps of multiple feature pyramid levels holistically and enhances the semantic information of the feature pyramid to improve accuracy. Our architecture is simple to implement in instance segmentation or object detection frameworks to boost accuracy. Using this method in Mask RCNN, our model achieves consistent improvement in precision on COCO Dataset with the computational overhead compared to the original feature pyramid network.

</details>

<details>

<summary>2019-03-16 08:24:39 - Recover and RELAX: Concern-Oriented Software Architecture Recovery for Systems Development and Maintenance</summary>

- *Daniel Link, Pooyan Behnamghader, Ramin Moazeni, Barry Boehm*

- `1903.06895v1` - [abs](http://arxiv.org/abs/1903.06895v1) - [pdf](http://arxiv.org/pdf/1903.06895v1)

> The stakeholders of a system are legitimately interested in whether and how its architecture reflects their respective concerns at each point of its development and maintenance processes. Having such knowledge available at all times would enable them to continually adjust their systems structure at each juncture and reduce the buildup of technical debt that can be hard to reduce once it has persisted over many iterations. Unfortunately, software systems often lack reliable and current documentation about their architecture. In order to remedy this situation, researchers have conceived a number of architectural recovery methods, some of them concern-oriented. However, the design choices forming the bases of most existing recovery methods make it so none of them have a complete set of desirable qualities for the purpose stated above. Tailoring a recovery to a system is either not possible or only through iterative experiments with numeric parameters. Furthermore, limitations in their scalability make it prohibitive to apply the existing techniques to large systems. Finally, since several current recovery methods employ non-deterministic sampling, their inconsistent results do not lend themselves well to tracking a systems course over several versions, as needed by its stakeholders. RELAX (RELiable Architecture EXtraction), a new concern-based recovery method that uses text classification, addresses these issues efficiently by (1) assembling the overall recovery result from smaller, independent parts, (2) basing it on an algorithm with linear time complexity and (3) being tailorable to the recovery of a single system or a sequence thereof through the selection of meaningfully named, semantic topics. An intuitive, informative architectural visualization rounds out RELAX's contributions. RELAX is illustrated on a number of existing open-source systems and compared to other recovery methods.

</details>

<details>

<summary>2019-03-16 18:08:37 - Feather: A Feature Model Transformation Language</summary>

- *Ahmet Serkan Karataş*

- `1903.06965v1` - [abs](http://arxiv.org/abs/1903.06965v1) - [pdf](http://arxiv.org/pdf/1903.06965v1)

> Feature modeling has been a very popular approach for variability management in software product lines. Building a feature model requires substantial domain expertise, however, even experts cannot foresee all future possibilities. Changing requirements can force a feature model to evolve in order to adapt to the new conditions. Feather is a language to describe model transformations that will evolve a feature model. This article presents the structure and foundations of Feather. First, the language elements, which consist of declarations to characterize the model to evolve and commands to manipulate its structure, are introduced. Then, semantics grounding in feature model properties are given for the commands in order to provide precise command definitions. Next, an interpreter that can realize the transformations described by the commands in a Feather script is presented. Finally, effectiveness of the language is discussed using two realistic examples, where one of the examples includes a system from a dynamic environment and the other employs a system that has a large feature model containing 1,227 features.

</details>

<details>

<summary>2019-03-17 17:42:29 - Topic-Guided Variational Autoencoders for Text Generation</summary>

- *Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan Shen, Changyou Chen, Lawrence Carin*

- `1903.07137v1` - [abs](http://arxiv.org/abs/1903.07137v1) - [pdf](http://arxiv.org/pdf/1903.07137v1)

> We propose a topic-guided variational autoencoder (TGVAE) model for text generation. Distinct from existing variational autoencoder (VAE) based approaches, which assume a simple Gaussian prior for the latent code, our model specifies the prior as a Gaussian mixture model (GMM) parametrized by a neural topic module. Each mixture component corresponds to a latent topic, which provides guidance to generate sentences under the topic. The neural topic module and the VAE-based neural sequence module in our model are learned jointly. In particular, a sequence of invertible Householder transformations is applied to endow the approximate posterior of the latent code with high flexibility during model inference. Experimental results show that our TGVAE outperforms alternative approaches on both unconditional and conditional text generation, which can generate semantically-meaningful sentences with various topics.

</details>

<details>

<summary>2019-03-18 16:55:47 - SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded Scene Representations</summary>

- *Shuaifeng Zhi, Michael Bloesch, Stefan Leutenegger, Andrew J. Davison*

- `1903.06482v2` - [abs](http://arxiv.org/abs/1903.06482v2) - [pdf](http://arxiv.org/pdf/1903.06482v2)

> Systems which incrementally create 3D semantic maps from image sequences must store and update representations of both geometry and semantic entities. However, while there has been much work on the correct formulation for geometrical estimation, state-of-the-art systems usually rely on simple semantic representations which store and update independent label estimates for each surface element (depth pixels, surfels, or voxels). Spatial correlation is discarded, and fused label maps are incoherent and noisy.   We introduce a new compact and optimisable semantic representation by training a variational auto-encoder that is conditioned on a colour image. Using this learned latent space, we can tackle semantic label fusion by jointly optimising the low-dimenional codes associated with each of a set of overlapping images, producing consistent fused label maps which preserve spatial correlation. We also show how this approach can be used within a monocular keyframe based semantic mapping system where a similar code approach is used for geometry. The probabilistic formulation allows a flexible formulation where we can jointly estimate motion, geometry and semantics in a unified optimisation.

</details>

<details>

<summary>2019-03-18 17:17:34 - Knowledge Graph Convolutional Networks for Recommender Systems</summary>

- *Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, Minyi Guo*

- `1904.12575v1` - [abs](http://arxiv.org/abs/1904.12575v1) - [pdf](http://arxiv.org/pdf/1904.12575v1)

> To alleviate sparsity and cold start problem of collaborative filtering based recommender systems, researchers and engineers usually collect attributes of users and items, and design delicate algorithms to exploit these additional information. In general, the attributes are not isolated but connected with each other, which forms a knowledge graph (KG). In this paper, we propose Knowledge Graph Convolutional Networks (KGCN), an end-to-end framework that captures inter-item relatedness effectively by mining their associated attributes on the KG. To automatically discover both high-order structure information and semantic information of the KG, we sample from the neighbors for each entity in the KG as their receptive field, then combine neighborhood information with bias when calculating the representation of a given entity. The receptive field can be extended to multiple hops away to model high-order proximity information and capture users' potential long-distance interests. Moreover, we implement the proposed KGCN in a minibatch fashion, which enables our model to operate on large datasets and KGs. We apply the proposed model to three datasets about movie, book, and music recommendation, and experiment results demonstrate that our approach outperforms strong recommender baselines.

</details>

<details>

<summary>2019-03-18 20:27:14 - A Survey of Electromagnetic Side-Channel Attacks and Discussion on their Case-Progressing Potential for Digital Forensics</summary>

- *Asanka Sayakkara, Nhien-An Le-Khac, Mark Scanlon*

- `1903.07703v1` - [abs](http://arxiv.org/abs/1903.07703v1) - [pdf](http://arxiv.org/pdf/1903.07703v1)

> The increasing prevalence of Internet of Things (IoT) devices has made it inevitable that their pertinence to digital forensic investigations will increase into the foreseeable future. These devices produced by various vendors often posses limited standard interfaces for communication, such as USB ports or WiFi/Bluetooth wireless interfaces. Meanwhile, with an increasing mainstream focus on the security and privacy of user data, built-in encryption is becoming commonplace in consumer-level computing devices, and IoT devices are no exception. Under these circumstances, a significant challenge is presented to digital forensic investigations where data from IoT devices needs to be analysed. This work explores the electromagnetic (EM) side-channel analysis literature for the purpose of assisting digital forensic investigations on IoT devices. EM side-channel analysis is a technique where unintentional electromagnetic emissions are used for eavesdropping on the operations and data handling of computing devices. The non-intrusive nature of EM side-channel approaches makes it a viable option to assist digital forensic investigations as these attacks require, and must result in, no modification to the target device. The literature on various EM side-channel analysis attack techniques are discussed - selected on the basis of their applicability in IoT device investigation scenarios. The insight gained from the background study is used to identify promising future applications of the technique for digital forensic analysis on IoT devices - potentially progressing a wide variety of currently hindered digital investigations.

</details>

<details>

<summary>2019-03-18 21:00:13 - Polyglot Contextual Representations Improve Crosslingual Transfer</summary>

- *Phoebe Mulcaire, Jungo Kasai, Noah A. Smith*

- `1902.09697v2` - [abs](http://arxiv.org/abs/1902.09697v2) - [pdf](http://arxiv.org/pdf/1902.09697v2)

> We introduce Rosita, a method to produce multilingual contextual word representations by training a single language model on text from multiple languages. Our method combines the advantages of contextual word representations with those of multilingual representation learning. We produce language models from dissimilar language pairs (English/Arabic and English/Chinese) and use them in dependency parsing, semantic role labeling, and named entity recognition, with comparisons to monolingual and non-contextual variants. Our results provide further evidence for the benefits of polyglot learning, in which representations are shared across multiple languages.

</details>

<details>

<summary>2019-03-19 01:37:05 - On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models</summary>

- *Paul Michel, Xian Li, Graham Neubig, Juan Miguel Pino*

- `1903.06620v2` - [abs](http://arxiv.org/abs/1903.06620v2) - [pdf](http://arxiv.org/pdf/1903.06620v2)

> Adversarial examples --- perturbations to the input of a model that elicit large changes in the output --- have been shown to be an effective way of assessing the robustness of sequence-to-sequence (seq2seq) models. However, these perturbations only indicate weaknesses in the model if they do not change the input so significantly that it legitimately results in changes in the expected output. This fact has largely been ignored in the evaluations of the growing body of related literature. Using the example of untargeted attacks on machine translation (MT), we propose a new evaluation framework for adversarial attacks on seq2seq models that takes the semantic equivalence of the pre- and post-perturbation input into account. Using this framework, we demonstrate that existing methods may not preserve meaning in general, breaking the aforementioned assumption that source side perturbations should not result in changes in the expected output. We further use this framework to demonstrate that adding additional constraints on attacks allows for adversarial perturbations that are more meaning-preserving, but nonetheless largely change the output sequence. Finally, we show that performing untargeted adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness, without hurting test performance. A toolkit implementing our evaluation framework is released at https://github.com/pmichel31415/teapot-nlp.

</details>

<details>

<summary>2019-03-19 03:12:53 - Agent Embeddings: A Latent Representation for Pole-Balancing Networks</summary>

- *Oscar Chang, Robert Kwiatkowski, Siyuan Chen, Hod Lipson*

- `1811.04516v4` - [abs](http://arxiv.org/abs/1811.04516v4) - [pdf](http://arxiv.org/pdf/1811.04516v4)

> We show that it is possible to reduce a high-dimensional object like a neural network agent into a low-dimensional vector representation with semantic meaning that we call agent embeddings, akin to word or face embeddings. This can be done by collecting examples of existing networks, vectorizing their weights, and then learning a generative model over the weight space in a supervised fashion. We investigate a pole-balancing task, Cart-Pole, as a case study and show that multiple new pole-balancing networks can be generated from their agent embeddings without direct access to training data from the Cart-Pole simulator. In general, the learned embedding space is helpful for mapping out the space of solutions for a given task. We observe in the case of Cart-Pole the surprising finding that good agents make different decisions despite learning similar representations, whereas bad agents make similar (bad) decisions while learning dissimilar representations. Linearly interpolating between the latent embeddings for a good agent and a bad agent yields an agent embedding that generates a network with intermediate performance, where the performance can be tuned according to the coefficient of interpolation. Linear extrapolation in the latent space also results in performance boosts, up to a point.

</details>

<details>

<summary>2019-03-19 07:05:59 - Personalized Neural Embeddings for Collaborative Filtering with Text</summary>

- *Guangneng Hu*

- `1903.07860v1` - [abs](http://arxiv.org/abs/1903.07860v1) - [pdf](http://arxiv.org/pdf/1903.07860v1)

> Collaborative filtering (CF) is a core technique for recommender systems. Traditional CF approaches exploit user-item relations (e.g., clicks, likes, and views) only and hence they suffer from the data sparsity issue. Items are usually associated with unstructured text such as article abstracts and product reviews. We develop a Personalized Neural Embedding (PNE) framework to exploit both interactions and words seamlessly. We learn such embeddings of users, items, and words jointly, and predict user preferences on items based on these learned representations. PNE estimates the probability that a user will like an item by two terms---behavior factors and semantic factors. On two real-world datasets, PNE shows better performance than four state-of-the-art baselines in terms of three metrics. We also show that PNE learns meaningful word embeddings by visualization.

</details>

<details>

<summary>2019-03-19 10:11:54 - Practical Semantic Parsing for Spoken Language Understanding</summary>

- *Marco Damonte, Rahul Goel, Tagyoung Chung*

- `1903.04521v3` - [abs](http://arxiv.org/abs/1903.04521v3) - [pdf](http://arxiv.org/pdf/1903.04521v3)

> Executable semantic parsing is the task of converting natural language utterances into logical forms that can be directly used as queries to get a response. We build a transfer learning framework for executable semantic parsing. We show that the framework is effective for Question Answering (Q&A) as well as for Spoken Language Understanding (SLU). We further investigate the case where a parser on a new domain can be learned by exploiting data on other domains, either via multi-task learning between the target domain and an auxiliary domain or via pre-training on the auxiliary domain and fine-tuning on the target domain. With either flavor of transfer learning, we are able to improve performance on most domains; we experiment with public data sets such as Overnight and NLmaps as well as with commercial SLU data. The experiments carried out on data sets that are different in nature show how executable semantic parsing can unify different areas of NLP such as Q&A and SLU.

</details>

<details>

<summary>2019-03-19 15:13:18 - Hierarchy-based Image Embeddings for Semantic Image Retrieval</summary>

- *Björn Barz, Joachim Denzler*

- `1809.09924v4` - [abs](http://arxiv.org/abs/1809.09924v4) - [pdf](http://arxiv.org/pdf/1809.09924v4)

> Deep neural networks trained for classification have been found to learn powerful image representations, which are also often used for other tasks such as comparing images w.r.t. their visual similarity. However, visual similarity does not imply semantic similarity. In order to learn semantically discriminative features, we propose to map images onto class embeddings whose pair-wise dot products correspond to a measure of semantic similarity between classes. Such an embedding does not only improve image retrieval results, but could also facilitate integrating semantics for other tasks, e.g., novelty detection or few-shot learning. We introduce a deterministic algorithm for computing the class centroids directly based on prior world-knowledge encoded in a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds, and ImageNet show that our learned semantic image embeddings improve the semantic consistency of image retrieval results by a large margin.

</details>

<details>

<summary>2019-03-19 15:20:19 - A Query System for Efficiently Investigating Complex Attack Behaviors for Enterprise Security</summary>

- *Peng Gao, Xusheng Xiao, Zhichun Li, Kangkook Jee, Fengyuan Xu, Sanjeev R. Kulkarni, Prateek Mittal*

- `1810.03464v3` - [abs](http://arxiv.org/abs/1810.03464v3) - [pdf](http://arxiv.org/pdf/1810.03464v3)

> The need for countering Advanced Persistent Threat (APT) attacks has led to the solutions that ubiquitously monitor system activities in each enterprise host, and perform timely attack investigation over the monitoring data for uncovering the attack sequence. However, existing general-purpose query systems lack explicit language constructs for expressing key properties of major attack behaviors, and their semantics-agnostic design often produces inefficient execution plans for queries. To address these limitations, we build AIQL, a novel query system that is designed with novel types of domain-specific optimizations to enable efficient attack investigation. AIQL provides (1) domain-specific data model and storage for storing the massive system monitoring data, (2) a domain-specific query language, Attack Investigation Query Language (AIQL) that integrates critical primitives for expressing major attack behaviors, and (3) an optimized query engine based on the characteristics of the data and the semantics of the query to efficiently schedule the execution. We have deployed AIQL in NEC Labs America comprising 150 hosts. In our demo, we aim to show the complete usage scenario of AIQL by (1) performing an APT attack in a controlled environment, and (2) using AIQL to investigate such attack by querying the collected system monitoring data that contains the attack traces. The audience will have the option to perform the APT attack themselves under our guidance, and interact with the system and investigate the attack via issuing queries and checking the query results through our web UI.

</details>

<details>

<summary>2019-03-20 00:51:01 - Deep Neural Networks Improve Radiologists' Performance in Breast Cancer Screening</summary>

- *Nan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, Stanisław Jastrzębski, Thibault Févry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, Sushma Gaddam, Leng Leng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, Hildegard Toth, Kristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, Stephanie Chung, Esther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras*

- `1903.08297v1` - [abs](http://arxiv.org/abs/1903.08297v1) - [pdf](http://arxiv.org/pdf/1903.08297v1)

> We present a deep convolutional neural network for breast cancer screening exam classification, trained and evaluated on over 200,000 exams (over 1,000,000 images). Our network achieves an AUC of 0.895 in predicting whether there is a cancer in the breast, when tested on the screening population. We attribute the high accuracy of our model to a two-stage training procedure, which allows us to use a very high-capacity patch-level network to learn from pixel-level labels alongside a network learning from macroscopic breast-level labels. To validate our model, we conducted a reader study with 14 readers, each reading 720 screening mammogram exams, and find our model to be as accurate as experienced radiologists when presented with the same data. Finally, we show that a hybrid model, averaging probability of malignancy predicted by a radiologist with a prediction of our neural network, is more accurate than either of the two separately. To better understand our results, we conduct a thorough analysis of our network's performance on different subpopulations of the screening population, model design, training procedure, errors, and properties of its internal representations.

</details>

<details>

<summary>2019-03-20 02:20:04 - Tracking the Evolution of Words with Time-reflective Text Representations</summary>

- *Roberto Camacho Barranco, Raimundo F. Dos Santos, M. Shahriar Hossain*

- `1807.04441v2` - [abs](http://arxiv.org/abs/1807.04441v2) - [pdf](http://arxiv.org/pdf/1807.04441v2)

> More than 80% of today's data is unstructured in nature, and these unstructured datasets evolve over time. A large part of these datasets are text documents generated by media outlets, scholarly articles in digital libraries, findings from scientific and professional communities, and social media. Vector space models were developed to analyze text data using data mining and machine learning algorithms. While ample vector space models exist for text data, the evolutionary aspect of ever-changing text corpora is still missing in vector-based representations. The advent of word embeddings has enabled us to create a contextual vector space, but the embeddings fail to consider the temporal aspects of the feature space successfully. This paper presents an approach to include temporal aspects in feature spaces. The inclusion of the time aspect in the feature space provides vectors for every natural language element, such as words or entities, at every timestamp. Such temporal word vectors allow us to track how the meaning of a word changes over time, by studying the changes in its neighborhood. Moreover, a time-reflective text representation will pave the way to a new set of text analytic abilities involving time series for text collections. In this paper, we present a time-reflective vector space model for temporal text data that is able to capture short and long-term changes in the meaning of words. We compare our approach with the limited literature on dynamic embeddings. We present qualitative and quantitative evaluations using the tracking of semantic evolution as the target application.

</details>

<details>

<summary>2019-03-20 02:57:07 - Activation Analysis of a Byte-Based Deep Neural Network for Malware Classification</summary>

- *Scott E. Coull, Christopher Gardner*

- `1903.04717v2` - [abs](http://arxiv.org/abs/1903.04717v2) - [pdf](http://arxiv.org/pdf/1903.04717v2)

> Feature engineering is one of the most costly aspects of developing effective machine learning models, and that cost is even greater in specialized problem domains, like malware classification, where expert skills are necessary to identify useful features. Recent work, however, has shown that deep learning models can be used to automatically learn feature representations directly from the raw, unstructured bytes of the binaries themselves. In this paper, we explore what these models are learning about malware. To do so, we examine the learned features at multiple levels of resolution, from individual byte embeddings to end-to-end analysis of the model. At each step, we connect these byte-oriented activations to their original semantics through parsing and disassembly of the binary to arrive at human-understandable features. Through our results, we identify several interesting features learned by the model and their connection to manually-derived features typically used by traditional machine learning models. Additionally, we explore the impact of training data volume and regularization on the quality of the learned features and the efficacy of the classifiers, revealing the somewhat paradoxical insight that better generalization does not necessarily result in better performance for byte-based malware classifiers.

</details>

<details>

<summary>2019-03-20 06:00:30 - A Methodology for Search Space Reduction in QoS Aware Semantic Web Service Composition</summary>

- *Soumi Chattopadhyay, Ansuman Banerjee*

- `1809.07045v2` - [abs](http://arxiv.org/abs/1809.07045v2) - [pdf](http://arxiv.org/pdf/1809.07045v2)

> The semantic information regulates the expressiveness of a web service. State-of-the-art approaches in web services research have used the semantics of a web service for different purposes, mainly for service discovery, composition, execution etc. In this paper, our main focus is on semantic driven Quality of Service (QoS) aware service composition. Most of the contemporary approaches on service composition have used the semantic information to combine the services appropriately to generate the composition solution. However, in this paper, our intention is to use the semantic information to expedite the service composition algorithm. Here, we present a service composition framework that uses semantic information of a web service to generate different clusters, where the services are semantically related within a cluster. Our final aim is to construct a composition solution using these clusters that can efficiently scale to large service spaces, while ensuring solution quality. Experimental results show the efficiency of our proposed method.

</details>

<details>

<summary>2019-03-20 08:53:05 - Contextual Compositionality Detection with External Knowledge Bases andWord Embeddings</summary>

- *Dongsheng Wang, Quichi Li, Lucas Chaves Lima, Jakob grue Simonsen, Christina Lioma*

- `1903.08389v1` - [abs](http://arxiv.org/abs/1903.08389v1) - [pdf](http://arxiv.org/pdf/1903.08389v1)

> When the meaning of a phrase cannot be inferred from the individual meanings of its words (e.g., hot dog), that phrase is said to be non-compositional. Automatic compositionality detection in multi-word phrases is critical in any application of semantic processing, such as search engines; failing to detect non-compositional phrases can hurt system effectiveness notably. Existing research treats phrases as either compositional or non-compositional in a deterministic manner. In this paper, we operationalize the viewpoint that compositionality is contextual rather than deterministic, i.e., that whether a phrase is compositional or non-compositional depends on its context. For example, the phrase `green card' is compositional when referring to a green colored card, whereas it is non-compositional when meaning permanent residence authorization. We address the challenge of detecting this type of contextual compositionality as follows: given a multi-word phrase, we enrich the word embedding representing its semantics with evidence about its global context (terms it often collocates with) as well as its local context (narratives where that phrase is used, which we call usage scenarios). We further extend this representation with information extracted from external knowledge bases. The resulting representation incorporates both localized context and more general usage of the phrase and allows to detect its compositionality in a non-deterministic and contextual way. Empirical evaluation of our model on a dataset of phrase compositionality, manually collected by crowdsourcing contextual compositionality assessments, shows that our model outperforms state-of-the-art baselines notably on detecting phrase compositionality.

</details>

<details>

<summary>2019-03-20 09:40:19 - Neural Check-Worthiness Ranking with Weak Supervision: Finding Sentences for Fact-Checking</summary>

- *Casper Hansen, Christian Hansen, Stephen Alstrup, Jakob Grue Simonsen, Christina Lioma*

- `1903.08404v1` - [abs](http://arxiv.org/abs/1903.08404v1) - [pdf](http://arxiv.org/pdf/1903.08404v1)

> Automatic fact-checking systems detect misinformation, such as fake news, by (i) selecting check-worthy sentences for fact-checking, (ii) gathering related information to the sentences, and (iii) inferring the factuality of the sentences. Most prior research on (i) uses hand-crafted features to select check-worthy sentences, and does not explicitly account for the recent finding that the top weighted terms in both check-worthy and non-check-worthy sentences are actually overlapping [15]. Motivated by this, we present a neural check-worthiness sentence ranking model that represents each word in a sentence by \textit{both} its embedding (aiming to capture its semantics) and its syntactic dependencies (aiming to capture its role in modifying the semantics of other terms in the sentence). Our model is an end-to-end trainable neural network for check-worthiness ranking, which is trained on large amounts of unlabelled data through weak supervision. Thorough experimental evaluation against state of the art baselines, with and without weak supervision, shows our model to be superior at all times (+13% in MAP and +28% at various Precision cut-offs from the best baseline with statistical significance). Empirical analysis of the use of weak supervision, word embedding pretraining on domain-specific data, and the use of syntactic dependencies of our model reveals that check-worthy sentences contain notably more identical syntactic dependencies than non-check-worthy sentences.

</details>

<details>

<summary>2019-03-20 12:46:10 - CaosDB - Research Data Management for Complex, Changing, and Automated Research Workflows</summary>

- *Timm Fitschen, Alexander Schlemmer, Daniel Hornung, Henrik tom Wörden, Ulrich Parlitz, Stefan Luther*

- `1801.07653v2` - [abs](http://arxiv.org/abs/1801.07653v2) - [pdf](http://arxiv.org/pdf/1801.07653v2)

> Here we present CaosDB, a Research Data Management System (RDMS) designed to ensure seamless integration of inhomogeneous data sources and repositories of legacy data. Its primary purpose is the management of data from biomedical sciences, both from simulations and experiments during the complete research data lifecycle. An RDMS for this domain faces particular challenges: Research data arise in huge amounts, from a wide variety of sources, and traverse a highly branched path of further processing. To be accepted by its users, an RDMS must be built around workflows of the scientists and practices and thus support changes in workflow and data structure. Nevertheless it should encourage and support the development and observation of standards and furthermore facilitate the automation of data acquisition and processing with specialized software. The storage data model of an RDMS must reflect these complexities with appropriate semantics and ontologies while offering simple methods for finding, retrieving, and understanding relevant data. We show how CaosDB responds to these challenges and give an overview of the CaosDB Server, its data model and its easy-to-learn CaosDB Query Language. We briefly discuss the status of the implementation, how we currently use CaosDB, and how we plan to use and extend it.

</details>

<details>

<summary>2019-03-20 14:35:16 - Ontology of Card Sleights</summary>

- *Aaron Sterling*

- `1903.08523v1` - [abs](http://arxiv.org/abs/1903.08523v1) - [pdf](http://arxiv.org/pdf/1903.08523v1)

> We present a machine-readable movement writing for sleight-of-hand moves with cards -- a "Labanotation of card magic." This scheme of movement writing contains 440 categories of motion, and appears to taxonomize all card sleights that have appeared in over 1500 publications. The movement writing is axiomatized in $\mathcal{SROIQ}$(D) Description Logic, and collected formally as an Ontology of Card Sleights, a computational ontology that extends the Basic Formal Ontology and the Information Artifact Ontology. The Ontology of Card Sleights is implemented in OWL DL, a Description Logic fragment of the Web Ontology Language. While ontologies have historically been used to classify at a less granular level, the algorithmic nature of card tricks allows us to transcribe a performer's actions step by step. We conclude by discussing design criteria we have used to ensure the ontology can be accessed and modified with a simple click-and-drag interface. This may allow database searches and performance transcriptions by users with card magic knowledge, but no ontology background.

</details>

<details>

<summary>2019-03-20 15:14:27 - Dilated deeply supervised networks for hippocampus segmentation in MRI</summary>

- *Lukas Folle, Sulaiman Vesal, Nishant Ravikumar, Andreas Maier*

- `1903.09097v1` - [abs](http://arxiv.org/abs/1903.09097v1) - [pdf](http://arxiv.org/pdf/1903.09097v1)

> Tissue loss in the hippocampi has been heavily correlated with the progression of Alzheimer's Disease (AD). The shape and structure of the hippocampus are important factors in terms of early AD diagnosis and prognosis by clinicians. However, manual segmentation of such subcortical structures in MR studies is a challenging and subjective task. In this paper, we investigate variants of the well known 3D U-Net, a type of convolution neural network (CNN) for semantic segmentation tasks. We propose an alternative form of the 3D U-Net, which uses dilated convolutions and deep supervision to incorporate multi-scale information into the model. The proposed method is evaluated on the task of hippocampus head and body segmentation in an MRI dataset, provided as part of the MICCAI 2018 segmentation decathlon challenge. The experimental results show that our approach outperforms other conventional methods in terms of different segmentation accuracy metrics.

</details>

<details>

<summary>2019-03-21 08:43:35 - BUT-FIT at SemEval-2019 Task 7: Determining the Rumour Stance with Pre-Trained Deep Bidirectional Transformers</summary>

- *Martin Fajcik, Lukáš Burget, Pavel Smrz*

- `1902.10126v2` - [abs](http://arxiv.org/abs/1902.10126v2) - [pdf](http://arxiv.org/pdf/1902.10126v2)

> This paper describes our system submitted to SemEval 2019 Task 7: RumourEval 2019: Determining Rumour Veracity and Support for Rumours, Subtask A (Gorrell et al., 2019). The challenge focused on classifying whether posts from Twitter and Reddit support, deny, query, or comment a hidden rumour, truthfulness of which is the topic of an underlying discussion thread. We formulate the problem as a stance classification, determining the rumour stance of a post with respect to the previous thread post and the source thread post. The recent BERT architecture was employed to build an end-to-end system which has reached the F1 score of 61.67% on the provided test data. It finished at the 2nd place in the competition, without any hand-crafted features, only 0.2% behind the winner.

</details>

<details>

<summary>2019-03-21 14:03:59 - Towards a Forensic Event Ontology to Assist Video Surveillance-based Vandalism Detection</summary>

- *Faranak Sobhani, Umberto Straccia*

- `1903.09012v1` - [abs](http://arxiv.org/abs/1903.09012v1) - [pdf](http://arxiv.org/pdf/1903.09012v1)

> The detection and representation of events is a critical element in automated surveillance systems. We present here an ontology for representing complex semantic events to assist video surveillance-based vandalism detection. The ontology contains the definition of a rich and articulated event vocabulary that is aimed at aiding forensic analysis to objectively identify and represent complex events. Our ontology has then been applied in the context of London Riots, which took place in 2011. We report also on the experiments conducted to support the classification of complex criminal events from video data.

</details>

<details>

<summary>2019-03-21 17:08:50 - Patch-based Progressive 3D Point Set Upsampling</summary>

- *Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, Olga Sorkine-Hornung*

- `1811.11286v3` - [abs](http://arxiv.org/abs/1811.11286v3) - [pdf](http://arxiv.org/pdf/1811.11286v3)

> We present a detail-driven deep neural network for point set upsampling. A high-resolution point set is essential for point-based rendering and surface reconstruction. Inspired by the recent success of neural image super-resolution techniques, we progressively train a cascade of patch-based upsampling networks on different levels of detail end-to-end. We propose a series of architectural design contributions that lead to a substantial performance boost. The effect of each technical contribution is demonstrated in an ablation study. Qualitative and quantitative experiments show that our method significantly outperforms the state-of-the-art learning-based and optimazation-based approaches, both in terms of handling low-resolution inputs and revealing high-fidelity details.

</details>

<details>

<summary>2019-03-21 21:38:20 - Inferring Compact Representations for Efficient Natural Language Understanding of Robot Instructions</summary>

- *Siddharth Patki, Andrea F. Daniele, Matthew R. Walter, Thomas M. Howard*

- `1903.09243v1` - [abs](http://arxiv.org/abs/1903.09243v1) - [pdf](http://arxiv.org/pdf/1903.09243v1)

> The speed and accuracy with which robots are able to interpret natural language is fundamental to realizing effective human-robot interaction. A great deal of attention has been paid to developing models and approximate inference algorithms that improve the efficiency of language understanding. However, existing methods still attempt to reason over a representation of the environment that is flat and unnecessarily detailed, which limits scalability. An open problem is then to develop methods capable of producing the most compact environment model sufficient for accurate and efficient natural language understanding. We propose a model that leverages environment-related information encoded within instructions to identify the subset of observations and perceptual classifiers necessary to perceive a succinct, instruction-specific environment representation. The framework uses three probabilistic graphical models trained from a corpus of annotated instructions to infer salient scene semantics, perceptual classifiers, and grounded symbols. Experimental results on two robots operating in different environments demonstrate that by exploiting the content and the structure of the instructions, our method learns compact environment representations that significantly improve the efficiency of natural language symbol grounding.

</details>

<details>

<summary>2019-03-23 17:34:25 - Action-Centered Information Retrieval</summary>

- *Marcello Balduccini, Emily LeBlanc*

- `1903.09850v1` - [abs](http://arxiv.org/abs/1903.09850v1) - [pdf](http://arxiv.org/pdf/1903.09850v1)

> Information Retrieval (IR) aims at retrieving documents that are most relevant to a query provided by a user. Traditional techniques rely mostly on syntactic methods. In some cases, however, links at a deeper semantic level must be considered. In this paper, we explore a type of IR task in which documents describe sequences of events, and queries are about the state of the world after such events. In this context, successfully matching documents and query requires considering the events' possibly implicit, uncertain effects and side-effects. We begin by analyzing the problem, then propose an action language based formalization, and finally automate the corresponding IR task using Answer Set Programming.

</details>

<details>

<summary>2019-03-24 17:23:57 - Software-Defined FPGA Accelerator Design for Mobile Deep Learning Applications</summary>

- *Panagiotis G. Mousouliotis, Loukas P. Petrou*

- `1902.03192v2` - [abs](http://arxiv.org/abs/1902.03192v2) - [pdf](http://arxiv.org/pdf/1902.03192v2)

> Recently, the field of deep learning has received great attention by the scientific community and it is used to provide improved solutions to many computer vision problems. Convolutional neural networks (CNNs) have been successfully used to attack problems such as object recognition, object detection, semantic segmentation, and scene understanding. The rapid development of deep learning goes hand by hand with the adaptation of GPUs for accelerating its processes, such as network training and inference. Even though FPGA design exists long before the use of GPUs for accelerating computations and despite the fact that high-level synthesis (HLS) tools are getting more attractive, the adaptation of FPGAs for deep learning research and application development is poor due to the requirement of hardware design related expertise. This work presents a workflow for deep learning mobile application acceleration on small low-cost low-power FPGA devices using HLS tools. This workflow eases the design of an improved version of the SqueezeJet accelerator used for the speedup of mobile-friendly low-parameter ImageNet class CNNs, such as the SqueezeNet v1.1 and the ZynqNet. Additionally, the workflow includes the development of an HLS-driven analytical model which is used for performance estimation of the accelerator. This model can be also used to direct the design process and lead to future design improvements and optimizations.

</details>

<details>

<summary>2019-03-25 02:44:46 - Checking is Believing: Event-Aware Program Anomaly Detection in Cyber-Physical Systems</summary>

- *Long Cheng, Ke Tian, Danfeng Yao, Lui Sha, Raheem A. Beyah*

- `1805.00074v2` - [abs](http://arxiv.org/abs/1805.00074v2) - [pdf](http://arxiv.org/pdf/1805.00074v2)

> Securing cyber-physical systems (CPS) against malicious attacks is of paramount importance because these attacks may cause irreparable damages to physical systems. Recent studies have revealed that control programs running on CPS devices suffer from both control-oriented attacks (e.g., code-injection or code-reuse attacks) and data-oriented attacks (e.g., non-control data attacks). Unfortunately, existing detection mechanisms are insufficient to detect runtime data-oriented exploits, due to the lack of runtime execution semantics checking. In this work, we propose Orpheus, a new security methodology for defending against data-oriented attacks by enforcing cyber-physical execution semantics. We first present a general method for reasoning cyber-physical execution semantics of a control program (i.e., causal dependencies between the physical context and program control flows), including the event identification and dependence analysis. As an instantiation of Orpheus, we then present a new program behavior model, i.e., the event-aware finite-state automaton (eFSA). eFSA takes advantage of the event-driven nature of CPS control programs and incorporates event checking in anomaly detection. It detects data-oriented exploits if a specific physical event is missing along with the corresponding event dependent state transition. We evaluate our prototype's performance by conducting case studies under data-oriented attacks. Results show that eFSA can successfully detect different runtime attacks. Our prototype on Raspberry Pi incurs a low overhead, taking 0.0001s for each state transition integrity checking, and 0.063s~0.211s for the cyber-physical contextual consistency checking.

</details>

<details>

<summary>2019-03-25 10:22:18 - VCWE: Visual Character-Enhanced Word Embeddings</summary>

- *Chi Sun, Xipeng Qiu, Xuanjing Huang*

- `1902.08795v2` - [abs](http://arxiv.org/abs/1902.08795v2) - [pdf](http://arxiv.org/pdf/1902.08795v2)

> Chinese is a logographic writing system, and the shape of Chinese characters contain rich syntactic and semantic information. In this paper, we propose a model to learn Chinese word embeddings via three-level composition: (1) a convolutional neural network to extract the intra-character compositionality from the visual shape of a character; (2) a recurrent neural network with self-attention to compose character representation into word embeddings; (3) the Skip-Gram framework to capture non-compositionality directly from the contextual information. Evaluations demonstrate the superior performance of our model on four tasks: word similarity, sentiment analysis, named entity recognition and part-of-speech tagging.

</details>

<details>

<summary>2019-03-25 11:00:20 - Aligning Vector-spaces with Noisy Supervised Lexicons</summary>

- *Noa Yehezkel Lubin, Jacob Goldberger, Yoav Goldberg*

- `1903.10238v1` - [abs](http://arxiv.org/abs/1903.10238v1) - [pdf](http://arxiv.org/pdf/1903.10238v1)

> The problem of learning to translate between two vector spaces given a set of aligned points arises in several application areas of NLP. Current solutions assume that the lexicon which defines the alignment pairs is noise-free. We consider the case where the set of aligned points is allowed to contain an amount of noise, in the form of incorrect lexicon pairs and show that this arises in practice by analyzing the edited dictionaries after the cleaning process. We demonstrate that such noise substantially degrades the accuracy of the learned translation when using current methods. We propose a model that accounts for noisy pairs. This is achieved by introducing a generative model with a compatible iterative EM algorithm. The algorithm jointly learns the noise level in the lexicon, finds the set of noisy pairs, and learns the mapping between the spaces. We demonstrate the effectiveness of our proposed algorithm on two alignment problems: bilingual word embedding translation, and mapping between diachronic embedding spaces for recovering the semantic shifts of words across time periods.

</details>

<details>

<summary>2019-03-25 18:09:03 - Learning Embodied Semantics via Music and Dance Semiotic Correlations</summary>

- *Francisco Afonso Raposo, David Martins de Matos, Ricardo Ribeiro*

- `1903.10534v1` - [abs](http://arxiv.org/abs/1903.10534v1) - [pdf](http://arxiv.org/pdf/1903.10534v1)

> Music semantics is embodied, in the sense that meaning is biologically mediated by and grounded in the human body and brain. This embodied cognition perspective also explains why music structures modulate kinetic and somatosensory perception. We leverage this aspect of cognition, by considering dance as a proxy for music perception, in a statistical computational model that learns semiotic correlations between music audio and dance video. We evaluate the ability of this model to effectively capture underlying semantics in a cross-modal retrieval task. Quantitative results, validated with statistical significance testing, strengthen the body of evidence for embodied cognition in music and show the model can recommend music audio for dance video queries and vice-versa.

</details>

<details>

<summary>2019-03-25 18:59:32 - Question Embeddings Based on Shannon Entropy: Solving intent classification task in goal-oriented dialogue system</summary>

- *Aleksandr Perevalov, Daniil Kurushin, Rustam Faizrakhmanov, Farida Khabibrakhmanova*

- `1904.00785v1` - [abs](http://arxiv.org/abs/1904.00785v1) - [pdf](http://arxiv.org/pdf/1904.00785v1)

> Question-answering systems and voice assistants are becoming major part of client service departments of many organizations, helping them to reduce the labor costs of staff. In many such systems, there is always natural language understanding module that solves intent classification task. This task is complicated because of its case-dependency - every subject area has its own semantic kernel. The state of art approaches for intent classification are different machine learning and deep learning methods that use text vector representations as input. The basic vector representation models such as Bag of words and TF-IDF generate sparse matrixes, which are becoming very big as the amount of input data grows. Modern methods such as word2vec and FastText use neural networks to evaluate word embeddings with fixed dimension size. As we are developing a question-answering system for students and enrollees of the Perm National Research Polytechnic University, we have faced the problem of user's intent detection. The subject area of our system is very specific, that is why there is a lack of training data. This aspect makes intent classification task more challenging for using state of the art deep learning methods. In this paper, we propose an approach of the questions embeddings representation based on calculation of Shannon entropy.The goal of the approach is to produce low dimensional question vectors as neural approaches do and to outperform related methods, described above in condition of small dataset. We evaluate and compare our model with existing ones using logistic regression and dataset that contains questions asked by students and enrollees. The data is labeled into six classes. Experimental comparison of proposed approach and other models revealed that proposed model performed better in the given task.

</details>

<details>

<summary>2019-03-25 19:11:57 - Coupled Recurrent Network (CRN)</summary>

- *Lin Sun, Kui Jia, Yuejia Shen, Silvio Savarese, Dit Yan Yeung, Bertram E. Shi*

- `1812.10071v2` - [abs](http://arxiv.org/abs/1812.10071v2) - [pdf](http://arxiv.org/pdf/1812.10071v2)

> Many semantic video analysis tasks can benefit from multiple, heterogenous signals. For example, in addition to the original RGB input sequences, sequences of optical flow are usually used to boost the performance of human action recognition in videos. To learn from these heterogenous input sources, existing methods reply on two-stream architectural designs that contain independent, parallel streams of Recurrent Neural Networks (RNNs). However, two-stream RNNs do not fully exploit the reciprocal information contained in the multiple signals, let alone exploit it in a recurrent manner. To this end, we propose in this paper a novel recurrent architecture, termed Coupled Recurrent Network (CRN), to deal with multiple input sources. In CRN, the parallel streams of RNNs are coupled together. Key design of CRN is a Recurrent Interpretation Block (RIB) that supports learning of reciprocal feature representations from multiple signals in a recurrent manner. Different from RNNs which stack the training loss at each time step or the last time step, we propose an effective and efficient training strategy for CRN. Experiments show the efficacy of the proposed CRN. In particular, we achieve the new state of the art on the benchmark datasets of human action recognition and multi-person pose estimation.

</details>

<details>

<summary>2019-03-25 19:16:23 - The Random Conditional Distribution for Higher-Order Probabilistic Inference</summary>

- *Zenna Tavares, Xin Zhang, Edgar Minaysan, Javier Burroni, Rajesh Ranganath, Armando Solar Lezama*

- `1903.10556v1` - [abs](http://arxiv.org/abs/1903.10556v1) - [pdf](http://arxiv.org/pdf/1903.10556v1)

> The need to condition distributional properties such as expectation, variance, and entropy arises in algorithmic fairness, model simplification, robustness and many other areas. At face value however, distributional properties are not random variables, and hence conditioning them is a semantic error and type error in probabilistic programming languages. On the other hand, distributional properties are contingent on other variables in the model, change in value when we observe more information, and hence in a precise sense are random variables too. In order to capture the uncertain over distributional properties, we introduce a probability construct -- the random conditional distribution -- and incorporate it into a probabilistic programming language Omega. A random conditional distribution is a higher-order random variable whose realizations are themselves conditional random variables. In Omega we extend distributional properties of random variables to random conditional distributions, such that for example while the expectation a real valued random variable is a real value, the expectation of a random conditional distribution is a distribution over expectations. As a consequence, it requires minimal syntax to encode inference problems over distributional properties, which so far have evaded treatment within probabilistic programming systems and probabilistic modeling in general. We demonstrate our approach case studies in algorithmic fairness and robustness.

</details>

<details>

<summary>2019-03-25 20:21:45 - Performance-Efficiency Trade-off of Low-Precision Numerical Formats in Deep Neural Networks</summary>

- *Zachariah Carmichael, Hamed F. Langroudi, Char Khazanov, Jeffrey Lillie, John L. Gustafson, Dhireesha Kudithipudi*

- `1903.10584v1` - [abs](http://arxiv.org/abs/1903.10584v1) - [pdf](http://arxiv.org/pdf/1903.10584v1)

> Deep neural networks (DNNs) have been demonstrated as effective prognostic models across various domains, e.g. natural language processing, computer vision, and genomics. However, modern-day DNNs demand high compute and memory storage for executing any reasonably complex task. To optimize the inference time and alleviate the power consumption of these networks, DNN accelerators with low-precision representations of data and DNN parameters are being actively studied. An interesting research question is in how low-precision networks can be ported to edge-devices with similar performance as high-precision networks. In this work, we employ the fixed-point, floating point, and posit numerical formats at $\leq$8-bit precision within a DNN accelerator, Deep Positron, with exact multiply-and-accumulate (EMAC) units for inference. A unified analysis quantifies the trade-offs between overall network efficiency and performance across five classification tasks. Our results indicate that posits are a natural fit for DNN inference, outperforming at $\leq$8-bit precision, and can be realized with competitive resource requirements relative to those of floating point.

</details>

<details>

<summary>2019-03-26 08:43:38 - Interoperability and machine-to-machine translation model with mappings to machine learning tasks</summary>

- *Jacob Nilsson, Fredrik Sandin, Jerker Delsing*

- `1903.10735v1` - [abs](http://arxiv.org/abs/1903.10735v1) - [pdf](http://arxiv.org/pdf/1903.10735v1)

> Modern large-scale automation systems integrate thousands to hundreds of thousands of physical sensors and actuators. Demands for more flexible reconfiguration of production systems and optimization across different information models, standards and legacy systems challenge current system interoperability concepts. Automatic semantic translation across information models and standards is an increasingly important problem that needs to be addressed to fulfill these demands in a cost-efficient manner under constraints of human capacity and resources in relation to timing requirements and system complexity. Here we define a translator-based operational interoperability model for interacting cyber-physical systems in mathematical terms, which includes system identification and ontology-based translation as special cases. We present alternative mathematical definitions of the translator learning task and mappings to similar machine learning tasks and solutions based on recent developments in machine learning. Possibilities to learn translators between artefacts without a common physical context, for example in simulations of digital twins and across layers of the automation pyramid are briefly discussed.

</details>

<details>

<summary>2019-03-26 09:20:05 - Linked Open Data Validity -- A Technical Report from ISWS 2018</summary>

- *Tayeb Abderrahmani Ghor, Esha Agrawal, Mehwish Alam, Omar Alqawasmeh, Claudia D'amato, Amina Annane, Amr Azzam, Andrew Berezovskyi, Russa Biswas, Mathias Bonduel, Quentin Brabant, Cristina-iulia Bucur, Elena Camossi, Valentina Anita Carriero, Shruthi Chari, David Chaves Fraga, Fiorela Ciroku, Michael Cochez, Hubert Curien, Vincenzo Cutrona, Rahma Dandan, Danilo Dess, Valerio Di Carlo, Ahmed El Amine Djebri, Marieke Van Erp, Faiq Miftakhul Falakh, Alba Fernndez Izquierdo, Giuseppe Futia, Aldo Gangemi, Simone Gasperoni, Arnaud Grall, Lars Heling, Pierre Henri, Noura Herradi, Subhi Issa, Samaneh Jozashoori, Nyoman Juniarta, Lucie-aime Kaffee, Ilkcan Keles, Prashant Khare, Viktor Kovtun, Valentina Leone, Siying Li, Sven Lieber, Pasquale Lisena, Tatiana Makhalova, Ludovica Marinucci, Thomas Minier, Benjamin Moreau, Alberto Moya Loustaunau, Durgesh Nandini, Sylwia Ozdowska, Amanda Pacini De Moura, Swati Padhee, Guillermo Palma, Pedro Del Pozo Jimnez, Valentina Presutti, Roberto Reda, Ettore Rizza, Henry Rosales-mndez, Sebastian Rudolph, Harald Sack, Luca Sciullo, Humasak Simanjuntak, Carlo Stomeo, Thiviyan Thanapalasingam, Tabea Tietz, Dalia Varanka, Maria-esther Vidal, Michael Wolowyk, Maximilian Zocholl*

- `1903.12554v1` - [abs](http://arxiv.org/abs/1903.12554v1) - [pdf](http://arxiv.org/pdf/1903.12554v1)

> Linked Open Data (LOD) is the publicly available RDF data in the Web. Each LOD entity is identfied by a URI and accessible via HTTP. LOD encodes globalscale knowledge potentially available to any human as well as artificial intelligence that may want to benefit from it as background knowledge for supporting their tasks. LOD has emerged as the backbone of applications in diverse fields such as Natural Language Processing, Information Retrieval, Computer Vision, Speech Recognition, and many more. Nevertheless, regardless of the specific tasks that LOD-based tools aim to address, the reuse of such knowledge may be challenging for diverse reasons, e.g. semantic heterogeneity, provenance, and data quality. As aptly stated by Heath et al. Linked Data might be outdated, imprecise, or simply wrong": there arouses a necessity to investigate the problem of linked data validity. This work reports a collaborative effort performed by nine teams of students, guided by an equal number of senior researchers, attending the International Semantic Web Research School (ISWS 2018) towards addressing such investigation from different perspectives coupled with different approaches to tackle the issue.

</details>

<details>

<summary>2019-03-26 12:43:09 - Musical Tempo and Key Estimation using Convolutional Neural Networks with Directional Filters</summary>

- *Hendrik Schreiber, Meinard Müller*

- `1903.10839v1` - [abs](http://arxiv.org/abs/1903.10839v1) - [pdf](http://arxiv.org/pdf/1903.10839v1)

> In this article we explore how the different semantics of spectrograms' time and frequency axes can be exploited for musical tempo and key estimation using Convolutional Neural Networks (CNN). By addressing both tasks with the same network architectures ranging from shallow, domain-specific approaches to deep variants with directional filters, we show that axis-aligned architectures perform similarly well as common VGG-style networks developed for computer vision, while being less vulnerable to confounding factors and requiring fewer model parameters.

</details>

<details>

<summary>2019-03-26 16:54:51 - PatchNet: A Tool for Deep Patch Classification</summary>

- *Thong Hoang, Julia Lawall, Richard J. Oentaryo, Yuan Tian, David Lo*

- `1903.02063v2` - [abs](http://arxiv.org/abs/1903.02063v2) - [pdf](http://arxiv.org/pdf/1903.02063v2)

> This work proposes PatchNet, an automated tool based on hierarchical deep learning for classifying patches by extracting features from commit messages and code changes. PatchNet contains a deep hierarchical structure that mirrors the hierarchical and sequential structure of a code change, differentiating it from the existing deep learning models on source code. PatchNet provides several options allowing users to select parameters for the training process. The tool has been validated in the context of automatic identification of stable-relevant patches in the Linux kernel and is potentially applicable to automate other software engineering tasks that can be formulated as patch classification problems. A video demonstrating PatchNet is available at https://goo.gl/CZjG6X. The PatchNet implementation is available at https://github.com/hvdthong/PatchNetTool.

</details>

<details>

<summary>2019-03-27 12:56:37 - Learning semantic sentence representations from visually grounded language without lexical knowledge</summary>

- *Danny Merkx, Stefan Frank*

- `1903.11393v1` - [abs](http://arxiv.org/abs/1903.11393v1) - [pdf](http://arxiv.org/pdf/1903.11393v1)

> Current approaches to learning semantic representations of sentences often use prior word-level knowledge. The current study aims to leverage visual information in order to capture sentence level semantics without the need for word embeddings. We use a multimodal sentence encoder trained on a corpus of images with matching text captions to produce visually grounded sentence embeddings. Deep Neural Networks are trained to map the two modalities to a common embedding space such that for an image the corresponding caption can be retrieved and vice versa. We show that our model achieves results comparable to the current state-of-the-art on two popular image-caption retrieval benchmark data sets: MSCOCO and Flickr8k. We evaluate the semantic content of the resulting sentence embeddings using the data from the Semantic Textual Similarity benchmark task and show that the multimodal embeddings correlate well with human semantic similarity judgements. The system achieves state-of-the-art results on several of these benchmarks, which shows that a system trained solely on multimodal data, without assuming any word representations, is able to capture sentence level semantics. Importantly, this result shows that we do not need prior knowledge of lexical level semantics in order to model sentence level semantics. These findings demonstrate the importance of visual information in semantics.

</details>

<details>

<summary>2019-03-27 14:36:19 - Import2vec - Learning Embeddings for Software Libraries</summary>

- *Bart Theeten, Frederik Vandeputte, Tom Van Cutsem*

- `1904.03990v1` - [abs](http://arxiv.org/abs/1904.03990v1) - [pdf](http://arxiv.org/pdf/1904.03990v1)

> We consider the problem of developing suitable learning representations (embeddings) for library packages that capture semantic similarity among libraries. Such representations are known to improve the performance of downstream learning tasks (e.g. classification) or applications such as contextual search and analogical reasoning.   We apply word embedding techniques from natural language processing (NLP) to train embeddings for library packages ("library vectors"). Library vectors represent libraries by similar context of use as determined by import statements present in source code. Experimental results obtained from training such embeddings on three large open source software corpora reveals that library vectors capture semantically meaningful relationships among software libraries, such as the relationship between frameworks and their plug-ins and libraries commonly used together within ecosystems such as big data infrastructure projects (in Java), front-end and back-end web development frameworks (in JavaScript) and data science toolkits (in Python).

</details>

<details>

<summary>2019-03-27 21:07:00 - Zero-shot Image Recognition Using Relational Matching, Adaptation and Calibration</summary>

- *Debasmit Das, C. S. George Lee*

- `1903.11701v1` - [abs](http://arxiv.org/abs/1903.11701v1) - [pdf](http://arxiv.org/pdf/1903.11701v1)

> Zero-shot learning (ZSL) for image classification focuses on recognizing novel categories that have no labeled data available for training. The learning is generally carried out with the help of mid-level semantic descriptors associated with each class. This semantic-descriptor space is generally shared by both seen and unseen categories. However, ZSL suffers from hubness, domain discrepancy and biased-ness towards seen classes. To tackle these problems, we propose a three-step approach to zero-shot learning. Firstly, a mapping is learned from the semantic-descriptor space to the image-feature space. This mapping learns to minimize both one-to-one and pairwise distances between semantic embeddings and the image features of the corresponding classes. Secondly, we propose test-time domain adaptation to adapt the semantic embedding of the unseen classes to the test data. This is achieved by finding correspondences between the semantic descriptors and the image features. Thirdly, we propose scaled calibration on the classification scores of the seen classes. This is necessary because the ZSL model is biased towards seen classes as the unseen classes are not used in the training. Finally, to validate the proposed three-step approach, we performed experiments on four benchmark datasets where the proposed method outperformed previous results. We also studied and analyzed the performance of each component of our proposed ZSL framework.

</details>

<details>

<summary>2019-03-27 23:03:48 - The Semantic Web Rule Language Expressiveness Extensions-A Survey</summary>

- *Abba Lawan, Abdur Rakib*

- `1903.11723v1` - [abs](http://arxiv.org/abs/1903.11723v1) - [pdf](http://arxiv.org/pdf/1903.11723v1)

> The Semantic Web Rule Language (SWRL) is a direct extension of OWL 2 DL with a subset of RuleML, and it is designed to be the rule language of the Semantic Web. This paper explores the state-of-the-art of SWRL's expressiveness extensions proposed over time. As a motivation, the effectiveness of the SWRL/OWL combination in modeling domain facts is discussed while some of the common expressive limitations of the combination are also highlighted. The paper then classifies and presents the relevant language extensions of the SWRL and their added expressive powers to the original SWRL definition. Furthermore, it provides a comparative analysis of the syntax and semantics of the proposed extensions. In conclusion, the decidability requirement and usability of each expressiveness extension are evaluated towards an efficient inclusion into the OWL ontologies.

</details>

<details>

<summary>2019-03-28 08:39:49 - Feature Fusion Encoder Decoder Network For Automatic Liver Lesion Segmentation</summary>

- *Xueying Chen, Rong Zhang, Pingkun Yan*

- `1903.11834v1` - [abs](http://arxiv.org/abs/1903.11834v1) - [pdf](http://arxiv.org/pdf/1903.11834v1)

> Liver lesion segmentation is a difficult yet critical task for medical image analysis. Recently, deep learning based image segmentation methods have achieved promising performance, which can be divided into three categories: 2D, 2.5D and 3D, based on the dimensionality of the models. However, 2.5D and 3D methods can have very high complexity and 2D methods may not perform satisfactorily. To obtain competitive performance with low complexity, in this paper, we propose a Feature-fusion Encoder-Decoder Network (FED-Net) based 2D segmentation model to tackle the challenging problem of liver lesion segmentation from CT images. Our feature fusion method is based on the attention mechanism, which fuses high-level features carrying semantic information with low-level features having image details. Additionally, to compensate for the information loss during the upsampling process, a dense upsampling convolution and a residual convolutional structure are proposed. We tested our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge and achieved competitive results compared with other state-of-the-art methods.

</details>

<details>

<summary>2019-03-28 09:30:16 - Mining Discourse Markers for Unsupervised Sentence Representation Learning</summary>

- *Damien Sileo, Tim Van-De-Cruys, Camille Pradel, Philippe Muller*

- `1903.11850v1` - [abs](http://arxiv.org/abs/1903.11850v1) - [pdf](http://arxiv.org/pdf/1903.11850v1)

> Current state of the art systems in NLP heavily rely on manually annotated datasets, which are expensive to construct. Very little work adequately exploits unannotated data -- such as discourse markers between sentences -- mainly because of data sparseness and ineffective extraction methods. In the present work, we propose a method to automatically discover sentence pairs with relevant discourse markers, and apply it to massive amounts of data. Our resulting dataset contains 174 discourse markers with at least 10k examples each, even for rare markers such as coincidentally or amazingly We use the resulting data as supervision for learning transferable sentence embeddings. In addition, we show that even though sentence representation learning through prediction of discourse markers yields state of the art results across different transfer tasks, it is not clear that our models made use of the semantic relation between sentences, thus leaving room for further improvements. Our datasets are publicly available (https://github.com/synapse-developpement/Discovery)

</details>

<details>

<summary>2019-03-28 14:47:33 - Multimodal Deep Network Embedding with Integrated Structure and Attribute Information</summary>

- *Conghui Zheng, Li Pan, Peng Wu*

- `1903.12019v1` - [abs](http://arxiv.org/abs/1903.12019v1) - [pdf](http://arxiv.org/pdf/1903.12019v1)

> Network embedding is the process of learning low-dimensional representations for nodes in a network, while preserving node features. Existing studies only leverage network structure information and focus on preserving structural features. However, nodes in real-world networks often have a rich set of attributes providing extra semantic information. It has been demonstrated that both structural and attribute features are important for network analysis tasks. To preserve both features, we investigate the problem of integrating structure and attribute information to perform network embedding and propose a Multimodal Deep Network Embedding (MDNE) method. MDNE captures the non-linear network structures and the complex interactions among structures and attributes, using a deep model consisting of multiple layers of non-linear functions. Since structures and attributes are two different types of information, a multimodal learning method is adopted to pre-process them and help the model to better capture the correlations between node structure and attribute information. We employ both structural proximity and attribute proximity in the loss function to preserve the respective features and the representations are obtained by minimizing the loss function. Results of extensive experiments on four real-world datasets show that the proposed method performs significantly better than baselines on a variety of tasks, which demonstrate the effectiveness and generality of our method.

</details>

<details>

<summary>2019-03-28 21:12:59 - In Search of Meaning: Lessons, Resources and Next Steps for Computational Analysis of Financial Discourse</summary>

- *Mahmoud El-Haj, Paul Rayson, Martin Walker, Steven Young, Vasiliki Simaki*

- `1903.12271v1` - [abs](http://arxiv.org/abs/1903.12271v1) - [pdf](http://arxiv.org/pdf/1903.12271v1)

> We critically assess mainstream accounting and finance research applying methods from computational linguistics (CL) to study financial discourse. We also review common themes and innovations in the literature and assess the incremental contributions of work applying CL methods over manual content analysis. Key conclusions emerging from our analysis are: (a) accounting and finance research is behind the curve in terms of CL methods generally and word sense disambiguation in particular; (b) implementation issues mean the proposed benefits of CL are often less pronounced than proponents suggest; (c) structural issues limit practical relevance; and (d) CL methods and high quality manual analysis represent complementary approaches to analyzing financial discourse. We describe four CL tools that have yet to gain traction in mainstream AF research but which we believe offer promising ways to enhance the study of meaning in financial discourse. The four tools are named entity recognition (NER), summarization, semantics and corpus linguistics.

</details>

<details>

<summary>2019-03-28 21:58:38 - A Type-coherent, Expressive Representation as an Initial Step to Language Understanding</summary>

- *Gene Louis Kim, Lenhart Schubert*

- `1903.09333v2` - [abs](http://arxiv.org/abs/1903.09333v2) - [pdf](http://arxiv.org/pdf/1903.09333v2)

> A growing interest in tasks involving language understanding by the NLP community has led to the need for effective semantic parsing and inference. Modern NLP systems use semantic representations that do not quite fulfill the nuanced needs for language understanding: adequately modeling language semantics, enabling general inferences, and being accurately recoverable. This document describes underspecified logical forms (ULF) for Episodic Logic (EL), which is an initial form for a semantic representation that balances these needs. ULFs fully resolve the semantic type structure while leaving issues such as quantifier scope, word sense, and anaphora unresolved; they provide a starting point for further resolution into EL, and enable certain structural inferences without further resolution. This document also presents preliminary results of creating a hand-annotated corpus of ULFs for the purpose of training a precise ULF parser, showing a three-person pairwise interannotator agreement of 0.88 on confident annotations. We hypothesize that a divide-and-conquer approach to semantic parsing starting with derivation of ULFs will lead to semantic analyses that do justice to subtle aspects of linguistic meaning, and will enable construction of more accurate semantic parsers.

</details>

<details>

<summary>2019-03-28 23:01:50 - Confusion2Vec: Towards Enriching Vector Space Word Representations with Representational Ambiguities</summary>

- *Prashanth Gurunath Shivakumar, Panayiotis Georgiou*

- `1811.03199v2` - [abs](http://arxiv.org/abs/1811.03199v2) - [pdf](http://arxiv.org/pdf/1811.03199v2)

> Word vector representations are a crucial part of Natural Language Processing (NLP) and Human Computer Interaction. In this paper, we propose a novel word vector representation, Confusion2Vec, motivated from the human speech production and perception that encodes representational ambiguity. Humans employ both acoustic similarity cues and contextual cues to decode information and we focus on a model that incorporates both sources of information. The representational ambiguity of acoustics, which manifests itself in word confusions, is often resolved by both humans and machines through contextual cues. A range of representational ambiguities can emerge in various domains further to acoustic perception, such as morphological transformations, paraphrasing for NLP tasks like machine translation etc. In this work, we present a case study in application to Automatic Speech Recognition (ASR), where the word confusions are related to acoustic similarity. We present several techniques to train an acoustic perceptual similarity representation ambiguity. We term this Confusion2Vec and learn on unsupervised-generated data from ASR confusion networks or lattice-like structures. Appropriate evaluations for the Confusion2Vec are formulated for gauging acoustic similarity in addition to semantic-syntactic and word similarity evaluations. The Confusion2Vec is able to model word confusions efficiently, without compromising on the semantic-syntactic word relations, thus effectively enriching the word vector space with extra task relevant ambiguity information. We provide an intuitive exploration of the 2-dimensional Confusion2Vec space using Principal Component Analysis of the embedding and relate to semantic, syntactic and acoustic relationships. The potential of Confusion2Vec in the utilization of uncertainty present in lattices is demonstrated through small examples relating to ASR error correction.

</details>

<details>

<summary>2019-03-29 17:22:40 - Integrating Semantic Knowledge to Tackle Zero-shot Text Classification</summary>

- *Jingqing Zhang, Piyawat Lertvittayakumjorn, Yike Guo*

- `1903.12626v1` - [abs](http://arxiv.org/abs/1903.12626v1) - [pdf](http://arxiv.org/pdf/1903.12626v1)

> Insufficient or even unavailable training data of emerging classes is a big challenge of many classification tasks, including text classification. Recognising text documents of classes that have never been seen in the learning stage, so-called zero-shot text classification, is therefore difficult and only limited previous works tackled this problem. In this paper, we propose a two-phase framework together with data augmentation and feature augmentation to solve this problem. Four kinds of semantic knowledge (word embeddings, class descriptions, class hierarchy, and a general knowledge graph) are incorporated into the proposed framework to deal with instances of unseen classes effectively. Experimental results show that each and the combination of the two phases achieve the best overall accuracy compared with baselines and recent approaches in classifying real-world texts under the zero-shot scenario.

</details>

<details>

<summary>2019-03-30 08:39:03 - Adaptive Adjustment with Semantic Feature Space for Zero-Shot Recognition</summary>

- *Jingcai Guo, Song Guo*

- `1904.00170v1` - [abs](http://arxiv.org/abs/1904.00170v1) - [pdf](http://arxiv.org/pdf/1904.00170v1)

> In most recent years, zero-shot recognition (ZSR) has gained increasing attention in machine learning and image processing fields. It aims at recognizing unseen class instances with knowledge transferred from seen classes. This is typically achieved by exploiting a pre-defined semantic feature space (FS), i.e., semantic attributes or word vectors, as a bridge to transfer knowledge between seen and unseen classes. However, due to the absence of unseen classes during training, the conventional ZSR easily suffers from domain shift and hubness problems. In this paper, we propose a novel ZSR learning framework that can handle these two issues well by adaptively adjusting semantic FS. To the best of our knowledge, our work is the first to consider the adaptive adjustment of semantic FS in ZSR. Moreover, our solution can be formulated to a more efficient framework that significantly boosts the training. Extensive experiments show the remarkable performance improvement of our model compared with other existing methods.

</details>

<details>

<summary>2019-03-31 09:23:17 - SART - Similarity, Analogies, and Relatedness for Tatar Language: New Benchmark Datasets for Word Embeddings Evaluation</summary>

- *Albina Khusainova, Adil Khan, Adín Ramírez Rivera*

- `1904.00365v1` - [abs](http://arxiv.org/abs/1904.00365v1) - [pdf](http://arxiv.org/pdf/1904.00365v1)

> There is a huge imbalance between languages currently spoken and corresponding resources to study them. Most of the attention naturally goes to the "big" languages: those which have the largest presence in terms of media and number of speakers. Other less represented languages sometimes do not even have a good quality corpus to study them. In this paper, we tackle this imbalance by presenting a new set of evaluation resources for Tatar, a language of the Turkic language family which is mainly spoken in Tatarstan Republic, Russia.   We present three datasets: Similarity and Relatedness datasets that consist of human scored word pairs and can be used to evaluate semantic models; and Analogies dataset that comprises analogy questions and allows to explore semantic, syntactic, and morphological aspects of language modeling. All three datasets build upon existing datasets for the English language and follow the same structure. However, they are not mere translations. They take into account specifics of the Tatar language and expand beyond the original datasets. We evaluate state-of-the-art word embedding models for two languages using our proposed datasets for Tatar and the original datasets for English and report our findings on performance comparison.

</details>


## 2019-04

<details>

<summary>2019-04-01 11:44:58 - Probabilistic DL Reasoning with Pinpointing Formulas: A Prolog-based Approach</summary>

- *Riccardo Zese, Giuseppe Cota, Evelina Lamma, Elena Bellodi, Fabrizio Riguzzi*

- `1809.06180v3` - [abs](http://arxiv.org/abs/1809.06180v3) - [pdf](http://arxiv.org/pdf/1809.06180v3)

> When modeling real world domains we have to deal with information that is incomplete or that comes from sources with different trust levels. This motivates the need for managing uncertainty in the Semantic Web. To this purpose, we introduced a probabilistic semantics, named DISPONTE, in order to combine description logics with probability theory. The probability of a query can be then computed from the set of its explanations by building a Binary Decision Diagram (BDD). The set of explanations can be found using the tableau algorithm, which has to handle non-determinism. Prolog, with its efficient handling of non-determinism, is suitable for implementing the tableau algorithm. TRILL and TRILLP are systems offering a Prolog implementation of the tableau algorithm. TRILLP builds a pinpointing formula, that compactly represents the set of explanations and can be directly translated into a BDD. Both reasoners were shown to outperform state-of-the-art DL reasoners. In this paper, we present an improvement of TRILLP, named TORNADO, in which the BDD is directly built during the construction of the tableau, further speeding up the overall inference process. An experimental comparison shows the effectiveness of TORNADO. All systems can be tried online in the TRILL on SWISH web application at http://trill.ml.unife.it/.

</details>

<details>

<summary>2019-04-01 11:55:47 - SNU_IDS at SemEval-2019 Task 3: Addressing Training-Test Class Distribution Mismatch in Conversational Classification</summary>

- *Sanghwan Bae, Jihun Choi, Sang-goo Lee*

- `1903.02163v2` - [abs](http://arxiv.org/abs/1903.02163v2) - [pdf](http://arxiv.org/pdf/1903.02163v2)

> We present several techniques to tackle the mismatch in class distributions between training and test data in the Contextual Emotion Detection task of SemEval 2019, by extending the existing methods for class imbalance problem. Reducing the distance between the distribution of prediction and ground truth, they consistently show positive effects on the performance. Also we propose a novel neural architecture which utilizes representation of overall context as well as of each utterance. The combination of the methods and the models achieved micro F1 score of about 0.766 on the final evaluation.

</details>

<details>

<summary>2019-04-01 14:19:28 - Significance-aware Information Bottleneck for Domain Adaptive Semantic Segmentation</summary>

- *Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang*

- `1904.00876v1` - [abs](http://arxiv.org/abs/1904.00876v1) - [pdf](http://arxiv.org/pdf/1904.00876v1)

> For unsupervised domain adaptation problems, the strategy of aligning the two domains in latent feature space through adversarial learning has achieved much progress in image classification, but usually fails in semantic segmentation tasks in which the latent representations are overcomplex. In this work, we equip the adversarial network with a "significance-aware information bottleneck (SIB)", to address the above problem. The new network structure, called SIBAN, enables a significance-aware feature purification before the adversarial adaptation, which eases the feature alignment and stabilizes the adversarial training course. In two domain adaptation tasks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, we validate that the proposed method can yield leading results compared with other feature-space alternatives. Moreover, SIBAN can even match the state-of-the-art output-space methods in segmentation accuracy, while the latter are often considered to be better choices for domain adaptive segmentation task.

</details>

<details>

<summary>2019-04-01 14:25:06 - Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation</summary>

- *Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, Yi Yang*

- `1809.09478v3` - [abs](http://arxiv.org/abs/1809.09478v3) - [pdf](http://arxiv.org/pdf/1809.09478v3)

> We consider the problem of unsupervised domain adaptation in semantic segmentation. The key in this campaign consists in reducing the domain shift, i.e., enforcing the data distributions of the two domains to be similar. A popular strategy is to align the marginal distribution in the feature space through adversarial learning. However, this global alignment strategy does not consider the local category-level feature distribution. A possible consequence of the global movement is that some categories which are originally well aligned between the source and target may be incorrectly mapped. To address this problem, this paper introduces a category-level adversarial network, aiming to enforce local semantic consistency during the trend of global alignment. Our idea is to take a close look at the category-level data distribution and align each class with an adaptive adversarial loss. Specifically, we reduce the weight of the adversarial loss for category-level aligned features while increasing the adversarial force for those poorly aligned. In this process, we decide how well a feature is category-level aligned between source and target by a co-training approach. In two domain adaptation tasks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, we validate that the proposed method matches the state of the art in segmentation accuracy.

</details>

<details>

<summary>2019-04-01 22:04:29 - Cyberthreat Detection from Twitter using Deep Neural Networks</summary>

- *Nuno Dionísio, Fernando Alves, Pedro M. Ferreira, Alysson Bessani*

- `1904.01127v1` - [abs](http://arxiv.org/abs/1904.01127v1) - [pdf](http://arxiv.org/pdf/1904.01127v1)

> To be prepared against cyberattacks, most organizations resort to security information and event management systems to monitor their infrastructures. These systems depend on the timeliness and relevance of the latest updates, patches and threats provided by cyberthreat intelligence feeds. Open source intelligence platforms, namely social media networks such as Twitter, are capable of aggregating a vast amount of cybersecurity-related sources. To process such information streams, we require scalable and efficient tools capable of identifying and summarizing relevant information for specified assets. This paper presents the processing pipeline of a novel tool that uses deep neural networks to process cybersecurity information received from Twitter. A convolutional neural network identifies tweets containing security-related information relevant to assets in an IT infrastructure. Then, a bidirectional long short-term memory network extracts named entities from these tweets to form a security alert or to fill an indicator of compromise. The proposed pipeline achieves an average 94% true positive rate and 91% true negative rate for the classification task and an average F1-score of 92% for the named entity recognition task, across three case study infrastructures.

</details>

<details>

<summary>2019-04-02 02:09:05 - A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence Representations</summary>

- *Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel*

- `1904.01173v1` - [abs](http://arxiv.org/abs/1904.01173v1) - [pdf](http://arxiv.org/pdf/1904.01173v1)

> We propose a generative model for a sentence that uses two latent variables, with one intended to represent the syntax of the sentence and the other to represent its semantics. We show we can achieve better disentanglement between semantic and syntactic representations by training with multiple losses, including losses that exploit aligned paraphrastic sentences and word-order information. We also investigate the effect of moving from bag-of-words to recurrent neural network modules. We evaluate our models as well as several popular pretrained embeddings on standard semantic similarity tasks and novel syntactic similarity tasks. Empirically, we find that the model with the best performing syntactic and semantic representations also gives rise to the most disentangled representations.

</details>

<details>

<summary>2019-04-02 08:57:55 - Temporal and Aspectual Entailment</summary>

- *Thomas Kober, Sander Bijl de Vroe, Mark Steedman*

- `1904.01297v1` - [abs](http://arxiv.org/abs/1904.01297v1) - [pdf](http://arxiv.org/pdf/1904.01297v1)

> Inferences regarding "Jane's arrival in London" from predications such as "Jane is going to London" or "Jane has gone to London" depend on tense and aspect of the predications. Tense determines the temporal location of the predication in the past, present or future of the time of utterance. The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of "going to London" is completed and whether its consequences hold at that time or not. While tense and aspect are among the most important factors for determining natural language inference, there has been very little work to show whether modern NLP models capture these semantic concepts. In this paper we propose a novel entailment dataset and analyse the ability of a range of recently proposed NLP models to perform inference on temporal predications. We show that the models encode a substantial amount of morphosyntactic information relating to tense and aspect, but fail to model inferences that require reasoning with these semantic properties.

</details>

<details>

<summary>2019-04-02 10:00:58 - Short Text Classification Improved by Feature Space Extension</summary>

- *Yanxuan Li*

- `1904.01313v1` - [abs](http://arxiv.org/abs/1904.01313v1) - [pdf](http://arxiv.org/pdf/1904.01313v1)

> With the explosive development of mobile Internet, short text has been applied extensively. The difference between classifying short text and long documents is that short text is of shortness and sparsity. Thus, it is challenging to deal with short text classification owing to its less semantic information. In this paper, we propose a novel topic-based convolutional neural network (TB-CNN) based on Latent Dirichlet Allocation (LDA) model and convolutional neural network. Comparing to traditional CNN methods, TB-CNN generates topic words with LDA model to reduce the sparseness and combines the embedding vectors of topic words and input words to extend feature space of short text. The validation results on IMDB movie review dataset show the improvement and effectiveness of TB-CNN.

</details>

<details>

<summary>2019-04-02 14:17:19 - Using Multi-Sense Vector Embeddings for Reverse Dictionaries</summary>

- *Michael A. Hedderich, Andrew Yates, Dietrich Klakow, Gerard de Melo*

- `1904.01451v1` - [abs](http://arxiv.org/abs/1904.01451v1) - [pdf](http://arxiv.org/pdf/1904.01451v1)

> Popular word embedding methods such as word2vec and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically cannot serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.

</details>

<details>

<summary>2019-04-02 17:02:19 - Understanding language-elicited EEG data by predicting it from a fine-tuned language model</summary>

- *Dan Schwartz, Tom Mitchell*

- `1904.01548v1` - [abs](http://arxiv.org/abs/1904.01548v1) - [pdf](http://arxiv.org/pdf/1904.01548v1)

> Electroencephalography (EEG) recordings of brain activity taken while participants read or listen to language are widely used within the cognitive neuroscience and psycholinguistics communities as a tool to study language comprehension. Several time-locked stereotyped EEG responses to word-presentations -- known collectively as event-related potentials (ERPs) -- are thought to be markers for semantic or syntactic processes that take place during comprehension. However, the characterization of each individual ERP in terms of what features of a stream of language trigger the response remains controversial. Improving this characterization would make ERPs a more useful tool for studying language comprehension. We take a step towards better understanding the ERPs by fine-tuning a language model to predict them. This new approach to analysis shows for the first time that all of the ERPs are predictable from embeddings of a stream of language. Prior work has only found two of the ERPs to be predictable. In addition to this analysis, we examine which ERPs benefit from sharing parameters during joint training. We find that two pairs of ERPs previously identified in the literature as being related to each other benefit from joint training, while several other pairs of ERPs that benefit from joint training are suggestive of potential relationships. Extensions of this analysis that further examine what kinds of information in the model embeddings relate to each ERP have the potential to elucidate the processes involved in human language comprehension.

</details>

<details>

<summary>2019-04-02 18:00:02 - Asking the Right Question: Inferring Advice-Seeking Intentions from Personal Narratives</summary>

- *Liye Fu, Jonathan P. Chang, Cristian Danescu-Niculescu-Mizil*

- `1904.01587v1` - [abs](http://arxiv.org/abs/1904.01587v1) - [pdf](http://arxiv.org/pdf/1904.01587v1)

> People often share personal narratives in order to seek advice from others. To properly infer the narrator's intention, one needs to apply a certain degree of common sense and social intuition. To test the capabilities of NLP systems to recover such intuition, we introduce the new task of inferring what is the advice-seeking goal behind a personal narrative. We formulate this as a cloze test, where the goal is to identify which of two advice-seeking questions was removed from a given narrative.   The main challenge in constructing this task is finding pairs of semantically plausible advice-seeking questions for given narratives. To address this challenge, we devise a method that exploits commonalities in experiences people share online to automatically extract pairs of questions that are appropriate candidates for the cloze task. This results in a dataset of over 20,000 personal narratives, each matched with a pair of related advice-seeking questions: one actually intended by the narrator, and the other one not. The dataset covers a very broad array of human experiences, from dating, to career options, to stolen iPads. We use human annotation to determine the degree to which the task relies on common sense and social intuition in addition to a semantic understanding of the narrative. By introducing several baselines for this new task we demonstrate its feasibility and identify avenues for better modeling the intention of the narrator.

</details>

<details>

<summary>2019-04-02 19:02:17 - On Constrained Open-World Probabilistic Databases</summary>

- *Tal Friedman, Guy Van den Broeck*

- `1902.10677v2` - [abs](http://arxiv.org/abs/1902.10677v2) - [pdf](http://arxiv.org/pdf/1902.10677v2)

> Increasing amounts of available data have led to a heightened need for representing large-scale probabilistic knowledge bases. One approach is to use a probabilistic database, a model with strong assumptions that allow for efficiently answering many interesting queries. Recent work on open-world probabilistic databases strengthens the semantics of these probabilistic databases by discarding the assumption that any information not present in the data must be false. While intuitive, these semantics are not sufficiently precise to give reasonable answers to queries. We propose overcoming these issues by using constraints to restrict this open world. We provide an algorithm for one class of queries, and establish a basic hardness result for another. Finally, we propose an efficient and tight approximation for a large class of queries.

</details>

<details>

<summary>2019-04-02 20:27:42 - A frame semantic overview of NLP-based information extraction for cancer-related EHR notes</summary>

- *Surabhi Datta, Elmer V Bernstam, Kirk Roberts*

- `1904.01655v1` - [abs](http://arxiv.org/abs/1904.01655v1) - [pdf](http://arxiv.org/pdf/1904.01655v1)

> Objective: There is a lot of information about cancer in Electronic Health Record (EHR) notes that can be useful for biomedical research provided natural language processing (NLP) methods are available to extract and structure this information. In this paper, we present a scoping review of existing clinical NLP literature for cancer. Methods: We identified studies describing an NLP method to extract specific cancer-related information from EHR sources from PubMed, Google Scholar, ACL Anthology, and existing reviews. Two exclusion criteria were used in this study. We excluded articles where the extraction techniques used were too broad to be represented as frames and also where very low-level extraction methods were used. 79 articles were included in the final review. We organized this information according to frame semantic principles to help identify common areas of overlap and potential gaps. Results: Frames were created from the reviewed articles pertaining to cancer information such as cancer diagnosis, tumor description, cancer procedure, breast cancer diagnosis, prostate cancer diagnosis and pain in prostate cancer patients. These frames included both a definition as well as specific frame elements (i.e. extractable attributes). We found that cancer diagnosis was the most common frame among the reviewed papers (36 out of 79), with recent work focusing on extracting information related to treatment and breast cancer diagnosis. Conclusion: The list of common frames described in this paper identifies important cancer-related information extracted by existing NLP techniques and serves as a useful resource for future researchers requiring cancer information extracted from EHR notes. We also argue, due to the heavy duplication of cancer NLP systems, that a general purpose resource of annotated cancer frames and corresponding NLP tools would be valuable.

</details>

<details>

<summary>2019-04-03 11:57:25 - Unsupervised Deep Structured Semantic Models for Commonsense Reasoning</summary>

- *Shuohang Wang, Sheng Zhang, Yelong Shen, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Jing Jiang*

- `1904.01938v1` - [abs](http://arxiv.org/abs/1904.01938v1) - [pdf](http://arxiv.org/pdf/1904.01938v1)

> Commonsense reasoning is fundamental to natural language understanding. While traditional methods rely heavily on human-crafted features and knowledge bases, we explore learning commonsense knowledge from a large amount of raw text via unsupervised learning. We propose two neural network models based on the Deep Structured Semantic Models (DSSM) framework to tackle two classic commonsense reasoning tasks, Winograd Schema challenges (WSC) and Pronoun Disambiguation (PDP). Evaluation shows that the proposed models effectively capture contextual information in the sentence and co-reference information between pronouns and nouns, and achieve significant improvement over previous state-of-the-art approaches.

</details>

<details>

<summary>2019-04-03 13:59:03 - R-grams: Unsupervised Learning of Semantic Units in Natural Language</summary>

- *Ariel Ekgren, Amaru Cuba Gyllensten, Magnus Sahlgren*

- `1808.04670v2` - [abs](http://arxiv.org/abs/1808.04670v2) - [pdf](http://arxiv.org/pdf/1808.04670v2)

> This paper investigates data-driven segmentation using Re-Pair or Byte Pair Encoding-techniques. In contrast to previous work which has primarily been focused on subword units for machine translation, we are interested in the general properties of such segments above the word level. We call these segments r-grams, and discuss their properties and the effect they have on the token frequency distribution. The proposed approach is evaluated by demonstrating its viability in embedding techniques, both in monolingual and multilingual test settings. We also provide a number of qualitative examples of the proposed methodology, demonstrating its viability as a language-invariant segmentation procedure.

</details>

<details>

<summary>2019-04-03 14:45:07 - Understanding Learning Dynamics Of Language Models with SVCCA</summary>

- *Naomi Saphra, Adam Lopez*

- `1811.00225v3` - [abs](http://arxiv.org/abs/1811.00225v3) - [pdf](http://arxiv.org/pdf/1811.00225v3)

> Research has shown that neural models implicitly encode linguistic features, but there has been no research showing \emph{how} these encodings arise as the models are trained. We present the first study on the learning dynamics of neural language models, using a simple and flexible analysis method called Singular Vector Canonical Correlation Analysis (SVCCA), which enables us to compare learned representations across time and across models, without the need to evaluate directly on annotated data. We probe the evolution of syntactic, semantic, and topic representations and find that part-of-speech is learned earlier than topic; that recurrent layers become more similar to those of a tagger during training; and embedding layers less similar. Our results and methods could inform better learning algorithms for NLP models, possibly to incorporate linguistic information more effectively.

</details>

<details>

<summary>2019-04-03 16:04:16 - Processing Tweets for Cybersecurity Threat Awareness</summary>

- *Fernando Alves, Aurélien Bettini, Pedro M. Ferreira, Alysson Bessani*

- `1904.02072v1` - [abs](http://arxiv.org/abs/1904.02072v1) - [pdf](http://arxiv.org/pdf/1904.02072v1)

> Receiving timely and relevant security information is crucial for maintaining a high-security level on an IT infrastructure. This information can be extracted from Open Source Intelligence published daily by users, security organisations, and researchers. In particular, Twitter has become an information hub for obtaining cutting-edge information about many subjects, including cybersecurity. This work proposes SYNAPSE, a Twitter-based streaming threat monitor that generates a continuously updated summary of the threat landscape related to a monitored infrastructure. Its tweet-processing pipeline is composed of filtering, feature extraction, binary classification, an innovative clustering strategy, and generation of Indicators of Compromise (IoCs). A quantitative evaluation considering all tweets from 80 accounts over more than 8 months (over 195.000 tweets), shows that our approach timely and successfully finds the majority of security-related tweets concerning an example IT infrastructure (true positive rate above 90%), incorrectly selects a small number of tweets as relevant (false positive rate under 10%), and summarises the results to very few IoCs per day. A qualitative evaluation of the IoCs generated by SYNAPSE demonstrates their relevance (based on the CVSS score and the availability of patches or exploits), and timeliness (based on threat disclosure dates from NVD).

</details>

<details>

<summary>2019-04-03 16:45:01 - Factorising AMR generation through syntax</summary>

- *Kris Cao, Stephen Clark*

- `1804.07707v2` - [abs](http://arxiv.org/abs/1804.07707v2) - [pdf](http://arxiv.org/pdf/1804.07707v2)

> Generating from Abstract Meaning Representation (AMR) is an underspecified problem, as many syntactic decisions are not constrained by the semantic graph. To explicitly account for this underspecification, we break down generating from AMR into two steps: first generate a syntactic structure, and then generate the surface form. We show that decomposing the generation process this way leads to state-of-the-art single model performance generating from AMR without additional unlabelled data. We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans.

</details>

<details>

<summary>2019-04-03 17:18:26 - Point Cloud Oversegmentation with Graph-Structured Deep Metric Learning</summary>

- *Loic Landrieu, Mohamed Boussaha*

- `1904.02113v1` - [abs](http://arxiv.org/abs/1904.02113v1) - [pdf](http://arxiv.org/pdf/1904.02113v1)

> We propose a new supervized learning framework for oversegmenting 3D point clouds into superpoints. We cast this problem as learning deep embeddings of the local geometry and radiometry of 3D points, such that the border of objects presents high contrasts. The embeddings are computed using a lightweight neural network operating on the points' local neighborhood. Finally, we formulate point cloud oversegmentation as a graph partition problem with respect to the learned embeddings.   This new approach allows us to set a new state-of-the-art in point cloud oversegmentation by a significant margin, on a dense indoor dataset (S3DIS) and a sparse outdoor one (vKITTI). Our best solution requires over five times fewer superpoints to reach similar performance than previously published methods on S3DIS. Furthermore, we show that our framework can be used to improve superpoint-based semantic segmentation algorithms, setting a new state-of-the-art for this task as well.

</details>

<details>

<summary>2019-04-03 17:59:41 - Constrained Generative Adversarial Networks for Interactive Image Generation</summary>

- *Eric Heim*

- `1904.02526v1` - [abs](http://arxiv.org/abs/1904.02526v1) - [pdf](http://arxiv.org/pdf/1904.02526v1)

> Generative Adversarial Networks (GANs) have received a great deal of attention due in part to recent success in generating original, high-quality samples from visual domains. However, most current methods only allow for users to guide this image generation process through limited interactions. In this work we develop a novel GAN framework that allows humans to be "in-the-loop" of the image generation process. Our technique iteratively accepts relative constraints of the form "Generate an image more like image A than image B". After each constraint is given, the user is presented with new outputs from the GAN, informing the next round of feedback. This feedback is used to constrain the output of the GAN with respect to an underlying semantic space that can be designed to model a variety of different notions of similarity (e.g. classes, attributes, object relationships, color, etc.). In our experiments, we show that our GAN framework is able to generate images that are of comparable quality to equivalent unsupervised GANs while satisfying a large number of the constraints provided by users, effectively changing a GAN into one that allows users interactive control over image generation without sacrificing image quality.

</details>

<details>

<summary>2019-04-03 22:45:40 - Syntax-aware Neural Semantic Role Labeling with Supertags</summary>

- *Jungo Kasai, Dan Friedman, Robert Frank, Dragomir Radev, Owen Rambow*

- `1903.05260v2` - [abs](http://arxiv.org/abs/1903.05260v2) - [pdf](http://arxiv.org/pdf/1903.05260v2)

> We introduce a new syntax-aware model for dependency-based semantic role labeling that outperforms syntax-agnostic models for English and Spanish. We use a BiLSTM to tag the text with supertags extracted from dependency parses, and we feed these supertags, along with words and parts of speech, into a deep highway BiLSTM for semantic role labeling. Our model combines the strengths of earlier models that performed SRL on the basis of a full dependency parse with more recent models that use no syntactic information at all. Our local and non-ensemble model achieves state-of-the-art performance on the CoNLL 09 English and Spanish datasets. SRL models benefit from syntactic information, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system.

</details>

<details>

<summary>2019-04-04 10:38:33 - Composition of Sentence Embeddings:Lessons from Statistical Relational Learning</summary>

- *Damien Sileo, Tim Van-De-Cruys, Camille Pradel, Philippe Muller*

- `1904.02464v1` - [abs](http://arxiv.org/abs/1904.02464v1) - [pdf](http://arxiv.org/pdf/1904.02464v1)

> Various NLP problems -- such as the prediction of sentence similarity, entailment, and discourse relations -- are all instances of the same general task: the modeling of semantic relations between a pair of textual elements. A popular model for such problems is to embed sentences into fixed size vectors, and use composition functions (e.g. concatenation or sum) of those vectors as features for the prediction. At the same time, composition of embeddings has been a main focus within the field of Statistical Relational Learning (SRL) whose goal is to predict relations between entities (typically from knowledge base triples). In this article, we show that previous work on relation prediction between texts implicitly uses compositions from baseline SRL models. We show that such compositions are not expressive enough for several tasks (e.g. natural language inference). We build on recent SRL models to address textual relational problems, showing that they are more expressive, and can alleviate issues from simpler compositions. The resulting models significantly improve the state of the art in both transferable sentence representation learning and relation prediction.

</details>

<details>

<summary>2019-04-04 11:12:31 - HLT@SUDA at SemEval 2019 Task 1: UCCA Graph Parsing as Constituent Tree Parsing</summary>

- *Wei Jiang, Zhenghua Li, Yu Zhang, Min Zhang*

- `1903.04153v2` - [abs](http://arxiv.org/abs/1903.04153v2) - [pdf](http://arxiv.org/pdf/1903.04153v2)

> This paper describes a simple UCCA semantic graph parsing approach. The key idea is to convert a UCCA semantic graph into a constituent tree, in which extra labels are deliberately designed to mark remote edges and discontinuous nodes for future recovery. In this way, we can make use of existing syntactic parsing techniques. Based on the data statistics, we recover discontinuous nodes directly according to the output labels of the constituent parser and use a biaffine classification model to recover the more complex remote edges. The classification model and the constituent parser are simultaneously trained under the multi-task learning framework. We use the multilingual BERT as extra features in the open tracks. Our system ranks the first place in the six English/German closed/open tracks among seven participating systems. For the seventh cross-lingual track, where there is little training data for French, we propose a language embedding approach to utilize English and German training data, and our result ranks the second place.

</details>

<details>

<summary>2019-04-04 18:14:23 - Neural Models of the Psychosemantics of `Most'</summary>

- *Lewis O'Sullivan, Shane Steinert-Threlkeld*

- `1904.02734v1` - [abs](http://arxiv.org/abs/1904.02734v1) - [pdf](http://arxiv.org/pdf/1904.02734v1)

> How are the meanings of linguistic expressions related to their use in concrete cognitive tasks? Visual identification tasks show human speakers can exhibit considerable variation in their understanding, representation and verification of certain quantifiers. This paper initiates an investigation into neural models of these psycho-semantic tasks. We trained two types of network -- a convolutional neural network (CNN) model and a recurrent model of visual attention (RAM) -- on the "most" verification task from \citet{Pietroski2009}, manipulating the visual scene and novel notions of task duration. Our results qualitatively mirror certain features of human performance (such as sensitivity to the ratio of set sizes, indicating a reliance on approximate number) while differing in interesting ways (such as exhibiting a subtly different pattern for the effect of image type). We conclude by discussing the prospects for using neural models as cognitive models of this and other psychosemantic tasks.

</details>

<details>

<summary>2019-04-04 19:17:05 - Blind Visual Motif Removal from a Single Image</summary>

- *Amir Hertz, Sharon Fogel, Rana Hanocka, Raja Giryes, Daniel Cohen-Or*

- `1904.02756v1` - [abs](http://arxiv.org/abs/1904.02756v1) - [pdf](http://arxiv.org/pdf/1904.02756v1)

> Many images shared over the web include overlaid objects, or visual motifs, such as text, symbols or drawings, which add a description or decoration to the image. For example, decorative text that specifies where the image was taken, repeatedly appears across a variety of different images. Often, the reoccurring visual motif, is semantically similar, yet, differs in location, style and content (e.g. text placement, font and letters). This work proposes a deep learning based technique for blind removal of such objects. In the blind setting, the location and exact geometry of the motif are unknown. Our approach simultaneously estimates which pixels contain the visual motif, and synthesizes the underlying latent image. It is applied to a single input image, without any user assistance in specifying the location of the motif, achieving state-of-the-art results for blind removal of both opaque and semi-transparent visual motifs.

</details>

<details>

<summary>2019-04-04 19:56:23 - Discrete Adversarial Attacks and Submodular Optimization with Applications to Text Classification</summary>

- *Qi Lei, Lingfei Wu, Pin-Yu Chen, Alexandros G. Dimakis, Inderjit S. Dhillon, Michael Witbrock*

- `1812.00151v2` - [abs](http://arxiv.org/abs/1812.00151v2) - [pdf](http://arxiv.org/pdf/1812.00151v2)

> Adversarial examples are carefully constructed modifications to an input that completely change the output of a classifier but are imperceptible to humans. Despite these successful attacks for continuous data (such as image and audio samples), generating adversarial examples for discrete structures such as text has proven significantly more challenging. In this paper we formulate the attacks with discrete input on a set function as an optimization task. We prove that this set function is submodular for some popular neural network text classifiers under simplifying assumption. This finding guarantees a $1-1/e$ approximation factor for attacks that use the greedy algorithm. Meanwhile, we show how to use the gradient of the attacked classifier to guide the greedy search. Empirical studies with our proposed optimization scheme show significantly improved attack ability and efficiency, on three different text classification tasks over various baselines. We also use a joint sentence and word paraphrasing technique to maintain the original semantics and syntax of the text. This is validated by a human subject evaluation in subjective metrics on the quality and semantic coherence of our generated adversarial text.

</details>

<details>

<summary>2019-04-04 21:34:45 - Deep Learning Sentiment Analysis of Amazon.com Reviews and Ratings</summary>

- *Nishit Shrestha, Fatma Nasoz*

- `1904.04096v1` - [abs](http://arxiv.org/abs/1904.04096v1) - [pdf](http://arxiv.org/pdf/1904.04096v1)

> Our study employs sentiment analysis to evaluate the compatibility of Amazon.com reviews with their corresponding ratings. Sentiment analysis is the task of identifying and classifying the sentiment expressed in a piece of text as being positive or negative. On e-commerce websites such as Amazon.com, consumers can submit their reviews along with a specific polarity rating. In some instances, there is a mismatch between the review and the rating. To identify the reviews with mismatched ratings we performed sentiment analysis using deep learning on Amazon.com product review data. Product reviews were converted to vectors using paragraph vector, which then was used to train a recurrent neural network with gated recurrent unit. Our model incorporated both semantic relationship of review text and product information. We also developed a web service application that predicts the rating score for a submitted review using the trained model and if there is a mismatch between predicted rating score and submitted rating score, it provides feedback to the reviewer.

</details>

<details>

<summary>2019-04-05 05:12:39 - The Lifted Matrix-Space Model for Semantic Composition</summary>

- *WooJin Chung, Sheng-Fu Wang, Samuel R. Bowman*

- `1711.03602v2` - [abs](http://arxiv.org/abs/1711.03602v2) - [pdf](http://arxiv.org/pdf/1711.03602v2)

> Tree-structured neural network architectures for sentence encoding draw inspiration from the approach to semantic composition generally seen in formal linguistics, and have shown empirical improvements over comparable sequence models by doing so. Moreover, adding multiplicative interaction terms to the composition functions in these models can yield significant further improvements. However, existing compositional approaches that adopt such a powerful composition function scale poorly, with parameter counts exploding as model dimension or vocabulary size grows. We introduce the Lifted Matrix-Space model, which uses a global transformation to map vector word embeddings to matrices, which can then be composed via an operation based on matrix-matrix multiplication. Its composition function effectively transmits a larger number of activations across layers with relatively few model parameters. We evaluate our model on the Stanford NLI corpus, the Multi-Genre NLI corpus, and the Stanford Sentiment Treebank and find that it consistently outperforms TreeLSTM (Tai et al., 2015), the previous best known composition function for tree-structured models.

</details>

<details>

<summary>2019-04-05 10:19:56 - NL-FIIT at SemEval-2019 Task 9: Neural Model Ensemble for Suggestion Mining</summary>

- *Samuel Pecar, Marian Simko, Maria Bielikova*

- `1904.02981v1` - [abs](http://arxiv.org/abs/1904.02981v1) - [pdf](http://arxiv.org/pdf/1904.02981v1)

> In this paper, we present neural model architecture submitted to the SemEval-2019 Task 9 competition: "Suggestion Mining from Online Reviews and Forums". We participated in both subtasks for domain specific and also cross-domain suggestion mining. We proposed a recurrent neural network architecture that employs Bi-LSTM layers and also self-attention mechanism. Our architecture tries to encode words via word representations using ELMo and ensembles multiple models to achieve better results. We performed experiments with different setups of our proposed model involving weighting of prediction classes for loss function. Our best model achieved in official test evaluation score of 0.6816 for subtask A and 0.6850 for subtask B. In official results, we achieved 12th and 10th place in subtasks A and B, respectively.

</details>

<details>

<summary>2019-04-05 16:34:48 - Evaluating Text-to-Image Matching using Binary Image Selection (BISON)</summary>

- *Hexiang Hu, Ishan Misra, Laurens van der Maaten*

- `1901.06595v2` - [abs](http://arxiv.org/abs/1901.06595v2) - [pdf](http://arxiv.org/pdf/1901.06595v2)

> Providing systems the ability to relate linguistic and visual content is one of the hallmarks of computer vision. Tasks such as text-based image retrieval and image captioning were designed to test this ability but come with evaluation measures that have a high variance or are difficult to interpret. We study an alternative task for systems that match text and images: given a text query, the system is asked to select the image that best matches the query from a pair of semantically similar images. The system's accuracy on this Binary Image SelectiON (BISON) task is interpretable, eliminates the reliability problems of retrieval evaluations, and focuses on the system's ability to understand fine-grained visual structure. We gather a BISON dataset that complements the COCO dataset and use it to evaluate modern text-based image retrieval and image captioning systems. Our results provide novel insights into the performance of these systems. The COCO-BISON dataset and corresponding evaluation code are publicly available from \url{http://hexianghu.com/bison/}.

</details>

<details>

<summary>2019-04-05 17:13:01 - Exploring Fine-Tuned Embeddings that Model Intensifiers for Emotion Analysis</summary>

- *Laura Bostan, Roman Klinger*

- `1904.03164v1` - [abs](http://arxiv.org/abs/1904.03164v1) - [pdf](http://arxiv.org/pdf/1904.03164v1)

> Adjective phrases like "a little bit surprised", "completely shocked", or "not stunned at all" are not handled properly by currently published state-of-the-art emotion classification and intensity prediction systems which use pre-dominantly non-contextualized word embeddings as input. Based on this finding, we analyze differences between embeddings used by these systems in regard to their capability of handling such cases. Furthermore, we argue that intensifiers in context of emotion words need special treatment, as is established for sentiment polarity classification, but not for more fine-grained emotion prediction. To resolve this issue, we analyze different aspects of a post-processing pipeline which enriches the word representations of such phrases. This includes expansion of semantic spaces at the phrase level and sub-word level followed by retrofitting to emotion lexica. We evaluate the impact of these steps with A La Carte and Bag-of-Substrings extensions based on pretrained GloVe, Word2vec, and fastText embeddings against a crowd-sourced corpus of intensity annotations for tweets containing our focus phrases. We show that the fastText-based models do not gain from handling these specific phrases under inspection. For Word2vec embeddings, we show that our post-processing pipeline improves the results by up to 8% on a novel dataset densely populated with intensifiers.

</details>

<details>

<summary>2019-04-05 18:31:06 - NELEC at SemEval-2019 Task 3: Think Twice Before Going Deep</summary>

- *Parag Agrawal, Anshuman Suri*

- `1904.03223v1` - [abs](http://arxiv.org/abs/1904.03223v1) - [pdf](http://arxiv.org/pdf/1904.03223v1)

> Existing Machine Learning techniques yield close to human performance on text-based classification tasks. However, the presence of multi-modal noise in chat data such as emoticons, slang, spelling mistakes, code-mixed data, etc. makes existing deep-learning solutions perform poorly. The inability of deep-learning systems to robustly capture these covariates puts a cap on their performance. We propose NELEC: Neural and Lexical Combiner, a system which elegantly combines textual and deep-learning based methods for sentiment classification. We evaluate our system as part of the third task of 'Contextual Emotion Detection in Text' as part of SemEval-2019. Our system performs significantly better than the baseline, as well as our deep-learning model benchmarks. It achieved a micro-averaged F1 score of 0.7765, ranking 3rd on the test-set leader-board. Our code is available at https://github.com/iamgroot42/nelec

</details>

<details>

<summary>2019-04-05 20:04:04 - Cross-Lingual Transfer of Semantic Roles: From Raw Text to Semantic Roles</summary>

- *Maryam Aminian, Mohammad Sadegh Rasooli, Mona Diab*

- `1904.03256v1` - [abs](http://arxiv.org/abs/1904.03256v1) - [pdf](http://arxiv.org/pdf/1904.03256v1)

> We describe a transfer method based on annotation projection to develop a dependency-based semantic role labeling system for languages for which no supervised linguistic information other than parallel data is available. Unlike previous work that presumes the availability of supervised features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features. Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features. Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank.

</details>

<details>

<summary>2019-04-06 14:41:39 - The problem with probabilistic DAG automata for semantic graphs</summary>

- *Ieva Vasiljeva, Sorcha Gilroy, Adam Lopez*

- `1810.12266v2` - [abs](http://arxiv.org/abs/1810.12266v2) - [pdf](http://arxiv.org/pdf/1810.12266v2)

> Semantic representations in the form of directed acyclic graphs (DAGs) have been introduced in recent years, and to model them, we need probabilistic models of DAGs. One model that has attracted some attention is the DAG automaton, but it has not been studied as a probabilistic model. We show that some DAG automata cannot be made into useful probabilistic models by the nearly universal strategy of assigning weights to transitions. The problem affects single-rooted, multi-rooted, and unbounded-degree variants of DAG automata, and appears to be pervasive. It does not affect planar variants, but these are problematic for other reasons.

</details>

<details>

<summary>2019-04-06 16:34:34 - Exploring the Attack Surface of Blockchain: A Systematic Overview</summary>

- *Muhammad Saad, Jeffrey Spaulding, Laurent Njilla, Charles Kamhoua, Sachin Shetty, DaeHun Nyang, Aziz Mohaisen*

- `1904.03487v1` - [abs](http://arxiv.org/abs/1904.03487v1) - [pdf](http://arxiv.org/pdf/1904.03487v1)

> In this paper, we systematically explore the attack surface of the Blockchain technology, with an emphasis on public Blockchains. Towards this goal, we attribute attack viability in the attack surface to 1) the Blockchain cryptographic constructs, 2) the distributed architecture of the systems using Blockchain, and 3) the Blockchain application context. To each of those contributing factors, we outline several attacks, including selfish mining, the 51% attack, Domain Name System (DNS) attacks, distributed denial-of-service (DDoS) attacks, consensus delay (due to selfish behavior or distributed denial-of-service attacks), Blockchain forks, orphaned and stale blocks, block ingestion, wallet thefts, smart contract attacks, and privacy attacks. We also explore the causal relationships between these attacks to demonstrate how various attack vectors are connected to one another. A secondary contribution of this work is outlining effective defense measures taken by the Blockchain technology or proposed by researchers to mitigate the effects of these attacks and patch associated vulnerabilities

</details>

<details>

<summary>2019-04-06 16:44:38 - Unsupervised Representation Adversarial Learning Network: from Reconstruction to Generation</summary>

- *Yuqian Zhou, Kuangxiao Gu, Thomas Huang*

- `1804.07353v2` - [abs](http://arxiv.org/abs/1804.07353v2) - [pdf](http://arxiv.org/pdf/1804.07353v2)

> A good representation for arbitrarily complicated data should have the capability of semantic generation, clustering and reconstruction. Previous research has already achieved impressive performance on either one. This paper aims at learning a disentangled representation effective for all of them in an unsupervised way. To achieve all the three tasks together, we learn the forward and inverse mapping between data and representation on the basis of a symmetric adversarial process. In theory, we minimize the upper bound of the two conditional entropy loss between the latent variables and the observations together to achieve the cycle consistency. The newly proposed RepGAN is tested on MNIST, fashionMNIST, CelebA, and SVHN datasets to perform unsupervised classification, generation and reconstruction tasks. The result demonstrates that RepGAN is able to learn a useful and competitive representation. To the author's knowledge, our work is the first one to achieve both a high unsupervised classification accuracy and low reconstruction error on MNIST. Codes are available at https://github.com/yzhouas/RepGAN-tensorflow.

</details>

<details>

<summary>2019-04-06 18:40:06 - C2S2: Cost-aware Channel Sparse Selection for Progressive Network Pruning</summary>

- *Chih-Yao Chiu, Hwann-Tzong Chen, Tyng-Luh Liu*

- `1904.03508v1` - [abs](http://arxiv.org/abs/1904.03508v1) - [pdf](http://arxiv.org/pdf/1904.03508v1)

> This paper describes a channel-selection approach for simplifying deep neural networks. Specifically, we propose a new type of generic network layer, called pruning layer, to seamlessly augment a given pre-trained model for compression. Each pruning layer, comprising $1 \times 1$ depth-wise kernels, is represented with a dual format: one is real-valued and the other is binary. The former enables a two-phase optimization process of network pruning to operate with an end-to-end differentiable network, and the latter yields the mask information for channel selection. Our method progressively performs the pruning task layer-wise, and achieves channel selection according to a sparsity criterion to favor pruning more channels. We also develop a cost-aware mechanism to prevent the compression from sacrificing the expected network performance. Our results for compressing several benchmark deep networks on image classification and semantic segmentation are comparable to those by state-of-the-art.

</details>

<details>

<summary>2019-04-06 19:40:44 - Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation</summary>

- *Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan Yuille, Li Fei-Fei*

- `1901.02985v2` - [abs](http://arxiv.org/abs/1901.02985v2) - [pdf](http://arxiv.org/pdf/1901.02985v2)

> Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification. In this paper, we study NAS for semantic image segmentation. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our architecture searched specifically for semantic image segmentation, attains state-of-the-art performance without any ImageNet pretraining.

</details>

<details>

<summary>2019-04-07 04:36:50 - Multi-Label Image Recognition with Graph Convolutional Networks</summary>

- *Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, Yanwen Guo*

- `1904.03582v1` - [abs](http://arxiv.org/abs/1904.03582v1) - [pdf](http://arxiv.org/pdf/1904.03582v1)

> The task of multi-label image recognition is to predict a set of object labels that present in an image. As objects normally co-occur in an image, it is desirable to model the label dependencies to improve the recognition performance. To capture and explore such important dependencies, we propose a multi-label classification model based on Graph Convolutional Network (GCN). The model builds a directed graph over the object labels, where each node (label) is represented by word embeddings of a label, and GCN is learned to map this label graph into a set of inter-dependent object classifiers. These classifiers are applied to the image descriptors extracted by another sub-net, enabling the whole network to be end-to-end trainable. Furthermore, we propose a novel re-weighted scheme to create an effective label correlation matrix to guide information propagation among the nodes in GCN. Experiments on two multi-label image recognition datasets show that our approach obviously outperforms other existing state-of-the-art methods. In addition, visualization analyses reveal that the classifiers learned by our model maintain meaningful semantic topology.

</details>

<details>

<summary>2019-04-07 08:39:10 - Extending planning knowledge using ontologies for goal opportunities</summary>

- *Mohannad Babli, Eva Onaindia, Eliseo Marzal*

- `1904.03606v1` - [abs](http://arxiv.org/abs/1904.03606v1) - [pdf](http://arxiv.org/pdf/1904.03606v1)

> Approaches to goal-directed behaviour including online planning and opportunistic planning tackle a change in the environment by generating alternative goals to avoid failures or seize opportunities. However, current approaches only address unanticipated changes related to objects or object types already defined in the planning task that is being solved. This article describes a domain-independent approach that advances the state of the art by extending the knowledge of a planning task with relevant objects of new types. The approach draws upon the use of ontologies, semantic measures, and ontology alignment to accommodate newly acquired data that trigger the formulation of goal opportunities inducing a better-valued plan.

</details>

<details>

<summary>2019-04-08 00:58:24 - The Probabilistic Object Detection Challenge</summary>

- *John Skinner, David Hall, Haoyang Zhang, Feras Dayoub, Niko Sünderhauf*

- `1903.07840v2` - [abs](http://arxiv.org/abs/1903.07840v2) - [pdf](http://arxiv.org/pdf/1903.07840v2)

> We introduce a new challenge for computer and robotic vision, the first ACRV Robotic Vision Challenge, Probabilistic Object Detection. Probabilistic object detection is a new variation on traditional object detection tasks, requiring estimates of spatial and semantic uncertainty. We extend the traditional bounding box format of object detection to express spatial uncertainty using gaussian distributions for the box corners. The challenge introduces a new test dataset of video sequences, which are designed to more closely resemble the kind of data available to a robotic system. We evaluate probabilistic detections using a new probability-based detection quality (PDQ) measure. The goal in creating this challenge is to draw the computer and robotic vision communities together, toward applying object detection solutions for practical robotics applications.

</details>

<details>

<summary>2019-04-08 01:13:43 - The Art, Science, and Engineering of Fuzzing: A Survey</summary>

- *Valentin J. M. Manes, HyungSeok Han, Choongwoo Han, Sang Kil Cha, Manuel Egele, Edward J. Schwartz, Maverick Woo*

- `1812.00140v4` - [abs](http://arxiv.org/abs/1812.00140v4) - [pdf](http://arxiv.org/pdf/1812.00140v4)

> Among the many software vulnerability discovery techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.

</details>

<details>

<summary>2019-04-08 05:43:22 - Reinforcement Learning Based Text Style Transfer without Parallel Training Corpus</summary>

- *Hongyu Gong, Suma Bhat, Lingfei Wu, Jinjun Xiong, Wen-mei Hwu*

- `1903.10671v2` - [abs](http://arxiv.org/abs/1903.10671v2) - [pdf](http://arxiv.org/pdf/1903.10671v2)

> Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning. Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus. In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. Our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency. Experimental results on two different style transfer tasks (sentiment transfer and formality transfer) show that our model outperforms state-of-the-art approaches. Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.

</details>

<details>

<summary>2019-04-08 09:17:56 - Multi-View Matrix Completion for Multi-Label Image Classification</summary>

- *Yong Luo, Tongliang Liu, Dacheng Tao, Chao Xu*

- `1904.03901v1` - [abs](http://arxiv.org/abs/1904.03901v1) - [pdf](http://arxiv.org/pdf/1904.03901v1)

> There is growing interest in multi-label image classification due to its critical role in web-based image analytics-based applications, such as large-scale image retrieval and browsing. Matrix completion has recently been introduced as a method for transductive (semi-supervised) multi-label classification, and has several distinct advantages, including robustness to missing data and background noise in both feature and label space. However, it is limited by only considering data represented by a single-view feature, which cannot precisely characterize images containing several semantic concepts. To utilize multiple features taken from different views, we have to concatenate the different features as a long vector. But this concatenation is prone to over-fitting and often leads to very high time complexity in MC based image classification. Therefore, we propose to weightedly combine the MC outputs of different views, and present the multi-view matrix completion (MVMC) framework for transductive multi-label image classification. To learn the view combination weights effectively, we apply a cross validation strategy on the labeled set. In the learning process, we adopt the average precision (AP) loss, which is particular suitable for multi-label image classification. A least squares loss formulation is also presented for the sake of efficiency, and the robustness of the algorithm based on the AP loss compared with the other losses is investigated. Experimental evaluation on two real world datasets (PASCAL VOC' 07 and MIR Flickr) demonstrate the effectiveness of MVMC for transductive (semi-supervised) multi-label image classification, and show that MVMC can exploit complementary properties of different features and output-consistent labels for improved multi-label image classification.

</details>

<details>

<summary>2019-04-08 12:19:53 - Real or Fake? User Behavior and Attitudes Related to Determining the Veracity of Social Media Posts</summary>

- *Linda Plotnick, Starr Hiltz, Sukeshini Grandhi, Julie Dugdale*

- `1904.03989v1` - [abs](http://arxiv.org/abs/1904.03989v1) - [pdf](http://arxiv.org/pdf/1904.03989v1)

> Citizens and Emergency Managers need to be able to distinguish ''fake'' (untrue) news posts from real news posts on social media during disasters. This paper is based on an online survey conducted in 2018 that produced 341 responses from invitations distributed via email and through Facebook. It explores to what extent and how citizens generally assess whether postings are ''true'' or ''fake,'' and describes indicators of the trustworthiness of content that users would like. The mean response on a semantic differential scale measuring how frequently users attempt to verify the news trustworthiness (a scale from 1-never to 5-always) was 3.37. The most frequent message characteristics citizens' use are grammar and the trustworthiness of the sender. Most respondents would find an indicator of trustworthiness helpful, with the most popular choice being a colored graphic. Limitations and implications for assessments of trustworthiness during disasters are discussed.

</details>

<details>

<summary>2019-04-08 12:21:47 - A Method to Discover Digital Collaborative Conversations in Business Collaborations</summary>

- *Antoine Flepp, Julie Dugdale, Fabrice Bourge, Tiphaine Marie-Cardot*

- `1905.06716v1` - [abs](http://arxiv.org/abs/1905.06716v1) - [pdf](http://arxiv.org/pdf/1905.06716v1)

> Many companies have a suite of digital tools, such as Enterprise Social Networks, conferencing and document sharing software, and email, to facilitate collaboration among employees. During, or at the end of a collaboration, documents are often produced. People who were not involved in the initial collaboration often have difficulties understanding parts of its content because they are lacking the overall context. We argue there is valuable contextual and collaborative knowledge contained in these tools (content and use) that can be used to understand the document. Our goal is to rebuild the conversations that took place over a messaging service and their links with a digital conferencing tool during document production. The novelty in our approach is to combine several conversation-threading methods to identify interesting links between distinct conversations. Specifically we combine header-field information with social, temporal and semantic proximities. Our findings suggest the messaging service and conferencing tool are used in a complementary way. The primary results confirm that combining different conversation threading approaches is efficient to detect and construct conversation threads from distinct digital conversations concerning the same document.

</details>

<details>

<summary>2019-04-08 18:17:36 - 3D Local Features for Direct Pairwise Registration</summary>

- *Haowen Deng, Tolga Birdal, Slobodan Ilic*

- `1904.04281v1` - [abs](http://arxiv.org/abs/1904.04281v1) - [pdf](http://arxiv.org/pdf/1904.04281v1)

> We present a novel, data driven approach for solving the problem of registration of two point cloud scans. Our approach is direct in the sense that a single pair of corresponding local patches already provides the necessary transformation cue for the global registration. To achieve that, we first endow the state of the art PPF-FoldNet auto-encoder (AE) with a pose-variant sibling, where the discrepancy between the two leads to pose-specific descriptors. Based upon this, we introduce RelativeNet, a relative pose estimation network to assign correspondence-specific orientations to the keypoints, eliminating any local reference frame computations. Finally, we devise a simple yet effective hypothesize-and-verify algorithm to quickly use the predictions and align two point sets. Our extensive quantitative and qualitative experiments suggests that our approach outperforms the state of the art in challenging real datasets of pairwise registration and that augmenting the keypoints with local pose information leads to better generalization and a dramatic speed-up.

</details>

<details>

<summary>2019-04-08 19:18:09 - Word Similarity Datasets for Thai: Construction and Evaluation</summary>

- *Ponrudee Netisopakul, Gerhard Wohlgenannt, Aleksei Pulich*

- `1904.04307v1` - [abs](http://arxiv.org/abs/1904.04307v1) - [pdf](http://arxiv.org/pdf/1904.04307v1)

> Distributional semantics in the form of word embeddings are an essential ingredient to many modern natural language processing systems. The quantification of semantic similarity between words can be used to evaluate the ability of a system to perform semantic interpretation. To this end, a number of word similarity datasets have been created for the English language over the last decades. For Thai language few such resources are available. In this work, we create three Thai word similarity datasets by translating and re-rating the popular WordSim-353, SimLex-999 and SemEval-2017-Task-2 datasets. The three datasets contain 1852 word pairs in total and have different characteristics in terms of difficulty, domain coverage, and notion of similarity (relatedness vs.~similarity). These features help to gain a broader picture of the properties of an evaluated word embedding model. We include baseline evaluations with existing Thai embedding models, and identify the high ratio of out-of-vocabulary words as one of the biggest challenges. All datasets, evaluation results, and a tool for easy evaluation of new Thai embedding models are available to the NLP community online.

</details>

<details>

<summary>2019-04-09 00:33:17 - Embodied Visual Recognition</summary>

- *Jianwei Yang, Zhile Ren, Mingze Xu, Xinlei Chen, David Crandall, Devi Parikh, Dhruv Batra*

- `1904.04404v1` - [abs](http://arxiv.org/abs/1904.04404v1) - [pdf](http://arxiv.org/pdf/1904.04404v1)

> Passive visual systems typically fail to recognize objects in the amodal setting where they are heavily occluded. In contrast, humans and other embodied agents have the ability to move in the environment, and actively control the viewing angle to better understand object shapes and semantics. In this work, we introduce the task of Embodied Visual Recognition (EVR): An agent is instantiated in a 3D environment close to an occluded target object, and is free to move in the environment to perform object classification, amodal object localization, and amodal object segmentation. To address this, we develop a new model called Embodied Mask R-CNN, for agents to learn to move strategically to improve their visual recognition abilities. We conduct experiments using the House3D environment. Experimental results show that: 1) agents with embodiment (movement) achieve better visual recognition performance than passive ones; 2) in order to improve visual recognition abilities, agents can learn strategical moving paths that are different from shortest paths.

</details>

<details>

<summary>2019-04-09 01:23:03 - Generate, Filter, and Rank: Grammaticality Classification for Production-Ready NLG Systems</summary>

- *Ashwini Challa, Kartikeya Upasani, Anusha Balakrishnan, Rajen Subba*

- `1904.03279v2` - [abs](http://arxiv.org/abs/1904.03279v2) - [pdf](http://arxiv.org/pdf/1904.03279v2)

> Neural approaches to Natural Language Generation (NLG) have been promising for goal-oriented dialogue. One of the challenges of productionizing these approaches, however, is the ability to control response quality, and ensure that generated responses are acceptable. We propose the use of a generate, filter, and rank framework, in which candidate responses are first filtered to eliminate unacceptable responses, and then ranked to select the best response. While acceptability includes grammatical correctness and semantic correctness, we focus only on grammaticality classification in this paper, and show that existing datasets for grammatical error correction don't correctly capture the distribution of errors that data-driven generators are likely to make. We release a grammatical classification and semantic correctness classification dataset for the weather domain that consists of responses generated by 3 data-driven NLG systems. We then explore two supervised learning approaches (CNNs and GBDTs) for classifying grammaticality. Our experiments show that grammaticality classification is very sensitive to the distribution of errors in the data, and that these distributions vary significantly with both the source of the response as well as the domain. We show that it's possible to achieve high precision with reasonable recall on our dataset.

</details>

<details>

<summary>2019-04-09 06:19:53 - Spatially Controllable Image Synthesis with Internal Representation Collaging</summary>

- *Ryohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji, Huachun Zhu*

- `1811.10153v2` - [abs](http://arxiv.org/abs/1811.10153v2) - [pdf](http://arxiv.org/pdf/1811.10153v2)

> We present a novel CNN-based image editing strategy that allows the user to change the semantic information of an image over an arbitrary region by manipulating the feature-space representation of the image in a trained GAN model. We will present two variants of our strategy: (1) spatial conditional batch normalization (sCBN), a type of conditional batch normalization with user-specifiable spatial weight maps, and (2) feature-blending, a method of directly modifying the intermediate features. Our methods can be used to edit both artificial image and real image, and they both can be used together with any GAN with conditional normalization layers. We will demonstrate the power of our method through experiments on various types of GANs trained on different datasets. Code will be available at https://github.com/pfnet-research/neural-collage.

</details>

<details>

<summary>2019-04-09 08:14:09 - Uncertainty Measures and Prediction Quality Rating for the Semantic Segmentation of Nested Multi Resolution Street Scene Images</summary>

- *Matthias Rottmann, Marius Schubert*

- `1904.04516v1` - [abs](http://arxiv.org/abs/1904.04516v1) - [pdf](http://arxiv.org/pdf/1904.04516v1)

> In the semantic segmentation of street scenes the reliability of the prediction and therefore uncertainty measures are of highest interest. We present a method that generates for each input image a hierarchy of nested crops around the image center and presents these, all re-scaled to the same size, to a neural network for semantic segmentation. The resulting softmax outputs are then post processed such that we can investigate mean and variance over all image crops as well as mean and variance of uncertainty heat maps obtained from pixel-wise uncertainty measures, like the entropy, applied to each crop's softmax output. In our tests, we use the publicly available DeepLabv3+ MobilenetV2 network (trained on the Cityscapes dataset) and demonstrate that the incorporation of crops improves the quality of the prediction and that we obtain more reliable uncertainty measures. These are then aggregated over predicted segments for either classifying between IoU=0 and IoU>0 (meta classification) or predicting the IoU via linear regression (meta regression). The latter yields reliable performance estimates for segmentation networks, in particular useful in the absence of ground truth. For the task of meta classification we obtain a classification accuracy of $81.93\%$ and an AUROC of $89.89\%$. For meta regression we obtain an $R^2$ value of $84.77\%$. These results yield significant improvements compared to other approaches.

</details>

<details>

<summary>2019-04-09 10:05:27 - Context Encoding Chest X-rays</summary>

- *Davide Belli, Shi Hu, Ecem Sogancioglu, Bram van Ginneken*

- `1812.00964v2` - [abs](http://arxiv.org/abs/1812.00964v2) - [pdf](http://arxiv.org/pdf/1812.00964v2)

> Chest X-rays are one of the most commonly used technologies for medical diagnosis. Many deep learning models have been proposed to improve and automate the abnormality detection task on this type of data. In this paper, we propose a different approach based on image inpainting under adversarial training first introduced by Goodfellow et al. We configure the context encoder model for this task and train it over 1.1M 128x128 images from healthy X-rays. The goal of our model is to reconstruct the missing central 64x64 patch. Once the model has learned how to inpaint healthy tissue, we test its performance on images with and without abnormalities. We discuss and motivate our results considering PSNR, MSE and SSIM scores as evaluation metrics. In addition, we conduct a 2AFC observer study showing that in half of the times an expert is unable to distinguish real images from the ones reconstructed using our model. By computing and visualizing the pixel-wise difference between the source and the reconstructed images, we can highlight abnormalities to simplify further detection and classification tasks.

</details>

<details>

<summary>2019-04-09 21:17:49 - AIRD: Adversarial Learning Framework for Image Repurposing Detection</summary>

- *Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, Iacopo Masi, Premkumar Natarajan*

- `1903.00788v3` - [abs](http://arxiv.org/abs/1903.00788v3) - [pdf](http://arxiv.org/pdf/1903.00788v3)

> Image repurposing is a commonly used method for spreading misinformation on social media and online forums, which involves publishing untampered images with modified metadata to create rumors and further propaganda. While manual verification is possible, given vast amounts of verified knowledge available on the internet, the increasing prevalence and ease of this form of semantic manipulation call for the development of robust automatic ways of assessing the semantic integrity of multimedia data. In this paper, we present a novel method for image repurposing detection that is based on the real-world adversarial interplay between a bad actor who repurposes images with counterfeit metadata and a watchdog who verifies the semantic consistency between images and their accompanying metadata, where both players have access to a reference dataset of verified content, which they can use to achieve their goals. The proposed method exhibits state-of-the-art performance on location-identity, subject-identity and painting-artist verification, showing its efficacy across a diverse set of scenarios.

</details>

<details>

<summary>2019-04-10 04:07:32 - Multi-Label Transfer Learning for Multi-Relational Semantic Similarity</summary>

- *Li Zhang, Steven R. Wilson, Rada Mihalcea*

- `1805.12501v2` - [abs](http://arxiv.org/abs/1805.12501v2) - [pdf](http://arxiv.org/pdf/1805.12501v2)

> Multi-relational semantic similarity datasets define the semantic relations between two short texts in multiple ways, e.g., similarity, relatedness, and so on. Yet, all the systems to date designed to capture such relations target one relation at a time. We propose a multi-label transfer learning approach based on LSTM to make predictions for several relations simultaneously and aggregate the losses to update the parameters. This multi-label regression approach jointly learns the information provided by the multiple relations, rather than treating them as separate tasks. Not only does this approach outperform the single-task approach and the traditional multi-task learning approach, but it also achieves state-of-the-art performance on all but one relation of the Human Activity Phrase dataset.

</details>

<details>

<summary>2019-04-10 11:12:02 - Reinforced Evolutionary Neural Architecture Search</summary>

- *Yukang Chen, Gaofeng Meng, Qian Zhang, Shiming Xiang, Chang Huang, Lisen Mu, Xinggang Wang*

- `1808.00193v3` - [abs](http://arxiv.org/abs/1808.00193v3) - [pdf](http://arxiv.org/pdf/1808.00193v3)

> Neural Architecture Search (NAS) is an important yet challenging task in network design due to its high computational consumption. To address this issue, we propose the Reinforced Evolutionary Neural Architecture Search (RE- NAS), which is an evolutionary method with the reinforced mutation for NAS. Our method integrates reinforced mutation into an evolution algorithm for neural architecture exploration, in which a mutation controller is introduced to learn the effects of slight modifications and make mutation actions. The reinforced mutation controller guides the model population to evolve efficiently. Furthermore, as child models can inherit parameters from their parents during evolution, our method requires very limited computational resources. In experiments, we conduct the proposed search method on CIFAR-10 and obtain a powerful network architecture, RENASNet. This architecture achieves a competitive result on CIFAR-10. The explored network architecture is transferable to ImageNet and achieves a new state-of-the-art accuracy, i.e., 75.7% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test its performance on semantic segmentation with DeepLabv3 on the PASCAL VOC. RENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83% mIOU without being pre-trained on COCO.

</details>

<details>

<summary>2019-04-10 15:52:13 - Simple BERT Models for Relation Extraction and Semantic Role Labeling</summary>

- *Peng Shi, Jimmy Lin*

- `1904.05255v1` - [abs](http://arxiv.org/abs/1904.05255v1) - [pdf](http://arxiv.org/pdf/1904.05255v1)

> We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.

</details>

<details>

<summary>2019-04-10 16:59:29 - CNM: An Interpretable Complex-valued Network for Matching</summary>

- *Qiuchi Li, Benyou Wang, Massimo Melucci*

- `1904.05298v1` - [abs](http://arxiv.org/abs/1904.05298v1) - [pdf](http://arxiv.org/pdf/1904.05298v1)

> This paper seeks to model human language by the mathematical framework of quantum physics. With the well-designed mathematical formulations in quantum physics, this framework unifies different linguistic units in a single complex-valued vector space, e.g. words as particles in quantum states and sentences as mixed systems. A complex-valued network is built to implement this framework for semantic matching. With well-constrained complex-valued components, the network admits interpretations to explicit physical meanings. The proposed complex-valued network for matching (CNM) achieves comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets.

</details>

<details>

<summary>2019-04-10 17:43:51 - Harvey Mudd College at SemEval-2019 Task 4: The Clint Buchanan Hyperpartisan News Detector</summary>

- *Mehdi Drissi, Pedro Sandoval, Vivaswat Ojha, Julie Medero*

- `1905.01962v1` - [abs](http://arxiv.org/abs/1905.01962v1) - [pdf](http://arxiv.org/pdf/1905.01962v1)

> We investigate the recently developed Bidirectional Encoder Representations from Transformers (BERT) model for the hyperpartisan news detection task. Using a subset of hand-labeled articles from SemEval as a validation set, we test the performance of different parameters for BERT models. We find that accuracy from two different BERT models using different proportions of the articles is consistently high, with our best-performing model on the validation set achieving 85% accuracy and the best-performing model on the test set achieving 77%. We further determined that our model exhibits strong consistency, labeling independent slices of the same article identically. Finally, we find that randomizing the order of word pieces dramatically reduces validation accuracy (to approximately 60%), but that shuffling groups of four or more word pieces maintains an accuracy of about 80%, indicating the model mainly gains value from local context.

</details>

<details>

<summary>2019-04-10 18:25:12 - An Improved Approach for Semantic Graph Composition with CCG</summary>

- *Austin Blodgett, Nathan Schneider*

- `1903.11770v2` - [abs](http://arxiv.org/abs/1903.11770v2) - [pdf](http://arxiv.org/pdf/1903.11770v2)

> This paper builds on previous work using Combinatory Categorial Grammar (CCG) to derive a transparent syntax-semantics interface for Abstract Meaning Representation (AMR) parsing. We define new semantics for the CCG combinators that is better suited to deriving AMR graphs. In particular, we define relation-wise alternatives for the application and composition combinators: these require that the two constituents being combined overlap in one AMR relation. We also provide a new semantics for type raising, which is necessary for certain constructions. Using these mechanisms, we suggest an analysis of eventive nouns, which present a challenge for deriving AMR graphs. Our theoretical analysis will facilitate future work on robust and transparent AMR parsing using CCG.

</details>

<details>

<summary>2019-04-11 07:08:09 - Searching News Articles Using an Event Knowledge Graph Leveraged by Wikidata</summary>

- *Charlotte Rudnik, Thibault Ehrhart, Olivier Ferret, Denis Teyssou, Raphaël Troncy, Xavier Tannier*

- `1904.05557v1` - [abs](http://arxiv.org/abs/1904.05557v1) - [pdf](http://arxiv.org/pdf/1904.05557v1)

> News agencies produce thousands of multimedia stories describing events happening in the world that are either scheduled such as sports competitions, political summits and elections, or breaking events such as military conflicts, terrorist attacks, natural disasters, etc. When writing up those stories, journalists refer to contextual background and to compare with past similar events. However, searching for precise facts described in stories is hard. In this paper, we propose a general method that leverages the Wikidata knowledge base to produce semantic annotations of news articles. Next, we describe a semantic search engine that supports both keyword based search in news articles and structured data search providing filters for properties belonging to specific event schemas that are automatically inferred.

</details>

<details>

<summary>2019-04-11 08:56:48 - Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study</summary>

- *Jorge A. Balazs, Yutaka Matsuo*

- `1904.05584v1` - [abs](http://arxiv.org/abs/1904.05584v1) - [pdf](http://arxiv.org/pdf/1904.05584v1)

> In this paper we study how different ways of combining character and word-level representations affect the quality of both final word and sentence representations. We provide strong empirical evidence that modeling characters improves the learned representations at the word and sentence levels, and that doing so is particularly useful when representing less frequent words. We further show that a feature-wise sigmoid gating mechanism is a robust method for creating representations that encode semantic similarity, as it performed reasonably well in several word similarity datasets. Finally, our findings suggest that properly capturing semantic similarity at the word level does not consistently yield improved performance in downstream sentence-level tasks. Our code is available at https://github.com/jabalazs/gating

</details>

<details>

<summary>2019-04-11 09:23:49 - Ontologies-based Architecture for Sociocultural Knowledge Co-Construction Systems</summary>

- *Guidedi Kaladzavi, Papa Fary Diallo, Cedric Béré, Olivier Corby, Isabelle Mirbel, Moussa Lo, Dina Taiwe Kolyang*

- `1904.05596v1` - [abs](http://arxiv.org/abs/1904.05596v1) - [pdf](http://arxiv.org/pdf/1904.05596v1)

> Considering the evolution of the semantic wiki engine based platforms, two main approaches could be distinguished: Ontologies for Wikis (OfW) and Wikis for Ontologies (WfO). OfW vision requires existing ontologies to be imported. Most of them use the RDF-based (Resource Description Framework) systems in conjunction with the standard SQL (Structured Query Language) database to manage and query semantic data. But, relational database is not an ideal type of storage for semantic data. A more natural data model for SMW (Semantic MediaWiki) is RDF, a data format that organizes information in graphs rather than in fixed database tables. This paper presents an ontology based architecture, which aims to implement this idea. The architecture mainly includes three layered functional architectures: Web User Interface Layer, Semantic Layer and Persistence Layer.

</details>

<details>

<summary>2019-04-11 10:51:18 - Discovering Fair Representations in the Data Domain</summary>

- *Novi Quadrianto, Viktoriia Sharmanska, Oliver Thomas*

- `1810.06755v2` - [abs](http://arxiv.org/abs/1810.06755v2) - [pdf](http://arxiv.org/pdf/1810.06755v2)

> Interpretability and fairness are critical in computer vision and machine learning applications, in particular when dealing with human outcomes, e.g. inviting or not inviting for a job interview based on application materials that may include photographs. One promising direction to achieve fairness is by learning data representations that remove the semantics of protected characteristics, and are therefore able to mitigate unfair outcomes. All available models however learn latent embeddings which comes at the cost of being uninterpretable. We propose to cast this problem as data-to-data translation, i.e. learning a mapping from an input domain to a fair target domain, where a fairness definition is being enforced. Here the data domain can be images, or any tabular data representation. This task would be straightforward if we had fair target data available, but this is not the case. To overcome this, we learn a highly unconstrained mapping by exploiting statistics of residuals - the difference between input data and its translated version - and the protected characteristics. When applied to the CelebA dataset of face images with gender attribute as the protected characteristic, our model enforces equality of opportunity by adjusting the eyes and lips regions. Intriguingly, on the same dataset we arrive at similar conclusions when using semantic attribute representations of images for translation. On face images of the recent DiF dataset, with the same gender attribute, our method adjusts nose regions. In the Adult income dataset, also with protected gender attribute, our model achieves equality of opportunity by, among others, obfuscating the wife and husband relationship. Analyzing those systematic changes will allow us to scrutinize the interplay of fairness criterion, chosen protected characteristics, and prediction performance.

</details>

<details>

<summary>2019-04-11 13:09:57 - Cross-topic distributional semantic representations via unsupervised mappings</summary>

- *Eleftheria Briakou, Nikos Athanasiou, Alexandros Potamianos*

- `1904.05674v1` - [abs](http://arxiv.org/abs/1904.05674v1) - [pdf](http://arxiv.org/pdf/1904.05674v1)

> In traditional Distributional Semantic Models (DSMs) the multiple senses of a polysemous word are conflated into a single vector space representation. In this work, we propose a DSM that learns multiple distributional representations of a word based on different topics. First, a separate DSM is trained for each topic and then each of the topic-based DSMs is aligned to a common vector space. Our unsupervised mapping approach is motivated by the hypothesis that words preserving their relative distances in different topic semantic sub-spaces constitute robust \textit{semantic anchors} that define the mappings between them. Aligned cross-topic representations achieve state-of-the-art results for the task of contextual word similarity. Furthermore, evaluation on NLP downstream tasks shows that multiple topic-based embeddings outperform single-prototype models.

</details>

<details>

<summary>2019-04-12 01:54:01 - Modeling Interpersonal Linguistic Coordination in Conversations using Word Mover's Distance</summary>

- *Md Nasir, Sandeep Nallan Chakravarthula, Brian Baucom, David C. Atkins, Panayiotis Georgiou, Shrikanth Narayanan*

- `1904.06002v1` - [abs](http://arxiv.org/abs/1904.06002v1) - [pdf](http://arxiv.org/pdf/1904.06002v1)

> Linguistic coordination is a well-established phenomenon in spoken conversations and often associated with positive social behaviors and outcomes. While there have been many attempts to measure lexical coordination or entrainment in literature, only a few have explored coordination in syntactic or semantic space. In this work, we attempt to combine these different aspects of coordination into a single measure by leveraging distances in a neural word representation space. In particular, we adopt the recently proposed Word Mover's Distance with word2vec embeddings and extend it to measure the dissimilarity in language used in multiple consecutive speaker turns. To validate our approach, we apply this measure for two case studies in the clinical psychology domain. We find that our proposed measure is correlated with the therapist's empathy towards their patient in Motivational Interviewing and with affective behaviors in Couples Therapy. In both case studies, our proposed metric exhibits higher correlation than previously proposed measures. When applied to the couples with relationship improvement, we also notice a significant decrease in the proposed measure over the course of therapy, indicating higher linguistic coordination.

</details>

<details>

<summary>2019-04-12 05:18:35 - Evaluating the Representational Hub of Language and Vision Models</summary>

- *Ravi Shekhar, Ece Takmaz, Raquel Fernández, Raffaella Bernardi*

- `1904.06038v1` - [abs](http://arxiv.org/abs/1904.06038v1) - [pdf](http://arxiv.org/pdf/1904.06038v1)

> The multimodal models used in the emerging field at the intersection of computational linguistics and computer vision implement the bottom-up processing of the `Hub and Spoke' architecture proposed in cognitive science to represent how the brain processes and combines multi-sensory inputs. In particular, the Hub is implemented as a neural network encoder. We investigate the effect on this encoder of various vision-and-language tasks proposed in the literature: visual question answering, visual reference resolution, and visually grounded dialogue. To measure the quality of the representations learned by the encoder, we use two kinds of analyses. First, we evaluate the encoder pre-trained on the different vision-and-language tasks on an existing diagnostic task designed to assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities.

</details>

<details>

<summary>2019-04-12 07:47:44 - An Argument-Marker Model for Syntax-Agnostic Proto-Role Labeling</summary>

- *Juri Opitz, Anette Frank*

- `1902.01349v2` - [abs](http://arxiv.org/abs/1902.01349v2) - [pdf](http://arxiv.org/pdf/1902.01349v2)

> Semantic proto-role labeling (SPRL) is an alternative to semantic role labeling (SRL) that moves beyond a categorical definition of roles, following Dowty's feature-based view of proto-roles. This theory determines agenthood vs. patienthood based on a participant's instantiation of more or less typical agent vs. patient properties, such as, for example, volition in an event. To perform SPRL, we develop an ensemble of hierarchical models with self-attention and concurrently learned predicate-argument-markers. Our method is competitive with the state-of-the art, overall outperforming previous work in two formulations of the task (multi-label and multi-variate Likert scale prediction). In contrast to previous work, our results do not depend on gold argument heads derived from supplementary gold tree banks.

</details>

<details>

<summary>2019-04-12 08:31:22 - Syntactic Interchangeability in Word Embedding Models</summary>

- *Daniel Hershcovich, Assaf Toledo, Alon Halfon, Noam Slonim*

- `1904.00669v2` - [abs](http://arxiv.org/abs/1904.00669v2) - [pdf](http://arxiv.org/pdf/1904.00669v2)

> Nearest neighbors in word embedding models are commonly observed to be semantically similar, but the relations between them can vary greatly. We investigate the extent to which word embedding models preserve syntactic interchangeability, as reflected by distances between word vectors, and the effect of hyper-parameters---context window size in particular. We use part of speech (POS) as a proxy for syntactic interchangeability, as generally speaking, words with the same POS are syntactically valid in the same contexts. We also investigate the relationship between interchangeability and similarity as judged by commonly-used word similarity benchmarks, and correlate the result with the performance of word embedding models on these benchmarks. Our results will inform future research and applications in the selection of word embedding model, suggesting a principle for an appropriate selection of the context window size parameter depending on the use-case.

</details>

<details>

<summary>2019-04-12 08:48:03 - A Crowdsourced Frame Disambiguation Corpus with Ambiguity</summary>

- *Anca Dumitrache, Lora Aroyo, Chris Welty*

- `1904.06101v1` - [abs](http://arxiv.org/abs/1904.06101v1) - [pdf](http://arxiv.org/pdf/1904.06101v1)

> We present a resource for the task of FrameNet semantic frame disambiguation of over 5,000 word-sentence pairs from the Wikipedia corpus. The annotations were collected using a novel crowdsourcing approach with multiple workers per sentence to capture inter-annotator disagreement. In contrast to the typical approach of attributing the best single frame to each word, we provide a list of frames with disagreement-based scores that express the confidence with which each frame applies to the word. This is based on the idea that inter-annotator disagreement is at least partly caused by ambiguity that is inherent to the text and frames. We have found many examples where the semantics of individual frames overlap sufficiently to make them acceptable alternatives for interpreting a sentence. We have argued that ignoring this ambiguity creates an overly arbitrary target for training and evaluating natural language processing systems - if humans cannot agree, why would we expect the correct answer from a machine to be any different? To process this data we also utilized an expanded lemma-set provided by the Framester system, which merges FN with WordNet to enhance coverage. Our dataset includes annotations of 1,000 sentence-word pairs whose lemmas are not part of FN. Finally we present metrics for evaluating frame disambiguation systems that account for ambiguity.

</details>

<details>

<summary>2019-04-12 10:45:19 - FrameRank: A Text Processing Approach to Video Summarization</summary>

- *Zhuo Lei, Chao Zhang, Qian Zhang, Guoping Qiu*

- `1904.05544v2` - [abs](http://arxiv.org/abs/1904.05544v2) - [pdf](http://arxiv.org/pdf/1904.05544v2)

> Video summarization has been extensively studied in the past decades. However, user-generated video summarization is much less explored since there lack large-scale video datasets within which human-generated video summaries are unambiguously defined and annotated. Toward this end, we propose a user-generated video summarization dataset - UGSum52 - that consists of 52 videos (207 minutes). In constructing the dataset, because of the subjectivity of user-generated video summarization, we manually annotate 25 summaries for each video, which are in total 1300 summaries. To the best of our knowledge, it is currently the largest dataset for user-generated video summarization.   Based on this dataset, we present FrameRank, an unsupervised video summarization method that employs a frame-to-frame level affinity graph to identify coherent and informative frames to summarize a video. We use the Kullback-Leibler(KL)-divergence-based graph to rank temporal segments according to the amount of semantic information contained in their frames. We illustrate the effectiveness of our method by applying it to three datasets SumMe, TVSum and UGSum52 and show it achieves state-of-the-art results.

</details>

<details>

<summary>2019-04-12 11:17:37 - Interpretable Classification from Skin Cancer Histology Slides Using Deep Learning: A Retrospective Multicenter Study</summary>

- *Peizhen Xie, Ke Zuo, Yu Zhang, Fangfang Li, Mingzhu Yin, Kai Lu*

- `1904.06156v1` - [abs](http://arxiv.org/abs/1904.06156v1) - [pdf](http://arxiv.org/pdf/1904.06156v1)

> For diagnosing melanoma, hematoxylin and eosin (H&E) stained tissue slides remains the gold standard. These images contain quantitative information in different magnifications. In the present study, we investigated whether deep convolutional neural networks can extract structural features of complex tissues directly from these massive size images in a patched way. In order to face the challenge arise from morphological diversity in histopathological slides, we built a multicenter database of 2241 digital whole-slide images from 1321 patients from 2008 to 2018. We trained both ResNet50 and Vgg19 using over 9.95 million patches by transferring learning, and test performance with two kinds of critical classifications: malignant melanomas versus benign nevi in separate and mixed magnification; and distinguish among nevi in maximum magnification. The CNNs achieves superior performance across both tasks, demonstrating an AI capable of classifying skin cancer in the analysis from histopathological images. For making the classifications reasonable, the visualization of CNN representations is furthermore used to identify cells between melanoma and nevi. Regions of interest (ROI) are also located which are significantly helpful, giving pathologists more support of correctly diagnosis.

</details>

<details>

<summary>2019-04-12 11:50:36 - Learning Graph Embeddings from WordNet-based Similarity Measures</summary>

- *Andrey Kutuzov, Mohammad Dorgham, Oleksiy Oliynyk, Chris Biemann, Alexander Panchenko*

- `1808.05611v4` - [abs](http://arxiv.org/abs/1808.05611v4) - [pdf](http://arxiv.org/pdf/1808.05611v4)

> We present path2vec, a new approach for learning graph embeddings that relies on structural measures of pairwise node similarities. The model learns representations for nodes in a dense space that approximate a given user-defined graph distance measure, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account. Evaluation of the proposed model on semantic similarity and word sense disambiguation tasks, using various WordNet-based similarity measures, show that our approach yields competitive results, outperforming strong graph embedding baselines. The model is computationally efficient, being orders of magnitude faster than the direct computation of graph-based distances.

</details>

<details>

<summary>2019-04-12 14:38:03 - AMS-SFE: Towards an Alignment of Manifold Structures via Semantic Feature Expansion for Zero-shot Learning</summary>

- *Jingcai Guo, Song Guo*

- `1904.06254v1` - [abs](http://arxiv.org/abs/1904.06254v1) - [pdf](http://arxiv.org/pdf/1904.06254v1)

> Zero-shot learning (ZSL) aims at recognizing unseen classes with knowledge transferred from seen classes. This is typically achieved by exploiting a semantic feature space (FS) shared by both seen and unseen classes, i.e., attributes or word vectors, as the bridge. However, due to the mutually disjoint of training (seen) and testing (unseen) data, existing ZSL methods easily and commonly suffer from the domain shift problem. To address this issue, we propose a novel model called AMS-SFE. It considers the Alignment of Manifold Structures by Semantic Feature Expansion. Specifically, we build up an autoencoder based model to expand the semantic features and joint with an alignment to an embedded manifold extracted from the visual FS of data. It is the first attempt to align these two FSs by way of expanding semantic features. Extensive experiments show the remarkable performance improvement of our model compared with other existing methods.

</details>

<details>

<summary>2019-04-12 15:20:37 - Locally Connected Spiking Neural Networks for Unsupervised Feature Learning</summary>

- *Daniel J. Saunders, Devdhar Patel, Hananel Hazan, Hava T. Siegelmann, Robert Kozma*

- `1904.06269v1` - [abs](http://arxiv.org/abs/1904.06269v1) - [pdf](http://arxiv.org/pdf/1904.06269v1)

> In recent years, Spiking Neural Networks (SNNs) have demonstrated great successes in completing various Machine Learning tasks. We introduce a method for learning image features by \textit{locally connected layers} in SNNs using spike-timing-dependent plasticity (STDP) rule. In our approach, sub-networks compete via competitive inhibitory interactions to learn features from different locations of the input space. These \textit{Locally-Connected SNNs} (LC-SNNs) manifest key topological features of the spatial interaction of biological neurons. We explore biologically inspired n-gram classification approach allowing parallel processing over various patches of the the image space. We report the classification accuracy of simple two-layer LC-SNNs on two image datasets, which match the state-of-art performance and are the first results to date. LC-SNNs have the advantage of fast convergence to a dataset representation, and they require fewer learnable parameters than other SNN approaches with unsupervised learning. Robustness tests demonstrate that LC-SNNs exhibit graceful degradation of performance despite the random deletion of large amounts of synapses and neurons.

</details>

<details>

<summary>2019-04-12 16:47:30 - Spatio-Temporal Deep Graph Infomax</summary>

- *Felix L. Opolka, Aaron Solomon, Cătălina Cangea, Petar Veličković, Pietro Liò, R Devon Hjelm*

- `1904.06316v1` - [abs](http://arxiv.org/abs/1904.06316v1) - [pdf](http://arxiv.org/pdf/1904.06316v1)

> Spatio-temporal graphs such as traffic networks or gene regulatory systems present challenges for the existing deep learning methods due to the complexity of structural changes over time. To address these issues, we introduce Spatio-Temporal Deep Graph Infomax (STDGI)---a fully unsupervised node representation learning approach based on mutual information maximization that exploits both the temporal and spatial dynamics of the graph. Our model tackles the challenging task of node-level regression by training embeddings to maximize the mutual information between patches of the graph, at any given time step, and between features of the central nodes of patches, in the future. We demonstrate through experiments and qualitative studies that the learned representations can successfully encode relevant information about the input graph and improve the predictive performance of spatio-temporal auto-regressive forecasting models.

</details>

<details>

<summary>2019-04-13 03:52:56 - Improving Distantly-supervised Entity Typing with Compact Latent Space Clustering</summary>

- *Bo Chen, Xiaotao Gu, Yufeng Hu, Siliang Tang, Guoping Hu, Yueting Zhuang, Xiang Ren*

- `1904.06475v1` - [abs](http://arxiv.org/abs/1904.06475v1) - [pdf](http://arxiv.org/pdf/1904.06475v1)

> Recently, distant supervision has gained great success on Fine-grained Entity Typing (FET). Despite its efficiency in reducing manual labeling efforts, it also brings the challenge of dealing with false entity type labels, as distant supervision assigns labels in a context agnostic manner. Existing works alleviated this issue with partial-label loss, but usually suffer from confirmation bias, which means the classifier fit a pseudo data distribution given by itself. In this work, we propose to regularize distantly supervised models with Compact Latent Space Clustering (CLSC) to bypass this problem and effectively utilize noisy data yet. Our proposed method first dynamically constructs a similarity graph of different entity mentions; infer the labels of noisy instances via label propagation. Based on the inferred labels, mention embeddings are updated accordingly to encourage entity mentions with close semantics to form a compact cluster in the embedding space,thus leading to better classification performance. Extensive experiments on standard benchmarks show that our CLSC model consistently outperforms state-of-the-art distantly supervised entity typing systems by a significant margin.

</details>

<details>

<summary>2019-04-13 09:08:46 - Short Text Topic Modeling Techniques, Applications, and Performance: A Survey</summary>

- *Qiang Jipeng, Qian Zhenyu, Li Yun, Yuan Yunhao, Wu Xindong*

- `1904.07695v1` - [abs](http://arxiv.org/abs/1904.07695v1) - [pdf](http://arxiv.org/pdf/1904.07695v1)

> Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the first comprehensive open-source library, called STTM, for use in Java that integrates all surveyed algorithms within a unified interface, benchmark datasets, to facilitate the expansion of new methods in this research field. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm.

</details>

<details>

<summary>2019-04-13 23:21:45 - Exploring Adversarial Examples in Malware Detection</summary>

- *Octavian Suciu, Scott E. Coull, Jeffrey Johns*

- `1810.08280v3` - [abs](http://arxiv.org/abs/1810.08280v3) - [pdf](http://arxiv.org/pdf/1810.08280v3)

> The convolutional neural network (CNN) architecture is increasingly being applied to new domains, such as malware detection, where it is able to learn malicious behavior from raw bytes extracted from executables. These architectures reach impressive performance with no feature engineering effort involved, but their robustness against active attackers is yet to be understood. Such malware detectors could face a new attack vector in the form of adversarial interference with the classification model. Existing evasion attacks intended to cause misclassification on test-time instances, which have been extensively studied for image classifiers, are not applicable because of the input semantics that prevents arbitrary changes to the binaries. This paper explores the area of adversarial examples for malware detection. By training an existing model on a production-scale dataset, we show that some previous attacks are less effective than initially reported, while simultaneously highlighting architectural weaknesses that facilitate new attack strategies for malware classification. Finally, we explore how generalizable different attack strategies are, the trade-offs when aiming to increase their effectiveness, and the transferability of single-step attacks.

</details>

<details>

<summary>2019-04-14 05:34:26 - Predicting the Argumenthood of English Prepositional Phrases</summary>

- *Najoung Kim, Kyle Rawlins, Benjamin Van Durme, Paul Smolensky*

- `1809.07889v4` - [abs](http://arxiv.org/abs/1809.07889v4) - [pdf](http://arxiv.org/pdf/1809.07889v4)

> Distinguishing between arguments and adjuncts of a verb is a longstanding, nontrivial problem. In natural language processing, argumenthood information is important in tasks such as semantic role labeling (SRL) and prepositional phrase (PP) attachment disambiguation. In theoretical linguistics, many diagnostic tests for argumenthood exist but they often yield conflicting and potentially gradient results. This is especially the case for syntactically oblique items such as PPs. We propose two PP argumenthood prediction tasks branching from these two motivations: (1) binary argument-adjunct classification of PPs in VerbNet, and (2) gradient argumenthood prediction using human judgments as gold standard, and report results from prediction models that use pretrained word embeddings and other linguistically informed features. Our best results on each task are (1) $acc.=0.955$, $F_1=0.954$ (ELMo+BiLSTM) and (2) Pearson's $r=0.624$ (word2vec+MLP). Furthermore, we demonstrate the utility of argumenthood prediction in improving sentence representations via performance gains on SRL when a sentence encoder is pretrained with our tasks.

</details>

<details>

<summary>2019-04-14 11:03:44 - Domain Generalization by Solving Jigsaw Puzzles</summary>

- *Fabio Maria Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, Tatiana Tommasi*

- `1903.06864v2` - [abs](http://arxiv.org/abs/1903.06864v2) - [pdf](http://arxiv.org/pdf/1903.06864v2)

> Human adaptability relies crucially on the ability to learn and merge knowledge both from supervised and unsupervised learning: the parents point out few important concepts, but then the children fill in the gaps on their own. This is particularly effective, because supervised learning can never be exhaustive and thus learning autonomously allows to discover invariances and regularities that help to generalize. In this paper we propose to apply a similar approach to the task of object recognition across domains: our model learns the semantic labels in a supervised fashion, and broadens its understanding of the data by learning from self-supervised signals how to solve a jigsaw puzzle on the same images. This secondary task helps the network to learn the concepts of spatial correlation while acting as a regularizer for the classification task. Multiple experiments on the PACS, VLCS, Office-Home and digits datasets confirm our intuition and show that this simple method outperforms previous domain generalization and adaptation solutions. An ablation study further illustrates the inner workings of our approach.

</details>

<details>

<summary>2019-04-14 11:54:05 - CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM</summary>

- *Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew J. Davison*

- `1804.00874v2` - [abs](http://arxiv.org/abs/1804.00874v2) - [pdf](http://arxiv.org/pdf/1804.00874v2)

> The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only.   We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM.

</details>

<details>

<summary>2019-04-14 17:32:44 - Text segmentation on multilabel documents: A distant-supervised approach</summary>

- *Saurav Manchanda, George Karypis*

- `1904.06730v1` - [abs](http://arxiv.org/abs/1904.06730v1) - [pdf](http://arxiv.org/pdf/1904.06730v1)

> Segmenting text into semantically coherent segments is an important task with applications in information retrieval and text summarization. Developing accurate topical segmentation requires the availability of training data with ground truth information at the segment level. However, generating such labeled datasets, especially for applications in which the meaning of the labels is user-defined, is expensive and time-consuming. In this paper, we develop an approach that instead of using segment-level ground truth information, it instead uses the set of labels that are associated with a document and are easier to obtain as the training data essentially corresponds to a multilabel dataset. Our method, which can be thought of as an instance of distant supervision, improves upon the previous approaches by exploiting the fact that consecutive sentences in a document tend to talk about the same topic, and hence, probably belong to the same class. Experiments on the text segmentation task on a variety of datasets show that the segmentation produced by our method beats the competing approaches on four out of five datasets and performs at par on the fifth dataset. On the multilabel text classification task, our method performs at par with the competing approaches, while requiring significantly less time to estimate than the competing approaches.

</details>

<details>

<summary>2019-04-14 23:20:34 - No Adjective Ordering Mystery, and No Raven Paradox, Just an Ontological Mishap</summary>

- *Walid S. Saba*

- `1904.06779v1` - [abs](http://arxiv.org/abs/1904.06779v1) - [pdf](http://arxiv.org/pdf/1904.06779v1)

> In the concluding remarks of Ontological Promiscuity Hobbs (1985) made what we believe to be a very insightful observation: given that semantics is an attempt at specifying the relation between language and the world, if "one can assume a theory of the world that is isomorphic to the way we talk about it ... then semantics becomes nearly trivial". But how exactly can we rectify our logical formalisms so that semantics, an endeavor that has occupied the most penetrating minds for over two centuries, can become (nearly) trivial, and what exactly does it mean to assume a theory of the world in our semantics? In this paper we hope to provide answers for both questions. First, we believe that a commonsense theory of the world can (and should) be embedded in our semantic formalisms resulting in a logical semantics grounded in commonsense metaphysics. Moreover, we believe the first step to accomplishing this vision is rectifying what we think was a crucial oversight in logical semantics, namely the failure to distinguish between two fundamentally different types of concepts: (i) ontological concepts, that correspond to what Cocchiarella (2001) calls first-intension concepts and are types in a strongly-typed ontology; and (ii) logical concepts (or second intension concepts), that are predicates corresponding to properties of (and relations between) objects of various ontological types1. In such a framework, which we will refer to henceforth by ontologik, it will be shown how type unification and other type operations can be used to account for the `missing text phenomenon' (MTP) (see Saba, 2019a) that is at the heart of most challenges in the semantics of natural language, by uncovering the significant amount of missing text that is never explicitly stated in everyday discourse, but is often implicitly assumed as shared background knowledge.

</details>

<details>

<summary>2019-04-15 09:35:11 - Semantic Source Code Models Using Identifier Embeddings</summary>

- *Vasiliki Efstathiou, Diomidis Spinellis*

- `1904.06929v1` - [abs](http://arxiv.org/abs/1904.06929v1) - [pdf](http://arxiv.org/pdf/1904.06929v1)

> The emergence of online open source repositories in the recent years has led to an explosion in the volume of openly available source code, coupled with metadata that relate to a variety of software development activities. As an effect, in line with recent advances in machine learning research, software maintenance activities are switching from symbolic formal methods to data-driven methods. In this context, the rich semantics hidden in source code identifiers provide opportunities for building semantic representations of code which can assist tasks of code search and reuse. To this end, we deliver in the form of pretrained vector space models, distributed code representations for six popular programming languages, namely, Java, Python, PHP, C, C++, and C#. The models are produced using fastText, a state-of-the-art library for learning word representations. Each model is trained on data from a single programming language; the code mined for producing all models amounts to over 13.000 repositories. We indicate dissimilarities between natural language and source code, as well as variations in coding conventions in between the different programming languages we processed. We describe how these heterogeneities guided the data preprocessing decisions we took and the selection of the training parameters in the released models. Finally, we propose potential applications of the models and discuss limitations of the models.

</details>

<details>

<summary>2019-04-15 14:35:40 - Semantic query-by-example speech search using visual grounding</summary>

- *Herman Kamper, Aristotelis Anastassiou, Karen Livescu*

- `1904.07078v1` - [abs](http://arxiv.org/abs/1904.07078v1) - [pdf](http://arxiv.org/pdf/1904.07078v1)

> A number of recent studies have started to investigate how speech systems can be trained on untranscribed speech by leveraging accompanying images at training time. Examples of tasks include keyword prediction and within- and across-mode retrieval. Here we consider how such models can be used for query-by-example (QbE) search, the task of retrieving utterances relevant to a given spoken query. We are particularly interested in semantic QbE, where the task is not only to retrieve utterances containing exact instances of the query, but also utterances whose meaning is relevant to the query. We follow a segmental QbE approach where variable-duration speech segments (queries, search utterances) are mapped to fixed-dimensional embedding vectors. We show that a QbE system using an embedding function trained on visually grounded speech data outperforms a purely acoustic QbE system in terms of both exact and semantic retrieval performance.

</details>

<details>

<summary>2019-04-15 15:55:06 - RF-Trojan: Leaking Kernel Data Using Register File Trojan</summary>

- *Mohammad Nasim Imtiaz Khan, Asmit De, Swaroop Ghosh*

- `1904.07144v1` - [abs](http://arxiv.org/abs/1904.07144v1) - [pdf](http://arxiv.org/pdf/1904.07144v1)

> Register Files (RFs) are the most frequently accessed memories in a microprocessor for fast and efficient computation and control logic. Segment registers and control registers are especially critical for maintaining the CPU mode of execution that determinesthe access privileges. In this work, we explore the vulnerabilities in RF and propose a class of hardware Trojans which can inject faults during read or retention mode. The Trojan trigger is activated if one pre-selected address of L1 data-cache is hammered for certain number of times. The trigger evades post-silicon test since the required number of hammering to trigger is significantly high even under process and temperature variation. Once activated, the trigger can deliver payloads to cause Bitcell Corruption (BC) and inject read error by Read Port (RP) and Local Bitline (LBL). We model the Trojan in GEM5 architectural simulator performing a privilege escalation. We propose countermeasures such as read verification leveraging multiport feature, securing control and segment registers by hashing and L1 address obfuscation.

</details>

<details>

<summary>2019-04-15 16:49:14 - Testing Deep Neural Networks</summary>

- *Youcheng Sun, Xiaowei Huang, Daniel Kroening, James Sharp, Matthew Hill, Rob Ashmore*

- `1803.04792v4` - [abs](http://arxiv.org/abs/1803.04792v4) - [pdf](http://arxiv.org/pdf/1803.04792v4)

> Deep neural networks (DNNs) have a wide range of applications, and software employing them must be thoroughly tested, especially in safety-critical domains. However, traditional software test coverage metrics cannot be applied directly to DNNs. In this paper, inspired by the MC/DC coverage criterion, we propose a family of four novel test criteria that are tailored to structural features of DNNs and their semantics. We validate the criteria by demonstrating that the generated test inputs guided via our proposed coverage criteria are able to capture undesired behaviours in a DNN. Test cases are generated using a symbolic approach and a gradient-based heuristic search. By comparing them with existing methods, we show that our criteria achieve a balance between their ability to find bugs (proxied using adversarial examples) and the computational cost of test case generation. Our experiments are conducted on state-of-the-art DNNs obtained using popular open source datasets, including MNIST, CIFAR-10 and ImageNet.

</details>

<details>

<summary>2019-04-15 20:15:46 - Natural Language Semantics With Pictures: Some Language & Vision Datasets and Potential Uses for Computational Semantics</summary>

- *David Schlangen*

- `1904.07318v1` - [abs](http://arxiv.org/abs/1904.07318v1) - [pdf](http://arxiv.org/pdf/1904.07318v1)

> Propelling, and propelled by, the "deep learning revolution", recent years have seen the introduction of ever larger corpora of images annotated with natural language expressions. We survey some of these corpora, taking a perspective that reverses the usual directionality, as it were, by viewing the images as semantic annotation of the natural language expressions. We discuss datasets that can be derived from the corpora, and tasks of potential interest for computational semanticists that can be defined on those. In this, we make use of relations provided by the corpora (namely, the link between expression and image, and that between two expressions linked to the same image) and relations that we can add (similarity relations between expressions, or between images). Specifically, we show that in this way we can create data that can be used to learn and evaluate lexical and compositional grounded semantics, and we show that the "linked to same image" relation tracks a semantic implication relation that is recognisable to annotators even in the absence of the linking image as evidence. Finally, as an example of possible benefits of this approach, we show that an exemplar-model-based approach to implication beats a (simple) distributional space-based one on some derived datasets, while lending itself to explainability.

</details>

<details>

<summary>2019-04-16 03:17:47 - FlowQA: Grasping Flow in History for Conversational Machine Comprehension</summary>

- *Hsin-Yuan Huang, Eunsol Choi, Wen-tau Yih*

- `1810.06683v3` - [abs](http://arxiv.org/abs/1810.06683v3) - [pdf](http://arxiv.org/pdf/1810.06683v3)

> Conversational machine comprehension requires the understanding of the conversation history, such as previous question/answer pairs, the document context, and the current question. To enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to approaches that concatenate previous questions/answers as input, Flow integrates the latent semantics of the conversation history more deeply. Our model, FlowQA, shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The effectiveness of Flow also shows in other tasks. By reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy.

</details>

<details>

<summary>2019-04-16 04:51:18 - SADIH: Semantic-Aware DIscrete Hashing</summary>

- *Zheng Zhang, Guo-sen Xie, Yang Li, Sheng Li, Zi Huang*

- `1904.01739v2` - [abs](http://arxiv.org/abs/1904.01739v2) - [pdf](http://arxiv.org/pdf/1904.01739v2)

> Due to its low storage cost and fast query speed, hashing has been recognized to accomplish similarity search in large-scale multimedia retrieval applications. Particularly supervised hashing has recently received considerable research attention by leveraging the label information to preserve the pairwise similarities of data points in the Hamming space. However, there still remain two crucial bottlenecks: 1) the learning process of the full pairwise similarity preservation is computationally unaffordable and unscalable to deal with big data; 2) the available category information of data are not well-explored to learn discriminative hash functions. To overcome these challenges, we propose a unified Semantic-Aware DIscrete Hashing (SADIH) framework, which aims to directly embed the transformed semantic information into the asymmetric similarity approximation and discriminative hashing function learning. Specifically, a semantic-aware latent embedding is introduced to asymmetrically preserve the full pairwise similarities while skillfully handle the cumbersome n times n pairwise similarity matrix. Meanwhile, a semantic-aware autoencoder is developed to jointly preserve the data structures in the discriminative latent semantic space and perform data reconstruction. Moreover, an efficient alternating optimization algorithm is proposed to solve the resulting discrete optimization problem. Extensive experimental results on multiple large-scale datasets demonstrate that our SADIH can clearly outperform the state-of-the-art baselines with the additional benefit of lower computational costs.

</details>

<details>

<summary>2019-04-16 04:52:19 - Deep Neural Network Based Hyperspectral Pixel Classification With Factorized Spectral-Spatial Feature Representation</summary>

- *Jingzhou Chen, Siyu Chen, Peilin Zhou, Yuntao Qian*

- `1904.07461v1` - [abs](http://arxiv.org/abs/1904.07461v1) - [pdf](http://arxiv.org/pdf/1904.07461v1)

> Deep learning has been widely used for hyperspectral pixel classification due to its ability of generating deep feature representation. However, how to construct an efficient and powerful network suitable for hyperspectral data is still under exploration. In this paper, a novel neural network model is designed for taking full advantage of the spectral-spatial structure of hyperspectral data. Firstly, we extract pixel-based intrinsic features from rich yet redundant spectral bands by a subnetwork with supervised pre-training scheme. Secondly, in order to utilize the local spatial correlation among pixels, we share the previous subnetwork as a spectral feature extractor for each pixel in a patch of image, after which the spectral features of all pixels in a patch are combined and feeded into the subsequent classification subnetwork. Finally, the whole network is further fine-tuned to improve its classification performance. Specially, the spectral-spatial factorization scheme is applied in our model architecture, making the network size and the number of parameters great less than the existing spectral-spatial deep networks for hyperspectral image classification. Experiments on the hyperspectral data sets show that, compared with some state-of-art deep learning methods, our method achieves better classification results while having smaller network size and less parameters.

</details>

<details>

<summary>2019-04-16 04:59:57 - Using Dynamic Analysis to Generate Disjunctive Invariants</summary>

- *ThanhVu Nguyen, Deepak Kapur, Westley Weimer, Stephanie Forrest*

- `1904.07463v1` - [abs](http://arxiv.org/abs/1904.07463v1) - [pdf](http://arxiv.org/pdf/1904.07463v1)

> Program invariants are important for defect detection, program verification, and program repair. However, existing techniques have limited support for important classes of invariants such as disjunctions, which express the semantics of conditional statements. We propose a method for generating disjunctive invariants over numerical domains, which are inexpressible using classical convex polyhedra. Using dynamic analysis and reformulating the problem in non-standard "max-plus" and "min-plus" algebras, our method constructs hulls over program trace points. Critically, we introduce and infer a weak class of such invariants that balances expressive power against the computational cost of generating nonconvex shapes in high dimensions.   Existing dynamic inference techniques often generate spurious invariants that fit some program traces but do not generalize. With the insight that generating dynamic invariants is easy, we propose to verify these invariants statically using k-inductive SMT theorem proving which allows us to validate invariants that are not classically inductive.   Results on difficult kernels involving nonlinear arithmetic and abstract arrays suggest that this hybrid approach efficiently generates and proves correct program invariants.

</details>

<details>

<summary>2019-04-16 09:22:48 - Interpreting and Understanding Graph Convolutional Neural Network using Gradient-based Attribution Method</summary>

- *Shangsheng Xie, Mingming Lu*

- `1903.03768v2` - [abs](http://arxiv.org/abs/1903.03768v2) - [pdf](http://arxiv.org/pdf/1903.03768v2)

> To solve the problem that convolutional neural networks (CNNs) are difficult to process non-grid type relational data like graphs, Kipf et al. proposed a graph convolutional neural network (GCN). The core idea of the GCN is to perform two-fold informational fusion for each node in a given graph during each iteration: the fusion of graph structure information and the fusion of node feature dimensions. Because of the characteristic of the combinatorial generalizations, GCN has been widely used in the fields of scene semantic relationship analysis, natural language processing and few-shot learning etc. However, due to its two-fold informational fusion involves mathematical irreversible calculations, it is hard to explain the decision reason for the prediction of the each node classification. Unfortunately, most of the existing attribution analysis methods concentrate on the models like CNNs, which are utilized to process grid-like data. It is difficult to apply those analysis methods to the GCN directly. It is because compared with the independence among CNNs input data, there is correlation between the GCN input data. This resulting in the existing attribution analysis methods can only obtain the partial model contribution from the central node features to the final decision of the GCN, but ignores the other model contribution from central node features and its neighbor nodes features to that decision. To this end, we propose a gradient attribution analysis method for the GCN called Node Attribution Method (NAM), which can get the model contribution from not only the central node but also its neighbor nodes to the GCN output. We also propose the Node Importance Visualization (NIV) method to visualize the central node and its neighbor nodes based on the value of the contribution...

</details>

<details>

<summary>2019-04-16 09:44:56 - Theoretical Foundations of Defeasible Description Logics</summary>

- *Katarina Britz, Giovanni Casini, Thomas Meyer, Kody Moodley, Uli Sattler, Ivan Varzinczak*

- `1904.07559v1` - [abs](http://arxiv.org/abs/1904.07559v1) - [pdf](http://arxiv.org/pdf/1904.07559v1)

> We extend description logics (DLs) with non-monotonic reasoning features. We start by investigating a notion of defeasible subsumption in the spirit of defeasible conditionals as studied by Kraus, Lehmann and Magidor in the propositional case. In particular, we consider a natural and intuitive semantics for defeasible subsumption, and investigate KLM-style syntactic properties for both preferential and rational subsumption. Our contribution includes two representation results linking our semantic constructions to the set of preferential and rational properties considered. Besides showing that our semantics is appropriate, these results pave the way for more effective decision procedures for defeasible reasoning in DLs. Indeed, we also analyse the problem of non-monotonic reasoning in DLs at the level of entailment and present an algorithm for the computation of rational closure of a defeasible ontology. Importantly, our algorithm relies completely on classical entailment and shows that the computational complexity of reasoning over defeasible ontologies is no worse than that of reasoning in the underlying classical DL ALC.

</details>

<details>

<summary>2019-04-16 17:26:34 - Double Transfer Learning for Breast Cancer Histopathologic Image Classification</summary>

- *Jonathan de Matos, Alceu de S. Britto Jr., Luiz E. S. Oliveira, Alessandro L. Koerich*

- `1904.07834v1` - [abs](http://arxiv.org/abs/1904.07834v1) - [pdf](http://arxiv.org/pdf/1904.07834v1)

> This work proposes a classification approach for breast cancer histopathologic images (HI) that uses transfer learning to extract features from HI using an Inception-v3 CNN pre-trained with ImageNet dataset. We also use transfer learning on training a support vector machine (SVM) classifier on a tissue labeled colorectal cancer dataset aiming to filter the patches from a breast cancer HI and remove the irrelevant ones. We show that removing irrelevant patches before training a second SVM classifier, improves the accuracy for classifying malign and benign tumors on breast cancer images. We are able to improve the classification accuracy in 3.7% using the feature extraction transfer learning and an additional 0.7% using the irrelevant patch elimination. The proposed approach outperforms the state-of-the-art in three out of the four magnification factors of the breast cancer dataset.

</details>

<details>

<summary>2019-04-16 17:49:50 - Temporal Cycle-Consistency Learning</summary>

- *Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman*

- `1904.07846v1` - [abs](http://arxiv.org/abs/1904.07846v1) - [pdf](http://arxiv.org/pdf/1904.07846v1)

> We introduce a self-supervised representation learning method based on the task of temporal alignment between videos. The method trains a network using temporal cycle consistency (TCC), a differentiable cycle-consistency loss that can be used to find correspondences across time in multiple videos. The resulting per-frame embeddings can be used to align videos by simply matching frames using the nearest-neighbors in the learned embedding space.   To evaluate the power of the embeddings, we densely label the Pouring and Penn Action video datasets for action phases. We show that (i) the learned embeddings enable few-shot classification of these action phases, significantly reducing the supervised training requirements; and (ii) TCC is complementary to other methods of self-supervised learning in videos, such as Shuffle and Learn and Time-Contrastive Networks. The embeddings are also used for a number of applications based on alignment (dense temporal correspondence) between video pairs, including transfer of metadata of synchronized modalities between videos (sounds, temporal semantic labels), synchronized playback of multiple videos, and anomaly detection. Project webpage: https://sites.google.com/view/temporal-cycle-consistency .

</details>

<details>

<summary>2019-04-16 20:09:41 - Semantic Characteristics of Schizophrenic Speech</summary>

- *Kfir Bar, Vered Zilberstein, Ido Ziv, Heli Baram, Nachum Dershowitz, Samuel Itzikowitz, Eiran Vadim Harel*

- `1904.07953v1` - [abs](http://arxiv.org/abs/1904.07953v1) - [pdf](http://arxiv.org/pdf/1904.07953v1)

> Natural language processing tools are used to automatically detect disturbances in transcribed speech of schizophrenia inpatients who speak Hebrew. We measure topic mutation over time and show that controls maintain more cohesive speech than inpatients. We also examine differences in how inpatients and controls use adjectives and adverbs to describe content words and show that the ones used by controls are more common than the those of inpatients. We provide experimental results and show their potential for automatically detecting schizophrenia in patients by means only of their speech patterns.

</details>

<details>

<summary>2019-04-16 20:36:07 - Multi-Channel Attention Selection GAN with Cascaded Semantic Guidance for Cross-View Image Translation</summary>

- *Hao Tang, Dan Xu, Nicu Sebe, Yanzhi Wang, Jason J. Corso, Yan Yan*

- `1904.06807v2` - [abs](http://arxiv.org/abs/1904.06807v2) - [pdf](http://arxiv.org/pdf/1904.06807v2)

> Cross-view image translation is challenging because it involves images with drastically different views and severe deformation. In this paper, we propose a novel approach named Multi-Channel Attention SelectionGAN (SelectionGAN) that makes it possible to generate images of natural scenes in arbitrary viewpoints, based on an image of the scene and a novel semantic map. The proposed SelectionGAN explicitly utilizes the semantic information and consists of two stages. In the first stage, the condition image and the target semantic map are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using a multi-channel attention selection mechanism. Moreover, uncertainty maps automatically learned from attentions are used to guide the pixel loss for better network optimization. Extensive experiments on Dayton, CVUSA and Ego2Top datasets show that our model is able to generate significantly better results than the state-of-the-art methods. The source code, data and trained models are available at https://github.com/Ha0Tang/SelectionGAN.

</details>

<details>

<summary>2019-04-16 21:22:39 - DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs</summary>

- *Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner*

- `1903.00161v2` - [abs](http://arxiv.org/abs/1903.00161v2) - [pdf](http://arxiv.org/pdf/1903.00161v2)

> Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.0%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.

</details>

<details>

<summary>2019-04-16 21:56:13 - Generative Code Modeling with Graphs</summary>

- *Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, Oleksandr Polozov*

- `1805.08490v2` - [abs](http://arxiv.org/abs/1805.08490v2) - [pdf](http://arxiv.org/pdf/1805.08490v2)

> Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. The generative procedure interleaves grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.

</details>

<details>

<summary>2019-04-17 00:18:00 - Robust Change Captioning</summary>

- *Dong Huk Park, Trevor Darrell, Anna Rohrbach*

- `1901.02527v2` - [abs](http://arxiv.org/abs/1901.02527v2) - [pdf](http://arxiv.org/pdf/1901.02527v2)

> Describing what has changed in a scene can be useful to a user, but only if generated text focuses on what is semantically relevant. It is thus important to distinguish distractors (e.g. a viewpoint change) from relevant changes (e.g. an object has moved). We present a novel Dual Dynamic Attention Model (DUDA) to perform robust Change Captioning. Our model learns to distinguish distractors from semantic changes, localize the changes via Dual Attention over "before" and "after" images, and accurately describe them in natural language via Dynamic Speaker, by adaptively focusing on the necessary visual inputs (e.g. "before" or "after" image). To study the problem in depth, we collect a CLEVR-Change dataset, built off the CLEVR engine, with 5 types of scene changes. We benchmark a number of baselines on our dataset, and systematically study different change types and robustness to distractors. We show the superiority of our DUDA model in terms of both change captioning and localization. We also show that our approach is general, obtaining state-of-the-art results on the recent realistic Spot-the-Diff dataset which has no distractors.

</details>

<details>

<summary>2019-04-17 06:20:53 - Patent Analytics Based on Feature Vector Space Model: A Case of IoT</summary>

- *Lei Lei, Jiaju Qi, Kan Zheng*

- `1904.08100v1` - [abs](http://arxiv.org/abs/1904.08100v1) - [pdf](http://arxiv.org/pdf/1904.08100v1)

> The number of approved patents worldwide increases rapidly each year, which requires new patent analytics to efficiently mine the valuable information attached to these patents. Vector space model (VSM) represents documents as high-dimensional vectors, where each dimension corresponds to a unique term. While originally proposed for information retrieval systems, VSM has also seen wide applications in patent analytics, and used as a fundamental tool to map patent documents to structured data. However, VSM method suffers from several limitations when applied to patent analysis tasks, such as loss of sentence-level semantics and curse-of-dimensionality problems. In order to address the above limitations, we propose a patent analytics based on feature vector space model (FVSM), where the FVSM is constructed by mapping patent documents to feature vectors extracted by convolutional neural networks (CNN). The applications of FVSM for three typical patent analysis tasks, i.e., patents similarity comparison, patent clustering, and patent map generation are discussed. A case study using patents related to Internet of Things (IoT) technology is illustrated to demonstrate the performance and effectiveness of FVSM. The proposed FVSM can be adopted by other patent analysis studies to replace VSM, based on which various big data learning tasks can be performed.

</details>

<details>

<summary>2019-04-17 11:53:52 - Guided Anisotropic Diffusion and Iterative Learning for Weakly Supervised Change Detection</summary>

- *Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau*

- `1904.08208v1` - [abs](http://arxiv.org/abs/1904.08208v1) - [pdf](http://arxiv.org/pdf/1904.08208v1)

> Large scale datasets created from user labels or openly available data have become crucial to provide training data for large scale learning algorithms. While these datasets are easier to acquire, the data are frequently noisy and unreliable, which is motivating research on weakly supervised learning techniques. In this paper we propose an iterative learning method that extracts the useful information from a large scale change detection dataset generated from open vector data to train a fully convolutional network which surpasses the performance obtained by naive supervised learning. We also propose the guided anisotropic diffusion algorithm, which improves semantic segmentation results using the input images as guides to perform edge preserving filtering, and is used in conjunction with the iterative training method to improve results.

</details>

<details>

<summary>2019-04-17 14:21:43 - True Parallel Graph Transformations: an Algebraic Approach Based on Weak Spans</summary>

- *Thierry Boy de la Tour, Rachid Echahed*

- `1904.08850v1` - [abs](http://arxiv.org/abs/1904.08850v1) - [pdf](http://arxiv.org/pdf/1904.08850v1)

> We address the problem of defining graph transformations by the simultaneous application of direct transformations even when these cannot be applied independently of each other. An algebraic approach is adopted, with production rules of the form $L\xleftarrow{l}K \xleftarrow{i} I \xrightarrow{r} R$, called weak spans. A parallel coherent transformation is introduced and shown to be a conservative extension of the interleaving semantics of parallel independent direct transformations. A categorical construction of finitely attributed structures is proposed, in which parallel coherent transformations can be built in a natural way. These notions are introduced and illustrated on detailed examples.

</details>

<details>

<summary>2019-04-17 19:55:20 - CraftAssist Instruction Parsing: Semantic Parsing for a Minecraft Assistant</summary>

- *Yacine Jernite, Kavya Srinet, Jonathan Gray, Arthur Szlam*

- `1905.01978v1` - [abs](http://arxiv.org/abs/1905.01978v1) - [pdf](http://arxiv.org/pdf/1905.01978v1)

> We propose a large scale semantic parsing dataset focused on instruction-driven communication with an agent in Minecraft. We describe the data collection process which yields additional 35K human generated instructions with their semantic annotations. We report the performance of three baseline models and find that while a dataset of this size helps us train a usable instruction parser, it still poses interesting generalization challenges which we hope will help develop better and more robust models.

</details>

<details>

<summary>2019-04-17 22:40:01 - Towards Open Intent Discovery for Conversational Text</summary>

- *Nikhita Vedula, Nedim Lipka, Pranav Maneriker, Srinivasan Parthasarathy*

- `1904.08524v1` - [abs](http://arxiv.org/abs/1904.08524v1) - [pdf](http://arxiv.org/pdf/1904.08524v1)

> Detecting and identifying user intent from text, both written and spoken, plays an important role in modelling and understand dialogs. Existing research for intent discovery model it as a classification task with a predefined set of known categories. To generailze beyond these preexisting classes, we define a new task of \textit{open intent discovery}. We investigate how intent can be generalized to those not seen during training. To this end, we propose a two-stage approach to this task - predicting whether an utterance contains an intent, and then tagging the intent in the input utterance. Our model consists of a bidirectional LSTM with a CRF on top to capture contextual semantics, subject to some constraints. Self-attention is used to learn long distance dependencies. Further, we adapt an adversarial training approach to improve robustness and perforamce across domains. We also present a dataset of 25k real-life utterances that have been labelled via crowd sourcing. Our experiments across different domains and real-world datasets show the effectiveness of our approach, with less than 100 annotated examples needed per unique domain to recognize diverse intents. The approach outperforms state-of-the-art baselines by 5-15% F1 score points.

</details>

<details>

<summary>2019-04-17 23:52:20 - Material Segmentation of Multi-View Satellite Imagery</summary>

- *Matthew Purri, Jia Xue, Kristin Dana, Matthew Leotta, Dan Lipsa, Zhixin Li, Bo Xu, Jie Shan*

- `1904.08537v1` - [abs](http://arxiv.org/abs/1904.08537v1) - [pdf](http://arxiv.org/pdf/1904.08537v1)

> Material recognition methods use image context and local cues for pixel-wise classification. In many cases only a single image is available to make a material prediction. Image sequences, routinely acquired in applications such as mutliview stereo, can provide a sampling of the underlying reflectance functions that reveal pixel-level material attributes. We investigate multi-view material segmentation using two datasets generated for building material segmentation and scene material segmentation from the SpaceNet Challenge satellite image dataset. In this paper, we explore the impact of multi-angle reflectance information by introducing the \textit{reflectance residual encoding}, which captures both the multi-angle and multispectral information present in our datasets. The residuals are computed by differencing the sparse-sampled reflectance function with a dictionary of pre-defined dense-sampled reflectance functions. Our proposed reflectance residual features improves material segmentation performance when integrated into pixel-wise and semantic segmentation architectures. At test time, predictions from individual segmentations are combined through softmax fusion and refined by building segment voting. We demonstrate robust and accurate pixelwise segmentation results using the proposed material segmentation pipeline.

</details>

<details>

<summary>2019-04-18 02:54:33 - Discriminative Supervised Hashing for Cross-Modal similarity Search</summary>

- *Jun Yu, Xiao-Jun Wu, Josef Kittler*

- `1812.07660v3` - [abs](http://arxiv.org/abs/1812.07660v3) - [pdf](http://arxiv.org/pdf/1812.07660v3)

> With the advantage of low storage cost and high retrieval efficiency, hashing techniques have recently been an emerging topic in cross-modal similarity search. As multiple modal data reflect similar semantic content, many researches aim at learning unified binary codes. However, discriminative hashing features learned by these methods are not adequate. This results in lower accuracy and robustness. We propose a novel hashing learning framework which jointly performs classifier learning, subspace learning and matrix factorization to preserve class-specific semantic content, termed Discriminative Supervised Hashing (DSH), to learn the discrimative unified binary codes for multi-modal data. Besides, reducing the loss of information and preserving the non-linear structure of data, DSH non-linearly projects different modalities into the common space in which the similarity among heterogeneous data points can be measured. Extensive experiments conducted on the three publicly available datasets demonstrate that the framework proposed in this paper outperforms several state-of -the-art methods.

</details>

<details>

<summary>2019-04-18 06:37:15 - Continual Learning for Sentence Representations Using Conceptors</summary>

- *Tianlin Liu, Lyle Ungar, João Sedoc*

- `1904.09187v1` - [abs](http://arxiv.org/abs/1904.09187v1) - [pdf](http://arxiv.org/pdf/1904.09187v1)

> Distributed representations of sentences have become ubiquitous in natural language processing tasks. In this paper, we consider a continual learning scenario for sentence representations: Given a sequence of corpora, we aim to optimize the sentence encoder with respect to the new corpus while maintaining its accuracy on the old corpora. To address this problem, we propose to initialize sentence encoders with the help of corpus-independent features, and then sequentially update sentence encoders using Boolean operations of conceptor matrices to learn corpus-dependent features. We evaluate our approach on semantic textual similarity tasks and show that our proposed sentence encoder can continually learn features from new corpora while retaining its competence on previously encountered corpora.

</details>

<details>

<summary>2019-04-18 07:52:54 - Ontology-based Design of Experiments on Big Data Solutions</summary>

- *Maximilian Zocholl, Elena Camossi, Anne-Laure Jousselme, Cyril Ray*

- `1904.08626v1` - [abs](http://arxiv.org/abs/1904.08626v1) - [pdf](http://arxiv.org/pdf/1904.08626v1)

> Big data solutions are designed to cope with data of huge Volume and wide Variety, that need to be ingested at high Velocity and have potential Veracity issues, challenging characteristics that are usually referred to as the "4Vs of Big Data". In order to evaluate possibly complex big data solutions, stress tests require to assess a large number of combinations of sub-components jointly with the possible big data variations. A formalization of the Design of Experiments (DoE) on big data solutions is aimed at ensuring the reproducibility of the experiments, facilitating their partitioning in sub-experiments and guaranteeing the consistency of their outcomes in a global assessment. In this paper, an ontology-based approach is proposed to support the evaluation of a big data system in two ways. Firstly, the approach formalizes a decomposition and recombination of the big data solution, allowing for the aggregation of component evaluation results at inter-component level. Secondly, existing work on DoE is translated into an ontology for supporting the selection of experiments. The proposed ontology-based approach offers the possibility to combine knowledge from the evaluation domain and the application domain. It exploits domain and inter-domain specific restrictions on the factor combinations in order to reduce the number of experiments. Contrary to existing approaches, the proposed use of ontologies is not limited to the assertional description and exploitation of past experiments but offers richer terminological descriptions for the development of a DoE from scratch. As an application example, a maritime big data solution to the problem of detecting and predicting vessel suspicious behaviour through mobility analysis is selected. The article is concluded with a sketch of future works.

</details>

<details>

<summary>2019-04-18 10:30:11 - An Ontological Analysis of Business Process Modeling and Execution</summary>

- *Robert Singer*

- `1905.00499v1` - [abs](http://arxiv.org/abs/1905.00499v1) - [pdf](http://arxiv.org/pdf/1905.00499v1)

> This work presents a fully elaborated ontology, defined via the Ontology Web Language (OWL), of the Business Process Model and Notation (BPMN) standard to define business process models, and we demonstrate that any BPMN model can be serialized as OWL file. Based on ontological analysis and a corresponding definition of a modeling notation as ontology we show that business process models can be transformed from one notation into another one as long as there are common underlying concepts; this is demonstrated with the case of an actor based, or subject-oriented, view on business processes. Furthermore, a reference architecture for Workflow Management Systems (WfMS) based on microservices is discussed which is capable of executing actor based business process models. As a transformation of BPMN models into the actor based view is generally possible, also BPMN models could be enacted. As a result, we can conclude that the actor system is a promising way to stimulate new ways to design workflow management systems and to design business process modeling languages which are more comfortable to use by non-experts without losing necessary expressiveness. Another result is that an ontology is a productive way to define a modeling notation as it can be used as knowledge base, it is a formal conceptualization of the underlying notions, and can be semantically enriched for further use.

</details>

<details>

<summary>2019-04-18 13:38:57 - BowTie - A deep learning feedforward neural network for sentiment analysis</summary>

- *Apostol Vassilev*

- `1904.12624v1` - [abs](http://arxiv.org/abs/1904.12624v1) - [pdf](http://arxiv.org/pdf/1904.12624v1)

> How to model and encode the semantics of human-written text and select the type of neural network to process it are not settled issues in sentiment analysis. Accuracy and transferability are critical issues in machine learning in general. These properties are closely related to the loss estimates for the trained model. I present a computationally-efficient and accurate feedforward neural network for sentiment prediction capable of maintaining low losses. When coupled with an effective semantics model of the text, it provides highly accurate models with low losses. Experimental results on representative benchmark datasets and comparisons to other methods show the advantages of the new approach.

</details>

<details>

<summary>2019-04-18 14:50:00 - New Subgraph Isomorphism Algorithms: Vertex versus Path-at-a-time Matching</summary>

- *Mosab Hassaan, Karam Gouda*

- `1904.08819v1` - [abs](http://arxiv.org/abs/1904.08819v1) - [pdf](http://arxiv.org/pdf/1904.08819v1)

> Graphs are widely used to model complicated data semantics in many application domains. In this paper, two novel and efficient algorithms Fast-ON and Fast-P are proposed for solving the subgraph isomorphism problem. The two algorithms are based on Ullman algorithm [Ullmann 1976], apply vertex-at-a-time matching manner and path-at-a-time matching manner respectively, and use effective heuristics to cut the search space. Comparing to the well-known algorithms, Fast-ON and Fast-P achieve up to 1-4 orders of magnitude speed-up for both dense and sparse graph data.

</details>

<details>

<summary>2019-04-18 15:22:33 - Inspecting and Interacting with Meaningful Music Representations using VAE</summary>

- *Ruihan Yang, Tianyao Chen, Yiyi Zhang, Gus Xia*

- `1904.08842v1` - [abs](http://arxiv.org/abs/1904.08842v1) - [pdf](http://arxiv.org/pdf/1904.08842v1)

> Variational Autoencoders(VAEs) have already achieved great results on image generation and recently made promising progress on music generation. However, the generation process is still quite difficult to control in the sense that the learned latent representations lack meaningful music semantics. It would be much more useful if people can modify certain music features, such as rhythm and pitch contour, via latent representations to test different composition ideas. In this paper, we propose a new method to inspect the pitch and rhythm interpretations of the latent representations and we name it disentanglement by augmentation. Based on the interpretable representations, an intuitive graphical user interface is designed for users to better direct the music creation process by manipulating the pitch contours and rhythmic complexity.

</details>

<details>

<summary>2019-04-18 21:33:15 - Genie: A Generator of Natural Language Semantic Parsers for Virtual Assistant Commands</summary>

- *Giovanni Campagna, Silei Xu, Mehrad Moradshahi, Richard Socher, Monica S. Lam*

- `1904.09020v1` - [abs](http://arxiv.org/abs/1904.09020v1) - [pdf](http://arxiv.org/pdf/1904.09020v1)

> To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort. We advocate formalizing the capability of virtual assistants with a Virtual Assistant Programming Language (VAPL) and using a neural semantic parser to translate natural language into VAPL code. Genie needs only a small realistic set of input sentences for validating the neural model. Developers write templates to synthesize data; Genie uses crowdsourced paraphrases and data augmentation, along with the synthesized data, to train a semantic parser. We also propose design principles that make VAPL languages amenable to natural language translation. We apply these principles to revise ThingTalk, the language used by the Almond virtual assistant. We use Genie to build the first semantic parser that can support compound virtual assistants commands with unquoted free-form parameters. Genie achieves a 62% accuracy on realistic user inputs. We demonstrate Genie's generality by showing a 19% and 31% improvement over the previous state of the art on a music skill, aggregate functions, and access control.

</details>

<details>

<summary>2019-04-18 22:12:01 - Making Meaning: Semiotics Within Predictive Knowledge Architectures</summary>

- *Alex Kearney, Oliver Oxton*

- `1904.09023v1` - [abs](http://arxiv.org/abs/1904.09023v1) - [pdf](http://arxiv.org/pdf/1904.09023v1)

> Within Reinforcement Learning, there is a fledgling approach to conceptualizing the environment in terms of predictions. Central to this predictive approach is the assertion that it is possible to construct ontologies in terms of predictions about sensation, behaviour, and time---to categorize the world into entities which express all aspects of the world using only predictions. This construction of ontologies is integral to predictive approaches to machine knowledge where objects are described exclusively in terms of how they are perceived. In this paper, we ground the Pericean model of semiotics in terms of Reinforcement Learning Methods, describing Peirce's Three Categories in the notation of General Value Functions. Using the Peircean model of semiotics, we demonstrate that predictions alone are insufficient to construct an ontology; however, we identify predictions as being integral to the meaning-making process. Moreover, we discuss how predictive knowledge provides a particularly stable foundation for semiosis\textemdash the process of making meaning\textemdash and suggest a possible avenue of research to design algorithmic methods which construct semantics and meaning using predictions.

</details>

<details>

<summary>2019-04-19 09:20:55 - AutoAlias: Automatic Variable-Precision Alias Analysis for Object-Oriented Programs</summary>

- *Victor Rivera, Bertrand Meyer*

- `1808.08748v2` - [abs](http://arxiv.org/abs/1808.08748v2) - [pdf](http://arxiv.org/pdf/1808.08748v2)

> The aliasing question (can two reference expressions point, during an execution, to the same object?) is both one of the most critical in practice, for applications ranging from compiler optimization to programmer verification, and one of the most heavily researched, with many hundreds of publications over several decades. One might then expect that good off-the-shelf solutions are widely available, ready to be plugged into a compiler or verifier. This is not the case. In practice, efficient and precise alias analysis remains an open problem.   We present a practical tool, AutoAlias, which can be used to perform automatic alias analysis for object-oriented programs. Based on the theory of "duality semantics", an application of Abstract Interpretation ideas, it is directed at object-oriented languages and has been implemented for Eiffel as an addition to the EiffelStudio environment. It offers variable-precision analysis, controllable through the choice of a constant that governs the number of fix point iterations: a higher number means better precision and higher computation time.   All the source code of AutoAlias, as well as detailed results of analyses reported in this article, are publicly available. Practical applications so far have covered a library of data structures and algorithms and a library for GUI creation. For the former, AutoAlias achieves a precision appropriate for practical purposes and execution times in the order of 25 seconds for about 8000 lines of intricate code. For the GUI library, AutoAlias produces the alias analysis in around 232 seconds for about 150000 lines of intricate code.

</details>

<details>

<summary>2019-04-19 13:48:02 - A context-aware knowledge acquisition for planning applications using ontologies</summary>

- *Mohannad Babli, Eva Onaindia*

- `1904.09845v1` - [abs](http://arxiv.org/abs/1904.09845v1) - [pdf](http://arxiv.org/pdf/1904.09845v1)

> Automated planning technology has developed significantly. Designing a planning model that allows an automated agent to be capable of reacting intelligently to unexpected events in a real execution environment yet remains a challenge. This article describes a domain-independent approach to allow the agent to be context-aware of its execution environment and the task it performs, acquire new information that is guaranteed to be related and more importantly manageable, and integrate such information into its model through the use of ontologies and semantic operations to autonomously formulate new objectives, resulting in a more human-like behaviour for handling unexpected events in the context of opportunities.

</details>

<details>

<summary>2019-04-19 15:10:56 - ERNIE: Enhanced Representation through Knowledge Integration</summary>

- *Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu*

- `1904.09223v1` - [abs](http://arxiv.org/abs/1904.09223v1) - [pdf](http://arxiv.org/pdf/1904.09223v1)

> We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.

</details>

<details>

<summary>2019-04-21 17:02:56 - Deep Metric Learning Beyond Binary Supervision</summary>

- *Sungyeon Kim, Minkyo Seo, Ivan Laptev, Minsu Cho, Suha Kwak*

- `1904.09626v1` - [abs](http://arxiv.org/abs/1904.09626v1) - [pdf](http://arxiv.org/pdf/1904.09626v1)

> Metric Learning for visual similarity has mostly adopted binary supervision indicating whether a pair of images are of the same class or not. Such a binary indicator covers only a limited subset of image relations, and is not sufficient to represent semantic similarity between images described by continuous and/or structured labels such as object poses, image captions, and scene graphs. Motivated by this, we present a novel method for deep metric learning using continuous labels. First, we propose a new triplet loss that allows distance ratios in the label space to be preserved in the learned metric space. The proposed loss thus enables our model to learn the degree of similarity rather than just the order. Furthermore, we design a triplet mining strategy adapted to metric learning with continuous labels. We address three different image retrieval tasks with continuous labels in terms of human poses, room layouts and image captions, and demonstrate the superior performance of our approach compared to previous methods.

</details>

<details>

<summary>2019-04-22 14:34:28 - Cut-free Calculi and Relational Semantics for Temporal STIT Logics</summary>

- *Kees van Berkel, Tim Lyon*

- `1904.09899v1` - [abs](http://arxiv.org/abs/1904.09899v1) - [pdf](http://arxiv.org/pdf/1904.09899v1)

> We present cut-free labelled sequent calculi for a central formalism in logics of agency: STIT logics with temporal operators. These include sequent systems for Ldm, Tstit and Xstit. All calculi presented possess essential structural properties such as contraction- and cut-admissibility. The labelled calculi G3Ldm and G3TSTIT are shown sound and complete relative to irreflexive temporal frames. Additionally, we extend current results by showing that also XSTIT can be characterized through relational frames, omitting the use of BT+AC frames.

</details>

<details>

<summary>2019-04-22 15:24:11 - Gradient-based Inference for Networks with Output Constraints</summary>

- *Jay Yoon Lee, Sanket Vaibhav Mehta, Michael Wick, Jean-Baptiste Tristan, Jaime Carbonell*

- `1707.08608v3` - [abs](http://arxiv.org/abs/1707.08608v3) - [pdf](http://arxiv.org/pdf/1707.08608v3)

> Practitioners apply neural networks to increasingly complex problems in natural language processing, such as syntactic parsing and semantic role labeling that have rich output structures. Many such structured-prediction problems require deterministic constraints on the output values; for example, in sequence-to-sequence syntactic parsing, we require that the sequential outputs encode valid trees. While hidden units might capture such properties, the network is not always able to learn such constraints from the training data alone, and practitioners must then resort to post-processing. In this paper, we present an inference method for neural networks that enforces deterministic constraints on outputs without performing rule-based post-processing or expensive discrete search. Instead, in the spirit of gradient-based training, we enforce constraints with gradient-based inference (GBI): for each input at test-time, we nudge continuous model weights until the network's unconstrained inference procedure generates an output that satisfies the constraints. We study the efficacy of GBI on three tasks with hard constraints: semantic role labeling, syntactic parsing, and sequence transduction. In each case, the algorithm not only satisfies constraints but improves accuracy, even when the underlying network is state-of-the-art.

</details>

<details>

<summary>2019-04-22 16:46:03 - Strand Spaces with Choice via a Process Algebra Semantics</summary>

- *Fan Yang, Santiago Escobar, Catherine Meadows, José Meseguer*

- `1904.09946v1` - [abs](http://arxiv.org/abs/1904.09946v1) - [pdf](http://arxiv.org/pdf/1904.09946v1)

> Roles in cryptographic protocols do not always have a linear execution, but may include choice points causing the protocol to continue along different paths. In this paper we address the problem of representing choice in the strand space model of cryptographic protocols, particularly as it is used in the Maude-NPA cryptographic protocol analysis tool. To achieve this goal, we develop and give formal semantics to a process algebra for cryptographic protocols that supports a rich taxonomy of choice primitives for composing strand spaces. In our taxonomy, deterministic and non-deterministic choices are broken down further. Non-deterministic choice can be either explicit, i.e., one of two paths is chosen, or implicit, i.e., the value of a variable is chosen non-deterministically. Likewise, deterministic choice can be either an explicit if-then-else choice, i.e., one path is chosen if a predicate is satisfied, while the other is chosen if it is not, or implicit deterministic choice, i.e., execution continues only if a certain pattern is matched. We have identified a class of choices which includes finite branching and some cases of infinite branching, which we address in this paper. We provide a bisimulation result between the expected forwards execution semantics of the new process algebra and the original symbolic backwards semantics of Maude-NPA that preserves attack reachability. We have fully integrated the process algebra syntax and its transformation into strands in Maude-NPA. We illustrate its expressive power and naturalness with various examples, and show how it can be effectively used in formal analysis. This allows users to write protocols from now on using the process syntax, which is more convenient for expressing choice than the strand space syntax, in which choice can only be specified implicitly, via two or more strands that are identical until the choice point.

</details>

<details>

<summary>2019-04-22 21:10:25 - Bold Hearts Team Description for RoboCup 2019 (Humanoid Kid Size League)</summary>

- *Marcus M. Scheunemann, Sander G. van Dijk, Rebecca Miko, Daniel Barry, George M. Evans, Alessandra Rossi, Daniel Polani*

- `1904.10066v1` - [abs](http://arxiv.org/abs/1904.10066v1) - [pdf](http://arxiv.org/pdf/1904.10066v1)

> We participated in the RoboCup 2018 competition in Montreal with our newly developed BoldBot based on the Darwin-OP and mostly self-printed custom parts. This paper is about the lessons learnt from that competition and further developments for the RoboCup 2019 competition. Firstly, we briefly introduce the team along with an overview of past achievements. We then present a simple, standalone 2D simulator we use for simplifying the entry for new members with making basic RoboCup concepts quickly accessible. We describe our approach for semantic-segmentation for our vision used in the 2018 competition, which replaced the lookup-table (LUT) implementation we had before. We also discuss the extra structural support we plan to add to the printed parts of the BoldBot and our transition to ROS 2 as our new middleware. Lastly, we will present a collection of open-source contributions of our team.

</details>

<details>

<summary>2019-04-22 23:36:51 - The Use of Unlabeled Data versus Labeled Data for Stopping Active Learning for Text Classification</summary>

- *Garrett Beatty, Ethan Kochis, Michael Bloodgood*

- `1901.09126v2` - [abs](http://arxiv.org/abs/1901.09126v2) - [pdf](http://arxiv.org/pdf/1901.09126v2)

> Annotation of training data is the major bottleneck in the creation of text classification systems. Active learning is a commonly used technique to reduce the amount of training data one needs to label. A crucial aspect of active learning is determining when to stop labeling data. Three potential sources for informing when to stop active learning are an additional labeled set of data, an unlabeled set of data, and the training data that is labeled during the process of active learning. To date, no one has compared and contrasted the advantages and disadvantages of stopping methods based on these three information sources. We find that stopping methods that use unlabeled data are more effective than methods that use labeled data.

</details>

<details>

<summary>2019-04-22 23:56:41 - Stopping Active Learning based on Predicted Change of F Measure for Text Classification</summary>

- *Michael Altschuler, Michael Bloodgood*

- `1901.09118v2` - [abs](http://arxiv.org/abs/1901.09118v2) - [pdf](http://arxiv.org/pdf/1901.09118v2)

> During active learning, an effective stopping method allows users to limit the number of annotations, which is cost effective. In this paper, a new stopping method called Predicted Change of F Measure will be introduced that attempts to provide the users an estimate of how much performance of the model is changing at each iteration. This stopping method can be applied with any base learner. This method is useful for reducing the data annotation bottleneck encountered when building text classification systems.

</details>

<details>

<summary>2019-04-23 16:40:06 - Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</summary>

- *Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, Xiaogang Wang*

- `1807.07860v2` - [abs](http://arxiv.org/abs/1807.07860v2) - [pdf](http://arxiv.org/pdf/1807.07860v2)

> Talking face generation aims to synthesize a sequence of face images that correspond to a clip of speech. This is a challenging task because face appearance variation and semantics of speech are coupled together in the subtle movements of the talking face regions. Existing works either construct specific face appearance model on specific subjects or model the transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We find that the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. This disentangled representation has an advantage where both audio and video can serve as inputs for generation. Extensive experiments show that the proposed approach generates realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns than previous work. We also demonstrate the learned audio-visual representation is extremely useful for the tasks of automatic lip reading and audio-video retrieval.

</details>

<details>

<summary>2019-04-23 16:44:32 - DPatch: An Adversarial Patch Attack on Object Detectors</summary>

- *Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Hai Li, Yiran Chen*

- `1806.02299v4` - [abs](http://arxiv.org/abs/1806.02299v4) - [pdf](http://arxiv.org/pdf/1806.02299v4)

> Object detectors have emerged as an indispensable module in modern computer vision systems. In this work, we propose DPatch -- a black-box adversarial-patch-based attack towards mainstream object detectors (i.e. Faster R-CNN and YOLO). Unlike the original adversarial patch that only manipulates image-level classifier, our DPatch simultaneously attacks the bounding box regression and object classification so as to disable their predictions. Compared to prior works, DPatch has several appealing properties: (1) DPatch can perform both untargeted and targeted effective attacks, degrading the mAP of Faster R-CNN and YOLO from 75.10% and 65.7% down to below 1%, respectively. (2) DPatch is small in size and its attacking effect is location-independent, making it very practical to implement real-world attacks. (3) DPatch demonstrates great transferability among different detectors as well as training datasets. For example, DPatch that is trained on Faster R-CNN can effectively attack YOLO, and vice versa. Extensive evaluations imply that DPatch can perform effective attacks under black-box setup, i.e., even without the knowledge of the attacked network's architectures and parameters. Successful realization of DPatch also illustrates the intrinsic vulnerability of the modern detector architectures to such patch-based adversarial attacks.

</details>

<details>

<summary>2019-04-24 06:55:50 - Predicting Student Performance in an Educational Game Using a Hidden Markov Model</summary>

- *Manie Tadayon, Greg Pottie*

- `1904.11857v1` - [abs](http://arxiv.org/abs/1904.11857v1) - [pdf](http://arxiv.org/pdf/1904.11857v1)

> Contributions: Prior studies on education have mostly followed the model of the cross sectional study, namely, examining the pretest and the posttest scores. This paper shows that students' knowledge throughout the intervention can be estimated by time series analysis using a hidden Markov model. Background: Analyzing time series and the interaction between the students and the game data can result in valuable information that cannot be gained by only cross sectional studies of the exams. Research Questions: Can a hidden Markov model be used to analyze the educational games? Can a hidden Markov model be used to make a prediction of the students' performance? Methodology: The study was conducted on (N=854) students who played the Save Patch game. Students were divided into class 1 and class 2. Class 1 students are those who scored lower in the test than class 2 students. The analysis is done by choosing various features of the game as the observations. Findings: The state trajectories can predict the students' performance accurately for both class 1 and class 2.

</details>

<details>

<summary>2019-04-24 23:53:01 - Investigating the Effects of Word Substitution Errors on Sentence Embeddings</summary>

- *Rohit Voleti, Julie M. Liss, Visar Berisha*

- `1811.07021v2` - [abs](http://arxiv.org/abs/1811.07021v2) - [pdf](http://arxiv.org/pdf/1811.07021v2)

> A key initial step in several natural language processing (NLP) tasks involves embedding phrases of text to vectors of real numbers that preserve semantic meaning. To that end, several methods have been recently proposed with impressive results on semantic similarity tasks. However, all of these approaches assume that perfect transcripts are available when generating the embeddings. While this is a reasonable assumption for analysis of written text, it is limiting for analysis of transcribed text. In this paper we investigate the effects of word substitution errors, such as those coming from automatic speech recognition errors (ASR), on several state-of-the-art sentence embedding methods. To do this, we propose a new simulator that allows the experimenter to induce ASR-plausible word substitution errors in a corpus at a desired word error rate. We use this simulator to evaluate the robustness of several sentence embedding methods. Our results show that pre-trained neural sentence encoders are both robust to ASR errors and perform well on textual similarity tasks after errors are introduced. Meanwhile, unweighted averages of word vectors perform well with perfect transcriptions, but their performance degrades rapidly on textual similarity tasks for text with word substitution errors.

</details>

<details>

<summary>2019-04-25 03:52:57 - Self-Supervised Learning via Conditional Motion Propagation</summary>

- *Xiaohang Zhan, Xingang Pan, Ziwei Liu, Dahua Lin, Chen Change Loy*

- `1903.11412v3` - [abs](http://arxiv.org/abs/1903.11412v3) - [pdf](http://arxiv.org/pdf/1903.11412v3)

> Intelligent agent naturally learns from motion. Various self-supervised algorithms have leveraged motion cues to learn effective visual representations. The hurdle here is that motion is both ambiguous and complex, rendering previous works either suffer from degraded learning efficacy, or resort to strong assumptions on object motions. In this work, we design a new learning-from-motion paradigm to bridge these gaps. Instead of explicitly modeling the motion probabilities, we design the pretext task as a conditional motion propagation problem. Given an input image and several sparse flow guidance vectors on it, our framework seeks to recover the full-image motion. Compared to other alternatives, our framework has several appealing properties: (1) Using sparse flow guidance during training resolves the inherent motion ambiguity, and thus easing feature learning. (2) Solving the pretext task of conditional motion propagation encourages the emergence of kinematically-sound representations that poss greater expressive power. Extensive experiments demonstrate that our framework learns structural and coherent features; and achieves state-of-the-art self-supervision performance on several downstream tasks including semantic segmentation, instance segmentation, and human parsing. Furthermore, our framework is successfully extended to several useful applications such as semi-automatic pixel-level annotation. Project page: "http://mmlab.ie.cuhk.edu.hk/projects/CMP/".

</details>

<details>

<summary>2019-04-25 17:20:31 - Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation</summary>

- *Gregory P. Meyer, Jake Charland, Darshan Hegde, Ankit Laddha, Carlos Vallespi-Gonzalez*

- `1904.11466v1` - [abs](http://arxiv.org/abs/1904.11466v1) - [pdf](http://arxiv.org/pdf/1904.11466v1)

> In this paper, we present an extension to LaserNet, an efficient and state-of-the-art LiDAR based 3D object detector. We propose a method for fusing image data with the LiDAR data and show that this sensor fusion method improves the detection performance of the model especially at long ranges. The addition of image data is straightforward and does not require image labels. Furthermore, we expand the capabilities of the model to perform 3D semantic segmentation in addition to 3D object detection. On a large benchmark dataset, we demonstrate our approach achieves state-of-the-art performance on both object detection and semantic segmentation while maintaining a low runtime.

</details>

<details>

<summary>2019-04-25 17:59:35 - Local Relation Networks for Image Recognition</summary>

- *Han Hu, Zheng Zhang, Zhenda Xie, Stephen Lin*

- `1904.11491v1` - [abs](http://arxiv.org/abs/1904.11491v1) - [pdf](http://arxiv.org/pdf/1904.11491v1)

> The convolution layer has been the dominant feature extractor in computer vision for years. However, the spatial aggregation in convolution is basically a pattern matching process that applies fixed filters which are inefficient at modeling visual elements with varying spatial distributions. This paper presents a new image feature extractor, called the local relation layer, that adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. With this relational approach, it can composite visual elements into higher-level entities in a more efficient manner that benefits semantic inference. A network built with local relation layers, called the Local Relation Network (LR-Net), is found to provide greater modeling capacity than its counterpart built with regular convolution on large-scale recognition tasks such as ImageNet classification.

</details>

<details>

<summary>2019-04-25 20:01:08 - Neural Text Generation from Rich Semantic Representations</summary>

- *Valerie Hajdik, Jan Buys, Michael W. Goodman, Emily M. Bender*

- `1904.11564v1` - [abs](http://arxiv.org/abs/1904.11564v1) - [pdf](http://arxiv.org/pdf/1904.11564v1)

> We propose neural models to generate high-quality text from structured representations based on Minimal Recursion Semantics (MRS). MRS is a rich semantic representation that encodes more precise semantic detail than other representations such as Abstract Meaning Representation (AMR). We show that a sequence-to-sequence model that maps a linearization of Dependency MRS, a graph-based representation of MRS, to English text can achieve a BLEU score of 66.11 when trained on gold data. The performance can be improved further using a high-precision, broad coverage grammar-based parser to generate a large silver training corpus, achieving a final BLEU score of 77.17 on the full test set, and 83.37 on the subset of test data most closely matching the silver data domain. Our results suggest that MRS-based representations are a good choice for applications that need both structured semantics and the ability to produce natural language text as output.

</details>

<details>

<summary>2019-04-25 22:32:22 - Divide, Denoise, and Defend against Adversarial Attacks</summary>

- *Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, Oncel Tuzel*

- `1802.06806v2` - [abs](http://arxiv.org/abs/1802.06806v2) - [pdf](http://arxiv.org/pdf/1802.06806v2)

> Deep neural networks, although shown to be a successful class of machine learning algorithms, are known to be extremely unstable to adversarial perturbations. Improving the robustness of neural networks against these attacks is important, especially for security-critical applications. To defend against such attacks, we propose dividing the input image into multiple patches, denoising each patch independently, and reconstructing the image, without losing significant image content. We call our method D3. This proposed defense mechanism is non-differentiable which makes it non-trivial for an adversary to apply gradient-based attacks. Moreover, we do not fine-tune the network with adversarial examples, making it more robust against unknown attacks. We present an analysis of the tradeoff between accuracy and robustness against adversarial attacks. We evaluate our method under black-box, grey-box, and white-box settings. On the ImageNet dataset, our method outperforms the state-of-the-art by 19.7% under grey-box setting, and performs comparably under black-box setting. For the white-box setting, the proposed method achieves 34.4% accuracy compared to the 0% reported in the recent works.

</details>

<details>

<summary>2019-04-26 01:02:38 - A Concept Specification and Abstraction-based Semantic Representation: Addressing the Barriers to Rule-based Machine Translation</summary>

- *Patrick Connor*

- `1807.02226v3` - [abs](http://arxiv.org/abs/1807.02226v3) - [pdf](http://arxiv.org/pdf/1807.02226v3)

> Rule-based machine translation is more data efficient than the big data-based machine translation approaches, making it appropriate for languages with low bilingual corpus resources -- i.e., minority languages. However, the rule-based approach has declined in popularity relative to its big data cousins primarily because of the extensive training and labour required to define the language rules. To address this, we present a semantic representation that 1) treats all bits of meaning as individual concepts that 2) modify or further specify one another to build a network that relates entities in space and time. Also, the representation can 3) encapsulate propositions and thereby define concepts in terms of other concepts, supporting the abstraction of underlying linguistic and ontological details. These features afford an exact, yet intuitive semantic representation aimed at handling the great variety in language and reducing labour and training time. The proposed natural language generation, parsing, and translation strategies are also amenable to probabilistic modeling and thus to learning the necessary rules from example data.

</details>

<details>

<summary>2019-04-26 06:50:54 - The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision</summary>

- *Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu*

- `1904.12584v1` - [abs](http://arxiv.org/abs/1904.12584v1) - [pdf](http://arxiv.org/pdf/1904.12584v1)

> We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.

</details>

<details>

<summary>2019-04-26 07:45:48 - Interactive user interface based on Convolutional Auto-encoders for annotating CT-scans</summary>

- *Martin Längkvist, Jonas Widell, Per Thunberg, Amy Loutfi, Mats Lidén*

- `1904.11701v1` - [abs](http://arxiv.org/abs/1904.11701v1) - [pdf](http://arxiv.org/pdf/1904.11701v1)

> High resolution computed tomography (HRCT) is the most important imaging modality for interstitial lung diseases, where the radiologists are interested in identifying certain patterns, and their volumetric and regional distribution. The use of machine learning can assist the radiologists with both these tasks by performing semantic segmentation. In this paper, we propose an interactive annotation-tool for semantic segmentation that assists the radiologist in labeling CT scans. The annotation tool is evaluated by six radiologists and radiology residents classifying healthy lung and reticular pattern i HRCT images. The usability of the system is evaluated with a System Usability Score (SUS) and interaction information from the readers that used the tool for annotating the CT volumes. It was discovered that the experienced usability and how the users interactied with the system differed between the users. A higher SUS-score was given by users that prioritized learning speed over model accuracy and spent less time with manual labeling and instead utilized the suggestions provided by the GUI. An analysis of the annotation variations between the readers show substantial agreement (Cohen's kappa=0.69) for classification of healthy and affected lung parenchyma in pulmonary fibrosis. The inter-reader variation is a challenge for the definition of ground truth.

</details>

<details>

<summary>2019-04-26 09:43:11 - Representation Similarity Analysis for Efficient Task taxonomy & Transfer Learning</summary>

- *Kshitij Dwivedi, Gemma Roig*

- `1904.11740v1` - [abs](http://arxiv.org/abs/1904.11740v1) - [pdf](http://arxiv.org/pdf/1904.11740v1)

> Transfer learning is widely used in deep neural network models when there are few labeled examples available. The common approach is to take a pre-trained network in a similar task and finetune the model parameters. This is usually done blindly without a pre-selection from a set of pre-trained models, or by finetuning a set of models trained on different tasks and selecting the best performing one by cross-validation. We address this problem by proposing an approach to assess the relationship between visual tasks and their task-specific models. Our method uses Representation Similarity Analysis (RSA), which is commonly used to find a correlation between neuronal responses from brain data and models. With RSA we obtain a similarity score among tasks by computing correlations between models trained on different tasks. Our method is efficient as it requires only pre-trained models, and a few images with no further training. We demonstrate the effectiveness and efficiency of our method for generating task taxonomy on Taskonomy dataset. We next evaluate the relationship of RSA with the transfer learning performance on Taskonomy tasks and a new task: Pascal VOC semantic segmentation. Our results reveal that models trained on tasks with higher similarity score show higher transfer learning performance. Surprisingly, the best transfer learning result for Pascal VOC semantic segmentation is not obtained from the pre-trained model on semantic segmentation, probably due to the domain differences, and our method successfully selects the high performing models.

</details>

<details>

<summary>2019-04-26 12:44:38 - Understanding the Behaviors of BERT in Ranking</summary>

- *Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu*

- `1904.07531v4` - [abs](http://arxiv.org/abs/1904.07531v4) - [pdf](http://arxiv.org/pdf/1904.07531v4)

> This paper studies the performances and behaviors of BERT in ranking tasks. We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking. Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model. Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking. Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker.

</details>

<details>

<summary>2019-04-26 17:52:06 - Learning Semantic Vector Representations of Source Code via a Siamese Neural Network</summary>

- *David Wehr, Halley Fede, Eleanor Pence, Bo Zhang, Guilherme Ferreira, John Walczyk, Joseph Hughes*

- `1904.11968v1` - [abs](http://arxiv.org/abs/1904.11968v1) - [pdf](http://arxiv.org/pdf/1904.11968v1)

> The abundance of open-source code, coupled with the success of recent advances in deep learning for natural language processing, has given rise to a promising new application of machine learning to source code. In this work, we explore the use of a Siamese recurrent neural network model on Python source code to create vectors which capture the semantics of code. We evaluate the quality of embeddings by identifying which problem from a programming competition the code solves. Our model significantly outperforms a bag-of-tokens embedding, providing promising results for improving code embeddings that can be used in future software engineering tasks.

</details>

<details>

<summary>2019-04-27 02:05:15 - SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)</summary>

- *Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar*

- `1903.08983v3` - [abs](http://arxiv.org/abs/1903.08983v3) - [pdf](http://arxiv.org/pdf/1903.08983v3)

> We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets. It featured three sub-tasks. In sub-task A, the goal was to discriminate between offensive and non-offensive posts. In sub-task B, the focus was on the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, about 800 teams signed up to participate in the task, and 115 of them submitted results, which we present and analyze in this report.

</details>

<details>

<summary>2019-04-27 03:56:32 - Forget the Learning Rate, Decay Loss</summary>

- *Jiakai Wei*

- `1905.00094v1` - [abs](http://arxiv.org/abs/1905.00094v1) - [pdf](http://arxiv.org/pdf/1905.00094v1)

> In the usual deep neural network optimization process, the learning rate is the most important hyper parameter, which greatly affects the final convergence effect. The purpose of learning rate is to control the stepsize and gradually reduce the impact of noise on the network. In this paper, we will use a fixed learning rate with method of decaying loss to control the magnitude of the update. We used Image classification, Semantic segmentation, and GANs to verify this method. Experiments show that the loss decay strategy can greatly improve the performance of the model

</details>

<details>

<summary>2019-04-27 09:31:59 - WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations</summary>

- *Mohammad Taher Pilehvar, Jose Camacho-Collados*

- `1808.09121v3` - [abs](http://arxiv.org/abs/1808.09121v3) - [pdf](http://arxiv.org/pdf/1808.09121v3)

> By design, word embeddings are unable to model the dynamic nature of words' semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.

</details>

<details>

<summary>2019-04-27 11:23:30 - Exploratory Data Analysis of a Network Telescope Traffic and Prediction of Port Probing Rates</summary>

- *Mehdi Zakroum, Abdellah Houmz, Mounir Ghogho, Ghita Mezzour, Abdelkader Lahmadi, Jérôme François, Mohammed El Koutbi*

- `1812.09790v2` - [abs](http://arxiv.org/abs/1812.09790v2) - [pdf](http://arxiv.org/pdf/1812.09790v2)

> Understanding the properties exhibited by large scale network probing traffic would improve cyber threat intelligence. In addition, the prediction of probing rates is a key feature for security practitioners in their endeavors for making better operational decisions and for enhancing their defense strategy skills. In this work, we study different aspects of the traffic captured by a /20 network telescope. First, we perform an exploratory data analysis of the collected probing activities. The investigation includes probing rates at the port level, services interesting top network probers and the distribution of probing rates by geolocation. Second, we extract the network probers exploration patterns. We model these behaviors using transition graphs decorated with probabilities of switching from a port to another. Finally, we assess the capacity of Non-stationary Autoregressive and Vector Autoregressive models in predicting port probing rates as a first step towards using more robust models for better forecasting performance.

</details>

<details>

<summary>2019-04-27 20:40:03 - Soft Marginal TransE for Scholarly Knowledge Graph Completion</summary>

- *Mojtaba Nayyeri, Sahar Vahdati, Jens Lehmann, Hamed Shariat Yazdi*

- `1904.12211v1` - [abs](http://arxiv.org/abs/1904.12211v1) - [pdf](http://arxiv.org/pdf/1904.12211v1)

> Knowledge graphs (KGs), i.e. representation of information as a semantic graph, provide a significant test bed for many tasks including question answering, recommendation, and link prediction. Various amount of scholarly metadata have been made vailable as knowledge graphs from the diversity of data providers and agents. However, these high-quantities of data remain far from quality criteria in terms of completeness while growing at a rapid pace. Most of the attempts in completing such KGs are following traditional data digitization, harvesting and collaborative curation approaches. Whereas, advanced AI-related approaches such as embedding models - specifically designed for such tasks - are usually evaluated for standard benchmarks such as Freebase and Wordnet. The tailored nature of such datasets prevents those approaches to shed the lights on more accurate discoveries. Application of such models on domain-specific KGs takes advantage of enriched meta-data and provides accurate results where the underlying domain can enormously benefit. In this work, the TransE embedding model is reconciled for a specific link prediction task on scholarly metadata. The results show a significant shift in the accuracy and performance evaluation of the model on a dataset with scholarly metadata. The newly proposed version of TransE obtains 99.9% for link prediction task while original TransE gets 95%. In terms of accuracy and Hit@10, TransE outperforms other embedding models such as ComplEx, TransH and TransR experimented over scholarly knowledge graphs

</details>

<details>

<summary>2019-04-27 21:14:21 - Towards Recognizing Phrase Translation Processes: Experiments on English-French</summary>

- *Yuming Zhai, Pooyan Safari, Gabriel Illouz, Alexandre Allauzen, Anne Vilnat*

- `1904.12213v1` - [abs](http://arxiv.org/abs/1904.12213v1) - [pdf](http://arxiv.org/pdf/1904.12213v1)

> When translating phrases (words or group of words), human translators, consciously or not, resort to different translation processes apart from the literal translation, such as Idiom Equivalence, Generalization, Particularization, Semantic Modulation, etc. Translators and linguists (such as Vinay and Darbelnet, Newmark, etc.) have proposed several typologies to characterize the different translation processes. However, to the best of our knowledge, there has not been effort to automatically classify these fine-grained translation processes. Recently, an English-French parallel corpus of TED Talks has been manually annotated with translation process categories, along with established annotation guidelines. Based on these annotated examples, we propose an automatic classification of translation processes at subsentential level. Experimental results show that we can distinguish non-literal translation from literal translation with an accuracy of 87.09%, and 55.20% for classifying among five non-literal translation processes. This work demonstrates that it is possible to automatically classify translation processes. Even with a small amount of annotated examples, our experiments show the directions that we can follow in future work. One of our long term objectives is leveraging this automatic classification to better control paraphrase extraction from bilingual parallel corpora.

</details>

<details>

<summary>2019-04-28 03:21:28 - UniVSE: Robust Visual Semantic Embeddings via Structured Semantic Representations</summary>

- *Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, Wei-Ying Ma*

- `1904.05521v2` - [abs](http://arxiv.org/abs/1904.05521v2) - [pdf](http://arxiv.org/pdf/1904.05521v2)

> We propose Unified Visual-Semantic Embeddings (UniVSE) for learning a joint space of visual and textual concepts. The space unifies the concepts at different levels, including objects, attributes, relations, and full scenes. A contrastive learning approach is proposed for the fine-grained alignment from only image-caption pairs. Moreover, we present an effective approach for enforcing the coverage of semantic components that appear in the sentence. We demonstrate the robustness of Unified VSE in defending text-domain adversarial attacks on cross-modal retrieval tasks. Such robustness also empowers the use of visual cues to resolve word dependencies in novel sentences.

</details>

<details>

<summary>2019-04-28 11:57:08 - A Feature Based Methodology for Variable Requirements Reverse Engineering</summary>

- *Anas Alhamwieh, Said Ghoul*

- `1904.12309v1` - [abs](http://arxiv.org/abs/1904.12309v1) - [pdf](http://arxiv.org/pdf/1904.12309v1)

> In the past years, software reverse engineering dealt with source code understanding. Nowadays, it is levered to software requirements abstract level, supported by feature model notations, language independent, and simpler than the source code reading. The recent relevant approaches face the following insufficiencies: lack of a complete integrated methodology, adapted feature model, feature patterns recognition, and Graph based slicing. This work aims to provide some solutions to the above challenges through an integrated methodology. The following results are unique. Elementary and configuration features are specified in a uniform way by introducing semantics specific attributes. The reverse engineering supports feature pattern recognition and requirements feature model graph-based slicing. The slicing criteria are rich enough to allow answering questions of software requirements maintainers. A comparison of this proposed methodology, based on effective criteria, with the similar works, seems to be valuable and competitive: the enrichment of the feature model and feature pattern recognition were never approached and the proposed slicing technique is more general, effective, and practical.

</details>

<details>

<summary>2019-04-28 13:57:54 - OPIEC: An Open Information Extraction Corpus</summary>

- *Kiril Gashteovski, Sebastian Wanner, Sven Hertling, Samuel Broscheit, Rainer Gemulla*

- `1904.12324v1` - [abs](http://arxiv.org/abs/1904.12324v1) - [pdf](http://arxiv.org/pdf/1904.12324v1)

> Open information extraction (OIE) systems extract relations and their arguments from natural language text in an unsupervised manner. The resulting extractions are a valuable resource for downstream tasks such as knowledge base construction, open question answering, or event schema induction. In this paper, we release, describe, and analyze an OIE corpus called OPIEC, which was extracted from the text of English Wikipedia. OPIEC complements the available OIE resources: It is the largest OIE corpus publicly available to date (over 340M triples) and contains valuable metadata such as provenance information, confidence scores, linguistic annotations, and semantic annotations including spatial and temporal information. We analyze the OPIEC corpus by comparing its content with knowledge bases such as DBpedia or YAGO, which are also based on Wikipedia. We found that most of the facts between entities present in OPIEC cannot be found in DBpedia and/or YAGO, that OIE facts often differ in the level of specificity compared to knowledge base facts, and that OIE open relations are generally highly polysemous. We believe that the OPIEC corpus is a valuable resource for future research on automated knowledge base construction.

</details>

<details>

<summary>2019-04-29 05:51:46 - The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers</summary>

- *Dongxiang Zhang, Lei Wang, Luming Zhang, Bing Tian Dai, Heng Tao Shen*

- `1808.07290v2` - [abs](http://arxiv.org/abs/1808.07290v2) - [pdf](http://arxiv.org/pdf/1808.07290v2)

> Solving mathematical word problems (MWPs) automatically is challenging, primarily due to the semantic gap between human-readable words and machine-understandable logics. Despite the long history dated back to the1960s, MWPs have regained intensive attention in the past few years with the advancement of Artificial Intelligence (AI). Solving MWPs successfully is considered as a milestone towards general AI. Many systems have claimed promising results in self-crafted and small-scale datasets. However, when applied on large and diverse datasets, none of the proposed methods in the literature achieves high precision, revealing that current MWP solvers still have much room for improvement. This motivated us to present a comprehensive survey to deliver a clear and complete picture of automatic math problem solvers. In this survey, we emphasize on algebraic word problems, summarize their extracted features and proposed techniques to bridge the semantic gap and compare their performance in the publicly accessible datasets. We also cover automatic solvers for other types of math problems such as geometric problems that require the understanding of diagrams. Finally, we identify several emerging research directions for the readers with interests in MWPs.

</details>

<details>

<summary>2019-04-29 07:56:18 - Stochastic Optimization of Sorting Networks via Continuous Relaxations</summary>

- *Aditya Grover, Eric Wang, Aaron Zweig, Stefano Ermon*

- `1903.08850v2` - [abs](http://arxiv.org/abs/1903.08850v2) - [pdf](http://arxiv.org/pdf/1903.08850v2)

> Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm.

</details>

<details>

<summary>2019-04-29 10:21:58 - Semantic Matching of Documents from Heterogeneous Collections: A Simple and Transparent Method for Practical Applications</summary>

- *Mark-Christoph Müller*

- `1904.12550v1` - [abs](http://arxiv.org/abs/1904.12550v1) - [pdf](http://arxiv.org/pdf/1904.12550v1)

> We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.

</details>

<details>

<summary>2019-04-30 03:14:33 - Structured Semantic Model supported Deep Neural Network for Click-Through Rate Prediction</summary>

- *Chenglei Niu, Guojing Zhong, Ying Liu, Yandong Zhang, Yongsheng Sun, Ailong He, Zhaoji Chen*

- `1812.01353v5` - [abs](http://arxiv.org/abs/1812.01353v5) - [pdf](http://arxiv.org/pdf/1812.01353v5)

> With the rapid development of online advertising and recommendation systems, click-through rate prediction is expected to play an increasingly important role.Recently many DNN-based models which follow a similar Embedding&MLP paradigm have been proposed, and have achieved good result in image/voice and nlp fields. In these methods the Wide&Deep model announced by Google plays a key role.Most models first map large scale sparse input features into low-dimensional vectors which are transformed to fixed-length vectors, then concatenated together before being fed into a multilayer perceptron (MLP) to learn non-linear relations among input features. The number of trainable variables normally grow dramatically the number of feature fields and the embedding dimension grow. It is a big challenge to get state-of-the-art result through training deep neural network and embedding together, which falls into local optimal or overfitting easily. In this paper, we propose an Structured Semantic Model (SSM) to tackles this challenge by designing a orthogonal base convolution and pooling model which adaptively learn the multi-scale base semantic representation between features supervised by the click label.The output of SSM are then used in the Wide&Deep for CTR prediction.Experiments on two public datasets as well as real Weibo production dataset with over 1 billion samples have demonstrated the effectiveness of our proposed approach with superior performance comparing to state-of-the-art methods.

</details>

<details>

<summary>2019-04-30 07:18:44 - ABC: A Big CAD Model Dataset For Geometric Deep Learning</summary>

- *Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo*

- `1812.06216v2` - [abs](http://arxiv.org/abs/1812.06216v2) - [pdf](http://arxiv.org/pdf/1812.06216v2)

> We introduce ABC-Dataset, a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms. As a use case for our dataset, we perform a large-scale benchmark for estimation of surface normals, comparing existing data driven methods and evaluating their performance against both the ground truth and traditional normal estimation methods.

</details>

<details>

<summary>2019-04-30 11:39:52 - Context-Aware Zero-Shot Learning for Object Recognition</summary>

- *Eloi Zablocki, Patrick Bordes, Benjamin Piwowarski, Laure Soulier, Patrick Gallinari*

- `1904.12638v2` - [abs](http://arxiv.org/abs/1904.12638v2) - [pdf](http://arxiv.org/pdf/1904.12638v2)

> Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging auxiliary knowledge, such as semantic representations. A limitation of previous approaches is that only intrinsic properties of objects, e.g. their visual appearance, are taken into account while their context, e.g. the surrounding objects in the image, is ignored. Following the intuitive principle that objects tend to be found in certain contexts but not others, we propose a new and challenging approach, context-aware ZSL, that leverages semantic representations in a new way to model the conditional likelihood of an object to appear in a given context. Finally, through extensive experiments conducted on Visual Genome, we show that contextual information can substantially improve the standard ZSL approach and is robust to unbalanced classes.

</details>

<details>

<summary>2019-04-30 12:44:22 - Semantic Referee: A Neural-Symbolic Framework for Enhancing Geospatial Semantic Segmentation</summary>

- *Marjan Alirezaie, Martin Längkvist, Michael Sioutis, Amy Loutfi*

- `1904.13196v1` - [abs](http://arxiv.org/abs/1904.13196v1) - [pdf](http://arxiv.org/pdf/1904.13196v1)

> Understanding why machine learning algorithms may fail is usually the task of the human expert that uses domain knowledge and contextual information to discover systematic shortcomings in either the data or the algorithm. In this paper, we propose a semantic referee, which is able to extract qualitative features of the errors emerging from deep machine learning frameworks and suggest corrections. The semantic referee relies on ontological reasoning about spatial knowledge in order to characterize errors in terms of their spatial relations with the environment. Using semantics, the reasoner interacts with the learning algorithm as a supervisor. In this paper, the proposed method of the interaction between a neural network classifier and a semantic referee shows how to improve the performance of semantic segmentation for satellite imagery data.

</details>

<details>

<summary>2019-04-30 14:08:37 - Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors</summary>

- *Vitalii Zhelezniak, Aleksandar Savkov, April Shen, Francesco Moramarco, Jack Flann, Nils Y. Hammerla*

- `1904.13264v1` - [abs](http://arxiv.org/abs/1904.13264v1) - [pdf](http://arxiv.org/pdf/1904.13264v1)

> Recent literature suggests that averaged word vectors followed by simple post-processing outperform many deep learning methods on semantic textual similarity tasks. Furthermore, when averaged word vectors are trained supervised on large corpora of paraphrases, they achieve state-of-the-art results on standard STS benchmarks. Inspired by these insights, we push the limits of word embeddings even further. We propose a novel fuzzy bag-of-words (FBoW) representation for text that contains all the words in the vocabulary simultaneously but with different degrees of membership, which are derived from similarities between word vectors. We show that max-pooled word vectors are only a special case of fuzzy BoW and should be compared via fuzzy Jaccard index rather than cosine similarity. Finally, we propose DynaMax, a completely unsupervised and non-parametric similarity measure that dynamically extracts and max-pools good features depending on the sentence pair. This method is both efficient and easy to implement, yet outperforms current baselines on STS tasks by a large margin and is even competitive with supervised word vectors trained to directly optimise cosine similarity.

</details>

<details>

<summary>2019-04-30 16:42:55 - Structured Prediction using cGANs with Fusion Discriminator</summary>

- *Faisal Mahmood, Wenhao Xu, Nicholas J. Durr, Jeremiah W. Johnson, Alan Yuille*

- `1904.13358v1` - [abs](http://arxiv.org/abs/1904.13358v1) - [pdf](http://arxiv.org/pdf/1904.13358v1)

> We propose the fusion discriminator, a single unified framework for incorporating conditional information into a generative adversarial network (GAN) for a variety of distinct structured prediction tasks, including image synthesis, semantic segmentation, and depth estimation. Much like commonly used convolutional neural network -- conditional Markov random field (CNN-CRF) models, the proposed method is able to enforce higher-order consistency in the model, but without being limited to a very specific class of potentials. The method is conceptually simple and flexible, and our experimental results demonstrate improvement on several diverse structured prediction tasks.

</details>

<details>

<summary>2019-04-30 18:29:28 - Personalized Ranking in eCommerce Search</summary>

- *Grigor Aslanyan, Aritra Mandal, Prathyusha Senthil Kumar, Amit Jaiswal, Manojkumar Rangasamy Kannadasan*

- `1905.00052v1` - [abs](http://arxiv.org/abs/1905.00052v1) - [pdf](http://arxiv.org/pdf/1905.00052v1)

> We address the problem of personalization in the context of eCommerce search. Specifically, we develop personalization ranking features that use in-session context to augment a generic ranker optimized for conversion and relevance. We use a combination of latent features learned from item co-clicks in historic sessions and content-based features that use item title and price. Personalization in search has been discussed extensively in the existing literature. The novelty of our work is combining and comparing content-based and content-agnostic features and showing that they complement each other to result in a significant improvement of the ranker. Moreover, our technique does not require an explicit re-ranking step, does not rely on learning user profiles from long term search behavior, and does not involve complex modeling of query-item-user features. Our approach captures item co-click propensity using lightweight item embeddings. We experimentally show that our technique significantly outperforms a generic ranker in terms of Mean Reciprocal Rank (MRR). We also provide anecdotal evidence for the semantic similarity captured by the item embeddings on the eBay search engine.

</details>


## 2019-05

<details>

<summary>2019-05-01 06:28:11 - Content Differences in Syntactic and Semantic Representations</summary>

- *Daniel Hershcovich, Omri Abend, Ari Rappoport*

- `1903.06494v5` - [abs](http://arxiv.org/abs/1903.06494v5) - [pdf](http://arxiv.org/pdf/1903.06494v5)

> Syntactic analysis plays an important role in semantic parsing, but the nature of this role remains a topic of ongoing debate. The debate has been constrained by the scarcity of empirical comparative studies between syntactic and semantic schemes, which hinders the development of parsing methods informed by the details of target schemes and constructions. We target this gap, and take Universal Dependencies (UD) and UCCA as a test case. After abstracting away from differences of convention or formalism, we find that most content divergences can be ascribed to: (1) UCCA's distinction between a Scene and a non-Scene; (2) UCCA's distinction between primary relations, secondary ones and participants; (3) different treatment of multi-word expressions, and (4) different treatment of inter-clause linkage. We further discuss the long tail of cases where the two schemes take markedly different approaches. Finally, we show that the proposed comparison methodology can be used for fine-grained evaluation of UCCA parsing, highlighting both challenges and potential sources for improvement. The substantial differences between the schemes suggest that semantic parsers are likely to benefit downstream text understanding applications beyond their syntactic counterparts.

</details>

<details>

<summary>2019-05-01 10:16:46 - Context-Dependent Semantic Parsing over Temporally Structured Data</summary>

- *Charles Chen, Razvan Bunescu*

- `1905.00245v1` - [abs](http://arxiv.org/abs/1905.00245v1) - [pdf](http://arxiv.org/pdf/1905.00245v1)

> We describe a new semantic parsing setting that allows users to query the system using both natural language questions and actions within a graphical user interface. Multiple time series belonging to an entity of interest are stored in a database and the user interacts with the system to obtain a better understanding of the entity's state and behavior, entailing sequences of actions and questions whose answers may depend on previous factual or navigational interactions. We design an LSTM-based encoder-decoder architecture that models context dependency through copying mechanisms and multiple levels of attention over inputs and previous outputs. When trained to predict tokens using supervised learning, the proposed architecture substantially outperforms standard sequence generation baselines. Training the architecture using policy gradient leads to further improvements in performance, reaching a sequence-level accuracy of 88.7% on artificial data and 74.8% on real data.

</details>

<details>

<summary>2019-05-01 10:52:54 - Model Comparison for Semantic Grouping</summary>

- *Francisco Vargas, Kamen Brestnichki, Nils Hammerla*

- `1904.13323v2` - [abs](http://arxiv.org/abs/1904.13323v2) - [pdf](http://arxiv.org/pdf/1904.13323v2)

> We introduce a probabilistic framework for quantifying the semantic similarity between two groups of embeddings. We formulate the task of semantic similarity as a model comparison task in which we contrast a generative model which jointly models two sentences versus one that does not. We illustrate how this framework can be used for the Semantic Textual Similarity tasks using clear assumptions about how the embeddings of words are generated. We apply model comparison that utilises information criteria to address some of the shortcomings of Bayesian model comparison, whilst still penalising model complexity. We achieve competitive results by applying the proposed framework with an appropriate choice of likelihood on the STS datasets.

</details>

<details>

<summary>2019-05-01 21:10:27 - Similarity Learning with Higher-Order Graph Convolutions for Brain Network Analysis</summary>

- *Guixiang Ma, Nesreen K. Ahmed, Ted Willke, Dipanjan Sengupta, Michael W. Cole, Nicholas B. Turk-Browne, Philip S. Yu*

- `1811.02662v5` - [abs](http://arxiv.org/abs/1811.02662v5) - [pdf](http://arxiv.org/pdf/1811.02662v5)

> Learning a similarity metric has gained much attention recently, where the goal is to learn a function that maps input patterns to a target space while preserving the semantic distance in the input space. While most related work focused on images, we focus instead on learning a similarity metric for neuroimages, such as fMRI and DTI images. We propose an end-to-end similarity learning framework called Higher-order Siamese GCN for multi-subject fMRI data analysis. The proposed framework learns the brain network representations via a supervised metric-based approach with siamese neural networks using two graph convolutional networks as the twin networks. Our proposed framework performs higher-order convolutions by incorporating higher-order proximity in graph convolutional networks to characterize and learn the community structure in brain connectivity networks. To the best of our knowledge, this is the first community-preserving similarity learning framework for multi-subject brain network analysis. Experimental results on four real fMRI datasets demonstrate the potential use cases of the proposed framework for multi-subject brain analysis in health and neuropsychiatric disorders. Our proposed approach achieves an average AUC gain of 75% compared to PCA, an average AUC gain of 65.5% compared to Spectral Embedding, and an average AUC gain of 24.3% compared to S-GCN across the four datasets, indicating promising application in clinical investigation and brain disease diagnosis.

</details>

<details>

<summary>2019-05-02 05:58:17 - Agnostic Lane Detection</summary>

- *Yuenan Hou*

- `1905.03704v1` - [abs](http://arxiv.org/abs/1905.03704v1) - [pdf](http://arxiv.org/pdf/1905.03704v1)

> Lane detection is an important yet challenging task in autonomous driving, which is affected by many factors, e.g., light conditions, occlusions caused by other vehicles, irrelevant markings on the road and the inherent long and thin property of lanes. Conventional methods typically treat lane detection as a semantic segmentation task, which assigns a class label to each pixel of the image. This formulation heavily depends on the assumption that the number of lanes is pre-defined and fixed and no lane changing occurs, which does not always hold. To make the lane detection model applicable to an arbitrary number of lanes and lane changing scenarios, we adopt an instance segmentation approach, which first differentiates lanes and background and then classify each lane pixel into each lane instance. Besides, a multi-task learning paradigm is utilized to better exploit the structural information and the feature pyramid architecture is used to detect extremely thin lanes. Three popular lane detection benchmarks, i.e., TuSimple, CULane and BDD100K, are used to validate the effectiveness of our proposed algorithm.

</details>

<details>

<summary>2019-05-02 06:48:24 - Dynamic Mini-batch SGD for Elastic Distributed Training: Learning in the Limbo of Resources</summary>

- *Haibin Lin, Hang Zhang, Yifei Ma, Tong He, Zhi Zhang, Sheng Zha, Mu Li*

- `1904.12043v2` - [abs](http://arxiv.org/abs/1904.12043v2) - [pdf](http://arxiv.org/pdf/1904.12043v2)

> With an increasing demand for training powers for deep learning algorithms and the rapid growth of computation resources in data centers, it is desirable to dynamically schedule different distributed deep learning tasks to maximize resource utilization and reduce cost. In this process, different tasks may receive varying numbers of machines at different time, a setting we call elastic distributed training. Despite the recent successes in large mini-batch distributed training, these methods are rarely tested in elastic distributed training environments and suffer degraded performance in our experiments, when we adjust the learning rate linearly immediately with respect to the batch size. One difficulty we observe is that the noise in the stochastic momentum estimation is accumulated over time and will have delayed effects when the batch size changes. We therefore propose to smoothly adjust the learning rate over time to alleviate the influence of the noisy momentum estimation. Our experiments on image classification, object detection and semantic segmentation have demonstrated that our proposed Dynamic SGD method achieves stabilized performance when varying the number of GPUs from 8 to 128. We also provide theoretical understanding on the optimality of linear learning rate scheduling and the effects of stochastic momentum.

</details>

<details>

<summary>2019-05-02 09:14:37 - InternalBlue - Bluetooth Binary Patching and Experimentation Framework</summary>

- *Dennis Mantz, Jiska Classen, Matthias Schulz, Matthias Hollick*

- `1905.00631v1` - [abs](http://arxiv.org/abs/1905.00631v1) - [pdf](http://arxiv.org/pdf/1905.00631v1)

> Bluetooth is one of the most established technologies for short range digital wireless data transmission. With the advent of wearables and the Internet of Things (IoT), Bluetooth has again gained importance, which makes security research and protocol optimizations imperative. Surprisingly, there is a lack of openly available tools and experimental platforms to scrutinize Bluetooth. In particular, system aspects and close to hardware protocol layers are mostly uncovered.   We reverse engineer multiple Broadcom Bluetooth chipsets that are widespread in off-the-shelf devices. Thus, we offer deep insights into the internal architecture of a popular commercial family of Bluetooth controllers used in smartphones, wearables, and IoT platforms. Reverse engineered functions can then be altered with our InternalBlue Python framework---outperforming evaluation kits, which are limited to documented and vendor-defined functions. The modified Bluetooth stack remains fully functional and high-performance. Hence, it provides a portable low-cost research platform.   InternalBlue is a versatile framework and we demonstrate its abilities by implementing tests and demos for known Bluetooth vulnerabilities. Moreover, we discover a novel critical security issue affecting a large selection of Broadcom chipsets that allows executing code within the attacked Bluetooth firmware. We further show how to use our framework to fix bugs in chipsets out of vendor support and how to add new security features to Bluetooth firmware.

</details>

<details>

<summary>2019-05-02 17:19:08 - Text Embeddings for Retrieval From a Large Knowledge Base</summary>

- *Tolgahan Cakaloglu, Christian Szegedy, Xiaowei Xu*

- `1810.10176v2` - [abs](http://arxiv.org/abs/1810.10176v2) - [pdf](http://arxiv.org/pdf/1810.10176v2)

> Text embedding representing natural language documents in a semantic vector space can be used for document retrieval using nearest neighbor lookup. In order to study the feasibility of neural models specialized for retrieval in a semantically meaningful way, we suggest the use of the Stanford Question Answering Dataset (SQuAD) in an open-domain question answering context, where the first task is to find paragraphs useful for answering a given question. First, we compare the quality of various text-embedding methods on the performance of retrieval and give an extensive empirical comparison on the performance of various non-augmented base embedding with, and without IDF weighting. Our main results are that by training deep residual neural models, specifically for retrieval purposes, can yield significant gains when it is used to augment existing embeddings. We also establish that deeper models are superior to this task. The best base baseline embeddings augmented by our learned neural approach improves the top-1 paragraph recall of the system by 14%.

</details>

<details>

<summary>2019-05-02 20:47:26 - Learning Programmatically Structured Representations with Perceptor Gradients</summary>

- *Svetlin Penkov, Subramanian Ramamoorthy*

- `1905.00956v1` - [abs](http://arxiv.org/abs/1905.00956v1) - [pdf](http://arxiv.org/pdf/1905.00956v1)

> We present the perceptor gradients algorithm -- a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification.

</details>

<details>

<summary>2019-05-02 23:41:47 - Neural Modular Control for Embodied Question Answering</summary>

- *Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra*

- `1810.11181v2` - [abs](http://arxiv.org/abs/1810.11181v2) - [pdf](http://arxiv.org/pdf/1810.11181v2)

> We present a modular approach for learning policies for navigation over long planning horizons from language input. Our hierarchical policy operates at multiple timescales, where the higher-level master policy proposes subgoals to be executed by specialized sub-policies. Our choice of subgoals is compositional and semantic, i.e. they can be sequentially combined in arbitrary orderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find kitchen', 'find refrigerator', etc.).   We use imitation learning to warm-start policies at each level of the hierarchy, dramatically increasing sample efficiency, followed by reinforcement learning. Independent reinforcement learning at each level of hierarchy enables sub-policies to adapt to consequences of their actions and recover from errors. Subsequent joint hierarchical training enables the master policy to adapt to the sub-policies.   On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al., 2018), requiring navigating diverse realistic indoor environments, our approach outperforms prior work by a significant margin, both in terms of navigation and question answering.

</details>

<details>

<summary>2019-05-03 02:34:09 - Efficient Discrete Supervised Hashing for Large-scale Cross-modal Retrieval</summary>

- *Tao Yao, Xiangwei Kong, Lianshan Yan, Wenjing Tang, Qi Tian*

- `1905.01304v1` - [abs](http://arxiv.org/abs/1905.01304v1) - [pdf](http://arxiv.org/pdf/1905.01304v1)

> Supervised cross-modal hashing has gained increasing research interest on large-scale retrieval task owning to its satisfactory performance and efficiency. However, it still has some challenging issues to be further studied: 1) most of them fail to well preserve the semantic correlations in hash codes because of the large heterogenous gap; 2) most of them relax the discrete constraint on hash codes, leading to large quantization error and consequent low performance; 3) most of them suffer from relatively high memory cost and computational complexity during training procedure, which makes them unscalable. In this paper, to address above issues, we propose a supervised cross-modal hashing method based on matrix factorization dubbed Efficient Discrete Supervised Hashing (EDSH). Specifically, collective matrix factorization on heterogenous features and semantic embedding with class labels are seamlessly integrated to learn hash codes. Therefore, the feature based similarities and semantic correlations can be both preserved in hash codes, which makes the learned hash codes more discriminative. Then an efficient discrete optimal algorithm is proposed to handle the scalable issue. Instead of learning hash codes bit-by-bit, hash codes matrix can be obtained directly which is more efficient. Extensive experimental results on three public real-world datasets demonstrate that EDSH produces a superior performance in both accuracy and scalability over some existing cross-modal hashing methods.

</details>

<details>

<summary>2019-05-03 07:52:32 - Semantic Segmentation of Video Sequences with Convolutional LSTMs</summary>

- *Andreas Pfeuffer, Karina Schulz, Klaus Dietmayer*

- `1905.01058v1` - [abs](http://arxiv.org/abs/1905.01058v1) - [pdf](http://arxiv.org/pdf/1905.01058v1)

> Most of the semantic segmentation approaches have been developed for single image segmentation, and hence, video sequences are currently segmented by processing each frame of the video sequence separately. The disadvantage of this is that temporal image information is not considered, which improves the performance of the segmentation approach. One possibility to include temporal information is to use recurrent neural networks. However, there are only a few approaches using recurrent networks for video segmentation so far. These approaches extend the encoder-decoder network architecture of well-known segmentation approaches and place convolutional LSTM layers between encoder and decoder. However, in this paper it is shown that this position is not optimal, and that other positions in the network exhibit better performance. Nowadays, state-of-the-art segmentation approaches rarely use the classical encoder-decoder structure, but use multi-branch architectures. These architectures are more complex, and hence, it is more difficult to place the recurrent units at a proper position. In this work, the multi-branch architectures are extended by convolutional LSTM layers at different positions and evaluated on two different datasets in order to find the best one. It turned out that the proposed approach outperforms the pure CNN-based approach for up to 1.6 percent.

</details>

<details>

<summary>2019-05-03 08:05:44 - Multi-Focus Image Fusion Using Sparse Representation and Coupled Dictionary Learning</summary>

- *Farshad G. Veshki, Sergiy A. Vorobyov*

- `1705.10574v3` - [abs](http://arxiv.org/abs/1705.10574v3) - [pdf](http://arxiv.org/pdf/1705.10574v3)

> We address the multi-focus image fusion problem, where multiple images captured with different focal settings are to be fused into an all-in-focus image of higher quality. Algorithms for this problem necessarily admit the source image characteristics along with focused and blurred features. However, most sparsity-based approaches use a single dictionary in focused feature space to describe multi-focus images, and ignore the representations in blurred feature space. We propose a multi-focus image fusion approach based on sparse representation using a coupled dictionary. It exploits the observations that the patches from a given training set can be sparsely represented by a couple of overcomplete dictionaries related to the focused and blurred categories of images and that a sparse approximation based on such coupled dictionary leads to a more flexible and therefore better fusion strategy than the one based on just selecting the sparsest representation in the original image estimate. In addition, to improve the fusion performance, we employ a coupled dictionary learning approach that enforces pairwise correlation between atoms of dictionaries learned to represent the focused and blurred feature spaces. We also discuss the advantages of the fusion approach based on coupled dictionary learning, and present efficient algorithms for fusion based on coupled dictionary learning. Extensive experimental comparisons with state-of-the-art multi-focus image fusion algorithms validate the effectiveness of the proposed approach.

</details>

<details>

<summary>2019-05-03 09:06:57 - Measuring Semantic Abstraction of Multilingual NMT with Paraphrase Recognition and Generation Tasks</summary>

- *Jörg Tiedemann, Yves Scherrer*

- `1808.06826v2` - [abs](http://arxiv.org/abs/1808.06826v2) - [pdf](http://arxiv.org/pdf/1808.06826v2)

> In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. We test this hypotheses by measuring the perplexity of such models when applied to paraphrases of the source language. The intuition is that an encoder produces better representations if a decoder is capable of recognizing synonymous sentences in the same language even though the model is never trained for that task. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English. The results show that the perplexity is significantly reduced in each of the cases, indicating that meaning can be grounded in translation. This is further supported by a study on paraphrase generation that we also include at the end of the paper.

</details>

<details>

<summary>2019-05-03 10:09:09 - Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs</summary>

- *Daniel Oñoro-Rubio, Mathias Niepert, Alberto García-Durán, Roberto González, Roberto J. López-Sastre*

- `1709.02314v6` - [abs](http://arxiv.org/abs/1709.02314v6) - [pdf](http://arxiv.org/pdf/1709.02314v6)

> A visual-relational knowledge graph (KG) is a multi-relational graph whose entities are associated with images. We explore novel machine learning approaches for answering visual-relational queries in web-extracted knowledge graphs. To this end, we have created ImageGraph, a KG with 1,330 relation types, 14,870 entities, and 829,931 images crawled from the web. With visual-relational KGs such as ImageGraph one can introduce novel probabilistic query types in which images are treated as first-class citizens. Both the prediction of relations between unseen images as well as multi-relational image retrieval can be expressed with specific families of visual-relational queries. We introduce novel combinations of convolutional networks and knowledge graph embedding methods to answer such queries. We also explore a zero-shot learning scenario where an image of an entirely new entity is linked with multiple relations to entities of an existing KG. The resulting multi-relational grounding of unseen entity images into a knowledge graph serves as a semantic entity representation. We conduct experiments to demonstrate that the proposed methods can answer these visual-relational queries efficiently and accurately.

</details>

<details>

<summary>2019-05-05 10:30:03 - A Typedriven Vector Semantics for Ellipsis with Anaphora using Lambek Calculus with Limited Contraction</summary>

- *Gijs Wijnholds, Mehrnoosh Sadrzadeh*

- `1905.01647v1` - [abs](http://arxiv.org/abs/1905.01647v1) - [pdf](http://arxiv.org/pdf/1905.01647v1)

> We develop a vector space semantics for verb phrase ellipsis with anaphora using type-driven compositional distributional semantics based on the Lambek calculus with limited contraction (LCC) of J\"ager (2006). Distributional semantics has a lot to say about the statistical collocation-based meanings of content words, but provides little guidance on how to treat function words. Formal semantics on the other hand, has powerful mechanisms for dealing with relative pronouns, coordinators, and the like. Type-driven compositional distributional semantics brings these two models together. We review previous compositional distributional models of relative pronouns, coordination and a restricted account of ellipsis in the DisCoCat framework of Coecke et al. (2010, 2013). We show how DisCoCat cannot deal with general forms of ellipsis, which rely on copying of information, and develop a novel way of connecting typelogical grammar to distributional semantics by assigning vector interpretable lambda terms to derivations of LCC in the style of Muskens & Sadrzadeh (2016). What follows is an account of (verb phrase) ellipsis in which word meanings can be copied: the meaning of a sentence is now a program with non-linear access to individual word embeddings. We present the theoretical setting, work out examples, and demonstrate our results on a toy distributional model motivated by data.

</details>

<details>

<summary>2019-05-05 19:57:32 - HHMM at SemEval-2019 Task 2: Unsupervised Frame Induction using Contextualized Word Embeddings</summary>

- *Saba Anwar, Dmitry Ustalov, Nikolay Arefyev, Simone Paolo Ponzetto, Chris Biemann, Alexander Panchenko*

- `1905.01739v1` - [abs](http://arxiv.org/abs/1905.01739v1) - [pdf](http://arxiv.org/pdf/1905.01739v1)

> We present our system for semantic frame induction that showed the best performance in Subtask B.1 and finished as the runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages.

</details>

<details>

<summary>2019-05-06 15:12:03 - Dialogue Act Classification with Context-Aware Self-Attention</summary>

- *Vipul Raheja, Joel Tetreault*

- `1904.02594v2` - [abs](http://arxiv.org/abs/1904.02594v2) - [pdf](http://arxiv.org/pdf/1904.02594v2)

> Recent work in Dialogue Act classification has treated the task as a sequence labeling problem using hierarchical deep neural networks. We build on this prior work by leveraging the effectiveness of a context-aware self-attention mechanism coupled with a hierarchical recurrent neural network. We conduct extensive evaluations on standard Dialogue Act classification datasets and show significant improvement over state-of-the-art results on the Switchboard Dialogue Act (SwDA) Corpus. We also investigate the impact of different utterance-level representation learning methods and show that our method is effective at capturing utterance-level semantic text representations while maintaining high accuracy.

</details>

<details>

<summary>2019-05-07 09:23:12 - Interactive Search and Exploration in Online Discussion Forums Using Multimodal Embeddings</summary>

- *Iva Gornishka, Stevan Rudinac, Marcel Worring*

- `1905.02430v1` - [abs](http://arxiv.org/abs/1905.02430v1) - [pdf](http://arxiv.org/pdf/1905.02430v1)

> In this paper we present a novel interactive multimodal learning system, which facilitates search and exploration in large networks of social multimedia users. It allows the analyst to identify and select users of interest, and to find similar users in an interactive learning setting. Our approach is based on novel multimodal representations of users, words and concepts, which we simultaneously learn by deploying a general-purpose neural embedding model. We show these representations to be useful not only for categorizing users, but also for automatically generating user and community profiles. Inspired by traditional summarization approaches, we create the profiles by selecting diverse and representative content from all available modalities, i.e. the text, image and user modality. The usefulness of the approach is evaluated using artificial actors, which simulate user behavior in a relevance feedback scenario. Multiple experiments were conducted in order to evaluate the quality of our multimodal representations, to compare different embedding strategies, and to determine the importance of different modalities. We demonstrate the capabilities of the proposed approach on two different multimedia collections originating from the violent online extremism forum Stormfront and the microblogging platform Twitter, which are particularly interesting due to the high semantic level of the discussions they feature.

</details>

<details>

<summary>2019-05-07 14:09:08 - Explainable Software Bot Contributions: Case Study of Automated Bug Fixes</summary>

- *Martin Monperrus*

- `1905.02597v1` - [abs](http://arxiv.org/abs/1905.02597v1) - [pdf](http://arxiv.org/pdf/1905.02597v1)

> In a software project, esp. in open-source, a contribution is a valuable piece of work made to the project: writing code, reporting bugs, translating, improving documentation, creating graphics, etc. We are now at the beginning of an exciting era where software bots will make contributions that are of similar nature than those by humans. Dry contributions, with no explanation, are often ignored or rejected, because the contribution is not understandable per se, because they are not put into a larger context, because they are not grounded on idioms shared by the core community of developers. We have been operating a program repair bot called Repairnator for 2 years and noticed the problem of "dry patches": a patch that does not say which bug it fixes, or that does not explain the effects of the patch on the system. We envision program repair systems that produce an "explainable bug fix": an integrated package of at least 1) a patch, 2) its explanation in natural or controlled language, and 3) a highlight of the behavioral difference with examples. In this paper, we generalize and suggest that software bot contributions must explainable, that they must be put into the context of the global software development conversation.

</details>

<details>

<summary>2019-05-08 02:12:18 - Automatic Inference of Minimalist Grammars using an SMT-Solver</summary>

- *Sagar Indurkhya*

- `1905.02869v1` - [abs](http://arxiv.org/abs/1905.02869v1) - [pdf](http://arxiv.org/pdf/1905.02869v1)

> We introduce (1) a novel parser for Minimalist Grammars (MG), encoded as a system of first-order logic formulae that may be evaluated using an SMT-solver, and (2) a novel procedure for inferring Minimalist Grammars using this parser. The input to this procedure is a sequence of sentences that have been annotated with syntactic relations such as semantic role labels (connecting arguments to predicates) and subject-verb agreement. The output of this procedure is a set of minimalist grammars, each of which is able to parse the sentences in the input sequence such that the parse for a sentence has the same syntactic relations as those specified in the annotation for that sentence. We applied this procedure to a set of sentences annotated with syntactic relations and evaluated the inferred grammars using cost functions inspired by the Minimum Description Length principle and the Subset principle. Inferred grammars that were optimal with respect to certain combinations of these cost functions were found to align with contemporary theories of syntax.

</details>

<details>

<summary>2019-05-08 09:24:48 - On the Feasibility of Automated Detection of Allusive Text Reuse</summary>

- *Enrique Manjavacas, Brian Long, Mike Kestemont*

- `1905.02973v1` - [abs](http://arxiv.org/abs/1905.02973v1) - [pdf](http://arxiv.org/pdf/1905.02973v1)

> The detection of allusive text reuse is particularly challenging due to the sparse evidence on which allusive references rely---commonly based on none or very few shared words. Arguably, lexical semantics can be resorted to since uncovering semantic relations between words has the potential to increase the support underlying the allusion and alleviate the lexical sparsity. A further obstacle is the lack of evaluation benchmark corpora, largely due to the highly interpretative character of the annotation process. In the present paper, we aim to elucidate the feasibility of automated allusion detection. We approach the matter from an Information Retrieval perspective in which referencing texts act as queries and referenced texts as relevant documents to be retrieved, and estimate the difficulty of benchmark corpus compilation by a novel inter-annotator agreement study on query segmentation. Furthermore, we investigate to what extent the integration of lexical semantic information derived from distributional models and ontologies can aid retrieving cases of allusive reuse. The results show that (i) despite low agreement scores, using manual queries considerably improves retrieval performance with respect to a windowing approach, and that (ii) retrieval performance can be moderately boosted with distributional semantics.

</details>

<details>

<summary>2019-05-08 20:48:28 - Learning Embeddings into Entropic Wasserstein Spaces</summary>

- *Charlie Frogner, Farzaneh Mirzazadeh, Justin Solomon*

- `1905.03329v1` - [abs](http://arxiv.org/abs/1905.03329v1) - [pdf](http://arxiv.org/pdf/1905.03329v1)

> Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions. Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric. Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures. We exploit this flexibility by learning an embedding that captures semantic information in the Wasserstein distance between embedded distributions. We examine empirically the representational capacity of our learned Wasserstein embeddings, showing that they can embed a wide variety of metric structures with smaller distortion than an equivalent Euclidean embedding. We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: We can visualize the high-dimensional embedding directly, since it is a probability distribution on a low-dimensional space. This obviates the need for dimensionality reduction techniques like t-SNE for visualization.

</details>

<details>

<summary>2019-05-08 22:08:28 - Xu: An Automated Query Expansion and Optimization Tool</summary>

- *Morgan Gallant, Haruna Isah, Farhana Zulkernine, Shahzad Khan*

- `1808.09353v2` - [abs](http://arxiv.org/abs/1808.09353v2) - [pdf](http://arxiv.org/pdf/1808.09353v2)

> The exponential growth of information on the Internet is a big challenge for information retrieval systems towards generating relevant results. Novel approaches are required to reformat or expand user queries to generate a satisfactory response and increase recall and precision. Query expansion (QE) is a technique to broaden users' queries by introducing additional tokens or phrases based on some semantic similarity metrics. The tradeoff is the added computational complexity to find semantically similar words and a possible increase in noise in information retrieval. Despite several research efforts on this topic, QE has not yet been explored enough and more work is needed on similarity matching and composition of query terms with an objective to retrieve a small set of most appropriate responses. QE should be scalable, fast, and robust in handling complex queries with a good response time and noise ceiling. In this paper, we propose Xu, an automated QE technique, using high dimensional clustering of word vectors and Datamuse API, an open source query engine to find semantically similar words. We implemented Xu as a command line tool and evaluated its performances using datasets containing news articles and human-generated QEs. The evaluation results show that Xu was better than Datamuse by achieving about 88% accuracy with reference to the human-generated QE.

</details>

<details>

<summary>2019-05-09 00:40:17 - Learning Self-Game-Play Agents for Combinatorial Optimization Problems</summary>

- *Ruiyang Xu, Karl Lieberherr*

- `1903.03674v2` - [abs](http://arxiv.org/abs/1903.03674v2) - [pdf](http://arxiv.org/pdf/1903.03674v2)

> Recent progress in reinforcement learning (RL) using self-game-play has shown remarkable performance on several board games (e.g., Chess and Go) as well as video games (e.g., Atari games and Dota2). It is plausible to consider that RL, starting from zero knowledge, might be able to gradually approximate a winning strategy after a certain amount of training. In this paper, we explore neural Monte-Carlo-Tree-Search (neural MCTS), an RL algorithm which has been applied successfully by DeepMind to play Go and Chess at a super-human level. We try to leverage the computational power of neural MCTS to solve a class of combinatorial optimization problems. Following the idea of Hintikka's Game-Theoretical Semantics, we propose the Zermelo Gamification (ZG) to transform specific combinatorial optimization problems into Zermelo games whose winning strategies correspond to the solutions of the original optimization problem. The ZG also provides a specially designed neural MCTS. We use a combinatorial planning problem for which the ground-truth policy is efficiently computable to demonstrate that ZG is promising.

</details>

<details>

<summary>2019-05-09 04:40:40 - Adversarial Defense Through Network Profiling Based Path Extraction</summary>

- *Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo, Yuhao Zhu*

- `1904.08089v2` - [abs](http://arxiv.org/abs/1904.08089v2) - [pdf](http://arxiv.org/pdf/1904.08089v2)

> Recently, researchers have started decomposing deep neural network models according to their semantics or functions. Recent work has shown the effectiveness of decomposed functional blocks for defending adversarial attacks, which add small input perturbation to the input image to fool the DNN models. This work proposes a profiling-based method to decompose the DNN models to different functional blocks, which lead to the effective path as a new approach to exploring DNNs' internal organization. Specifically, the per-image effective path can be aggregated to the class-level effective path, through which we observe that adversarial images activate effective path different from normal images. We propose an effective path similarity-based method to detect adversarial images with an interpretable model, which achieve better accuracy and broader applicability than the state-of-the-art technique.

</details>

<details>

<summary>2019-05-09 06:38:12 - Bidirectional RNN-based Few-shot Training for Detecting Multi-stage Attack</summary>

- *Di Zhao, Jiqiang Liu, Jialin Wang, Wenjia Niu, Endong Tong, Tong Chen, Gang Li*

- `1905.03454v1` - [abs](http://arxiv.org/abs/1905.03454v1) - [pdf](http://arxiv.org/pdf/1905.03454v1)

> "Feint Attack", as a new type of APT attack, has become the focus of attention. It adopts a multi-stage attacks mode which can be concluded as a combination of virtual attacks and real attacks. Under the cover of virtual attacks, real attacks can achieve the real purpose of the attacker, as a result, it often caused huge losses inadvertently. However, to our knowledge, all previous works use common methods such as Causal-Correlation or Cased-based to detect outdated multi-stage attacks. Few attentions have been paid to detect the "Feint Attack", because the difficulty of detection lies in the diversification of the concept of "Feint Attack" and the lack of professional datasets, many detection methods ignore the semantic relationship in the attack. Aiming at the existing challenge, this paper explores a new method to solve the problem. In the attack scenario, the fuzzy clustering method based on attribute similarity is used to mine multi-stage attack chains. Then we use a few-shot deep learning algorithm (SMOTE&CNN-SVM) and bidirectional Recurrent Neural Network model (Bi-RNN) to obtain the "Feint Attack" chains. "Feint Attack" is simulated by the real attack inserted in the normal causal attack chain, and the addition of the real attack destroys the causal relationship of the original attack chain. So we used Bi-RNN coding to obtain the hidden feature of "Feint Attack" chain. In the end, our method achieved the goal to detect the "Feint Attack" accurately by using the LLDoS1.0 and LLDoS2.0 of DARPA2000 and CICIDS2017 of Canadian Institute for Cybersecurity.

</details>

<details>

<summary>2019-05-09 10:28:57 - Semantic Search using Spreading Activation based on Ontology</summary>

- *Ngo Minh Vuong*

- `1905.06114v1` - [abs](http://arxiv.org/abs/1905.06114v1) - [pdf](http://arxiv.org/pdf/1905.06114v1)

> Currently, the text document retrieval systems have many challenges in exploring the semantics of queries and documents. Each query implies information which does not appear in the query but the documents related with the information are also expected by user. The disadvantage of the previous spreading activation algorithms could be many irrelevant concepts added to the query. In this paper, a proposed novel algorithm is only activate and add to the query named entities which are related with original entities in the query and explicit relations in the query.

</details>

<details>

<summary>2019-05-09 12:50:19 - Extensions to Justification Theory</summary>

- *Simon Marynissen*

- `1905.06184v1` - [abs](http://arxiv.org/abs/1905.06184v1) - [pdf](http://arxiv.org/pdf/1905.06184v1)

> Justification theory is a unifying framework for semantics of non-monotonic logics. It is built on the notion of a justification, which intuitively is a graph that explains the truth value of certain facts in a structure. Knowledge representation languages covered by justification theory include logic programs, argumentation frameworks, inductive definitions, and nested inductive and coinductive definitions. In addition, justifications are also used for implementation purposes. They are used to compute unfounded sets in modern ASP solvers, can be used to check for relevance of atoms in complete search algorithms, and recent lazy grounding algorithms are built on top of them. In this extended abstract, we lay out possible extensions to justification theory.

</details>

<details>

<summary>2019-05-10 11:18:08 - Disease Identification From Unstructured User Input</summary>

- *Fahim Faisal, Shafkat Ahmed Bhuiyan, Abu Raihan Mostofa Kamal*

- `1905.01987v2` - [abs](http://arxiv.org/abs/1905.01987v2) - [pdf](http://arxiv.org/pdf/1905.01987v2)

> A method to identify probable diseases from the unstructured textual input (eg, health forum posts) by incorporating a lexicographic and semantic feature based two-phase text classification module and a symptom-disease correlation-based similarity measurement module. One notable aspect of my approach was to develop a competent algorithm to extract all inherent features from the data source to make a better decision.

</details>

<details>

<summary>2019-05-10 12:16:10 - A New Anchor Word Selection Method for the Separable Topic Discovery</summary>

- *Kun He, Wu Wang, Xiaosen Wang, John E. Hopcroft*

- `1905.06109v1` - [abs](http://arxiv.org/abs/1905.06109v1) - [pdf](http://arxiv.org/pdf/1905.06109v1)

> Separable Non-negative Matrix Factorization (SNMF) is an important method for topic modeling, where "separable" assumes every topic contains at least one anchor word, defined as a word that has non-zero probability only on that topic. SNMF focuses on the word co-occurrence patterns to reveal topics by two steps: anchor word selection and topic recovery. The quality of the anchor words strongly influences the quality of the extracted topics. Existing anchor word selection algorithm is to greedily find an approximate convex hull in a high-dimensional word co-occurrence space. In this work, we propose a new method for the anchor word selection by associating the word co-occurrence probability with the words similarity and assuming that the most different words on semantic are potential candidates for the anchor words. Therefore, if the similarity of a word-pair is very low, then the two words are very likely to be the anchor words. According to the statistical information of text corpora, we can get the similarity of all word-pairs. We build the word similarity graph where the nodes correspond to words and weights on edges stand for the word-pair similarity. Following this way, we design a greedy method to find a minimum edge-weight anchor clique of a given size in the graph for the anchor word selection. Extensive experiments on real-world corpus demonstrate the effectiveness of the proposed anchor word selection method that outperforms the common convex hull-based methods on the revealed topic quality. Meanwhile, our method is much faster than typical SNMF based method.

</details>

<details>

<summary>2019-05-10 15:42:35 - Semantic Segmentation of Seismic Images</summary>

- *Daniel Civitarese, Daniela Szwarcman, Emilio Vital Brazil, Bianca Zadrozny*

- `1905.04307v1` - [abs](http://arxiv.org/abs/1905.04307v1) - [pdf](http://arxiv.org/pdf/1905.04307v1)

> Almost all work to understand Earth's subsurface on a large scale relies on the interpretation of seismic surveys by experts who segment the survey (usually a cube) into layers; a process that is very time demanding. In this paper, we present a new deep neural network architecture specially designed to semantically segment seismic images with a minimal amount of training data. To achieve this, we make use of a transposed residual unit that replaces the traditional dilated convolution for the decode block. Also, instead of using a predefined shape for up-scaling, our network learns all the steps to upscale the features from the encoder. We train our neural network using the Penobscot 3D dataset; a real seismic dataset acquired offshore Nova Scotia, Canada. We compare our approach with two well-known deep neural network topologies: Fully Convolutional Network and U-Net. In our experiments, we show that our approach can achieve more than 99 percent of the mean intersection over union (mIOU) metric, outperforming the existing topologies. Moreover, our qualitative results show that the obtained model can produce masks very close to human interpretation with very little discontinuity.

</details>

<details>

<summary>2019-05-10 22:24:55 - GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering</summary>

- *Drew A. Hudson, Christopher D. Manning*

- `1902.09506v3` - [abs](http://arxiv.org/abs/1902.09506v3) - [pdf](http://arxiv.org/pdf/1902.09506v3)

> We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages scene graph structures to create 22M diverse reasoning questions, all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. An extensive analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We strongly hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding for images and language.

</details>

<details>

<summary>2019-05-11 02:02:55 - Controlled Natural Languages and Default Reasoning</summary>

- *Tiantian Gao*

- `1905.04422v1` - [abs](http://arxiv.org/abs/1905.04422v1) - [pdf](http://arxiv.org/pdf/1905.04422v1)

> Controlled natural languages (CNLs) are effective languages for knowledge representation and reasoning. They are designed based on certain natural languages with restricted lexicon and grammar. CNLs are unambiguous and simple as opposed to their base languages. They preserve the expressiveness and coherence of natural languages. In this report, we focus on a class of CNLs, called machine-oriented CNLs, which have well-defined semantics that can be deterministically translated into formal languages, such as Prolog, to do logical reasoning. Over the past 20 years, a number of machine-oriented CNLs emerged and have been used in many application domains for problem solving and question answering. However, few of them support non-monotonic inference. In our work, we propose non-monotonic extensions of CNL to support defeasible reasoning.   In the first part of this report, we survey CNLs and compare three influential systems: Attempto Controlled English (ACE), Processable English (PENG), and Computer-processable English (CPL). We compare their language design, semantic interpretations, and reasoning services. In the second part of this report, we first identify typical non-monotonicity in natural languages, such as defaults, exceptions and conversational implicatures. Then, we propose their representation in CNL and the corresponding formalizations in a form of defeasible reasoning known as Logic Programming with Defaults and Argumentation Theory (LPDA).

</details>

<details>

<summary>2019-05-11 05:13:06 - Ranking-based Deep Cross-modal Hashing</summary>

- *Xuanwu Liu, Guoxian Yu, Carlotta Domeniconi, Jun Wang, Yazhou Ren, Maozu Guo*

- `1905.04450v1` - [abs](http://arxiv.org/abs/1905.04450v1) - [pdf](http://arxiv.org/pdf/1905.04450v1)

> Cross-modal hashing has been receiving increasing interests for its low storage cost and fast query speed in multi-modal data retrievals. However, most existing hashing methods are based on hand-crafted or raw level features of objects, which may not be optimally compatible with the coding process. Besides, these hashing methods are mainly designed to handle simple pairwise similarity. The complex multilevel ranking semantic structure of instances associated with multiple labels has not been well explored yet. In this paper, we propose a ranking-based deep cross-modal hashing approach (RDCMH). RDCMH firstly uses the feature and label information of data to derive a semi-supervised semantic ranking list. Next, to expand the semantic representation power of hand-crafted features, RDCMH integrates the semantic ranking information into deep cross-modal hashing and jointly optimizes the compatible parameters of deep feature representations and of hashing functions. Experiments on real multi-modal datasets show that RDCMH outperforms other competitive baselines and achieves the state-of-the-art performance in cross-modal retrieval applications.

</details>

<details>

<summary>2019-05-11 16:50:55 - Manifold Mixup: Better Representations by Interpolating Hidden States</summary>

- *Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio*

- `1806.05236v7` - [abs](http://arxiv.org/abs/1806.05236v7) - [pdf](http://arxiv.org/pdf/1806.05236v7)

> Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.

</details>

<details>

<summary>2019-05-11 17:50:12 - Semantic categories of artifacts and animals reflect efficient coding</summary>

- *Noga Zaslavsky, Terry Regier, Naftali Tishby, Charles Kemp*

- `1905.04562v1` - [abs](http://arxiv.org/abs/1905.04562v1) - [pdf](http://arxiv.org/pdf/1905.04562v1)

> It has been argued that semantic categories across languages reflect pressure for efficient communication. Recently, this idea has been cast in terms of a general information-theoretic principle of efficiency, the Information Bottleneck (IB) principle, and it has been shown that this principle accounts for the emergence and evolution of named color categories across languages, including soft structure and patterns of inconsistent naming. However, it is not yet clear to what extent this account generalizes to semantic domains other than color. Here we show that it generalizes to two qualitatively different semantic domains: names for containers, and for animals. First, we show that container naming in Dutch and French is near-optimal in the IB sense, and that IB broadly accounts for soft categories and inconsistent naming patterns in both languages. Second, we show that a hierarchy of animal categories derived from IB captures cross-linguistic tendencies in the growth of animal taxonomies. Taken together, these findings suggest that fundamental information-theoretic principles of efficient coding may shape semantic categories across languages and across domains.

</details>

<details>

<summary>2019-05-11 20:47:30 - Language in Our Time: An Empirical Analysis of Hashtags</summary>

- *Yang Zhang*

- `1905.04590v1` - [abs](http://arxiv.org/abs/1905.04590v1) - [pdf](http://arxiv.org/pdf/1905.04590v1)

> Hashtags in online social networks have gained tremendous popularity during the past five years. The resulting large quantity of data has provided a new lens into modern society. Previously, researchers mainly rely on data collected from Twitter to study either a certain type of hashtags or a certain property of hashtags. In this paper, we perform the first large-scale empirical analysis of hashtags shared on Instagram, the major platform for hashtag-sharing. We study hashtags from three different dimensions including the temporal-spatial dimension, the semantic dimension, and the social dimension. Extensive experiments performed on three large-scale datasets with more than 7 million hashtags in total provide a series of interesting observations. First, we show that the temporal patterns of hashtags can be categorized into four different clusters, and people tend to share fewer hashtags at certain places and more hashtags at others. Second, we observe that a non-negligible proportion of hashtags exhibit large semantic displacement. We demonstrate hashtags that are more uniformly shared among users, as quantified by the proposed hashtag entropy, are less prone to semantic displacement. In the end, we propose a bipartite graph embedding model to summarize users' hashtag profiles, and rely on these profiles to perform friendship prediction. Evaluation results show that our approach achieves an effective prediction with AUC (area under the ROC curve) above 0.8 which demonstrates the strong social signals possessed in hashtags.

</details>

<details>

<summary>2019-05-12 10:42:20 - Rough Contact in General Rough Mereology</summary>

- *A. Mani*

- `1905.04689v1` - [abs](http://arxiv.org/abs/1905.04689v1) - [pdf](http://arxiv.org/pdf/1905.04689v1)

> Theories of rough mereology have originated from diverse semantic considerations from contexts relating to study of databases, to human reasoning. These ideas of origin, especially in the latter context, are intensely complex. In this research, concepts of rough contact relations are introduced and rough mereologies are situated in relation to general spatial mereology by the present author. These considerations are restricted to her rough mereologies that seek to avoid contamination.

</details>

<details>

<summary>2019-05-12 14:29:57 - One-Shot Image-to-Image Translation via Part-Global Learning with a Multi-adversarial Framework</summary>

- *Ziqiang Zheng, Zhibin Yu, Haiyong Zheng, Yang Yang, Heng Tao Shen*

- `1905.04729v1` - [abs](http://arxiv.org/abs/1905.04729v1) - [pdf](http://arxiv.org/pdf/1905.04729v1)

> It is well known that humans can learn and recognize objects effectively from several limited image samples. However, learning from just a few images is still a tremendous challenge for existing main-stream deep neural networks. Inspired by analogical reasoning in the human mind, a feasible strategy is to translate the abundant images of a rich source domain to enrich the relevant yet different target domain with insufficient image data. To achieve this goal, we propose a novel, effective multi-adversarial framework (MA) based on part-global learning, which accomplishes one-shot cross-domain image-to-image translation. In specific, we first devise a part-global adversarial training scheme to provide an efficient way for feature extraction and prevent discriminators being over-fitted. Then, a multi-adversarial mechanism is employed to enhance the image-to-image translation ability to unearth the high-level semantic representation. Moreover, a balanced adversarial loss function is presented, which aims to balance the training data and stabilize the training process. Extensive experiments demonstrate that the proposed approach can obtain impressive results on various datasets between two extremely imbalanced image domains and outperform state-of-the-art methods on one-shot image-to-image translation.

</details>

<details>

<summary>2019-05-12 19:23:56 - AFSCR: Annotation of Functional Satisfaction Conditions and their Reconciliation within i* models</summary>

- *Novarun Deb, Nabendu Chaki*

- `1905.04777v1` - [abs](http://arxiv.org/abs/1905.04777v1) - [pdf](http://arxiv.org/pdf/1905.04777v1)

> Context: Researchers, both in industry and academia, are facing the challenge of leveraging the benefits of goal oriented requirements engineering (GORE) techniques to business compliance management. This requires analyzing goal models along with their semantics. However, most prominent goal modeling frameworks have no means of capturing the semantics of goals (except what is trivially conveyed by their nomenclature).   Objective: In this paper, we propose the Annotation of Functional Satisfaction Conditions and their Reconciliation (AFSCR) framework for doing the same. The entire framework is presented with respect to i* modeling constructs.   Method: This is a semi-automated framework that requires analysts to annotate individual goals with their immediate goal satisfaction conditions. The AFSCR framework can then reconcile these satisfaction conditions for every goal and verify whether the derived set of cumulative satisfaction conditions is in harmony with the intended set of goal satisfaction conditions.   Result: If the derived and intended sets of satisfaction conditions are in conflict, the framework raises entailment and/or consistency flags. Whenever a conflict is flagged, the framework also provides alternate solutions and possible workaround strategies to the analysts by refactoring the given i* model.   Conclusion: In this paper we present a new framework that uses satisfaction conditions for going beyond the nomenclature and capturing the functional semantics of the goals within i* models. The analysis performed during the reconciliation process is generic enough and can be adapted to any goal modeling framework if required.

</details>

<details>

<summary>2019-05-13 06:13:27 - Similarity Grouping-Guided Neural Network Modeling for Maritime Time Series Prediction</summary>

- *Yan Li, Ryan Wen Liu, Zhao Liu, Jingxian Liu*

- `1905.04872v1` - [abs](http://arxiv.org/abs/1905.04872v1) - [pdf](http://arxiv.org/pdf/1905.04872v1)

> Reliable and accurate prediction of time series plays a crucial role in maritime industry, such as economic investment, transportation planning, port planning and design, etc. The dynamic growth of maritime time series has the predominantly complex, nonlinear and non-stationary properties. To guarantee high-quality prediction performance, we propose to first adopt the empirical mode decomposition (EMD) and ensemble EMD (EEMD) methods to decompose the original time series into high- and low-frequency components. The low-frequency components can be easily predicted directly through traditional neural network (NN) methods. It is more difficult to predict high-frequency components due to their properties of weak mathematical regularity. To take advantage of the inherent self-similarities within high-frequency components, these components will be divided into several continuous small (overlapping) segments. The grouped segments with high similarities are then selected to form more proper training datasets for traditional NN methods. This regrouping strategy can assist in enhancing the prediction accuracy of high-frequency components. The final prediction result is obtained by integrating the predicted high- and low-frequency components. Our proposed three-step prediction frameworks benefit from the time series decomposition and similar segments grouping. Experiments on both port cargo throughput and vessel traffic flow have illustrated its superior performance in terms of prediction accuracy and robustness.

</details>

<details>

<summary>2019-05-13 08:53:31 - Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs</summary>

- *Lingbing Guo, Zequn Sun, Wei Hu*

- `1905.04914v1` - [abs](http://arxiv.org/abs/1905.04914v1) - [pdf](http://arxiv.org/pdf/1905.04914v1)

> We study the problem of knowledge graph (KG) embedding. A widely-established assumption to this problem is that similar entities are likely to have similar relational roles. However, existing related methods derive KG embeddings mainly based on triple-level learning, which lack the capability of capturing long-term relational dependencies of entities. Moreover, triple-level learning is insufficient for the propagation of semantic information among entities, especially for the case of cross-KG embedding. In this paper, we propose recurrent skipping networks (RSNs), which employ a skipping mechanism to bridge the gaps between entities. RSNs integrate recurrent neural networks (RNNs) with residual learning to efficiently capture the long-term relational dependencies within and between KGs. We design an end-to-end framework to support RSNs on different tasks. Our experimental results showed that RSNs outperformed state-of-the-art embedding-based methods for entity alignment and achieved competitive performance for KG completion.

</details>

<details>

<summary>2019-05-13 12:17:36 - Composite Event Recognition for Maritime Monitoring</summary>

- *Manolis Pitsikalis, Alexander Artikis, Richard Dreo, Cyril Ray, Elena Camossi, Anne-Laure Jousselme*

- `1903.03078v3` - [abs](http://arxiv.org/abs/1903.03078v3) - [pdf](http://arxiv.org/pdf/1903.03078v3)

> Maritime monitoring systems support safe shipping as they allow for the real-time detection of dangerous, suspicious and illegal vessel activities. We present such a system using the Run-Time Event Calculus, a composite event recognition system with formal, declarative semantics. For effective recognition, we developed a library of maritime patterns in close collaboration with domain experts. We present a thorough evaluation of the system and the patterns both in terms of predictive accuracy and computational efficiency, using real-world datasets of vessel position streams and contextual geographical information.

</details>

<details>

<summary>2019-05-13 13:26:17 - Joint Object and State Recognition using Language Knowledge</summary>

- *Ahmad Babaeian Jelodar, Yu Sun*

- `1905.08843v1` - [abs](http://arxiv.org/abs/1905.08843v1) - [pdf](http://arxiv.org/pdf/1905.08843v1)

> The state of an object is an important piece of knowledge in robotics applications. States and objects are intertwined together, meaning that object information can help recognize the state of an image and vice versa. This paper addresses the state identification problem in cooking related images and uses state and object predictions together to improve the classification accuracy of objects and their states from a single image. The pipeline presented in this paper includes a CNN with a double classification layer and the Concept-Net language knowledge graph on top. The language knowledge creates a semantic likelihood between objects and states. The resulting object and state confidences from the deep architecture are used together with object and state relatedness estimates from a language knowledge graph to produce marginal probabilities for objects and states. The marginal probabilities and confidences of objects (or states) are fused together to improve the final object (or state) classification results. Experiments on a dataset of cooking objects show that using a language knowledge graph on top of a deep neural network effectively enhances object and state classification.

</details>

<details>

<summary>2019-05-13 19:43:54 - Classifying Norm Conflicts using Learned Semantic Representations</summary>

- *João Paulo Aires, Roger Granada, Juarez Monteiro, Rodrigo C. Barros, Felipe Meneguzzi*

- `1906.02121v1` - [abs](http://arxiv.org/abs/1906.02121v1) - [pdf](http://arxiv.org/pdf/1906.02121v1)

> While most social norms are informal, they are often formalized by companies in contracts to regulate trades of goods and services. When poorly written, contracts may contain normative conflicts resulting from opposing deontic meanings or contradict specifications. As contracts tend to be long and contain many norms, manually identifying such conflicts requires human-effort, which is time-consuming and error-prone. Automating such task benefits contract makers increasing productivity and making conflict identification more reliable. To address this problem, we introduce an approach to detect and classify norm conflicts in contracts by converting them into latent representations that preserve both syntactic and semantic information and training a model to classify norm conflicts in four conflict types. Our results reach the new state of the art when compared to a previous approach.

</details>

<details>

<summary>2019-05-14 04:59:01 - HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding</summary>

- *Sheng Zhou, Jiajun Bu, Xin Wang, Jiawei Chen, Can Wang*

- `1902.01475v2` - [abs](http://arxiv.org/abs/1902.01475v2) - [pdf](http://arxiv.org/pdf/1902.01475v2)

> Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.

</details>

<details>

<summary>2019-05-14 09:34:25 - Image search using multilingual texts: a cross-modal learning approach between image and text</summary>

- *Maxime Portaz, Hicham Randrianarivo, Adrien Nivaggioli, Estelle Maudet, Christophe Servan, Sylvain Peyronnet*

- `1903.11299v3` - [abs](http://arxiv.org/abs/1903.11299v3) - [pdf](http://arxiv.org/pdf/1903.11299v3)

> Multilingual (or cross-lingual) embeddings represent several languages in a unique vector space. Using a common embedding space enables for a shared semantic between words from different languages. In this paper, we propose to embed images and texts into a unique distributional vector space, enabling to search images by using text queries expressing information needs related to the (visual) content of images, as well as using image similarity. Our framework forces the representation of an image to be similar to the representation of the text that describes it. Moreover, by using multilingual embeddings we ensure that words from two different languages have close descriptors and thus are attached to similar images. We provide experimental evidence of the efficiency of our approach by experimenting it on two datasets: Common Objects in COntext (COCO) [19] and Multi30K [7].

</details>

<details>

<summary>2019-05-14 12:07:13 - Assessing the Difficulty of Classifying ConceptNet Relations in a Multi-Label Classification Setting</summary>

- *Maria Becker, Michael Staniek, Vivi Nastase, Anette Frank*

- `1905.05538v1` - [abs](http://arxiv.org/abs/1905.05538v1) - [pdf](http://arxiv.org/pdf/1905.05538v1)

> Commonsense knowledge relations are crucial for advanced NLU tasks. We examine the learnability of such relations as represented in CONCEPTNET, taking into account their specific properties, which can make relation classification difficult: a given concept pair can be linked by multiple relation types, and relations can have multi-word arguments of diverse semantic types. We explore a neural open world multi-label classification approach that focuses on the evaluation of classification accuracy for individual relations. Based on an in-depth study of the specific properties of the CONCEPTNET resource, we investigate the impact of different relation representations and model variations. Our analysis reveals that the complexity of argument types and relation ambiguity are the most important challenges to address. We design a customized evaluation method to address the incompleteness of the resource that can be expanded in future work.

</details>

<details>

<summary>2019-05-14 16:42:33 - Timeline-based Planning and Execution with Uncertainty: Theory, Modeling Methodologies and Practice</summary>

- *Alessandro Umbrico*

- `1905.05713v1` - [abs](http://arxiv.org/abs/1905.05713v1) - [pdf](http://arxiv.org/pdf/1905.05713v1)

> Automated Planning is one of the main research field of Artificial Intelligence since its beginnings. Research in Automated Planning aims at developing general reasoners (i.e., planners) capable of automatically solve complex problems. Broadly speaking, planners rely on a general model characterizing the possible states of the world and the actions that can be performed in order to change the status of the world. Given a model and an initial known state, the objective of a planner is to synthesize a set of actions needed to achieve a particular goal state. The classical approach to planning roughly corresponds to the description given above. The timeline-based approach is a particular planning paradigm capable of integrating causal and temporal reasoning within a unified solving process. This approach has been successfully applied in many real-world scenarios although a common interpretation of the related planning concepts is missing. Indeed, there are significant differences among the existing frameworks that apply this technique. Each framework relies on its own interpretation of timeline-based planning and therefore it is not easy to compare these systems. Thus, the objective of this work is to investigate the timeline-based approach to planning by addressing several aspects ranging from the semantics of the related planning concepts to the modeling and solving techniques. Specifically, the main contributions of this PhD work consist of: (i) the proposal of a formal characterization of the timeline-based approach capable of dealing with temporal uncertainty; (ii) the proposal of a hierarchical modeling and solving approach; (iii) the development of a general purpose framework for planning and execution with timelines; (iv) the validation{\dag}of this approach in real-world manufacturing scenarios.

</details>

<details>

<summary>2019-05-15 05:36:39 - A Systematic Evaluation of Transient Execution Attacks and Defenses</summary>

- *Claudio Canella, Jo Van Bulck, Michael Schwarz, Moritz Lipp, Benjamin von Berg, Philipp Ortner, Frank Piessens, Dmitry Evtyushkin, Daniel Gruss*

- `1811.05441v3` - [abs](http://arxiv.org/abs/1811.05441v3) - [pdf](http://arxiv.org/pdf/1811.05441v3)

> Research on transient execution attacks including Spectre and Meltdown showed that exception or branch misprediction events might leave secret-dependent traces in the CPU's microarchitectural state. This observation led to a proliferation of new Spectre and Meltdown attack variants and even more ad-hoc defenses (e.g., microcode and software patches). Both the industry and academia are now focusing on finding effective defenses for known issues. However, we only have limited insight on residual attack surface and the completeness of the proposed defenses.   In this paper, we present a systematization of transient execution attacks. Our systematization uncovers 6 (new) transient execution attacks that have been overlooked and not been investigated so far: 2 new exploitable Meltdown effects: Meltdown-PK (Protection Key Bypass) on Intel, and Meltdown-BND (Bounds Check Bypass) on Intel and AMD; and 4 new Spectre mistraining strategies. We evaluate the attacks in our classification tree through proof-of-concept implementations on 3 major CPU vendors (Intel, AMD, ARM). Our systematization yields a more complete picture of the attack surface and allows for a more systematic evaluation of defenses. Through this systematic evaluation, we discover that most defenses, including deployed ones, cannot fully mitigate all attack variants.

</details>

<details>

<summary>2019-05-15 05:55:58 - Learning-based Single-step Quantitative Susceptibility Mapping Reconstruction Without Brain Extraction</summary>

- *Hongjiang Wei, Steven Cao, Yuyao Zhang, Xiaojun Guan, Fuhua Yan, Kristen W. Yeom, Chunlei Liu*

- `1905.05953v1` - [abs](http://arxiv.org/abs/1905.05953v1) - [pdf](http://arxiv.org/pdf/1905.05953v1)

> Quantitative susceptibility mapping (QSM) estimates the underlying tissue magnetic susceptibility from MRI gradient-echo phase signal and typically requires several processing steps. These steps involve phase unwrapping, brain volume extraction, background phase removal and solving an ill-posed inverse problem. The resulting susceptibility map is known to suffer from inaccuracy near the edges of the brain tissues, in part due to imperfect brain extraction, edge erosion of the brain tissue and the lack of phase measurement outside the brain. This inaccuracy has thus hindered the application of QSM for measuring the susceptibility of tissues near the brain edges, e.g., quantifying cortical layers and generating superficial venography. To address these challenges, we propose a learning-based QSM reconstruction method that directly estimates the magnetic susceptibility from total phase images without the need for brain extraction and background phase removal, referred to as autoQSM. The neural network has a modified U-net structure and is trained using QSM maps computed by a two-step QSM method. 209 healthy subjects with ages ranging from 11 to 82 years were employed for patch-wise network training. The network was validated on data dissimilar to the training data, e.g. in vivo mouse brain data and brains with lesions, which suggests that the network has generalized and learned the underlying mathematical relationship between magnetic field perturbation and magnetic susceptibility. AutoQSM was able to recover magnetic susceptibility of anatomical structures near the edges of the brain including the veins covering the cortical surface, spinal cord and nerve tracts near the mouse brain boundaries. The advantages of high-quality maps, no need for brain volume extraction and high reconstruction speed demonstrate its potential for future applications.

</details>

<details>

<summary>2019-05-15 08:44:51 - Relation Structure-Aware Heterogeneous Information Network Embedding</summary>

- *Yuanfu Lu, Chuan Shi, Linmei Hu, Zhiyuan Liu*

- `1905.08027v1` - [abs](http://arxiv.org/abs/1905.08027v1) - [pdf](http://arxiv.org/pdf/1905.08027v1)

> Heterogeneous information network (HIN) embedding aims to embed multiple types of nodes into a low-dimensional space. Although most existing HIN embedding methods consider heterogeneous relations in HINs, they usually employ one single model for all relations without distinction, which inevitably restricts the capability of network embedding. In this paper, we take the structural characteristics of heterogeneous relations into consideration and propose a novel Relation structure-aware Heterogeneous Information Network Embedding model (RHINE). By exploring the real-world networks with thorough mathematical analysis, we present two structure-related measures which can consistently distinguish heterogeneous relations into two categories: Affiliation Relations (ARs) and Interaction Relations (IRs). To respect the distinctive characteristics of relations, in our RHINE, we propose different models specifically tailored to handle ARs and IRs, which can better capture the structures and semantics of the networks. At last, we combine and optimize these models in a unified and elegant manner. Extensive experiments on three real-world datasets demonstrate that our model significantly outperforms the state-of-the-art methods in various tasks, including node clustering, link prediction, and node classification.

</details>

<details>

<summary>2019-05-15 16:40:14 - Machine learning approach for segmenting glands in colon histology images using local intensity and texture features</summary>

- *Rupali Khatun, Soumick Chatterjee*

- `1905.08611v1` - [abs](http://arxiv.org/abs/1905.08611v1) - [pdf](http://arxiv.org/pdf/1905.08611v1)

> Colon Cancer is one of the most common types of cancer. The treatment is planned to depend on the grade or stage of cancer. One of the preconditions for grading of colon cancer is to segment the glandular structures of tissues. Manual segmentation method is very time-consuming, and it leads to life risk for the patients. The principal objective of this project is to assist the pathologist to accurate detection of colon cancer. In this paper, the authors have proposed an algorithm for an automatic segmentation of glands in colon histology using local intensity and texture features. Here the dataset images are cropped into patches with different window sizes and taken the intensity of those patches, and also calculated texture-based features. Random forest classifier has been used to classify this patch into different labels. A multilevel random forest technique in a hierarchical way is proposed. This solution is fast, accurate and it is very much applicable in a clinical setup.

</details>

<details>

<summary>2019-05-15 17:48:56 - What do you learn from context? Probing for sentence structure in contextualized word representations</summary>

- *Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, Ellie Pavlick*

- `1905.06316v1` - [abs](http://arxiv.org/abs/1905.06316v1) - [pdf](http://arxiv.org/pdf/1905.06316v1)

> Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.

</details>

<details>

<summary>2019-05-16 13:49:36 - RelExt: Relation Extraction using Deep Learning approaches for Cybersecurity Knowledge Graph Improvement</summary>

- *Aditya Pingle, Aritran Piplai, Sudip Mittal, Anupam Joshi, James Holt, Richard Zak*

- `1905.02497v2` - [abs](http://arxiv.org/abs/1905.02497v2) - [pdf](http://arxiv.org/pdf/1905.02497v2)

> Security Analysts that work in a `Security Operations Center' (SoC) play a major role in ensuring the security of the organization. The amount of background knowledge they have about the evolving and new attacks makes a significant difference in their ability to detect attacks. Open source threat intelligence sources, like text descriptions about cyber-attacks, can be stored in a structured fashion in a cybersecurity knowledge graph. A cybersecurity knowledge graph can be paramount in aiding a security analyst to detect cyber threats because it stores a vast range of cyber threat information in the form of semantic triples which can be queried. A semantic triple contains two cybersecurity entities with a relationship between them. In this work, we propose a system to create semantic triples over cybersecurity text, using deep learning approaches to extract possible relationships. We use the set of semantic triples generated through our system to assert in a cybersecurity knowledge graph. Security Analysts can retrieve this data from the knowledge graph, and use this information to form a decision about a cyber-attack.

</details>

<details>

<summary>2019-05-16 14:30:00 - The Remarkable Role of Similarity in Redundancy-based Program Repair</summary>

- *Zimin Chen, Martin Monperrus*

- `1811.05703v3` - [abs](http://arxiv.org/abs/1811.05703v3) - [pdf](http://arxiv.org/pdf/1811.05703v3)

> Recently, there have been original attempts to use the concept of "code similarity" in program repair, suggesting that similarity analysis has an important role in the repair process. However, there is no dedicated work to characterize and quantify the role of similarity in redundancy-based program repair, where the patch is composed from source code taken from somewhere else. This is where our paper makes a major contribution: we perform a deep and systematic analysis of the role of code similarity during the exploration of the repair search space. We define and set up a large-scale experiment based on four code similarity metrics that capture different similarities: character, token, semantic and structure similarity. Overall, we have computed 56 million similarity score over 15 million source code components. We show that with similarity analysis, at least 90% of search space can be ignored to find the correct patch. Code similarity is capable of ranking the correct repair ingredient first in 4 - 33 % of the considered cases.

</details>

<details>

<summary>2019-05-16 16:01:24 - Better Security Bug Report Classification via Hyperparameter Optimization</summary>

- *Rui Shu, Tianpei Xia, Laurie Williams, Tim Menzies*

- `1905.06872v1` - [abs](http://arxiv.org/abs/1905.06872v1) - [pdf](http://arxiv.org/pdf/1905.06872v1)

> When security bugs are detected, they should be (a)~discussed privately by security software engineers; and (b)~not mentioned to the general public until security patches are available. Software engineers usually report bugs to bug tracking system, and label them as security bug reports (SBRs) or not-security bug reports (NSBRs), while SBRs have a higher priority to be fixed before exploited by attackers than NSBRs. Yet suspected security bug reports are often publicly disclosed because the mislabelling issues ( i.e., mislabel security bug reports as not-security bug report). The goal of this paper is to aid software developers to better classify bug reports that identify security vulnerabilities as security bug reports through parameter tuning of learners and data pre-processor. Previous work has applied text analytics and machine learning learners to classify which reported bugs are security related. We improve on that work, as shown by our analysis of five open source projects. We apply hyperparameter optimization to (a)~the control parameters of a learner; and (b)~the data pre-processing methods that handle the case where the target class is a small fraction of all the data. We show that optimizing the pre-processor is more useful than optimizing the learners. We also show that improvements gained from our approach can be very large. For example, using the same data sets as recently analyzed by our baseline approach, we show that adjusting the data pre-processing results in improvements to classification recall of 35% to 65% (median to max) with moderate increment of false positive rate.

</details>

<details>

<summary>2019-05-16 16:15:01 - TraceWalk: Semantic-based Process Graph Embedding for Consistency Checking</summary>

- *Chen Qian, Lijie Wen, Akhil Kumar*

- `1905.06883v1` - [abs](http://arxiv.org/abs/1905.06883v1) - [pdf](http://arxiv.org/pdf/1905.06883v1)

> Process consistency checking (PCC), an interdiscipline of natural language processing (NLP) and business process management (BPM), aims to quantify the degree of (in)consistencies between graphical and textual descriptions of a process. However, previous studies heavily depend on a great deal of complex expert-defined knowledge such as alignment rules and assessment metrics, thus suffer from the problems of low accuracy and poor adaptability when applied in open-domain scenarios. To address the above issues, this paper makes the first attempt that uses deep learning to perform PCC. Specifically, we proposed TraceWalk, using semantic information of process graphs to learn latent node representations, and integrates it into a convolutional neural network (CNN) based model called TraceNet to predict consistencies. The theoretical proof formally provides the PCC's lower limit and experimental results demonstrate that our approach performs more accurately than state-of-the-art baselines.

</details>

<details>

<summary>2019-05-16 16:56:16 - Predicting Future Lane Changes of Other Highway Vehicles using RNN-based Deep Models</summary>

- *Sajan Patel, Brent Griffin, Kristofer Kusano, Jason J. Corso*

- `1801.04340v4` - [abs](http://arxiv.org/abs/1801.04340v4) - [pdf](http://arxiv.org/pdf/1801.04340v4)

> In the event of sensor failure, autonomous vehicles need to safely execute emergency maneuvers while avoiding other vehicles on the road. To accomplish this, the sensor-failed vehicle must predict the future semantic behaviors of other drivers, such as lane changes, as well as their future trajectories given a recent window of past sensor observations. We address the first issue of semantic behavior prediction in this paper, which is a precursor to trajectory prediction, by introducing a framework that leverages the power of recurrent neural networks (RNNs) and graphical models. Our goal is to predict the future categorical driving intent, for lane changes, of neighboring vehicles up to three seconds into the future given as little as a one-second window of past LIDAR, GPS, inertial, and map data.   We collect real-world data containing over 20 hours of highway driving using an autonomous Toyota vehicle. We propose a composite RNN model by adopting the methodology of Structural Recurrent Neural Networks (RNNs) to learn factor functions and take advantage of both the high-level structure of graphical models and the sequence modeling power of RNNs, which we expect to afford more transparent modeling and activity than opaque, single RNN models. To demonstrate our approach, we validate our model using authentic interstate highway driving to predict the future lane change maneuvers of other vehicles neighboring our autonomous vehicle. We find that our composite Structural RNN outperforms baselines by as much as 12% in balanced accuracy metrics.

</details>

<details>

<summary>2019-05-17 02:57:13 - Predicting Solar Flares Using a Long Short-Term Memory Network</summary>

- *Hao Liu, Chang Liu, Jason T. L. Wang, Haimin Wang*

- `1905.07095v1` - [abs](http://arxiv.org/abs/1905.07095v1) - [pdf](http://arxiv.org/pdf/1905.07095v1)

> We present a long short-term memory (LSTM) network for predicting whether an active region (AR) would produce a gamma-class flare within the next 24 hours. We consider three gamma classes, namely >=M5.0 class, >=M class, and >=C class, and build three LSTM models separately, each corresponding to a gamma class. Each LSTM model is used to make predictions of its corresponding gamma-class flares. The essence of our approach is to model data samples in an AR as time series and use LSTMs to capture temporal information of the data samples. Each data sample has 40 features including 25 magnetic parameters obtained from the Space-weather HMI Active Region Patches (SHARP) and related data products as well as 15 flare history parameters. We survey the flare events that occurred from 2010 May to 2018 May, using the GOES X-ray flare catalogs provided by the National Centers for Environmental Information (NCEI), and select flares with identified ARs in the NCEI flare catalogs. These flare events are used to build the labels (positive vs. negative) of the data samples. Experimental results show that (i) using only 14-22 most important features including both flare history and magnetic parameters can achieve better performance than using all the 40 features together; (ii) our LSTM network outperforms related machine learning methods in predicting the labels of the data samples. To our knowledge, this is the first time that LSTMs have been used for solar flare prediction.

</details>

<details>

<summary>2019-05-17 07:38:19 - Targeted Greybox Fuzzing with Static Lookahead Analysis</summary>

- *Valentin Wüstholz, Maria Christakis*

- `1905.07147v1` - [abs](http://arxiv.org/abs/1905.07147v1) - [pdf](http://arxiv.org/pdf/1905.07147v1)

> Automatic test generation typically aims to generate inputs that explore new paths in the program under test in order to find bugs. Existing work has, therefore, focused on guiding the exploration toward program parts that are more likely to contain bugs by using an offline static analysis.   In this paper, we introduce a novel technique for targeted greybox fuzzing using an online static analysis that guides the fuzzer toward a set of target locations, for instance, located in recently modified parts of the program. This is achieved by first semantically analyzing each program path that is explored by an input in the fuzzer's test suite. The results of this analysis are then used to control the fuzzer's specialized power schedule, which determines how often to fuzz inputs from the test suite. We implemented our technique by extending a state-of-the-art, industrial fuzzer for Ethereum smart contracts and evaluate its effectiveness on 27 real-world benchmarks. Using an online analysis is particularly suitable for the domain of smart contracts since it does not require any code instrumentation---instrumentation to contracts changes their semantics. Our experiments show that targeted fuzzing significantly outperforms standard greybox fuzzing for reaching 83% of the challenging target locations (up to 14x of median speed-up).

</details>

<details>

<summary>2019-05-17 16:18:02 - OpenEDS: Open Eye Dataset</summary>

- *Stephan J. Garbin, Yiru Shen, Immo Schuetz, Robert Cavin, Gregory Hughes, Sachin S. Talathi*

- `1905.03702v2` - [abs](http://arxiv.org/abs/1905.03702v2) - [pdf](http://arxiv.org/pdf/1905.03702v2)

> We present a large scale data set, OpenEDS: Open Eye Dataset, of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eyefacing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from randomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. A baseline experiment has been evaluated on OpenEDS for the task of semantic segmentation of pupil, iris, sclera and background, with the mean intersectionover-union (mIoU) of 98.3 %. We anticipate that OpenEDS will create opportunities to researchers in the eye tracking community and the broader machine learning and computer vision community to advance the state of eye-tracking for VR applications. The dataset is available for download upon request at https://research.fb.com/programs/openeds-challenge

</details>

<details>

<summary>2019-05-17 16:26:05 - Don't Blame Distributional Semantics if it can't do Entailment</summary>

- *Matthijs Westera, Gemma Boleda*

- `1905.07356v1` - [abs](http://arxiv.org/abs/1905.07356v1) - [pdf](http://arxiv.org/pdf/1905.07356v1)

> Distributional semantics has had enormous empirical success in Computational Linguistics and Cognitive Science in modeling various semantic phenomena, such as semantic similarity, and distributional models are widely used in state-of-the-art Natural Language Processing systems. However, the theoretical status of distributional semantics within a broader theory of language and cognition is still unclear: What does distributional semantics model? Can it be, on its own, a fully adequate model of the meanings of linguistic expressions? The standard answer is that distributional semantics is not fully adequate in this regard, because it falls short on some of the central aspects of formal semantic approaches: truth conditions, entailment, reference, and certain aspects of compositionality. We argue that this standard answer rests on a misconception: These aspects do not belong in a theory of expression meaning, they are instead aspects of speaker meaning, i.e., communicative intentions in a particular context. In a slogan: words do not refer, speakers do. Clearing this up enables us to argue that distributional semantics on its own is an adequate model of expression meaning. Our proposal sheds light on the role of distributional semantics in a broader theory of language and cognition, its relationship to formal semantics, and its place in computational models.

</details>

<details>

<summary>2019-05-17 18:17:54 - Semi-Blind Spatially-Variant Deconvolution in Optical Microscopy with Local Point Spread Function Estimation By Use Of Convolutional Neural Networks</summary>

- *Adrian Shajkofci, Michael Liebling*

- `1803.07452v4` - [abs](http://arxiv.org/abs/1803.07452v4) - [pdf](http://arxiv.org/pdf/1803.07452v4)

> We present a semi-blind, spatially-variant deconvolution technique aimed at optical microscopy that combines a local estimation step of the point spread function (PSF) and deconvolution using a spatially variant, regularized Richardson-Lucy algorithm. To find the local PSF map in a computationally tractable way, we train a convolutional neural network to perform regression of an optical parametric model on synthetically blurred image patches. We deconvolved both synthetic and experimentally-acquired data, and achieved an improvement of image SNR of 1.00 dB on average, compared to other deconvolution algorithms.

</details>

<details>

<summary>2019-05-17 20:45:49 - A semantic-aided particle filter approach for AUV localization</summary>

- *Francesco Maurelli, Szymon Krupinski*

- `1905.07470v1` - [abs](http://arxiv.org/abs/1905.07470v1) - [pdf](http://arxiv.org/pdf/1905.07470v1)

> This paper presents a novel approach to AUV localization, based on a semantic-aided particle filter. Particle filters have been used successfully for robotics localization since many years. Most of the approaches are however based on geometric measurements and geometric information and simulations. In the past years more and more efforts from research goes towards cognitive robotics and the marine domain is not exception. Moving from signal to symbol becomes therefore paramount for more complex applications. This paper presents a contribution in the well-known area of underwater localization, incorporating semantic information. An extension to the standard particle filter approach is presented, based on semantic information of the environment. A comparison with the geometric approach shows the advantages of a semantic layer to successfully perform self-localization.

</details>

<details>

<summary>2019-05-17 21:01:59 - Dueling Decoders: Regularizing Variational Autoencoder Latent Spaces</summary>

- *Bryan Seybold, Emily Fertig, Alex Alemi, Ian Fischer*

- `1905.07478v1` - [abs](http://arxiv.org/abs/1905.07478v1) - [pdf](http://arxiv.org/pdf/1905.07478v1)

> Variational autoencoders learn unsupervised data representations, but these models frequently converge to minima that fail to preserve meaningful semantic information. For example, variational autoencoders with autoregressive decoders often collapse into autodecoders, where they learn to ignore the encoder input. In this work, we demonstrate that adding an auxiliary decoder to regularize the latent space can prevent this collapse, but successful auxiliary decoding tasks are domain dependent. Auxiliary decoders can increase the amount of semantic information encoded in the latent space and visible in the reconstructions. The semantic information in the variational autoencoder's representation is only weakly correlated with its rate, distortion, or evidence lower bound. Compared to other popular strategies that modify the training objective, our regularization of the latent space generally increased the semantic information content.

</details>

<details>

<summary>2019-05-18 13:35:33 - BERTSel: Answer Selection with Pre-trained Models</summary>

- *Dongfang Li, Yifei Yu, Qingcai Chen, Xinyu Li*

- `1905.07588v1` - [abs](http://arxiv.org/abs/1905.07588v1) - [pdf](http://arxiv.org/pdf/1905.07588v1)

> Recently, pre-trained models have been the dominant paradigm in natural language processing. They achieved remarkable state-of-the-art performance across a wide range of related tasks, such as textual entailment, natural language inference, question answering, etc. BERT, proposed by Devlin et.al., has achieved a better marked result in GLUE leaderboard with a deep transformer architecture. Despite its soaring popularity, however, BERT has not yet been applied to answer selection. This task is different from others with a few nuances: first, modeling the relevance and correctness of candidates matters compared to semantic relatedness and syntactic structure; second, the length of an answer may be different from other candidates and questions. In this paper. we are the first to explore the performance of fine-tuning BERT for answer selection. We achieved STOA results across five popular datasets, demonstrating the success of pre-trained models in this task.

</details>

<details>

<summary>2019-05-18 14:32:53 - Semantic flow in language networks</summary>

- *Edilson A. Corrêa Jr., Vanessa Q. Marinho, Diego R. Amancio*

- `1905.07595v1` - [abs](http://arxiv.org/abs/1905.07595v1) - [pdf](http://arxiv.org/pdf/1905.07595v1)

> In this study we propose a framework to characterize documents based on their semantic flow. The proposed framework encompasses a network-based model that connected sentences based on their semantic similarity. Semantic fields are detected using standard community detection methods. as the story unfolds, transitions between semantic fields are represent in Markov networks, which in turned are characterized via network motifs (subgraphs). Here we show that the proposed framework can be used to classify books according to their style and publication dates. Remarkably, even without a systematic optimization of parameters, philosophy and investigative books were discriminated with an accuracy rate of 92.5%. Because this model captures semantic features of texts, it could be used as an additional feature in traditional network-based models of texts that capture only syntactical/stylistic information, as it is the case of word adjacency (co-occurrence) networks.

</details>

<details>

<summary>2019-05-19 12:42:41 - Leveraging Semantic Embeddings for Safety-Critical Applications</summary>

- *Thomas Brunner, Frederik Diehl, Michael Truong Le, Alois Knoll*

- `1905.07733v1` - [abs](http://arxiv.org/abs/1905.07733v1) - [pdf](http://arxiv.org/pdf/1905.07733v1)

> Semantic Embeddings are a popular way to represent knowledge in the field of zero-shot learning. We observe their interpretability and discuss their potential utility in a safety-critical context. Concretely, we propose to use them to add introspection and error detection capabilities to neural network classifiers. First, we show how to create embeddings from symbolic domain knowledge. We discuss how to use them for interpreting mispredictions and propose a simple error detection scheme. We then introduce the concept of semantic distance: a real-valued score that measures confidence in the semantic space. We evaluate this score on a traffic sign classifier and find that it achieves near state-of-the-art performance, while being significantly faster to compute than other confidence scores. Our approach requires no changes to the original network and is thus applicable to any task for which domain knowledge is available.

</details>

<details>

<summary>2019-05-19 16:28:51 - Phish-IRIS: A New Approach for Vision Based Brand Prediction of Phishing Web Pages via Compact Visual Descriptors</summary>

- *Firat Coskun Dalgic, Ahmet Selman Bozkir, Murat Aydos*

- `1905.07767v1` - [abs](http://arxiv.org/abs/1905.07767v1) - [pdf](http://arxiv.org/pdf/1905.07767v1)

> Phishing, a continuously growing cyber threat, aims to obtain innocent users' credentials by deceiving them via presenting fake web pages which mimic their legitimate targets. To date, various attempts have been carried out in order to detect phishing pages. In this study, we treat the problem of phishing web page identification as an image classification task and propose a machine learning augmented pure vision based approach which extracts and classifies compact visual features from web page screenshots. For this purpose, we employed several MPEG7 and MPEG7-like compact visual descriptors (SCD, CLD, CEDD, FCTH and JCD) to reveal color and edge based discriminative visual cues. Throughout the feature extraction process we have followed two different schemes working on either whole screenshots in a "holistic" manner or equal sized "patches" constructing a coarse-to-fine "pyramidal" representation. Moreover, for the task of image classification, we have built SVM and Random Forest based machine learning models. In order to assess the performance and generalization capability of the proposed approach, we have collected a mid-sized corpus covering 14 distinct brands and involving 2852 samples. According to the conducted experiments, our approach reaches up to 90.5% F1 score via SCD. As a result, compared to other studies, the suggested approach presents a lightweight schema serving competitive accuracy and superior feature extraction and inferring speed that enables it to be used as a browser plugin.

</details>

<details>

<summary>2019-05-19 18:23:14 - Correlation Coefficients and Semantic Textual Similarity</summary>

- *Vitalii Zhelezniak, Aleksandar Savkov, April Shen, Nils Y. Hammerla*

- `1905.07790v1` - [abs](http://arxiv.org/abs/1905.07790v1) - [pdf](http://arxiv.org/pdf/1905.07790v1)

> A large body of research into semantic textual similarity has focused on constructing state-of-the-art embeddings using sophisticated modelling, careful choice of learning signals and many clever tricks. By contrast, little attention has been devoted to similarity measures between these embeddings, with cosine similarity being used unquestionably in the majority of cases. In this work, we illustrate that for all common word vectors, cosine similarity is essentially equivalent to the Pearson correlation coefficient, which provides some justification for its use. We thoroughly characterise cases where Pearson correlation (and thus cosine similarity) is unfit as similarity measure. Importantly, we show that Pearson correlation is appropriate for some word vectors but not others. When it is not appropriate, we illustrate how common non-parametric rank correlation coefficients can be used instead to significantly improve performance. We support our analysis with a series of evaluations on word-level and sentence-level semantic textual similarity benchmarks. On the latter, we show that even the simplest averaged word vectors compared by rank correlation easily rival the strongest deep representations compared by cosine similarity.

</details>

<details>

<summary>2019-05-20 03:02:44 - Boundary Loss for Remote Sensing Imagery Semantic Segmentation</summary>

- *Alexey Bokhovkin, Evgeny Burnaev*

- `1905.07852v1` - [abs](http://arxiv.org/abs/1905.07852v1) - [pdf](http://arxiv.org/pdf/1905.07852v1)

> In response to the growing importance of geospatial data, its analysis including semantic segmentation becomes an increasingly popular task in computer vision today. Convolutional neural networks are powerful visual models that yield hierarchies of features and practitioners widely use them to process remote sensing data. When performing remote sensing image segmentation, multiple instances of one class with precisely defined boundaries are often the case, and it is crucial to extract those boundaries accurately. The accuracy of segments boundaries delineation influences the quality of the whole segmented areas explicitly. However, widely-used segmentation loss functions such as BCE, IoU loss or Dice loss do not penalize misalignment of boundaries sufficiently. In this paper, we propose a novel loss function, namely a differentiable surrogate of a metric accounting accuracy of boundary detection. We can use the loss function with any neural network for binary segmentation. We performed validation of our loss function with various modifications of UNet on a synthetic dataset, as well as using real-world data (ISPRS Potsdam, INRIA AIL). Trained with the proposed loss function, models outperform baseline methods in terms of IoU score.

</details>

<details>

<summary>2019-05-20 03:14:11 - Target Based Speech Act Classification in Political Campaign Text</summary>

- *Shivashankar Subramanian, Trevor Cohn, Timothy Baldwin*

- `1905.07856v1` - [abs](http://arxiv.org/abs/1905.07856v1) - [pdf](http://arxiv.org/pdf/1905.07856v1)

> We study pragmatics in political campaign text, through analysis of speech acts and the target of each utterance. We propose a new annotation schema incorporating domain-specific speech acts, such as commissive-action, and present a novel annotated corpus of media releases and speech transcripts from the 2016 Australian election cycle. We show how speech acts and target referents can be modeled as sequential classification, and evaluate several techniques, exploiting contextualized word representations, semi-supervised learning, task dependencies and speaker meta-data.

</details>

<details>

<summary>2019-05-20 17:31:59 - Vision-based Navigation of Autonomous Vehicle in Roadway Environments with Unexpected Hazards</summary>

- *Mhafuzul Islam, Mahsrur Chowdhury, Hongda Li, Hongxin Hu*

- `1810.03967v3` - [abs](http://arxiv.org/abs/1810.03967v3) - [pdf](http://arxiv.org/pdf/1810.03967v3)

> Vision-based navigation of autonomous vehicles primarily depends on the Deep Neural Network (DNN) based systems in which the controller obtains input from sensors/detectors, such as cameras and produces a vehicle control output, such as a steering wheel angle to navigate the vehicle safely in a roadway traffic environment. Typically, these DNN-based systems of the autonomous vehicle are trained through supervised learning; however, recent studies show that a trained DNN-based system can be compromised by perturbation or adversarial inputs. Similarly, this perturbation can be introduced into the DNN-based systems of autonomous vehicle by unexpected roadway hazards, such as debris and roadblocks. In this study, we first introduce a roadway hazardous environment (both intentional and unintentional roadway hazards) that can compromise the DNN-based navigational system of an autonomous vehicle, and produces an incorrect steering wheel angle, which can cause crashes resulting in fatality and injury. Then, we develop a DNN-based autonomous vehicle driving system using object detection and semantic segmentation to mitigate the adverse effect of this type of hazardous environment, which helps the autonomous vehicle to navigate safely around such hazards. We find that our developed DNN-based autonomous vehicle driving system including hazardous object detection and semantic segmentation improves the navigational ability of an autonomous vehicle to avoid a potential hazard by 21% compared to the traditional DNN-based autonomous vehicle driving system.

</details>

<details>

<summary>2019-05-20 19:30:35 - Explicit Utilization of General Knowledge in Machine Reading Comprehension</summary>

- *Chao Wang, Hui Jiang*

- `1809.03449v3` - [abs](http://arxiv.org/abs/1809.03449v3) - [pdf](http://arxiv.org/pdf/1809.03449v3)

> To bridge the gap between Machine Reading Comprehension (MRC) models and human beings, which is mainly reflected in the hunger for data and the robustness to noise, in this paper, we explore how to integrate the neural networks of MRC models with the general knowledge of human beings. On the one hand, we propose a data enrichment method, which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage-question pair. On the other hand, we propose an end-to-end MRC model named as Knowledge Aided Reader (KAR), which explicitly uses the above extracted general knowledge to assist its attention mechanisms. Based on the data enrichment method, KAR is comparable in performance with the state-of-the-art MRC models, and significantly more robust to noise than them. When only a subset (20%-80%) of the training examples are available, KAR outperforms the state-of-the-art MRC models by a large margin, and is still reasonably robust to noise.

</details>

<details>

<summary>2019-05-20 20:34:49 - AiDroid: When Heterogeneous Information Network Marries Deep Neural Network for Real-time Android Malware Detection</summary>

- *Yanfang Ye, Shifu Hou, Lingwei Chen, Jingwei Lei, Wenqiang Wan, Jiabin Wang, Qi Xiong, Fudong Shao*

- `1811.01027v2` - [abs](http://arxiv.org/abs/1811.01027v2) - [pdf](http://arxiv.org/pdf/1811.01027v2)

> The explosive growth and increasing sophistication of Android malware call for new defensive techniques that are capable of protecting mobile users against novel threats. In this paper, we first extract the runtime Application Programming Interface (API) call sequences from Android apps, and then analyze higher-level semantic relations within the ecosystem to comprehensively characterize the apps. To model different types of entities (i.e., app, API, IMEI, signature, affiliation) and the rich semantic relations among them, we then construct a structural heterogeneous information network (HIN) and present meta-path based approach to depict the relatedness over apps. To efficiently classify nodes (e.g., apps) in the constructed HIN, we propose the HinLearning method to first obtain in-sample node embeddings and then learn representations of out-of-sample nodes without rerunning/adjusting HIN embeddings at the first attempt. Afterwards, we design a deep neural network (DNN) classifier taking the learned HIN representations as inputs for Android malware detection. A comprehensive experimental study on the large-scale real sample collections from Tencent Security Lab is performed to compare various baselines. Promising experimental results demonstrate that our developed system AiDroid which integrates our proposed method outperforms others in real-time Android malware detection. AiDroid has already been incorporated into Tencent Mobile Security product that serves millions of users worldwide.

</details>

<details>

<summary>2019-05-20 21:54:26 - Why Machines Cannot Learn Mathematics, Yet</summary>

- *André Greiner-Petter, Terry Ruas, Moritz Schubotz, Akiko Aizawa, William Grosky, Bela Gipp*

- `1905.08359v1` - [abs](http://arxiv.org/abs/1905.08359v1) - [pdf](http://arxiv.org/pdf/1905.08359v1)

> Nowadays, Machine Learning (ML) is seen as the universal solution to improve the effectiveness of information retrieval (IR) methods. However, while mathematics is a precise and accurate science, it is usually expressed by less accurate and imprecise descriptions, contributing to the relative dearth of machine learning applications for IR in this domain. Generally, mathematical documents communicate their knowledge with an ambiguous, context-dependent, and non-formal language. Given recent advances in ML, it seems canonical to apply ML techniques to represent and retrieve mathematics semantically. In this work, we apply popular text embedding techniques to the arXiv collection of STEM documents and explore how these are unable to properly understand mathematics from that corpus. In addition, we also investigate the missing aspects that would allow mathematics to be learned by computers.

</details>

<details>

<summary>2019-05-20 23:10:42 - Word Usage Similarity Estimation with Sentence Representations and Automatic Substitutes</summary>

- *Aina Garí Soler, Marianna Apidianaki, Alexandre Allauzen*

- `1905.08377v1` - [abs](http://arxiv.org/abs/1905.08377v1) - [pdf](http://arxiv.org/pdf/1905.08377v1)

> Usage similarity estimation addresses the semantic proximity of word instances in different contexts. We apply contextualized (ELMo and BERT) word and sentence embeddings to this task, and propose supervised models that leverage these representations for prediction. Our models are further assisted by lexical substitute annotations automatically assigned to word instances by context2vec, a neural model that relies on a bidirectional LSTM. We perform an extensive comparison of existing word and sentence representations on benchmark datasets addressing both graded and binary similarity. The best performing models outperform previous methods in both settings.

</details>

<details>

<summary>2019-05-21 00:44:58 - An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation</summary>

- *Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, Denys Poshyvanyk*

- `1812.08693v2` - [abs](http://arxiv.org/abs/1812.08693v2) - [pdf](http://arxiv.org/pdf/1812.08693v2)

> Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub, in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9-50% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.

</details>

<details>

<summary>2019-05-21 02:37:11 - Story Ending Prediction by Transferable BERT</summary>

- *Zhongyang Li, Xiao Ding, Ting Liu*

- `1905.07504v2` - [abs](http://arxiv.org/abs/1905.07504v2) - [pdf](http://arxiv.org/pdf/1905.07504v2)

> Recent advances, such as GPT and BERT, have shown success in incorporating a pre-trained transformer language model and fine-tuning operation to improve downstream NLP systems. However, this framework still has some fundamental problems in effectively incorporating supervised knowledge from other related tasks. In this study, we investigate a transferable BERT (TransBERT) training framework, which can transfer not only general language knowledge from large-scale unlabeled data but also specific kinds of knowledge from various semantically related supervised tasks, for a target task. Particularly, we propose utilizing three kinds of transfer tasks, including natural language inference, sentiment classification, and next action prediction, to further train BERT based on a pre-trained model. This enables the model to get a better initialization for the target task. We take story ending prediction as the target task to conduct experiments. The final result, an accuracy of 91.8%, dramatically outperforms previous state-of-the-art baseline methods. Several comparative experiments give some helpful suggestions on how to select transfer tasks. Error analysis shows what are the strength and weakness of BERT-based models for story ending prediction.

</details>

<details>

<summary>2019-05-21 05:54:10 - Spatially Constrained Spectral Clustering Algorithms for Region Delineation</summary>

- *Shuai Yuan, Pang-Ning Tan, Kendra Spence Cheruvelil, Sarah M. Collins, Patricia A. Soranno*

- `1905.08451v1` - [abs](http://arxiv.org/abs/1905.08451v1) - [pdf](http://arxiv.org/pdf/1905.08451v1)

> Regionalization is the task of dividing up a landscape into homogeneous patches with similar properties. Although this task has a wide range of applications, it has two notable challenges. First, it is assumed that the resulting regions are both homogeneous and spatially contiguous. Second, it is well-recognized that landscapes are hierarchical such that fine-scale regions are nested wholly within broader-scale regions. To address these two challenges, first, we develop a spatially constrained spectral clustering framework for region delineation that incorporates the tradeoff between region homogeneity and spatial contiguity. The framework uses a flexible, truncated exponential kernel to represent the spatial contiguity constraints, which is integrated with the landscape feature similarity matrix for region delineation. To address the second challenge, we extend the framework to create fine-scale regions that are nested within broader-scaled regions using a greedy, recursive bisection approach. We present a case study of a terrestrial ecology data set in the United States that compares the proposed framework with several baseline methods for regionalization. Experimental results suggest that the proposed framework for regionalization outperforms the baseline methods, especially in terms of balancing region contiguity and homogeneity, as well as creating regions of more similar size, which is often a desired trait of regions.

</details>

<details>

<summary>2019-05-21 08:57:10 - Verification Artifacts in Cooperative Verification: Survey and Unifying Component Framework</summary>

- *Dirk Beyer, Heike Wehrheim*

- `1905.08505v1` - [abs](http://arxiv.org/abs/1905.08505v1) - [pdf](http://arxiv.org/pdf/1905.08505v1)

> The goal of cooperative verification is to combine verification approaches in such a way that they work together to verify a system model. In particular, cooperative verifiers provide exchangeable information (verification artifacts) to other verifiers or consume such information from other verifiers with the goal of increasing the overall effectiveness and efficiency of the verification process. This paper first gives an overview over approaches for leveraging strengths of different techniques, algorithms, and tools in order to increase the power and abilities of the state of the art in software verification. Second, we specifically outline cooperative verification approaches and discuss their employed verification artifacts. We formalize all artifacts in a uniform way, thereby fixing their semantics and providing verifiers with a precise meaning of the exchanged information.

</details>

<details>

<summary>2019-05-21 14:47:35 - MultiWiki: Interlingual Text Passage Alignment in Wikipedia</summary>

- *Simon Gottschalk, Elena Demidova*

- `1905.08675v1` - [abs](http://arxiv.org/abs/1905.08675v1) - [pdf](http://arxiv.org/pdf/1905.08675v1)

> In this article we address the problem of text passage alignment across interlingual article pairs in Wikipedia. We develop methods that enable the identification and interlinking of text passages written in different languages and containing overlapping information. Interlingual text passage alignment can enable Wikipedia editors and readers to better understand language-specific context of entities, provide valuable insights in cultural differences and build a basis for qualitative analysis of the articles. An important challenge in this context is the trade-off between the granularity of the extracted text passages and the precision of the alignment. Whereas short text passages can result in more precise alignment, longer text passages can facilitate a better overview of the differences in an article pair. To better understand these aspects from the user perspective, we conduct a user study at the example of the German, Russian and the English Wikipedia and collect a user-annotated benchmark. Then we propose MultiWiki -- a method that adopts an integrated approach to the text passage alignment using semantic similarity measures and greedy algorithms and achieves precise results with respect to the user-defined alignment. MultiWiki demonstration is publicly available and currently supports four language pairs.

</details>

<details>

<summary>2019-05-21 15:14:34 - EventKG - the Hub of Event Knowledge on the Web - and Biographical Timeline Generation</summary>

- *Simon Gottschalk, Elena Demidova*

- `1905.08794v1` - [abs](http://arxiv.org/abs/1905.08794v1) - [pdf](http://arxiv.org/pdf/1905.08794v1)

> One of the key requirements to facilitate the semantic analytics of information regarding contemporary and historical events on the Web, in the news and in social media is the availability of reference knowledge repositories containing comprehensive representations of events, entities and temporal relations. Existing knowledge graphs, with popular examples including DBpedia, YAGO and Wikidata, focus mostly on entity-centric information and are insufficient in terms of their coverage and completeness with respect to events and temporal relations. In this article we address this limitation, formalise the concept of a temporal knowledge graph and present its instantiation - EventKG. EventKG is a multilingual event-centric temporal knowledge graph that incorporates over 690 thousand events and over 2.3 million temporal relations obtained from several large-scale knowledge graphs and semi-structured sources and makes them available through a canonical RDF representation. Whereas popular entities often possess hundreds of relations within a temporal knowledge graph such as EventKG, generating a concise overview of the most important temporal relations for a given entity is a challenging task. In this article we demonstrate an application of EventKG to biographical timeline generation, where we adopt a distant supervision method to identify relations most relevant for an entity biography. Our evaluation results provide insights on the characteristics of EventKG and demonstrate the effectiveness of the proposed biographical timeline generation method.

</details>

<details>

<summary>2019-05-21 15:43:47 - A Discrete Empirical Interpolation Method for Interpretable Immersion and Embedding of Nonlinear Manifolds</summary>

- *Samuel E. Otto, Clarence W. Rowley*

- `1905.07619v2` - [abs](http://arxiv.org/abs/1905.07619v2) - [pdf](http://arxiv.org/pdf/1905.07619v2)

> Manifold learning techniques seek to discover structure-preserving mappings of high-dimensional data into low-dimensional spaces.   While the new sets of coordinates specified by these mappings can closely parameterize the data, they are generally complicated nonlinear functions of the original variables. This makes them difficult to interpret physically.   Furthermore, in data-driven model reduction applications the governing equations may have structure that is destroyed by nonlinear mapping into coordinates on an inertial manifold, creating a computational bottleneck for simulations.   Instead, we propose to identify a small collection of the original variables which are capable of uniquely determining all others either locally via immersion or globally via embedding of the underlying manifold.   When the data lies on a low-dimensional subspace the existing discrete empirical interpolation method (DEIM) accomplishes this with recent variants employing greedy algorithms based on pivoted QR (PQR) factorizations.   However, low-dimensional manifolds coming from a variety of applications, particularly from advection-dominated PDEs, do not lie in or near any low-dimensional subspace.   Our proposed approach extends DEIM to data lying near nonlinear manifolds by applying a similar pivoted QR procedure simultaneously on collections of patches making up locally linear approximations of the manifold, resulting in a novel simultaneously pivoted QR (SimPQR) algorithm.   The immersion provided by SimPQR can be extended to an embedding by applying SimPQR a second time to a modified collection of vectors.   The SimPQR method for computing these `nonlinear DEIM' (NLDEIM) coordinates is successfully applied to real-world data lying near an inertial manifold in a cylinder wake flow as well as data coming from a viscous Burgers equation with different initial conditions.

</details>

<details>

<summary>2019-05-21 23:31:45 - Enhancing Domain Word Embedding via Latent Semantic Imputation</summary>

- *Shibo Yao, Dantong Yu, Keli Xiao*

- `1905.08900v1` - [abs](http://arxiv.org/abs/1905.08900v1) - [pdf](http://arxiv.org/pdf/1905.08900v1)

> We present a novel method named Latent Semantic Imputation (LSI) to transfer external knowledge into semantic space for enhancing word embedding. The method integrates graph theory to extract the latent manifold structure of the entities in the affinity space and leverages non-negative least squares with standard simplex constraints and power iteration method to derive spectral embeddings. It provides an effective and efficient approach to combining entity representations defined in different Euclidean spaces. Specifically, our approach generates and imputes reliable embedding vectors for low-frequency words in the semantic space and benefits downstream language tasks that depend on word embedding. We conduct comprehensive experiments on a carefully designed classification problem and language modeling and demonstrate the superiority of the enhanced embedding via LSI over several well-known benchmark embeddings. We also confirm the consistency of the results under different parameter settings of our method.

</details>

<details>

<summary>2019-05-22 08:35:09 - Connectivity Lower Bounds in Broadcast Congested Clique</summary>

- *Shreyas Pai, Sriram V. Pemmaraju*

- `1905.09016v1` - [abs](http://arxiv.org/abs/1905.09016v1) - [pdf](http://arxiv.org/pdf/1905.09016v1)

> We prove three new lower bounds for graph connectivity in the $1$-bit broadcast congested clique model, BCC$(1)$. First, in the KT-$0$ version of BCC$(1)$, in which nodes are aware of neighbors only through port numbers, we show an $\Omega(\log n)$ round lower bound for CONNECTIVITY even for constant-error randomized Monte Carlo algorithms. The deterministic version of this result can be obtained via the well-known "edge-crossing" argument, but, the randomized version of this result requires establishing new combinatorial results regarding the indistinguishability graph induced by inputs. In our second result, we show that the $\Omega(\log n)$ lower bound result extends to the KT-$1$ version of the BCC$(1)$ model, in which nodes are aware of IDs of all neighbors, though our proof works only for deterministic algorithms. Since nodes know IDs of their neighbors in the KT-$1$ model, it is no longer possible to play "edge-crossing" tricks; instead we present a reduction from the 2-party communication complexity problem PARTITION in which Alice and Bob are give two set partitions on $[n]$ and are required to determine if the join of these two set partitions equals the trivial one-part set partition. While our KT-$1$ CONNECTIVITY lower bound holds only for deterministic algorithms, in our third result we extend this $\Omega(\log n)$ KT-1 lower bound to constant-error Monte Carlo algorithms for the closely related CONNECTED COMPONENTS problem. We use information-theoretic techniques to obtain this result. All our results hold for the seemingly easy special case of CONNECTIVITY in which an algorithm has to distinguish an instance with one cycle from an instance with multiple cycles. Our results showcase three rather different lower bound techniques and lay the groundwork for further improvements in lower bounds for CONNECTIVITY in the BCC$(1)$ model.

</details>

<details>

<summary>2019-05-22 09:24:17 - Spatial Sampling Network for Fast Scene Understanding</summary>

- *Davide Mazzini, Raimondo Schettini*

- `1905.09033v1` - [abs](http://arxiv.org/abs/1905.09033v1) - [pdf](http://arxiv.org/pdf/1905.09033v1)

> We propose a network architecture to perform efficient scene understanding. This work presents three main novelties: the first is an Improved Guided Upsampling Module that can replace in toto the decoder part in common semantic segmentation networks. Our second contribution is the introduction of a new module based on spatial sampling to perform Instance Segmentation. It provides a very fast instance segmentation, needing only thresholding as post-processing step at inference time. Finally, we propose a novel efficient network design that includes the new modules and test it against different datasets for outdoor scene understanding. To our knowledge, our network is one of the themost efficient architectures for scene understanding published to date, furthermore being 8.6% more accurate than the fastest competitor on semantic segmentation and almost five times faster than the most efficient network for instance segmentation.

</details>

<details>

<summary>2019-05-22 10:13:48 - Retrieving Multi-Entity Associations: An Evaluation of Combination Modes for Word Embeddings</summary>

- *Gloria Feher, Andreas Spitz, Michael Gertz*

- `1905.09052v1` - [abs](http://arxiv.org/abs/1905.09052v1) - [pdf](http://arxiv.org/pdf/1905.09052v1)

> Word embeddings have gained significant attention as learnable representations of semantic relations between words, and have been shown to improve upon the results of traditional word representations. However, little effort has been devoted to using embeddings for the retrieval of entity associations beyond pairwise relations. In this paper, we use popular embedding methods to train vector representations of an entity-annotated news corpus, and evaluate their performance for the task of predicting entity participation in news events versus a traditional word cooccurrence network as a baseline. To support queries for events with multiple participating entities, we test a number of combination modes for the embedding vectors. While we find that even the best combination modes for word embeddings do not quite reach the performance of the full cooccurrence network, especially for rare entities, we observe that different embedding methods model different types of relations, thereby indicating the potential for ensemble methods.

</details>

<details>

<summary>2019-05-22 16:18:06 - A Note on Reasoning on $\textit{DL-Lite}_{\cal R}$ with Defeasibility</summary>

- *Loris Bozzato, Thomas Eiter, Luciano Serafini*

- `1905.09221v1` - [abs](http://arxiv.org/abs/1905.09221v1) - [pdf](http://arxiv.org/pdf/1905.09221v1)

> Representation of defeasible information is of interest in description logics, as it is related to the need of accommodating exceptional instances in knowledge bases. In this direction, in our previous works we presented a datalog translation for reasoning on (contextualized) OWL RL knowledge bases with a notion of justified exceptions on defeasible axioms. While it covers a relevant fragment of OWL, the resulting reasoning process needs a complex encoding in order to capture reasoning on negative information. In this paper, we consider the case of knowledge bases in $\textit{DL-Lite}_{\cal R}$, i.e. the language underlying OWL QL. We provide a definition for $\textit{DL-Lite}_{\cal R}$ knowledge bases with defeasible axioms and study their properties. The limited form of $\textit{DL-Lite}_{\cal R}$ axioms allows us to formulate a simpler encoding into datalog (under answer set semantics) with direct rules for reasoning on negative information. The resulting materialization method gives rise to a complete reasoning procedure for instance checking in $\textit{DL-Lite}_{\cal R}$ with defeasible axioms.

</details>

<details>

<summary>2019-05-22 20:16:25 - Hey Google, What Exactly Do Your Security Patches Tell Us? A Large-Scale Empirical Study on Android Patched Vulnerabilities</summary>

- *Sadegh Farhang, Mehmet Bahadir Kirdan, Aron Laszka, Jens Grossklags*

- `1905.09352v1` - [abs](http://arxiv.org/abs/1905.09352v1) - [pdf](http://arxiv.org/pdf/1905.09352v1)

> In this paper, we perform a comprehensive study of 2,470 patched Android vulnerabilities that we collect from different data sources such as Android security bulletins, CVEDetails, Qualcomm Code Aurora, AOSP Git repository, and Linux Patchwork. In our data analysis, we focus on determining the affected layers, OS versions, severity levels, and common weakness enumerations (CWE) associated with the patched vulnerabilities. Further, we assess the timeline of each vulnerability, including discovery and patch dates. We find that (i) even though the number of patched vulnerabilities changes considerably from month to month, the relative number of patched vulnerabilities for each severity level remains stable over time, (ii) there is a significant delay in patching vulnerabilities that originate from the Linux community or concern Qualcomm components, even though Linux and Qualcomm provide and release their own patches earlier, (iii) different AOSP versions receive security updates for different periods of time, (iv) for 94% of patched Android vulnerabilities, the date of disclosure in public datasets is not before the patch release date, (v) there exist some inconsistencies among public vulnerability data sources, e.g., some CVE IDs are listed in Android Security bulletins with detailed information, but in CVEDetails they are listed as unknown, (vi) many patched vulnerabilities for newer Android versions likely also affect older versions that do not receive security patches due to end-of-life.

</details>

<details>

<summary>2019-05-22 23:03:24 - A Multi-Resolution Word Embedding for Document Retrieval from Large Unstructured Knowledge Bases</summary>

- *Tolgahan Cakaloglu, Xiaowei Xu*

- `1902.00663v7` - [abs](http://arxiv.org/abs/1902.00663v7) - [pdf](http://arxiv.org/pdf/1902.00663v7)

> Deep language models learning a hierarchical representation proved to be a powerful tool for natural language processing, text mining and information retrieval. However, representations that perform well for retrieval must capture semantic meaning at different levels of abstraction or context-scopes. In this paper, we propose a new method to generate multi-resolution word embeddings that represent documents at multiple resolutions in terms of context-scopes. In order to investigate its performance,we use the Stanford Question Answering Dataset (SQuAD) and the Question Answering by Search And Reading (QUASAR) in an open-domain question-answering setting, where the first task is to find documents useful for answering a given question. To this end, we first compare the quality of various text-embedding methods for retrieval performance and give an extensive empirical comparison with the performance of various non-augmented base embeddings with and without multi-resolution representation. We argue that multi-resolution word embeddings are consistently superior to the original counterparts and deep residual neural models specifically trained for retrieval purposes can yield further significant gains when they are used for augmenting those embeddings.

</details>

<details>

<summary>2019-05-23 03:58:59 - Ensemble Model Patching: A Parameter-Efficient Variational Bayesian Neural Network</summary>

- *Oscar Chang, Yuling Yao, David Williams-King, Hod Lipson*

- `1905.09453v1` - [abs](http://arxiv.org/abs/1905.09453v1) - [pdf](http://arxiv.org/pdf/1905.09453v1)

> Two main obstacles preventing the widespread adoption of variational Bayesian neural networks are the high parameter overhead that makes them infeasible on large networks, and the difficulty of implementation, which can be thought of as "programming overhead." MC dropout [Gal and Ghahramani, 2016] is popular because it sidesteps these obstacles. Nevertheless, dropout is often harmful to model performance when used in networks with batch normalization layers [Li et al., 2018], which are an indispensable part of modern neural networks. We construct a general variational family for ensemble-based Bayesian neural networks that encompasses dropout as a special case. We further present two specific members of this family that work well with batch normalization layers, while retaining the benefits of low parameter and programming overhead, comparable to non-Bayesian training. Our proposed methods improve predictive accuracy and achieve almost perfect calibration on a ResNet-18 trained with ImageNet.

</details>

<details>

<summary>2019-05-23 04:57:22 - A Taxonomy of Modeling Approaches for Systems-of-Systems Dynamic Architectures: Overview and Prospects</summary>

- *Ahmad Mohsin, Naeem Khalid Janjua, Syed MS Islam, Valdemar Vicente Graciano Neto*

- `1902.09090v4` - [abs](http://arxiv.org/abs/1902.09090v4) - [pdf](http://arxiv.org/pdf/1902.09090v4)

> Systems-of-Systems (SoS) result from the collaboration of independent Constituent Systems (CSs) to achieve particular missions. CSs are not totally known at design time, and may also leave or join SoS at runtime, which turns the SoS architecture to be inherently dynamic, forming new architectural configurations and impacting the overall system quality attributes (i.e. performance, security and reliability). Therefore, it is vital to model and evaluate the impact of these stochastic architectural changes on SoS properties at abstract level at the early stage in order to analyze and select appropriate architectural design. Architectural description languages (ADL) have been proposed and used to deal with SoS dynamic architectures. However, we still envision gaps to be bridged and challenges to be addressed in the forthcoming years. This paper presents a broad discussion on the state-of-the-art notations to model and analyze SoS dynamic architectures. The main contribution this paper is threefold: (i) providing results of a literature review on the support of available architecture modeling approaches for SoS and an analysis of their semantic extension to support specification of SoS dynamic architectures, and (ii) a corresponding taxonomy for modeling SoS obtained as a result of the literature review. Besides, we also discuss future directions and challenges to be overcome in the forthcoming years.

</details>

<details>

<summary>2019-05-23 08:50:15 - Theme-aware generation model for chinese lyrics</summary>

- *Jie Wang, Xinyan Zhao*

- `1906.02134v1` - [abs](http://arxiv.org/abs/1906.02134v1) - [pdf](http://arxiv.org/pdf/1906.02134v1)

> With rapid development of neural networks, deep-learning has been extended to various natural language generation fields, such as machine translation, dialogue generation and even literature creation. In this paper, we propose a theme-aware language generation model for Chinese music lyrics, which improves the theme-connectivity and coherence of generated paragraphs greatly. A multi-channel sequence-to-sequence (seq2seq) model encodes themes and previous sentences as global and local contextual information. Moreover, attention mechanism is incorporated for sequence decoding, enabling to fuse context into predicted next texts. To prepare appropriate train corpus, LDA (Latent Dirichlet Allocation) is applied for theme extraction. Generated lyrics is grammatically correct and semantically coherent with selected themes, which offers a valuable modelling method in other fields including multi-turn chatbots, long paragraph generation and etc.

</details>

<details>

<summary>2019-05-23 08:57:45 - MemoryRanger Prevents Hijacking FILE_OBJECT Structures in Windows Kernel</summary>

- *Igor Korkin*

- `1905.09543v1` - [abs](http://arxiv.org/abs/1905.09543v1) - [pdf](http://arxiv.org/pdf/1905.09543v1)

> Windows OS kernel memory is one of the main targets of cyber-attacks. By launching such attacks, hackers are succeeding in process privilege escalation and tampering with users data by accessing kernel mode memory. This paper considers a new example of such an attack, which results in access to the files opened in an exclusive mode. Windows built-in security features prevent such legal access, but attackers can circumvent them by patching dynamically allocated objects. The research shows that the Windows 10, version 1809 x64 is vulnerable to this attack. The paper provides an example of using MemoryRanger, a hypervisor-based solution to prevent such attack by running kernel-mode drivers in isolated kernel memory enclaves.

</details>

<details>

<summary>2019-05-23 09:20:52 - Implicit Background Estimation for Semantic Segmentation</summary>

- *Charles Lehman, Dogancan Temel, Ghassan AlRegib*

- `1905.13306v1` - [abs](http://arxiv.org/abs/1905.13306v1) - [pdf](http://arxiv.org/pdf/1905.13306v1)

> Scene understanding and semantic segmentation are at the core of many computer vision tasks, many of which, involve interacting with humans in potentially dangerous ways. It is therefore paramount that techniques for principled design of robust models be developed. In this paper, we provide analytic and empirical evidence that correcting potentially errant non-distinct mappings that result from the softmax function can result in improving robustness characteristics on a state-of-the-art semantic segmentation model with minimal impact to performance and minimal changes to the code base.

</details>

<details>

<summary>2019-05-23 09:44:50 - Knowledge Graph Embedding Bi-Vector Models for Symmetric Relation</summary>

- *Jinkui Yao, Lianghua Xu*

- `1905.09557v1` - [abs](http://arxiv.org/abs/1905.09557v1) - [pdf](http://arxiv.org/pdf/1905.09557v1)

> Knowledge graph embedding (KGE) models have been proposed to improve the performance of knowledge graph reasoning. However, there is a general phenomenon in most of KGEs, as the training progresses, the symmetric relations tend to zero vector, if the symmetric triples ratio is high enough in the dataset. This phenomenon causes subsequent tasks, e.g. link prediction etc., of symmetric relations to fail. The root cause of the problem is that KGEs do not utilize the semantic information of symmetric relations. We propose KGE bi-vector models, which represent the symmetric relations as vector pair, significantly increasing the processing capability of the symmetry relations. We generate the benchmark datasets based on FB15k and WN18 by completing the symmetric relation triples to verify models. The experiment results of our models clearly affirm the effectiveness and superiority of our models against baseline.

</details>

<details>

<summary>2019-05-23 17:40:41 - Interpreting Adversarially Trained Convolutional Neural Networks</summary>

- *Tianyuan Zhang, Zhanxing Zhu*

- `1905.09797v1` - [abs](http://arxiv.org/abs/1905.09797v1) - [pdf](http://arxiv.org/pdf/1905.09797v1)

> We attempt to interpret how adversarially trained convolutional neural networks (AT-CNNs) recognize objects. We design systematic approaches to interpret AT-CNNs in both qualitative and quantitative ways and compare them with normally trained models. Surprisingly, we find that adversarial training alleviates the texture bias of standard CNNs when trained on object recognition tasks, and helps CNNs learn a more shape-biased representation. We validate our hypothesis from two aspects. First, we compare the salience maps of AT-CNNs and standard CNNs on clean images and images under different transformations. The comparison could visually show that the prediction of the two types of CNNs is sensitive to dramatically different types of features. Second, to achieve quantitative verification, we construct additional test datasets that destroy either textures or shapes, such as style-transferred version of clean data, saturated images and patch-shuffled ones, and then evaluate the classification accuracy of AT-CNNs and normal CNNs on these datasets. Our findings shed some light on why AT-CNNs are more robust than those normally trained ones and contribute to a better understanding of adversarial training over CNNs from an interpretation perspective.

</details>

<details>

<summary>2019-05-23 20:59:12 - Compositional generalization in a deep seq2seq model by separating syntax and semantics</summary>

- *Jake Russin, Jason Jo, Randall C. O'Reilly, Yoshua Bengio*

- `1904.09708v3` - [abs](http://arxiv.org/abs/1904.09708v3) - [pdf](http://arxiv.org/pdf/1904.09708v3)

> Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in neuroscience suggesting separate brain systems for syntactic and semantic processing, we implement a modification to standard approaches in neural machine translation, imposing an analogous separation. The novel model, which we call Syntactic Attention, substantially outperforms standard methods in deep learning on the SCAN dataset, a compositional generalization task, without any hand-engineered features or additional supervision. Our work suggests that separating syntactic from semantic learning may be a useful heuristic for capturing compositional structure.

</details>

<details>

<summary>2019-05-24 02:02:35 - Implicit Label Augmentation on Partially Annotated Clips via Temporally-Adaptive Features Learning</summary>

- *Yongxi Lu, Ziyao Tang, Tara Javidi*

- `1905.10000v1` - [abs](http://arxiv.org/abs/1905.10000v1) - [pdf](http://arxiv.org/pdf/1905.10000v1)

> Partially annotated clips contain rich temporal contexts that can complement the sparse key frame annotations in providing supervision for model training. We present a novel paradigm called Temporally-Adaptive Features (TAF) learning that can utilize such data to learn better single frame models. By imposing distinct temporal change rate constraints on different factors in the model, TAF enables learning from unlabeled frames using context to enhance model accuracy. TAF generalizes "slow feature" learning and we present much stronger empirical evidence than prior works, showing convincing gains for the challenging semantic segmentation task over a variety of architecture designs and on two popular datasets. TAF can be interpreted as an implicit label augmentation method but is a more principled formulation compared to existing explicit augmentation techniques. Our work thus connects two promising methods that utilize partially annotated clips for single frame model training and can inspire future explorations in this direction.

</details>

<details>

<summary>2019-05-24 03:25:59 - Computationally Efficient Deep Neural Network for Computed Tomography Image Reconstruction</summary>

- *Dufan Wu, Kyungsang Kim, Quanzheng Li*

- `1810.03999v3` - [abs](http://arxiv.org/abs/1810.03999v3) - [pdf](http://arxiv.org/pdf/1810.03999v3)

> Deep-neural-network-based image reconstruction has demonstrated promising performance in medical imaging for under-sampled and low-dose scenarios. However, it requires large amount of memory and extensive time for the training. It is especially challenging to train the reconstruction networks for three-dimensional computed tomography (CT) because of the high resolution of CT images. The purpose of this work is to reduce the memory and time consumption of the training of the reconstruction networks for CT to make it practical for current hardware, while maintaining the quality of the reconstructed images.   We unrolled the proximal gradient descent algorithm for iterative image reconstruction to finite iterations and replaced the terms related to the penalty function with trainable convolutional neural networks (CNN). The network was trained greedily iteration by iteration in the image-domain on patches, which requires reasonable amount of memory and time on mainstream graphics processing unit (GPU). To overcome the local-minimum problem caused by greedy learning, we used deep UNet as the CNN and incorporated separable quadratic surrogate with ordered subsets for data fidelity, so that the solution could escape from easy local minimums and achieve better image quality.   The proposed method achieved comparable image quality with state-of-the-art neural network for CT image reconstruction on 2D sparse-view and limited-angle problems on the low-dose CT challenge dataset.

</details>

<details>

<summary>2019-05-24 05:29:33 - Outline Generation: Understanding the Inherent Content Structure of Documents</summary>

- *Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, Xueqi Cheng*

- `1905.10039v1` - [abs](http://arxiv.org/abs/1905.10039v1) - [pdf](http://arxiv.org/pdf/1905.10039v1)

> In this paper, we introduce and tackle the Outline Generation (OG) task, which aims to unveil the inherent content structure of a multi-paragraph document by identifying its potential sections and generating the corresponding section headings. Without loss of generality, the OG task can be viewed as a novel structured summarization task. To generate a sound outline, an ideal OG model should be able to capture three levels of coherence, namely the coherence between context paragraphs, that between a section and its heading, and that between context headings. The first one is the foundation for section identification, while the latter two are critical for consistent heading generation. In this work, we formulate the OG task as a hierarchical structured prediction problem, i.e., to first predict a sequence of section boundaries and then a sequence of section headings accordingly. We propose a novel hierarchical structured neural generation model, named HiStGen, for the task. Our model attempts to capture the three-level coherence via the following ways. First, we introduce a Markov paragraph dependency mechanism between context paragraphs for section identification. Second, we employ a section-aware attention mechanism to ensure the semantic coherence between a section and its heading. Finally, we leverage a Markov heading dependency mechanism and a review mechanism between context headings to improve the consistency and eliminate duplication between section headings. Besides, we build a novel WIKIOG dataset, a public collection which consists of over 1.75 million document-outline pairs for research on the OG task. Experimental results on our benchmark dataset demonstrate that our model can significantly outperform several state-of-the-art sequential generation models for the OG task.

</details>

<details>

<summary>2019-05-24 08:23:13 - PCC Net: Perspective Crowd Counting via Spatial Convolutional Network</summary>

- *Junyu Gao, Qi Wang, Xuelong Li*

- `1905.10085v1` - [abs](http://arxiv.org/abs/1905.10085v1) - [pdf](http://arxiv.org/pdf/1905.10085v1)

> Crowd counting from a single image is a challenging task due to high appearance similarity, perspective changes and severe congestion. Many methods only focus on the local appearance features and they cannot handle the aforementioned challenges. In order to tackle them, we propose a Perspective Crowd Counting Network (PCC Net), which consists of three parts: 1) Density Map Estimation (DME) focuses on learning very local features for density map estimation; 2) Random High-level Density Classification (R-HDC) extracts global features to predict the coarse density labels of random patches in images; 3) Fore-/Background Segmentation (FBS) encodes mid-level features to segments the foreground and background. Besides, the DULR module is embedded in PCC Net to encode the perspective changes on four directions (Down, Up, Left and Right). The proposed PCC Net is verified on five mainstream datasets, which achieves the state-of-the-art performance on the one and attains the competitive results on the other four datasets. The source code is available at https://github.com/gjy3035/PCC-Net.

</details>

<details>

<summary>2019-05-24 09:55:57 - Robust Semantic Segmentation in Adverse Weather Conditions by means of Sensor Data Fusion</summary>

- *Andreas Pfeuffer, Klaus Dietmayer*

- `1905.10117v1` - [abs](http://arxiv.org/abs/1905.10117v1) - [pdf](http://arxiv.org/pdf/1905.10117v1)

> A robust and reliable semantic segmentation in adverse weather conditions is very important for autonomous cars, but most state-of-the-art approaches only achieve high accuracy rates in optimal weather conditions. The reason is that they are only optimized for good weather conditions and given noise models. However, most of them fail, if data with unknown disturbances occur, and their performance decrease enormously. One possibility to still obtain reliable results is to observe the environment with different sensor types, such as camera and lidar, and to fuse the sensor data by means of neural networks, since different sensors behave differently in diverse weather conditions. Hence, the sensors can complement each other by means of an appropriate sensor data fusion. Nevertheless, the fusion-based approaches are still susceptible to disturbances and fail to classify disturbed image areas correctly. This problem can be solved by means of a special training method, the so called Robust Learning Method (RLM), a method by which the neural network learns to handle unknown noise. In this work, two different sensor fusion architectures for semantic segmentation are compared and evaluated on several datasets. Furthermore, it is shown that the RLM increases the robustness in adverse weather conditions enormously, and achieve good results although no disturbance model has been learned by the neural network.

</details>

<details>

<summary>2019-05-25 05:28:03 - Soft Contextual Data Augmentation for Neural Machine Translation</summary>

- *Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao Qin, Wengang Zhou, Xueqi Cheng, Tie-Yan Liu*

- `1905.10523v1` - [abs](http://arxiv.org/abs/1905.10523v1) - [pdf](http://arxiv.org/pdf/1905.10523v1)

> While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. In this paper, we present a novel data augmentation method for neural machine translation. Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced, the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation datasets demonstrate the superiority of our method over strong baselines.

</details>

<details>

<summary>2019-05-25 08:53:05 - Chinese User Service Intention Classification Based on Hybrid Neural Network</summary>

- *Shengbin Jia, Yang Xiang*

- `1809.09408v2` - [abs](http://arxiv.org/abs/1809.09408v2) - [pdf](http://arxiv.org/pdf/1809.09408v2)

> In order to satisfy the consumers' increasing personalized service demand, the Intelligent service has arisen. User service intention recognition is an important challenge for intelligent service system to provide precise service. It is difficult for the intelligent system to understand the semantics of user demand which leads to poor recognition effect, because of the noise in user requirement descriptions. Therefore, a hybrid neural network classification model based on BiLSTM and CNN is proposed to recognize users service intentions. The model can fuse the temporal semantics and spatial semantics of the user descriptions. The experimental results show that our model achieves a better effect compared with other models, reaching 0.94 on the F1 score.

</details>

<details>

<summary>2019-05-25 15:52:13 - Dynamic Epistemic Logic with ASP Updates: Application to Conditional Planning</summary>

- *Pedro Cabalar, Jorge Fandinno, Luis Fariñas del Cerro*

- `1905.10621v1` - [abs](http://arxiv.org/abs/1905.10621v1) - [pdf](http://arxiv.org/pdf/1905.10621v1)

> Dynamic Epistemic Logic (DEL) is a family of multimodal logics that has proved to be very successful for epistemic reasoning in planning tasks. In this logic, the agent's knowledge is captured by modal epistemic operators whereas the system evolution is described in terms of (some subset of) dynamic logic modalities in which actions are usually represented as semantic objects called event models. In this paper, we study a variant of DEL, that wecall DEL[ASP], where actions are syntactically described by using an Answer Set Programming (ASP) representation instead of event models. This representation directly inherits high level expressive features like indirect effects, qualifications, state constraints, defaults, or recursive fluents that are common in ASP descriptions of action domains. Besides, we illustrate how this approach can be applied for obtaining conditional plans in single-agent, partially observable domains where knowledge acquisition may be represented as indirect effects of actions.

</details>

<details>

<summary>2019-05-25 20:37:20 - MoMIT: Porting a JavaScript Interpreter on a Quarter Coin</summary>

- *Rodrigo Morales, Ruben Saborido, Yann-Gaël Guéhéneuc*

- `1906.03304v1` - [abs](http://arxiv.org/abs/1906.03304v1) - [pdf](http://arxiv.org/pdf/1906.03304v1)

> The Internet of Things (IoT) is a network of physical, heterogeneous, connected devices providing services through private networks and the Internet. It connects a range of new devices to the Internet so they can communicate with Web servers and other devices around the world. Today's standard platform for communicating Web pages and Web apps is JavaScript (JS) and extending the same standard platform to connect IoT devices seems more than appropriate. However, porting JS applications to the large variety of IoT devices, specifically on System-on-a-Chip (SoCs) devices (\eg~Arduino Uno, Particle \photon), is challenging because these devices are constrained in terms of memory and storage capacity. Running JS applications adds an overhead of resources to deploy a code interpreter on the devices. Also, running JS applications may not be possible ``as is'' on some device missing some hardware/software capabilities. To address this problem, we propose \momit~a multiobjective optimization approach to miniaturize JS applications to run on IoT constrained devices. To validate \momit, we miniaturize a JS interpreter to execute a testbed comprised of 23 applications and measure their performances before and after applying the miniaturization process. We implement \momit~using three different search algorithms and found that it can reduce code size, memory usage, and CPU time by median values of 31\%, 56\%, and 36\% respectively. Finally, MoMIT ported the miniaturized JS interpreters up to to 2 SoCs additional devices, in comparison of using default JS interpreter features.

</details>

<details>

<summary>2019-05-25 22:36:05 - Sherlock: A Deep Learning Approach to Semantic Data Type Detection</summary>

- *Madelon Hulsebos, Kevin Hu, Michiel Bakker, Emanuel Zgraggen, Arvind Satyanarayan, Tim Kraska, Çağatay Demiralp, César Hidalgo*

- `1905.10688v1` - [abs](http://arxiv.org/abs/1905.10688v1) - [pdf](http://arxiv.org/pdf/1905.10688v1)

> Correctly detecting the semantic type of data columns is crucial for data science tasks such as automated data cleaning, schema matching, and data discovery. Existing data preparation and analysis systems rely on dictionary lookups and regular expression matching to detect semantic types. However, these matching-based approaches often are not robust to dirty data and only detect a limited number of types. We introduce Sherlock, a multi-input deep neural network for detecting semantic types. We train Sherlock on $686,765$ data columns retrieved from the VizNet corpus by matching $78$ semantic types from DBpedia to column headers. We characterize each matched column with $1,588$ features describing the statistical properties, character distributions, word embeddings, and paragraph vectors of column values. Sherlock achieves a support-weighted F$_1$ score of $0.89$, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations.

</details>

<details>

<summary>2019-05-25 23:24:15 - Adversarial Distillation for Ordered Top-k Attacks</summary>

- *Zekun Zhang, Tianfu Wu*

- `1905.10695v1` - [abs](http://arxiv.org/abs/1905.10695v1) - [pdf](http://arxiv.org/pdf/1905.10695v1)

> Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, especially white-box targeted attacks. One scheme of learning attacks is to design a proper adversarial objective function that leads to the imperceptible perturbation for any test image (e.g., the Carlini-Wagner (C&W) method). Most methods address targeted attacks in the Top-1 manner. In this paper, we propose to learn ordered Top-k attacks (k>= 1) for image classification tasks, that is to enforce the Top-k predicted labels of an adversarial example to be the k (randomly) selected and ordered labels (the ground-truth label is exclusive). To this end, we present an adversarial distillation framework: First, we compute an adversarial probability distribution for any given ordered Top-k targeted labels with respect to the ground-truth of a test image. Then, we learn adversarial examples by minimizing the Kullback-Leibler (KL) divergence together with the perturbation energy penalty, similar in spirit to the network distillation method. We explore how to leverage label semantic similarities in computing the targeted distributions, leading to knowledge-oriented attacks. In experiments, we thoroughly test Top-1 and Top-5 attacks in the ImageNet-1000 validation dataset using two popular DNNs trained with clean ImageNet-1000 train dataset, ResNet-50 and DenseNet-121. For both models, our proposed adversarial distillation approach outperforms the C&W method in the Top-1 setting, as well as other baseline methods. Our approach shows significant improvement in the Top-5 setting against a strong modified C&W method.

</details>

<details>

<summary>2019-05-26 04:34:07 - TEE-aided Write Protection Against Privileged Data Tampering</summary>

- *Lianying Zhao, Mohammad Mannan*

- `1905.10723v1` - [abs](http://arxiv.org/abs/1905.10723v1) - [pdf](http://arxiv.org/pdf/1905.10723v1)

> Unauthorized data alteration has been a longstanding threat since the emergence of malware. System and application software can be reinstalled and hardware can be replaced, but user data is priceless in many cases. Especially in recent years, ransomware has become high-impact due to its direct monetization model. State-of-the-art defenses are mostly based on known signature or behavior analysis, and more importantly, require an uncompromised OS kernel. However, malware with the highest software privileges has shown its obvious existence. We propose to move from current detection/recovery based mechanisms to data loss prevention, where the focus is on armoring data instead of counteracting malware. Our solution, Inuksuk, relies on today's Trusted Execution Environments (TEEs), as available both on the CPU and storage device, to achieve programmable write protection. We back up a copy of user-selected files as write-protected at all times, and subsequent updates are written as new versions securely through TEE. We implement Inuksuk on Windows 7 and 10, and Linux (Ubuntu); our core design is OS and application agnostic, and incurs no run-time performance penalty for applications. File transfer disruption can be eliminated or alleviated through access modes and customizable update policies (e.g., interval, granularity). For Inuksuk's adoptability in modern OSes, we have also ported Flicker (EuroSys 2008), a defacto standard tool for in-OS privileged TEE management, to the latest 64-bit Windows.

</details>

<details>

<summary>2019-05-26 04:56:24 - Programming with Applicative-like expressions</summary>

- *Jan Malakhovski, Sergei Soloviev*

- `1905.10728v1` - [abs](http://arxiv.org/abs/1905.10728v1) - [pdf](http://arxiv.org/pdf/1905.10728v1)

> The fact that Applicative type class allows one to express simple parsers in a variable-less combinatorial style is well appreciated among Haskell programmers for its conceptual simplicity, ease of use, and usefulness for semi-automated code generation (metaprogramming).   We notice that such Applicative computations can be interpreted as providing a mechanism to construct a data type with "ports" "pluggable" by subcomputations. We observe that it is this property that makes them so much more convenient in practice than the usual way of building the same computations using conventional composition. We distill this observation into a more general algebraic structure of (and/or technique for expressing) "Applicative-like" computations and demonstrate several other instances of this structure.   Our interest in all of this comes from the fact that the aforementioned instances allow us to express arbitrary transformations between simple data types of a single constructor (similarly to how Applicative parsing allows to transform from streams of Chars to such data types) using a style that closely follows conventional Applicative computations, thus greatly simplifying (if not completely automating away) a lot of boiler-plate code present in many functional programs.

</details>

<details>

<summary>2019-05-26 14:54:52 - Evaluation of basic modules for isolated spelling error correction in Polish texts</summary>

- *Szymon Rutkowski*

- `1905.10810v1` - [abs](http://arxiv.org/abs/1905.10810v1) - [pdf](http://arxiv.org/pdf/1905.10810v1)

> Spelling error correction is an important problem in natural language processing, as a prerequisite for good performance in downstream tasks as well as an important feature in user-facing applications. For texts in Polish language, there exist works on specific error correction solutions, often developed for dealing with specialized corpora, but not evaluations of many different approaches on big resources of errors. We begin to address this problem by testing some basic and promising methods on PlEWi, a corpus of annotated spelling extracted from Polish Wikipedia. These modules may be further combined with appropriate solutions for error detection and context awareness. Following our results, combining edit distance with cosine distance of semantic vectors may be suggested for interpretable systems, while an LSTM, particularly enhanced by ELMo embeddings, seems to offer the best raw performance.

</details>

<details>

<summary>2019-05-26 23:57:07 - Learning Scalable and Precise Representation of Program Semantics</summary>

- *Ke Wang*

- `1905.05251v3` - [abs](http://arxiv.org/abs/1905.05251v3) - [pdf](http://arxiv.org/pdf/1905.05251v3)

> Neural program embedding has shown potential in aiding the analysis of large-scale, complicated software. Newly proposed deep neural architectures pride themselves on learning program semantics rather than superficial syntactic features. However, by considering the source code only, the vast majority of neural networks do not capture a deep, precise representation of program semantics. In this paper, we present \dypro, a novel deep neural network that learns from program execution traces. Compared to the prior dynamic models, not only is \dypro capable of generalizing across multiple executions for learning a program's dynamic semantics in its entirety, but \dypro is also more efficient when dealing with programs yielding long execution traces. For evaluation, we task \dypro with semantic classification (i.e. categorizing programs based on their semantics) and compared it against two prominent static models: Gated Graph Neural Network and TreeLSTM. We find that \dypro achieves the highest prediction accuracy among all models. To further reveal the capacity of all aforementioned deep neural architectures, we examine if the models can learn to detect deeper semantic properties of a program. In particular given a task of recognizing loop invariants, we show \dypro beats all static models by a wide margin.

</details>

<details>

<summary>2019-05-27 02:44:56 - Utility-Optimized Local Differential Privacy Mechanisms for Distribution Estimation</summary>

- *Takao Murakami, Yusuke Kawamoto*

- `1807.11317v7` - [abs](http://arxiv.org/abs/1807.11317v7) - [pdf](http://arxiv.org/pdf/1807.11317v7)

> LDP (Local Differential Privacy) has been widely studied to estimate statistics of personal data (e.g., distribution underlying the data) while protecting users' privacy. Although LDP does not require a trusted third party, it regards all personal data equally sensitive, which causes excessive obfuscation hence the loss of utility. In this paper, we introduce the notion of ULDP (Utility-optimized LDP), which provides a privacy guarantee equivalent to LDP only for sensitive data. We first consider the setting where all users use the same obfuscation mechanism, and propose two mechanisms providing ULDP: utility-optimized randomized response and utility-optimized RAPPOR. We then consider the setting where the distinction between sensitive and non-sensitive data can be different from user to user. For this setting, we propose a personalized ULDP mechanism with semantic tags to estimate the distribution of personal data with high utility while keeping secret what is sensitive for each user. We show theoretically and experimentally that our mechanisms provide much higher utility than the existing LDP mechanisms when there are a lot of non-sensitive data. We also show that when most of the data are non-sensitive, our mechanisms even provide almost the same utility as non-private mechanisms in the low privacy regime.

</details>

<details>

<summary>2019-05-27 05:51:11 - Attention-based Supply-Demand Prediction for Autonomous Vehicles</summary>

- *Zikai Zhang, Yidong Li, Hairong Dong, Yizhe You, Fengping Zhao*

- `1905.10983v1` - [abs](http://arxiv.org/abs/1905.10983v1) - [pdf](http://arxiv.org/pdf/1905.10983v1)

> As one of the important functions of the intelligent transportation system (ITS), supply-demand prediction for autonomous vehicles provides a decision basis for its control. In this paper, we present two prediction models (i.e. ARLP model and Advanced ARLP model) based on two system environments that only the current day's historical data is available or several days' historical data are available. These two models jointly consider the spatial, temporal, and semantic relations. Spatial dependency is captured with residual network and dimension reduction. Short term temporal dependency is captured with LSTM. Long term temporal dependency and temporal shifting are captured with LSTM and attention mechanism. Semantic dependency is captured with multi-attention mechanism and autocorrelation coefficient method. Extensive experiments show that our frameworks provide more accurate and stable prediction results than the existing methods.

</details>

<details>

<summary>2019-05-27 10:22:20 - A Self-Attention Joint Model for Spoken Language Understanding in Situational Dialog Applications</summary>

- *Mengyang Chen, Jin Zeng, Jie Lou*

- `1905.11393v1` - [abs](http://arxiv.org/abs/1905.11393v1) - [pdf](http://arxiv.org/pdf/1905.11393v1)

> Spoken language understanding (SLU) acts as a critical component in goal-oriented dialog systems. It typically involves identifying the speakers intent and extracting semantic slots from user utterances, which are known as intent detection (ID) and slot filling (SF). SLU problem has been intensively investigated in recent years. However, these methods just constrain SF results grammatically, solve ID and SF independently, or do not fully utilize the mutual impact of the two tasks. This paper proposes a multi-head self-attention joint model with a conditional random field (CRF) layer and a prior mask. The experiments show the effectiveness of our model, as compared with state-of-the-art models. Meanwhile, online education in China has made great progress in the last few years. But there are few intelligent educational dialog applications for students to learn foreign languages. Hence, we design an intelligent dialog robot equipped with different scenario settings to help students learn communication skills.

</details>

<details>

<summary>2019-05-27 16:33:41 - SparseFool: a few pixels make a big difference</summary>

- *Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard*

- `1811.02248v4` - [abs](http://arxiv.org/abs/1811.02248v4) - [pdf](http://arxiv.org/pdf/1811.02248v4)

> Deep Neural Networks have achieved extraordinary results on image classification tasks, but have been shown to be vulnerable to attacks with carefully crafted perturbations of the input data. Although most attacks usually change values of many image's pixels, it has been shown that deep networks are also vulnerable to sparse alterations of the input. However, no computationally efficient method has been proposed to compute sparse perturbations. In this paper, we exploit the low mean curvature of the decision boundary, and propose SparseFool, a geometry inspired sparse attack that controls the sparsity of the perturbations. Extensive evaluations show that our approach computes sparse perturbations very fast, and scales efficiently to high dimensional data. We further analyze the transferability and the visual effects of the perturbations, and show the existence of shared semantic information across the images and the networks. Finally, we show that adversarial training can only slightly improve the robustness against sparse additive perturbations computed with SparseFool.

</details>

<details>

<summary>2019-05-27 18:44:54 - COSET: A Benchmark for Evaluating Neural Program Embeddings</summary>

- *Ke Wang, Mihai Christodorescu*

- `1905.11445v1` - [abs](http://arxiv.org/abs/1905.11445v1) - [pdf](http://arxiv.org/pdf/1905.11445v1)

> Neural program embedding can be helpful in analyzing large software, a task that is challenging for traditional logic-based program analyses due to their limited scalability. A key focus of recent machine-learning advances in this area is on modeling program semantics instead of just syntax. Unfortunately evaluating such advances is not obvious, as program semantics does not lend itself to straightforward metrics. In this paper, we introduce a benchmarking framework called COSET for standardizing the evaluation of neural program embeddings. COSET consists of a diverse dataset of programs in source-code format, labeled by human experts according to a number of program properties of interest. A point of novelty is a suite of program transformations included in COSET. These transformations when applied to the base dataset can simulate natural changes to program code due to optimization and refactoring and can serve as a "debugging" tool for classification mistakes. We conducted a pilot study on four prominent models: TreeLSTM, gated graph neural network (GGNN), AST-Path neural network (APNN), and DYPRO. We found that COSET is useful in identifying the strengths and limitations of each model and in pinpointing specific syntactic and semantic characteristics of programs that pose challenges.

</details>

<details>

<summary>2019-05-27 21:25:51 - A Knowledge Graph-based Approach for Exploring the U.S. Opioid Epidemic</summary>

- *Maulik R. Kamdar, Tymor Hamamsy, Shea Shelton, Ayin Vala, Tome Eftimov, James Zou, Suzanne Tamang*

- `1905.11513v1` - [abs](http://arxiv.org/abs/1905.11513v1) - [pdf](http://arxiv.org/pdf/1905.11513v1)

> The United States is in the midst of an opioid epidemic with recent estimates indicating that more than 130 people die every day due to drug overdose. The over-prescription and addiction to opioid painkillers, heroin, and synthetic opioids, has led to a public health crisis and created a huge social and economic burden. Statistical learning methods that use data from multiple clinical centers across the US to detect opioid over-prescribing trends and predict possible opioid misuse are required. However, the semantic heterogeneity in the representation of clinical data across different centers makes the development and evaluation of such methods difficult and non-trivial. We create the Opioid Drug Knowledge Graph (ODKG) -- a network of opioid-related drugs, active ingredients, formulations, combinations, and brand names. We use the ODKG to normalize drug strings in a clinical data warehouse consisting of patient data from over 400 healthcare facilities in 42 different states. We showcase the use of ODKG to generate summary statistics of opioid prescription trends across US regions. These methods and resources can aid the development of advanced and scalable models to monitor the opioid epidemic and to detect illicit opioid misuse behavior. Our work is relevant to policymakers and pain researchers who wish to systematically assess factors that contribute to opioid over-prescribing and iatrogenic opioid addiction in the US.

</details>

<details>

<summary>2019-05-27 22:51:39 - Compositional pre-training for neural semantic parsing</summary>

- *Amir Ziai*

- `1905.11531v1` - [abs](http://arxiv.org/abs/1905.11531v1) - [pdf](http://arxiv.org/pdf/1905.11531v1)

> Semantic parsing is the process of translating natural language utterances into logical forms, which has many important applications such as question answering and instruction following. Sequence-to-sequence models have been very successful across many NLP tasks. However, a lack of task-specific prior knowledge can be detrimental to the performance of these models. Prior work has used frameworks for inducing grammars over the training examples, which capture conditional independence properties that the model can leverage. Inspired by the recent success stories such as BERT we set out to extend this augmentation framework into two stages. The first stage is to pre-train using a corpus of augmented examples in an unsupervised manner. The second stage is to fine-tune to a domain-specific task. In addition, since the pre-training stage is separate from the training on the main task we also expand the universe of possible augmentations without causing catastrophic inference. We also propose a novel data augmentation strategy that interchanges tokens that co-occur in similar contexts to produce new training pairs. We demonstrate that the proposed two-stage framework is beneficial for improving the parsing accuracy in a standard dataset called GeoQuery for the task of generating logical forms from a set of questions about the US geography.

</details>

<details>

<summary>2019-05-28 09:16:56 - Deep Scale-spaces: Equivariance Over Scale</summary>

- *Daniel E. Worrall, Max Welling*

- `1905.11697v1` - [abs](http://arxiv.org/abs/1905.11697v1) - [pdf](http://arxiv.org/pdf/1905.11697v1)

> We introduce deep scale-spaces (DSS), a generalization of convolutional neural networks, exploiting the scale symmetry structure of conventional image recognition tasks. Put plainly, the class of an image is invariant to the scale at which it is viewed. We construct scale equivariant cross-correlations based on a principled extension of convolutions, grounded in the theory of scale-spaces and semigroups. As a very basic operation, these cross-correlations can be used in almost any modern deep learning architecture in a plug-and-play manner. We demonstrate our networks on the Patch Camelyon and Cityscapes datasets, to prove their utility and perform introspective studies to further understand their properties.

</details>

<details>

<summary>2019-05-28 15:10:30 - Miss Tools and Mr Fruit: Emergent communication in agents learning about object affordances</summary>

- *Diane Bouchacourt, Marco Baroni*

- `1905.11871v1` - [abs](http://arxiv.org/abs/1905.11871v1) - [pdf](http://arxiv.org/pdf/1905.11871v1)

> Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge.

</details>

<details>

<summary>2019-05-28 17:49:19 - Empirical Review of Java Program Repair Tools: A Large-Scale Experiment on 2,141 Bugs and 23,551 Repair Attempts</summary>

- *Thomas Durieux, Fernanda Madeiral, Matias Martinez, Rui Abreu*

- `1905.11973v1` - [abs](http://arxiv.org/abs/1905.11973v1) - [pdf](http://arxiv.org/pdf/1905.11973v1)

> In the past decade, research on test-suite-based automatic program repair has grown significantly. Each year, new approaches and implementations are featured in major software engineering venues. However, most of those approaches are evaluated on a single benchmark of bugs, which are also rarely reproduced by other researchers. In this paper, we present a large-scale experiment using 11 Java test-suite-based repair tools and 5 benchmarks of bugs. Our goal is to have a better understanding of the current state of automatic program repair tools on a large diversity of benchmarks. Our investigation is guided by the hypothesis that the repairability of repair tools might not be generalized across different benchmarks of bugs. We found that the 11 tools 1) are able to generate patches for 21% of the bugs from the 5 benchmarks, and 2) have better performance on Defects4J compared to other benchmarks, by generating patches for 47% of the bugs from Defects4J compared to 10-30% of bugs from the other benchmarks. Our experiment comprises 23,551 repair attempts in total, which we used to find the causes of non-patch generation. These causes are reported in this paper, which can help repair tool designers to improve their techniques and tools.

</details>

<details>

<summary>2019-05-28 18:51:14 - Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases</summary>

- *Yu Chen, Lingfei Wu, Mohammed J. Zaki*

- `1903.02188v3` - [abs](http://arxiv.org/abs/1903.02188v3) - [pdf](http://arxiv.org/pdf/1903.02188v3)

> When answering natural language questions over knowledge bases (KBs), different question components and KB aspects play different roles. However, most existing embedding-based methods for knowledge base question answering (KBQA) ignore the subtle inter-relationships between the question and the KB (e.g., entity types, relation paths and context). In this work, we propose to directly model the two-way flow of interactions between the questions and the KB via a novel Bidirectional Attentive Memory Network, called BAMnet. Requiring no external resources and only very few hand-crafted features, on the WebQuestions benchmark, our method significantly outperforms existing information-retrieval based methods, and remains competitive with (hand-crafted) semantic parsing based methods. Also, since we use attention mechanisms, our method offers better interpretability compared to other baselines.

</details>

<details>

<summary>2019-05-28 20:21:19 - SEMA: an Extended Semantic Evaluation Metric for AMR</summary>

- *Rafael T. Anchieta, Marco A. S. Cabezudo, Thiago A. S. Pardo*

- `1905.12069v1` - [abs](http://arxiv.org/abs/1905.12069v1) - [pdf](http://arxiv.org/pdf/1905.12069v1)

> Abstract Meaning Representation (AMR) is a recently designed semantic representation language intended to capture the meaning of a sentence, which may be represented as a single-rooted directed acyclic graph with labeled nodes and edges. The automatic evaluation of this structure plays an important role in the development of better systems, as well as for semantic annotation. Despite there is one available metric, smatch, it has some drawbacks. For instance, smatch creates a self-relation on the root of the graph, has weights for different error types, and does not take into account the dependence of the elements in the AMR structure. With these drawbacks, smatch masks several problems of the AMR parsers and distorts the evaluation of the AMRs. In view of this, in this paper, we introduce an extended metric to evaluate AMR parsers, which deals with the drawbacks of the smatch metric. Finally, we compare both metrics, using four well-known AMR parsers, and we argue that our metric is more refined, robust, fairer, and faster than smatch.

</details>

<details>

<summary>2019-05-28 21:32:02 - Parallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae</summary>

- *Piero Molino, Yang Wang, Jiawei Zhang*

- `1905.12099v1` - [abs](http://arxiv.org/abs/1905.12099v1) - [pdf](http://arxiv.org/pdf/1905.12099v1)

> Embeddings are a fundamental component of many modern machine learning and natural language processing models. Understanding them and visualizing them is essential for gathering insights about the information they capture and the behavior of the models. State of the art in analyzing embeddings consists in projecting them in two-dimensional planes without any interpretable semantics associated to the axes of the projection, which makes detailed analyses and comparison among multiple sets of embeddings challenging. In this work, we propose to use explicit axes defined as algebraic formulae over embeddings to project them into a lower dimensional, but semantically meaningful subspace, as a simple yet effective analysis and visualization methodology. This methodology assigns an interpretable semantics to the measures of variability and the axes of visualizations, allowing for both comparisons among different sets of embeddings and fine-grained inspection of the embedding spaces. We demonstrate the power of the proposed methodology through a series of case studies that make use of visualizations constructed around the underlying methodology and through a user study. The results show how the methodology is effective at providing more profound insights than classical projection methods and how it is widely applicable to many other use cases.

</details>

<details>

<summary>2019-05-28 23:40:33 - Reasoning Visual Dialogs with Structural and Partial Observations</summary>

- *Zilong Zheng, Wenguan Wang, Siyuan Qi, Song-Chun Zhu*

- `1904.05548v2` - [abs](http://arxiv.org/abs/1904.05548v2) - [pdf](http://arxiv.org/pdf/1904.05548v2)

> We propose a novel model to address the task of Visual Dialog which exhibits complex dialog structures. To obtain a reasonable answer based on the current question and the dialog history, the underlying semantic dependencies between dialog entities are essential. In this paper, we explicitly formalize this task as inference in a graphical model with partially observed nodes and unknown graph structures (relations in dialog). The given dialog entities are viewed as the observed nodes. The answer to a given question is represented by a node with missing value. We first introduce an Expectation Maximization algorithm to infer both the underlying dialog structures and the missing node values (desired answers). Based on this, we proceed to propose a differentiable graph neural network (GNN) solution that approximates this process. Experiment results on the VisDial and VisDial-Q datasets show that our model outperforms comparative methods. It is also observed that our method can infer the underlying dialog structure for better dialog reasoning.

</details>

<details>

<summary>2019-05-29 07:55:17 - Learning Multilingual Word Embeddings Using Image-Text Data</summary>

- *Karan Singhal, Karthik Raman, Balder ten Cate*

- `1905.12260v1` - [abs](http://arxiv.org/abs/1905.12260v1) - [pdf](http://arxiv.org/pdf/1905.12260v1)

> There has been significant interest recently in learning multilingual word embeddings -- in which semantically similar words across languages have similar embeddings. State-of-the-art approaches have relied on expensive labeled data, which is unavailable for low-resource languages, or have involved post-hoc unification of monolingual embeddings. In the present paper, we investigate the efficacy of multilingual embeddings learned from weakly-supervised image-text data. In particular, we propose methods for learning multilingual embeddings using image-text data, by enforcing similarity between the representations of the image and that of the text. Our experiments reveal that even without using any expensive labeled data, a bag-of-words-based embedding model trained on image-text data achieves performance comparable to the state-of-the-art on crosslingual semantic similarity tasks.

</details>

<details>

<summary>2019-05-29 12:42:17 - Cooperative Learning of Disjoint Syntax and Semantics</summary>

- *Serhii Havrylov, Germán Kruszewski, Armand Joulin*

- `1902.09393v2` - [abs](http://arxiv.org/abs/1902.09393v2) - [pdf](http://arxiv.org/pdf/1902.09393v2)

> There has been considerable attention devoted to models that learn to jointly infer an expression's syntactic structure and its semantics. Yet, \citet{NangiaB18} has recently shown that the current best systems fail to learn the correct parsing strategy on mathematical expressions generated from a simple context-free grammar. In this work, we present a recursive model inspired by \newcite{ChoiYL18} that reaches near perfect accuracy on this task. Our model is composed of two separated modules for syntax and semantics. They are cooperatively trained with standard continuous and discrete optimization schemes. Our model does not require any linguistic structure for supervision and its recursive nature allows for out-of-domain generalization with little loss in performance. Additionally, our approach performs competitively on several natural language tasks, such as Natural Language Inference or Sentiment Analysis.

</details>

<details>

<summary>2019-05-29 14:16:28 - Neural Review Rating Prediction with Hierarchical Attentions and Latent Factors</summary>

- *Xianchen Wang, Hongtao Liu, Peiyi Wang, Fangzhao Wu, Hongyan Xu, Wenjun Wang, Xing Xie*

- `1906.01511v1` - [abs](http://arxiv.org/abs/1906.01511v1) - [pdf](http://arxiv.org/pdf/1906.01511v1)

> Text reviews can provide rich useful semantic information for modeling users and items, which can benefit rating prediction in recommendation. Different words and reviews may have different informativeness for users or items. Besides, different users and items should be personalized. Most existing works regard all reviews equally or utilize a general attention mechanism. In this paper, we propose a hierarchical attention model fusing latent factor model for rating prediction with reviews, which can focus on important words and informative reviews. Specially, we use the factor vectors of Latent Factor Model to guide the attention network and combine the factor vectors with feature representation learned from reviews to predict the final ratings. Experiments on real-world datasets validate the effectiveness of our approach.

</details>

<details>

<summary>2019-05-29 18:08:07 - A Survey on Semantic Parsing</summary>

- *Aishwarya Kamath, Rajarshi Das*

- `1812.00978v3` - [abs](http://arxiv.org/abs/1812.00978v3) - [pdf](http://arxiv.org/pdf/1812.00978v3)

> A significant amount of information in today's world is stored in structured and semi-structured knowledge bases. Efficient and simple methods to query them are essential and must not be restricted to only those who have expertise in formal query languages. The field of semantic parsing deals with converting natural language utterances to logical forms that can be easily executed on a knowledge base. In this survey, we examine the various components of a semantic parsing system and discuss prominent work ranging from the initial rule based methods to the current neural approaches to program synthesis. We also discuss methods that operate using varying levels of supervision and highlight the key challenges involved in the learning of such systems.

</details>

<details>

<summary>2019-05-29 20:49:53 - Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding</summary>

- *Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl Vondrick, Shih-Fu Chang*

- `1811.11683v2` - [abs](http://arxiv.org/abs/1811.11683v2) - [pdf](http://arxiv.org/pdf/1811.11683v2)

> We address the problem of phrase grounding by lear ing a multi-level common semantic space shared by the textual and visual modalities. We exploit multiple levels of feature maps of a Deep Convolutional Neural Network, as well as contextualized word and sentence embeddings extracted from a character-based language model. Following dedicated non-linear mappings for visual features at each level, word, and sentence embeddings, we obtain multiple instantiations of our common semantic space in which comparisons between any target text and the visual content is performed with cosine similarity. We guide the model by a multi-level multimodal attention mechanism which outputs attended visual features at each level. The best level is chosen to be compared with text content for maximizing the pertinence scores of image-sentence pairs of the ground truth. Experiments conducted on three publicly available datasets show significant performance gains (20%-60% relative) over the state-of-the-art in phrase localization and set a new performance record on those datasets. We provide a detailed ablation study to show the contribution of each element of our approach and release our code on GitHub.

</details>

<details>

<summary>2019-05-29 22:15:38 - Unsupervised Paraphrasing without Translation</summary>

- *Aurko Roy, David Grangier*

- `1905.12752v1` - [abs](http://arxiv.org/abs/1905.12752v1) - [pdf](http://arxiv.org/pdf/1905.12752v1)

> Paraphrasing exemplifies the ability to abstract semantic content from surface forms. Recent work on automatic paraphrasing is dominated by methods leveraging Machine Translation (MT) as an intermediate step. This contrasts with humans, who can paraphrase without being bilingual. This work proposes to learn paraphrasing models from an unlabeled monolingual corpus only. To that end, we propose a residual variant of vector-quantized variational auto-encoder.   We compare with MT-based approaches on paraphrase identification, generation, and training augmentation. Monolingual paraphrasing outperforms unsupervised translation in all settings. Comparisons with supervised translation are more mixed: monolingual paraphrasing is interesting for identification and augmentation; supervised translation is superior for generation.

</details>

<details>

<summary>2019-05-29 22:43:29 - Batch weight for domain adaptation with mass shift</summary>

- *Mikołaj Bińkowski, R Devon Hjelm, Aaron Courville*

- `1905.12760v1` - [abs](http://arxiv.org/abs/1905.12760v1) - [pdf](http://arxiv.org/pdf/1905.12760v1)

> Unsupervised domain transfer is the task of transferring or translating samples from a source distribution to a different target distribution. Current solutions unsupervised domain transfer often operate on data on which the modes of the distribution are well-matched, for instance have the same frequencies of classes between source and target distributions. However, these models do not perform well when the modes are not well-matched, as would be the case when samples are drawn independently from two different, but related, domains. This mode imbalance is problematic as generative adversarial networks (GANs), a successful approach in this setting, are sensitive to mode frequency, which results in a mismatch of semantics between source samples and generated samples of the target distribution. We propose a principled method of re-weighting training samples to correct for such mass shift between the transferred distributions, which we call batch-weight. We also provide rigorous probabilistic setting for domain transfer and new simplified objective for training transfer networks, an alternative to complex, multi-component loss functions used in the current state-of-the art image-to-image translation models. The new objective stems from the discrimination of joint distributions and enforces cycle-consistency in an abstract, high-level, rather than pixel-wise, sense. Lastly, we experimentally show the effectiveness of the proposed methods in several image-to-image translation tasks.

</details>

<details>

<summary>2019-05-30 12:05:33 - Learning by Active Nonlinear Diffusion</summary>

- *Mauro Maggioni, James M. Murphy*

- `1905.12989v1` - [abs](http://arxiv.org/abs/1905.12989v1) - [pdf](http://arxiv.org/pdf/1905.12989v1)

> This article proposes an active learning method for high dimensional data, based on intrinsic data geometries learned through diffusion processes on graphs. Diffusion distances are used to parametrize low-dimensional structures on the dataset, which allow for high-accuracy labelings of the dataset with only a small number of carefully chosen labels. The geometric structure of the data suggests regions that have homogeneous labels, as well as regions with high label complexity that should be queried for labels. The proposed method enjoys theoretical performance guarantees on a general geometric data model, in which clusters corresponding to semantically meaningful classes are permitted to have nonlinear geometries, high ambient dimensionality, and suffer from significant noise and outlier corruption. The proposed algorithm is implemented in a manner that is quasilinear in the number of unlabeled data points, and exhibits competitive empirical performance on synthetic datasets and real hyperspectral remote sensing images.

</details>

<details>

<summary>2019-05-30 13:07:58 - Multiple Character Embeddings for Chinese Word Segmentation</summary>

- *Jingkang Wang, Jianing Zhou, Jie Zhou, Gongshen Liu*

- `1808.04963v3` - [abs](http://arxiv.org/abs/1808.04963v3) - [pdf](http://arxiv.org/pdf/1808.04963v3)

> Chinese word segmentation (CWS) is often regarded as a character-based sequence labeling task in most current works which have achieved great success with the help of powerful neural networks. However, these works neglect an important clue: Chinese characters incorporate both semantic and phonetic meanings. In this paper, we introduce multiple character embeddings including Pinyin Romanization and Wubi Input, both of which are easily accessible and effective in depicting semantics of characters. We propose a novel shared Bi-LSTM-CRF model to fuse linguistic features efficiently by sharing the LSTM network during the training procedure. Extensive experiments on five corpora show that extra embeddings help obtain a significant improvement in labeling accuracy. Specifically, we achieve the state-of-the-art performance in AS and CityU corpora with F1 scores of 96.9 and 97.3, respectively without leveraging any external lexical resources.

</details>

<details>

<summary>2019-05-30 13:32:47 - Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks</summary>

- *Charith Mendis, Alex Renda, Saman Amarasinghe, Michael Carbin*

- `1808.07412v2` - [abs](http://arxiv.org/abs/1808.07412v2) - [pdf](http://arxiv.org/pdf/1808.07412v2)

> Predicting the number of clock cycles a processor takes to execute a block of assembly instructions in steady state (the throughput) is important for both compiler designers and performance engineers. Building an analytical model to do so is especially complicated in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures in that it is tedious, error prone, and must be performed from scratch for each processor generation. In this paper we present Ithemal, the first tool which learns to predict the throughput of a set of instructions. Ithemal uses a hierarchical LSTM--based approach to predict throughput based on the opcodes and operands of instructions in a basic block. We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in compiler backends and static machine code analyzers. In particular, our model has less than half the error of state-of-the-art analytical models (LLVM's llvm-mca and Intel's IACA). Ithemal is also able to predict these throughput values just as fast as the aforementioned tools, and is easily ported across a variety of processor microarchitectures with minimal developer effort.

</details>

<details>

<summary>2019-05-30 14:39:53 - Characterizing the Shape of Activation Space in Deep Neural Networks</summary>

- *Thomas Gebhart, Paul Schrater, Alan Hylton*

- `1901.09496v2` - [abs](http://arxiv.org/abs/1901.09496v2) - [pdf](http://arxiv.org/pdf/1901.09496v2)

> The representations learned by deep neural networks are difficult to interpret in part due to their large parameter space and the complexities introduced by their multi-layer structure. We introduce a method for computing persistent homology over the graphical activation structure of neural networks, which provides access to the task-relevant substructures activated throughout the network for a given input. This topological perspective provides unique insights into the distributed representations encoded by neural networks in terms of the shape of their activation structures. We demonstrate the value of this approach by showing an alternative explanation for the existence of adversarial examples. By studying the topology of network activations across multiple architectures and datasets, we find that adversarial perturbations do not add activations that target the semantic structure of the adversarial class as previously hypothesized. Rather, adversarial examples are explainable as alterations to the dominant activation structures induced by the original image, suggesting the class representations learned by deep networks are problematically sparse on the input space.

</details>

<details>

<summary>2019-05-30 20:10:14 - Learning Semantic Annotations for Tabular Data</summary>

- *Jiaoyan Chen, Ernesto Jimenez-Ruiz, Ian Horrocks, Charles Sutton*

- `1906.00781v1` - [abs](http://arxiv.org/abs/1906.00781v1) - [pdf](http://arxiv.org/pdf/1906.00781v1)

> The usefulness of tabular data such as web tables critically depends on understanding their semantics. This study focuses on column type prediction for tables without any meta data. Unlike traditional lexical matching-based methods, we propose a deep prediction model that can fully exploit a table's contextual semantics, including table locality features learned by a Hybrid Neural Network (HNN), and inter-column semantics features learned by a knowledge base (KB) lookup and query answering algorithm.It exhibits good performance not only on individual table sets, but also when transferring from one table set to another.

</details>

<details>

<summary>2019-05-30 21:43:57 - Grammar-based Neural Text-to-SQL Generation</summary>

- *Kevin Lin, Ben Bogin, Mark Neumann, Jonathan Berant, Matt Gardner*

- `1905.13326v1` - [abs](http://arxiv.org/abs/1905.13326v1) - [pdf](http://arxiv.org/pdf/1905.13326v1)

> The sequence-to-sequence paradigm employed by neural text-to-SQL models typically performs token-level decoding and does not consider generating SQL hierarchically from a grammar. Grammar-based decoding has shown significant improvements for other semantic parsing tasks, but SQL and other general programming languages have complexities not present in logical formalisms that make writing hierarchical grammars difficult. We introduce techniques to handle these complexities, showing how to construct a schema-dependent grammar with minimal over-generation. We analyze these techniques on ATIS and Spider, two challenging text-to-SQL datasets, demonstrating that they yield 14--18\% relative reductions in error.

</details>

<details>

<summary>2019-05-31 02:12:41 - Supervised Online Hashing via Similarity Distribution Learning</summary>

- *Mingbao Lin, Rongrong Ji, Shen Chen, Feng Zheng, Xiaoshuai Sun, Baochang Zhang, Liujuan Cao, Guodong Guo, Feiyue Huang*

- `1905.13382v1` - [abs](http://arxiv.org/abs/1905.13382v1) - [pdf](http://arxiv.org/pdf/1905.13382v1)

> Online hashing has attracted extensive research attention when facing streaming data. Most online hashing methods, learning binary codes based on pairwise similarities of training instances, fail to capture the semantic relationship, and suffer from a poor generalization in large-scale applications due to large variations. In this paper, we propose to model the similarity distributions between the input data and the hashing codes, upon which a novel supervised online hashing method, dubbed as Similarity Distribution based Online Hashing (SDOH), is proposed, to keep the intrinsic semantic relationship in the produced Hamming space. Specifically, we first transform the discrete similarity matrix into a probability matrix via a Gaussian-based normalization to address the extremely imbalanced distribution issue. And then, we introduce a scaling Student t-distribution to solve the challenging initialization problem, and efficiently bridge the gap between the known and unknown distributions. Lastly, we align the two distributions via minimizing the Kullback-Leibler divergence (KL-diverence) with stochastic gradient descent (SGD), by which an intuitive similarity constraint is imposed to update hashing model on the new streaming data with a powerful generalizing ability to the past data. Extensive experiments on three widely-used benchmarks validate the superiority of the proposed SDOH over the state-of-the-art methods in the online retrieval task.

</details>

<details>

<summary>2019-05-31 05:16:15 - Constructive Type-Logical Supertagging with Self-Attention Networks</summary>

- *Konstantinos Kogkalidis, Michael Moortgat, Tejaswini Deoskar*

- `1905.13418v1` - [abs](http://arxiv.org/abs/1905.13418v1) - [pdf](http://arxiv.org/pdf/1905.13418v1)

> We propose a novel application of self-attention networks towards grammar induction. We present an attention-based supertagger for a refined type-logical grammar, trained on constructing types inductively. In addition to achieving a high overall type accuracy, our model is able to learn the syntax of the grammar's type system along with its denotational semantics. This lifts the closed world assumption commonly made by lexicalized grammar supertaggers, greatly enhancing its generalization potential. This is evidenced both by its adequate accuracy over sparse word types and its ability to correctly construct complex types never seen during training, which, to the best of our knowledge, was as of yet unaccomplished.

</details>

<details>

<summary>2019-05-31 08:42:28 - Misleading Authorship Attribution of Source Code using Adversarial Learning</summary>

- *Erwin Quiring, Alwin Maier, Konrad Rieck*

- `1905.12386v2` - [abs](http://arxiv.org/abs/1905.12386v2) - [pdf](http://arxiv.org/pdf/1905.12386v2)

> In this paper, we present a novel attack against authorship attribution of source code. We exploit that recent attribution methods rest on machine learning and thus can be deceived by adversarial examples of source code. Our attack performs a series of semantics-preserving code transformations that mislead learning-based attribution but appear plausible to a developer. The attack is guided by Monte-Carlo tree search that enables us to operate in the discrete domain of source code. In an empirical evaluation with source code from 204 programmers, we demonstrate that our attack has a substantial effect on two recent attribution methods, whose accuracy drops from over 88% to 1% under attack. Furthermore, we show that our attack can imitate the coding style of developers with high accuracy and thereby induce false attributions. We conclude that current approaches for authorship attribution are inappropriate for practical application and there is a need for resilient analysis techniques.

</details>

<details>

<summary>2019-05-31 19:22:29 - Table2Vec: Neural Word and Entity Embeddings for Table Population and Retrieval</summary>

- *Li Deng, Shuo Zhang, Krisztian Balog*

- `1906.00041v1` - [abs](http://arxiv.org/abs/1906.00041v1) - [pdf](http://arxiv.org/pdf/1906.00041v1)

> Tables contain valuable knowledge in a structured form. We employ neural language modeling approaches to embed tabular data into vector spaces. Specifically, we consider different table elements, such caption, column headings, and cells, for training word and entity embeddings. These embeddings are then utilized in three particular table-related tasks, row population, column population, and table retrieval, by incorporating them into existing retrieval models as additional semantic similarity signals. Evaluation results show that table embeddings can significantly improve upon the performance of state-of-the-art baselines.

</details>

<details>

<summary>2019-05-31 20:43:22 - ANA at SemEval-2019 Task 3: Contextual Emotion detection in Conversations through hierarchical LSTMs and BERT</summary>

- *Chenyang Huang, Amine Trabelsi, Osmar R. Zaïane*

- `1904.00132v2` - [abs](http://arxiv.org/abs/1904.00132v2) - [pdf](http://arxiv.org/pdf/1904.00132v2)

> This paper describes the system submitted by ANA Team for the SemEval-2019 Task 3: EmoContext. We propose a novel Hierarchical LSTMs for Contextual Emotion Detection (HRLCE) model. It classifies the emotion of an utterance given its conversational context. The results show that, in this task, our HRCLE outperforms the most recent state-of-the-art text classification framework: BERT. We combine the results generated by BERT and HRCLE to achieve an overall score of 0.7709 which ranked 5th on the final leader board of the competition among 165 Teams.

</details>

<details>

<summary>2019-05-31 20:45:15 - Improving the Similarity Measure of Determinantal Point Processes for Extractive Multi-Document Summarization</summary>

- *Sangwoo Cho, Logan Lebanoff, Hassan Foroosh, Fei Liu*

- `1906.00072v1` - [abs](http://arxiv.org/abs/1906.00072v1) - [pdf](http://arxiv.org/pdf/1906.00072v1)

> The most important obstacles facing multi-document summarization include excessive redundancy in source descriptions and the looming shortage of training data. These obstacles prevent encoder-decoder models from being used directly, but optimization-based methods such as determinantal point processes (DPPs) are known to handle them well. In this paper we seek to strengthen a DPP-based method for extractive multi-document summarization by presenting a novel similarity measure inspired by capsule networks. The approach measures redundancy between a pair of sentences based on surface form and semantic information. We show that our DPP system with improved similarity measure performs competitively, outperforming strong summarization baselines on benchmark datasets. Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions.

</details>

<details>

<summary>2019-05-31 20:54:19 - Learning to Generalize from Sparse and Underspecified Rewards</summary>

- *Rishabh Agarwal, Chen Liang, Dale Schuurmans, Mohammad Norouzi*

- `1902.07198v4` - [abs](http://arxiv.org/abs/1902.07198v4) - [pdf](http://arxiv.org/pdf/1902.07198v4)

> We consider the problem of learning from sparse and underspecified rewards, where an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. Such success-failure rewards are often underspecified: they do not distinguish between purposeful and accidental success. Generalization from underspecified rewards hinges on discounting spurious trajectories that attain accidental success, while learning from sparse feedback requires effective exploration. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy. We propose Meta Reward Learning (MeRL) to construct an auxiliary reward function that provides more refined feedback for learning. The parameters of the auxiliary reward function are optimized with respect to the validation performance of a trained policy. The MeRL approach outperforms our alternative reward learning technique based on Bayesian Optimization, and achieves the state-of-the-art on weakly-supervised semantic parsing. It improves previous work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL datasets respectively.

</details>

<details>

<summary>2019-05-31 22:23:15 - Out of Sight But Not Out of Mind: An Answer Set Programming Based Online Abduction Framework for Visual Sensemaking in Autonomous Driving</summary>

- *Jakob Suchan, Mehul Bhatt, Srikrishna Varadarajan*

- `1906.00107v1` - [abs](http://arxiv.org/abs/1906.00107v1) - [pdf](http://arxiv.org/pdf/1906.00107v1)

> We demonstrate the need and potential of systematically integrated vision and semantics} solutions for visual sensemaking (in the backdrop of autonomous driving). A general method for online visual sensemaking using answer set programming is systematically formalised and fully implemented. The method integrates state of the art in (deep learning based) visual computing, and is developed as a modular framework usable within hybrid architectures for perception & control. We evaluate and demo with community established benchmarks KITTIMOD and MOT. As use-case, we focus on the significance of human-centred visual sensemaking ---e.g., semantic representation and explainability, question-answering, commonsense interpolation--- in safety-critical autonomous driving situations.

</details>


## 2019-06

<details>

<summary>2019-06-01 02:38:18 - aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model</summary>

- *Liu Yang, Qingyao Ai, Jiafeng Guo, W. Bruce Croft*

- `1801.01641v2` - [abs](http://arxiv.org/abs/1801.01641v2) - [pdf](http://arxiv.org/pdf/1801.01641v2)

> As an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs) have recently been proposed for semantic matching of questions and answers. To achieve good results, however, these models have been combined with additional features such as word overlap or BM25 scores. Without this combination, these models perform significantly worse than methods based on linguistic feature engineering. In this paper, we propose an attention based neural matching model for ranking short answer text. We adopt value-shared weighting scheme instead of position-shared weighting scheme for combining different matching signals and incorporate question term importance learning using question attention network. Using the popular benchmark TREC QA data, we show that the relatively simple aNMM model can significantly outperform other neural network models that have been used for the question answering task, and is competitive with models that are combined with additional features. When aNMM is combined with additional features, it outperforms all baselines.

</details>

<details>

<summary>2019-06-01 06:11:06 - Patch Learning</summary>

- *Dongrui Wu, Jerry M. Mendel*

- `1906.00158v1` - [abs](http://arxiv.org/abs/1906.00158v1) - [pdf](http://arxiv.org/pdf/1906.00158v1)

> There have been different strategies to improve the performance of a machine learning model, e.g., increasing the depth, width, and/or nonlinearity of the model, and using ensemble learning to aggregate multiple base/weak learners in parallel or in series. This paper proposes a novel strategy called patch learning (PL) for this problem. It consists of three steps: 1) train an initial global model using all training data; 2) identify from the initial global model the patches which contribute the most to the learning error, and train a (local) patch model for each such patch; and, 3) update the global model using training data that do not fall into any patch. To use a PL model, we first determine if the input falls into any patch. If yes, then the corresponding patch model is used to compute the output. Otherwise, the global model is used. We explain in detail how PL can be implemented using fuzzy systems. Five regression problems on 1D/2D/3D curve fitting, nonlinear system identification, and chaotic time-series prediction, verified its effectiveness. To our knowledge, the PL idea has not appeared in the literature before, and it opens up a promising new line of research in machine learning.

</details>

<details>

<summary>2019-06-01 18:35:12 - How to best use Syntax in Semantic Role Labelling</summary>

- *Yufei Wang, Mark Johnson, Stephen Wan, Yifang Sun, Wei Wang*

- `1906.00266v1` - [abs](http://arxiv.org/abs/1906.00266v1) - [pdf](http://arxiv.org/pdf/1906.00266v1)

> There are many different ways in which external information might be used in an NLP task. This paper investigates how external syntactic information can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL'05 and CoNLL'12 benchmarks.

</details>

<details>

<summary>2019-06-02 10:05:26 - Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data</summary>

- *Shizhe Chen, Qin Jin, Alexander Hauptmann*

- `1906.00378v1` - [abs](http://arxiv.org/abs/1906.00378v1) - [pdf](http://arxiv.org/pdf/1906.00378v1)

> Bilingual lexicon induction, translating words from the source language to the target language, is a long-standing natural language processing task. Recent endeavors prove that it is promising to employ images as pivot to learn the lexicon induction without reliance on parallel corpora. However, these vision-based approaches simply associate words with entire images, which are constrained to translate concrete words and require object-centered images. We humans can understand words better when they are within a sentence with context. Therefore, in this paper, we propose to utilize images and their associated captions to address the limitations of previous approaches. We propose a multi-lingual caption model trained with different mono-lingual multimodal data to map words in different languages into joint spaces. Two types of word representation are induced from the multi-lingual caption model: linguistic features and localized visual features. The linguistic feature is learned from the sentence contexts with visual semantic constraints, which is beneficial to learn translation for words that are less visual-relevant. The localized visual feature is attended to the region in the image that correlates to the word, so that it alleviates the image restriction for salient visual representation. The two types of features are complementary for word translation. Experimental results on multiple language pairs demonstrate the effectiveness of our proposed method, which substantially outperforms previous vision-based approaches without using any parallel sentences or supervision of seed word pairs.

</details>

<details>

<summary>2019-06-02 12:48:56 - GASC: Genre-Aware Semantic Change for Ancient Greek</summary>

- *Valerio Perrone, Marco Palma, Simon Hengchen, Alessandro Vatri, Jim Q. Smith, Barbara McGillivray*

- `1903.05587v2` - [abs](http://arxiv.org/abs/1903.05587v2) - [pdf](http://arxiv.org/pdf/1903.05587v2)

> Word meaning changes over time, depending on linguistic and extra-linguistic factors. Associating a word's correct meaning in its historical context is a central challenge in diachronic research, and is relevant to a range of NLP tasks, including information retrieval and semantic search in historical texts. Bayesian models for semantic change have emerged as a powerful tool to address this challenge, providing explicit and interpretable representations of semantic change phenomena. However, while corpora typically come with rich metadata, existing models are limited by their inability to exploit contextual information (such as text genre) beyond the document time-stamp. This is particularly critical in the case of ancient languages, where lack of data and long diachronic span make it harder to draw a clear distinction between polysemy (the fact that a word has several senses) and semantic change (the process of acquiring, losing, or changing senses), and current systems perform poorly on these languages. We develop GASC, a dynamic semantic change model that leverages categorical metadata about the texts' genre to boost inference and uncover the evolution of meanings in Ancient Greek corpora. In a new evaluation framework, our model achieves improved predictive performance compared to the state of the art.

</details>

<details>

<summary>2019-06-02 20:06:50 - Self-Supervised Deep Learning on Point Clouds by Reconstructing Space</summary>

- *Jonathan Sauder, Bjarne Sievers*

- `1901.08396v2` - [abs](http://arxiv.org/abs/1901.08396v2) - [pdf](http://arxiv.org/pdf/1901.08396v2)

> Point clouds provide a flexible and natural representation usable in countless applications such as robotics or self-driving cars. Recently, deep neural networks operating on raw point cloud data have shown promising results on supervised learning tasks such as object classification and semantic segmentation. While massive point cloud datasets can be captured using modern scanning technology, manually labelling such large 3D point clouds for supervised learning tasks is a cumbersome process. This necessitates methods that can learn from unlabelled data to significantly reduce the number of annotated samples needed in supervised learning. We propose a self-supervised learning task for deep learning on raw point cloud data in which a neural network is trained to reconstruct point clouds whose parts have been randomly rearranged. While solving this task, representations that capture semantic properties of the point cloud are learned. Our method is agnostic of network architecture and outperforms current unsupervised learning approaches in downstream object classification tasks. We show experimentally, that pre-training with our method before supervised training improves the performance of state-of-the-art models and significantly improves sample efficiency.

</details>

<details>

<summary>2019-06-03 02:51:40 - Incorporating Biological Knowledge with Factor Graph Neural Network for Interpretable Deep Learning</summary>

- *Tianle Ma, Aidong Zhang*

- `1906.00537v1` - [abs](http://arxiv.org/abs/1906.00537v1) - [pdf](http://arxiv.org/pdf/1906.00537v1)

> While deep learning has achieved great success in many fields, one common criticism about deep learning is its lack of interpretability. In most cases, the hidden units in a deep neural network do not have a clear semantic meaning or correspond to any physical entities. However, model interpretability and explainability are crucial in many biomedical applications. To address this challenge, we developed the Factor Graph Neural Network model that is interpretable and predictable by combining probabilistic graphical models with deep learning. We directly encode biological knowledge such as Gene Ontology as a factor graph into the model architecture, making the model transparent and interpretable. Furthermore, we devised an attention mechanism that can capture multi-scale hierarchical interactions among biological entities such as genes and Gene Ontology terms. With parameter sharing mechanism, the unrolled Factor Graph Neural Network model can be trained with stochastic depth and generalize well. We applied our model to two cancer genomic datasets to predict target clinical variables and achieved better results than other traditional machine learning and deep learning models. Our model can also be used for gene set enrichment analysis and selecting Gene Ontology terms that are important to target clinical variables.

</details>

<details>

<summary>2019-06-03 05:30:41 - Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing</summary>

- *Ben Bogin, Matt Gardner, Jonathan Berant*

- `1905.06241v2` - [abs](http://arxiv.org/abs/1905.06241v2) - [pdf](http://arxiv.org/pdf/1905.06241v2)

> Research on parsing language to SQL has largely ignored the structure of the database (DB) schema, either because the DB was very simple, or because it was observed at both training and test time. In Spider, a recently-released text-to-SQL dataset, new and complex DBs are given at test time, and so the structure of the DB schema can inform the predicted SQL query. In this paper, we present an encoder-decoder semantic parser, where the structure of the DB schema is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. Evaluation shows that encoding the schema structure improves our parser accuracy from 33.8% to 39.4%, dramatically above the current state of the art, which is at 19.7%.

</details>

<details>

<summary>2019-06-03 07:17:35 - An Extensive Review of Computational Dance Automation Techniques and Applications</summary>

- *Manish Joshi, Sangeeta Jadhav*

- `1906.00606v1` - [abs](http://arxiv.org/abs/1906.00606v1) - [pdf](http://arxiv.org/pdf/1906.00606v1)

> Dance is an art and when technology meets this kind of art, it's a novel attempt in itself. Several researchers have attempted to automate several aspects of dance, right from dance notation to choreography. Furthermore, we have encountered several applications of dance automation like e-learning, heritage preservation, etc. Despite several attempts by researchers for more than two decades in various styles of dance all round the world, we found a review paper that portrays the research status in this area dating to 1990 \cite{politis1990computers}. Hence, we decide to come up with a comprehensive review article that showcases several aspects of dance automation.   This paper is an attempt to review research work reported in the literature, categorize and group all research work completed so far in the field of automating dance. We have explicitly identified six major categories corresponding to the use of computers in dance automation namely dance representation, dance capturing, dance semantics, dance generation, dance processing approaches and applications of dance automation systems. We classified several research papers under these categories according to their research approach and functionality. With the help of proposed categories and subcategories one can easily determine the state of research and the new avenues left for exploration in the field of dance automation.

</details>

<details>

<summary>2019-06-03 09:52:17 - Unsupervised Neural Generative Semantic Hashing</summary>

- *Casper Hansen, Christian Hansen, Jakob Grue Simonsen, Stephen Alstrup, Christina Lioma*

- `1906.00671v1` - [abs](http://arxiv.org/abs/1906.00671v1) - [pdf](http://arxiv.org/pdf/1906.00671v1)

> Fast similarity search is a key component in large-scale information retrieval, where semantic hashing has become a popular strategy for representing documents as binary hash codes. Recent advances in this area have been obtained through neural network based models: generative models trained by learning to reconstruct the original documents. We present a novel unsupervised generative semantic hashing approach, \textit{Ranking based Semantic Hashing} (RBSH) that consists of both a variational and a ranking based component. Similarly to variational autoencoders, the variational component is trained to reconstruct the original document conditioned on its generated hash code, and as in prior work, it only considers documents individually. The ranking component solves this limitation by incorporating inter-document similarity into the hash code generation, modelling document ranking through a hinge loss. To circumvent the need for labelled data to compute the hinge loss, we use a weak labeller and thus keep the approach fully unsupervised.   Extensive experimental evaluation on four publicly available datasets against traditional baselines and recent state-of-the-art methods for semantic hashing shows that RBSH significantly outperforms all other methods across all evaluated hash code lengths. In fact, RBSH hash codes are able to perform similarly to state-of-the-art hash codes while using 2-4x fewer bits.

</details>

<details>

<summary>2019-06-03 09:52:47 - Contextually Propagated Term Weights for Document Representation</summary>

- *Casper Hansen, Christian Hansen, Stephen Alstrup, Jakob Grue Simonsen, Christina Lioma*

- `1906.00674v1` - [abs](http://arxiv.org/abs/1906.00674v1) - [pdf](http://arxiv.org/pdf/1906.00674v1)

> Word embeddings predict a word from its neighbours by learning small, dense embedding vectors. In practice, this prediction corresponds to a semantic score given to the predicted word (or term weight). We present a novel model that, given a target word, redistributes part of that word's weight (that has been computed with word embeddings) across words occurring in similar contexts as the target word. Thus, our model aims to simulate how semantic meaning is shared by words occurring in similar contexts, which is incorporated into bag-of-words document representations. Experimental evaluation in an unsupervised setting against 8 state of the art baselines shows that our model yields the best micro and macro F1 scores across datasets of increasing difficulty.

</details>

<details>

<summary>2019-06-03 13:51:45 - Static Code Analysis of Multilanguage Software Systems</summary>

- *Anas Shatnawi, Hafedh Mili, Manel Abdellatif, Yann-Gaël Guéhéneuc, Naouel Moha, Geoffrey Hecht, Ghizlane El Boussaidi, Jean Privat*

- `1906.00815v1` - [abs](http://arxiv.org/abs/1906.00815v1) - [pdf](http://arxiv.org/pdf/1906.00815v1)

> Identifying dependency call graphs of multilanguage software systems using static code analysis is challenging. The different languages used in developing today's systems often have different lexical, syntactical, and semantic rules that make thorough analysis difficult. Also, they offer different modularization and dependency mechanisms, both within and between components. Finally, they promote and--or require varieties of frameworks offering different sets of services, which introduce hidden dependencies, invisible with current static code analysis approaches. In this paper, we identify five important challenges that static code analysis must overcome with multilanguage systems and we propose requirements to handle them. Then, we present solutions of these requirements to handle JEE applications, which combine server-side Java source code with a number of client-side Web dialects (e.g., JSP, JSF) while relying on frameworks (e.g., Web and EJB containers) that create hidden dependencies. Finally, we evaluate our implementations of the solutions by developing a set of tools to analyze JEE applications to build a dependency call graph and by applying these tools on two sample JEE applications. Our evaluation shows that our tools can solve the identified challenges and improve the recall in the identification of multilanguage dependencies compared to standard JEE static code analysis and, thus, indirectly that the proposed requirements are useful to build multilanguage static code analysis.

</details>

<details>

<summary>2019-06-03 15:01:53 - Hierarchical Auxiliary Learning</summary>

- *Jaehoon Cha, Kyeong Soo Kim, Sanghyuk Lee*

- `1906.00852v1` - [abs](http://arxiv.org/abs/1906.00852v1) - [pdf](http://arxiv.org/pdf/1906.00852v1)

> Conventional application of convolutional neural networks (CNNs) for image classification and recognition is based on the assumption that all target classes are equal(i.e., no hierarchy) and exclusive of one another (i.e., no overlap). CNN-based image classifiers built on this assumption, therefore, cannot take into account an innate hierarchy among target classes (e.g., cats and dogs in animal image classification) or additional information that can be easily derived from the data (e.g.,numbers larger than five in the recognition of handwritten digits), thereby resulting in scalability issues when the number of target classes is large. Combining two related but slightly different ideas of hierarchical classification and logical learning by auxiliary inputs, we propose a new learning framework called hierarchical auxiliary learning, which not only address the scalability issues with a large number of classes but also could further reduce the classification/recognition errors with a reasonable number of classes. In the hierarchical auxiliary learning, target classes are semantically or non-semantically grouped into superclasses, which turns the original problem of mapping between an image and its target class into a new problem of mapping between a pair of an image and its superclass and the target class. To take the advantage of superclasses, we introduce an auxiliary block into a neural network, which generates auxiliary scores used as additional information for final classification/recognition; in this paper, we add the auxiliary block between the last residual block and the fully-connected output layer of the ResNet. Experimental results demonstrate that the proposed hierarchical auxiliary learning can reduce classification errors up to 0.56, 1.6 and 3.56 percent with MNIST, SVHN and CIFAR-10 datasets, respectively.

</details>

<details>

<summary>2019-06-03 19:21:42 - A Language-Agnostic Model for Semantic Source Code Labeling</summary>

- *Ben Gelman, Bryan Hoyle, Jessica Moore, Joshua Saxe, David Slater*

- `1906.01032v1` - [abs](http://arxiv.org/abs/1906.01032v1) - [pdf](http://arxiv.org/pdf/1906.01032v1)

> Code search and comprehension have become more difficult in recent years due to the rapid expansion of available source code. Current tools lack a way to label arbitrary code at scale while maintaining up-to-date representations of new programming languages, libraries, and functionalities. Comprehensive labeling of source code enables users to search for documents of interest and obtain a high-level understanding of their contents. We use Stack Overflow code snippets and their tags to train a language-agnostic, deep convolutional neural network to automatically predict semantic labels for source code documents. On Stack Overflow code snippets, we demonstrate a mean area under ROC of 0.957 over a long-tailed list of 4,508 tags. We also manually validate the model outputs on a diverse set of unlabeled source code documents retrieved from Github, and we obtain a top-1 accuracy of 86.6%. This strongly indicates that the model successfully transfers its knowledge from Stack Overflow snippets to arbitrary source code documents.

</details>

<details>

<summary>2019-06-03 19:33:13 - Transforming Complex Sentences into a Semantic Hierarchy</summary>

- *Christina Niklaus, Matthias Cetto, Andre Freitas, Siegfried Handschuh*

- `1906.01038v1` - [abs](http://arxiv.org/abs/1906.01038v1) - [pdf](http://arxiv.org/pdf/1906.01038v1)

> We present an approach for recursively splitting and rephrasing complex English sentences into a novel semantic hierarchy of simplified sentences, with each of them presenting a more regular structure that may facilitate a wide variety of artificial intelligence tasks, such as machine translation (MT) or information extraction (IE). Using a set of hand-crafted transformation rules, input sentences are recursively transformed into a two-layered hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations. In this way, the semantic relationship of the decomposed constituents is preserved in the output, maintaining its interpretability for downstream applications. Both a thorough manual analysis and automatic evaluation across three datasets from two different domains demonstrate that the proposed syntactic simplification approach outperforms the state of the art in structural text simplification. Moreover, an extrinsic evaluation shows that when applying our framework as a preprocessing step the performance of state-of-the-art Open IE systems can be improved by up to 346% in precision and 52% in recall. To enable reproducible research, all code is provided online.

</details>

<details>

<summary>2019-06-03 21:18:54 - Fine-Grained Temporal Relation Extraction</summary>

- *Siddharth Vashishtha, Benjamin Van Durme, Aaron Steven White*

- `1902.01390v2` - [abs](http://arxiv.org/abs/1902.01390v2) - [pdf](http://arxiv.org/pdf/1902.01390v2)

> We present a novel semantic framework for modeling temporal relations and event durations that maps pairs of events to real-valued scales. We use this framework to construct the largest temporal relations dataset to date, covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to train models for jointly predicting fine-grained temporal relations and event durations. We report strong results on our data and show the efficacy of a transfer-learning approach for predicting categorical relations.

</details>

<details>

<summary>2019-06-04 04:04:17 - Passage Ranking with Weak Supervision</summary>

- *Peng Xu, Xiaofei Ma, Ramesh Nallapati, Bing Xiang*

- `1905.05910v2` - [abs](http://arxiv.org/abs/1905.05910v2) - [pdf](http://arxiv.org/pdf/1905.05910v2)

> In this paper, we propose a \textit{weak supervision} framework for neural ranking tasks based on the data programming paradigm \citep{Ratner2016}, which enables us to leverage multiple weak supervision signals from different sources. Empirically, we consider two sources of weak supervision signals, unsupervised ranking functions and semantic feature similarities. We train a BERT-based passage-ranking model (which achieves new state-of-the-art performances on two benchmark datasets with full supervision) in our weak supervision framework. Without using ground-truth training labels, BERT-PR models outperform BM25 baseline by a large margin on all three datasets and even beat the previous state-of-the-art results with full supervision on two of the datasets.

</details>

<details>

<summary>2019-06-04 05:54:02 - Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog</summary>

- *Zhe Gan, Yu Cheng, Ahmed El Kholy, Linjie Li, Jingjing Liu, Jianfeng Gao*

- `1902.00579v2` - [abs](http://arxiv.org/abs/1902.00579v2) - [pdf](http://arxiv.org/pdf/1902.00579v2)

> This paper presents a new model for visual dialog, Recurrent Dual Attention Network (ReDAN), using multi-step reasoning to answer a series of questions about an image. In each question-answering turn of a dialog, ReDAN infers the answer progressively through multiple reasoning steps. In each step of the reasoning process, the semantic representation of the question is updated based on the image and the previous dialog history, and the recurrently-refined representation is used for further reasoning in the subsequent step. On the VisDial v1.0 dataset, the proposed ReDAN model achieves a new state-of-the-art of 64.47% NDCG score. Visualization on the reasoning process further demonstrates that ReDAN can locate context-relevant visual and textual clues via iterative refinement, which can lead to the correct answer step-by-step.

</details>

<details>

<summary>2019-06-04 06:35:51 - Augmenting Neural Response Generation with Context-Aware Topical Attention</summary>

- *Nouha Dziri, Ehsan Kamalloo, Kory W. Mathewson, Osmar Zaiane*

- `1811.01063v2` - [abs](http://arxiv.org/abs/1811.01063v2) - [pdf](http://arxiv.org/pdf/1811.01063v2)

> Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic. In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines.

</details>

<details>

<summary>2019-06-04 07:13:49 - Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning Easy-First Strategies</summary>

- *Shuhei Kurita, Anders Søgaard*

- `1906.01239v1` - [abs](http://arxiv.org/abs/1906.01239v1) - [pdf](http://arxiv.org/pdf/1906.01239v1)

> In Semantic Dependency Parsing (SDP), semantic relations form directed acyclic graphs, rather than trees. We propose a new iterative predicate selection (IPS) algorithm for SDP. Our IPS algorithm combines the graph-based and transition-based parsing approaches in order to handle multiple semantic head words. We train the IPS model using a combination of multi-task learning and task-specific policy gradient training. Trained this way, IPS achieves a new state of the art on the SemEval 2015 Task 18 datasets. Furthermore, we observe that policy gradient training learns an easy-first strategy.

</details>

<details>

<summary>2019-06-04 09:04:48 - Interpreting OWL Complex Classes in AutomationML based on Bidirectional Translation</summary>

- *Yingbing Hua, Björn Hein*

- `1906.04240v1` - [abs](http://arxiv.org/abs/1906.04240v1) - [pdf](http://arxiv.org/pdf/1906.04240v1)

> The World Wide Web Consortium (W3C) has published several recommendations for building and storing ontologies, including the most recent OWL 2 Web Ontology Language (OWL). These initiatives have been followed by practical implementations that popularize OWL in various domains. For example, OWL has been used for conceptual modeling in industrial engineering, and its reasoning facilities are used to provide a wealth of services, e.g. model diagnosis, automated code generation, and semantic integration. More specifically, recent studies have shown that OWL is well suited for harmonizing information of engineering tools stored as AutomationML (AML) files. However, OWL and its tools can be cumbersome for direct use by engineers such that an ontology expert is often required in practice. Although much attention has been paid in the literature to overcome this issue by transforming OWL ontologies from/to AML models automatically, dealing with OWL complex classes remains an open research question. In this paper, we introduce the AML concept models for representing OWL complex classes in AutomationML, and present algorithms for the bidirectional translation between OWL complex classes and their corresponding AML concept models. We show that this approach provides an efficient and intuitive interface for nonexperts to visualize, modify, and create OWL complex classes.

</details>

<details>

<summary>2019-06-04 11:03:49 - A Cross-Sentence Latent Variable Model for Semi-Supervised Text Sequence Matching</summary>

- *Jihun Choi, Taeuk Kim, Sang-goo Lee*

- `1906.01343v1` - [abs](http://arxiv.org/abs/1906.01343v1) - [pdf](http://arxiv.org/pdf/1906.01343v1)

> We present a latent variable model for predicting the relationship between a pair of text sequences. Unlike previous auto-encoding--based approaches that consider each sequence separately, our proposed framework utilizes both sequences within a single model by generating a sequence that has a given relationship with a source sequence. We further extend the cross-sentence generating framework to facilitate semi-supervised training. We also define novel semantic constraints that lead the decoder network to generate semantically plausible and diverse sequences. We demonstrate the effectiveness of the proposed model from quantitative and qualitative experiments, while achieving state-of-the-art results on semi-supervised natural language inference and paraphrase identification.

</details>

<details>

<summary>2019-06-04 11:35:58 - ERNIE: Enhanced Language Representation with Informative Entities</summary>

- *Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu*

- `1905.07129v3` - [abs](http://arxiv.org/abs/1905.07129v3) - [pdf](http://arxiv.org/pdf/1905.07129v3)

> Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code of this paper can be obtained from https://github.com/thunlp/ERNIE.

</details>

<details>

<summary>2019-06-04 11:46:37 - NNE: A Dataset for Nested Named Entity Recognition in English Newswire</summary>

- *Nicky Ringland, Xiang Dai, Ben Hachey, Sarvnaz Karimi, Cecile Paris, James R. Curran*

- `1906.01359v1` - [abs](http://arxiv.org/abs/1906.01359v1) - [pdf](http://arxiv.org/pdf/1906.01359v1)

> Named entity recognition (NER) is widely used in natural language processing applications and downstream tasks. However, most NER tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions. We describe NNE---a fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER.

</details>

<details>

<summary>2019-06-04 13:01:42 - CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication</summary>

- *Jin-Hwa Kim, Nikita Kitaev, Xinlei Chen, Marcus Rohrbach, Byoung-Tak Zhang, Yuandong Tian, Dhruv Batra, Devi Parikh*

- `1712.05558v3` - [abs](http://arxiv.org/abs/1712.05558v3) - [pdf](http://arxiv.org/pdf/1712.05558v3)

> In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel "crosstalk" evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.

</details>

<details>

<summary>2019-06-04 13:03:00 - NLSC: Unrestricted Natural Language-based Service Composition through Sentence Embeddings</summary>

- *Oscar J. Romero, Ankit Dangi, Sushma A. Akoju*

- `1901.07910v3` - [abs](http://arxiv.org/abs/1901.07910v3) - [pdf](http://arxiv.org/pdf/1901.07910v3)

> Current approaches for service composition (assemblies of atomic services) require developers to use: (a) domain-specific semantics to formalize services that restrict the vocabulary for their descriptions, and (b) translation mechanisms for service retrieval to convert unstructured user requests to strongly-typed semantic representations. In our work, we argue that effort to developing service descriptions, request translations, and matching mechanisms could be reduced using unrestricted natural language; allowing both: (1) end-users to intuitively express their needs using natural language, and (2) service developers to develop services without relying on syntactic/semantic description languages. Although there are some natural language-based service composition approaches, they restrict service retrieval to syntactic/semantic matching. With recent developments in Machine learning and Natural Language Processing, we motivate the use of Sentence Embeddings by leveraging richer semantic representations of sentences for service description, matching and retrieval. Experimental results show that service composition development effort may be reduced by more than 44\% while keeping a high precision/recall when matching high-level user requests with low-level service method invocations.

</details>

<details>

<summary>2019-06-04 13:07:35 - SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference</summary>

- *Martin Schmitt, Hinrich Schütze*

- `1906.01393v1` - [abs](http://arxiv.org/abs/1906.01393v1) - [pdf](http://arxiv.org/pdf/1906.01393v1)

> We present SherLIiC, a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types. Due to our candidate selection process based on strong distributional evidence, SherLIiC is much harder than existing testbeds because distributional evidence is of little utility in the classification of InfCands. We also show that, due to its construction, many of SherLIiC's correct InfCands are novel and missing from existing rule bases. We evaluate a number of strong baselines on SherLIiC, ranging from semantic vector space models to state of the art neural models of natural language inference (NLI). We show that SherLIiC poses a tough challenge to existing NLI systems.

</details>

<details>

<summary>2019-06-04 13:54:47 - Tracing Antisemitic Language Through Diachronic Embedding Projections: France 1789-1914</summary>

- *Rocco Tripodi, Massimo Warglien, Simon Levis Sullam, Deborah Paci*

- `1906.01440v1` - [abs](http://arxiv.org/abs/1906.01440v1) - [pdf](http://arxiv.org/pdf/1906.01440v1)

> We investigate some aspects of the history of antisemitism in France, one of the cradles of modern antisemitism, using diachronic word embeddings. We constructed a large corpus of French books and periodicals issues that contain a keyword related to Jews and performed a diachronic word embedding over the 1789-1914 period. We studied the changes over time in the semantic spaces of 4 target words and performed embedding projections over 6 streams of antisemitic discourse. This allowed us to track the evolution of antisemitic bias in the religious, economic, socio-politic, racial, ethic and conspiratorial domains. Projections show a trend of growing antisemitism, especially in the years starting in the mid-80s and culminating in the Dreyfus affair. Our analysis also allows us to highlight the peculiar adverse bias towards Judaism in the broader context of other religions.

</details>

<details>

<summary>2019-06-04 17:39:52 - Transferable Neural Projection Representations</summary>

- *Chinnadhurai Sankar, Sujith Ravi, Zornitsa Kozareva*

- `1906.01605v1` - [abs](http://arxiv.org/abs/1906.01605v1) - [pdf](http://arxiv.org/pdf/1906.01605v1)

> Neural word representations are at the core of many state-of-the-art natural language processing models. A widely used approach is to pre-train, store and look up word or character embedding matrices. While useful, such representations occupy huge memory making it hard to deploy on-device and often do not generalize to unknown words due to vocabulary pruning.   In this paper, we propose a skip-gram based architecture coupled with Locality-Sensitive Hashing (LSH) projections to learn efficient dynamically computable representations. Our model does not need to store lookup tables as representations are computed on-the-fly and require low memory footprint. The representations can be trained in an unsupervised fashion and can be easily transferred to other NLP tasks. For qualitative evaluation, we analyze the nearest neighbors of the word representations and discover semantically similar words even with misspellings. For quantitative evaluation, we plug our transferable projections into a simple LSTM and run it on multiple NLP tasks and show how our transferable projections achieve better performance compared to prior work.

</details>

<details>

<summary>2019-06-04 19:19:46 - Time-Out: Temporal Referencing for Robust Modeling of Lexical Semantic Change</summary>

- *Haim Dubossarsky, Simon Hengchen, Nina Tahmasebi, Dominik Schlechtweg*

- `1906.01688v1` - [abs](http://arxiv.org/abs/1906.01688v1) - [pdf](http://arxiv.org/pdf/1906.01688v1)

> State-of-the-art models of lexical semantic change detection suffer from noise stemming from vector space alignment. We have empirically tested the Temporal Referencing method for lexical semantic change and show that, by avoiding alignment, it is less affected by this noise. We show that, trained on a diachronic corpus, the skip-gram with negative sampling architecture with temporal referencing outperforms alignment models on a synthetic task as well as a manual testset. We introduce a principled way to simulate lexical semantic change and systematically control for possible biases.

</details>

<details>

<summary>2019-06-05 01:56:50 - Generating Multiple Diverse Responses with Multi-Mapping and Posterior Mapping Selection</summary>

- *Chaotao Chen, Jinhua Peng, Fan Wang, Jun Xu, Hua Wu*

- `1906.01781v1` - [abs](http://arxiv.org/abs/1906.01781v1) - [pdf](http://arxiv.org/pdf/1906.01781v1)

> In human conversation an input post is open to multiple potential responses, which is typically regarded as a one-to-many problem. Promising approaches mainly incorporate multiple latent mechanisms to build the one-to-many relationship. However, without accurate selection of the latent mechanism corresponding to the target response during training, these methods suffer from a rough optimization of latent mechanisms. In this paper, we propose a multi-mapping mechanism to better capture the one-to-many relationship, where multiple mapping modules are employed as latent mechanisms to model the semantic mappings from an input post to its diverse responses. For accurate optimization of latent mechanisms, a posterior mapping selection module is designed to select the corresponding mapping module according to the target response for further optimization. We also introduce an auxiliary matching loss to facilitate the optimization of posterior mapping selection. Empirical results demonstrate the superiority of our model in generating multiple diverse and informative responses over the state-of-the-art methods.

</details>

<details>

<summary>2019-06-05 07:35:37 - AssemblyNet: A Novel Deep Decision-Making Process for Whole Brain MRI Segmentation</summary>

- *Pierrick Coupé, Boris Mansencal, Michaël Clément, Rémi Giraud, Baudouin Denis de Senneville, Vinh-Thong Ta, Vincent Lepetit, José V. Manjon*

- `1906.01862v1` - [abs](http://arxiv.org/abs/1906.01862v1) - [pdf](http://arxiv.org/pdf/1906.01862v1)

> Whole brain segmentation using deep learning (DL) is a very challenging task since the number of anatomical labels is very high compared to the number of available training images. To address this problem, previous DL methods proposed to use a global convolution neural network (CNN) or few independent CNNs. In this paper, we present a novel ensemble method based on a large number of CNNs processing different overlapping brain areas. Inspired by parliamentary decision-making systems, we propose a framework called AssemblyNet, made of two "assemblies" of U-Nets. Such a parliamentary system is capable of dealing with complex decisions and reaching a consensus quickly. AssemblyNet introduces sharing of knowledge among neighboring U-Nets, an "amendment" procedure made by the second assembly at higher-resolution to refine the decision taken by the first one, and a final decision obtained by majority voting. When using the same 45 training images, AssemblyNet outperforms global U-Net by 28% in terms of the Dice metric, patch-based joint label fusion by 15% and SLANT-27 by 10%. Finally, AssemblyNet demonstrates high capacity to deal with limited training data to achieve whole brain segmentation in practical training and testing times.

</details>

<details>

<summary>2019-06-05 12:54:14 - Every child should have parents: a taxonomy refinement algorithm based on hyperbolic term embeddings</summary>

- *Rami Aly, Shantanu Acharya, Alexander Ossa, Arne Köhn, Chris Biemann, Alexander Panchenko*

- `1906.02002v1` - [abs](http://arxiv.org/abs/1906.02002v1) - [pdf](http://arxiv.org/pdf/1906.02002v1)

> We introduce the use of Poincar\'e embeddings to improve existing state-of-the-art approaches to domain-specific taxonomy induction from text as a signal for both relocating wrong hyponym terms within a (pre-induced) taxonomy as well as for attaching disconnected terms in a taxonomy. This method substantially improves previous state-of-the-art results on the SemEval-2016 Task 13 on taxonomy extraction. We demonstrate the superiority of Poincar\'e embeddings over distributional semantic representations, supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space.

</details>

<details>

<summary>2019-06-05 13:55:18 - Hierarchical Annotation of Images with Two-Alternative-Forced-Choice Metric Learning</summary>

- *Niels Hellinga, Vlado Menkovski*

- `1905.09523v2` - [abs](http://arxiv.org/abs/1905.09523v2) - [pdf](http://arxiv.org/pdf/1905.09523v2)

> Many tasks such as retrieval and recommendations can significantly benefit from structuring the data, commonly in a hierarchical way. To achieve this through annotations of high dimensional data such as images or natural text can be significantly labor intensive. We propose an approach for uncovering the hierarchical structure of data based on efficient discriminative testing rather than annotations of individual datapoints. Using two-alternative-forced-choice (2AFC) testing and deep metric learning we achieve embedding of the data in semantic space where we are able to successfully hierarchically cluster. We actively select triplets for the 2AFC test such that the modeling process is highly efficient with respect to the number of tests presented to the annotator. We empirically demonstrate the feasibility of the method by confirming the shape bias on synthetic data and extract hierarchical structure on the Fashion-MNIST dataset to a finer granularity than the original labels.

</details>

<details>

<summary>2019-06-05 14:08:08 - Symbolic inductive bias for visually grounded learning of spoken language</summary>

- *Grzegorz Chrupała*

- `1812.09244v3` - [abs](http://arxiv.org/abs/1812.09244v3) - [pdf](http://arxiv.org/pdf/1812.09244v3)

> A widespread approach to processing spoken language is to first automatically transcribe it into text. An alternative is to use an end-to-end approach: recent works have proposed to learn semantic embeddings of spoken language from images with spoken captions, without an intermediate transcription step. We propose to use multitask learning to exploit existing transcribed speech within the end-to-end setting. We describe a three-task architecture which combines the objectives of matching spoken captions with corresponding images, speech with text, and text with images. We show that the addition of the speech/text task leads to substantial performance improvements on image retrieval when compared to training the speech/image task in isolation. We conjecture that this is due to a strong inductive bias transcribed speech provides to the model, and offer supporting evidence for this.

</details>

<details>

<summary>2019-06-05 14:26:11 - Correlating neural and symbolic representations of language</summary>

- *Grzegorz Chrupała, Afra Alishahi*

- `1905.06401v2` - [abs](http://arxiv.org/abs/1905.06401v2) - [pdf](http://arxiv.org/pdf/1905.06401v2)

> Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP. Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then apply our methods to correlate neural representations of English sentences with their constituency parse trees.

</details>

<details>

<summary>2019-06-05 19:45:21 - An Imitation Learning Approach to Unsupervised Parsing</summary>

- *Bowen Li, Lili Mou, Frank Keller*

- `1906.02276v1` - [abs](http://arxiv.org/abs/1906.02276v1) - [pdf](http://arxiv.org/pdf/1906.02276v1)

> Recently, there has been an increasing interest in unsupervised parsers that optimize semantically oriented objectives, typically using reinforcement learning. Unfortunately, the learned trees often do not match actual syntax trees well. Shen et al. (2018) propose a structured attention mechanism for language modeling (PRPN), which induces better syntactic structures but relies on ad hoc heuristics. Also, their model lacks interpretability as it is not grounded in parsing actions. In our work, we propose an imitation learning approach to unsupervised parsing, where we transfer the syntactic knowledge induced by the PRPN to a Tree-LSTM model with discrete parsing actions. Its policy is then refined by Gumbel-Softmax training towards a semantically oriented objective. We evaluate our approach on the All Natural Language Inference dataset and show that it achieves a new state of the art in terms of parsing $F$-score, outperforming our base models, including the PRPN.

</details>

<details>

<summary>2019-06-05 20:05:18 - SParC: Cross-Domain Semantic Parsing in Context</summary>

- *Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, Dragomir Radev*

- `1906.02285v1` - [abs](http://arxiv.org/abs/1906.02285v1) - [pdf](http://arxiv.org/pdf/1906.02285v1)

> We present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2% over all questions and less than10% over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc.

</details>

<details>

<summary>2019-06-05 22:23:43 - MNIST-C: A Robustness Benchmark for Computer Vision</summary>

- *Norman Mu, Justin Gilmer*

- `1906.02337v1` - [abs](http://arxiv.org/abs/1906.02337v1) - [pdf](http://arxiv.org/pdf/1906.02337v1)

> We introduce the MNIST-C dataset, a comprehensive suite of 15 corruptions applied to the MNIST test set, for benchmarking out-of-distribution robustness in computer vision. Through several experiments and visualizations we demonstrate that our corruptions significantly degrade performance of state-of-the-art computer vision models while preserving the semantic content of the test images. In contrast to the popular notion of adversarial robustness, our model-agnostic corruptions do not seek worst-case performance but are instead designed to be broad and diverse, capturing multiple failure modes of modern models. In fact, we find that several previously published adversarial defenses significantly degrade robustness as measured by MNIST-C. We hope that our benchmark serves as a useful tool for future work in designing systems that are able to learn robust feature representations that capture the underlying semantics of the input.

</details>

<details>

<summary>2019-06-06 08:21:04 - StyleNAS: An Empirical Study of Neural Architecture Search to Uncover Surprisingly Fast End-to-End Universal Style Transfer Networks</summary>

- *Jie An, Haoyi Xiong, Jinwen Ma, Jiebo Luo, Jun Huan*

- `1906.02470v1` - [abs](http://arxiv.org/abs/1906.02470v1) - [pdf](http://arxiv.org/pdf/1906.02470v1)

> Neural Architecture Search (NAS) has been widely studied for designing discriminative deep learning models such as image classification, object detection, and semantic segmentation. As a large number of priors have been obtained through the manual design of architectures in the fields, NAS is usually considered as a supplement approach. In this paper, we have significantly expanded the application areas of NAS by performing an empirical study of NAS to search generative models, or specifically, auto-encoder based universal style transfer, which lacks systematic exploration, if any, from the architecture search aspect. In our work, we first designed a search space where common operators for image style transfer such as VGG-based encoders, whitening and coloring transforms (WCT), convolution kernels, instance normalization operators, and skip connections were searched in a combinatorial approach. With a simple yet effective parallel evolutionary NAS algorithm with multiple objectives, we derived the first group of end-to-end deep networks for universal photorealistic style transfer. Comparing to random search, a NAS method that is gaining popularity recently, we demonstrated that carefully designed search strategy leads to much better architecture design. Finally compared to existing universal style transfer networks for photorealistic rendering such as PhotoWCT that stacks multiple well-trained auto-encoders and WCT transforms in a non-end-to-end manner, the architectures designed by StyleNAS produce better style-transferred images with details preserving, using a tiny number of operators/parameters, and enjoying around 500x inference time speed-up.

</details>

<details>

<summary>2019-06-06 09:09:15 - TBar: Revisiting Template-based Automated Program Repair</summary>

- *Kui Liu, Anil Koyuncu, Dongsun Kim, Tegawendé F. Bissyandé*

- `1903.08409v2` - [abs](http://arxiv.org/abs/1903.08409v2) - [pdf](http://arxiv.org/pdf/1903.08409v2)

> We revisit the performance of template-based APR to build comprehensive knowledge about the effectiveness of fix patterns, and to highlight the importance of complementary steps such as fault localization or donor code retrieval. To that end, we first investigate the literature to collect, summarize and label recurrently-used fix patterns. Based on the investigation, we build TBar, a straightforward APR tool that systematically attempts to apply these fix patterns to program bugs. We thoroughly evaluate TBar on the Defects4J benchmark. In particular, we assess the actual qualitative and quantitative diversity of fix patterns, as well as their effectiveness in yielding plausible or correct patches. Eventually, we find that, assuming a perfect fault localization, TBar correctly/plausibly fixes 74/101 bugs. Replicating a standard and practical pipeline of APR assessment, we demonstrate that TBar correctly fixes 43 bugs from Defects4J, an unprecedented performance in the literature (including all approaches, i.e., template-based, stochastic mutation-based or synthesis-based APR).

</details>

<details>

<summary>2019-06-06 10:44:41 - Derivational Morphological Relations in Word Embeddings</summary>

- *Tomáš Musil, Jonáš Vidra, David Mareček*

- `1906.02510v1` - [abs](http://arxiv.org/abs/1906.02510v1) - [pdf](http://arxiv.org/pdf/1906.02510v1)

> Derivation is a type of a word-formation process which creates new words from existing ones by adding, changing or deleting affixes. In this paper, we explore the potential of word embeddings to identify properties of word derivations in the morphologically rich Czech language. We extract derivational relations between pairs of words from DeriNet, a Czech lexical network, which organizes almost one million Czech lemmata into derivational trees. For each such pair, we compute the difference of the embeddings of the two words, and perform unsupervised clustering of the resulting vectors. Our results show that these clusters largely match manually annotated semantic categories of the derivational relations (e.g. the relation 'bake--baker' belongs to category 'actor', and a correct clustering puts it into the same cluster as 'govern--governor').

</details>

<details>

<summary>2019-06-06 13:08:29 - Scaling and Benchmarking Self-Supervised Visual Representation Learning</summary>

- *Priya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra*

- `1905.01235v2` - [abs](http://arxiv.org/abs/1905.01235v2) - [pdf](http://arxiv.org/pdf/1905.01235v2)

> Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.

</details>

<details>

<summary>2019-06-06 13:58:44 - On the Effectiveness of Laser Speckle Contrast Imaging and Deep Neural Networks for Detecting Known and Unknown Fingerprint Presentation Attacks</summary>

- *Hengameh Mirzaalian, Mohamed Hussein, Wael Abd-Almageed*

- `1906.02595v1` - [abs](http://arxiv.org/abs/1906.02595v1) - [pdf](http://arxiv.org/pdf/1906.02595v1)

> Fingerprint presentation attack detection (FPAD) is becoming an increasingly challenging problem due to the continuous advancement of attack techniques, which generate `realistic-looking' fake fingerprint presentations. Recently, laser speckle contrast imaging (LSCI) has been introduced as a new sensing modality for FPAD. LSCI has the interesting characteristic of capturing the blood flow under the skin surface. Toward studying the importance and effectiveness of LSCI for FPAD, we conduct a comprehensive study using different patch-based deep neural network architectures. Our studied architectures include 2D and 3D convolutional networks as well as a recurrent network using long short-term memory (LSTM) units. The study demonstrates that strong FPAD performance can be achieved using LSCI. We evaluate the different models over a new large dataset. The dataset consists of 3743 bona fide samples, collected from 335 unique subjects, and 218 presentation attack samples, including six different types of attacks. To examine the effect of changing the training and testing sets, we conduct a 3-fold cross validation evaluation. To examine the effect of the presence of an unseen attack, we apply a leave-one-attack out strategy. The FPAD classification results of the networks, which are separately optimized and tuned for the temporal and spatial patch-sizes, indicate that the best performance is achieved by LSTM.

</details>

<details>

<summary>2019-06-06 17:54:24 - Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation</summary>

- *Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, Ekin D. Cubuk*

- `1906.02611v1` - [abs](http://arxiv.org/abs/1906.02611v1) - [pdf](http://arxiv.org/pdf/1906.02611v1)

> Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging. Prior work has argued that there is an inherent trade-off between robustness and accuracy, which is exemplified by standard data augment techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. To overcome this trade-off, we introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image. Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNetCommon Corruptions benchmarks while also improving accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise(similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). Finally, we show that Patch Gaussian can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment, and improves performance on the COCO object detection benchmark.

</details>

<details>

<summary>2019-06-06 18:41:03 - Adversarial Semantic Alignment for Improved Image Captions</summary>

- *Pierre L. Dognin, Igor Melnyk, Youssef Mroueh, Jarret Ross, Tom Sercu*

- `1805.00063v3` - [abs](http://arxiv.org/abs/1805.00063v3) - [pdf](http://arxiv.org/pdf/1805.00063v3)

> In this paper we study image captioning as a conditional GAN training, proposing both a context-aware LSTM captioner and co-attentive discriminator, which enforces semantic alignment between images and captions. We empirically focus on the viability of two training methods: Self-critical Sequence Training (SCST) and Gumbel Straight-Through (ST) and demonstrate that SCST shows more stable gradient behavior and improved results over Gumbel ST, even without accessing discriminator gradients directly. We also address the problem of automatic evaluation for captioning models and introduce a new semantic score, and show its correlation to human judgement. As an evaluation paradigm, we argue that an important criterion for a captioner is the ability to generalize to compositions of objects that do not usually co-occur together. To this end, we introduce a small captioned Out of Context (OOC) test set. The OOC set, combined with our semantic score, are the proposed new diagnosis tools for the captioning community. When evaluated on OOC and MS-COCO benchmarks, we show that SCST-based training has a strong performance in both semantic score and human evaluation, promising to be a valuable new approach for efficient discrete GAN training.

</details>

<details>

<summary>2019-06-07 03:37:17 - Delta Embedding Learning</summary>

- *Xiao Zhang, Ji Wu, Dejing Dou*

- `1812.04160v2` - [abs](http://arxiv.org/abs/1812.04160v2) - [pdf](http://arxiv.org/pdf/1812.04160v2)

> Unsupervised word embeddings have become a popular approach of word representation in NLP tasks. However there are limitations to the semantics represented by unsupervised embeddings, and inadequate fine-tuning of embeddings can lead to suboptimal performance. We propose a novel learning technique called Delta Embedding Learning, which can be applied to general NLP tasks to improve performance by optimized tuning of the word embeddings. A structured regularization is applied to the embeddings to ensure they are tuned in an incremental way. As a result, the tuned word embeddings become better word representations by absorbing semantic information from supervision without "forgetting." We apply the method to various NLP tasks and see a consistent improvement in performance. Evaluation also confirms the tuned word embeddings have better semantic properties.

</details>

<details>

<summary>2019-06-07 09:19:47 - A Wind of Change: Detecting and Evaluating Lexical Semantic Change across Times and Domains</summary>

- *Dominik Schlechtweg, Anna Hätty, Marco del Tredici, Sabine Schulte im Walde*

- `1906.02979v1` - [abs](http://arxiv.org/abs/1906.02979v1) - [pdf](http://arxiv.org/pdf/1906.02979v1)

> We perform an interdisciplinary large-scale evaluation for detecting lexical semantic divergences in a diachronic and in a synchronic task: semantic sense changes across time, and semantic sense changes across domains. Our work addresses the superficialness and lack of comparison in assessing models of diachronic lexical change, by bringing together and extending benchmark models on a common state-of-the-art evaluation task. In addition, we demonstrate that the same evaluation task and modelling approaches can successfully be utilised for the synchronic detection of domain-specific sense divergences in the field of term extraction.

</details>

<details>

<summary>2019-06-07 11:05:30 - On the Compositionality Prediction of Noun Phrases using Poincaré Embeddings</summary>

- *Abhik Jana, Dmitry Puzyrev, Alexander Panchenko, Pawan Goyal, Chris Biemann, Animesh Mukherjee*

- `1906.03007v1` - [abs](http://arxiv.org/abs/1906.03007v1) - [pdf](http://arxiv.org/pdf/1906.03007v1)

> The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. We introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality. In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincar\'e embeddings in addition to the distributional information to detect compositionality for noun phrases. Using a weighted average of the distributional similarity and a Poincar\'e similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. Unlike traditional approaches that solely use an unsupervised setting, we have also framed the problem as a supervised task, obtaining comparable improvements. Further, we publicly release our Poincar\'e embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus.

</details>

<details>

<summary>2019-06-07 13:31:09 - Improving Relation Extraction by Pre-trained Language Representations</summary>

- *Christoph Alt, Marc Hübner, Leonhard Hennig*

- `1906.03088v1` - [abs](http://arxiv.org/abs/1906.03088v1) - [pdf](http://arxiv.org/pdf/1906.03088v1)

> Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step. Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages. Similarly, pre-processing introduces an additional source of error. To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018]. Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions. TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task. TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively. Furthermore, we observe a significant increase in sample efficiency. With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset. We open-source our trained models, experiments, and source code.

</details>

<details>

<summary>2019-06-07 20:49:28 - Semantic Fuzzing with Zest</summary>

- *Rohan Padhye, Caroline Lemieux, Koushik Sen, Mike Papadakis, Yves Le Traon*

- `1812.00078v3` - [abs](http://arxiv.org/abs/1812.00078v3) - [pdf](http://arxiv.org/pdf/1812.00078v3)

> Programs expecting structured inputs often consist of both a syntactic analysis stage, which parses raw input, and a semantic analysis stage, which conducts checks on the parsed input and executes the core logic of the program. Generator-based testing tools in the lineage of QuickCheck are a promising way to generate random syntactically valid test inputs for these programs. We present Zest, a technique which automatically guides QuickCheck-like randominput generators to better explore the semantic analysis stage of test programs. Zest converts random-input generators into deterministic parametric generators. We present the key insight that mutations in the untyped parameter domain map to structural mutations in the input domain. Zest leverages program feedback in the form of code coverage and input validity to perform feedback-directed parameter search. We evaluate Zest against AFL and QuickCheck on five Java programs: Maven, Ant, BCEL, Closure, and Rhino. Zest covers 1.03x-2.81x as many branches within the benchmarks semantic analysis stages as baseline techniques. Further, we find 10 new bugs in the semantic analysis stages of these benchmarks. Zest is the most effective technique in finding these bugs reliably and quickly, requiring at most 10 minutes on average to find each bug.

</details>

<details>

<summary>2019-06-07 20:51:52 - DropConnect Is Effective in Modeling Uncertainty of Bayesian Deep Networks</summary>

- *Aryan Mobiny, Hien V. Nguyen, Supratik Moulik, Naveen Garg, Carol C. Wu*

- `1906.04569v1` - [abs](http://arxiv.org/abs/1906.04569v1) - [pdf](http://arxiv.org/pdf/1906.04569v1)

> Deep neural networks (DNNs) have achieved state-of-the-art performances in many important domains, including medical diagnosis, security, and autonomous driving. In these domains where safety is highly critical, an erroneous decision can result in serious consequences. While a perfect prediction accuracy is not always achievable, recent work on Bayesian deep networks shows that it is possible to know when DNNs are more likely to make mistakes. Knowing what DNNs do not know is desirable to increase the safety of deep learning technology in sensitive applications. Bayesian neural networks attempt to address this challenge. However, traditional approaches are computationally intractable and do not scale well to large, complex neural network architectures. In this paper, we develop a theoretical framework to approximate Bayesian inference for DNNs by imposing a Bernoulli distribution on the model weights. This method, called MC-DropConnect, gives us a tool to represent the model uncertainty with little change in the overall model structure or computational cost. We extensively validate the proposed algorithm on multiple network architectures and datasets for classification and semantic segmentation tasks. We also propose new metrics to quantify the uncertainty estimates. This enables an objective comparison between MC-DropConnect and prior approaches. Our empirical results demonstrate that the proposed framework yields significant improvement in both prediction accuracy and uncertainty estimation quality compared to the state of the art.

</details>

<details>

<summary>2019-06-08 12:30:36 - How Different Is It Between Machine-Generated and Developer-Provided Patches? An Empirical Study on The Correct Patches Generated by Automated Program Repair Techniques</summary>

- *Shangwen Wang, Ming Wen, Liqian Chen, Xin Yi, Xiaoguang Mao*

- `1906.03447v1` - [abs](http://arxiv.org/abs/1906.03447v1) - [pdf](http://arxiv.org/pdf/1906.03447v1)

> Background: Over the years, Automated Program Repair (APR) has attracted much attention from both academia and industry since it can reduce the costs in fixing bugs. However, how to assess the patch correctness remains to be an open challenge. Two widely adopted ways to approach this challenge, including manually checking and validating using automated generated tests, are biased (i.e., suffering from subjectivity and low precision respectively). Aim: To address this concern, we propose to conduct an empirical study towards understanding the correct patches that are generated by existing state-of-the-art APR techniques, aiming at providing guidelines for future assessment of patches. Method: To this end, we first present a Literature Review (LR) on the reported correct patches generated by recent techniques on the Defects4J benchmark and collect 177 correct patches after a process of sanity check. We investigate how these machine-generated correct patches achieve semantic equivalence, but syntactic difference compared with developer-provided ones, how these patches distribute in different projects and APR techniques, and how the characteristics of a bug affect the patches generated for it. Results: Our main findings include 1) we do not need to fix bugs exactly like how developers do since we observe that 25.4% (45/177) of the correct patches generated by APR techniques are syntactically different from developer-provided ones; 2) the distribution of machine-generated correct patches diverges for the aspects of Defects4J projects and APR techniques; and 3) APR techniques tend to generate patches that are different from those by developers for bugs with large patch sizes. Conclusion: Our study not only verifies the conclusions from previous studies but also highlights implications for future study towards assessing patch correctness.

</details>

<details>

<summary>2019-06-09 04:18:33 - Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems</summary>

- *Ting-Rui Chiang, Yun-Nung Chen*

- `1811.00720v2` - [abs](http://arxiv.org/abs/1811.00720v2) - [pdf](http://arxiv.org/pdf/1811.00720v2)

> Solving math word problems is a challenging task that requires accurate natural language understanding to bridge natural language texts and math expressions. Motivated by the intuition about how human generates the equations given the problem texts, this paper presents a neural approach to automatically solve math word problems by operating symbols according to their semantic meanings in texts. This paper views the process of generating equation as a bridge between the semantic world and the symbolic world, where the proposed neural math solver is based on an encoder-decoder framework. In the proposed model, the encoder is designed to understand the semantics of problems, and the decoder focuses on tracking semantic meanings of the generated symbols and then deciding which symbol to generate next. The preliminary experiments are conducted in a dataset Math23K, and our model significantly outperforms both the state-of-the-art single model and the best non-retrieval-based model over about 10% accuracy, demonstrating the effectiveness of bridging the symbolic and semantic worlds from math word problems.

</details>

<details>

<summary>2019-06-09 07:08:20 - Fine-grained Event Categorization with Heterogeneous Graph Convolutional Networks</summary>

- *Hao Peng, Jianxin Li, Qiran Gong, Yangqiu Song, Yuanxing Ning, Kunfeng Lai, Philip S. Yu*

- `1906.04580v1` - [abs](http://arxiv.org/abs/1906.04580v1) - [pdf](http://arxiv.org/pdf/1906.04580v1)

> Events are happening in real-world and real-time, which can be planned and organized occasions involving multiple people and objects. Social media platforms publish a lot of text messages containing public events with comprehensive topics. However, mining social events is challenging due to the heterogeneous event elements in texts and explicit and implicit social network structures. In this paper, we design an event meta-schema to characterize the semantic relatedness of social events and build an event-based heterogeneous information network (HIN) integrating information from external knowledge base, and propose a novel Pair-wise Popularity Graph Convolutional Network (PP-GCN) based fine-grained social event categorization model. We propose a Knowledgeable meta-paths Instances based social Event Similarity (KIES) between events and build a weighted adjacent matrix as input to the PP-GCN model. Comprehensive experiments on real data collections are conducted to compare various social event detection and clustering tasks. Experimental results demonstrate that our proposed framework outperforms other alternative social event categorization techniques.

</details>

<details>

<summary>2019-06-09 07:23:45 - Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for Large-Scale Multi-Label Text Classification</summary>

- *Hao Peng, Jianxin Li, Qiran Gong, Senzhang Wang, Lifang He, Bo Li, Lihong Wang, Philip S. Yu*

- `1906.04898v1` - [abs](http://arxiv.org/abs/1906.04898v1) - [pdf](http://arxiv.org/pdf/1906.04898v1)

> CNNs, RNNs, GCNs, and CapsNets have shown significant insights in representation learning and are widely used in various text mining tasks such as large-scale multi-label text classification. However, most existing deep models for multi-label text classification consider either the non-consecutive and long-distance semantics or the sequential semantics, but how to consider them both coherently is less studied. In addition, most existing methods treat output labels as independent methods, but ignore the hierarchical relations among them, leading to useful semantic information loss. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent CNNs framework for large-scale multi-label text classification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding words-matrix representation which preserves both the non-consecutive, long-distance and local sequential semantics. Then the words-matrix is input to the proposed attentional graph capsule recurrent CNNs for more effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-label text classification by comparing with state-of-the-art approaches.

</details>

<details>

<summary>2019-06-09 09:45:24 - Probing for Semantic Classes: Diagnosing the Meaning Content of Word Embeddings</summary>

- *Yadollah Yaghoobzadeh, Katharina Kann, Timothy J. Hazen, Eneko Agirre, Hinrich Schütze*

- `1906.03608v1` - [abs](http://arxiv.org/abs/1906.03608v1) - [pdf](http://arxiv.org/pdf/1906.03608v1)

> Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and word senses, where word senses from different words are related by semantic classes. This is the basis for novel diagnostic tests for an embedding's content: we probe word embeddings for semantic classes and analyze the embedding space by classifying embeddings into semantic classes. Our main findings are: (i) Information about a sense is generally represented well in a single-vector embedding - if the sense is frequent. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses.

</details>

<details>

<summary>2019-06-09 16:56:31 - Question Answering as Global Reasoning over Semantic Abstractions</summary>

- *Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Dan Roth*

- `1906.03672v1` - [abs](http://arxiv.org/abs/1906.03672v1) - [pdf](http://arxiv.org/pdf/1906.03672v1)

> We propose a novel method for exploiting the semantic structure of text to answer multiple-choice questions. The approach is especially suitable for domains that require reasoning over a diverse set of linguistic constructs but have limited training data. To address these challenges, we present the first system, to the best of our knowledge, that reasons over a wide range of semantic abstractions of the text, which are derived using off-the-shelf, general-purpose, pre-trained natural language modules such as semantic role labelers, coreference resolvers, and dependency parsers. Representing multiple abstractions as a family of graphs, we translate question answering (QA) into a search for an optimal subgraph that satisfies certain global and local properties. This formulation generalizes several prior structured QA systems. Our system, SEMANTICILP, demonstrates strong performance on two domains simultaneously. In particular, on a collection of challenging science QA datasets, it outperforms various state-of-the-art approaches, including neural models, broad coverage information retrieval, and specialized techniques using structured knowledge bases, by 2%-6%.

</details>

<details>

<summary>2019-06-09 17:30:52 - Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations</summary>

- *David Acuna, Amlan Kar, Sanja Fidler*

- `1904.07934v2` - [abs](http://arxiv.org/abs/1904.07934v2) - [pdf](http://arxiv.org/pdf/1904.07934v2)

> We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.

</details>

<details>

<summary>2019-06-09 18:11:47 - Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention</summary>

- *Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, William Yang Wang*

- `1905.12866v3` - [abs](http://arxiv.org/abs/1905.12866v3) - [pdf](http://arxiv.org/pdf/1905.12866v3)

> Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.

</details>

<details>

<summary>2019-06-09 19:14:18 - UBC-NLP at SemEval-2019 Task 6:Ensemble Learning of Offensive Content With Enhanced Training Data</summary>

- *Arun Rajendran, Chiyu Zhang, Muhammad Abdul-Mageed*

- `1906.03692v1` - [abs](http://arxiv.org/abs/1906.03692v1) - [pdf](http://arxiv.org/pdf/1906.03692v1)

> We examine learning offensive content on Twitter with limited, imbalanced data. For the purpose, we investigate the utility of using various data enhancement methods with a host of classical ensemble classifiers. Among the 75 participating teams in SemEval-2019 sub-task B, our system ranks 6th (with 0.706 macro F1-score). For sub-task C, among the 65 participating teams, our system ranks 9th (with 0.587 macro F1-score).

</details>

<details>

<summary>2019-06-09 23:12:40 - Write, Execute, Assess: Program Synthesis with a REPL</summary>

- *Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, Armando Solar-Lezama*

- `1906.04604v1` - [abs](http://arxiv.org/abs/1906.04604v1) - [pdf](http://arxiv.org/pdf/1906.04604v1)

> We present a neural program synthesis approach integrating components which write, execute, and assess code to navigate the search space of possible programs. We equip the search process with an interpreter or a read-eval-print-loop (REPL), which immediately executes partially written programs, exposing their semantics. The REPL addresses a basic challenge of program synthesis: tiny changes in syntax can lead to huge changes in semantics. We train a pair of models, a policy that proposes the new piece of code to write, and a value function that assesses the prospects of the code written so-far. At test time we can combine these models with a Sequential Monte Carlo algorithm. We apply our approach to two domains: synthesizing text editing programs and inferring 2D and 3D graphics programs.

</details>

<details>

<summary>2019-06-10 07:10:07 - Practical Byte-Granular Memory Blacklisting using Califorms</summary>

- *Hiroshi Sasaki, Miguel A. Arroyo, M. Tarek Ibn Ziad, Koustubha Bhat, Kanad Sinha, Simha Sethumadhavan*

- `1906.01838v3` - [abs](http://arxiv.org/abs/1906.01838v3) - [pdf](http://arxiv.org/pdf/1906.01838v3)

> Recent rapid strides in memory safety tools and hardware have improved software quality and security. While coarse-grained memory safety has improved, achieving memory safety at the granularity of individual objects remains a challenge due to high performance overheads which can be between ~1.7x-2.2x. In this paper, we present a novel idea called Califorms, and associated program observations, to obtain a low overhead security solution for practical, byte-granular memory safety.   The idea we build on is called memory blacklisting, which prohibits a program from accessing certain memory regions based on program semantics. State of the art hardware-supported memory blacklisting while much faster than software blacklisting creates memory fragmentation (of the order of few bytes) for each use of the blacklisted location. In this paper, we observe that metadata used for blacklisting can be stored in dead spaces in a program's data memory and that this metadata can be integrated into microarchitecture by changing the cache line format. Using these observations, Califorms based system proposed in this paper reduces the performance overheads of memory safety to ~1.02x-1.16x while providing byte-granular protection and maintaining very low hardware overheads.   The low overhead offered by Califorms enables always on, memory safety for small and large objects alike, and the fundamental idea of storing metadata in empty spaces, and microarchitecture can be used for other security and performance applications.

</details>

<details>

<summary>2019-06-10 08:42:57 - SCGDet: Malware Detection using Semantic Features Based on Reachability Relation</summary>

- *Renjie Lu*

- `1906.04632v1` - [abs](http://arxiv.org/abs/1906.04632v1) - [pdf](http://arxiv.org/pdf/1906.04632v1)

> Recently, with the booming development of software industry, more and more malware variants are designed to perform malicious behaviors. The evolution of malware makes it difficult to detect using traditional signature-based methods. Moreover, malware detection has important effect on system security. In this paper, we present SCGDet, which is a novel malware detection method based on system call graph model (SCGM). We first develop a system call pruning method, which can exclude system calls that have little impact on malware detection. Then we propose the SCGM, which can capture the semantic features of run-time program by grouping the system calls based on the reachability relation. We aim to obtain the generic representation of malicious behaviors with similar system call patterns. We evaluate the performance of SCGDet using different machine learning algorithms on the dataset including 854 malware samples and 740 benign samples. Compared with the traditional n-gram method, the SCGDet has the smaller feature space, the higher detection accuracy and the lower false positives. Experimental results show that SCGDet can reduce the average FPR of 14.75% and improve the average Accuracy of 8.887%, and can obtain a TPR of 97.44%, an FPR of 1.96% and an Accuracy of 97.78% in the best case.

</details>

<details>

<summary>2019-06-10 09:17:30 - Few-Shot Learning with Per-Sample Rich Supervision</summary>

- *Roman Visotsky, Yuval Atzmon, Gal Chechik*

- `1906.03859v1` - [abs](http://arxiv.org/abs/1906.03859v1) - [pdf](http://arxiv.org/pdf/1906.03859v1)

> Learning with few samples is a major challenge for parameter-rich models like deep networks. In contrast, people learn complex new concepts even from very few examples, suggesting that the sample complexity of learning can often be reduced. Many approaches to few-shot learning build on transferring a representation from well-sampled classes, or using meta learning to favor architectures that can learn with few samples. Unfortunately, such approaches often struggle when learning in an online way or with non-stationary data streams. Here we describe a new approach to learn with fewer samples, by using additional information that is provided per sample. Specifically, we show how the sample complexity can be reduced by providing semantic information about the relevance of features per sample, like information about the presence of objects in a scene or confidence of detecting attributes in an image. We provide an improved generalization error bound for this case. We cast the problem of using per-sample feature relevance by using a new ellipsoid-margin loss, and develop an online algorithm that minimizes this loss effectively. Empirical evaluation on two machine vision benchmarks for scene classification and fine-grain bird classification demonstrate the benefits of this approach for few-shot learning.

</details>

<details>

<summary>2019-06-10 12:57:56 - Multimodal Logical Inference System for Visual-Textual Entailment</summary>

- *Riko Suzuki, Hitomi Yanaka, Masashi Yoshikawa, Koji Mineshima, Daisuke Bekki*

- `1906.03952v1` - [abs](http://arxiv.org/abs/1906.03952v1) - [pdf](http://arxiv.org/pdf/1906.03952v1)

> A large amount of research about multimodal inference across text and vision has been recently developed to obtain visually grounded word and sentence representations. In this paper, we use logic-based representations as unified meaning representations for texts and images and present an unsupervised multimodal logical inference system that can effectively prove entailment relations between them. We show that by combining semantic parsing and theorem proving, the system can handle semantically complex sentences for visual-textual inference.

</details>

<details>

<summary>2019-06-10 14:25:46 - Nail Polish Try-On: Realtime Semantic Segmentation of Small Objects for Native and Browser Smartphone AR Applications</summary>

- *Brendan Duke, Abdalla Ahmed, Edmund Phung, Irina Kezele, Parham Aarabi*

- `1906.02222v2` - [abs](http://arxiv.org/abs/1906.02222v2) - [pdf](http://arxiv.org/pdf/1906.02222v2)

> We provide a system for semantic segmentation of small objects that enables nail polish try-on AR applications to run client-side in realtime in native and web mobile applications. By adjusting input resolution and neural network depth, our model design enables a smooth trade-off of performance and runtime, with the highest performance setting achieving~\num{94.5} mIoU at 29.8ms runtime in native applications on an iPad Pro. We also provide a postprocessing and rendering algorithm for nail polish try-on, which integrates with our semantic segmentation and fingernail base-tip direction predictions.

</details>

<details>

<summary>2019-06-10 14:57:52 - Joint Semantic Domain Alignment and Target Classifier Learning for Unsupervised Domain Adaptation</summary>

- *Dong-Dong Chen, Yisen Wang, Jinfeng Yi, Zaiyi Chen, Zhi-Hua Zhou*

- `1906.04053v1` - [abs](http://arxiv.org/abs/1906.04053v1) - [pdf](http://arxiv.org/pdf/1906.04053v1)

> Unsupervised domain adaptation aims to transfer the classifier learned from the source domain to the target domain in an unsupervised manner. With the help of target pseudo-labels, aligning class-level distributions and learning the classifier in the target domain are two widely used objectives. Existing methods often separately optimize these two individual objectives, which makes them suffer from the neglect of the other. However, optimizing these two aspects together is not trivial. To alleviate the above issues, we propose a novel method that jointly optimizes semantic domain alignment and target classifier learning in a holistic way. The joint optimization mechanism can not only eliminate their weaknesses but also complement their strengths. The theoretical analysis also verifies the favor of the joint optimization mechanism. Extensive experiments on benchmark datasets show that the proposed method yields the best performance in comparison with the state-of-the-art unsupervised domain adaptation methods.

</details>

<details>

<summary>2019-06-10 21:54:33 - Automated Curriculum Learning for Turn-level Spoken Language Understanding with Weak Supervision</summary>

- *Hao Lang, Wen Wang*

- `1906.04291v1` - [abs](http://arxiv.org/abs/1906.04291v1) - [pdf](http://arxiv.org/pdf/1906.04291v1)

> We propose a learning approach for turn-level spoken language understanding, which facilitates a user to speak one or more utterances compositionally in a turn for completing a task (e.g., voice ordering). A typical pipelined approach for these understanding tasks requires non-trivial annotation effort for developing its multiple components. Also, the pipeline is difficult to port to a new domain or scale up. To address these problems, we propose an end-to-end statistical model with weak supervision. We employ randomized beam search with memory augmentation (RBSMA) to solve complicated problems for which long promising trajectories are usually difficult to explore. Furthermore, considering the diversity of problem complexity, we explore automated curriculum learning (CL) for weak supervision to accelerate exploration and learning. We evaluate the proposed approach on real-world user logs of a commercial voice ordering system. Results demonstrate that when trained on a small number of end-to-end annotated sessions collected with low cost, our model performs comparably to the deployed pipelined system, saving the development labor over an order of magnitude. The RBSMA algorithm improves the test set accuracy by 7.8% relative compared to the standard beam search. Automated CL leads to better generalization and further improves the test set accuracy by 5% relative.

</details>

<details>

<summary>2019-06-11 03:17:53 - Deep Unified Multimodal Embeddings for Understanding both Content and Users in Social Media Networks</summary>

- *Karan Sikka, Lucas Van Bramer, Ajay Divakaran*

- `1905.07075v3` - [abs](http://arxiv.org/abs/1905.07075v3) - [pdf](http://arxiv.org/pdf/1905.07075v3)

> There has been an explosion of multimodal content generated on social media networks in the last few years, which has necessitated a deeper understanding of social media content and user behavior. We present a novel content-independent content-user-reaction model for social multimedia content analysis. Compared to prior works that generally tackle semantic content understanding and user behavior modeling in isolation, we propose a generalized solution to these problems within a unified framework. We embed users, images and text drawn from open social media in a common multimodal geometric space, using a novel loss function designed to cope with distant and disparate modalities, and thereby enable seamless three-way retrieval. Our model not only outperforms unimodal embedding based methods on cross-modal retrieval tasks but also shows improvements stemming from jointly solving the two tasks on Twitter data. We also show that the user embeddings learned within our joint multimodal embedding model are better at predicting user interests compared to those learned with unimodal content on Instagram data. Our framework thus goes beyond the prior practice of using explicit leader-follower link information to establish affiliations by extracting implicit content-centric affiliations from isolated users. We provide qualitative results to show that the user clusters emerging from learned embeddings have consistent semantics and the ability of our model to discover fine-grained semantics from noisy and unstructured data. Our work reveals that social multimodal content is inherently multimodal and possesses a consistent structure because in social networks meaning is created through interactions between users and content.

</details>

<details>

<summary>2019-06-11 03:51:09 - PAN: Projective Adversarial Network for Medical Image Segmentation</summary>

- *Naji Khosravan, Aliasghar Mortazi, Michael Wallace, Ulas Bagci*

- `1906.04378v1` - [abs](http://arxiv.org/abs/1906.04378v1) - [pdf](http://arxiv.org/pdf/1906.04378v1)

> Adversarial learning has been proven to be effective for capturing long-range and high-level label consistencies in semantic segmentation. Unique to medical imaging, capturing 3D semantics in an effective yet computationally efficient way remains an open problem. In this study, we address this computational burden by proposing a novel projective adversarial network, called PAN, which incorporates high-level 3D information through 2D projections. Furthermore, we introduce an attention module into our framework that helps for a selective integration of global information directly from our segmentor to our adversarial network. For the clinical application we chose pancreas segmentation from CT scans. Our proposed framework achieved state-of-the-art performance without adding to the complexity of the segmentor.

</details>

<details>

<summary>2019-06-11 08:54:23 - Reinforcement Learning of Minimalist Numeral Grammars</summary>

- *Peter beim Graben, Ronald Römer, Werner Meyer, Markus Huber, Matthias Wolff*

- `1906.04447v1` - [abs](http://arxiv.org/abs/1906.04447v1) - [pdf](http://arxiv.org/pdf/1906.04447v1)

> Speech-controlled user interfaces facilitate the operation of devices and household functions to laymen. State-of-the-art language technology scans the acoustically analyzed speech signal for relevant keywords that are subsequently inserted into semantic slots to interpret the user's intent. In order to develop proper cognitive information and communication technologies, simple slot-filling should be replaced by utterance meaning transducers (UMT) that are based on semantic parsers and a \emph{mental lexicon}, comprising syntactic, phonetic and semantic features of the language under consideration. This lexicon must be acquired by a cognitive agent during interaction with its users. We outline a reinforcement learning algorithm for the acquisition of the syntactic morphology and arithmetic semantics of English numerals, based on minimalist grammar (MG), a recent computational implementation of generative linguistics. Number words are presented to the agent by a teacher in form of utterance meaning pairs (UMP) where the meanings are encoded as arithmetic terms from a suitable term algebra. Since MG encodes universal linguistic competence through inference rules, thereby separating innate linguistic knowledge from the contingently acquired lexicon, our approach unifies generative grammar and reinforcement learning, hence potentially resolving the still pending Chomsky-Skinner controversy.

</details>

<details>

<summary>2019-06-11 13:37:11 - Semantically Constrained Multilayer Annotation: The Case of Coreference</summary>

- *Jakob Prange, Nathan Schneider, Omri Abend*

- `1906.00663v3` - [abs](http://arxiv.org/abs/1906.00663v3) - [pdf](http://arxiv.org/pdf/1906.00663v3)

> We propose a coreference annotation scheme as a layer on top of the Universal Conceptual Cognitive Annotation foundational layer, treating units in predicate-argument structure as a basis for entity and event mentions. We argue that this allows coreference annotators to sidestep some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with annotations from other schemes.

</details>

<details>

<summary>2019-06-11 16:30:27 - Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network</summary>

- *Sunil Kumar Sahu, Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou*

- `1906.04684v1` - [abs](http://arxiv.org/abs/1906.04684v1) - [pdf](http://arxiv.org/pdf/1906.04684v1)

> Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.

</details>

<details>

<summary>2019-06-11 17:15:32 - Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations</summary>

- *Amita Misra, Mansurul Bhuiyan, Jalal Mahmud, Saurabh Tripathy*

- `1906.04706v1` - [abs](http://arxiv.org/abs/1906.04706v1) - [pdf](http://arxiv.org/pdf/1906.04706v1)

> Twitter customer service interactions have recently emerged as an effective platform to respond and engage with customers. In this work, we explore the role of negation in customer service interactions, particularly applied to sentiment analysis. We define rules to identify true negation cues and scope more suited to conversational data than existing general review data. Using semantic knowledge and syntactic structure from constituency parse trees, we propose an algorithm for scope detection that performs comparable to state of the art BiLSTM. We further investigate the results of negation scope detection for the sentiment prediction task on customer service conversation data using both a traditional SVM and a Neural Network. We propose an antonym dictionary based method for negation applied to a CNN-LSTM combination model for sentiment analysis. Experimental results show that the antonym-based method outperforms the previous lexicon-based and neural network methods.

</details>

<details>

<summary>2019-06-11 17:35:02 - Deep Learning based Emotion Recognition System Using Speech Features and Transcriptions</summary>

- *Suraj Tripathi, Abhay Kumar, Abhiram Ramesh, Chirag Singh, Promod Yenigalla*

- `1906.05681v1` - [abs](http://arxiv.org/abs/1906.05681v1) - [pdf](http://arxiv.org/pdf/1906.05681v1)

> This paper proposes a speech emotion recognition method based on speech features and speech transcriptions (text). Speech features such as Spectrogram and Mel-frequency Cepstral Coefficients (MFCC) help retain emotion-related low-level characteristics in speech whereas text helps capture semantic meaning, both of which help in different aspects of emotion detection. We experimented with several Deep Neural Network (DNN) architectures, which take in different combinations of speech features and text as inputs. The proposed network architectures achieve higher accuracies when compared to state-of-the-art methods on a benchmark dataset. The combined MFCC-Text Convolutional Neural Network (CNN) model proved to be the most accurate in recognizing emotions in IEMOCAP data.

</details>

<details>

<summary>2019-06-11 17:38:28 - From Fully Supervised to Zero Shot Settings for Twitter Hashtag Recommendation</summary>

- *Abhay Kumar, Nishant Jain, Suraj Tripathi, Chirag Singh*

- `1906.04914v1` - [abs](http://arxiv.org/abs/1906.04914v1) - [pdf](http://arxiv.org/pdf/1906.04914v1)

> We propose a comprehensive end-to-end pipeline for Twitter hashtags recommendation system including data collection, supervised training setting and zero shot training setting. In the supervised training setting, we have proposed and compared the performance of various deep learning architectures, namely Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Transformer Network. However, it is not feasible to collect data for all possible hashtag labels and train a classifier model on them. To overcome this limitation, we propose a Zero Shot Learning (ZSL) paradigm for predicting unseen hashtag labels by learning the relationship between the semantic space of tweets and the embedding space of hashtag labels. We evaluated various state-of-the-art ZSL methods like Convex combination of Semantic Embedding (ConSE), Embarrassingly Simple Zero-Shot Learning (ESZSL) and Deep Embedding Model for Zero-Shot Learning (DEM-ZSL) for the hashtag recommendation task. We demonstrate the effectiveness and scalability of ZSL methods for the recommendation of unseen hashtags. To the best of our knowledge, this is the first quantitative evaluation of ZSL methods to date for unseen hashtags recommendations from tweet text.

</details>

<details>

<summary>2019-06-11 21:34:09 - Weakly-supervised Compositional FeatureAggregation for Few-shot Recognition</summary>

- *Ping Hu, Ximeng Sun, Kate Saenko, Stan Sclaroff*

- `1906.04833v1` - [abs](http://arxiv.org/abs/1906.04833v1) - [pdf](http://arxiv.org/pdf/1906.04833v1)

> Learning from a few examples is a challenging task for machine learning. While recent progress has been made for this problem, most of the existing methods ignore the compositionality in visual concept representation (e.g. objects are built from parts or composed of semantic attributes), which is key to the human ability to easily learn from a small number of examples. To enhance the few-shot learning models with compositionality, in this paper we present the simple yet powerful Compositional Feature Aggregation (CFA) module as a weakly-supervised regularization for deep networks. Given the deep feature maps extracted from the input, our CFA module first disentangles the feature space into disjoint semantic subspaces that model different attributes, and then bilinearly aggregates the local features within each of these subspaces. CFA explicitly regularizes the representation with both semantic and spatial compositionality to produce discriminative representations for few-shot recognition tasks. Moreover, our method does not need any supervision for attributes and object parts during training, thus can be conveniently plugged into existing models for end-to-end optimization while keeping the model size and computation cost nearly the same. Extensive experiments on few-shot image classification and action recognition tasks demonstrate that our method provides substantial improvements over recent state-of-the-art methods.

</details>

<details>

<summary>2019-06-12 02:48:57 - Does BLEU Score Work for Code Migration?</summary>

- *Ngoc Tran, Hieu Tran, Son Nguyen, Hoan Nguyen, Tien N. Nguyen*

- `1906.04903v1` - [abs](http://arxiv.org/abs/1906.04903v1) - [pdf](http://arxiv.org/pdf/1906.04903v1)

> Statistical machine translation (SMT) is a fast-growing sub-field of computational linguistics. Until now, the most popular automatic metric to measure the quality of SMT is BiLingual Evaluation Understudy (BLEU) score. Lately, SMT along with the BLEU metric has been applied to a Software Engineering task named code migration. (In)Validating the use of BLEU score could advance the research and development of SMT-based code migration tools. Unfortunately, there is no study to approve or disapprove the use of BLEU score for source code. In this paper, we conducted an empirical study on BLEU score to (in)validate its suitability for the code migration task due to its inability to reflect the semantics of source code. In our work, we use human judgment as the ground truth to measure the semantic correctness of the migrated code. Our empirical study demonstrates that BLEU does not reflect translation quality due to its weak correlation with the semantic correctness of translated code. We provided counter-examples to show that BLEU is ineffective in comparing the translation quality between SMT-based models. Due to BLEU's ineffectiveness for code migration task, we propose an alternative metric RUBY, which considers lexical, syntactical, and semantic representations of source code. We verified that RUBY achieves a higher correlation coefficient with the semantic correctness of migrated code, 0.775 in comparison with 0.583 of BLEU score. We also confirmed the effectiveness of RUBY in reflecting the changes in translation quality of SMT-based translation models. With its advantages, RUBY can be used to evaluate SMT-based code migration models.

</details>

<details>

<summary>2019-06-12 11:27:38 - Unified Semantic Parsing with Weak Supervision</summary>

- *Priyanka Agrawal, Parag Jain, Ayushi Dalmia, Abhishek Bansal, Ashish Mittal, Karthik Sankaranarayanan*

- `1906.05062v1` - [abs](http://arxiv.org/abs/1906.05062v1) - [pdf](http://arxiv.org/pdf/1906.05062v1)

> Semantic parsing over multiple knowledge bases enables a parser to exploit structural similarities of programs across the multiple domains. However, the fundamental challenge lies in obtaining high-quality annotations of (utterance, program) pairs across various domains needed for training such models. To overcome this, we propose a novel framework to build a unified multi-domain enabled semantic parser trained only with weak supervision (denotations). Weakly supervised training is particularly arduous as the program search space grows exponentially in a multi-domain setting. To solve this, we incorporate a multi-policy distillation mechanism in which we first train domain-specific semantic parsers (teachers) using weak supervision in the absence of the ground truth programs, followed by training a single unified parser (student) from the domain specific policies obtained from these teachers. The resultant semantic parser is not only compact but also generalizes better, and generates more accurate programs. It further does not require the user to provide a domain label while querying. On the standard Overnight dataset (containing multiple domains), we demonstrate that the proposed model improves performance by 20% in terms of denotation accuracy in comparison to baseline techniques.

</details>

<details>

<summary>2019-06-12 14:20:15 - Measuring the compositionality of noun-noun compounds over time</summary>

- *Prajit Dhar, Janis Pagel, Lonneke van der Plas*

- `1906.02563v2` - [abs](http://arxiv.org/abs/1906.02563v2) - [pdf](http://arxiv.org/pdf/1906.02563v2)

> We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.

</details>

<details>

<summary>2019-06-12 16:22:41 - VolMap: A Real-time Model for Semantic Segmentation of a LiDAR surrounding view</summary>

- *Hager Radi, Waleed Ali*

- `1906.11873v1` - [abs](http://arxiv.org/abs/1906.11873v1) - [pdf](http://arxiv.org/pdf/1906.11873v1)

> This paper introduces VolMap, a real-time approach for the semantic segmentation of a 3D LiDAR surrounding view system in autonomous vehicles. We designed an optimized deep convolution neural network that can accurately segment the point cloud produced by a 360\degree{} LiDAR setup, where the input consists of a volumetric bird-eye view with LiDAR height layers used as input channels. We further investigated the usage of multi-LiDAR setup and its effect on the performance of the semantic segmentation task. Our evaluations are carried out on a large scale 3D object detection benchmark containing a LiDAR cocoon setup, along with KITTI dataset, where the per-point segmentation labels are derived from 3D bounding boxes. We show that VolMap achieved an excellent balance between high accuracy and real-time running on CPU.

</details>

<details>

<summary>2019-06-12 17:29:22 - Representation Learning for Words and Entities</summary>

- *Pushpendre Rastogi*

- `1906.05651v1` - [abs](http://arxiv.org/abs/1906.05651v1) - [pdf](http://arxiv.org/pdf/1906.05651v1)

> This thesis presents new methods for unsupervised learning of distributed representations of words and entities from text and knowledge bases. The first algorithm presented in the thesis is a multi-view algorithm for learning representations of words called Multiview Latent Semantic Analysis (MVLSA). By incorporating up to 46 different types of co-occurrence statistics for the same vocabulary of english words, I show that MVLSA outperforms other state-of-the-art word embedding models. Next, I focus on learning entity representations for search and recommendation and present the second method of this thesis, Neural Variational Set Expansion (NVSE). NVSE is also an unsupervised learning method, but it is based on the Variational Autoencoder framework. Evaluations with human annotators show that NVSE can facilitate better search and recommendation of information gathered from noisy, automatic annotation of unstructured natural language corpora. Finally, I move from unstructured data and focus on structured knowledge graphs. I present novel approaches for learning embeddings of vertices and edges in a knowledge graph that obey logical constraints.

</details>

<details>

<summary>2019-06-13 08:33:23 - Enforcing temporal consistency in Deep Learning segmentation of brain MR images</summary>

- *Malav Bateriwala, Pierrick Bourgeat*

- `1906.07160v1` - [abs](http://arxiv.org/abs/1906.07160v1) - [pdf](http://arxiv.org/pdf/1906.07160v1)

> Longitudinal analysis has great potential to reveal developmental trajectories and monitor disease progression in medical imaging. This process relies on consistent and robust joint 4D segmentation. Traditional techniques are dependent on the similarity of images over time and the use of subject-specific priors to reduce random variation and improve the robustness and sensitivity of the overall longitudinal analysis. This is however slow and computationally intensive as subject-specific templates need to be rebuilt every time. The focus of this work to accelerate this analysis with the use of deep learning. The proposed approach is based on deep CNNs and incorporates semantic segmentation and provides a longitudinal relationship for the same subject. The proposed approach is based on deep CNNs and incorporates semantic segmentation and provides a longitudinal relationship for the same subject. The state of art using 3D patches as inputs to modified Unet provides results around ${0.91 \pm 0.5}$ Dice and using multi-view atlas in CNNs provide around the same results. In this work, different models are explored, each offers better accuracy and fast results while increasing the segmentation quality. These methods are evaluated on 135 scans from the EADC-ADNI Harmonized Hippocampus Protocol. Proposed CNN based segmentation approaches demonstrate how 2D segmentation using prior slices can provide similar results to 3D segmentation while maintaining good continuity in the 3D dimension and improved speed. Just using 2D modified sagittal slices provide us a better Dice and longitudinal analysis for a given subject. For the ADNI dataset, using the simple UNet CNN technique gives us ${0.84 \pm 0.5}$ and while using modified CNN techniques on the same input yields ${0.89 \pm 0.5}$. Rate of atrophy and RMS error are calculated for several test cases using various methods and analyzed.

</details>

<details>

<summary>2019-06-13 11:46:12 - Antonym-Synonym Classification Based on New Sub-space Embeddings</summary>

- *Muhammad Asif Ali, Yifang Sun, Xiaoling Zhou, Wei Wang, Xiang Zhao*

- `1906.05612v1` - [abs](http://arxiv.org/abs/1906.05612v1) - [pdf](http://arxiv.org/pdf/1906.05612v1)

> Distinguishing antonyms from synonyms is a key challenge for many NLP applications focused on the lexical-semantic relation extraction. Existing solutions relying on large-scale corpora yield low performance because of huge contextual overlap of antonym and synonym pairs. We propose a novel approach entirely based on pre-trained embeddings. We hypothesize that the pre-trained embeddings comprehend a blend of lexical-semantic information and we may distill the task-specific information using Distiller, a model proposed in this paper. Later, a classifier is trained based on features constructed from the distilled sub-spaces along with some word level features to distinguish antonyms from synonyms. Experimental results show that the proposed model outperforms existing research on antonym synonym distinction in both speed and performance.

</details>

<details>

<summary>2019-06-13 15:40:50 - Semantic Change and Semantic Stability: Variation is Key</summary>

- *Claire Bowern*

- `1906.05760v1` - [abs](http://arxiv.org/abs/1906.05760v1) - [pdf](http://arxiv.org/pdf/1906.05760v1)

> I survey some recent approaches to studying change in the lexicon, particularly change in meaning across phylogenies. I briefly sketch an evolutionary approach to language change and point out some issues in recent approaches to studying semantic change that rely on temporally stratified word embeddings. I draw illustrations from lexical cognate models in Pama-Nyungan to identify meaning classes most appropriate for lexical phylogenetic inference, particularly highlighting the importance of variation in studying change over time.

</details>

<details>

<summary>2019-06-13 17:04:46 - Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise</summary>

- *Chris Larson, Tarek Lahlou, Diana Mingels, Zachary Kulis, Erik Mueller*

- `1906.05678v1` - [abs](http://arxiv.org/abs/1906.05678v1) - [pdf](http://arxiv.org/pdf/1906.05678v1)

> Speech processing systems rely on robust feature extraction to handle phonetic and semantic variations found in natural language. While techniques exist for desensitizing features to common noise patterns produced by Speech-to-Text (STT) and Text-to-Speech (TTS) systems, the question remains how to best leverage state-of-the-art language models (which capture rich semantic features, but are trained on only written text) on inputs with ASR errors. In this paper, we present Telephonetic, a data augmentation framework that helps robustify language model features to ASR corrupted inputs. To capture phonetic alterations, we employ a character-level language model trained using probabilistic masking. Phonetic augmentations are generated in two stages: a TTS encoder (Tacotron 2, WaveGlow) and a STT decoder (DeepSpeech). Similarly, semantic perturbations are produced by sampling from nearby words in an embedding space, which is computed using the BERT language model. Words are selected for augmentation according to a hierarchical grammar sampling strategy. Telephonetic is evaluated on the Penn Treebank (PTB) corpus, and demonstrates its effectiveness as a bootstrapping technique for transferring neural language models to the speech domain. Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.

</details>

<details>

<summary>2019-06-13 20:07:28 - Multi-task Pairwise Neural Ranking for Hashtag Segmentation</summary>

- *Mounica Maddela, Wei Xu, Daniel Preoţiuc-Pietro*

- `1906.00790v2` - [abs](http://arxiv.org/abs/1906.00790v2) - [pdf](http://arxiv.org/pdf/1906.00790v2)

> Hashtags are often employed on social media and beyond to add metadata to a textual utterance with the goal of increasing discoverability, aiding search, or providing additional semantics. However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations. Our novel neural approaches demonstrate 24.6% error reduction in hashtag segmentation accuracy compared to the current state-of-the-art method. Finally, we demonstrate that a deeper understanding of hashtag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the SemEval 2017 sentiment analysis dataset.

</details>

<details>

<summary>2019-06-13 22:25:11 - Jointly Learning Semantic Parser and Natural Language Generator via Dual Information Maximization</summary>

- *Hai Ye, Wenjie Li, Lu Wang*

- `1906.00575v3` - [abs](http://arxiv.org/abs/1906.00575v3) - [pdf](http://arxiv.org/pdf/1906.00575v3)

> Semantic parsing aims to transform natural language (NL) utterances into formal meaning representations (MRs), whereas an NL generator achieves the reverse: producing a NL description for some given MRs. Despite this intrinsic connection, the two tasks are often studied separately in prior work. In this paper, we model the duality of these two tasks via a joint learning framework, and demonstrate its effectiveness of boosting the performance on both tasks. Concretely, we propose a novel method of dual information maximization (DIM) to regularize the learning process, where DIM empirically maximizes the variational lower bounds of expected joint distributions of NL and MRs. We further extend DIM to a semi-supervision setup (SemiDIM), which leverages unlabeled data of both tasks. Experiments on three datasets of dialogue management and code generation (and summarization) show that performance on both semantic parsing and NL generation can be consistently improved by DIM, in both supervised and semi-supervised setups.

</details>

<details>

<summary>2019-06-13 23:00:57 - 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</summary>

- *Christopher Choy, JunYoung Gwak, Silvio Savarese*

- `1904.08755v4` - [abs](http://arxiv.org/abs/1904.08755v4) - [pdf](http://arxiv.org/pdf/1904.08755v4)

> In many robotics and VR/AR applications, 3D-videos are readily-available sources of input (a continuous sequence of depth images, or LIDAR scans). However, those 3D-videos are processed frame-by-frame either through 2D convnets or 3D perception algorithms. In this work, we propose 4-dimensional convolutional neural networks for spatio-temporal perception that can directly process such 3D-videos using high-dimensional convolutions. For this, we adopt sparse tensors and propose the generalized sparse convolution that encompasses all discrete convolutions. To implement the generalized sparse convolution, we create an open-source auto-differentiation library for sparse tensors that provides extensive functions for high-dimensional convolutional neural networks. We create 4D spatio-temporal convolutional neural networks using the library and validate them on various 3D semantic segmentation benchmarks and proposed 4D datasets for 3D-video perception. To overcome challenges in the 4D space, we propose the hybrid kernel, a special case of the generalized sparse convolution, and the trilateral-stationary conditional random field that enforces spatio-temporal consistency in the 7D space-time-chroma space. Experimentally, we show that convolutional neural networks with only generalized 3D sparse convolutions can outperform 2D or 2D-3D hybrid methods by a large margin. Also, we show that on 3D-videos, 4D spatio-temporal convolutional neural networks are robust to noise, outperform 3D convolutional neural networks and are faster than the 3D counterpart in some cases.

</details>

<details>

<summary>2019-06-14 02:43:01 - Divide and Conquer the Embedding Space for Metric Learning</summary>

- *Artsiom Sanakoyeu, Vadim Tschernezki, Uta Büchler, Björn Ommer*

- `1906.05990v1` - [abs](http://arxiv.org/abs/1906.05990v1) - [pdf](http://arxiv.org/pdf/1906.05990v1)

> Learning the embedding space, where semantically similar objects are located close together and dissimilar objects far apart, is a cornerstone of many computer vision applications. Existing approaches usually learn a single metric in the embedding space for all available data points, which may have a very complex non-uniform distribution with different notions of similarity between objects, e.g. appearance, shape, color or semantic meaning. Approaches for learning a single distance metric often struggle to encode all different types of relationships and do not generalize well. In this work, we propose a novel easy-to-implement divide and conquer approach for deep metric learning, which significantly improves the state-of-the-art performance of metric learning. Our approach utilizes the embedding space more efficiently by jointly splitting the embedding space and data into $K$ smaller sub-problems. It divides both, the data and the embedding space into $K$ subsets and learns $K$ separate distance metrics in the non-overlapping subspaces of the embedding space, defined by groups of neurons in the embedding layer of the neural network. The proposed approach increases the convergence speed and improves generalization since the complexity of each sub-problem is reduced compared to the original one. We show that our approach outperforms the state-of-the-art by a large margin in retrieval, clustering and re-identification tasks on CUB200-2011, CARS196, Stanford Online Products, In-shop Clothes and PKU VehicleID datasets.

</details>

<details>

<summary>2019-06-14 16:37:40 - Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index</summary>

- *Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur P. Parikh, Ali Farhadi, Hannaneh Hajishirzi*

- `1906.05807v2` - [abs](http://arxiv.org/abs/1906.05807v2) - [pdf](http://arxiv.org/pdf/1906.05807v2)

> Existing open-domain question answering (QA) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query. In this paper, we introduce the query-agnostic indexable representation of document phrases that can drastically speed up open-domain QA and also allows us to reach long-tail targets. In particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents. Leveraging optimization strategies, our model can be trained in a single 4-GPU server and serve entire Wikipedia (up to 60 billion phrases) under 2TB with CPUs only. Our experiments on SQuAD-Open show that our model is more accurate than DrQA (Chen et al., 2017) with 6000x reduced computational cost, which translates into at least 58x faster end-to-end inference benchmark on CPUs.

</details>

<details>

<summary>2019-06-14 18:09:46 - Curate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural NLG</summary>

- *Shereen Oraby, Vrindavan Harrison, Abteen Ebrahimi, Marilyn Walker*

- `1906.01334v2` - [abs](http://arxiv.org/abs/1906.01334v2) - [pdf](http://arxiv.org/pdf/1906.01334v2)

> Neural natural language generation (NNLG) from structured meaning representations has become increasingly popular in recent years. While we have seen progress with generating syntactically correct utterances that preserve semantics, various shortcomings of NNLG systems are clear: new tasks require new training data which is not available or straightforward to acquire, and model outputs are simple and may be dull and repetitive. This paper addresses these two critical challenges in NNLG by: (1) scalably (and at no cost) creating training datasets of parallel meaning representations and reference texts with rich style markup by using data from freely available and naturally descriptive user reviews, and (2) systematically exploring how the style markup enables joint control of semantic and stylistic aspects of neural model output. We present YelpNLG, a corpus of 300,000 rich, parallel meaning representations and highly stylistically varied reference texts spanning different restaurant attributes, and describe a novel methodology that can be scalably reused to generate NLG datasets for other domains. The experiments show that the models control important aspects, including lexical choice of adjectives, output length, and sentiment, allowing the models to successfully hit multiple style targets without sacrificing semantics.

</details>

<details>

<summary>2019-06-15 01:26:56 - Robust or Private? Adversarial Training Makes Models More Vulnerable to Privacy Attacks</summary>

- *Felipe A. Mejia, Paul Gamble, Zigfried Hampel-Arias, Michael Lomnitz, Nina Lopatina, Lucas Tindall, Maria Alejandra Barrios*

- `1906.06449v1` - [abs](http://arxiv.org/abs/1906.06449v1) - [pdf](http://arxiv.org/pdf/1906.06449v1)

> Adversarial training was introduced as a way to improve the robustness of deep learning models to adversarial attacks. This training method improves robustness against adversarial attacks, but increases the models vulnerability to privacy attacks. In this work we demonstrate how model inversion attacks, extracting training data directly from the model, previously thought to be intractable become feasible when attacking a robustly trained model. The input space for a traditionally trained model is dominated by adversarial examples - data points that strongly activate a certain class but lack semantic meaning - this makes it difficult to successfully conduct model inversion attacks. We demonstrate this effect using the CIFAR-10 dataset under three different model inversion attacks, a vanilla gradient descent method, gradient based method at different scales, and a generative adversarial network base attacks.

</details>

<details>

<summary>2019-06-15 06:47:16 - Automatic Acrostic Couplet Generation with Three-Stage Neural Network Pipelines</summary>

- *Haoshen Fan, Jie Wang, Bojin Zhuang, Shaojun Wang, Jing Xiao*

- `1906.09321v1` - [abs](http://arxiv.org/abs/1906.09321v1) - [pdf](http://arxiv.org/pdf/1906.09321v1)

> As one of the quintessence of Chinese traditional culture, couplet compromises two syntactically symmetric clauses equal in length, namely, an antecedent and subsequent clause. Moreover, corresponding characters and phrases at the same position of the two clauses are paired with each other under certain constraints of semantic and/or syntactic relatedness. Automatic couplet generation is recognized as a challenging problem even in the Artificial Intelligence field. In this paper, we comprehensively study on automatic generation of acrostic couplet with the first characters defined by users. The complete couplet generation is mainly divided into three stages, that is, antecedent clause generation pipeline, subsequent clause generation pipeline and clause re-ranker. To realize semantic and/or syntactic relatedness between two clauses, attention-based Sequence-to-Sequence (S2S) neural network is employed. Moreover, to provide diverse couplet candidates for re-ranking, a cluster-based beam search approach is incorporated into the S2S network. Both BLEU metrics and human judgments have demonstrated the effectiveness of our proposed method. Eventually, a mini-program based on this generation system is developed and deployed on Wechat for real users.

</details>

<details>

<summary>2019-06-15 08:13:32 - A weakly supervised sequence tagging and grammar induction approach to semantic frame slot filling</summary>

- *Janneke van de Loo, Guy De Pauw, Walter Daelemans*

- `1906.06493v1` - [abs](http://arxiv.org/abs/1906.06493v1) - [pdf](http://arxiv.org/pdf/1906.06493v1)

> This paper describes continuing work on semantic frame slot filling for a command and control task using a weakly-supervised approach. We investigate the advantages of using retraining techniques that take the output of a hierarchical hidden markov model as input to two inductive approaches: (1) discriminative sequence labelers based on conditional random fields and memory-based learning and (2) probabilistic context-free grammar induction. Experimental results show that this setup can significantly improve F-scores without the need for additional information sources. Furthermore, qualitative analysis shows that the weakly supervised technique is able to automatically induce an easily interpretable and syntactically appropriate grammar for the domain and task at hand.

</details>

<details>

<summary>2019-06-15 09:09:12 - A Syllable-Structured, Contextually-Based Conditionally Generation of Chinese Lyrics</summary>

- *Xu Lu, Jie Wang, Bojin Zhuang, Shaojun Wang, Jing Xiao*

- `1906.09322v1` - [abs](http://arxiv.org/abs/1906.09322v1) - [pdf](http://arxiv.org/pdf/1906.09322v1)

> This paper presents a novel, syllable-structured Chinese lyrics generation model given a piece of original melody. Most previously reported lyrics generation models fail to include the relationship between lyrics and melody. In this work, we propose to interpret lyrics-melody alignments as syllable structural information and use a multi-channel sequence-to-sequence model with considering both phrasal structures and semantics. Two different RNN encoders are applied, one of which is for encoding syllable structures while the other for semantic encoding with contextual sentences or input keywords. Moreover, a large Chinese lyrics corpus for model training is leveraged. With automatic and human evaluations, results demonstrate the effectiveness of our proposed lyrics generation model. To the best of our knowledge, there is few previous reports on lyrics generation considering both music and linguistic perspectives.

</details>

<details>

<summary>2019-06-15 09:20:41 - Automatic Conditional Generation of Personalized Social Media Short Texts</summary>

- *Ziwen Wang, Jie Wang, Haiqian Gu, Fei Su, Bojin Zhuang*

- `1906.09324v1` - [abs](http://arxiv.org/abs/1906.09324v1) - [pdf](http://arxiv.org/pdf/1906.09324v1)

> Automatic text generation has received much attention owing to rapid development of deep neural networks. In general, text generation systems based on statistical language model will not consider anthropomorphic characteristics, which results in machine-like generated texts. To fill the gap, we propose a conditional language generation model with Big Five Personality (BFP) feature vectors as input context, which writes human-like short texts. The short text generator consists of a layer of long short memory network (LSTM), where a BFP feature vector is concatenated as one part of input for each cell. To enable supervised training generation model, a text classification model based convolution neural network (CNN) has been used to prepare BFP-tagged Chinese micro-blog corpora. Validated by a BFP linguistic computational model, our generated Chinese short texts exhibit discriminative personality styles, which are also syntactically correct and semantically smooth with appropriate emoticons. With combination of natural language generation with psychological linguistics, our proposed BFP-dependent text generation model can be widely used for individualization in machine translation, image caption, dialogue generation and so on.

</details>

<details>

<summary>2019-06-15 15:26:26 - Injecting Prior Knowledge for Transfer Learning into Reinforcement Learning Algorithms using Logic Tensor Networks</summary>

- *Samy Badreddine, Michael Spranger*

- `1906.06576v1` - [abs](http://arxiv.org/abs/1906.06576v1) - [pdf](http://arxiv.org/pdf/1906.06576v1)

> Human ability at solving complex tasks is helped by priors on object and event semantics of their environment. This paper investigates the use of similar prior knowledge for transfer learning in Reinforcement Learning agents. In particular, the paper proposes to use a first-order-logic language grounded in deep neural networks to represent facts about objects and their semantics in the real world. Facts are provided as background knowledge a priori to learning a policy for how to act in the world. The priors are injected with the conventional input in a single agent architecture. As proof-of-concept, the paper tests the system in simple experiments that show the importance of symbolic abstraction and flexible fact derivation. The paper shows that the proposed system can learn to take advantage of both the symbolic layer and the image layer in a single decision selection module.

</details>

<details>

<summary>2019-06-15 21:50:31 - Joint Visual-Textual Embedding for Multimodal Style Search</summary>

- *Gil Sadeh, Lior Fritz, Gabi Shalev, Eduard Oks*

- `1906.06620v1` - [abs](http://arxiv.org/abs/1906.06620v1) - [pdf](http://arxiv.org/pdf/1906.06620v1)

> We introduce a multimodal visual-textual search refinement method for fashion garments. Existing search engines do not enable intuitive, interactive, refinement of retrieved results based on the properties of a particular product. We propose a method to retrieve similar items, based on a query item image and textual refinement properties. We believe this method can be leveraged to solve many real-life customer scenarios, in which a similar item in a different color, pattern, length or style is desired. We employ a joint embedding training scheme in which product images and their catalog textual metadata are mapped closely in a shared space. This joint visual-textual embedding space enables manipulating catalog images semantically, based on textual refinement requirements. We propose a new training objective function, Mini-Batch Match Retrieval, and demonstrate its superiority over the commonly used triplet loss. Additionally, we demonstrate the feasibility of adding an attribute extraction module, trained on the same catalog data, and demonstrate how to integrate it within the multimodal search to boost its performance. We introduce an evaluation protocol with an associated benchmark, and compare several approaches.

</details>

<details>

<summary>2019-06-16 04:26:24 - Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks</summary>

- *Namyong Park, Andrey Kan, Xin Luna Dong, Tong Zhao, Christos Faloutsos*

- `1905.08865v2` - [abs](http://arxiv.org/abs/1905.08865v2) - [pdf](http://arxiv.org/pdf/1905.08865v2)

> How can we estimate the importance of nodes in a knowledge graph (KG)? A KG is a multi-relational graph that has proven valuable for many tasks including question answering and semantic search. In this paper, we present GENI, a method for tackling the problem of estimating node importance in KGs, which enables several downstream applications such as item recommendation and resource allocation. While a number of approaches have been developed to address this problem for general graphs, they do not fully utilize information available in KGs, or lack flexibility needed to model complex relationship between entities and their importance. To address these limitations, we explore supervised machine learning algorithms. In particular, building upon recent advancement of graph neural networks (GNNs), we develop GENI, a GNN-based method designed to deal with distinctive challenges involved with predicting node importance in KGs. Our method performs an aggregation of importance scores instead of aggregating node embeddings via predicate-aware attention mechanism and flexible centrality adjustment. In our evaluation of GENI and existing methods on predicting node importance in real-world KGs with different characteristics, GENI achieves 5-17% higher NDCG@100 than the state of the art.

</details>

<details>

<summary>2019-06-16 23:01:32 - Floors are Flat: Leveraging Semantics for Real-Time Surface Normal Prediction</summary>

- *Steven Hickson, Karthik Raveendran, Alireza Fathi, Kevin Murphy, Irfan Essa*

- `1906.06792v1` - [abs](http://arxiv.org/abs/1906.06792v1) - [pdf](http://arxiv.org/pdf/1906.06792v1)

> We propose 4 insights that help to significantly improve the performance of deep learning models that predict surface normals and semantic labels from a single RGB image. These insights are: (1) denoise the "ground truth" surface normals in the training set to ensure consistency with the semantic labels; (2) concurrently train on a mix of real and synthetic data, instead of pretraining on synthetic and finetuning on real; (3) jointly predict normals and semantics using a shared model, but only backpropagate errors on pixels that have valid training labels; (4) slim down the model and use grayscale instead of color inputs. Despite the simplicity of these steps, we demonstrate consistently improved results on several datasets, using a model that runs at 12 fps on a standard mobile phone.

</details>

<details>

<summary>2019-06-17 07:07:01 - Robust Zero-Shot Cross-Domain Slot Filling with Example Values</summary>

- *Darsh J Shah, Raghav Gupta, Amir A Fayazi, Dilek Hakkani-Tur*

- `1906.06870v1` - [abs](http://arxiv.org/abs/1906.06870v1) - [pdf](http://arxiv.org/pdf/1906.06870v1)

> Task-oriented dialog systems increasingly rely on deep learning-based slot filling models, usually needing extensive labeled training data for target domains. Often, however, little to no target domain training data may be available, or the training and target domain schemas may be misaligned, as is common for web forms on similar websites. Prior zero-shot slot filling models use slot descriptions to learn concepts, but are not robust to misaligned schemas. We propose utilizing both the slot description and a small number of examples of slot values, which may be easily available, to learn semantic representations of slots which are transferable across domains and robust to misaligned schemas. Our approach outperforms state-of-the-art models on two multi-domain datasets, especially in the low-data setting.

</details>

<details>

<summary>2019-06-17 07:07:41 - Universal Barcode Detector via Semantic Segmentation</summary>

- *Andrey Zharkov, Ivan Zagaynov*

- `1906.06281v2` - [abs](http://arxiv.org/abs/1906.06281v2) - [pdf](http://arxiv.org/pdf/1906.06281v2)

> Barcodes are used in many commercial applications, thus fast and robust reading is important. There are many different types of barcodes, some of them look similar while others are completely different. In this paper we introduce new fast and robust deep learning detector based on semantic segmentation approach. It is capable of detecting barcodes of any type simultaneously both in the document scans and in the wild by means of a single model. The detector achieves state-of-the-art results on the ArTe-Lab 1D Medium Barcode Dataset with detection rate 0.995. Moreover, developed detector can deal with more complicated object shapes like very long but narrow or very small barcodes. The proposed approach can also identify types of detected barcodes and performs at real-time speed on CPU environment being much faster than previous state-of-the-art approaches.

</details>

<details>

<summary>2019-06-17 08:26:43 - ParNet: Position-aware Aggregated Relation Network for Image-Text matching</summary>

- *Yaxian Xia, Lun Huang, Wenmin Wang, Xiaoyong Wei, Wenmin Wang*

- `1906.06892v1` - [abs](http://arxiv.org/abs/1906.06892v1) - [pdf](http://arxiv.org/pdf/1906.06892v1)

> Exploring fine-grained relationship between entities(e.g. objects in image or words in sentence) has great contribution to understand multimedia content precisely. Previous attention mechanism employed in image-text matching either takes multiple self attention steps to gather correspondences or uses image objects (or words) as context to infer image-text similarity. However, they only take advantage of semantic information without considering that objects' relative position also contributes to image understanding. To this end, we introduce a novel position-aware relation module to model both the semantic and spatial relationship simultaneously for image-text matching in this paper. Given an image, our method utilizes the location of different objects to capture spatial relationship innovatively. With the combination of semantic and spatial relationship, it's easier to understand the content of different modalities (images and sentences) and capture fine-grained latent correspondences of image-text pairs. Besides, we employ a two-step aggregated relation module to capture interpretable alignment of image-text pairs. The first step, we call it intra-modal relation mechanism, in which we computes responses between different objects in an image or different words in a sentence separately; The second step, we call it inter-modal relation mechanism, in which the query plays a role of textual context to refine the relationship among object proposals in an image. In this way, our position-aware aggregated relation network (ParNet) not only knows which entities are relevant by attending on different objects (words) adaptively, but also adjust the inter-modal correspondence according to the latent alignments according to query's content. Our approach achieves the state-of-the-art results on MS-COCO dataset.

</details>

<details>

<summary>2019-06-17 13:36:21 - Semi-Supervised Semantic Mapping through Label Propagation with Semantic Texture Meshes</summary>

- *Radu Alexandru Rosu, Jan Quenzel, Sven Behnke*

- `1906.07029v1` - [abs](http://arxiv.org/abs/1906.07029v1) - [pdf](http://arxiv.org/pdf/1906.07029v1)

> Scene understanding is an important capability for robots acting in unstructured environments. While most SLAM approaches provide a geometrical representation of the scene, a semantic map is necessary for more complex interactions with the surroundings. Current methods treat the semantic map as part of the geometry which limits scalability and accuracy. We propose to represent the semantic map as a geometrical mesh and a semantic texture coupled at independent resolution. The key idea is that in many environments the geometry can be greatly simplified without loosing fidelity, while semantic information can be stored at a higher resolution, independent of the mesh. We construct a mesh from depth sensors to represent the scene geometry and fuse information into the semantic texture from segmentations of individual RGB views of the scene. Making the semantics persistent in a global mesh enables us to enforce temporal and spatial consistency of the individual view predictions. For this, we propose an efficient method of establishing consensus between individual segmentations by iteratively retraining semantic segmentation with the information stored within the map and using the retrained segmentation to re-fuse the semantics. We demonstrate the accuracy and scalability of our approach by reconstructing semantic maps of scenes from NYUv2 and a scene spanning large buildings.

</details>

<details>

<summary>2019-06-17 14:01:33 - Making Fast Graph-based Algorithms with Graph Metric Embeddings</summary>

- *Andrey Kutuzov, Mohammad Dorgham, Oleksiy Oliynyk, Chris Biemann, Alexander Panchenko*

- `1906.07040v1` - [abs](http://arxiv.org/abs/1906.07040v1) - [pdf](http://arxiv.org/pdf/1906.07040v1)

> The computation of distance measures between nodes in graphs is inefficient and does not scale to large graphs. We explore dense vector representations as an effective way to approximate the same information: we introduce a simple yet efficient and effective approach for learning graph embeddings. Instead of directly operating on the graph structure, our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g.the shortest path distance or distance measures that take information beyond the graph structure into account. We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks and show evaluations on the WordNet graph and two knowledge base graphs.

</details>

<details>

<summary>2019-06-17 15:06:47 - The Attack Generator: A Systematic Approach Towards Constructing Adversarial Attacks</summary>

- *Felix Assion, Peter Schlicht, Florens Greßner, Wiebke Günther, Fabian Hüger, Nico Schmidt, Umair Rasheed*

- `1906.07077v1` - [abs](http://arxiv.org/abs/1906.07077v1) - [pdf](http://arxiv.org/pdf/1906.07077v1)

> Most state-of-the-art machine learning (ML) classification systems are vulnerable to adversarial perturbations. As a consequence, adversarial robustness poses a significant challenge for the deployment of ML-based systems in safety- and security-critical environments like autonomous driving, disease detection or unmanned aerial vehicles. In the past years we have seen an impressive amount of publications presenting more and more new adversarial attacks. However, the attack research seems to be rather unstructured and new attacks often appear to be random selections from the unlimited set of possible adversarial attacks. With this publication, we present a structured analysis of the adversarial attack creation process. By detecting different building blocks of adversarial attacks, we outline the road to new sets of adversarial attacks. We call this the "attack generator". In the pursuit of this objective, we summarize and extend existing adversarial perturbation taxonomies. The resulting taxonomy is then linked to the application context of computer vision systems for autonomous vehicles, i.e. semantic segmentation and object detection. Finally, in order to prove the usefulness of the attack generator, we investigate existing semantic segmentation attacks with respect to the detected defining components of adversarial attacks.

</details>

<details>

<summary>2019-06-17 16:18:28 - Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing</summary>

- *Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin*

- `1906.07108v1` - [abs](http://arxiv.org/abs/1906.07108v1) - [pdf](http://arxiv.org/pdf/1906.07108v1)

> In this paper, we present an approach to incorporate retrieved datapoints as supporting evidence for context-dependent semantic parsing, such as generating source code conditioned on the class environment. Our approach naturally combines a retrieval model and a meta-learner, where the former learns to find similar datapoints from the training data, and the latter considers retrieved datapoints as a pseudo task for fast adaptation. Specifically, our retriever is a context-aware encoder-decoder model with a latent variable which takes context environment into consideration, and our meta-learner learns to utilize retrieved datapoints in a model-agnostic meta-learning paradigm for fast adaptation. We conduct experiments on CONCODE and CSQA datasets, where the context refers to class environment in JAVA codes and conversational history, respectively. We use sequence-to-action model as the base semantic parser, which performs the state-of-the-art accuracy on both datasets. Results show that both the context-aware retriever and the meta-learning strategy improve accuracy, and our approach performs better than retrieve-and-edit baselines.

</details>

<details>

<summary>2019-06-17 18:54:51 - Constrained Decoding for Neural NLG from Compositional Representations in Task-Oriented Dialogue</summary>

- *Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, Rajen Subba*

- `1906.07220v1` - [abs](http://arxiv.org/abs/1906.07220v1) - [pdf](http://arxiv.org/pdf/1906.07220v1)

> Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.

</details>

<details>

<summary>2019-06-17 21:31:40 - A Structured Distributional Model of Sentence Meaning and Processing</summary>

- *Emmanuele Chersoni, Enrico Santus, Ludovica Pannitto, Alessandro Lenci, Philippe Blache, Chu-Ren Huang*

- `1906.07280v1` - [abs](http://arxiv.org/abs/1906.07280v1) - [pdf](http://arxiv.org/pdf/1906.07280v1)

> Most compositional distributional semantic models represent sentence meaning with a single vector. In this paper, we propose a Structured Distributional Model (SDM) that combines word embeddings with formal semantics and is based on the assumption that sentences represent events and situations. The semantic representation of a sentence is a formal structure derived from Discourse Representation Theory and containing distributional vectors. This structure is dynamically and incrementally built by integrating knowledge about events and their typical participants, as they are activated by lexical items. Event knowledge is modeled as a graph extracted from parsed corpora and encoding roles and relationships between participants that are represented as distributional vectors. SDM is grounded on extensive psycholinguistic research showing that generalized knowledge about events stored in semantic memory plays a key role in sentence comprehension. We evaluate SDM on two recently introduced compositionality datasets, and our results show that combining a simple compositional model with event knowledge constantly improves performances, even with different types of word embeddings.

</details>

<details>

<summary>2019-06-17 21:53:10 - Tabula nearly rasa: Probing the Linguistic Knowledge of Character-Level Neural Language Models Trained on Unsegmented Text</summary>

- *Michael Hahn, Marco Baroni*

- `1906.07285v1` - [abs](http://arxiv.org/abs/1906.07285v1) - [pdf](http://arxiv.org/pdf/1906.07285v1)

> Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our "near tabula rasa" RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage.

</details>

<details>

<summary>2019-06-17 23:48:05 - Towards Transfer Learning for End-to-End Speech Synthesis from Deep Pre-Trained Language Models</summary>

- *Wei Fang, Yu-An Chung, James Glass*

- `1906.07307v1` - [abs](http://arxiv.org/abs/1906.07307v1) - [pdf](http://arxiv.org/pdf/1906.07307v1)

> Modern text-to-speech (TTS) systems are able to generate audio that sounds almost as natural as human speech. However, the bar of developing high-quality TTS systems remains high since a sizable set of studio-quality <text, audio> pairs is usually required. Compared to commercial data used to develop state-of-the-art systems, publicly available data are usually worse in terms of both quality and size. Audio generated by TTS systems trained on publicly available data tends to not only sound less natural, but also exhibits more background noise. In this work, we aim to lower TTS systems' reliance on high-quality data by providing them the textual knowledge extracted by deep pre-trained language models during training. In particular, we investigate the use of BERT to assist the training of Tacotron-2, a state of the art TTS consisting of an encoder and an attention-based decoder. BERT representations learned from large amounts of unlabeled text data are shown to contain very rich semantic and syntactic information about the input text, and have potential to be leveraged by a TTS system to compensate the lack of high-quality data. We incorporate BERT as a parallel branch to the Tacotron-2 encoder with its own attention head. For an input text, it is simultaneously passed into BERT and the Tacotron-2 encoder. The representations extracted by the two branches are concatenated and then fed to the decoder. As a preliminary study, although we have not found incorporating BERT into Tacotron-2 generates more natural or cleaner speech at a human-perceivable level, we observe improvements in other aspects such as the model is being significantly better at knowing when to stop decoding such that there is much less babbling at the end of the synthesized audio and faster convergence during training.

</details>

<details>

<summary>2019-06-18 04:48:29 - Fast Adjustable Threshold For Uniform Neural Network Quantization (Winning solution of LPIRC-II)</summary>

- *Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev*

- `1812.07872v3` - [abs](http://arxiv.org/abs/1812.07872v3) - [pdf](http://arxiv.org/pdf/1812.07872v3)

> Neural network quantization procedure is the necessary step for porting of neural networks to mobile devices. Quantization allows accelerating the inference, reducing memory consumption and model size. It can be performed without fine-tuning using calibration procedure (calculation of parameters necessary for quantization), or it is possible to train the network with quantization from scratch. Training with quantization from scratch on the labeled data is rather long and resource-consuming procedure. Quantization of network without fine-tuning leads to accuracy drop because of outliers which appear during the calibration. In this article we suggest to simplify the quantization procedure significantly by introducing the trained scale factors for quantization thresholds. It allows speeding up the process of quantization with fine-tuning up to 8 epochs as well as reducing the requirements to the set of train images. By our knowledge, the proposed method allowed us to get the first public available quantized version of MNAS without significant accuracy reduction - 74.8% vs 75.3% for original full-precision network. Model and code are ready for use and available at: https://github.com/agoncharenko1992/FAT-fast_adjustable_threshold.

</details>

<details>

<summary>2019-06-18 05:14:17 - Curriculum Learning Strategies for Hindi-English Codemixed Sentiment Analysis</summary>

- *Anirudh Dahiya, Neeraj Battan, Manish Shrivastava, Dipti Mishra Sharma*

- `1906.07382v1` - [abs](http://arxiv.org/abs/1906.07382v1) - [pdf](http://arxiv.org/pdf/1906.07382v1)

> Sentiment Analysis and other semantic tasks are commonly used for social media textual analysis to gauge public opinion and make sense from the noise on social media. The language used on social media not only commonly diverges from the formal language, but is compounded by codemixing between languages, especially in large multilingual societies like India.   Traditional methods for learning semantic NLP tasks have long relied on end to end task specific training, requiring expensive data creation process, even more so for deep learning methods. This challenge is even more severe for resource scarce texts like codemixed language pairs, with lack of well learnt representations as model priors, and task specific datasets can be few and small in quantities to efficiently exploit recent deep learning approaches. To address above challenges, we introduce curriculum learning strategies for semantic tasks in code-mixed Hindi-English (Hi-En) texts, and investigate various training strategies for enhancing model performance. Our method outperforms the state of the art methods for Hi-En codemixed sentiment analysis by 3.31% accuracy, and also shows better model robustness in terms of convergence, and variance in test performance.

</details>

<details>

<summary>2019-06-18 07:57:22 - Modeling Semantic Relationship in Multi-turn Conversations with Hierarchical Latent Variables</summary>

- *Lei Shen, Yang Feng, Haolan Zhan*

- `1906.07429v1` - [abs](http://arxiv.org/abs/1906.07429v1) - [pdf](http://arxiv.org/pdf/1906.07429v1)

> Multi-turn conversations consist of complex semantic structures, and it is still a challenge to generate coherent and diverse responses given previous utterances. It's practical that a conversation takes place under a background, meanwhile, the query and response are usually most related and they are consistent in topic but also different in content. However, little work focuses on such hierarchical relationship among utterances. To address this problem, we propose a Conversational Semantic Relationship RNN (CSRR) model to construct the dependency explicitly. The model contains latent variables in three hierarchies. The discourse-level one captures the global background, the pair-level one stands for the common topic information between query and response, and the utterance-level ones try to represent differences in content. Experimental results show that our model significantly improves the quality of responses in terms of fluency, coherence and diversity compared to baseline methods.

</details>

<details>

<summary>2019-06-18 11:14:20 - From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images</summary>

- *Taibou Birgui Sekou, Moncef Hidane, Julien Olivier, Hubert Cardot*

- `1904.03892v2` - [abs](http://arxiv.org/abs/1904.03892v2) - [pdf](http://arxiv.org/pdf/1904.03892v2)

> Deep learning based models, generally, require a large number of samples for appropriate training, a requirement that is difficult to satisfy in the medical field. This issue can usually be avoided with a proper initialization of the weights. On the task of medical image segmentation in general, two techniques are oftentimes employed to tackle the training of a deep network $f_T$. The first one consists in reusing some weights of a network $f_S$ pre-trained on a large scale database ($e.g.$ ImageNet). This procedure, also known as $transfer$ $learning$, happens to reduce the flexibility when it comes to new network design since $f_T$ is constrained to match some parts of $f_S$. The second commonly used technique consists in working on image patches to benefit from the large number of available patches. This paper brings together these two techniques and propose to train $arbitrarily$ $designed$ $networks$ that segment an image in one forward pass, with a focus on relatively small databases. An experimental work have been carried out on the tasks of retinal blood vessel segmentation and the optic disc one, using four publicly available databases. Furthermore, three types of network are considered, going from a very light weighted network to a densely connected one. The final results show the efficiency of the proposed framework along with state of the art results on all the databases.

</details>

<details>

<summary>2019-06-18 12:27:25 - Mimicking Human Process: Text Representation via Latent Semantic Clustering for Classification</summary>

- *Xiaoye Tan, Rui Yan, Chongyang Tao, Mingrui Wu*

- `1906.07525v1` - [abs](http://arxiv.org/abs/1906.07525v1) - [pdf](http://arxiv.org/pdf/1906.07525v1)

> Considering that words with different characteristic in the text have different importance for classification, grouping them together separately can strengthen the semantic expression of each part. Thus we propose a new text representation scheme by clustering words according to their latent semantics and composing them together to get a set of cluster vectors, which are then concatenated as the final text representation. Evaluation on five classification benchmarks proves the effectiveness of our method. We further conduct visualization analysis showing statistical clustering results and verifying the validity of our motivation.

</details>

<details>

<summary>2019-06-18 13:32:20 - Hyperintensional Reasoning based on Natural Language Knowledge Base</summary>

- *Marie Duží, Aleš Horák*

- `1906.07562v1` - [abs](http://arxiv.org/abs/1906.07562v1) - [pdf](http://arxiv.org/pdf/1906.07562v1)

> The success of automated reasoning techniques over large natural-language texts heavily relies on a fine-grained analysis of natural language assumptions. While there is a common agreement that the analysis should be hyperintensional, most of the automatic reasoning systems are still based on an intensional logic, at the best. In this paper, we introduce the system of reasoning based on a fine-grained, hyperintensional analysis. To this end we apply Tichy's Transparent Intensional Logic (TIL) with its procedural semantics. TIL is a higher-order, hyperintensional logic of partial functions, in particular apt for a fine-grained natural-language analysis. Within TIL we recognise three kinds of context, namely extensional, intensional and hyperintensional, in which a particular natural-language term, or rather its meaning, can occur. Having defined the three kinds of context and implemented an algorithm of context recognition, we are in a position to develop and implement an extensional logic of hyperintensions with the inference machine that should neither over-infer nor under-infer.

</details>

<details>

<summary>2019-06-18 14:06:40 - Towards Robust Named Entity Recognition for Historic German</summary>

- *Stefan Schweter, Johannes Baiter*

- `1906.07592v1` - [abs](http://arxiv.org/abs/1906.07592v1) - [pdf](http://arxiv.org/pdf/1906.07592v1)

> Recent advances in language modeling using deep neural networks have shown that these models learn representations, that vary with the network depth from morphology to semantic relationships like co-reference. We apply pre-trained language models to low-resource named entity recognition for Historic German. We show on a series of experiments that character-based pre-trained language models do not run into trouble when faced with low-resource datasets. Our pre-trained character-based language models improve upon classical CRF-based methods and previous work on Bi-LSTMs by boosting F1 score performance by up to 6%. Our pre-trained language and NER models are publicly available under https://github.com/stefan-it/historic-ner .

</details>

<details>

<summary>2019-06-18 14:19:52 - Curriculum-based transfer learning for an effective end-to-end spoken language understanding and domain portability</summary>

- *Antoine Caubrière, Natalia Tomashenko, Antoine Laurent, Emmanuel Morin, Nathalie Camelin, Yannick Estève*

- `1906.07601v1` - [abs](http://arxiv.org/abs/1906.07601v1) - [pdf](http://arxiv.org/pdf/1906.07601v1)

> We present an end-to-end approach to extract semantic concepts directly from the speech audio signal. To overcome the lack of data available for this spoken language understanding approach, we investigate the use of a transfer learning strategy based on the principles of curriculum learning. This approach allows us to exploit out-of-domain data that can help to prepare a fully neural architecture. Experiments are carried out on the French MEDIA and PORTMEDIA corpora and show that this end-to-end SLU approach reaches the best results ever published on this task. We compare our approach to a classical pipeline approach that uses ASR, POS tagging, lemmatizer, chunker... and other NLP tools that aim to enrich ASR outputs that feed an SLU text to concepts system. Last, we explore the promising capacity of our end-to-end SLU approach to address the problem of domain portability.

</details>

<details>

<summary>2019-06-18 23:06:32 - A Static Analysis-based Cross-Architecture Performance Prediction Using Machine Learning</summary>

- *Newsha Ardalani, Urmish Thakker, Aws Albarghouthi, Karu Sankaralingam*

- `1906.07840v1` - [abs](http://arxiv.org/abs/1906.07840v1) - [pdf](http://arxiv.org/pdf/1906.07840v1)

> Porting code from CPU to GPU is costly and time-consuming; Unless much time is invested in development and optimization, it is not obvious, a priori, how much speed-up is achievable or how much room is left for improvement. Knowing the potential speed-up a priori can be very useful: It can save hundreds of engineering hours, help programmers with prioritization and algorithm selection. We aim to address this problem using machine learning in a supervised setting, using solely the single-threaded source code of the program, without having to run or profile the code. We propose a static analysis-based cross-architecture performance prediction framework (Static XAPP) which relies solely on program properties collected using static analysis of the CPU source code and predicts whether the potential speed-up is above or below a given threshold. We offer preliminary results that show we can achieve 94% accuracy in binary classification, in average, across different thresholds

</details>

<details>

<summary>2019-06-19 03:45:37 - Memetic EDA-Based Approaches to Comprehensive Quality-Aware Automated Semantic Web Service Composition</summary>

- *Chen Wang, Hui Ma, Gang Chen, Sven Hartmann*

- `1906.07900v1` - [abs](http://arxiv.org/abs/1906.07900v1) - [pdf](http://arxiv.org/pdf/1906.07900v1)

> Comprehensive quality-aware automated semantic web service composition is an NP-hard problem, where service composition workflows are unknown, and comprehensive quality, i.e., Quality of services (QoS) and Quality of semantic matchmaking (QoSM) are simultaneously optimized. The objective of this problem is to find a solution with optimized or near-optimized overall QoS and QoSM within polynomial time over a service request. In this paper, we proposed novel memetic EDA-based approaches to tackle this problem. The proposed method investigates the effectiveness of several neighborhood structures of composite services by proposing domain-dependent local search operators. Apart from that, a joint strategy of the local search procedure is proposed to integrate with a modified EDA to reduce the overall computation time of our memetic approach. To better demonstrate the effectiveness and scalability of our approach, we create a more challenging, augmented version of the service composition benchmark based on WSC-08 \cite{bansal2008wsc} and WSC-09 \cite{kona2009wsc}. Experimental results on this benchmark show that one of our proposed memetic EDA-based approach (i.e., MEEDA-LOP) significantly outperforms existing state-of-the-art algorithms.

</details>

<details>

<summary>2019-06-19 03:52:42 - Multimodal Abstractive Summarization for How2 Videos</summary>

- *Shruti Palaskar, Jindrich Libovický, Spandana Gella, Florian Metze*

- `1906.07901v1` - [abs](http://arxiv.org/abs/1906.07901v1) - [pdf](http://arxiv.org/pdf/1906.07901v1)

> In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to "compress" text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.

</details>

<details>

<summary>2019-06-19 06:10:58 - SAR Image Change Detection via Spatial Metric Learning with an Improved Mahalanobis Distance</summary>

- *Rongfang Wang, Jia-Wei Chen, Yule Wang, Licheng Jiao, Mi Wang*

- `1906.07930v1` - [abs](http://arxiv.org/abs/1906.07930v1) - [pdf](http://arxiv.org/pdf/1906.07930v1)

> The log-ratio (LR) operator has been widely employed to generate the difference image for synthetic aperture radar (SAR) image change detection. However, the difference image generated by this pixel-wise operator can be subject to SAR images speckle and unavoidable registration errors between bitemporal SAR images. In this letter, we proposed a spatial metric learning method to obtain a difference image more robust to the speckle by learning a metric from a set of constraint pairs. In the proposed method, spatial context is considered in constructing constraint pairs, each of which consists of patches in the same location of bitemporal SAR images. Then, a semi-definite positive metric matrix $\bf M$ can be obtained by the optimization with the max-margin criterion. Finally, we verify our proposed method on four challenging datasets of bitemporal SAR images. Experimental results demonstrate that the difference map obtained by our proposed method outperforms than other state-of-art methods.

</details>

<details>

<summary>2019-06-19 08:04:32 - Watset: Local-Global Graph Clustering with Applications in Sense and Frame Induction</summary>

- *Dmitry Ustalov, Alexander Panchenko, Chris Biemann, Simone Paolo Ponzetto*

- `1808.06696v4` - [abs](http://arxiv.org/abs/1808.06696v4) - [pdf](http://arxiv.org/pdf/1808.06696v4)

> We present a detailed theoretical and computational analysis of the Watset meta-algorithm for fuzzy graph clustering, which has been found to be widely applicable in a variety of domains. This algorithm creates an intermediate representation of the input graph that reflects the "ambiguity" of its nodes. Then, it uses hard clustering to discover clusters in this "disambiguated" intermediate graph. After outlining the approach and analyzing its computational complexity, we demonstrate that Watset shows competitive results in three applications: unsupervised synset induction from a synonymy graph, unsupervised semantic frame induction from dependency triples, and unsupervised semantic class induction from a distributional thesaurus. Our algorithm is generic and can be also applied to other networks of linguistic data.

</details>

<details>

<summary>2019-06-19 11:04:51 - Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction</summary>

- *Christoph Alt, Marc Hübner, Leonhard Hennig*

- `1906.08646v1` - [abs](http://arxiv.org/abs/1906.08646v1) - [pdf](http://arxiv.org/pdf/1906.08646v1)

> Distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. Current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. While achieving state-of-the-art results, we observed these models to be biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. To address this gap, we utilize a pre-trained language model, the OpenAI Generative Pre-trained Transformer (GPT) [Radford et al., 2018]. The GPT and similar models have been shown to capture semantic and syntactic features, and also a notable amount of "common-sense" knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. By extending the GPT to the distantly supervised setting, and fine-tuning it on the NYT10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. Manual and automated evaluation of our model shows that it achieves a state-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs especially well at higher recall levels.

</details>

<details>

<summary>2019-06-19 20:29:31 - Learning Compressed Sentence Representations for On-Device Text Processing</summary>

- *Dinghan Shen, Pengyu Cheng, Dhanasekar Sundararaman, Xinyuan Zhang, Qian Yang, Meng Tang, Asli Celikyilmaz, Lawrence Carin*

- `1906.08340v1` - [abs](http://arxiv.org/abs/1906.08340v1) - [pdf](http://arxiv.org/pdf/1906.08340v1)

> Vector representations of sentences, trained on massive text corpora, are widely used as generic sentence embeddings across a variety of NLP problems. The learned representations are generally assumed to be continuous and real-valued, giving rise to a large memory footprint and slow retrieval speed, which hinders their applicability to low-resource (memory and computation) platforms, such as mobile devices. In this paper, we propose four different strategies to transform continuous and generic sentence embeddings into a binarized form, while preserving their rich semantic information. The introduced methods are evaluated across a wide range of downstream tasks, where the binarized sentence embeddings are demonstrated to degrade performance by only about 2% relative to their continuous counterparts, while reducing the storage requirement by over 98%. Moreover, with the learned binary representations, the semantic relatedness of two sentences can be evaluated by simply calculating their Hamming distance, which is more computational efficient compared with the inner product operation between continuous embeddings. Detailed analysis and case study further validate the effectiveness of proposed methods.

</details>

<details>

<summary>2019-06-19 21:54:20 - A Latent Variable Model Approach to PMI-based Word Embeddings</summary>

- *Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski*

- `1502.03520v8` - [abs](http://arxiv.org/abs/1502.03520v8) - [pdf](http://arxiv.org/pdf/1502.03520v8)

> Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods.   This paper proposes a new generative model, a dynamic version of the log-linear topic model of~\citet{mnih2007three}. The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by~\citet{mikolov2013efficient} and many subsequent papers.   Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.

</details>

<details>

<summary>2019-06-19 21:56:25 - Considerations for the Interpretation of Bias Measures of Word Embeddings</summary>

- *Inom Mirzaev, Anthony Schulte, Michael Conover, Sam Shah*

- `1906.08379v1` - [abs](http://arxiv.org/abs/1906.08379v1) - [pdf](http://arxiv.org/pdf/1906.08379v1)

> Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.

</details>

<details>

<summary>2019-06-20 01:56:57 - CUTIE: Learning to Understand Documents with Convolutional Universal Text Information Extractor</summary>

- *Xiaohui Zhao, Endi Niu, Zhuo Wu, Xiaoguang Wang*

- `1903.12363v4` - [abs](http://arxiv.org/abs/1903.12363v4) - [pdf](http://arxiv.org/pdf/1903.12363v4)

> Extracting key information from documents, such as receipts or invoices, and preserving the interested texts to structured data is crucial in the document-intensive streamline processes of office automation in areas that includes but not limited to accounting, financial, and taxation areas. To avoid designing expert rules for each specific type of document, some published works attempt to tackle the problem by learning a model to explore the semantic context in text sequences based on the Named Entity Recognition (NER) method in the NLP field. In this paper, we propose to harness the effective information from both semantic meaning and spatial distribution of texts in documents. Specifically, our proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations. We further explore the effect of employing different structures of convolutional neural network and propose a fast and portable structure. We demonstrate the effectiveness of the proposed method on a dataset with up to $4,484$ labelled receipts, without any pre-training or post-processing, achieving state of the art performance that is much better than the NER based methods in terms of either speed and accuracy. Experimental results also demonstrate that the proposed CUTIE model being able to achieve good performance with a much smaller amount of training data.

</details>

<details>

<summary>2019-06-20 11:04:57 - On Physical Adversarial Patches for Object Detection</summary>

- *Mark Lee, Zico Kolter*

- `1906.11897v1` - [abs](http://arxiv.org/abs/1906.11897v1) - [pdf](http://arxiv.org/pdf/1906.11897v1)

> In this paper, we demonstrate a physical adversarial patch attack against object detectors, notably the YOLOv3 detector. Unlike previous work on physical object detection attacks, which required the patch to overlap with the objects being misclassified or avoiding detection, we show that a properly designed patch can suppress virtually all the detected objects in the image. That is, we can place the patch anywhere in the image, causing all existing objects in the image to be missed entirely by the detector, even those far away from the patch itself. This in turn opens up new lines of physical attacks against object detection systems, which require no modification of the objects in a scene. A demo of the system can be found at https://youtu.be/WXnQjbZ1e7Y.

</details>

<details>

<summary>2019-06-20 11:09:01 - Low-dimensional Embodied Semantics for Music and Language</summary>

- *Francisco Afonso Raposo, David Martins de Matos, Ricardo Ribeiro*

- `1906.11759v1` - [abs](http://arxiv.org/abs/1906.11759v1) - [pdf](http://arxiv.org/pdf/1906.11759v1)

> Embodied cognition states that semantics is encoded in the brain as firing patterns of neural circuits, which are learned according to the statistical structure of human multimodal experience. However, each human brain is idiosyncratically biased, according to its subjective experience history, making this biological semantic machinery noisy with respect to the overall semantics inherent to media artifacts, such as music and language excerpts. We propose to represent shared semantics using low-dimensional vector embeddings by jointly modeling several brains from human subjects. We show these unsupervised efficient representations outperform the original high-dimensional fMRI voxel spaces in proxy music genre and language topic classification tasks. We further show that joint modeling of several subjects increases the semantic richness of the learned latent vector spaces.

</details>

<details>

<summary>2019-06-20 11:09:12 - Neural Collective Entity Linking Based on Recurrent Random Walk Network Learning</summary>

- *Mengge Xue, Weiming Cai, Jinsong Su, Linfeng Song, Yubin Ge, Yubao Liu, Bin Wang*

- `1906.09320v1` - [abs](http://arxiv.org/abs/1906.09320v1) - [pdf](http://arxiv.org/pdf/1906.09320v1)

> Benefiting from the excellent ability of neural networks on learning semantic representations, existing studies for entity linking (EL) have resorted to neural networks to exploit both the local mention-to-entity compatibility and the global interdependence between different EL decisions for target entity disambiguation. However, most neural collective EL methods depend entirely upon neural networks to automatically model the semantic dependencies between different EL decisions, which lack of the guidance from external knowledge. In this paper, we propose a novel end-to-end neural network with recurrent random-walk layers for collective EL, which introduces external knowledge to model the semantic interdependence between different EL decisions. Specifically, we first establish a model based on local context features, and then stack random-walk layers to reinforce the evidence for related EL decisions into high-probability decisions, where the semantic interdependence between candidate entities is mainly induced from an external knowledge base. Finally, a semantic regularizer that preserves the collective EL decisions consistency is incorporated into the conventional objective function, so that the external knowledge base can be fully exploited in collective EL decisions. Experimental results and in-depth analysis on various datasets show that our model achieves better performance than other state-of-the-art models. Our code and data are released at \url{https://github.com/DeepLearnXMU/RRWEL}.

</details>

<details>

<summary>2019-06-20 12:05:13 - Hindi Question Generation Using Dependency Structures</summary>

- *Kaveri Anuranjana, Vijjini Anvesh Rao, Radhika Mamidi*

- `1906.08570v1` - [abs](http://arxiv.org/abs/1906.08570v1) - [pdf](http://arxiv.org/pdf/1906.08570v1)

> Hindi question answering systems suffer from a lack of data. To address the same, this paper presents an approach towards automatic question generation. We present a rule-based system for question generation in Hindi by formalizing question transformation methods based on karaka-dependency theory. We use a Hindi dependency parser to mark the karaka roles and use IndoWordNet a Hindi ontology to detect the semantic category of the karaka role heads to generate the interrogatives. We analyze how one sentence can have multiple generations from the same karaka role's rule. The generations are manually annotated by multiple annotators on a semantic and syntactic scale for evaluation. Further, we constrain our generation with the help of various semantic and syntactic filters so as to improve the generation quality. Using these methods, we are able to generate diverse questions, significantly more than number of sentences fed to the system.

</details>

<details>

<summary>2019-06-20 13:45:17 - Zero-shot Learning and Knowledge Transfer in Music Classification and Tagging</summary>

- *Jeong Choi, Jongpil Lee, Jiyoung Park, Juhan Nam*

- `1906.08615v1` - [abs](http://arxiv.org/abs/1906.08615v1) - [pdf](http://arxiv.org/pdf/1906.08615v1)

> Music classification and tagging is conducted through categorical supervised learning with a fixed set of labels. In principle, this cannot make predictions on unseen labels. Zero-shot learning is an approach to solve the problem by using side information about the semantic labels. We recently investigated this concept of zero-shot learning in music classification and tagging task by projecting both audio and label space on a single semantic space. In this work, we extend the work to verify the generalization ability of zero-shot learning model by conducting knowledge transfer to different music corpora.

</details>

<details>

<summary>2019-06-20 21:32:33 - Customer Segmentation of Wireless Trajectory Data</summary>

- *Matthew R Karlsen, Sotiris K. Moschoyiannis*

- `1906.08874v1` - [abs](http://arxiv.org/abs/1906.08874v1) - [pdf](http://arxiv.org/pdf/1906.08874v1)

> Wireless trajectory data consists of a number of (time, point) entries where each point is associated with a particular wireless device (WAP or BLE beacon) tied to a location identifier, such as a place name. A trajectory relates to a particular mobile device. Such data can be clustered `semantically' to identify similar trajectories, where similarity relates to non-geographic characteristics such as the type of location visited. Here we present a new approach to semantic trajectory clustering for such data. The approach is applicable to interpreting data that does not contain geographical coordinates, and thus contributes to the current literature on semantic trajectory clustering. The literature does not appear to provide such an approach, instead focusing on trajectory data where latitude and longitude data is available.   We apply the techniques developed above in the context of the Onward Journey Planner Application, with the motivation of providing on-line recommendations for onward journey options in a context-specific manner. The trajectories analysed indicate commute patterns on the London Underground. Points are only recorded for communication with WAP and BLE beacons within the rail network. This context presents additional challenge since the trajectories are `truncated', with no true origin and destination details.   In the above context we find that there are a range of travel patterns in the data, without the existence of distinct clusters. Suggestions are made concerning how to approach the problem of provision of on-line recommendations with such a data set. Thoughts concerning the related problem of prediction of journey route and destination are also provided.

</details>

<details>

<summary>2019-06-21 01:02:13 - Harnessing Evolution for Multi-Hunk Program Repair</summary>

- *Seemanta Saha, Ripon K. Saha, Mukul R. Prasad*

- `1906.08903v1` - [abs](http://arxiv.org/abs/1906.08903v1) - [pdf](http://arxiv.org/pdf/1906.08903v1)

> Despite significant advances in automatic program repair (APR)techniques over the past decade, practical deployment remains an elusive goal. One of the important challenges in this regard is the general inability of current APR techniques to produce patches that require edits in multiple locations, i.e., multi-hunk patches. In this work, we present a novel APR technique that generalizes single-hunk repair techniques to include an important class of multi-hunk bugs, namely bugs that may require applying a substantially similar patch at a number of locations. We term such sets of repair locations as evolutionary siblings - similar looking code, instantiated in similar contexts, that are expected to undergo similar changes. At the heart of our proposed method is an analysis to accurately identify a set of evolutionary siblings, for a given bug. This analysis leverages three distinct sources of information, namely the test-suite spectrum, a novel code similarity analysis, and the revision history of the project. The discovered siblings are then simultaneously repaired in a similar fashion. We instantiate this technique in a tool called Hercules and demonstrate that it is able to correctly fix 49 bugs in the Defects4J dataset, the highest of any individual APR technique to date. This includes 15 multi-hunk bugs and overall 13 bugs which have not been fixed by any other technique so far.

</details>

<details>

<summary>2019-06-21 03:25:30 - Exploiting Entity BIO Tag Embeddings and Multi-task Learning for Relation Extraction with Imbalanced Data</summary>

- *Wei Ye, Bo Li, Rui Xie, Zhonghao Sheng, Long Chen, Shikun Zhang*

- `1906.08931v1` - [abs](http://arxiv.org/abs/1906.08931v1) - [pdf](http://arxiv.org/pdf/1906.08931v1)

> In practical scenario, relation extraction needs to first identify entity pairs that have relation and then assign a correct relation class. However, the number of non-relation entity pairs in context (negative instances) usually far exceeds the others (positive instances), which negatively affects a model's performance. To mitigate this problem, we propose a multi-task architecture which jointly trains a model to perform relation identification with cross-entropy loss and relation classification with ranking loss. Meanwhile, we observe that a sentence may have multiple entities and relation mentions, and the patterns in which the entities appear in a sentence may contain useful semantic information that can be utilized to distinguish between positive and negative instances. Thus we further incorporate the embeddings of character-wise/word-wise BIO tag from the named entity recognition task into character/word embeddings to enrich the input representation. Experiment results show that our proposed approach can significantly improve the performance of a baseline model with more than 10% absolute increase in F1-score, and outperform the state-of-the-art models on ACE 2005 Chinese and English corpus. Moreover, BIO tag embeddings are particularly effective and can be used to improve other models as well.

</details>

<details>

<summary>2019-06-21 04:35:14 - Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions</summary>

- *Joey Hong, Benjamin Sapp, James Philbin*

- `1906.08945v1` - [abs](http://arxiv.org/abs/1906.08945v1) - [pdf](http://arxiv.org/pdf/1906.08945v1)

> We focus on the problem of predicting future states of entities in complex, real-world driving scenarios. Previous research has used low-level signals to predict short time horizons, and has not addressed how to leverage key assets relied upon heavily by industry self-driving systems: (1) large 3D perception efforts which provide highly accurate 3D states of agents with rich attributes, and (2) detailed and accurate semantic maps of the environment (lanes, traffic lights, crosswalks, etc). We present a unified representation which encodes such high-level semantic information in a spatial grid, allowing the use of deep convolutional models to fuse complex scene context. This enables learning entity-entity and entity-environment interactions with simple, feed-forward computations in each timestep within an overall temporal model of an agent's behavior. We propose different ways of modelling the future as a distribution over future states using standard supervised learning. We introduce a novel dataset providing industry-grade rich perception and semantic inputs, and empirically show we can effectively learn fundamentals of driving behavior.

</details>

<details>

<summary>2019-06-21 13:16:00 - A Novel Fuzzy Search Approach over Encrypted Data with Improved Accuracy and Efficiency</summary>

- *Jinkun Cao, Jinhao Zhu, Liwei Lin, Zhengui Xue, Ruhui Ma, Haibing Guan*

- `1904.12111v2` - [abs](http://arxiv.org/abs/1904.12111v2) - [pdf](http://arxiv.org/pdf/1904.12111v2)

> As cloud computing becomes prevalent in recent years, more and more enterprises and individuals outsource their data to cloud servers. To avoid privacy leaks, outsourced data usually is encrypted before being sent to cloud servers, which disables traditional search schemes for plain text. To meet both end of security and searchability, search-supported encryption is proposed. However, many previous schemes suffer severe vulnerability when typos and semantic diversity exist in query requests. To overcome such flaw, higher error-tolerance is always expected for search-supported encryption design, sometimes defined as 'fuzzy search'. In this paper, we propose a new scheme of multi-keyword fuzzy search over encrypted and outsourced data. Our approach introduces a new mechanism to map a natural language expression into a word-vector space. Compared with previous approaches, our design shows higher robustness when multiple kinds of typos are involved. Besides, our approach is enhanced with novel data structures to improve search efficiency. These two innovations can work well for both accuracy and efficiency. Moreover, these designs will not hurt the fundamental security. Experiments on a real-world dataset demonstrate the effectiveness of our proposed approach, which outperforms currently popular approaches focusing on similar tasks.

</details>

<details>

<summary>2019-06-21 15:35:03 - Explainable Fact Checking with Probabilistic Answer Set Programming</summary>

- *Naser Ahmadi, Joohyung Lee, Paolo Papotti, Mohammed Saeed*

- `1906.09198v1` - [abs](http://arxiv.org/abs/1906.09198v1) - [pdf](http://arxiv.org/pdf/1906.09198v1)

> One challenge in fact checking is the ability to improve the transparency of the decision. We present a fact checking method that uses reference information in knowledge graphs (KGs) to assess claims and explain its decisions. KGs contain a formal representation of knowledge with semantic descriptions of entities and their relationships. We exploit such rich semantics to produce interpretable explanations for the fact checking output. As information in a KG is inevitably incomplete, we rely on logical rule discovery and on Web text mining to gather the evidence to assess a given claim. Uncertain rules and facts are turned into logical programs and the checking task is modeled as an inference problem in a probabilistic extension of answer set programs. Experiments show that the probabilistic inference enables the efficient labeling of claims with interpretable explanations, and the quality of the results is higher than state of the art baselines.

</details>

<details>

<summary>2019-06-21 18:23:17 - SurfCon: Synonym Discovery on Privacy-Aware Clinical Data</summary>

- *Zhen Wang, Xiang Yue, Soheil Moosavinasab, Yungui Huang, Simon Lin, Huan Sun*

- `1906.09285v1` - [abs](http://arxiv.org/abs/1906.09285v1) - [pdf](http://arxiv.org/pdf/1906.09285v1)

> Unstructured clinical texts contain rich health-related information. To better utilize the knowledge buried in clinical texts, discovering synonyms for a medical query term has become an important task. Recent automatic synonym discovery methods leveraging raw text information have been developed. However, to preserve patient privacy and security, it is usually quite difficult to get access to large-scale raw clinical texts. In this paper, we study a new setting named synonym discovery on privacy-aware clinical data (i.e., medical terms extracted from the clinical texts and their aggregated co-occurrence counts, without raw clinical texts). To solve the problem, we propose a new framework SurfCon that leverages two important types of information in the privacy-aware clinical data, i.e., the surface form information, and the global context information for synonym discovery. In particular, the surface form module enables us to detect synonyms that look similar while the global context module plays a complementary role to discover synonyms that are semantically similar but in different surface forms, and both allow us to deal with the OOV query issue (i.e., when the query is not found in the given data). We conduct extensive experiments and case studies on publicly available privacy-aware clinical data, and show that SurfCon can outperform strong baseline methods by large margins under various settings.

</details>

<details>

<summary>2019-06-21 19:34:19 - Neural Machine Translating from Natural Language to SPARQL</summary>

- *Xiaoyu Yin, Dagmar Gromann, Sebastian Rudolph*

- `1906.09302v1` - [abs](http://arxiv.org/abs/1906.09302v1) - [pdf](http://arxiv.org/pdf/1906.09302v1)

> SPARQL is a highly powerful query language for an ever-growing number of Linked Data resources and Knowledge Graphs. Using it requires a certain familiarity with the entities in the domain to be queried as well as expertise in the language's syntax and semantics, none of which average human web users can be assumed to possess. To overcome this limitation, automatically translating natural language questions to SPARQL queries has been a vibrant field of research. However, to this date, the vast success of deep learning methods has not yet been fully propagated to this research problem. This paper contributes to filling this gap by evaluating the utilization of eight different Neural Machine Translation (NMT) models for the task of translating from natural language to the structured query language SPARQL. While highlighting the importance of high-quantity and high-quality datasets, the results show a dominance of a CNN-based architecture with a BLEU score of up to 98 and accuracy of up to 94%.

</details>

<details>

<summary>2019-06-22 00:14:49 - Multi-Perspective Relevance Matching with Hierarchical ConvNets for Social Media Search</summary>

- *Jinfeng Rao, Wei Yang, Yuhao Zhang, Ferhan Ture, Jimmy Lin*

- `1805.08159v2` - [abs](http://arxiv.org/abs/1805.08159v2) - [pdf](http://arxiv.org/pdf/1805.08159v2)

> Despite substantial interest in applications of neural networks to information retrieval, neural ranking models have only been applied to standard ad hoc retrieval tasks over web pages and newswire documents. This paper proposes MP-HCNN (Multi-Perspective Hierarchical Convolutional Neural Network) a novel neural ranking model specifically designed for ranking short social media posts. We identify document length, informal language, and heterogeneous relevance signals as features that distinguish documents in our domain, and present a model specifically designed with these characteristics in mind. Our model uses hierarchical convolutional layers to learn latent semantic soft-match relevance signals at the character, word, and phrase levels. A pooling-based similarity measurement layer integrates evidence from multiple types of matches between the query, the social media post, as well as URLs contained in the post. Extensive experiments using Twitter data from the TREC Microblog Tracks 2011--2014 show that our model significantly outperforms prior feature-based as well and existing neural ranking models. To our best knowledge, this paper presents the first substantial work tackling search over social media posts using neural ranking models.

</details>

<details>

<summary>2019-06-22 13:28:32 - Learning with fuzzy hypergraphs: a topical approach to query-oriented text summarization</summary>

- *Hadrien Van Lierde, Tommy W. S. Chow*

- `1906.09445v1` - [abs](http://arxiv.org/abs/1906.09445v1) - [pdf](http://arxiv.org/pdf/1906.09445v1)

> Existing graph-based methods for extractive document summarization represent sentences of a corpus as the nodes of a graph or a hypergraph in which edges depict relationships of lexical similarity between sentences. Such approaches fail to capture semantic similarities between sentences when they express a similar information but have few words in common and are thus lexically dissimilar. To overcome this issue, we propose to extract semantic similarities based on topical representations of sentences. Inspired by the Hierarchical Dirichlet Process, we propose a probabilistic topic model in order to infer topic distributions of sentences. As each topic defines a semantic connection among a group of sentences with a certain degree of membership for each sentence, we propose a fuzzy hypergraph model in which nodes are sentences and fuzzy hyperedges are topics. To produce an informative summary, we extract a set of sentences from the corpus by simultaneously maximizing their relevance to a user-defined query, their centrality in the fuzzy hypergraph and their coverage of topics present in the corpus. We formulate a polynomial time algorithm building on the theory of submodular functions to solve the associated optimization problem. A thorough comparative analysis with other graph-based summarization systems is included in the paper. Our obtained results show the superiority of our method in terms of content coverage of the summaries.

</details>

<details>

<summary>2019-06-22 13:53:50 - Assisted Sound Sample Generation with Musical Conditioning in Adversarial Auto-Encoders</summary>

- *Adrien Bitton, Philippe Esling, Antoine Caillon, Martin Fouilleul*

- `1904.06215v2` - [abs](http://arxiv.org/abs/1904.06215v2) - [pdf](http://arxiv.org/pdf/1904.06215v2)

> Generative models have thrived in computer vision, enabling unprecedented image processes. Yet the results in audio remain less advanced. Our project targets real-time sound synthesis from a reduced set of high-level parameters, including semantic controls that can be adapted to different sound libraries and specific tags. These generative variables should allow expressive modulations of target musical qualities and continuously mix into new styles. To this extent we train AEs on an orchestral database of individual note samples, along with their intrinsic attributes: note class, timbre domain and extended playing techniques. We condition the decoder for control over the rendered note attributes and use latent adversarial training for learning expressive style parameters that can ultimately be mixed. We evaluate both generative performances and latent representation. Our ablation study demonstrates the effectiveness of the musical conditioning mechanisms. The proposed model generates notes as magnitude spectrograms from any probabilistic latent code samples, with expressive control of orchestral timbres and playing styles. Its training data subsets can directly be visualized in the 3D latent representation. Waveform rendering can be done offline with GLA. In order to allow real-time interactions, we fine-tune the decoder with a pretrained MCNN and embed the full waveform generation pipeline in a plugin. Moreover the encoder could be used to process new input samples, after manipulating their latent attribute representation, the decoder can generate sample variations as an audio effect would. Our solution remains rather fast to train, it can directly be applied to other sound domains, including an user's libraries with custom sound tags that could be mapped to specific generative controls. As a result, it fosters creativity and intuitive audio style experimentations.

</details>

<details>

<summary>2019-06-22 14:02:14 - Semantically Driven Auto-completion</summary>

- *Konstantine Arkoudas, Mohamed Yahya*

- `1906.09450v1` - [abs](http://arxiv.org/abs/1906.09450v1) - [pdf](http://arxiv.org/pdf/1906.09450v1)

> The Bloomberg Terminal has been a leading source of financial data and analytics for over 30 years. Through its thousands of functions, the Terminal allows its users to query and run analytics over a large array of data sources, including structured, semi-structured, and unstructured data; as well as plot charts, set up event-driven alerts and triggers, create interactive maps, exchange information via instant and email-style messages, and so on. To improve user experience, we have been building question answering systems that can understand a wide range of natural language constructions for various domains that are of fundamental interest to our users. Such natural language interfaces, while exceedingly helpful to users, introduce a number of usability challenges of their own. We tackle some of these challenges through auto-completion for query formulation. A distinguishing mark of our auto-complete systems is that they are based on and guided by corresponding semantic parsing systems. We describe the auto-complete problem as it arises in this setting, the novel algorithms that we use to solve it, and report on the quality of the results and the efficiency of our approach.

</details>

<details>

<summary>2019-06-23 02:56:02 - Cross-lingual Data Transformation and Combination for Text Classification</summary>

- *Jun Jiang, Shumao Pang, Xia Zhao, Liwei Wang, Andrew Wen, Hongfang Liu, Qianjin Feng*

- `1906.09543v1` - [abs](http://arxiv.org/abs/1906.09543v1) - [pdf](http://arxiv.org/pdf/1906.09543v1)

> Text classification is a fundamental task for text data mining. In order to train a generalizable model, a large volume of text must be collected. To address data insufficiency, cross-lingual data may occasionally be necessary. Cross-lingual data sources may however suffer from data incompatibility, as text written in different languages can hold distinct word sequences and semantic patterns. Machine translation and word embedding alignment provide an effective way to transform and combine data for cross-lingual data training. To the best of our knowledge, there has been little work done on evaluating how the methodology used to conduct semantic space transformation and data combination affects the performance of classification models trained from cross-lingual resources. In this paper, we systematically evaluated the performance of two commonly used CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network) text classifiers with differing data transformation and combination strategies. Monolingual models were trained from English and French alongside their translated and aligned embeddings. Our results suggested that semantic space transformation may conditionally promote the performance of monolingual models. Bilingual models were trained from a combination of both English and French. Our results indicate that a cross-lingual classification model can significantly benefit from cross-lingual data by learning from translated or aligned embedding spaces.

</details>

<details>

<summary>2019-06-23 19:59:42 - On Quantum Chosen-Ciphertext Attacks and Learning with Errors</summary>

- *Gorjan Alagic, Stacey Jeffery, Maris Ozols, Alexander Poremba*

- `1808.09655v2` - [abs](http://arxiv.org/abs/1808.09655v2) - [pdf](http://arxiv.org/pdf/1808.09655v2)

> Large-scale quantum computing is a significant threat to classical public-key cryptography. In strong "quantum access" security models, numerous symmetric-key cryptosystems are also vulnerable. We consider classical encryption in a model which grants the adversary quantum oracle access to encryption and decryption, but where the latter is restricted to non-adaptive (i.e., pre-challenge) queries only. We define this model formally using appropriate notions of ciphertext indistinguishability and semantic security (which are equivalent by standard arguments) and call it QCCA1 in analogy to the classical CCA1 security model. Using a bound on quantum random-access codes, we show that the standard PRF- and PRP-based encryption schemes are QCCA1-secure when instantiated with quantum-secure primitives.   We then revisit standard IND-CPA-secure Learning with Errors (LWE) encryption and show that leaking just one quantum decryption query (and no other queries or leakage of any kind) allows the adversary to recover the full secret key with constant success probability. In the classical setting, by contrast, recovering the key uses a linear number of decryption queries, and this is optimal. The algorithm at the core of our attack is a (large-modulus version of) the well-known Bernstein-Vazirani algorithm. We emphasize that our results should *not* be interpreted as a weakness of these cryptosystems in their stated security setting (i.e., post-quantum chosen-plaintext secrecy). Rather, our results mean that, if these cryptosystems are exposed to chosen-ciphertext attacks (e.g., as a result of deployment in an inappropriate real-world setting) then quantum attacks are even more devastating than classical ones.

</details>

<details>

<summary>2019-06-23 21:12:24 - Augmenting Transfer Learning with Semantic Reasoning</summary>

- *Freddy Lecue, Jiaoyan Chen, Jeff Z. Pan, Huajun Chen*

- `1905.13672v2` - [abs](http://arxiv.org/abs/1905.13672v2) - [pdf](http://arxiv.org/pdf/1905.13672v2)

> Transfer learning aims at building robust prediction models by transferring knowledge gained from one problem to another. In the semantic Web, learning tasks are enhanced with semantic representations. We exploit their semantics to augment transfer learning by dealing with when to transfer with semantic measurements and what to transfer with semantic embeddings. We further present a general framework that integrates the above measurements and embeddings with existing transfer learning algorithms for higher performance. It has demonstrated to be robust in two real-world applications: bus delay forecasting and air quality forecasting.

</details>

<details>

<summary>2019-06-24 00:18:47 - AMR Parsing as Sequence-to-Graph Transduction</summary>

- *Sheng Zhang, Xutai Ma, Kevin Duh, Benjamin Van Durme*

- `1905.08704v2` - [abs](http://arxiv.org/abs/1905.08704v2) - [pdf](http://arxiv.org/pdf/1905.08704v2)

> We propose an attention-based model that treats AMR parsing as sequence-to-graph transduction. Unlike most AMR parsers that rely on pre-trained aligners, external semantic resources, or data augmentation, our proposed parser is aligner-free, and it can be effectively trained with limited amounts of labeled AMR data. Our experimental results outperform all previously reported SMATCH scores, on both AMR 2.0 (76.3% F1 on LDC2017T10) and AMR 1.0 (70.2% F1 on LDC2014T12).

</details>

<details>

<summary>2019-06-24 04:18:38 - EDIMA: Early Detection of IoT Malware Network Activity Using Machine Learning Techniques</summary>

- *Ayush Kumar, Teng Joon Lim*

- `1906.09715v1` - [abs](http://arxiv.org/abs/1906.09715v1) - [pdf](http://arxiv.org/pdf/1906.09715v1)

> The widespread adoption of Internet of Things has led to many security issues. Post the Mirai-based DDoS attack in 2016 which compromised IoT devices, a host of new malware using Mirai's leaked source code and targeting IoT devices have cropped up, e.g. Satori, Reaper, Amnesia, Masuta etc. These malware exploit software vulnerabilities to infect IoT devices instead of open TELNET ports (like Mirai) making them more difficult to block using existing solutions such as firewalls. In this research, we present EDIMA, a distributed modular solution which can be used towards the detection of IoT malware network activity in large-scale networks (e.g. ISP, enterprise networks) during the scanning/infecting phase rather than during an attack. EDIMA employs machine learning algorithms for edge devices' traffic classification, a packet traffic feature vector database, a policy module and an optional packet sub-sampling module. We evaluate the classification performance of EDIMA through testbed experiments and present the results obtained.

</details>

<details>

<summary>2019-06-24 08:18:12 - Embedding Projection for Targeted Cross-Lingual Sentiment: Model Comparisons and a Real-World Study</summary>

- *Jeremy Barnes, Roman Klinger*

- `1906.10519v1` - [abs](http://arxiv.org/abs/1906.10519v1) - [pdf](http://arxiv.org/pdf/1906.10519v1)

> Sentiment analysis benefits from large, hand-annotated resources in order to train and test machine learning models, which are often data hungry. While some languages, e.g., English, have a vast array of these resources, most under-resourced languages do not, especially for fine-grained sentiment tasks, such as aspect-level or targeted sentiment analysis. To improve this situation, we propose a cross-lingual approach to sentiment analysis that is applicable to under-resourced languages and takes into account target-level information. This model incorporates sentiment information into bilingual distributional representations, by jointly optimizing them for semantics and sentiment, showing state-of-the-art performance at sentence-level when combined with machine translation. The adaptation to targeted sentiment analysis on multiple domains shows that our model outperforms other projection-based bilingual embedding methods on binary targeted sentiment tasks. Our analysis on ten languages demonstrates that the amount of unlabeled monolingual data has surprisingly little effect on the sentiment results. As expected, the choice of annotated source language for projection to a target leads to better results for source-target language pairs which are similar. Therefore, our results suggest that more efforts should be spent on the creation of resources for less similar languages to those which are resource-rich already. Finally, a domain mismatch leads to a decreased performance. This suggests resources in any language should ideally cover varieties of domains.

</details>

<details>

<summary>2019-06-24 08:49:17 - Safe Trajectory Generation for Complex Urban Environments Using Spatio-temporal Semantic Corridor</summary>

- *Wenchao Ding, Lu Zhang, Jing Chen, Shaojie Shen*

- `1906.09788v1` - [abs](http://arxiv.org/abs/1906.09788v1) - [pdf](http://arxiv.org/pdf/1906.09788v1)

> Planning safe trajectories for autonomous vehicles in complex urban environments is challenging since there are numerous semantic elements (such as dynamic agents, traffic lights and speed limits) to consider. These semantic elements may have different mathematical descriptions such as obstacle, constraint and cost. It is non-trivial to tune the effects from different combinations of semantic elements for a stable and generalizable behavior. In this paper, we propose a novel unified spatio-temporal semantic corridor (SSC) structure, which provides a level of abstraction for different types of semantic elements. The SSC consists of a series of mutually connected collision-free cubes with dynamical constraints posed by the semantic elements in the spatio-temporal domain. The trajectory generation problem then boils down to a general quadratic programming (QP) formulation. Thanks to the unified SSC representation, our framework can generalize to any combination of semantic elements. Moreover, our formulation provides a theoretical guarantee that the entire trajectory is safe and constraint-satisfied, by using the convex hull and hodograph properties of piecewise Bezier curve parameterization. We also release the code of our method to accommodate benchmarking.

</details>

<details>

<summary>2019-06-24 09:43:38 - Hybrid-Learning approach toward situation recognition and handling</summary>

- *Hossein Rajaby Faghihi, Mohammad Amin Fazli, Jafar Habibi*

- `1906.09816v1` - [abs](http://arxiv.org/abs/1906.09816v1) - [pdf](http://arxiv.org/pdf/1906.09816v1)

> The success of smart environments largely depends on their smartness of understanding the environments' ongoing situations. Accordingly, this task is an essence to smart environment central processors. Obtaining knowledge from the environment is often through sensors, and the response to a particular circumstance is offered by actuators. This can be improved by getting user feedback, and capturing environmental changes. Machine learning techniques and semantic reasoning tools are widely used in this area to accomplish the goal of interpretation. In this paper, we have proposed a hybrid approach utilizing both machine learning and semantic reasoning tools to derive a better understanding from sensors. This method uses situation templates jointly with a decision tree to adapt the system knowledge to the environment. To test this approach we have used a simulation process which has resulted in a better precision for detecting situations in an ongoing environment involving living agents while capturing its dynamic nature.

</details>

<details>

<summary>2019-06-24 12:56:36 - Deep Exemplar-based Video Colorization</summary>

- *Bo Zhang, Mingming He, Jing Liao, Pedro V. Sander, Lu Yuan, Amine Bermak, Dong Chen*

- `1906.09909v1` - [abs](http://arxiv.org/abs/1906.09909v1) - [pdf](http://arxiv.org/pdf/1906.09909v1)

> This paper presents the first end-to-end network for exemplar-based video colorization. The main challenge is to achieve temporal consistency while remaining faithful to the reference style. To address this issue, we introduce a recurrent framework that unifies the semantic correspondence and color propagation steps. Both steps allow a provided reference image to guide the colorization of every frame, thus reducing accumulated propagation errors. Video frames are colorized in sequence based on the colorization history, and its coherency is further enforced by the temporal consistency loss. All of these components, learned end-to-end, help produce realistic videos with good temporal stability. Experiments show our result is superior to the state-of-the-art methods both quantitatively and qualitatively.

</details>

<details>

<summary>2019-06-24 13:44:34 - Integrating Knowledge and Reasoning in Image Understanding</summary>

- *Somak Aditya, Yezhou Yang, Chitta Baral*

- `1906.09954v1` - [abs](http://arxiv.org/abs/1906.09954v1) - [pdf](http://arxiv.org/pdf/1906.09954v1)

> Deep learning based data-driven approaches have been successfully applied in various image understanding applications ranging from object recognition, semantic segmentation to visual question answering. However, the lack of knowledge integration as well as higher-level reasoning capabilities with the methods still pose a hindrance. In this work, we present a brief survey of a few representative reasoning mechanisms, knowledge integration methods and their corresponding image understanding applications developed by various groups of researchers, approaching the problem from a variety of angles. Furthermore, we discuss upon key efforts on integrating external knowledge with neural networks. Taking cues from these efforts, we conclude by discussing potential pathways to improve reasoning capabilities.

</details>

<details>

<summary>2019-06-24 14:59:12 - Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation</summary>

- *Daniel Loureiro, Alipio Jorge*

- `1906.10007v1` - [abs](http://arxiv.org/abs/1906.10007v1) - [pdf](http://arxiv.org/pdf/1906.10007v1)

> Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling (NLM) that addresses the issue of meaning conflation hampering traditional word embeddings. In this work, we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level embeddings with full-coverage of WordNet, and without recourse to explicit knowledge of sense distributions or task-specific modelling. As a result, a simple Nearest Neighbors (k-NN) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. We also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. Finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs.

</details>

<details>

<summary>2019-06-24 17:15:10 - Incorporating Chinese Radicals Into Neural Machine Translation: Deeper Than Character Level</summary>

- *Lifeng Han, Shaohui Kuang*

- `1805.01565v3` - [abs](http://arxiv.org/abs/1805.01565v3) - [pdf](http://arxiv.org/pdf/1805.01565v3)

> In neural machine translation (NMT), researchers face the challenge of un-seen (or out-of-vocabulary OOV) words translation. To solve this, some researchers propose the splitting of western languages such as English and German into sub-words or compounds. In this paper, we try to address this OOV issue and improve the NMT adequacy with a harder language Chinese whose characters are even more sophisticated in composition. We integrate the Chinese radicals into the NMT model with different settings to address the unseen words challenge in Chinese to English translation. On the other hand, this also can be considered as semantic part of the MT system since the Chinese radicals usually carry the essential meaning of the words they are constructed in. Meaningful radicals and new characters can be integrated into the NMT systems with our models. We use an attention-based NMT system as a strong baseline system. The experiments on standard Chinese-to-English NIST translation shared task data 2006 and 2008 show that our designed models outperform the baseline model in a wide range of state-of-the-art evaluation metrics including LEPOR, BEER, and CharacTER, in addition to BLEU and NIST scores, especially on the adequacy-level translation. We also have some interesting findings from the results of our various experiment settings about the performance of words and characters in Chinese NMT, which is different with other languages. For instance, the fully character level NMT may perform well or the state of the art in some other languages as researchers demonstrated recently, however, in the Chinese NMT model, word boundary knowledge is important for the model learning.

</details>

<details>

<summary>2019-06-24 17:30:09 - Modeling financial analysts' decision making via the pragmatics and semantics of earnings calls</summary>

- *Katherine A. Keith, Amanda Stent*

- `1906.02868v2` - [abs](http://arxiv.org/abs/1906.02868v2) - [pdf](http://arxiv.org/pdf/1906.02868v2)

> Every fiscal quarter, companies hold earnings calls in which company executives respond to questions from analysts. After these calls, analysts often change their price target recommendations, which are used in equity research reports to help investors make decisions. In this paper, we examine analysts' decision making behavior as it pertains to the language content of earnings calls. We identify a set of 20 pragmatic features of analysts' questions which we correlate with analysts' pre-call investor recommendations. We also analyze the degree to which semantic and pragmatic features from an earnings call complement market data in predicting analysts' post-call changes in price targets. Our results show that earnings calls are moderately predictive of analysts' decisions even though these decisions are influenced by a number of other factors including private communication with company executives and market conditions. A breakdown of model errors indicates disparate performance on calls from different market sectors.

</details>

<details>

<summary>2019-06-24 17:48:27 - Implicitly Learning to Reason in First-Order Logic</summary>

- *Vaishak Belle, Brendan Juba*

- `1906.10106v1` - [abs](http://arxiv.org/abs/1906.10106v1) - [pdf](http://arxiv.org/pdf/1906.10106v1)

> We consider the problem of answering queries about formulas of first-order logic based on background knowledge partially represented explicitly as other formulas, and partially represented as examples independently drawn from a fixed probability distribution. PAC semantics, introduced by Valiant, is one rigorous, general proposal for learning to reason in formal languages: although weaker than classical entailment, it allows for a powerful model theoretic framework for answering queries while requiring minimal assumptions about the form of the distribution in question. To date, however, the most significant limitation of that approach, and more generally most machine learning approaches with robustness guarantees, is that the logical language is ultimately essentially propositional, with finitely many atoms. Indeed, the theoretical findings on the learning of relational theories in such generality have been resoundingly negative. This is despite the fact that first-order logic is widely argued to be most appropriate for representing human knowledge. In this work, we present a new theoretical approach to robustly learning to reason in first-order logic, and consider universally quantified clauses over a countably infinite domain. Our results exploit symmetries exhibited by constants in the language, and generalize the notion of implicit learnability to show how queries can be computed against (implicitly) learned first-order background knowledge.

</details>

<details>

<summary>2019-06-24 17:59:19 - Query-driven PAC-Learning for Reasoning</summary>

- *Brendan Juba*

- `1906.10118v1` - [abs](http://arxiv.org/abs/1906.10118v1) - [pdf](http://arxiv.org/pdf/1906.10118v1)

> We consider the problem of learning rules from a data set that support a proof of a given query, under Valiant's PAC-Semantics. We show how any backward proof search algorithm that is sufficiently oblivious to the contents of its knowledge base can be modified to learn such rules while it searches for a proof using those rules. We note that this gives such algorithms for standard logics such as chaining and resolution.

</details>

<details>

<summary>2019-06-24 19:47:23 - Multimodal and Multi-view Models for Emotion Recognition</summary>

- *Gustavo Aguilar, Viktor Rozgić, Weiran Wang, Chao Wang*

- `1906.10198v1` - [abs](http://arxiv.org/abs/1906.10198v1) - [pdf](http://arxiv.org/pdf/1906.10198v1)

> Studies on emotion recognition (ER) show that combining lexical and acoustic information results in more robust and accurate models. The majority of the studies focus on settings where both modalities are available in training and evaluation. However, in practice, this is not always the case; getting ASR output may represent a bottleneck in a deployment pipeline due to computational complexity or privacy-related constraints. To address this challenge, we study the problem of efficiently combining acoustic and lexical modalities during training while still providing a deployable acoustic model that does not require lexical inputs. We first experiment with multimodal models and two attention mechanisms to assess the extent of the benefits that lexical information can provide. Then, we frame the task as a multi-view learning problem to induce semantic information from a multimodal model into our acoustic-only network using a contrastive loss function. Our multimodal model outperforms the previous state of the art on the USC-IEMOCAP dataset reported on lexical and acoustic information. Additionally, our multi-view-trained acoustic network significantly surpasses models that have been exclusively trained with acoustic features.

</details>

<details>

<summary>2019-06-24 19:58:49 - Multi-level analysis of compiler induced variability and performance tradeoffs</summary>

- *Michael Bentley, Ian Briggs, Ganesh Gopalakrishnan, Dong H. Ahn, Ignacio Laguna, Gregory L. Lee, Holger E. Jones*

- `1811.05618v2` - [abs](http://arxiv.org/abs/1811.05618v2) - [pdf](http://arxiv.org/pdf/1811.05618v2)

> Successful HPC software applications are long-lived. When ported across machines and their compilers, these applications often produce different numerical results, many of which are unacceptable. Such variability is also a concern while optimizing the code more aggressively to gain performance. Efficient tools that help locate the program units (files and functions) within which most of the variability occurs are badly needed, both to plan for code ports and to root-cause errors due to variability when they happen in the field. In this work, we offer an enhanced version of the open-source testing framework FLiT to serve these roles. Key new features of FLiT include a suite of bisection algorithms that help locate the root causes of variability. Another added feature allows an analysis of the tradeoffs between performance and the degree of variability. Our new contributions also include a collection of case studies. Results on the MFEM finite-element library include variability/performance tradeoffs, and the identification of a (hitherto unknown) abnormal level of result-variability even under mild compiler optimizations. Results from studying the Laghos proxy application include identifying a significantly divergent floating-point result-variability and successful root-causing down to the problematic function over as little as 14 program executions. Finally, in an evaluation of 4,376 controlled injections of floating-point perturbations on the LULESH proxy application, we showed that the FLiT framework has 100 precision and recall in discovering the file and function locations of the injections all within an average of only 15 program executions.

</details>

<details>

<summary>2019-06-25 06:16:05 - Learning a sparse database for patch-based medical image segmentation</summary>

- *Moti Freiman, Hannes Nickisch, Holger Schmitt, Pal Maurovich-Horvat, Patrick Donnelly, Mani Vembar, Liran Goshen*

- `1906.10338v1` - [abs](http://arxiv.org/abs/1906.10338v1) - [pdf](http://arxiv.org/pdf/1906.10338v1)

> We introduce a functional for the learning of an optimal database for patch-based image segmentation with application to coronary lumen segmentation from coronary computed tomography angiography (CCTA) data. The proposed functional consists of fidelity, sparseness and robustness to small-variations terms and their associated weights. Existing work address database optimization by prototype selection aiming to optimize the database by either adding or removing prototypes according to a set of predefined rules. In contrast, we formulate the database optimization task as an energy minimization problem that can be solved using standard numerical tools. We apply the proposed database optimization functional to the task of optimizing a database for patch-base coronary lumen segmentation. Our experiments using the publicly available MICCAI 2012 coronary lumen segmentation challenge data show that optimizing the database using the proposed approach reduced database size by 96% while maintaining the same level of lumen segmentation accuracy. Moreover, we show that the optimized database yields an improved specificity of CCTA based fractional flow reserve (0.73 vs 0.7 for all lesions and 0.68 vs 0.65 for obstructive lesions) using a training set of 132 (76 obstructive) coronary lesions with invasively measured FFR as the reference.

</details>

<details>

<summary>2019-06-25 17:25:35 - Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements</summary>

- *Venkata Subrahmanyan Govindarajan, Benjamin Van Durme, Aaron Steven White*

- `1901.11429v2` - [abs](http://arxiv.org/abs/1901.11429v2) - [pdf](http://arxiv.org/pdf/1901.11429v2)

> We present a novel semantic framework for modeling linguistic expressions of generalization---generic, habitual, and episodic statements---as combinations of simple, real-valued referential properties of predicates and their arguments. We use this framework to construct a dataset covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to probe the efficacy of type-level and token-level information---including hand-engineered features and static (GloVe) and contextual (ELMo) word embeddings---for predicting expressions of generalization. Data and code are available at decomp.io.

</details>

<details>

<summary>2019-06-26 02:48:20 - A Framework for Evaluating Agricultural Ontologies</summary>

- *Anat Goldstein, Lior Fink, Gilad Ravid*

- `1906.10450v2` - [abs](http://arxiv.org/abs/1906.10450v2) - [pdf](http://arxiv.org/pdf/1906.10450v2)

> An ontology is a formal representation of domain knowledge, which can be interpreted by machines. In recent years, ontologies have become a major tool for domain knowledge representation and a core component of many knowledge management systems, decision support systems and other intelligent systems, inter alia, in the context of agriculture. A review of the existing literature on agricultural ontologies, however, reveals that most of the studies, which propose agricultural ontologies, are lacking an explicit evaluation procedure. This is undesired because without well-structured evaluation processes, it is difficult to consider the value of ontologies to research and practice. Moreover, it is difficult to rely on such ontologies and share them on the Semantic Web or between semantic aware applications. With the growing number of ontology-based agricultural systems and the increasing popularity of the Semantic Web, it becomes essential that such development and evaluation methods are put forward to guide future efforts of ontology development. Our work contributes to the literature on agricultural ontologies, by presenting a method for evaluating agricultural ontologies, which seems to be missing from most existing studies on agricultural ontologies. The framework supports the matching of appropriate evaluation methods for a given ontology based on the ontology's purpose.

</details>

<details>

<summary>2019-06-26 10:10:07 - Latent Multi-Criteria Ratings for Recommendations</summary>

- *Pan Li, Alexander Tuzhilin*

- `1906.10948v1` - [abs](http://arxiv.org/abs/1906.10948v1) - [pdf](http://arxiv.org/pdf/1906.10948v1)

> Multi-criteria recommender systems have been increasingly valuable for helping consumers identify the most relevant items based on different dimensions of user experiences. However, previously proposed multi-criteria models did not take into account latent embeddings generated from user reviews, which capture latent semantic relations between users and items. To address these concerns, we utilize variational autoencoders to map user reviews into latent embeddings, which are subsequently compressed into low-dimensional discrete vectors. The resulting compressed vectors constitute latent multi-criteria ratings that we use for the recommendation purposes via standard multi-criteria recommendation methods. We show that the proposed latent multi-criteria rating approach outperforms several baselines significantly and consistently across different datasets and performance evaluation measures.

</details>

<details>

<summary>2019-06-26 10:45:50 - End-to-End 3D-PointCloud Semantic Segmentation for Autonomous Driving</summary>

- *Mohammed Abdou, Mahmoud Elkhateeb, Ibrahim Sobh, Ahmad Elsallab*

- `1906.10964v1` - [abs](http://arxiv.org/abs/1906.10964v1) - [pdf](http://arxiv.org/pdf/1906.10964v1)

> 3D semantic scene labeling is a fundamental task for Autonomous Driving. Recent work shows the capability of Deep Neural Networks in labeling 3D point sets provided by sensors like LiDAR, and Radar. Imbalanced distribution of classes in the dataset is one of the challenges that face 3D semantic scene labeling task. This leads to misclassifying for the non-dominant classes which suffer from two main problems: a) rare appearance in the dataset, and b) few sensor points reflected from one object of these classes. This paper proposes a Weighted Self-Incremental Transfer Learning as a generalized methodology that solves the imbalanced training dataset problems. It re-weights the components of the loss function computed from individual classes based on their frequencies in the training dataset, and applies Self-Incremental Transfer Learning by running the Neural Network model on non-dominant classes first, then dominant classes one-by-one are added. The experimental results introduce a new 3D point cloud semantic segmentation benchmark for KITTI dataset.

</details>

<details>

<summary>2019-06-26 11:07:29 - Defending Adversarial Attacks by Correcting logits</summary>

- *Yifeng Li, Lingxi Xie, Ya Zhang, Rui Zhang, Yanfeng Wang, Qi Tian*

- `1906.10973v1` - [abs](http://arxiv.org/abs/1906.10973v1) - [pdf](http://arxiv.org/pdf/1906.10973v1)

> Generating and eliminating adversarial examples has been an intriguing topic in the field of deep learning. While previous research verified that adversarial attacks are often fragile and can be defended via image-level processing, it remains unclear how high-level features are perturbed by such attacks. We investigate this issue from a new perspective, which purely relies on logits, the class scores before softmax, to detect and defend adversarial attacks. Our defender is a two-layer network trained on a mixed set of clean and perturbed logits, with the goal being recovering the original prediction. Upon a wide range of adversarial attacks, our simple approach shows promising results with relatively high accuracy in defense, and the defender can transfer across attackers with similar properties. More importantly, our defender can work in the scenarios that image data are unavailable, and enjoys high interpretability especially at the semantic level.

</details>

<details>

<summary>2019-06-26 13:43:57 - Security Update Labels: Establishing Economic Incentives for Security Patching of IoT Consumer Products</summary>

- *Philipp Morgner, Christoph Mai, Nicole Koschate-Fischer, Felix Freiling, Zinaida Benenson*

- `1906.11094v1` - [abs](http://arxiv.org/abs/1906.11094v1) - [pdf](http://arxiv.org/pdf/1906.11094v1)

> With the expansion of the Internet of Things (IoT), the number of security incidents due to insecure and misconfigured IoT devices is increasing. Especially on the consumer market, manufacturers focus on new features and early releases at the expense of a comprehensive security strategy. Hence, experts have started calling for regulation of the IoT consumer market, while policymakers are seeking for suitable regulatory approaches. We investigate how manufacturers can be incentivized to increase sustainable security efforts for IoT products. We propose mandatory security update labels that inform consumers during buying decisions about the willingness of the manufacturer to provide security updates in the future. Mandatory means that the labels explicitly state when security updates are not guaranteed. We conducted a user study with more than 1,400 participants to assess the importance of security update labels for the consumer choice by means of a conjoint analysis. The results show that the availability of security updates (until which date the updates are guaranteed) accounts for 8% to 35% impact on overall consumers' choice, depending on the perceived security risk of the product category. For products with a high perceived security risk, this availability is twice as important as other high-ranked product attributes. Moreover, provisioning time for security updates (how quickly the product will be patched after a vulnerability is discovered) additionally accounts for 7% to 25% impact on consumers' choices. The proposed labels are intuitively understood by consumers, do not require product assessments by third parties before release, and have a potential to incentivize manufacturers to provide sustainable security support.

</details>

<details>

<summary>2019-06-26 13:51:59 - A Deep Decoder Structure Based on WordEmbedding Regression for An Encoder-Decoder Based Model for Image Captioning</summary>

- *Ahmad Asadi, Reza Safabakhsh*

- `1906.12188v1` - [abs](http://arxiv.org/abs/1906.12188v1) - [pdf](http://arxiv.org/pdf/1906.12188v1)

> Generating textual descriptions for images has been an attractive problem for the computer vision and natural language processing researchers in recent years. Dozens of models based on deep learning have been proposed to solve this problem. The existing approaches are based on neural encoder-decoder structures equipped with the attention mechanism. These methods strive to train decoders to minimize the log likelihood of the next word in a sentence given the previous ones, which results in the sparsity of the output space. In this work, we propose a new approach to train decoders to regress the word embedding of the next word with respect to the previous ones instead of minimizing the log likelihood. The proposed method is able to learn and extract long-term information and can generate longer fine-grained captions without introducing any external memory cell. Furthermore, decoders trained by the proposed technique can take the importance of the generated words into consideration while generating captions. In addition, a novel semantic attention mechanism is proposed that guides attention points through the image, taking the meaning of the previously generated word into account. We evaluate the proposed approach with the MS-COCO dataset. The proposed model outperformed the state of the art models especially in generating longer captions. It achieved a CIDEr score equal to 125.0 and a BLEU-4 score equal to 50.5, while the best scores of the state of the art models are 117.1 and 48.0, respectively.

</details>

<details>

<summary>2019-06-26 14:11:41 - From Multi-modal Property Dataset to Robot-centric Conceptual Knowledge About Household Objects</summary>

- *Madhura Thosar, Christian A. Mueller, Georg Jaeger, Johannes Schleiss, Narender Pulugu, Ravi Mallikarjun Chennaboina, Sai Vivek Jeevangekar, Andreas Birk, Max Pfingsthorn, Sebastian Zug*

- `1906.11114v1` - [abs](http://arxiv.org/abs/1906.11114v1) - [pdf](http://arxiv.org/pdf/1906.11114v1)

> Tool-use applications in robotics require conceptual knowledge about objects for informed decision making and object interactions. State-of-the-art methods employ hand-crafted symbolic knowledge which is defined from a human perspective and grounded into sensory data afterwards. However, due to different sensing and acting capabilities of robots, their conceptual understanding of objects must be generated from a robot's perspective entirely, which asks for robot-centric conceptual knowledge about objects. With this goal in mind, this article motivates that such knowledge should be based on physical and functional properties of objects. Consequently, a selection of ten properties is defined and corresponding extraction methods are proposed. This multi-modal property extraction forms the basis on which our second contribution, a robot-centric knowledge generation is build on. It employs unsupervised clustering methods to transform numerical property data into symbols, and Bivariate Joint Frequency Distributions and Sample Proportion to generate conceptual knowledge about objects using the robot-centric symbols. A preliminary implementation of the proposed framework is employed to acquire a dataset comprising physical and functional property data of 110 houshold objects. This Robot-Centric dataSet (RoCS) is used to evaluate the framework regarding the property extraction methods, the semantics of the considered properties within the dataset and its usefulness in real-world applications such as tool substitution.

</details>

<details>

<summary>2019-06-26 15:53:59 - Canonicalizing Knowledge Base Literals</summary>

- *Jiaoyan Chen, Ernesto Jimenez-Ruiz, Ian Horrocks*

- `1906.11180v1` - [abs](http://arxiv.org/abs/1906.11180v1) - [pdf](http://arxiv.org/pdf/1906.11180v1)

> Ontology-based knowledge bases (KBs) like DBpedia are very valuable resources, but their usefulness and usability is limited by various quality issues. One such issue is the use of string literals instead of semantically typed entities. In this paper we study the automated canonicalization of such literals, i.e., replacing the literal with an existing entity from the KB or with a new entity that is typed using classes from the KB. We propose a framework that combines both reasoning and machine learning in order to predict the relevant entities and types, and we evaluate this framework against state-of-the-art baselines for both semantic typing and entity matching.

</details>

<details>

<summary>2019-06-27 02:00:32 - Deep Instance-Level Hard Negative Mining Model for Histopathology Images</summary>

- *Meng Li, Lin Wu, Arnold Wiliem, Kun Zhao, Teng Zhang, Brian C. Lovell*

- `1906.09681v3` - [abs](http://arxiv.org/abs/1906.09681v3) - [pdf](http://arxiv.org/pdf/1906.09681v3)

> Histopathology image analysis can be considered as a Multiple instance learning (MIL) problem, where the whole slide histopathology image (WSI) is regarded as a bag of instances (i.e, patches) and the task is to predict a single class label to the WSI. However, in many real-life applications such as computational pathology, discovering the key instances that trigger the bag label is of great interest because it provides reasons for the decision made by the system. In this paper, we propose a deep convolutional neural network (CNN) model that addresses the primary task of a bag classification on a WSI and also learns to identify the response of each instance to provide interpretable results to the final prediction. We incorporate the attention mechanism into the proposed model to operate the transformation of instances and learn attention weights to allow us to find key patches. To perform a balanced training, we introduce adaptive weighing in each training bag to explicitly adjust the weight distribution in order to concentrate more on the contribution of hard samples. Based on the learned attention weights, we further develop a solution to boost the classification performance by generating the bags with hard negative instances. We conduct extensive experiments on colon and breast cancer histopathology data and show that our framework achieves state-of-the-art performance.

</details>

<details>

<summary>2019-06-27 09:26:22 - On the notion of number in humans and machines</summary>

- *Norbert Bátfai, Dávid Papp, Gergő Bogacsovics, Máté Szabó, Viktor Szilárd Simkó, Márió Bersenszki, Gergely Szabó, Lajos Kovács, Ferencz Kovács, Erik Szilveszter Varga*

- `1906.12213v1` - [abs](http://arxiv.org/abs/1906.12213v1) - [pdf](http://arxiv.org/pdf/1906.12213v1)

> In this paper, we performed two types of software experiments to study the numerosity classification (subitizing) in humans and machines. Experiments focus on a particular kind of task is referred to as Semantic MNIST or simply SMNIST where the numerosity of objects placed in an image must be determined. The experiments called SMNIST for Humans are intended to measure the capacity of the Object File System in humans. In this type of experiment the measurement result is in well agreement with the value known from the cognitive psychology literature. The experiments called SMNIST for Machines serve similar purposes but they investigate existing, well known (but originally developed for other purpose) and under development deep learning computer programs. These measurement results can be interpreted similar to the results from SMNIST for Humans. The main thesis of this paper can be formulated as follows: in machines the image classification artificial neural networks can learn to distinguish numerosities with better accuracy when these numerosities are smaller than the capacity of OFS in humans. Finally, we outline a conceptual framework to investigate the notion of number in humans and machines.

</details>

<details>

<summary>2019-06-27 14:25:11 - Adversarial Pixel-Level Generation of Semantic Images</summary>

- *Emanuele Ghelfi, Paolo Galeone, Michele De Simoni, Federico Di Mattia*

- `1906.12195v1` - [abs](http://arxiv.org/abs/1906.12195v1) - [pdf](http://arxiv.org/pdf/1906.12195v1)

> Generative Adversarial Networks (GANs) have obtained extraordinary success in the generation of realistic images, a domain where a lower pixel-level accuracy is acceptable. We study the problem, not yet tackled in the literature, of generating semantic images starting from a prior distribution. Intuitively this problem can be approached using standard methods and architectures. However, a better-suited approach is needed to avoid generating blurry, hallucinated and thus unusable images since tasks like semantic segmentation require pixel-level exactness. In this work, we present a novel architecture for learning to generate pixel-level accurate semantic images, namely Semantic Generative Adversarial Networks (SemGANs). The experimental evaluation shows that our architecture outperforms standard ones from both a quantitative and a qualitative point of view in many semantic image generation tasks.

</details>

<details>

<summary>2019-06-27 15:54:57 - Semantic expressive capacity with bounded memory</summary>

- *Antoine Venant, Alexander Koller*

- `1906.11752v1` - [abs](http://arxiv.org/abs/1906.11752v1) - [pdf](http://arxiv.org/pdf/1906.11752v1)

> We investigate the capacity of mechanisms for compositional semantic parsing to describe relations between sentences and semantic representations.   We prove that in order to represent certain relations, mechanisms which are syntactically projective must be able to remember an unbounded number of locations in the semantic representations, where nonprojective mechanisms need not.   This is the first result of this kind, and has consequences both for grammar-based and for neural systems.

</details>

<details>

<summary>2019-06-27 18:23:27 - Relating Simple Sentence Representations in Deep Neural Networks and the Brain</summary>

- *Sharmistha Jat, Hao Tang, Partha Talukdar, Tom Mitchell*

- `1906.11861v1` - [abs](http://arxiv.org/abs/1906.11861v1) - [pdf](http://arxiv.org/pdf/1906.11861v1)

> What is the relationship between sentence representations learned by deep recurrent models against those encoded by the brain? Is there any correspondence between hidden layers of these recurrent models and brain regions when processing sentences? Can these deep models be used to synthesize brain data which can then be utilized in other extrinsic tasks? We investigate these questions using sentences with simple syntax and semantics (e.g., The bone was eaten by the dog.). We consider multiple neural network architectures, including recently proposed ELMo and BERT. We use magnetoencephalography (MEG) brain recording data collected from human subjects when they were reading these simple sentences.   Overall, we find that BERT's activations correlate the best with MEG brain data. We also find that the deep network representation can be used to generate brain data from new sentences to augment existing brain data.   To the best of our knowledge, this is the first work showing that the MEG brain recording when reading a word in a sentence can be used to distinguish earlier words in the sentence. Our exploration is also the first to use deep neural network representations to generate synthetic brain data and to show that it helps in improving subsequent stimuli decoding task accuracy.

</details>

<details>

<summary>2019-06-27 19:14:29 - Memory and Resource Leak Defects and their Repairs in Java Projects</summary>

- *Mohammadreza Ghanavati, Diego Costa, Janos Seboek, David Lo, Artur Andrzejak*

- `1810.00101v2` - [abs](http://arxiv.org/abs/1810.00101v2) - [pdf](http://arxiv.org/pdf/1810.00101v2)

> Despite huge software engineering efforts and programming language support, resource and memory leaks are still a troublesome issue, even in memory-managed languages such as Java. Understanding the properties of leak-inducing defects, how the leaks manifest, and how they are repaired is an essential prerequisite for designing better approaches for avoidance, diagnosis, and repair of leak-related bugs.   We conduct a detailed empirical study on 491 issues from 15 large open-source Java projects. The study proposes taxonomies for the leak types, for the defects causing them, and for the repair actions. We investigate, under several aspects, the distributions within each taxonomy and the relationships between them. We find that manual code inspection and manual runtime detection are still the main methods for leak detection. We find that most of the errors manifest on error-free execution paths, and developers repair the leak defects in a shorter time than non-leak defects. We also identify 13 recurring code transformations in the repair patches. Based on our findings, we draw a variety of implications on how developers can avoid, detect, isolate and repair leak-related bugs.

</details>

<details>

<summary>2019-06-28 03:29:38 - A Neural-based Program Decompiler</summary>

- *Cheng Fu, Huili Chen, Haolan Liu, Xinyun Chen, Yuandong Tian, Farinaz Koushanfar, Jishen Zhao*

- `1906.12029v1` - [abs](http://arxiv.org/abs/1906.12029v1) - [pdf](http://arxiv.org/pdf/1906.12029v1)

> Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda, the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior performance compared to baseline approaches. We assess Coda's performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82\% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0\% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70\% program accuracy.

</details>

<details>

<summary>2019-06-28 06:22:54 - Black-box Adversarial Attacks on Video Recognition Models</summary>

- *Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey, Yu-Gang Jiang*

- `1904.05181v2` - [abs](http://arxiv.org/abs/1904.05181v2) - [pdf](http://arxiv.org/pdf/1904.05181v2)

> Deep neural networks (DNNs) are known for their vulnerability to adversarial examples. These are examples that have undergone small, carefully crafted perturbations, and which can easily fool a DNN into making misclassifications at test time. Thus far, the field of adversarial research has mainly focused on image models, under either a white-box setting, where an adversary has full access to model parameters, or a black-box setting where an adversary can only query the target model for probabilities or labels. Whilst several white-box attacks have been proposed for video models, black-box video attacks are still unexplored. To close this gap, we propose the first black-box video attack framework, called V-BAD. V-BAD utilizes tentative perturbations transferred from image models, and partition-based rectifications found by the NES on partitions (patches) of tentative perturbations, to obtain good adversarial gradient estimates with fewer queries to the target model. V-BAD is equivalent to estimating the projection of an adversarial gradient on a selected subspace. Using three benchmark video datasets, we demonstrate that V-BAD can craft both untargeted and targeted attacks to fool two state-of-the-art deep video recognition models. For the targeted attack, it achieves $>$93\% success rate using only an average of $3.4 \sim 8.4 \times 10^4$ queries, a similar number of queries to state-of-the-art black-box image attacks. This is despite the fact that videos often have two orders of magnitude higher dimensionality than static images. We believe that V-BAD is a promising new tool to evaluate and improve the robustness of video recognition models to black-box adversarial attacks.

</details>

<details>

<summary>2019-06-28 08:32:46 - Uncovering the Semantics of Wikipedia Categories</summary>

- *Nicolas Heist, Heiko Paulheim*

- `1906.12089v1` - [abs](http://arxiv.org/abs/1906.12089v1) - [pdf](http://arxiv.org/pdf/1906.12089v1)

> The Wikipedia category graph serves as the taxonomic backbone for large-scale knowledge graphs like YAGO or Probase, and has been used extensively for tasks like entity disambiguation or semantic similarity estimation. Wikipedia's categories are a rich source of taxonomic as well as non-taxonomic information. The category 'German science fiction writers', for example, encodes the type of its resources (Writer), as well as their nationality (German) and genre (Science Fiction). Several approaches in the literature make use of fractions of this encoded information without exploiting its full potential. In this paper, we introduce an approach for the discovery of category axioms that uses information from the category network, category instances, and their lexicalisations. With DBpedia as background knowledge, we discover 703k axioms covering 502k of Wikipedia's categories and populate the DBpedia knowledge graph with additional 4.4M relation assertions and 3.3M type assertions at more than 87% and 90% precision, respectively.

</details>

<details>

<summary>2019-06-28 11:32:13 - Node Representation Learning for Directed Graphs</summary>

- *Megha Khosla, Jurek Leonhardt, Wolfgang Nejdl, Avishek Anand*

- `1810.09176v4` - [abs](http://arxiv.org/abs/1810.09176v4) - [pdf](http://arxiv.org/pdf/1810.09176v4)

> We propose a novel approach for learning node representations in directed graphs, which maintains separate views or embedding spaces for the two distinct node roles induced by the directionality of the edges. We argue that the previous approaches either fail to encode the edge directionality or their encodings cannot be generalized across tasks. With our simple \emph{alternating random walk} strategy, we generate role specific vertex neighborhoods and train node embeddings in their corresponding source/target roles while fully exploiting the semantics of directed graphs. We also unearth the limitations of evaluations on directed graphs in previous works and propose a clear strategy for evaluating link prediction and graph reconstruction in directed graphs. We conduct extensive experiments to showcase our effectiveness on several real-world datasets on link prediction, node classification and graph reconstruction tasks. We show that the embeddings from our approach are indeed robust, generalizable and well performing across multiple kinds of tasks and graphs. We show that we consistently outperform all baselines for node classification task. In addition to providing a theoretical interpretation of our method we also show that we are considerably more robust than the other directed graph approaches.

</details>

<details>

<summary>2019-06-29 01:25:29 - Helion: Enabling a Natural Perspective of Home Automation</summary>

- *Sunil Manandhar, Kevin Moran, Kaushal Kafle, Ruhao Tang, Denys Poshyvanyk, Adwait Nadkarni*

- `1907.00124v1` - [abs](http://arxiv.org/abs/1907.00124v1) - [pdf](http://arxiv.org/pdf/1907.00124v1)

> Security researchers have recently discovered significant security and safety issues related to home automation and developed approaches to address them. Such approaches often face design and evaluation challenges which arise from their restricted perspective of home automation that is bounded by the IoT apps they analyze. The challenges of past work can be overcome by relying on a deeper understanding of realistic home automation usage. More specifically, the availability of natural home automation scenarios, i.e., sequences of home automation events that may realistically occur in an end-user's home, could help security researchers design better security/safety systems. This paper presents Helion, a framework for building a natural perspective of home automation. Helion identifies the regularities in user-driven home automation, i.e., from user-driven routines that are increasingly being created by users through intuitive platform UIs. Our intuition for designing Helion is that smart home event sequences created by users exhibit an inherent set of semantic patterns, or naturalness that can be modeled and used to generate valid and useful scenarios. To evaluate our approach, we first empirically demonstrate that this naturalness hypothesis holds, with a corpus of 30,518 home automation events, constructed from 273 routines collected from 40 users. We then demonstrate that the scenarios generated by Helion are reasonable and valid from an end-user perspective, through an evaluation with 16 external evaluators. We further show the usefulness of Helion's scenarios by generating 17 home security/safety policies with significantly less effort than existing approaches. We conclude by discussing key takeaways and future research challenges enabled by Helion's natural perspective of home automation.

</details>

<details>

<summary>2019-06-29 07:19:08 - NSEEN: Neural Semantic Embedding for Entity Normalization</summary>

- *Shobeir Fakhraei, Joel Mathew, Jose Luis Ambite*

- `1811.07514v2` - [abs](http://arxiv.org/abs/1811.07514v2) - [pdf](http://arxiv.org/pdf/1811.07514v2)

> Much of human knowledge is encoded in text, available in scientific publications, books, and the web. Given the rapid growth of these resources, we need automated methods to extract such knowledge into machine-processable structures, such as knowledge graphs. An important task in this process is entity normalization, which consists of mapping noisy entity mentions in text to canonical entities in well-known reference sets. However, entity normalization is a challenging problem; there often are many textual forms for a canonical entity that may not be captured in the reference set, and entities mentioned in text may include many syntactic variations, or errors. The problem is particularly acute in scientific domains, such as biology. To address this problem, we have developed a general, scalable solution based on a deep Siamese neural network model to embed the semantic information about the entities, as well as their syntactic variations. We use these embeddings for fast mapping of new entities to large reference sets, and empirically show the effectiveness of our framework in challenging bio-entity normalization datasets.

</details>

<details>

<summary>2019-06-29 19:43:54 - MeshAdv: Adversarial Meshes for Visual Recognition</summary>

- *Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, Mingyan Liu*

- `1810.05206v2` - [abs](http://arxiv.org/abs/1810.05206v2) - [pdf](http://arxiv.org/pdf/1810.05206v2)

> Highly expressive models such as deep neural networks (DNNs) have been widely applied to various applications. However, recent studies show that DNNs are vulnerable to adversarial examples, which are carefully crafted inputs aiming to mislead the predictions. Currently, the majority of these studies have focused on perturbation added to image pixels, while such manipulation is not physically realistic. Some works have tried to overcome this limitation by attaching printable 2D patches or painting patterns onto surfaces, but can be potentially defended because 3D shape features are intact. In this paper, we propose meshAdv to generate "adversarial 3D meshes" from objects that have rich shape features but minimal textural variation. To manipulate the shape or texture of the objects, we make use of a differentiable renderer to compute accurate shading on the shape and propagate the gradient. Extensive experiments show that the generated 3D meshes are effective in attacking both classifiers and object detectors. We evaluate the attack under different viewpoints. In addition, we design a pipeline to perform black-box attack on a photorealistic renderer with unknown rendering parameters.

</details>

<details>

<summary>2019-06-30 12:10:52 - COS960: A Chinese Word Similarity Dataset of 960 Word Pairs</summary>

- *Junjie Huang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Maosong Sun*

- `1906.00247v2` - [abs](http://arxiv.org/abs/1906.00247v2) - [pdf](http://arxiv.org/pdf/1906.00247v2)

> Word similarity computation is a widely recognized task in the field of lexical semantics. Most proposed tasks test on similarity of word pairs of single morpheme, while few works focus on words of two morphemes or more morphemes. In this work, we propose COS960, a benchmark dataset with 960 pairs of Chinese wOrd Similarity, where all the words have two morphemes in three Part of Speech (POS) tags with their human annotated similarity rather than relatedness. We give a detailed description of dataset construction and annotation process, and test on a range of word embedding models. The dataset of this paper can be obtained from https://github.com/thunlp/COS960.

</details>

<details>

<summary>2019-06-30 13:59:01 - Adversarially Trained Deep Neural Semantic Hashing Scheme for Subjective Search in Fashion Inventory</summary>

- *Saket Singh, Debdoot Sheet, Mithun Dasgupta*

- `1907.00382v1` - [abs](http://arxiv.org/abs/1907.00382v1) - [pdf](http://arxiv.org/pdf/1907.00382v1)

> The simple approach of retrieving a closest match of a query image from one in the gallery, compares an image pair using sum of absolute difference in pixel or feature space. The process is computationally expensive, ill-posed to illumination, background composition, pose variation, as well as inefficient to be deployed on gallery sets with more than 1000 elements. Hashing is a faster alternative which involves representing images in reduced dimensional simple feature spaces. Encoding images into binary hash codes enables similarity comparison in an image-pair using the Hamming distance measure. The challenge, however, lies in encoding the images using a semantic hashing scheme that lets subjective neighbors lie within the tolerable Hamming radius. This work presents a solution employing adversarial learning of a deep neural semantic hashing network for fashion inventory retrieval. It consists of a feature extracting convolutional neural network (CNN) learned to (i) minimize error in classifying type of clothing, (ii) minimize hamming distance between semantic neighbors and maximize distance between semantically dissimilar images, (iii) maximally scramble a discriminator's ability to identify the corresponding hash code-image pair when processing a semantically similar query-gallery image pair. Experimental validation for fashion inventory search yields a mean average precision (mAP) of 90.65% in finding the closest match as compared to 53.26% obtained by the prior art of deep Cauchy hashing for hamming space retrieval.

</details>

<details>

<summary>2019-06-30 14:54:01 - A Novel Bi-directional Interrelated Model for Joint Intent Detection and Slot Filling</summary>

- *Haihong E, Peiqing Niu, Zhongfu Chen, Meina Song*

- `1907.00390v1` - [abs](http://arxiv.org/abs/1907.00390v1) - [pdf](http://arxiv.org/pdf/1907.00390v1)

> A spoken language understanding (SLU) system includes two main tasks, slot filling (SF) and intent detection (ID). The joint model for the two tasks is becoming a tendency in SLU. But the bi-directional interrelated connections between the intent and slots are not established in the existing joint models. In this paper, we propose a novel bi-directional interrelated model for joint intent detection and slot filling. We introduce an SF-ID network to establish direct connections for the two tasks to help them promote each other mutually. Besides, we design an entirely new iteration mechanism inside the SF-ID network to enhance the bi-directional interrelated connections. The experimental results show that the relative improvement in the sentence-level semantic frame accuracy of our model is 3.79% and 5.42% on ATIS and Snips datasets, respectively, compared to the state-of-the-art model.

</details>

<details>

<summary>2019-06-30 16:38:59 - Prospects for Declarative Mathematical Modeling of Complex Biological Systems</summary>

- *Eric Mjolsness*

- `1804.11044v2` - [abs](http://arxiv.org/abs/1804.11044v2) - [pdf](http://arxiv.org/pdf/1804.11044v2)

> Declarative modeling uses symbolic expressions to represent models. With such expressions one can formalize high-level mathematical computations on models that would be difficult or impossible to perform directly on a lower-level simulation program, in a general-purpose programming language. Examples of such computations on models include model analysis, relatively general-purpose model-reduction maps, and the initial phases of model implementation, all of which should preserve or approximate the mathematical semantics of a complex biological model. The potential advantages are particularly relevant in the case of developmental modeling, wherein complex spatial structures exhibit dynamics at molecular, cellular, and organogenic levels to relate genotype to multicellular phenotype. Multiscale modeling can benefit from both the expressive power of declarative modeling languages and the application of model reduction methods to link models across scale. Based on previous work, here we define declarative modeling of complex biological systems by defining the operator algebra semantics of an increasingly powerful series of declarative modeling languages including reaction-like dynamics of parameterized and extended objects; we define semantics-preserving implementation and semantics-approximating model reduction transformations; and we outline a "meta-hierarchy" for organizing declarative models and the mathematical methods that can fruitfully manipulate them.

</details>


## 2019-07

<details>

<summary>2019-07-01 08:41:21 - A Semantics-Based Hybrid Approach on Binary Code Similarity Comparison</summary>

- *Yikun Hu, Hui Wang, Yuanyuan Zhang, Bodong Li, Dawu Gu*

- `1907.01374v1` - [abs](http://arxiv.org/abs/1907.01374v1) - [pdf](http://arxiv.org/pdf/1907.01374v1)

> Binary code similarity comparison is a methodology for identifying similar or identical code fragments in binary programs. It is indispensable in fields of software engineering and security, which has many important applications (e.g., plagiarism detection, bug detection). With the widespread of smart and IoT (Internet of Things) devices, an increasing number of programs are ported to multiple architectures (e.g. ARM, MIPS). It becomes necessary to detect similar binary code across architectures as well. The main challenge of this topic lies in the semantics-equivalent code transformation resulting from different compilation settings, code obfuscation, and varied instruction set architectures. Another challenge is the trade-off between comparison accuracy and coverage. Unfortunately, existing methods still heavily rely on semantics-less code features which are susceptible to the code transformation. Additionally, they perform the comparison merely either in a static or in a dynamic manner, which cannot achieve high accuracy and coverage simultaneously. In this paper, we propose a semantics-based hybrid method to compare binary function similarity. We execute the reference function with test cases, then emulate the execution of every target function with the runtime information migrated from the reference function. Semantic signatures are extracted during the execution as well as the emulation. Lastly, similarity scores are calculated from the signatures to measure the likeness of functions. We have implemented the method in a prototype system designated as BinMatch and evaluate it with nine real-word projects compiled with different compilation settings, on variant architectures, and with commonly-used obfuscation methods, totally performing over 100 million pairs of function comparison.

</details>

<details>

<summary>2019-07-01 08:44:30 - Weak Supervision Enhanced Generative Network for Question Generation</summary>

- *Yutong Wang, Jiyuan Zheng, Qijiong Liu, Zhou Zhao, Jun Xiao, Yueting Zhuang*

- `1907.00607v1` - [abs](http://arxiv.org/abs/1907.00607v1) - [pdf](http://arxiv.org/pdf/1907.00607v1)

> Automatic question generation according to an answer within the given passage is useful for many applications, such as question answering system, dialogue system, etc. Current neural-based methods mostly take two steps which extract several important sentences based on the candidate answer through manual rules or supervised neural networks and then use an encoder-decoder framework to generate questions about these sentences. These approaches neglect the semantic relations between the answer and the context of the whole passage which is sometimes necessary for answering the question. To address this problem, we propose the Weak Supervision Enhanced Generative Network (WeGen) which automatically discovers relevant features of the passage given the answer span in a weakly supervised manner to improve the quality of generated questions. More specifically, we devise a discriminator, Relation Guider, to capture the relations between the whole passage and the associated answer and then the Multi-Interaction mechanism is deployed to transfer the knowledge dynamically for our question generation system. Experiments show the effectiveness of our method in both automatic evaluations and human evaluations.

</details>

<details>

<summary>2019-07-01 08:48:39 - One Network for Multi-Domains: Domain Adaptive Hashing with Intersectant Generative Adversarial Network</summary>

- *Tao He, Yuan-Fang Li, Lianli Gao, Dongxiang Zhang, Jingkuan Song*

- `1907.00612v1` - [abs](http://arxiv.org/abs/1907.00612v1) - [pdf](http://arxiv.org/pdf/1907.00612v1)

> With the recent explosive increase of digital data, image recognition and retrieval become a critical practical application. Hashing is an effective solution to this problem, due to its low storage requirement and high query speed. However, most of past works focus on hashing in a single (source) domain. Thus, the learned hash function may not adapt well in a new (target) domain that has a large distributional difference with the source domain. In this paper, we explore an end-to-end domain adaptive learning framework that simultaneously and precisely generates discriminative hash codes and classifies target domain images. Our method encodes two domains images into a semantic common space, followed by two independent generative adversarial networks arming at crosswise reconstructing two domains' images, reducing domain disparity and improving alignment in the shared space. We evaluate our framework on {four} public benchmark datasets, all of which show that our method is superior to the other state-of-the-art methods on the tasks of object recognition and image retrieval.

</details>

<details>

<summary>2019-07-01 09:14:38 - Data Complexity and Rewritability of Ontology-Mediated Queries in Metric Temporal Logic under the Event-Based Semantics (Full Version)</summary>

- *Vladislav Ryzhikov, Przemyslaw Andrzej Walega, Michael Zakharyaschev*

- `1905.12990v2` - [abs](http://arxiv.org/abs/1905.12990v2) - [pdf](http://arxiv.org/pdf/1905.12990v2)

> We investigate the data complexity of answering queries mediated by metric temporal logic ontologies under the event-based semantics assuming that data instances are finite timed words timestamped with binary fractions. We identify classes of ontology-mediated queries answering which can be done in AC0, NC1, L, NL, P, and coNP for data complexity, provide their rewritings to first-order logic and its extensions with primitive recursion, transitive closure or datalog, and establish lower complexity bounds.

</details>

<details>

<summary>2019-07-01 11:28:34 - Labeling, Cutting, Grouping: an Efficient Text Line Segmentation Method for Medieval Manuscripts</summary>

- *Michele Alberti, Lars Vögtlin, Vinaychandran Pondenkandath, Mathias Seuret, Rolf Ingold, Marcus Liwicki*

- `1906.11894v2` - [abs](http://arxiv.org/abs/1906.11894v2) - [pdf](http://arxiv.org/pdf/1906.11894v2)

> This paper introduces a new way for text-line extraction by integrating deep-learning based pre-classification and state-of-the-art segmentation methods. Text-line extraction in complex handwritten documents poses a significant challenge, even to the most modern computer vision algorithms. Historical manuscripts are a particularly hard class of documents as they present several forms of noise, such as degradation, bleed-through, interlinear glosses, and elaborated scripts. In this work, we propose a novel method which uses semantic segmentation at pixel level as intermediate task, followed by a text-line extraction step. We measured the performance of our method on a recent dataset of challenging medieval manuscripts and surpassed state-of-the-art results by reducing the error by 80.7%. Furthermore, we demonstrate the effectiveness of our approach on various other datasets written in different scripts. Hence, our contribution is two-fold. First, we demonstrate that semantic pixel segmentation can be used as strong denoising pre-processing step before performing text line extraction. Second, we introduce a novel, simple and robust algorithm that leverages the high-quality semantic segmentation to achieve a text-line extraction performance of 99.42% line IU on a challenging dataset.

</details>

<details>

<summary>2019-07-01 12:34:15 - Verifying that a compiler preserves concurrent value-dependent information-flow security</summary>

- *Robert Sison, Toby Murray*

- `1907.00713v1` - [abs](http://arxiv.org/abs/1907.00713v1) - [pdf](http://arxiv.org/pdf/1907.00713v1)

> It is common to prove by reasoning over source code that programs do not leak sensitive data. But doing so leaves a gap between reasoning and reality that can only be filled by accounting for the behaviour of the compiler. This task is complicated when programs enforce value-dependent information-flow security properties (in which classification of locations can vary depending on values in other locations) and complicated further when programs exploit shared-variable concurrency.   Prior work has formally defined a notion of concurrency-aware refinement for preserving value-dependent security properties. However, that notion is considerably more complex than standard refinement definitions typically applied in the verification of semantics preservation by compilers. To date it remains unclear whether it can be applied to a realistic compiler, because there exist no general decomposition principles for separating it into smaller, more familiar, proof obligations.   In this work, we provide such a decomposition principle, which we show can almost halve the complexity of proving secure refinement. Further, we demonstrate its applicability to secure compilation, by proving in Isabelle/HOL the preservation of value-dependent security by a proof-of-concept compiler from an imperative While language to a generic RISC-style assembly language, for programs with shared-memory concurrency mediated by locking primitives. Finally, we execute our compiler in Isabelle on a While language model of the Cross Domain Desktop Compositor, demonstrating to our knowledge the first use of a compiler verification result to carry an information-flow security property down to the assembly-level model of a non-trivial concurrent program.

</details>

<details>

<summary>2019-07-01 12:56:11 - On Privacy Protection of Latent Dirichlet Allocation Model Training</summary>

- *Fangyuan Zhao, Xuebin Ren, Shusen Yang, Xinyu Yang*

- `1906.01178v2` - [abs](http://arxiv.org/abs/1906.01178v2) - [pdf](http://arxiv.org/pdf/1906.01178v2)

> Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for discovery of hidden semantic architecture of text datasets, and plays a fundamental role in many machine learning applications. However, like many other machine learning algorithms, the process of training a LDA model may leak the sensitive information of the training datasets and bring significant privacy risks. To mitigate the privacy issues in LDA, we focus on studying privacy-preserving algorithms of LDA model training in this paper. In particular, we first develop a privacy monitoring algorithm to investigate the privacy guarantee obtained from the inherent randomness of the Collapsed Gibbs Sampling (CGS) process in a typical LDA training algorithm on centralized curated datasets. Then, we further propose a locally private LDA training algorithm on crowdsourced data to provide local differential privacy for individual data contributors. The experimental results on real-world datasets demonstrate the effectiveness of our proposed algorithms.

</details>

<details>

<summary>2019-07-01 14:49:07 - Universal audio synthesizer control with normalizing flows</summary>

- *Philippe Esling, Naotake Masuda, Adrien Bardet, Romeo Despres, Axel Chemla--Romeu-Santos*

- `1907.00971v1` - [abs](http://arxiv.org/abs/1907.00971v1) - [pdf](http://arxiv.org/pdf/1907.00971v1)

> The ubiquity of sound synthesizers has reshaped music production and even entirely defined new music genres. However, the increasing complexity and number of parameters in modern synthesizers make them harder to master. Hence, the development of methods allowing to easily create and explore with synthesizers is a crucial need. Here, we introduce a novel formulation of audio synthesizer control. We formalize it as finding an organized latent audio space that represents the capabilities of a synthesizer, while constructing an invertible mapping to the space of its parameters. By using this formulation, we show that we can address simultaneously automatic parameter inference, macro-control learning and audio-based preset exploration within a single model. To solve this new formulation, we rely on Variational Auto-Encoders (VAE) and Normalizing Flows (NF) to organize and map the respective auditory and parameter spaces. We introduce the disentangling flows, which allow to perform the invertible mapping between separate latent spaces, while steering the organization of some latent dimensions to match target variation factors by splitting the objective as partial density evaluation. We evaluate our proposal against a large set of baseline models and show its superiority in both parameter inference and audio reconstruction. We also show that the model disentangles the major factors of audio variations as latent dimensions, that can be directly used as macro-parameters. We also show that our model is able to learn semantic controls of a synthesizer by smoothly mapping to its parameters. Finally, we discuss the use of our model in creative applications and its real-time implementation in Ableton Live

</details>

<details>

<summary>2019-07-01 15:38:39 - System Misuse Detection via Informed Behavior Clustering and Modeling</summary>

- *Linara Adilova, Livin Natious, Siming Chen, Olivier Thonnard, Michael Kamp*

- `1907.00874v1` - [abs](http://arxiv.org/abs/1907.00874v1) - [pdf](http://arxiv.org/pdf/1907.00874v1)

> One of the main tasks of cybersecurity is recognizing malicious interactions with an arbitrary system. Currently, the logging information from each interaction can be collected in almost unrestricted amounts, but identification of attacks requires a lot of effort and time of security experts. We propose an approach for identifying fraud activity through modeling normal behavior in interactions with a system via machine learning methods, in particular LSTM neural networks. In order to enrich the modeling with system specific knowledge, we propose to use an interactive visual interface that allows security experts to identify semantically meaningful clusters of interactions. These clusters incorporate domain knowledge and lead to more precise behavior modeling via informed machine learning. We evaluate the proposed approach on a dataset containing logs of interactions with an administrative interface of login and security server. Our empirical results indicate that the informed modeling is capable of capturing normal behavior, which can then be used to detect abnormal behavior.

</details>

<details>

<summary>2019-07-01 17:20:02 - Semantic Product Search</summary>

- *Priyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Lakshman, Weitian, Ding, Ankit Shingavi, Choon Hui Teo, Hao Gu, Bing Yin*

- `1907.00937v1` - [abs](http://arxiv.org/abs/1907.00937v1) - [pdf](http://arxiv.org/pdf/1907.00937v1)

> We study the problem of semantic matching in product search, that is, given a customer query, retrieve all semantically related products from the catalog. Pure lexical matching via an inverted index falls short in this respect due to several factors: a) lack of understanding of hypernyms, synonyms, and antonyms, b) fragility to morphological variants (e.g. "woman" vs. "women"), and c) sensitivity to spelling errors. To address these issues, we train a deep learning model for semantic matching using customer behavior data. Much of the recent work on large-scale semantic search using deep learning focuses on ranking for web search. In contrast, semantic matching for product search presents several novel challenges, which we elucidate in this paper. We address these challenges by a) developing a new loss function that has an inbuilt threshold to differentiate between random negative examples, impressed but not purchased examples, and positive examples (purchased items), b) using average pooling in conjunction with n-grams to capture short-range linguistic patterns, c) using hashing to handle out of vocabulary tokens, and d) using a model parallel training architecture to scale across 8 GPUs. We present compelling offline results that demonstrate at least 4.7% improvement in Recall@100 and 14.5% improvement in mean average precision (MAP) over baseline state-of-the-art semantic search methods using the same tokenization method. Moreover, we present results and discuss learnings from online A/B tests which demonstrate the efficacy of our method.

</details>

<details>

<summary>2019-07-01 23:20:45 - Representation, Exploration and Recommendation of Music Playlists</summary>

- *Piyush Papreja, Hemanth Venkateswara, Sethuraman Panchanathan*

- `1907.01098v1` - [abs](http://arxiv.org/abs/1907.01098v1) - [pdf](http://arxiv.org/pdf/1907.01098v1)

> Playlists have become a significant part of our listening experience because of the digital cloud-based services such as Spotify, Pandora, Apple Music. Owing to the meteoric rise in the usage of playlists, recommending playlists is crucial to music services today. Although there has been a lot of work done in playlist prediction, the area of playlist representation hasn't received that level of attention. Over the last few years, sequence-to-sequence models, especially in the field of natural language processing, have shown the effectiveness of learned embeddings in capturing the semantic characteristics of sequences. We can apply similar concepts to music to learn fixed length representations for playlists and use those representations for downstream tasks such as playlist discovery, browsing, and recommendation. In this work, we formulate the problem of learning a fixed-length playlist representation in an unsupervised manner, using Sequence-to-sequence (Seq2seq) models, interpreting playlists as sentences and songs as words. We compare our model with two other encoding architectures for baseline comparison. We evaluate our work using the suite of tasks commonly used for assessing sentence embeddings, along with a few additional tasks pertaining to music, and a recommendation task to study the traits captured by the playlist embeddings and their effectiveness for the purpose of music recommendation.

</details>

<details>

<summary>2019-07-02 00:31:05 - Spoken Language Intent Detection using Confusion2Vec</summary>

- *Prashanth Gurunath Shivakumar, Mu Yang, Panayiotis Georgiou*

- `1904.03576v3` - [abs](http://arxiv.org/abs/1904.03576v3) - [pdf](http://arxiv.org/pdf/1904.03576v3)

> Decoding speaker's intent is a crucial part of spoken language understanding (SLU). The presence of noise or errors in the text transcriptions, in real life scenarios make the task more challenging. In this paper, we address the spoken language intent detection under noisy conditions imposed by automatic speech recognition (ASR) systems. We propose to employ confusion2vec word feature representation to compensate for the errors made by ASR and to increase the robustness of the SLU system. The confusion2vec, motivated from human speech production and perception, models acoustic relationships between words in addition to the semantic and syntactic relations of words in human language. We hypothesize that ASR often makes errors relating to acoustically similar words, and the confusion2vec with inherent model of acoustic relationships between words is able to compensate for the errors. We demonstrate through experiments on the ATIS benchmark dataset, the robustness of the proposed model to achieve state-of-the-art results under noisy ASR conditions. Our system reduces classification error rate (CER) by 20.84% and improves robustness by 37.48% (lower CER degradation) relative to the previous state-of-the-art going from clean to noisy transcripts. Improvements are also demonstrated when training the intent detection models on noisy transcripts.

</details>

<details>

<summary>2019-07-02 01:09:35 - Neural Semantic Parsing with Anonymization for Command Understanding in General-Purpose Service Robots</summary>

- *Nick Walker, Yu-Tang Peng, Maya Cakmak*

- `1907.01115v1` - [abs](http://arxiv.org/abs/1907.01115v1) - [pdf](http://arxiv.org/pdf/1907.01115v1)

> Service robots are envisioned to undertake a wide range of tasks at the request of users. Semantic parsing is one way to convert natural language commands given to these robots into executable representations. Methods for creating semantic parsers, however, rely either on large amounts of data or on engineered lexical features and parsing rules, which has limited their application in robotics. To address this challenge, we propose an approach that leverages neural semantic parsing methods in combination with contextual word embeddings to enable the training of a semantic parser with little data and without domain specific parser engineering. Key to our approach is the use of an anonymized target representation which is more easily learned by the parser. In most cases, this simplified representation can trivially be transformed into an executable format, and in others the parse can be completed through further interaction with the user. We evaluate this approach in the context of the RoboCup@Home General Purpose Service Robot task, where we have collected a corpus of paraphrased versions of commands from the standardized command generator. Our results show that neural semantic parsers can predict the logical form of unseen commands with 89% accuracy. We release our data and the details of our models to encourage further development from the RoboCup and service robotics communities.

</details>

<details>

<summary>2019-07-02 04:54:17 - Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems</summary>

- *Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C. H. Hoi*

- `1907.01166v1` - [abs](http://arxiv.org/abs/1907.01166v1) - [pdf](http://arxiv.org/pdf/1907.01166v1)

> Developing Video-Grounded Dialogue Systems (VGDS), where a dialogue is conducted based on visual and audio aspects of a given video, is significantly more challenging than traditional image or text-grounded dialogue systems because (1) feature space of videos span across multiple picture frames, making it difficult to obtain semantic information; and (2) a dialogue agent must perceive and process information from different modalities (audio, video, caption, etc.) to obtain a comprehensive understanding. Most existing work is based on RNNs and sequence-to-sequence architectures, which are not very effective for capturing complex long-term dependencies (like in videos). To overcome this, we propose Multimodal Transformer Networks (MTN) to encode videos and incorporate information from different modalities. We also propose query-aware attention through an auto-encoder to extract query-aware features from non-text modalities. We develop a training procedure to simulate token-level decoding to improve the quality of generated responses during inference. We get state of the art performance on Dialogue System Technology Challenge 7 (DSTC7). Our model also generalizes to another multimodal visual-grounded dialogue task, and obtains promising performance. We implemented our models using PyTorch and the code is released at https://github.com/henryhungle/MTN.

</details>

<details>

<summary>2019-07-02 07:49:40 - Syntax Evolution: Problems and Recursion</summary>

- *Ramón Casares*

- `1508.03040v7` - [abs](http://arxiv.org/abs/1508.03040v7) - [pdf](http://arxiv.org/pdf/1508.03040v7)

> To investigate the evolution of syntax, we need to ascertain the evolutionary r\^ole of syntax and, before that, the very nature of syntax. Here, we will assume that syntax is computing. And then, since we are computationally Turing complete, we meet an evolutionary anomaly, the anomaly of sytax: we are syntactically too competent for syntax. Assuming that problem solving is computing, and realizing that the evolutionary advantage of Turing completeness is full problem solving and not syntactic proficiency, we explain the anomaly of syntax by postulating that syntax and problem solving co-evolved in humans towards Turing completeness. Examining the requirements that full problem solving impose on language, we find firstly that semantics is not sufficient and that syntax is necessary to represent problems. Our final conclusion is that full problem solving requires a functional semantics on an infinite tree-structured syntax. Besides these results, the introduction of Turing completeness and problem solving to explain the evolution of syntax should help us to fit the evolution of language within the evolution of cognition, giving us some new clues to understand the elusive relation between language and thinking.

</details>

<details>

<summary>2019-07-02 16:29:02 - Obj-GloVe: Scene-Based Contextual Object Embedding</summary>

- *Canwen Xu, Zhenzhong Chen, Chenliang Li*

- `1907.01478v1` - [abs](http://arxiv.org/abs/1907.01478v1) - [pdf](http://arxiv.org/pdf/1907.01478v1)

> Recently, with the prevalence of large-scale image dataset, the co-occurrence information among classes becomes rich, calling for a new way to exploit it to facilitate inference. In this paper, we propose Obj-GloVe, a generic scene-based contextual embedding for common visual objects, where we adopt the word embedding method GloVe to exploit the co-occurrence between entities. We train the embedding on pre-processed Open Images V4 dataset and provide extensive visualization and analysis by dimensionality reduction and projecting the vectors along a specific semantic axis, and showcasing the nearest neighbors of the most common objects. Furthermore, we reveal the potential applications of Obj-GloVe on object detection and text-to-image synthesis, then verify its effectiveness on these two applications respectively.

</details>

<details>

<summary>2019-07-02 19:17:43 - Combating the Filter Bubble: Designing for Serendipity in a University Course Recommendation System</summary>

- *Zachary A. Pardos, Weijie Jiang*

- `1907.01591v1` - [abs](http://arxiv.org/abs/1907.01591v1) - [pdf](http://arxiv.org/pdf/1907.01591v1)

> Collaborative filtering based algorithms, including Recurrent Neural Networks (RNN), tend towards predicting a perpetuation of past observed behavior. In a recommendation context, this can lead to an overly narrow set of suggestions lacking in serendipity and inadvertently placing the user in what is known as a "filter bubble." In this paper, we grapple with the issue of the filter bubble in the context of a course recommendation system in production at a public university. Most universities in the United States encourage students to explore developing interests while simultaneously advising them to adhere to course taking norms which progress them towards graduation. These competing objectives, and the stakes involved for students, make this context a particularly meaningful one for investigating real-world recommendation strategies. We introduce a novel modification to the skip-gram model applied to nine years of historic course enrollment sequences to learn course vector representations used to diversify recommendations based on similarity to a student's specified favorite course. This model, which we call multifactor2vec, is intended to improve the semantics of the primary token embedding by also learning embeddings of potentially conflated factors of the token (e.g., instructor). Our offline testing found this model improved accuracy and recall on our course similarity and analogy validation sets over a standard skip-gram. Incorporating course catalog description text resulted in further improvements. We compare the performance of these models to the system's existing RNN-based recommendations with a user study of undergraduates (N = 70) rating six characteristics of their course recommendations. Results of the user study show a dramatic lack of novelty in RNN recommendations and depict the characteristic trade-offs that make serendipity difficult to achieve.

</details>

<details>

<summary>2019-07-02 21:34:15 - Sentiment Adaptive End-to-End Dialog Systems</summary>

- *Weiyan Shi, Zhou Yu*

- `1804.10731v3` - [abs](http://arxiv.org/abs/1804.10731v3) - [pdf](http://arxiv.org/pdf/1804.10731v3)

> End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.

</details>

<details>

<summary>2019-07-03 02:28:48 - Compositional Structure Learning for Sequential Video Data</summary>

- *Kyoung-Woon On, Eun-Sol Kim, Yu-Jung Heo, Byoung-Tak Zhang*

- `1907.01709v1` - [abs](http://arxiv.org/abs/1907.01709v1) - [pdf](http://arxiv.org/pdf/1907.01709v1)

> Conventional sequential learning methods such as Recurrent Neural Networks (RNNs) focus on interactions between consecutive inputs, i.e. first-order Markovian dependency. However, most of sequential data, as seen with videos, have complex temporal dependencies that imply variable-length semantic flows and their compositions, and those are hard to be captured by conventional methods. Here, we propose Temporal Dependency Networks (TDNs) for learning video data by discovering these complex structures of the videos. The TDNs represent video as a graph whose nodes and edges correspond to frames of the video and their dependencies respectively. Via a parameterized kernel with graph-cut and graph convolutions, the TDNs find compositional temporal dependencies of the data in multilevel graph forms. We evaluate the proposed method on the large-scale video dataset Youtube-8M. The experimental results show that our model efficiently learns the complex semantic structure of video data.

</details>

<details>

<summary>2019-07-03 02:30:36 - Mask Embedding in conditional GAN for Guided Synthesis of High Resolution Images</summary>

- *Yinhao Ren, Zhe Zhu, Yingzhou Li, Joseph Lo*

- `1907.01710v1` - [abs](http://arxiv.org/abs/1907.01710v1) - [pdf](http://arxiv.org/pdf/1907.01710v1)

> Recent advancements in conditional Generative Adversarial Networks (cGANs) have shown promises in label guided image synthesis. Semantic masks, such as sketches and label maps, are another intuitive and effective form of guidance in image synthesis. Directly incorporating the semantic masks as constraints dramatically reduces the variability and quality of the synthesized results. We observe this is caused by the incompatibility of features from different inputs (such as mask image and latent vector) of the generator. To use semantic masks as guidance whilst providing realistic synthesized results with fine details, we propose to use mask embedding mechanism to allow for a more efficient initial feature projection in the generator. We validate the effectiveness of our approach by training a mask guided face generator using CELEBA-HQ dataset. We can generate realistic and high resolution facial images up to the resolution of 512*512 with a mask guidance. Our code is publicly available.

</details>

<details>

<summary>2019-07-03 03:16:39 - Improving Semantic Segmentation via Video Propagation and Label Relaxation</summary>

- *Yi Zhu, Karan Sapra, Fitsum A. Reda, Kevin J. Shih, Shawn Newsam, Andrew Tao, Bryan Catanzaro*

- `1812.01593v3` - [abs](http://arxiv.org/abs/1812.01593v3) - [pdf](http://arxiv.org/pdf/1812.01593v3)

> Semantic segmentation requires large amounts of pixel-wise annotations to learn accurate models. In this paper, we present a video prediction-based methodology to scale up training sets by synthesizing new training samples in order to improve the accuracy of semantic segmentation networks. We exploit video prediction models' ability to predict future frames in order to also predict future labels. A joint propagation strategy is also proposed to alleviate mis-alignments in synthesized samples. We demonstrate that training segmentation models on datasets augmented by the synthesized samples leads to significant improvements in accuracy. Furthermore, we introduce a novel boundary label relaxation technique that makes training robust to annotation noise and propagation artifacts along object boundaries. Our proposed methods achieve state-of-the-art mIoUs of 83.5% on Cityscapes and 82.9% on CamVid. Our single model, without model ensembles, achieves 72.8% mIoU on the KITTI semantic segmentation test set, which surpasses the winning entry of the ROB challenge 2018. Our code and videos can be found at https://nv-adlr.github.io/publication/2018-Segmentation.

</details>

<details>

<summary>2019-07-03 16:50:50 - Real-time Claim Detection from News Articles and Retrieval of Semantically-Similar Factchecks</summary>

- *Ben Adler, Giacomo Boscaini-Gilroy*

- `1907.02030v1` - [abs](http://arxiv.org/abs/1907.02030v1) - [pdf](http://arxiv.org/pdf/1907.02030v1)

> Factchecking has always been a part of the journalistic process. However with newsroom budgets shrinking it is coming under increasing pressure just as the amount of false information circulating is on the rise. We therefore propose a method to increase the efficiency of the factchecking process, using the latest developments in Natural Language Processing (NLP). This method allows us to compare incoming claims to an existing corpus and return similar, factchecked, claims in a live system-allowing factcheckers to work simultaneously without duplicating their work.

</details>

<details>

<summary>2019-07-03 16:53:28 - Combining Q&A Pair Quality and Question Relevance Features on Community-based Question Retrieval</summary>

- *Dong Li, Lin Li*

- `1907.02031v1` - [abs](http://arxiv.org/abs/1907.02031v1) - [pdf](http://arxiv.org/pdf/1907.02031v1)

> The Q&A community has become an important way for people to access knowledge and information from the Internet. However, the existing translation based on models does not consider the query specific semantics when assigning weights to query terms in question retrieval. So we improve the term weighting model based on the traditional topic translation model and further considering the quality characteristics of question and answer pairs, this paper proposes a communitybased question retrieval method that combines question and answer on quality and question relevance (T2LM+). We have also proposed a question retrieval method based on convolutional neural networks. The results show that Compared with the relatively advanced methods, the two methods proposed in this paper increase MAP by 4.91% and 6.31%.

</details>

<details>

<summary>2019-07-03 18:58:49 - Use of OWL and Semantic Web Technologies at Pinterest</summary>

- *Rafael S. Gonçalves, Matthew Horridge, Rui Li, Yu Liu, Mark A. Musen, Csongor I. Nyulas, Evelyn Obamos, Dhananjay Shrouty, David Temple*

- `1907.02106v1` - [abs](http://arxiv.org/abs/1907.02106v1) - [pdf](http://arxiv.org/pdf/1907.02106v1)

> Pinterest is a popular Web application that has over 250 million active users. It is a visual discovery engine for finding ideas for recipes, fashion, weddings, home decoration, and much more. In the last year, the company adopted Semantic Web technologies to create a knowledge graph that aims to represent the vast amount of content and users on Pinterest, to help both content recommendation and ads targeting. In this paper, we present the engineering of an OWL ontology---the Pinterest Taxonomy---that forms the core of Pinterest's knowledge graph, the Pinterest Taste Graph. We describe modeling choices and enhancements to WebProt\'eg\'e that we used for the creation of the ontology. In two months, eight Pinterest engineers, without prior experience of OWL and WebProt\'eg\'e, revamped an existing taxonomy of noisy terms into an OWL ontology. We share our experience and present the key aspects of our work that we believe will be useful for others working in this area.

</details>

<details>

<summary>2019-07-03 22:19:37 - Analyzing the Cross-Sensor Portability of Neural Network Architectures for LiDAR-based Semantic Labeling</summary>

- *Florian Piewak, Peter Pinggera, Marius Zöllner*

- `1907.02149v1` - [abs](http://arxiv.org/abs/1907.02149v1) - [pdf](http://arxiv.org/pdf/1907.02149v1)

> State-of-the-art approaches for the semantic labeling of LiDAR point clouds heavily rely on the use of deep Convolutional Neural Networks (CNNs). However, transferring network architectures across different LiDAR sensor types represents a significant challenge, especially due to sensor specific design choices with regard to network architecture as well as data representation. In this paper we propose a new CNN architecture for the point-wise semantic labeling of LiDAR data which achieves state-of-the-art results while increasing portability across sensor types. This represents a significant advantage given the fast-paced development of LiDAR hardware technology. We perform a thorough quantitative cross-sensor analysis of semantic labeling performance in comparison to a state-of-the-art reference method. Our evaluation shows that the proposed architecture is indeed highly portable, yielding an improvement of 10 percentage points in the Intersection-over-Union (IoU) score when compared to the reference approach. Further, the results indicate that the proposed network architecture can provide an efficient way for the automated generation of large-scale training data for novel LiDAR sensor types without the need for extensive manual annotation or multi-modal label transfer.

</details>

<details>

<summary>2019-07-03 23:07:01 - Deep Learning-Based Semantic Segmentation of Microscale Objects</summary>

- *Ekta U. Samani, Wei Guo, Ashis G. Banerjee*

- `1907.03576v1` - [abs](http://arxiv.org/abs/1907.03576v1) - [pdf](http://arxiv.org/pdf/1907.03576v1)

> Accurate estimation of the positions and shapes of microscale objects is crucial for automated imaging-guided manipulation using a non-contact technique such as optical tweezers. Perception methods that use traditional computer vision algorithms tend to fail when the manipulation environments are crowded. In this paper, we present a deep learning model for semantic segmentation of the images representing such environments. Our model successfully performs segmentation with a high mean Intersection Over Union score of 0.91.

</details>

<details>

<summary>2019-07-04 02:21:44 - Learning to Generate Corrective Patches using Neural Machine Translation</summary>

- *Hideaki Hata, Emad Shihab, Graham Neubig*

- `1812.07170v2` - [abs](http://arxiv.org/abs/1812.07170v2) - [pdf](http://arxiv.org/pdf/1812.07170v2)

> Bug fixing is generally a manually-intensive task. However, recent work has proposed the idea of automated program repair, which aims to repair (at least a subset of) bugs in different ways such as code mutation, etc. Following in the same line of work as automated bug repair, in this paper we aim to leverage past fixes to propose fixes of current/future bugs. Specifically, we propose Ratchet, a corrective patch generation system using neural machine translation. By learning corresponding pre-correction and post-correction code in past fixes with a neural sequence-to-sequence model, Ratchet is able to generate a fix code for a given bug-prone code query. We perform an empirical study with five open source projects, namely Ambari, Camel, Hadoop, Jetty and Wicket, to evaluate the effectiveness of Ratchet. Our findings show that Ratchet can generate syntactically valid statements 98.7% of the time, and achieve an F1-measure between 0.29 - 0.83 with respect to the actual fixes adopted in the code base. In addition, we perform a qualitative validation using 20 participants to see whether the generated statements can be helpful in correcting bugs. Our survey showed that Ratchet's output was considered to be helpful in fixing the bugs on many occasions, even if fix was not 100% correct.

</details>

<details>

<summary>2019-07-04 05:41:18 - Attention based Convolutional Recurrent Neural Network for Environmental Sound Classification</summary>

- *Zhichao Zhang, Shugong Xu, Tianhao Qiao, Shunqing Zhang, Shan Cao*

- `1907.02230v1` - [abs](http://arxiv.org/abs/1907.02230v1) - [pdf](http://arxiv.org/pdf/1907.02230v1)

> Environmental sound classification (ESC) is a challenging problem due to the complexity of sounds. The ESC performance is heavily dependent on the effectiveness of representative features extracted from the environmental sounds. However, ESC often suffers from the semantically irrelevant frames and silent frames. In order to deal with this, we employ a frame-level attention model to focus on the semantically relevant frames and salient frames. Specifically, we first propose an convolutional recurrent neural network to learn spectro-temporal features and temporal correlations. Then, we extend our convolutional RNN model with a frame-level attention mechanism to learn discriminative feature representations for ESC. Experiments were conducted on ESC-50 and ESC-10 datasets. Experimental results demonstrated the effectiveness of the proposed method and achieved the state-of-the-art performance in terms of classification accuracy.

</details>

<details>

<summary>2019-07-04 07:10:37 - Concept Transfer Learning for Adaptive Language Understanding</summary>

- *Su Zhu, Kai Yu*

- `1706.00927v3` - [abs](http://arxiv.org/abs/1706.00927v3) - [pdf](http://arxiv.org/pdf/1706.00927v3)

> Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated. To address this issue, in this paper, a novel concept transfer learning approach is proposed. Here, substructures within literal concept definition are investigated to reveal the relationship between concepts. A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of {\em atomic concepts}. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2\&3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed method. In particular, we achieve state-of-the-art performance ($F_1$-score 96.08\%) on ATIS by only using lexicon features.

</details>

<details>

<summary>2019-07-04 14:32:39 - Morphological Word Embeddings</summary>

- *Ryan Cotterell, Hinrich Schütze*

- `1907.02423v1` - [abs](http://arxiv.org/abs/1907.02423v1) - [pdf](http://arxiv.org/pdf/1907.02423v1)

> Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semi-supervised learning, encouraging the vectors to encode a word's morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study.

</details>

<details>

<summary>2019-07-04 16:19:33 - Security Implications Of Compiler Optimizations On Cryptography -- A Review</summary>

- *A. P. Shivarpatna Venkatesh, A. Bhat Handadi, M. Mory*

- `1907.02530v1` - [abs](http://arxiv.org/abs/1907.02530v1) - [pdf](http://arxiv.org/pdf/1907.02530v1)

> When implementing secure software, developers must ensure certain requirements, such as the erasure of secret data after its use and execution in real time. Such requirements are not explicitly captured by the C language and could potentially be violated by compiler optimizations. As a result, developers typically use indirect methods to hide their code's semantics from the compiler and avoid unwanted optimizations. However, such workarounds are not permanent solutions, as increasingly efficient compiler optimization causes code that was considered secure in the past now vulnerable. This paper is a literature review of (1) the security complications caused by compiler optimizations, (2) approaches used by developers to mitigate optimization problems, and (3) recent academic efforts towards enabling security engineers to communicate implicit security requirements to the compiler. In addition, we present a short study of six cryptographic libraries and how they approach the issue of ensuring security requirements. With this paper, we highlight the need for software developers and compiler designers to work together in order to design efficient systems for writing secure software.

</details>

<details>

<summary>2019-07-05 12:29:46 - A Study of the Effect of Resolving Negation and Sentiment Analysis in Recognizing Text Entailment for Arabic</summary>

- *Fatima T. AL-Khawaldeh*

- `1907.03871v1` - [abs](http://arxiv.org/abs/1907.03871v1) - [pdf](http://arxiv.org/pdf/1907.03871v1)

> Recognizing the entailment relation showed that its influence to extract the semantic inferences in wide-ranging natural language processing domains (text summarization, question answering, etc.) and enhanced the results of their output. For Arabic language, few attempts concerns with Arabic entailment problem. This paper aims to increase the entailment accuracy for Arabic texts by resolving negation of the text-hypothesis pair and determining the polarity of the text-hypothesis pair whether it is Positive, Negative or Neutral. It is noticed that the absence of negation detection feature gives inaccurate results when detecting the entailment relation since the negation revers the truth. The negation words are considered stop words and removed from the text-hypothesis pair which may lead wrong entailment decision. Another case not solved previously, it is impossible that the positive text entails negative text and vice versa. In this paper, in order to classify the text-hypothesis pair polarity, a sentiment analysis tool is used. We show that analyzing the polarity of the text-hypothesis pair increases the entailment accuracy. to evaluate our approach we used a dataset for Arabic textual entailment (ArbTEDS) consisted of 618 text-hypothesis pairs and showed that the Arabic entailment accuracy is increased by resolving negation for entailment relation and analyzing the polarity of the text-hypothesis pair.

</details>

<details>

<summary>2019-07-05 18:08:22 - RED: A ReRAM-based Deconvolution Accelerator</summary>

- *Zichen Fan, Ziru Li, Bing Li, Yiran Chen, Hai, Li*

- `1907.02987v1` - [abs](http://arxiv.org/abs/1907.02987v1) - [pdf](http://arxiv.org/pdf/1907.02987v1)

> Deconvolution has been widespread in neural networks. For example, it is essential for performing unsupervised learning in generative adversarial networks or constructing fully convolutional networks for semantic segmentation. Resistive RAM (ReRAM)-based processing-in-memory architecture has been widely explored in accelerating convolutional computation and demonstrates good performance. Performing deconvolution on existing ReRAM-based accelerator designs, however, suffers from long latency and high energy consumption because deconvolutional computation includes not only convolution but also extra add-on operations. To realize the more efficient execution for deconvolution, we analyze its computation requirement and propose a ReRAM-based accelerator design, namely, RED. More specific, RED integrates two orthogonal methods, the pixel-wise mapping scheme for reducing redundancy caused by zero-inserting operations and the zero-skipping data flow for increasing the computation parallelism and therefore improving performance. Experimental evaluations show that compared to the state-of-the-art ReRAM-based accelerator, RED can speed up operation 3.69x~1.15x and reduce 8%~88.36% energy consumption.

</details>

<details>

<summary>2019-07-05 19:47:10 - NeuType: A Simple and Effective Neural Network Approach for Predicting Missing Entity Type Information in Knowledge Bases</summary>

- *Jon Arne Bø Hovda, Darío Garigliotti, Krisztian Balog*

- `1907.03007v1` - [abs](http://arxiv.org/abs/1907.03007v1) - [pdf](http://arxiv.org/pdf/1907.03007v1)

> Knowledge bases store information about the semantic types of entities, which can be utilized in a range of information access tasks. This information, however, is often incomplete, due to new entities emerging on a daily basis. We address the task of automatically assigning types to entities in a knowledge base from a type taxonomy. Specifically, we present two neural network architectures, which take short entity descriptions and, optionally, information about related entities as input. Using the DBpedia knowledge base for experimental evaluation, we demonstrate that these simple architectures yield significant improvements over the current state of the art.

</details>

<details>

<summary>2019-07-05 20:36:08 - Accel: A Corrective Fusion Network for Efficient Semantic Segmentation on Video</summary>

- *Samvit Jain, Xin Wang, Joseph Gonzalez*

- `1807.06667v4` - [abs](http://arxiv.org/abs/1807.06667v4) - [pdf](http://arxiv.org/pdf/1807.06667v4)

> We present Accel, a novel semantic video segmentation system that achieves high accuracy at low inference cost by combining the predictions of two network branches: (1) a reference branch that extracts high-detail features on a reference keyframe, and warps these features forward using frame-to-frame optical flow estimates, and (2) an update branch that computes features of adjustable quality on the current frame, performing a temporal update at each video frame. The modularity of the update branch, where feature subnetworks of varying layer depth can be inserted (e.g. ResNet-18 to ResNet-101), enables operation over a new, state-of-the-art accuracy-throughput trade-off spectrum. Over this curve, Accel models achieve both higher accuracy and faster inference times than the closest comparable single-frame segmentation networks. In general, Accel significantly outperforms previous work on efficient semantic video segmentation, correcting warping-related error that compounds on datasets with complex dynamics. Accel is end-to-end trainable and highly modular: the reference network, the optical flow network, and the update network can each be selected independently, depending on application requirements, and then jointly fine-tuned. The result is a robust, general system for fast, high-accuracy semantic segmentation on video.

</details>

<details>

<summary>2019-07-05 22:41:02 - BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer</summary>

- *Guan-Lin Chao, Ian Lane*

- `1907.03040v1` - [abs](http://arxiv.org/abs/1907.03040v1) - [pdf](http://arxiv.org/pdf/1907.03040v1)

> An important yet rarely tackled problem in dialogue state tracking (DST) is scalability for dynamic ontology (e.g., movie, restaurant) and unseen slot values. We focus on a specific condition, where the ontology is unknown to the state tracker, but the target slot value (except for none and dontcare), possibly unseen during training, can be found as word segment in the dialogue context. Prior approaches often rely on candidate generation from n-gram enumeration or slot tagger outputs, which can be inefficient or suffer from error propagation. We propose BERT-DST, an end-to-end dialogue state tracker which directly extracts slot values from the dialogue context. We use BERT as dialogue context encoder whose contextualized language representations are suitable for scalable DST to identify slot values from their semantic context. Furthermore, we employ encoder parameter sharing across all slots with two advantages: (1) Number of parameters does not grow linearly with the ontology. (2) Language representation knowledge can be transferred among slots. Empirical evaluation shows BERT-DST with cross-slot parameter sharing outperforms prior work on the benchmark scalable DST datasets Sim-M and Sim-R, and achieves competitive performance on the standard DSTC2 and WOZ 2.0 datasets.

</details>

<details>

<summary>2019-07-06 04:40:48 - Generating Sentences from Disentangled Syntactic and Semantic Spaces</summary>

- *Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xinyu Dai, Jiajun Chen*

- `1907.05789v1` - [abs](http://arxiv.org/abs/1907.05789v1) - [pdf](http://arxiv.org/pdf/1907.05789v1)

> Variational auto-encoders (VAEs) are widely used in natural language generation due to the regularization of the latent space. However, generating sentences from the continuous latent space does not explicitly model the syntactic information. In this paper, we propose to generate sentences from disentangled syntactic and semantic spaces. Our proposed method explicitly models syntactic information in the VAE's latent space by using the linearized tree sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntax-transfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work.

</details>

<details>

<summary>2019-07-06 08:29:21 - Qwant Research @DEFT 2019: Document matching and information retrieval using clinical cases</summary>

- *Estelle Maudet, Oralie Cattan, Maureen de Seyssel, Christophe Servan*

- `1907.05790v1` - [abs](http://arxiv.org/abs/1907.05790v1) - [pdf](http://arxiv.org/pdf/1907.05790v1)

> This paper reports on Qwant Research contribution to tasks 2 and 3 of the DEFT 2019's challenge, focusing on French clinical cases analysis. Task 2 is a task on semantic similarity between clinical cases and discussions. For this task, we propose an approach based on language models and evaluate the impact on the results of different preprocessings and matching techniques. For task 3, we have developed an information extraction system yielding very encouraging results accuracy-wise. We have experimented two different approaches, one based on the exclusive use of neural networks, the other based on a linguistic analysis.

</details>

<details>

<summary>2019-07-06 10:04:12 - Learning Neural Sequence-to-Sequence Models from Weak Feedback with Bipolar Ramp Loss</summary>

- *Laura Jehl, Carolin Lawrence, Stefan Riezler*

- `1907.03748v1` - [abs](http://arxiv.org/abs/1907.03748v1) - [pdf](http://arxiv.org/pdf/1907.03748v1)

> In many machine learning scenarios, supervision by gold labels is not available and consequently neural models cannot be trained directly by maximum likelihood estimation (MLE). In a weak supervision scenario, metric-augmented objectives can be employed to assign feedback to model outputs, which can be used to extract a supervision signal for training. We present several objectives for two separate weakly supervised tasks, machine translation and semantic parsing. We show that objectives should actively discourage negative outputs in addition to promoting a surrogate gold structure. This notion of bipolarity is naturally present in ramp loss objectives, which we adapt to neural models. We show that bipolar ramp loss objectives outperform other non-bipolar ramp loss objectives and minimum risk training (MRT) on both weakly supervised tasks, as well as on a supervised machine translation task. Additionally, we introduce a novel token-level ramp loss objective, which is able to outperform even the best sequence-level ramp loss on both weakly supervised tasks.

</details>

<details>

<summary>2019-07-06 10:45:45 - Best Practices for Learning Domain-Specific Cross-Lingual Embeddings</summary>

- *Lena Shakurova, Beata Nyari, Chao Li, Mihai Rotaru*

- `1907.03112v1` - [abs](http://arxiv.org/abs/1907.03112v1) - [pdf](http://arxiv.org/pdf/1907.03112v1)

> Cross-lingual embeddings aim to represent words in multiple languages in a shared vector space by capturing semantic similarities across languages. They are a crucial component for scaling tasks to multiple languages by transferring knowledge from languages with rich resources to low-resource languages. A common approach to learning cross-lingual embeddings is to train monolingual embeddings separately for each language and learn a linear projection from the monolingual spaces into a shared space, where the mapping relies on a small seed dictionary. While there are high-quality generic seed dictionaries and pre-trained cross-lingual embeddings available for many language pairs, there is little research on how they perform on specialised tasks. In this paper, we investigate the best practices for constructing the seed dictionary for a specific domain. We evaluate the embeddings on the sequence labelling task of Curriculum Vitae parsing and show that the size of a bilingual dictionary, the frequency of the dictionary words in the domain corpora and the source of data (task-specific vs generic) influence the performance. We also show that the less training data is available in the low-resource language, the more the construction of the bilingual dictionary matters, and demonstrate that some of the choices are crucial in the zero-shot transfer learning case.

</details>

<details>

<summary>2019-07-06 21:13:17 - I Am Not What I Write: Privacy Preserving Text Representation Learning</summary>

- *Ghazaleh Beigi, Kai Shu, Ruocheng Guo, Suhang Wang, Huan Liu*

- `1907.03189v1` - [abs](http://arxiv.org/abs/1907.03189v1) - [pdf](http://arxiv.org/pdf/1907.03189v1)

> Online users generate tremendous amounts of textual information by participating in different activities, such as writing reviews and sharing tweets. This textual data provides opportunities for researchers and business partners to study and understand individuals. However, this user-generated textual data not only can reveal the identity of the user but also may contain individual's private information (e.g., age, location, gender). Hence, "you are what you write" as the saying goes. Publishing the textual data thus compromises the privacy of individuals who provided it. The need arises for data publishers to protect people's privacy by anonymizing the data before publishing it. It is challenging to design effective anonymization techniques for textual information which minimizes the chances of re-identification and does not contain users' sensitive information (high privacy) while retaining the semantic meaning of the data for given tasks (high utility). In this paper, we study this problem and propose a novel double privacy preserving text representation learning framework, DPText, which learns a textual representation that (1) is differentially private, (2) does not contain private information and (3) retains high utility for the given task. Evaluating on two natural language processing tasks, i.e., sentiment analysis and part of speech tagging, we show the effectiveness of this approach in terms of preserving both privacy and utility.

</details>

<details>

<summary>2019-07-07 05:41:55 - Joint Lifelong Topic Model and Manifold Ranking for Document Summarization</summary>

- *Jianying Lin, Rui Liu, Quanye Jia*

- `1907.03224v1` - [abs](http://arxiv.org/abs/1907.03224v1) - [pdf](http://arxiv.org/pdf/1907.03224v1)

> Due to the manifold ranking method has a significant effect on the ranking of unknown data based on known data by using a weighted network, many researchers use the manifold ranking method to solve the document summarization task. However, their models only consider the original features but ignore the semantic features of sentences when they construct the weighted networks for the manifold ranking method. To solve this problem, we proposed two improved models based on the manifold ranking method. One is combining the topic model and manifold ranking method (JTMMR) to solve the document summarization task. This model not only uses the original feature, but also uses the semantic feature to represent the document, which can improve the accuracy of the manifold ranking method. The other one is combining the lifelong topic model and manifold ranking method (JLTMMR). On the basis of the JTMMR, this model adds the constraint of knowledge to improve the quality of the topic. At the same time, we also add the constraint of the relationship between documents to dig out a better document semantic features. The JTMMR model can improve the effect of the manifold ranking method by using the better semantic feature. Experiments show that our models can achieve a better result than other baseline models for multi-document summarization task. At the same time, our models also have a good performance on the single document summarization task. After combining with a few basic surface features, our model significantly outperforms some model based on deep learning in recent years. After that, we also do an exploring work for lifelong machine learning by analyzing the effect of adding feedback. Experiments show that the effect of adding feedback to our model is significant.

</details>

<details>

<summary>2019-07-07 06:02:05 - Graph based Neural Networks for Event Factuality Prediction using Syntactic and Semantic Structures</summary>

- *Amir Pouran Ben Veyseh, Thien Huu Nguyen, Dejing Dou*

- `1907.03227v1` - [abs](http://arxiv.org/abs/1907.03227v1) - [pdf](http://arxiv.org/pdf/1907.03227v1)

> Event factuality prediction (EFP) is the task of assessing the degree to which an event mentioned in a sentence has happened. For this task, both syntactic and semantic information are crucial to identify the important context words. The previous work for EFP has only combined these information in a simple way that cannot fully exploit their coordination. In this work, we introduce a novel graph-based neural network for EFP that can integrate the semantic and syntactic information more effectively. Our experiments demonstrate the advantage of the proposed model for EFP.

</details>

<details>

<summary>2019-07-07 06:21:47 - Improving Cross-Domain Performance for Relation Extraction via Dependency Prediction and Information Flow Control</summary>

- *Amir Pouran Ben Veyseh, Thien Huu Nguyen, Dejing Dou*

- `1907.03230v1` - [abs](http://arxiv.org/abs/1907.03230v1) - [pdf](http://arxiv.org/pdf/1907.03230v1)

> Relation Extraction (RE) is one of the fundamental tasks in Information Extraction and Natural Language Processing. Dependency trees have been shown to be a very useful source of information for this task. The current deep learning models for relation extraction has mainly exploited this dependency information by guiding their computation along the structures of the dependency trees. One potential problem with this approach is it might prevent the models from capturing important context information beyond syntactic structures and cause the poor cross-domain generalization. This paper introduces a novel method to use dependency trees in RE for deep learning models that jointly predicts dependency and semantics relations. We also propose a new mechanism to control the information flow in the model based on the input entity mentions. Our extensive experiments on benchmark datasets show that the proposed model outperforms the existing methods for RE significantly.

</details>

<details>

<summary>2019-07-07 11:43:55 - Clustering Prominent People and Organizations in Topic-Specific Text Corpora</summary>

- *Abdulkareem Alsudais, Hovig Tchalian*

- `1807.10800v2` - [abs](http://arxiv.org/abs/1807.10800v2) - [pdf](http://arxiv.org/pdf/1807.10800v2)

> Named entities in text documents are the names of people, organization, location or other types of objects in the documents that exist in the real world. A persisting research challenge is to use computational techniques to identify such entities in text documents. Once identified, several text mining tools and algorithms can be utilized to leverage these discovered named entities and improve NLP applications. In this paper, a method that clusters prominent names of people and organizations based on their semantic similarity in a text corpus is proposed. The method relies on common named entity recognition techniques and on recent word embeddings models. The semantic similarity scores generated using the word embeddings models for the named entities are used to cluster similar entities of the people and organizations types. Two human judges evaluated ten variations of the method after it was run on a corpus that consists of 4,821 articles on a specific topic. The performance of the method was measured using three quantitative measures. The results of these three metrics demonstrate that the method is effective in clustering semantically similar named entities.

</details>

<details>

<summary>2019-07-07 18:23:17 - Fast ES-RNN: A GPU Implementation of the ES-RNN Algorithm</summary>

- *Andrew Redd, Kaung Khin, Aldo Marini*

- `1907.03329v1` - [abs](http://arxiv.org/abs/1907.03329v1) - [pdf](http://arxiv.org/pdf/1907.03329v1)

> Due to their prevalence, time series forecasting is crucial in multiple domains. We seek to make state-of-the-art forecasting fast, accessible, and generalizable. ES-RNN is a hybrid between classical state space forecasting models and modern RNNs that achieved a 9.4% sMAPE improvement in the M4 competition. Crucially, ES-RNN implementation requires per-time series parameters. By vectorizing the original implementation and porting the algorithm to a GPU, we achieve up to 322x training speedup depending on batch size with similar results as those reported in the original submission. Our code can be found at: https://github.com/damitkwr/ESRNN-GPU

</details>

<details>

<summary>2019-07-08 02:16:11 - Improving Neural Relation Extraction with Implicit Mutual Relations</summary>

- *Jun Kuang, Yixin Cao, Jianbing Zheng, Xiangnan He, Ming Gao, Aoying Zhou*

- `1907.05333v1` - [abs](http://arxiv.org/abs/1907.05333v1) - [pdf](http://arxiv.org/pdf/1907.05333v1)

> Relation extraction (RE) aims at extracting the relation between two entities from the text corpora. It is a crucial task for Knowledge Graph (KG) construction. Most existing methods predict the relation between an entity pair by learning the relation from the training sentences, which contain the targeted entity pair. In contrast to existing distant supervision approaches that suffer from insufficient training corpora to extract relations, our proposal of mining implicit mutual relation from the massive unlabeled corpora transfers the semantic information of entity pairs into the RE model, which is more expressive and semantically plausible. After constructing an entity proximity graph based on the implicit mutual relations, we preserve the semantic relations of entity pairs via embedding each vertex of the graph into a low-dimensional space. As a result, we can easily and flexibly integrate the implicit mutual relations and other entity information, such as entity types, into the existing RE methods.   Our experimental results on a New York Times and another Google Distant Supervision datasets suggest that our proposed neural RE framework provides a promising improvement for the RE task, and significantly outperforms the state-of-the-art methods. Moreover, the component for mining implicit mutual relations is so flexible that can help to improve the performance of both CNN-based and RNN-based RE models significant.

</details>

<details>

<summary>2019-07-08 02:47:28 - Joint Slot Filling and Intent Detection via Capsule Neural Networks</summary>

- *Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, Philip S. Yu*

- `1812.09471v2` - [abs](http://arxiv.org/abs/1812.09471v2) - [pdf](http://arxiv.org/pdf/1812.09471v2)

> Being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding. The existing works either treat slot filling and intent detection separately in a pipeline manner, or adopt joint models which sequentially label slots while summarizing the utterance-level intent without explicitly preserving the hierarchical relationship among words, slots, and intents. To exploit the semantic hierarchy for effective modeling, we propose a capsule-based neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. A re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation. Experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures, as well as existing natural language understanding services.

</details>

<details>

<summary>2019-07-08 07:38:57 - parboiled2: a macro-based approach for effective generators of parsing expressions grammars in Scala</summary>

- *Alexander A. Myltsev*

- `1907.03436v1` - [abs](http://arxiv.org/abs/1907.03436v1) - [pdf](http://arxiv.org/pdf/1907.03436v1)

> In today's computerized world, parsing is ubiquitous. Developers parse logs, queries to databases and websites, programming and natural languages. When Java ecosystem maturity, concise syntax, and runtime speed matters, developers choose parboiled2 that generates grammars for parsing expression grammars (PEG). The following open source libraries have chosen parboiled2 for parsing facilities: - akka-http is the Streaming-first HTTP server/module of Lightbend Akka - Sangria is a Scala GraphQL implementation - http4s is a minimal, idiomatic Scala interface for HTTP - cornichon is Scala DSL for testing HTTP JSON API - scala-uri is a simple Scala library for building and parsing URIs The library uses a wide range of Scala facilities to provide required functionality. We also discuss the extensions to PEGs. In particular, we show the implementation of an internal Scala DSL that features intuitive syntax and semantics. We demonstrate how parboiled2 extensively uses Scala typing to verify DSL integrity. We also show the connections to inner structures of parboiled2, which can give the developer a better understanding of how to compose more effective grammars. Finally, we expose how a grammar is expanded with Scala Macros to an effective runtime code.

</details>

<details>

<summary>2019-07-08 14:57:07 - Do Transformer Attention Heads Provide Transparency in Abstractive Summarization?</summary>

- *Joris Baan, Maartje ter Hoeve, Marlies van der Wees, Anne Schuth, Maarten de Rijke*

- `1907.00570v2` - [abs](http://arxiv.org/abs/1907.00570v2) - [pdf](http://arxiv.org/pdf/1907.00570v2)

> Learning algorithms become more powerful, often at the cost of increased complexity. In response, the demand for algorithms to be transparent is growing. In NLP tasks, attention distributions learned by attention-based deep learning models are used to gain insights in the models' behavior. To which extent is this perspective valid for all NLP tasks? We investigate whether distributions calculated by different attention heads in a transformer architecture can be used to improve transparency in the task of abstractive summarization. To this end, we present both a qualitative and quantitative analysis to investigate the behavior of the attention heads. We show that some attention heads indeed specialize towards syntactically and semantically distinct input. We propose an approach to evaluate to which extent the Transformer model relies on specifically learned attention distributions. We also discuss what this implies for using attention distributions as a means of transparency.

</details>

<details>

<summary>2019-07-08 18:29:58 - XFake: Explainable Fake News Detector with Visualizations</summary>

- *Fan Yang, Shiva K. Pentyala, Sina Mohseni, Mengnan Du, Hao Yuan, Rhema Linder, Eric D. Ragan, Shuiwang Ji, Xia Hu*

- `1907.07757v1` - [abs](http://arxiv.org/abs/1907.07757v1) - [pdf](http://arxiv.org/pdf/1907.07757v1)

> In this demo paper, we present the XFake system, an explainable fake news detector that assists end-users to identify news credibility. To effectively detect and interpret the fakeness of news items, we jointly consider both attributes (e.g., speaker) and statements. Specifically, MIMIC, ATTN and PERT frameworks are designed, where MIMIC is built for attribute analysis, ATTN is for statement semantic analysis and PERT is for statement linguistic analysis. Beyond the explanations extracted from the designed frameworks, relevant supporting examples as well as visualization are further provided to facilitate the interpretation. Our implemented system is demonstrated on a real-world dataset crawled from PolitiFact, where thousands of verified political news have been collected.

</details>

<details>

<summary>2019-07-08 19:03:09 - Leveraging Semantics for Incremental Learning in Multi-Relational Embeddings</summary>

- *Angel Daruna, Weiyu Liu, Zsolt Kira, Sonia Chernova*

- `1905.12181v2` - [abs](http://arxiv.org/abs/1905.12181v2) - [pdf](http://arxiv.org/pdf/1905.12181v2)

> Service robots benefit from encoding information in semantically meaningful ways to enable more robust task execution. Prior work has shown multi-relational embeddings can encode semantic knowledge graphs to promote generalizability and scalability, but only within a batched learning paradigm. We present Incremental Semantic Initialization (ISI), an incremental learning approach that enables novel semantic concepts to be initialized in the embedding in relation to previously learned embeddings of semantically similar concepts. We evaluate ISI on mined AI2Thor and MatterPort3D datasets; our experiments show that on average ISI improves immediate query performance by 41.4%. Additionally, ISI methods on average reduced the number of epochs required to approach model convergence by 78.2%.

</details>

<details>

<summary>2019-07-08 20:56:27 - Annotary: A Concolic Execution System for Developing Secure Smart Contracts</summary>

- *Konrad Weiss, Julian Schütte*

- `1907.03868v1` - [abs](http://arxiv.org/abs/1907.03868v1) - [pdf](http://arxiv.org/pdf/1907.03868v1)

> Ethereum smart contracts are executable programs, deployed on a peer-to-peer network and executed in a consensus-based fashion. Their bytecode is public, immutable and once deployed to the blockchain, cannot be patched anymore. As smart contracts may hold Ether worth of several million dollars, they are attractive targets for attackers and indeed some contracts have successfully been exploited in the recent past, resulting in tremendous financial losses. The correctness of smart contracts is thus of utmost importance. While first approaches on formal verification exist, they demand users to be well-versed in formal methods which are alien to many developers and are only able to analyze individual contracts, without considering their execution environment, i.e., calls to external contracts, sequences of transaction, and values from the actual blockchain storage. In this paper, we present Annotary, a concolic execution framework to analyze smart contracts for vulnerabilities, supported by annotations which developers write directly in the Solidity source code. In contrast to existing work, Annotary supports analysis of inter-transactional, inter-contract control flows and combines symbolic execution of EVM bytecode with a resolution of concrete values from the public Ethereum blockchain. While the analysis of Annotary tends to weight precision higher than soundness, we analyze inter-transactional call chains to eliminate false positives from unreachable states that traditional symbolic execution would not be able to handle. We present the annotation and analysis concepts of Annotary, explain its implementation on top of the Laser symbolic virtual machine, and demonstrate its usage as a plugin for the Sublime Text editor.

</details>

<details>

<summary>2019-07-08 21:39:29 - An Intrinsic Nearest Neighbor Analysis of Neural Machine Translation Architectures</summary>

- *Hamidreza Ghader, Christof Monz*

- `1907.03885v1` - [abs](http://arxiv.org/abs/1907.03885v1) - [pdf](http://arxiv.org/pdf/1907.03885v1)

> Earlier approaches indirectly studied the information captured by the hidden states of recurrent and non-recurrent neural machine translation models by feeding them into different classifiers. In this paper, we look at the encoder hidden states of both transformer and recurrent machine translation models from the nearest neighbors perspective. We investigate to what extent the nearest neighbors share information with the underlying word embeddings as well as related WordNet entries. Additionally, we study the underlying syntactic structure of the nearest neighbors to shed light on the role of syntactic similarities in bringing the neighbors together. We compare transformer and recurrent models in a more intrinsic way in terms of capturing lexical semantics and syntactic structures, in contrast to extrinsic approaches used by previous works. In agreement with the extrinsic evaluations in the earlier works, our experimental results show that transformers are superior in capturing lexical semantics, but not necessarily better in capturing the underlying syntax. Additionally, we show that the backward recurrent layer in a recurrent model learns more about the semantics of words, whereas the forward recurrent layer encodes more context.

</details>

<details>

<summary>2019-07-09 04:39:24 - Neural or Statistical: An Empirical Study on Language Models for Chinese Input Recommendation on Mobile</summary>

- *Hainan Zhang, Yanyan Lan, Jiafeng Guo, Jun Xu, Xueqi Cheng*

- `1907.05340v1` - [abs](http://arxiv.org/abs/1907.05340v1) - [pdf](http://arxiv.org/pdf/1907.05340v1)

> Chinese input recommendation plays an important role in alleviating human cost in typing Chinese words, especially in the scenario of mobile applications. The fundamental problem is to predict the conditional probability of the next word given the sequence of previous words. Therefore, statistical language models, i.e.~n-grams based models, have been extensively used on this task in real application. However, the characteristics of extremely different typing behaviors usually lead to serious sparsity problem, even n-gram with smoothing will fail. A reasonable approach to tackle this problem is to use the recently proposed neural models, such as probabilistic neural language model, recurrent neural network and word2vec. They can leverage more semantically similar words for estimating the probability. However, there is no conclusion on which approach of the two will work better in real application. In this paper, we conduct an extensive empirical study to show the differences between statistical and neural language models. The experimental results show that the two different approach have individual advantages, and a hybrid approach will bring a significant improvement.

</details>

<details>

<summary>2019-07-09 04:46:31 - To Tune or Not To Tune? How About the Best of Both Worlds?</summary>

- *Ran Wang, Haibo Su, Chunye Wang, Kailin Ji, Jupeng Ding*

- `1907.05338v1` - [abs](http://arxiv.org/abs/1907.05338v1) - [pdf](http://arxiv.org/pdf/1907.05338v1)

> The introduction of pre-trained language models has revolutionized natural language research communities. However, researchers still know relatively little regarding their theoretical and empirical properties. In this regard, Peters et al. perform several experiments which demonstrate that it is better to adapt BERT with a light-weight task-specific head, rather than building a complex one on top of the pre-trained language model, and freeze the parameters in the said language model. However, there is another option to adopt. In this paper, we propose a new adaptation method which we first train the task model with the BERT parameters frozen and then fine-tune the entire model together. Our experimental results show that our model adaptation method can achieve 4.7% accuracy improvement in semantic similarity task, 0.99% accuracy improvement in sequence labeling task and 0.72% accuracy improvement in the text classification task.

</details>

<details>

<summary>2019-07-09 12:01:35 - On the Semantic Interpretability of Artificial Intelligence Models</summary>

- *Vivian S. Silva, André Freitas, Siegfried Handschuh*

- `1907.04105v1` - [abs](http://arxiv.org/abs/1907.04105v1) - [pdf](http://arxiv.org/pdf/1907.04105v1)

> Artificial Intelligence models are becoming increasingly more powerful and accurate, supporting or even replacing humans' decision making. But with increased power and accuracy also comes higher complexity, making it hard for users to understand how the model works and what the reasons behind its predictions are. Humans must explain and justify their decisions, and so do the AI models supporting them in this process, making semantic interpretability an emerging field of study. In this work, we look at interpretability from a broader point of view, going beyond the machine learning scope and covering different AI fields such as distributional semantics and fuzzy logic, among others. We examine and classify the models according to their nature and also based on how they introduce interpretability features, analyzing how each approach affects the final users and pointing to gaps that still need to be addressed to provide more human-centered interpretability solutions.

</details>

<details>

<summary>2019-07-09 16:47:50 - UW-BHI at MEDIQA 2019: An Analysis of Representation Methods for Medical Natural Language Inference</summary>

- *William R. Kearns, Wilson Lau, Jason A. Thomas*

- `1907.04286v1` - [abs](http://arxiv.org/abs/1907.04286v1) - [pdf](http://arxiv.org/pdf/1907.04286v1)

> Recent advances in distributed language modeling have led to large performance increases on a variety of natural language processing (NLP) tasks. However, it is not well understood how these methods may be augmented by knowledge-based approaches. This paper compares the performance and internal representation of an Enhanced Sequential Inference Model (ESIM) between three experimental conditions based on the representation method: Bidirectional Encoder Representations from Transformers (BERT), Embeddings of Semantic Predications (ESP), or Cui2Vec. The methods were evaluated on the Medical Natural Language Inference (MedNLI) subtask of the MEDIQA 2019 shared task. This task relied heavily on semantic understanding and thus served as a suitable evaluation set for the comparison of these representation methods.

</details>

<details>

<summary>2019-07-09 17:46:17 - Multilingual Universal Sentence Encoder for Semantic Retrieval</summary>

- *Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil*

- `1907.04307v1` - [abs](http://arxiv.org/abs/1907.04307v1) - [pdf](http://arxiv.org/pdf/1907.04307v1)

> We introduce two pre-trained retrieval focused multilingual sentence encoding models, respectively based on the Transformer and CNN model architectures. The models embed text from 16 languages into a single semantic space using a multi-task trained dual-encoder that learns tied representations using translation based bridge tasks (Chidambaram al., 2018). The models provide performance that is competitive with the state-of-the-art on: semantic retrieval (SR), translation pair bitext retrieval (BR) and retrieval question answering (ReQA). On English transfer learning tasks, our sentence-level embeddings approach, and in some cases exceed, the performance of monolingual, English only, sentence embedding models. Our models are made available for download on TensorFlow Hub.

</details>

<details>

<summary>2019-07-09 18:23:32 - Transfer Learning from Audio-Visual Grounding to Speech Recognition</summary>

- *Wei-Ning Hsu, David Harwath, James Glass*

- `1907.04355v1` - [abs](http://arxiv.org/abs/1907.04355v1) - [pdf](http://arxiv.org/pdf/1907.04355v1)

> Transfer learning aims to reduce the amount of data required to excel at a new task by re-using the knowledge acquired from learning other related tasks. This paper proposes a novel transfer learning scenario, which distills robust phonetic features from grounding models that are trained to tell whether a pair of image and speech are semantically correlated, without using any textual transcripts. As semantics of speech are largely determined by its lexical content, grounding models learn to preserve phonetic information while disregarding uncorrelated factors, such as speaker and channel. To study the properties of features distilled from different layers, we use them as input separately to train multiple speech recognition models. Empirical results demonstrate that layers closer to input retain more phonetic information, while following layers exhibit greater invariance to domain shift. Moreover, while most previous studies include training data for speech recognition for feature extractor training, our grounding models are not trained on any of those data, indicating more universal applicability to new domains.

</details>

<details>

<summary>2019-07-09 19:29:43 - Modeling the Role of Context Dependency in the Recognition and Manifestation of Entrepreneurial Opportunity</summary>

- *Murad A. Mithani, Tomas Veloz, Liane Gabora*

- `1309.4744v4` - [abs](http://arxiv.org/abs/1309.4744v4) - [pdf](http://arxiv.org/pdf/1309.4744v4)

> The paper uses the SCOP theory of concepts to model the role of environmental context on three levels of entrepreneurial opportunity: idea generation, idea development, and entrepreneurial decision. The role of contextual-fit in the generation and development of ideas is modeled as the collapse of their superposition state into one of the potential states that composes this superposition. The projection of this collapsed state on the socio-economic basis results in interference of the developed idea with the perceptions of the supporting community, undergoing an eventual collapse for an entrepreneurial decision that reflects the shared vision of its stakeholders. The developed idea may continue to evolve due to continuous or discontinuous changes in the environment. The model offers unique insights into the effects of external influences on entrepreneurial decisions.

</details>

<details>

<summary>2019-07-09 20:04:24 - On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference</summary>

- *Yonatan Belinkov, Adam Poliak, Stuart M. Shieber, Benjamin Van Durme, Alexander M. Rush*

- `1907.04389v1` - [abs](http://arxiv.org/abs/1907.04389v1) - [pdf](http://arxiv.org/pdf/1907.04389v1)

> Popular Natural Language Inference (NLI) datasets have been shown to be tainted by hypothesis-only biases. Adversarial learning may help models ignore sensitive biases and spurious correlations in data. We evaluate whether adversarial learning can be used in NLI to encourage models to learn representations free of hypothesis-only biases. Our analyses indicate that the representations learned via adversarial learning may be less biased, with only small drops in NLI accuracy.

</details>

<details>

<summary>2019-07-10 08:16:27 - Deep Multi Label Classification in Affine Subspaces</summary>

- *Thomas Kurmann, Pablo Marquez Neila, Sebastian Wolf, Raphael Sznitman*

- `1907.04563v1` - [abs](http://arxiv.org/abs/1907.04563v1) - [pdf](http://arxiv.org/pdf/1907.04563v1)

> Multi-label classification (MLC) problems are becoming increasingly popular in the context of medical imaging. This has in part been driven by the fact that acquiring annotations for MLC is far less burdensome than for semantic segmentation and yet provides more expressiveness than multi-class classification. However, to train MLCs, most methods have resorted to similar objective functions as with traditional multi-class classification settings. We show in this work that such approaches are not optimal and instead propose a novel deep MLC classification method in affine subspace. At its core, the method attempts to pull features of class-labels towards different affine subspaces while maximizing the distance between them. We evaluate the method using two MLC medical imaging datasets and show a large performance increase compared to previous multi-label frameworks. This method can be seen as a plug-in replacement loss function and is trainable in an end-to-end fashion.

</details>

<details>

<summary>2019-07-10 10:50:22 - Neural Networks as Explicit Word-Based Rules</summary>

- *Jindřich Libovický*

- `1907.04613v1` - [abs](http://arxiv.org/abs/1907.04613v1) - [pdf](http://arxiv.org/pdf/1907.04613v1)

> Filters of convolutional networks used in computer vision are often visualized as image patches that maximize the response of the filter. We use the same approach to interpret weight matrices in simple architectures for natural language processing tasks. We interpret a convolutional network for sentiment classification as word-based rules. Using the rule, we recover the performance of the original model.

</details>

<details>

<summary>2019-07-10 12:09:55 - Extraction of digital wavefront sets using applied harmonic analysis and deep neural networks</summary>

- *Héctor Andrade-Loarca, Gitta Kutyniok, Ozan Öktem, Philipp Petersen*

- `1901.01388v2` - [abs](http://arxiv.org/abs/1901.01388v2) - [pdf](http://arxiv.org/pdf/1901.01388v2)

> Microlocal analysis provides deep insight into singularity structures and is often crucial for solving inverse problems, predominately, in imaging sciences. Of particular importance is the analysis of wavefront sets and the correct extraction of those. In this paper, we introduce the first algorithmic approach to extract the wavefront set of images, which combines data-based and model-based methods. Based on a celebrated property of the shearlet transform to unravel information on the wavefront set, we extract the wavefront set of an image by first applying a discrete shearlet transform and then feeding local patches of this transform to a deep convolutional neural network trained on labeled data. The resulting algorithm outperforms all competing algorithms in edge-orientation and ramp-orientation detection.

</details>

<details>

<summary>2019-07-10 14:20:43 - Modeling Semantic Compositionality with Sememe Knowledge</summary>

- *Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen, Qun Liu, Maosong Sun*

- `1907.04744v1` - [abs](http://arxiv.org/abs/1907.04744v1) - [pdf](http://arxiv.org/pdf/1907.04744v1)

> Semantic compositionality (SC) refers to the phenomenon that the meaning of a complex linguistic unit can be composed of the meanings of its constituents. Most related works focus on using complicated compositionality functions to model SC while few works consider external knowledge in models. In this paper, we verify the effectiveness of sememes, the minimum semantic units of human languages, in modeling SC by a confirmatory experiment. Furthermore, we make the first attempt to incorporate sememe knowledge into SC models, and employ the sememeincorporated models in learning representations of multiword expressions, a typical task of SC. In experiments, we implement our models by incorporating knowledge from a famous sememe knowledge base HowNet and perform both intrinsic and extrinsic evaluations. Experimental results show that our models achieve significant performance boost as compared to the baseline methods without considering sememe knowledge. We further conduct quantitative analysis and case studies to demonstrate the effectiveness of applying sememe knowledge in modeling SC. All the code and data of this paper can be obtained on https://github.com/thunlp/Sememe-SC.

</details>

<details>

<summary>2019-07-11 03:52:13 - Incremental Semantic Mapping with Unsupervised On-line Learning</summary>

- *Ygor C. N. Sousa, Hansenclever F. Bassani*

- `1907.04001v2` - [abs](http://arxiv.org/abs/1907.04001v2) - [pdf](http://arxiv.org/pdf/1907.04001v2)

> This paper introduces an incremental semantic mapping approach, with on-line unsupervised learning, based on Self-Organizing Maps (SOM) for robotic agents. The method includes a mapping module, which incrementally creates a topological map of the environment, enriched with objects recognized around each topological node, and a module of places categorization, endowed with an incremental unsupervised learning SOM with on-line training. The proposed approach was tested in experiments with real-world data, in which it demonstrates promising capabilities of incremental acquisition of topological maps enriched with semantic information, and for clustering together similar places based on this information. The approach was also able to continue learning from newly visited environments without degrading the information previously learned.

</details>

<details>

<summary>2019-07-11 07:34:03 - DeepIlluminance: Contextual Illuminance Estimation via Deep Neural Networks</summary>

- *Jun Zhang, Tong Zheng, Shengping Zhang, Meng Wang*

- `1905.04791v2` - [abs](http://arxiv.org/abs/1905.04791v2) - [pdf](http://arxiv.org/pdf/1905.04791v2)

> Computational color constancy refers to the estimation of the scene illumination and makes the perceived color relatively stable under varying illumination. In the past few years, deep Convolutional Neural Networks (CNNs) have delivered superior performance in illuminant estimation. Several representative methods formulate it as a multi-label prediction problem by learning the local appearance of image patches using CNNs. However, these approaches inevitably make incorrect estimations for the ambiguous patches affected by their neighborhood contexts. Inaccurate local estimates are likely to bring in degraded performance when combining into a global prediction. To address the above issues, we propose a contextual deep network for patch-based illuminant estimation equipped with refinement. First, the contextual net with a center-surround architecture extracts local contextual features from image patches, and generates initial illuminant estimates and the corresponding color corrected patches. The patches are sampled based on the observation that pixels with large color differences describe the illumination well. Then, the refinement net integrates the input patches with the corrected patches in conjunction with the use of intermediate features to improve the performance. To train such a network with numerous parameters, we propose a stage-wise training strategy, in which the features and the predicted illuminant from previous stages are provided to the next learning stage with more finer estimates recovered. Experiments show that our approach obtains competitive performance on two illuminant estimation benchmarks.

</details>

<details>

<summary>2019-07-11 08:41:53 - No Word is an Island -- A Transformation Weighting Model for Semantic Composition</summary>

- *Corina Dima, Daniël de Kok, Neele Witte, Erhard Hinrichs*

- `1907.05048v1` - [abs](http://arxiv.org/abs/1907.05048v1) - [pdf](http://arxiv.org/pdf/1907.05048v1)

> Composition models of distributional semantics are used to construct phrase representations from the representations of their words. Composition models are typically situated on two ends of a spectrum. They either have a small number of parameters but compose all phrases in the same way, or they perform word-specific compositions at the cost of a far larger number of parameters. In this paper we propose transformation weighting (TransWeight), a composition model that consistently outperforms existing models on nominal compounds, adjective-noun phrases and adverb-adjective phrases in English, German and Dutch. TransWeight drastically reduces the number of parameters needed compared to the best model in the literature by composing similar words in the same way.

</details>

<details>

<summary>2019-07-11 08:55:21 - Wasserstein-Fisher-Rao Document Distance</summary>

- *Zihao Wang, Datong Zhou, Yong Zhang, Hao Wu, Chenglong Bao*

- `1904.10294v2` - [abs](http://arxiv.org/abs/1904.10294v2) - [pdf](http://arxiv.org/pdf/1904.10294v2)

> As a fundamental problem of natural language processing, it is important to measure the distance between different documents. Among the existing methods, the Word Mover's Distance (WMD) has shown remarkable success in document semantic matching for its clear physical insight as a parameter-free model. However, WMD is essentially based on the classical Wasserstein metric, thus it often fails to robustly represent the semantic similarity between texts of different lengths. In this paper, we apply the newly developed Wasserstein-Fisher-Rao (WFR) metric from unbalanced optimal transport theory to measure the distance between different documents. The proposed WFR document distance maintains the great interpretability and simplicity as WMD. We demonstrate that the WFR document distance has significant advantages when comparing the texts of different lengths. In addition, an accelerated Sinkhorn based algorithm with GPU implementation has been developed for the fast computation of WFR distances. The KNN classification results on eight datasets have shown its clear improvement over WMD.

</details>

<details>

<summary>2019-07-11 10:06:20 - MeetUp! A Corpus of Joint Activity Dialogues in a Visual Environment</summary>

- *Nikolai Ilinykh, Sina Zarrieß, David Schlangen*

- `1907.05084v1` - [abs](http://arxiv.org/abs/1907.05084v1) - [pdf](http://arxiv.org/pdf/1907.05084v1)

> Building computer systems that can converse about their visual environment is one of the oldest concerns of research in Artificial Intelligence and Computational Linguistics (see, for example, Winograd's 1972 SHRDLU system). Only recently, however, have methods from computer vision and natural language processing become powerful enough to make this vision seem more attainable. Pushed especially by developments in computer vision, many data sets and collection environments have recently been published that bring together verbal interaction and visual processing. Here, we argue that these datasets tend to oversimplify the dialogue part, and we propose a task---MeetUp!---that requires both visual and conversational grounding, and that makes stronger demands on representations of the discourse. MeetUp! is a two-player coordination game where players move in a visual environment, with the objective of finding each other. To do so, they must talk about what they see, and achieve mutual understanding. We describe a data collection and show that the resulting dialogues indeed exhibit the dialogue phenomena of interest, while also challenging the language & vision aspect.

</details>

<details>

<summary>2019-07-11 12:27:55 - Learning Blended, Precise Semantic Program Embeddings</summary>

- *Ke Wang, Zhendong Su*

- `1907.02136v2` - [abs](http://arxiv.org/abs/1907.02136v2) - [pdf](http://arxiv.org/pdf/1907.02136v2)

> Learning neural program embeddings is key to utilizing deep neural networks in program languages research --- precise and efficient program representations enable the application of deep models to a wide range of program analysis tasks. Existing approaches predominately learn to embed programs from their source code, and, as a result, they do not capture deep, precise program semantics. On the other hand, models learned from runtime information critically depend on the quality of program executions, thus leading to trained models with highly variant quality. This paper tackles these inherent weaknesses of prior approaches by introducing a new deep neural network, \liger, which learns program representations from a mixture of symbolic and concrete execution traces. We have evaluated \liger on \coset, a recently proposed benchmark suite for evaluating neural program embeddings. Results show \liger (1) is significantly more accurate than the state-of-the-art syntax-based models Gated Graph Neural Network and code2vec in classifying program semantics, and (2) requires on average 10x fewer executions covering 74\% fewer paths than the state-of-the-art dynamic model \dypro. Furthermore, we extend \liger to predict the name for a method from its body's vector representation. Learning on the same set of functions (more than 170K in total), \liger significantly outperforms code2seq, the previous state-of-the-art for method name prediction.

</details>

<details>

<summary>2019-07-11 12:31:30 - Deep Active Learning for Axon-Myelin Segmentation on Histology Data</summary>

- *Melanie Lubrano di Scandalea, Christian S. Perone, Mathieu Boudreau, Julien Cohen-Adad*

- `1907.05143v1` - [abs](http://arxiv.org/abs/1907.05143v1) - [pdf](http://arxiv.org/pdf/1907.05143v1)

> Semantic segmentation is a crucial task in biomedical image processing, which recent breakthroughs in deep learning have allowed to improve. However, deep learning methods in general are not yet widely used in practice since they require large amount of data for training complex models. This is particularly challenging for biomedical images, because data and ground truths are a scarce resource. Annotation efforts for biomedical images come with a real cost, since experts have to manually label images at pixel-level on samples usually containing many instances of the target anatomy (e.g. in histology samples: neurons, astrocytes, mitochondria, etc.). In this paper we provide a framework for Deep Active Learning applied to a real-world scenario. Our framework relies on the U-Net architecture and overall uncertainty measure to suggest which sample to annotate. It takes advantage of the uncertainty measure obtained by taking Monte Carlo samples while using Dropout regularization scheme. Experiments were done on spinal cord and brain microscopic histology samples to perform a myelin segmentation task. Two realistic small datasets of 14 and 24 images were used, from different acquisition settings (Serial Block-Face Electron Microscopy and Transmitting Electron Microscopy) and showed that our method reached a maximum Dice value after adding 3 uncertainty-selected samples to the initial training set, versus 15 randomly-selected samples, thereby significantly reducing the annotation effort. We focused on a plausible scenario and showed evidence that this straightforward implementation achieves a high segmentation performance with very few labelled samples. We believe our framework may benefit any biomedical researcher willing to obtain fast and accurate image segmentation on their own dataset. The code is freely available at https://github.com/neuropoly/deep-active-learning.

</details>

<details>

<summary>2019-07-12 02:45:08 - Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification</summary>

- *Xin Huang, Boli Chen, Lin Xiao, Liping Jing*

- `1905.10070v2` - [abs](http://arxiv.org/abs/1905.10070v2) - [pdf](http://arxiv.org/pdf/1905.10070v2)

> Extreme multi-label text classification (XMTC) aims at tagging a document with most relevant labels from an extremely large-scale label set. It is a challenging problem especially for the tail labels because there are only few training documents to build classifier. This paper is motivated to better explore the semantic relationship between each document and extreme labels by taking advantage of both document content and label correlation. Our objective is to establish an explicit label-aware representation for each document with a hybrid attention deep neural network model(LAHA). LAHA consists of three parts. The first part adopts a multi-label self-attention mechanism to detect the contribution of each word to labels. The second part exploits the label structure and document content to determine the semantic connection between words and labels in a same latent space. An adaptive fusion strategy is designed in the third part to obtain the final label-aware document representation so that the essence of previous two parts can be sufficiently integrated. Extensive experiments have been conducted on six benchmark datasets by comparing with the state-of-the-art methods. The results show the superiority of our proposed LAHA method, especially on the tail labels.

</details>

<details>

<summary>2019-07-12 08:35:40 - iFixR: Bug Report driven Program Repair</summary>

- *Anil Koyuncu, Kui Liu, Tegawendé F. Bissyandé, Dongsun Kim, Martin Monperrus, Jacques Klein, Yves Le Traon*

- `1907.05620v1` - [abs](http://arxiv.org/abs/1907.05620v1) - [pdf](http://arxiv.org/pdf/1907.05620v1)

> Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-and-validate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefully-reorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).

</details>

<details>

<summary>2019-07-12 13:37:46 - Gated-SCNN: Gated Shape CNNs for Semantic Segmentation</summary>

- *Towaki Takikawa, David Acuna, Varun Jampani, Sanja Fidler*

- `1907.05740v1` - [abs](http://arxiv.org/abs/1907.05740v1) - [pdf](http://arxiv.org/pdf/1907.05740v1)

> Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. Here, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch, i.e. shape stream, that processes information in parallel to the classical stream. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.

</details>

<details>

<summary>2019-07-13 10:12:10 - Compositional Semantic Parsing Across Graphbanks</summary>

- *Matthias Lindemann, Jonas Groschwitz, Alexander Koller*

- `1906.11746v2` - [abs](http://arxiv.org/abs/1906.11746v2) - [pdf](http://arxiv.org/pdf/1906.11746v2)

> Most semantic parsers that map sentences to graph-based meaning representations are hand-designed for specific graphbanks. We present a compositional neural semantic parser which achieves, for the first time, competitive accuracies across a diverse range of graphbanks. Incorporating BERT embeddings and multi-task learning improves the accuracy further, setting new states of the art on DM, PAS, PSD, AMR 2015 and EDS.

</details>

<details>

<summary>2019-07-13 12:17:00 - Image Evolution Trajectory Prediction and Classification from Baseline using Learning-based Patch Atlas Selection for Early Diagnosis</summary>

- *Can Gafuroglu, Islem Rekik*

- `1907.06064v1` - [abs](http://arxiv.org/abs/1907.06064v1) - [pdf](http://arxiv.org/pdf/1907.06064v1)

> Patients initially diagnosed with early mild cognitive impairment (eMCI) are known to be a clinically heterogeneous group with very subtle patterns of brain atrophy. To examine the boarders between normal controls (NC) and eMCI, Magnetic Resonance Imaging (MRI) was extensively used as a non-invasive imaging modality to pin-down subtle changes in brain images of MCI patients. However, eMCI research remains limited by the number of available MRI acquisition timepoints. Ideally, one would learn how to diagnose MCI patients in an early stage from MRI data acquired at a single timepoint, while leveraging 'non-existing' follow-up observations. To this aim, we propose novel supervised and unsupervised frameworks that learn how to jointly predict and label the evolution trajectory of intensity patches, each seeded at a specific brain landmark, from a baseline intensity patch. Specifically, both strategies aim to identify the best training atlas patches at baseline timepoint to predict and classify the evolution trajectory of a given testing baseline patch. The supervised technique learns how to select the best atlas patches by training bidirectional mappings from the space of pairwise patch similarities to their corresponding prediction errors -when one patch was used to predict the other. On the other hand, the unsupervised technique learns a manifold of baseline atlas and testing patches using multiple kernels to well capture patch distributions at multiple scales. Once the best baseline atlas patches are selected, we retrieve their evolution trajectories and average them to predict the evolution trajectory of the testing baseline patch. Next, we input the predicted trajectories to an ensemble of linear classifiers, each trained at a specific landmark. Our classification accuracy increased by up to 10% points in comparison to single timepoint-based classification methods.

</details>

<details>

<summary>2019-07-13 21:52:48 - The Materials Science Procedural Text Corpus: Annotating Materials Synthesis Procedures with Shallow Semantic Structures</summary>

- *Sheshera Mysore, Zach Jensen, Edward Kim, Kevin Huang, Haw-Shiuan Chang, Emma Strubell, Jeffrey Flanigan, Andrew McCallum, Elsa Olivetti*

- `1905.06939v2` - [abs](http://arxiv.org/abs/1905.06939v2) - [pdf](http://arxiv.org/pdf/1905.06939v2)

> Materials science literature contains millions of materials synthesis procedures described in unstructured natural language text. Large-scale analysis of these synthesis procedures would facilitate deeper scientific understanding of materials synthesis and enable automated synthesis planning. Such analysis requires extracting structured representations of synthesis procedures from the raw text as a first step. To facilitate the training and evaluation of synthesis extraction models, we introduce a dataset of 230 synthesis procedures annotated by domain experts with labeled graphs that express the semantics of the synthesis sentences. The nodes in this graph are synthesis operations and their typed arguments, and labeled edges specify relations between the nodes. We describe this new resource in detail and highlight some specific challenges to annotating scientific text with shallow semantic structure. We make the corpus available to the community to promote further research and development of scientific information extraction systems.

</details>

<details>

<summary>2019-07-13 22:48:31 - Tackling Graphical NLP problems with Graph Recurrent Networks</summary>

- *Linfeng Song*

- `1907.06142v1` - [abs](http://arxiv.org/abs/1907.06142v1) - [pdf](http://arxiv.org/pdf/1907.06142v1)

> How to properly model graphs is a long-existing and important problem in NLP area, where several popular types of graphs are knowledge graphs, semantic graphs and dependency graphs. Comparing with other data structures, such as sequences and trees, graphs are generally more powerful in representing complex correlations among entities. For example, a knowledge graph stores real-word entities (such as "Barack_Obama" and "U.S.") and their relations (such as "live_in" and "lead_by"). Properly encoding a knowledge graph is beneficial to user applications, such as question answering and knowledge discovery. Modeling graphs is also very challenging, probably because graphs usually contain massive and cyclic relations.   Recent years have witnessed the success of deep learning, especially RNN-based models, on many NLP problems. Besides, RNNs and their variations have been extensively studied on several graph problems and showed preliminary successes. Despite the successes that have been achieved, RNN-based models suffer from several major drawbacks on graphs. First, they can only consume sequential data, thus linearization is required to serialize input graphs, resulting in the loss of important structural information. Second, the serialization results are usually very long, so it takes a long time for RNNs to encode them.   In this thesis, we propose a novel graph neural network, named graph recurrent network (GRN). We study our GRN model on 4 very different tasks, such as machine reading comprehension, relation extraction and machine translation. Some take undirected graphs without edge labels, while the others have directed ones with edge labels. To consider these important differences, we gradually enhance our GRN model, such as further considering edge labels and adding an RNN decoder. Carefully designed experiments show the effectiveness of GRN on all these tasks.

</details>

<details>

<summary>2019-07-14 06:46:16 - Promotion of Answer Value Measurement with Domain Effects in Community Question Answering Systems</summary>

- *Binbin Jin, Enhong Chen, Hongke Zhao, Zhenya Huang, Qi Liu, Hengshu Zhu, Shui Yu*

- `1906.00156v2` - [abs](http://arxiv.org/abs/1906.00156v2) - [pdf](http://arxiv.org/pdf/1906.00156v2)

> In the area of community question answering (CQA), answer selection and answer ranking are two tasks which are applied to help users quickly access valuable answers. Existing solutions mainly exploit the syntactic or semantic correlation between a question and its related answers (Q&A), where the multi-facet domain effects in CQA are still underexplored. In this paper, we propose a unified model, Enhanced Attentive Recurrent Neural Network (EARNN), for both answer selection and answer ranking tasks by taking full advantages of both Q&A semantics and multi-facet domain effects (i.e., topic effects and timeliness). Specifically, we develop a serialized LSTM to learn the unified representations of Q&A, where two attention mechanisms at either sentence-level or word-level are designed for capturing the deep effects of topics. Meanwhile, the emphasis of Q&A can be automatically distinguished. Furthermore, we design a time-sensitive ranking function to model the timeliness in CQA. To effectively train EARNN, a question-dependent pairwise learning strategy is also developed. Finally, we conduct extensive experiments on a real-world dataset from Quora. Experimental results validate the effectiveness and interpretability of our proposed EARNN model.

</details>

<details>

<summary>2019-07-14 11:14:14 - Automatic Repair and Type Binding of Undeclared Variables using Neural Networks</summary>

- *Venkatesh Theru Mohan, Ali Jannesari*

- `1907.06205v1` - [abs](http://arxiv.org/abs/1907.06205v1) - [pdf](http://arxiv.org/pdf/1907.06205v1)

> Deep learning had been used in program analysis for the prediction of hidden software defects using software defect datasets, security vulnerabilities using generative adversarial networks as well as identifying syntax errors by learning a trained neural machine translation on program codes. However, all these approaches either require defect datasets or bug-free source codes that are executable for training the deep learning model. Our neural network model is neither trained with any defect datasets nor bug-free programming source codes, instead it is trained using structural semantic details of Abstract Syntax Tree (AST) where each node represents a construct appearing in the source code. This model is implemented to fix one of the most common semantic errors, such as undeclared variable errors as well as infer their type information before program compilation. By this approach, the model has achieved in correctly locating and identifying 81% of the programs on prutor dataset of 1059 programs with only undeclared variable errors and also inferring their types correctly in 80% of the programs.

</details>

<details>

<summary>2019-07-15 04:48:34 - Ranking sentences from product description & bullets for better search</summary>

- *Prateek Verma, Aliasgar Kutiyanawala, Ke Shen*

- `1907.06330v1` - [abs](http://arxiv.org/abs/1907.06330v1) - [pdf](http://arxiv.org/pdf/1907.06330v1)

> Products in an ecommerce catalog contain information-rich fields like description and bullets that can be useful to extract entities (attributes) using NER based systems. However, these fields are often verbose and contain lot of information that is not relevant from a search perspective. Treating each sentence within these fields equally can lead to poor full text match and introduce problems in extracting attributes to develop ontologies, semantic search etc. To address this issue, we describe two methods based on extractive summarization with reinforcement learning by leveraging information in product titles and search click through logs to rank sentences from bullets, description, etc. Finally, we compare the accuracy of these two models.

</details>

<details>

<summary>2019-07-15 15:08:47 - Patterns of Effort Contribution and Demand and User Classification based on Participation Patterns in NPM Ecosystem</summary>

- *Tapajit Dey, Yuxing Ma, Audris Mockus*

- `1907.06538v1` - [abs](http://arxiv.org/abs/1907.06538v1) - [pdf](http://arxiv.org/pdf/1907.06538v1)

> Background: Open source requires participation of volunteer and commercial developers (users) in order to deliver functional high-quality components. Developers both contribute effort in the form of patches and demand effort from the component maintainers to resolve issues reported against it. Aim: Identify and characterize patterns of effort contribution and demand throughout the open source supply chain and investigate if and how these patterns vary with developer activity; identify different groups of developers; and predict developers' company affiliation based on their participation patterns. Method: 1,376,946 issues and pull-requests created for 4433 NPM packages with over 10,000 monthly downloads and full (public) commit activity data of the 272,142 issue creators is obtained and analyzed and dependencies on NPM packages are identified. Fuzzy c-means clustering algorithm is used to find the groups among the users based on their effort contribution and demand patterns, and Random Forest is used as the predictive modeling technique to identify their company affiliations. Result: Users contribute and demand effort primarily from packages that they depend on directly with only a tiny fraction of contributions and demand going to transitive dependencies. A significant portion of demand goes into packages outside the users' respective supply chains (constructed based on publicly visible version control data). Three and two different groups of users are observed based on the effort demand and effort contribution patterns respectively. The Random Forest model used for identifying the company affiliation of the users gives a AUC-ROC value of 0.68. Conclusion: Our results give new insights into effort demand and supply at different parts of the supply chain of the NPM ecosystem and its users and suggests the need to increase visibility further upstream.

</details>

<details>

<summary>2019-07-15 15:51:27 - A semi-holographic hyperdimensional representation system for hardware-friendly cognitive computing</summary>

- *A. Serb, I. Kobyzev, J. Wang, T. Prodromakis*

- `1907.05688v2` - [abs](http://arxiv.org/abs/1907.05688v2) - [pdf](http://arxiv.org/pdf/1907.05688v2)

> One of the main, long-term objectives of artificial intelligence is the creation of thinking machines. To that end, substantial effort has been placed into designing cognitive systems; i.e. systems that can manipulate semantic-level information. A substantial part of that effort is oriented towards designing the mathematical machinery underlying cognition in a way that is very efficiently implementable in hardware. In this work we propose a 'semi-holographic' representation system that can be implemented in hardware using only multiplexing and addition operations, thus avoiding the need for expensive multiplication. The resulting architecture can be readily constructed by recycling standard microprocessor elements and is capable of performing two key mathematical operations frequently used in cognition, superposition and binding, within a budget of below 6 pJ for 64- bit operands. Our proposed 'cognitive processing unit' (CoPU) is intended as just one (albeit crucial) part of much larger cognitive systems where artificial neural networks of all kinds and associative memories work in concord to give rise to intelligence.

</details>

<details>

<summary>2019-07-15 16:53:43 - Medical Concept Representation Learning from Claims Data and Application to Health Plan Payment Risk Adjustment</summary>

- *Qiu-Yue Zhong, Andrew H. Fairless, Jasmine M. McCammon, Farbod Rahmanian*

- `1907.06600v1` - [abs](http://arxiv.org/abs/1907.06600v1) - [pdf](http://arxiv.org/pdf/1907.06600v1)

> Risk adjustment has become an increasingly important tool in healthcare. It has been extensively applied to payment adjustment for health plans to reflect the expected cost of providing coverage for members. Risk adjustment models are typically estimated using linear regression, which does not fully exploit the information in claims data. Moreover, the development of such linear regression models requires substantial domain expert knowledge and computational effort for data preprocessing. In this paper, we propose a novel approach for risk adjustment that uses semantic embeddings to represent patient medical histories. Embeddings efficiently represent medical concepts learned from diagnostic, procedure, and prescription codes in patients' medical histories. This approach substantially reduces the need for feature engineering. Our results show that models using embeddings had better performance than a commercial risk adjustment model on the task of prospective risk score prediction.

</details>

<details>

<summary>2019-07-15 19:01:39 - CupQ: A New Clinical Literature Search Engine</summary>

- *Jesse Wang, Henry Kautz*

- `1907.06697v1` - [abs](http://arxiv.org/abs/1907.06697v1) - [pdf](http://arxiv.org/pdf/1907.06697v1)

> A new clinical literature search engine, called CupQ, is presented. It aims to help clinicians stay updated with medical knowledge. Although PubMed is currently one of the most widely used digital libraries for biomedical information, it frequently does not return clinically relevant results. CupQ utilizes a ranking algorithm that filters non-medical journals, compares semantic similarity between queries, and incorporates journal impact factor and publication date. It organizes search results into useful categories for medical practitioners: reviews, guidelines, and studies. Qualitative comparisons suggest that CupQ may return more clinically relevant information than PubMed. CupQ is available at https://cupq.io/.

</details>

<details>

<summary>2019-07-16 09:25:13 - Conversational Help for Task Completion and Feature Discovery in Personal Assistants</summary>

- *Madan Gopal Jhawar, Vipindeep Vangala, Nishchay Sharma, Ankur Hayatnagarkar, Mansi Saxena, Swati Valecha*

- `1907.07564v1` - [abs](http://arxiv.org/abs/1907.07564v1) - [pdf](http://arxiv.org/pdf/1907.07564v1)

> Intelligent Personal Assistants (IPAs) have become widely popular in recent times. Most of the commercial IPAs today support a wide range of skills including Alarms, Reminders, Weather Updates, Music, News, Factual Questioning-Answering, etc. The list grows every day, making it difficult to remember the command structures needed to execute various tasks. An IPA must have the ability to communicate information about supported skills and direct users towards the right commands needed to execute them. Users interact with personal assistants in natural language. A query is defined to be a Help Query if it seeks information about a personal assistant's capabilities, or asks for instructions to execute a task. In this paper, we propose an interactive system which identifies help queries and retrieves appropriate responses. Our system comprises of a C-BiLSTM based classifier, which is a fusion of Convolutional Neural Networks (CNN) and Bidirectional LSTM (BiLSTM) architectures, to detect help queries and a semantic Approximate Nearest Neighbours (ANN) module to map the query to an appropriate predefined response. Evaluation of our system on real-world queries from a commercial IPA and a detailed comparison with popular traditional machine learning and deep learning based models reveal that our system outperforms other approaches and returns relevant responses for help queries.

</details>

<details>

<summary>2019-07-16 14:12:22 - NILE: Fast Natural Language Processing for Electronic Health Records</summary>

- *Sheng Yu, Tianrun Cai, Tianxi Cai*

- `1311.6063v5` - [abs](http://arxiv.org/abs/1311.6063v5) - [pdf](http://arxiv.org/pdf/1311.6063v5)

> Objective: Narrative text in Electronic health records (EHR) contain rich information for medical and data science studies. This paper introduces the design and performance of Narrative Information Linear Extraction (NILE), a natural language processing (NLP) package for EHR analysis that we share with the medical informatics community. Methods: NILE uses a modified prefix-tree search algorithm for named entity recognition, which can detect prefix and suffix sharing. The semantic analyses are implemented as rule-based finite state machines. Analyses include negation, location, modification, family history, and ignoring. Result: The processing speed of NILE is hundreds to thousands times faster than existing NLP software for medical text. The accuracy of presence analysis of NILE is on par with the best performing models on the 2010 i2b2/VA NLP challenge data. Conclusion: The speed, accuracy, and being able to operate via API make NILE a valuable addition to the NLP software for medical informatics and data science.

</details>

<details>

<summary>2019-07-16 14:17:06 - Data Selection for training Semantic Segmentation CNNs with cross-dataset weak supervision</summary>

- *Panagiotis Meletis, Rob Romijnders, Gijs Dubbelman*

- `1907.07023v1` - [abs](http://arxiv.org/abs/1907.07023v1) - [pdf](http://arxiv.org/pdf/1907.07023v1)

> Training convolutional networks for semantic segmentation with strong (per-pixel) and weak (per-bounding-box) supervision requires a large amount of weakly labeled data. We propose two methods for selecting the most relevant data with weak supervision. The first method is designed for finding visually similar images without the need of labels and is based on modeling image representations with a Gaussian Mixture Model (GMM). As a byproduct of GMM modeling, we present useful insights on characterizing the data generating distribution. The second method aims at finding images with high object diversity and requires only the bounding box labels. Both methods are developed in the context of automated driving and experimentation is conducted on Cityscapes and Open Images datasets. We demonstrate performance gains by reducing the amount of employed weakly labeled images up to 100 times for Open Images and up to 20 times for Cityscapes.

</details>

<details>

<summary>2019-07-16 14:32:33 - Neural Language Model Based Training Data Augmentation for Weakly Supervised Early Rumor Detection</summary>

- *Sooji Han, Jie Gao, Fabio Ciravegna*

- `1907.07033v1` - [abs](http://arxiv.org/abs/1907.07033v1) - [pdf](http://arxiv.org/pdf/1907.07033v1)

> The scarcity and class imbalance of training data are known issues in current rumor detection tasks. We propose a straight-forward and general-purpose data augmentation technique which is beneficial to early rumor detection relying on event propagation patterns. The key idea is to exploit massive unlabeled event data sets on social media to augment limited labeled rumor source tweets. This work is based on rumor spreading patterns revealed by recent rumor studies and semantic relatedness between labeled and unlabeled data. A state-of-the-art neural language model (NLM) and large credibility-focused Twitter corpora are employed to learn context-sensitive representations of rumor tweets. Six different real-world events based on three publicly available rumor datasets are employed in our experiments to provide a comparative evaluation of the effectiveness of the method. The results show that our method can expand the size of an existing rumor data set nearly by 200% and corresponding social context (i.e., conversational threads) by 100% with reasonable quality. Preliminary experiments with a state-of-the-art deep learning-based rumor detection model show that augmented data can alleviate over-fitting and class imbalance caused by limited train data and can help to train complex neural networks (NNs). With augmented data, the performance of rumor detection can be improved by 12.1% in terms of F-score. Our experiments also indicate that augmented training data can help to generalize rumor detection models on unseen rumors.

</details>

<details>

<summary>2019-07-16 16:14:09 - STRASS: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings</summary>

- *Léo Bouscarrat, Antoine Bonnefoy, Thomas Peel, Cécile Pereira*

- `1907.07323v1` - [abs](http://arxiv.org/abs/1907.07323v1) - [pdf](http://arxiv.org/pdf/1907.07323v1)

> This paper introduces STRASS: Summarization by TRAnsformation Selection and Scoring. It is an extractive text summarization method which leverages the semantic information in existing sentence embedding spaces. Our method creates an extractive summary by selecting the sentences with the closest embeddings to the document embedding. The model learns a transformation of the document embedding to minimize the similarity between the extractive summary and the ground truth summary. As the transformation is only composed of a dense layer, the training can be done on CPU, therefore, inexpensive. Moreover, inference time is short and linear according to the number of sentences. As a second contribution, we introduce the French CASS dataset, composed of judgments from the French Court of cassation and their corresponding summaries. On this dataset, our results show that our method performs similarly to the state of the art extractive methods with effective training and inferring time.

</details>

<details>

<summary>2019-07-16 17:27:21 - Efficient Segmentation: Learning Downsampling Near Semantic Boundaries</summary>

- *Dmitrii Marin, Zijian He, Peter Vajda, Priyam Chatterjee, Sam Tsai, Fei Yang, Yuri Boykov*

- `1907.07156v1` - [abs](http://arxiv.org/abs/1907.07156v1) - [pdf](http://arxiv.org/pdf/1907.07156v1)

> Many automated processes such as auto-piloting rely on a good semantic segmentation as a critical component. To speed up performance, it is common to downsample the input frame. However, this comes at the cost of missed small objects and reduced accuracy at semantic boundaries. To address this problem, we propose a new content-adaptive downsampling technique that learns to favor sampling locations near semantic boundaries of target classes. Cost-performance analysis shows that our method consistently outperforms the uniform sampling improving balance between accuracy and computational efficiency. Our adaptive sampling gives segmentation with better quality of boundaries and more reliable support for smaller-size objects.

</details>

<details>

<summary>2019-07-16 19:32:57 - DeepTrax: Embedding Graphs of Financial Transactions</summary>

- *C. Bayan Bruss, Anish Khazane, Jonathan Rider, Richard Serpe, Antonia Gogoglou, Keegan E. Hines*

- `1907.07225v1` - [abs](http://arxiv.org/abs/1907.07225v1) - [pdf](http://arxiv.org/pdf/1907.07225v1)

> Financial transactions can be considered edges in a heterogeneous graph between entities sending money and entities receiving money. For financial institutions, such a graph is likely large (with millions or billions of edges) while also sparsely connected. It becomes challenging to apply machine learning to such large and sparse graphs. Graph representation learning seeks to embed the nodes of a graph into a Euclidean vector space such that graph topological properties are preserved after the transformation. In this paper, we present a novel application of representation learning to bipartite graphs of credit card transactions in order to learn embeddings of account and merchant entities. Our framework is inspired by popular approaches in graph embeddings and is trained on two internal transaction datasets. This approach yields highly effective embeddings, as quantified by link prediction AUC and F1 score. Further, the resulting entity vectors retain intuitive semantic similarity that is explored through visualizations and other qualitative analyses. Finally, we show how these embeddings can be used as features in downstream machine learning business applications such as fraud detection.

</details>

<details>

<summary>2019-07-16 22:58:35 - A New Distribution Version of Boneh-Goh-Nissim Cryptosystem: Security and performance analysis</summary>

- *Oualid Benamara, Fatiha Merazka*

- `1907.07282v1` - [abs](http://arxiv.org/abs/1907.07282v1) - [pdf](http://arxiv.org/pdf/1907.07282v1)

> The aim of this paper is to provide two distributed versions of the Boneh-Goh-Nissim Cryptosystem (BGNC). We give a proof of the semantic security for the first one. This guaranties that our algorithm is semantically secure in the contest of active non-adaptive adversaries. Furthermore, we prove that the second version of our distributed scheme is computationally more efficient than the ElGamal destributed elliptic curve cryptosystem (EDECC) and secure under the Subgroup Decision problem (SDP) assumption.

</details>

<details>

<summary>2019-07-17 03:41:51 - A General Framework of Learning Multi-Vehicle Interaction Patterns from Videos</summary>

- *Chengyuan Zhang, Jiacheng Zhu, Wenshuo Wang, Ding Zhao*

- `1907.07315v1` - [abs](http://arxiv.org/abs/1907.07315v1) - [pdf](http://arxiv.org/pdf/1907.07315v1)

> Semantic learning and understanding of multi-vehicle interaction patterns in a cluttered driving environment are essential but challenging for autonomous vehicles to make proper decisions. This paper presents a general framework to gain insights into intricate multi-vehicle interaction patterns from bird's-eye view traffic videos. We adopt a Gaussian velocity field to describe the time-varying multi-vehicle interaction behaviors and then use deep autoencoders to learn associated latent representations for each temporal frame. Then, we utilize a hidden semi-Markov model with a hierarchical Dirichlet process as a prior to segment these sequential representations into granular components, also called traffic primitives, corresponding to interaction patterns. Experimental results demonstrate that our proposed framework can extract traffic primitives from videos, thus providing a semantic way to analyze multi-vehicle interaction patterns, even for cluttered driving scenarios that are far messier than human beings can cope with.

</details>

<details>

<summary>2019-07-17 08:22:58 - Embedding-based Silhouette Community Detection</summary>

- *Blaž Škrlj, Jan Kralj, Nada Lavrač*

- `1908.02556v1` - [abs](http://arxiv.org/abs/1908.02556v1) - [pdf](http://arxiv.org/pdf/1908.02556v1)

> Mining complex data in the form of networks is of increasing interest in many scientific disciplines. Network communities correspond to densely connected subnetworks, and often represent key functional parts of real-world systems. In this work, we propose Silhouette Community Detection (SCD), an approach for detecting communities, based on clustering of network node embeddings, i.e. real valued representations of nodes derived from their neighborhoods. We investigate the performance of the proposed SCD approach on 234 synthetic networks, as well as on a real-life social network. Even though SCD is not based on any form of modularity optimization, it performs comparably or better than state-of-the-art community detection algorithms, such as the InfoMap and Louvain algorithms. Further, we demonstrate how SCD's outputs can be used along with domain ontologies in semantic subgroup discovery, yielding human-understandable explanations of communities detected in a real-life protein interaction network. Being embedding-based, SCD is widely applicable and can be tested out-of-the-box as part of many existing network learning and exploration pipelines.

</details>

<details>

<summary>2019-07-17 10:03:38 - SUMBT: Slot-Utterance Matching for Universal and Scalable Belief Tracking</summary>

- *Hwaran Lee, Jinsik Lee, Tae-Yoon Kim*

- `1907.07421v1` - [abs](http://arxiv.org/abs/1907.07421v1) - [pdf](http://arxiv.org/pdf/1907.07421v1)

> In goal-oriented dialog systems, belief trackers estimate the probability distribution of slot-values at every dialog turn. Previous neural approaches have modeled domain- and slot-dependent belief trackers, and have difficulty in adding new slot-values, resulting in lack of flexibility of domain ontology configurations. In this paper, we propose a new approach to universal and scalable belief tracker, called slot-utterance matching belief tracker (SUMBT). The model learns the relations between domain-slot-types and slot-values appearing in utterances through attention mechanisms based on contextual semantic vectors. Furthermore, the model predicts slot-value labels in a non-parametric way. From our experiments on two dialog corpora, WOZ 2.0 and MultiWOZ, the proposed model showed performance improvement in comparison with slot-dependent methods and achieved the state-of-the-art joint accuracy.

</details>

<details>

<summary>2019-07-17 13:52:54 - Almawave-SLU: A new dataset for SLU in Italian</summary>

- *Valentina Bellomaria, Giuseppe Castellucci, Andrea Favalli, Raniero Romagnoli*

- `1907.07526v1` - [abs](http://arxiv.org/abs/1907.07526v1) - [pdf](http://arxiv.org/pdf/1907.07526v1)

> The widespread use of conversational and question answering systems made it necessary to improve the performances of speaker intent detection and understanding of related semantic slots, i.e., Spoken Language Understanding (SLU). Often, these tasks are approached with supervised learning methods, which needs considerable labeled datasets. This paper presents the first Italian dataset for SLU. It is derived through a semi-automatic procedure and is used as a benchmark of various open source and commercial systems.

</details>

<details>

<summary>2019-07-17 16:28:00 - RGB and LiDAR fusion based 3D Semantic Segmentation for Autonomous Driving</summary>

- *Khaled El Madawy, Hazem Rashed, Ahmad El Sallab, Omar Nasr, Hanan Kamel, Senthil Yogamani*

- `1906.00208v2` - [abs](http://arxiv.org/abs/1906.00208v2) - [pdf](http://arxiv.org/pdf/1906.00208v2)

> LiDAR has become a standard sensor for autonomous driving applications as they provide highly precise 3D point clouds. LiDAR is also robust for low-light scenarios at night-time or due to shadows where the performance of cameras is degraded. LiDAR perception is gradually becoming mature for algorithms including object detection and SLAM. However, semantic segmentation algorithm remains to be relatively less explored. Motivated by the fact that semantic segmentation is a mature algorithm on image data, we explore sensor fusion based 3D segmentation. Our main contribution is to convert the RGB image to a polar-grid mapping representation used for LiDAR and design early and mid-level fusion architectures. Additionally, we design a hybrid fusion architecture that combines both fusion algorithms. We evaluate our algorithm on KITTI dataset which provides segmentation annotation for cars, pedestrians and cyclists. We evaluate two state-of-the-art architectures namely SqueezeSeg and PointSeg and improve the mIoU score by 10 % in both cases relative to the LiDAR only baseline.

</details>

<details>

<summary>2019-07-17 20:00:22 - Weakly supervised training of pixel resolution segmentation models on whole slide images</summary>

- *Nicolas Pinchaud*

- `1905.12931v2` - [abs](http://arxiv.org/abs/1905.12931v2) - [pdf](http://arxiv.org/pdf/1905.12931v2)

> We present a novel approach to train pixel resolution segmentation models on whole slide images in a weakly supervised setup. The model is trained to classify patches extracted from slides. This leads the training to be made under noisy labeled data. We solve the problem with two complementary strategies. First, the patches are sampled online using the model's knowledge by focusing on regions where the model's confidence is higher. Second, we propose an extension of the KL divergence that is robust to noisy labels. Our preliminary experiment on CAMELYON 16 data set show promising results. The model can successfully segment tumor areas with strong morphological consistency.

</details>

<details>

<summary>2019-07-18 10:36:03 - Deep Neural Models for Medical Concept Normalization in User-Generated Texts</summary>

- *Zulfat Miftahutdinov, Elena Tutubalina*

- `1907.07972v1` - [abs](http://arxiv.org/abs/1907.07972v1) - [pdf](http://arxiv.org/pdf/1907.07972v1)

> In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a health-related entity mention in a free-form text to a concept in a controlled vocabulary, usually to the standard thesaurus in the Unified Medical Language System (UMLS). This is a challenging task since medical terminology is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem with powerful neural networks such as recurrent neural networks and contextualized word representation models trained to obtain semantic representations of social media expressions. Our experimental evaluation over three different benchmarks shows that neural architectures leverage the semantic meaning of the entity mention and significantly outperform an existing state of the art models.

</details>

<details>

<summary>2019-07-18 13:42:22 - Exploiting bilateral symmetry in brain lesion segmentation</summary>

- *Kevin Raina, Uladzimir Yahorau, Tanya Schmah*

- `1907.08196v1` - [abs](http://arxiv.org/abs/1907.08196v1) - [pdf](http://arxiv.org/pdf/1907.08196v1)

> Brain lesions, including stroke and tumours, have a high degree of variability in terms of location, size, intensity and form, making automatic segmentation difficult. We propose an improvement to existing segmentation methods by exploiting the bilateral quasi-symmetry of healthy brains, which breaks down when lesions are present. Specifically, we use nonlinear registration of a neuroimage to a reflected version of itself ("reflective registration") to determine for each voxel its homologous (corresponding) voxel in the other hemisphere. A patch around the homologous voxel is added as a set of new features to the segmentation algorithm. To evaluate this method, we implemented two different CNN-based multimodal MRI stroke lesion segmentation algorithms, and then augmented them by adding extra symmetry features using the reflective registration method described above. For each architecture, we compared the performance with and without symmetry augmentation, on the SISS Training dataset of the Ischemic Stroke Lesion Segmentation Challenge (ISLES) 2015 challenge. Using affine reflective registration improves performance over baseline, but nonlinear reflective registration gives significantly better results: an improvement in Dice coefficient of 13 percentage points over baseline for one architecture and 9 points for the other. We argue for the broad applicability of adding symmetric features to existing segmentation algorithms, specifically using nonlinear, template-free methods.

</details>

<details>

<summary>2019-07-18 16:36:18 - On Accurate Evaluation of GANs for Language Generation</summary>

- *Stanislau Semeniuta, Aliaksei Severyn, Sylvain Gelly*

- `1806.04936v3` - [abs](http://arxiv.org/abs/1806.04936v3) - [pdf](http://arxiv.org/pdf/1806.04936v3)

> Generative Adversarial Networks (GANs) are a promising approach to language generation. The latest works introducing novel GAN models for language generation use n-gram based metrics for evaluation and only report single scores of the best run. In this paper, we argue that this often misrepresents the true picture and does not tell the full story, as GAN models can be extremely sensitive to the random initialization and small deviations from the best hyperparameter choice. In particular, we demonstrate that the previously used BLEU score is not sensitive to semantic deterioration of generated texts and propose alternative metrics that better capture the quality and diversity of the generated samples. We also conduct a set of experiments comparing a number of GAN models for text with a conventional Language Model (LM) and find that neither of the considered models performs convincingly better than the LM.

</details>

<details>

<summary>2019-07-18 16:48:47 - Observing LOD using Equivalent Set Graphs: it is mostly flat and sparsely linked</summary>

- *Luigi Asprino, Wouter Beek, Paolo Ciancarini, Frank van Harmelen, Valentina Presutti*

- `1906.08097v3` - [abs](http://arxiv.org/abs/1906.08097v3) - [pdf](http://arxiv.org/pdf/1906.08097v3)

> This paper presents an empirical study aiming at understanding the modeling style and the overall semantic structure of Linked Open Data. We observe how classes, properties and individuals are used in practice. We also investigate how hierarchies of concepts are structured, and how much they are linked. In addition to discussing the results, this paper contributes (i) a conceptual framework, including a set of metrics, which generalises over the observable constructs; (ii) an open source implementation that facilitates its application to other Linked Data knowledge graphs.

</details>

<details>

<summary>2019-07-19 15:02:16 - Predicting Human Activities from User-Generated Content</summary>

- *Steven R. Wilson, Rada Mihalcea*

- `1907.08540v1` - [abs](http://arxiv.org/abs/1907.08540v1) - [pdf](http://arxiv.org/pdf/1907.08540v1)

> The activities we do are linked to our interests, personality, political preferences, and decisions we make about the future. In this paper, we explore the task of predicting human activities from user-generated content. We collect a dataset containing instances of social media users writing about a range of everyday activities. We then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities. We train a neural network model to make predictions about which clusters contain activities that were performed by a given user based on the text of their previous posts and self-description. Additionally, we explore the degree to which incorporating inferred user traits into our model helps with this prediction task.

</details>

<details>

<summary>2019-07-20 13:14:37 - Incorporating Syntactic and Semantic Information in Word Embeddings using Graph Convolutional Networks</summary>

- *Shikhar Vashishth, Manik Bhandari, Prateek Yadav, Piyush Rai, Chiranjib Bhattacharyya, Partha Talukdar*

- `1809.04283v4` - [abs](http://arxiv.org/abs/1809.04283v4) - [pdf](http://arxiv.org/pdf/1809.04283v4)

> Word embeddings have been widely adopted across several NLP applications. Most existing word embedding methods utilize sequential context of a word to learn its embedding. While there have been some attempts at utilizing syntactic context of a word, such methods result in an explosion of the vocabulary size. In this paper, we overcome this problem by proposing SynGCN, a flexible Graph Convolution based method for learning word embeddings. SynGCN utilizes the dependency context of a word without increasing the vocabulary size. Word embeddings learned by SynGCN outperform existing methods on various intrinsic and extrinsic tasks and provide an advantage when used with ELMo. We also propose SemGCN, an effective framework for incorporating diverse semantic knowledge for further enhancing learned word representations. We make the source code of both models available to encourage reproducible research.

</details>

<details>

<summary>2019-07-20 14:38:34 - Generative Steganography by Sampling</summary>

- *Zhuo Zhang, Jia Liu, Yan Ke, Yu Lei, Jun Li, Minqing Zhang, Xiaoyuan Yang*

- `1804.10531v2` - [abs](http://arxiv.org/abs/1804.10531v2) - [pdf](http://arxiv.org/pdf/1804.10531v2)

> In this paper, a novel data-driven information hiding scheme called generative steganography by sampling (GSS) is proposed. Unlike in traditional modification-based steganography, in our method the stego image is directly sampled by a powerful generator: no explicit cover is used. Both parties share a secret key used for message embedding and extraction. The Jensen-Shannon divergence is introduced as a new criterion for evaluating the security of generative steganography. Based on these principles, we propose a simple practical generative steganography method that uses semantic image inpainting. The message is written in advance to an uncorrupted region that needs to be retained in the corrupted image. Then, the corrupted image with the secret message is fed into a Generator trained by a generative adversarial network (GAN) for semantic completion. Message loss and prior loss terms are proposed for penalizing message extraction error and unrealistic stego image. In our design, we first train a generator whose training target is the generation of new data samples from the same distribution as that of existing training data. Next, for the trained generator, backpropagation to the message and prior loss are introduced to optimize the coding of the input noise data for the generator. The presented experiments demonstrate the potential of the proposed framework based on both qualitative and quantitative evaluations of the generated stego images.

</details>

<details>

<summary>2019-07-20 16:39:06 - Towards meta-interpretive learning of programming language semantics</summary>

- *Sándor Bartha, James Cheney*

- `1907.08834v1` - [abs](http://arxiv.org/abs/1907.08834v1) - [pdf](http://arxiv.org/pdf/1907.08834v1)

> We introduce a new application for inductive logic programming: learning the semantics of programming languages from example evaluations. In this short paper, we explored a simplified task in this domain using the Metagol meta-interpretive learning system. We highlighted the challenging aspects of this scenario, including abstracting over function symbols, nonterminating examples, and learning non-observed predicates, and proposed extensions to Metagol helpful for overcoming these challenges, which may prove useful in other domains.

</details>

<details>

<summary>2019-07-21 05:26:36 - A Corpus for Reasoning About Natural Language Grounded in Photographs</summary>

- *Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi*

- `1811.00491v3` - [abs](http://arxiv.org/abs/1811.00491v3) - [pdf](http://arxiv.org/pdf/1811.00491v3)

> We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.

</details>

<details>

<summary>2019-07-21 07:16:55 - Word Sense Disambiguation using Diffusion Kernel PCA</summary>

- *Bilge Sipal, Ozcan Sari, Asena Teke, Nurullah Demirci*

- `1908.01832v1` - [abs](http://arxiv.org/abs/1908.01832v1) - [pdf](http://arxiv.org/pdf/1908.01832v1)

> One of the major problems in natural language processing (NLP) is the word sense disambiguation (WSD) problem. It is the task of computationally identifying the right sense of a polysemous word based on its context. Resolving the WSD problem boosts the accuracy of many NLP focused algorithms such as text classification and machine translation. In this paper, we introduce a new supervised algorithm for WSD, that is based on Kernel PCA and Semantic Diffusion Kernel, which is called Diffusion Kernel PCA (DKPCA). DKPCA grasps the semantic similarities within terms, and it is based on PCA. These properties enable us to perform feature extraction and dimension reduction guided by semantic similarities and within the algorithm. Our empirical results on SensEval data demonstrate that DKPCA achieves higher or very close accuracy results compared to SVM and KPCA with various well-known kernels when the labeled data ratio is meager. Considering the scarcity of labeled data, whereas large quantities of unlabeled textual data are easily accessible, these are highly encouraging first results to develop DKPCA further.

</details>

<details>

<summary>2019-07-21 17:58:15 - Perspectival Knowledge in PSOA RuleML: Representation, Model Theory, and Translation</summary>

- *Harold Boley, Gen Zou*

- `1712.02869v3` - [abs](http://arxiv.org/abs/1712.02869v3) - [pdf](http://arxiv.org/pdf/1712.02869v3)

> In Positional-Slotted Object-Applicative (PSOA) RuleML, a predicate application (atom) can have an Object IDentifier (OID) and descriptors that may be positional arguments (tuples) or attribute-value pairs (slots). PSOA RuleML explicitly specifies for each descriptor whether it is to be interpreted under the perspective of the predicate in whose scope it occurs. This predicate-dependency dimension refines the space between oidless, positional atoms (relationships) and oidful, slotted atoms (framepoints): While relationships use only a predicate-scope-sensitive (predicate-dependent) tuple and framepoints use only predicate-scope-insensitive (predicate-independent) slots, PSOA uses a systematics of orthogonal constructs also permitting atoms with (predicate-)independent tuples and atoms with (predicate-)dependent slots. This supports data and knowledge representation where a slot attribute can have different values depending on the predicate. PSOA thus extends object-oriented multi-membership and multiple inheritance. Based on objectification, PSOA laws are given: Besides unscoping and centralization, the semantic restriction and transformation of describution permits rescoping of one atom's independent descriptors to another atom with the same OID but a different predicate. For inheritance, default descriptors are realized by rules. On top of a metamodel and a Grailog visualization, PSOA's atom systematics for facts, queries, and rules is explained. The presentation and (XML-)serialization syntaxes of PSOA RuleML are introduced. Its model-theoretic semantics is formalized by extending the interpretation functions for dependent descriptors. The open-source PSOATransRun system realizes PSOA RuleML by a translator to runtime predicates, including for dependent tuples (prdtupterm) and slots (prdsloterm). Our tests show efficiency advantages of dependent and tupled modeling.

</details>

<details>

<summary>2019-07-22 00:18:34 - Covering up bias in CelebA-like datasets with Markov blankets: A post-hoc cure for attribute prior avoidance</summary>

- *Vinay Uday Prabhu, Dian Ang Yap, Alexander Wang, John Whaley*

- `1907.12917v1` - [abs](http://arxiv.org/abs/1907.12917v1) - [pdf](http://arxiv.org/pdf/1907.12917v1)

> Attribute prior avoidance entails subconscious or willful non-modeling of (meta)attributes that datasets are oft born with, such as the 40 semantic facial attributes associated with the CelebA and CelebA-HQ datasets. The consequences of this infirmity, we discover, are especially stark in state-of-the-art deep generative models learned on these datasets that just model the pixel-space measurements, resulting in an inter-attribute bias-laden latent space. This viscerally manifests itself when we perform face manipulation experiments based on latent vector interpolations. In this paper, we address this and propose a post-hoc solution that utilizes an Ising attribute prior learned in the attribute space and showcase its efficacy via qualitative experiments.

</details>

<details>

<summary>2019-07-22 02:43:46 - Online Adaptive Image Reconstruction (OnAIR) Using Dictionary Models</summary>

- *Brian E. Moore, Saiprasad Ravishankar, Raj Rao Nadakuditi, Jeffrey A. Fessler*

- `1809.01817v3` - [abs](http://arxiv.org/abs/1809.01817v3) - [pdf](http://arxiv.org/pdf/1809.01817v3)

> Sparsity and low-rank models have been popular for reconstructing images and videos from limited or corrupted measurements. Dictionary or transform learning methods are useful in applications such as denoising, inpainting, and medical image reconstruction. This paper proposes a framework for online (or time-sequential) adaptive reconstruction of dynamic image sequences from linear (typically undersampled) measurements. We model the spatiotemporal patches of the underlying dynamic image sequence as sparse in a dictionary, and we simultaneously estimate the dictionary and the images sequentially from streaming measurements. Multiple constraints on the adapted dictionary are also considered such as a unitary matrix, or low-rank dictionary atoms that provide additional efficiency or robustness. The proposed online algorithms are memory efficient and involve simple updates of the dictionary atoms, sparse coefficients, and images. Numerical experiments demonstrate the usefulness of the proposed methods in inverse problems such as video reconstruction or inpainting from noisy, subsampled pixels, and dynamic magnetic resonance image reconstruction from very limited measurements.

</details>

<details>

<summary>2019-07-22 03:12:30 - Open Problems in a Logic of Gossips</summary>

- *Krzysztof R. Apt, Dominik Wojtczak*

- `1907.09097v1` - [abs](http://arxiv.org/abs/1907.09097v1) - [pdf](http://arxiv.org/pdf/1907.09097v1)

> Gossip protocols are programs used in a setting in which each agent holds a secret and the aim is to reach a situation in which all agents know all secrets. Such protocols rely on a point-to-point or group communication. Distributed epistemic gossip protocols use epistemic formulas in the component programs for the agents. The advantage of the use of epistemic logic is that the resulting protocols are very concise and amenable for a simple verification.   Recently, we introduced a natural modal logic that allows one to express distributed epistemic gossip protocols and to reason about their correctness. We proved that the resulting protocols are implementable and that all aspects of their correctness, including termination, are decidable. To establish these results we showed that both the definition of semantics and of truth of the underlying logic are decidable. We also showed that the analogous results hold for an extension of this logic with the 'common knowledge' operator.   However, several, often deceptively simple, questions about this logic and the corresponding gossip protocols remain open. The purpose of this paper is to list and elucidate these questions and provide for them an appropriate background information in the form of partial of related results.

</details>

<details>

<summary>2019-07-22 03:18:48 - Exploiting Belief Bases for Building Rich Epistemic Structures</summary>

- *Emiliano Lorini*

- `1907.09114v1` - [abs](http://arxiv.org/abs/1907.09114v1) - [pdf](http://arxiv.org/pdf/1907.09114v1)

> We introduce a semantics for epistemic logic exploiting a belief base abstraction. Differently from existing Kripke-style semantics for epistemic logic in which the notions of possible world and epistemic alternative are primitive, in the proposed semantics they are non-primitive but are defined from the concept of belief base. We show that this semantics allows us to define the universal epistemic model in a simpler and more compact way than existing inductive constructions of it. We provide (i) a number of semantic equivalence results for both the basic epistemic language with "individual belief" operators and its extension by the notion of "only believing", and (ii) a lower bound complexity result for epistemic logic model checking relative to the universal epistemic model.

</details>

<details>

<summary>2019-07-22 10:07:47 - Incremental Answer Set Programming with Overgrounding</summary>

- *Francesco Calimeri, Giovambattista Ianni, Francesco Pacenza, Simona Perri, Jessica Zangari*

- `1907.09212v1` - [abs](http://arxiv.org/abs/1907.09212v1) - [pdf](http://arxiv.org/pdf/1907.09212v1)

> Repeated executions of reasoning tasks for varying inputs are necessary in many applicative settings, such as stream reasoning. In this context, we propose an incremental grounding approach for the answer set semantics. We focus on the possibility of generating incrementally larger ground logic programs equivalent to a given non-ground one; so called overgrounded programs can be reused in combination with deliberately many different sets of inputs. Updating overgrounded programs requires a small effort, thus making the instantiation of logic programs considerably faster when grounding is repeated on a series of inputs similar to each other. Notably, the proposed approach works "under the hood", relieving designers of logic programs from controlling technical aspects of grounding engines and answer set systems. In this work we present the theoretical basis of the proposed incremental grounding technique, we illustrate the consequent repeated evaluation strategy and report about our experiments. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2019-07-22 10:23:20 - Multi-Modal Adversarial Autoencoders for Recommendations of Citations and Subject Labels</summary>

- *Lukas Galke, Florian Mai, Iacopo Vagliano, Ansgar Scherp*

- `1907.12366v1` - [abs](http://arxiv.org/abs/1907.12366v1) - [pdf](http://arxiv.org/pdf/1907.12366v1)

> We present multi-modal adversarial autoencoders for recommendation and evaluate them on two different tasks: citation recommendation and subject label recommendation. We analyze the effects of adversarial regularization, sparsity, and different input modalities. By conducting 408 experiments, we show that adversarial regularization consistently improves the performance of autoencoders for recommendation. We demonstrate, however, that the two tasks differ in the semantics of item co-occurrence in the sense that item co-occurrence resembles relatedness in case of citations, yet implies diversity in case of subject labels. Our results reveal that supplying the partial item set as input is only helpful, when item co-occurrence resembles relatedness. When facing a new recommendation task it is therefore crucial to consider the semantics of item co-occurrence for the choice of an appropriate model.

</details>

<details>

<summary>2019-07-22 11:02:06 - Learning cross-lingual phonological and orthagraphic adaptations: a case study in improving neural machine translation between low-resource languages</summary>

- *Saurav Jha, Akhilesh Sudhakar, Anil Kumar Singh*

- `1811.08816v2` - [abs](http://arxiv.org/abs/1811.08816v2) - [pdf](http://arxiv.org/pdf/1811.08816v2)

> Out-of-vocabulary (OOV) words can pose serious challenges for machine translation (MT) tasks, and in particular, for low-resource language (LRL) pairs, i.e., language pairs for which few or no parallel corpora exist. Our work adapts variants of seq2seq models to perform transduction of such words from Hindi to Bhojpuri (an LRL instance), learning from a set of cognate pairs built from a bilingual dictionary of Hindi--Bhojpuri words. We demonstrate that our models can be effectively used for language pairs that have limited parallel corpora; our models work at the character level to grasp phonetic and orthographic similarities across multiple types of word adaptations, whether synchronic or diachronic, loan words or cognates. We describe the training aspects of several character level NMT systems that we adapted to this task and characterize their typical errors. Our method improves BLEU score by 6.3 on the Hindi-to-Bhojpuri translation task. Further, we show that such transductions can generalize well to other languages by applying it successfully to Hindi -- Bangla cognate pairs. Our work can be seen as an important step in the process of: (i) resolving the OOV words problem arising in MT tasks, (ii) creating effective parallel corpora for resource-constrained languages, and (iii) leveraging the enhanced semantic knowledge captured by word-level embeddings to perform character-level tasks.

</details>

<details>

<summary>2019-07-22 13:25:27 - Syntax-aware Neural Semantic Role Labeling</summary>

- *Qingrong Xia, Zhenghua Li, Min Zhang, Meishan Zhang, Guohong Fu, Rui Wang, Luo Si*

- `1907.09312v1` - [abs](http://arxiv.org/abs/1907.09312v1) - [pdf](http://arxiv.org/pdf/1907.09312v1)

> Semantic role labeling (SRL), also known as shallow semantic parsing, is an important yet challenging task in NLP. Motivated by the close correlation between syntactic and semantic structures, traditional discrete-feature-based SRL approaches make heavy use of syntactic features. In contrast, deep-neural-network-based approaches usually encode the input sentence as a word sequence without considering the syntactic structures. In this work, we investigate several previous approaches for encoding syntactic trees, and make a thorough study on whether extra syntax-aware representations are beneficial for neural SRL models. Experiments on the benchmark CoNLL-2005 dataset show that syntax-aware SRL approaches can effectively improve performance over a strong baseline with external word representations from ELMo. With the extra syntax-aware representations, our approaches achieve new state-of-the-art 85.6 F1 (single model) and 86.6 F1 (ensemble) on the test data, outperforming the corresponding strong baselines with ELMo by 0.8 and 1.0, respectively. Detailed error analysis are conducted to gain more insights on the investigated approaches.

</details>

<details>

<summary>2019-07-22 13:39:33 - Deep recommender engine based on efficient product embeddings neural pipeline</summary>

- *Laurentiu Piciu, Andrei Damian, Nicolae Tapus, Andrei Simion-Constantinescu, Bogdan Dumitrescu*

- `1903.09942v2` - [abs](http://arxiv.org/abs/1903.09942v2) - [pdf](http://arxiv.org/pdf/1903.09942v2)

> Predictive analytics systems are currently one of the most important areas of research and development within the Artificial Intelligence domain and particularly in Machine Learning. One of the "holy grails" of predictive analytics is the research and development of the "perfect" recommendation system. In our paper, we propose an advanced pipeline model for the multi-task objective of determining product complementarity, similarity and sales prediction using deep neural models applied to big-data sequential transaction systems. Our highly parallelized hybrid model pipeline consists of both unsupervised and supervised models, used for the objectives of generating semantic product embeddings and predicting sales, respectively. Our experimentation and benchmarking processes have been done using pharma industry retail real-life transactional Big-Data streams.

</details>

<details>

<summary>2019-07-22 14:33:43 - VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions</summary>

- *Pranava Madhyastha, Josiah Wang, Lucia Specia*

- `1907.09340v1` - [abs](http://arxiv.org/abs/1907.09340v1) - [pdf](http://arxiv.org/pdf/1907.09340v1)

> We address the task of evaluating image description generation systems. We propose a novel image-aware metric for this task: VIFIDEL. It estimates the faithfulness of a generated caption with respect to the content of the actual image, based on the semantic similarity between labels of objects depicted in images and words in the description. The metric is also able to take into account the relative importance of objects mentioned in human reference descriptions during evaluation. Even if these human reference descriptions are not available, VIFIDEL can still reliably evaluate system descriptions. The metric achieves high correlation with human judgments on two well-known datasets and is competitive with metrics that depend on human references

</details>

<details>

<summary>2019-07-22 16:37:36 - Context-Aware Convolutional Neural Network for Grading of Colorectal Cancer Histology Images</summary>

- *Muhammad Shaban, Ruqayya Awan, Muhammad Moazam Fraz, Ayesha Azam, David Snead, Nasir M. Rajpoot*

- `1907.09478v1` - [abs](http://arxiv.org/abs/1907.09478v1) - [pdf](http://arxiv.org/pdf/1907.09478v1)

> Digital histology images are amenable to the application of convolutional neural network (CNN) for analysis due to the sheer size of pixel data present in them. CNNs are generally used for representation learning from small image patches (e.g. 224x224) extracted from digital histology images due to computational and memory constraints. However, this approach does not incorporate high-resolution contextual information in histology images. We propose a novel way to incorporate larger context by a context-aware neural network based on images with a dimension of 1,792x1,792 pixels. The proposed framework first encodes the local representation of a histology image into high dimensional features then aggregates the features by considering their spatial organization to make a final prediction. The proposed method is evaluated for colorectal cancer grading and breast cancer classification. A comprehensive analysis of some variants of the proposed method is presented. Our method outperformed the traditional patch-based approaches, problem-specific methods, and existing context-based methods quantitatively by a margin of 3.61%. Code and dataset related information is available at this link: https://tia-lab.github.io/Context-Aware-CNN

</details>

<details>

<summary>2019-07-22 16:53:33 - Paracoherent Answer Set Semantics meets Argumentation Frameworks</summary>

- *Giovanni Amendola, Francesco Ricca*

- `1907.09426v1` - [abs](http://arxiv.org/abs/1907.09426v1) - [pdf](http://arxiv.org/pdf/1907.09426v1)

> In the last years, abstract argumentation has met with great success in AI, since it has served to capture several non-monotonic logics for AI. Relations between argumentation framework (AF) semantics and logic programming ones are investigating more and more. In particular, great attention has been given to the well-known stable extensions of an AF, that are closely related to the answer sets of a logic program. However, if a framework admits a small incoherent part, no stable extension can be provided. To overcome this shortcoming, two semantics generalizing stable extensions have been studied, namely semi-stable and stage. In this paper, we show that another perspective is possible on incoherent AFs, called paracoherent extensions, as they have a counterpart in paracoherent answer set semantics. We compare this perspective with semi-stable and stage semantics, by showing that computational costs remain unchanged, and moreover an interesting symmetric behaviour is maintained. Under consideration for acceptance in TPLP.

</details>

<details>

<summary>2019-07-22 18:57:14 - Maximizing Stylistic Control and Semantic Accuracy in NLG: Personality Variation and Discourse Contrast</summary>

- *Vrindavan Harrison, Lena Reed, Shereen Oraby, Marilyn Walker*

- `1907.09527v1` - [abs](http://arxiv.org/abs/1907.09527v1) - [pdf](http://arxiv.org/pdf/1907.09527v1)

> Neural generation methods for task-oriented dialogue typically generate from a meaning representation that is populated using a database of domain information, such as a table of data describing a restaurant. While earlier work focused solely on the semantic fidelity of outputs, recent work has started to explore methods for controlling the style of the generated text while simultaneously achieving semantic accuracy. Here we experiment with two stylistic benchmark tasks, generating language that exhibits variation in personality, and generating discourse contrast. We report a huge performance improvement in both stylistic control and semantic accuracy over the state of the art on both of these benchmarks. We test several different models and show that putting stylistic conditioning in the decoder and eliminating the semantic re-ranker used in earlier models results in more than 15 points higher BLEU for Personality, with a reduction of semantic error to near zero. We also report an improvement from .75 to .81 in controlling contrast and a reduction in semantic error from 16% to 2%.

</details>

<details>

<summary>2019-07-22 19:54:20 - On the Equivalence Between Abstract Dialectical Frameworks and Logic Programs</summary>

- *João Alcântara, Samy Sá, Juan Acosta-Guadarrama*

- `1907.09548v1` - [abs](http://arxiv.org/abs/1907.09548v1) - [pdf](http://arxiv.org/pdf/1907.09548v1)

> Abstract Dialectical Frameworks (ADFs) are argumentation frameworks where each node is associated with an acceptance condition. This allows us to model different types of dependencies as supports and attacks. Previous studies provided a translation from Normal Logic Programs (NLPs) to ADFs and proved the stable models semantics for a normal logic program has an equivalent semantics to that of the corresponding ADF. However, these studies failed in identifying a semantics for ADFs equivalent to a three-valued semantics (as partial stable models and well-founded models) for NLPs. In this work, we focus on a fragment of ADFs, called Attacking Dialectical Frameworks (ADF$^+$s), and provide a translation from NLPs to ADF$^+$s robust enough to guarantee the equivalence between partial stable models, well-founded models, regular models, stable models semantics for NLPs and respectively complete models, grounded models, preferred models, stable models for ADFs. In addition, we define a new semantics for ADF$^+$s, called L-stable, and show it is equivalent to the L-stable semantics for NLPs. This paper is under consideration for acceptance in TPLP.

</details>

<details>

<summary>2019-07-22 20:27:43 - Better Paracoherent Answer Sets with Less Resources</summary>

- *Giovanni Amendola, Carmine Dodaro, Francesco Ricca*

- `1907.09560v1` - [abs](http://arxiv.org/abs/1907.09560v1) - [pdf](http://arxiv.org/pdf/1907.09560v1)

> Answer Set Programming (ASP) is a well-established formalism for logic programming. Problem solving in ASP requires to write an ASP program whose answers sets correspond to solutions. Albeit the non-existence of answer sets for some ASP programs can be considered as a modeling feature, it turns out to be a weakness in many other cases, and especially for query answering. Paracoherent answer set semantics extend the classical semantics of ASP to draw meaningful conclusions also from incoherent programs, with the result of increasing the range of applications of ASP. State of the art implementations of paracoherent ASP adopt the semi-equilibrium semantics, but cannot be lifted straightforwardly to compute efficiently the (better) split semi-equilibrium semantics that discards undesirable semi-equilibrium models. In this paper an efficient evaluation technique for computing a split semi-equilibrium model is presented. An experiment on hard benchmarks shows that better paracoherent answer sets can be computed consuming less computational resources than existing methods. Under consideration for acceptance in TPLP.

</details>

<details>

<summary>2019-07-22 23:43:34 - Analysing user identity via time-sensitive semantic edit distance (t-SED): A case study of Russian trolls on Twitter</summary>

- *Dongwoo Kim, Timothy Graham, Zimin Wan, Marian-Andrei Rizoiu*

- `1901.05228v2` - [abs](http://arxiv.org/abs/1901.05228v2) - [pdf](http://arxiv.org/pdf/1901.05228v2)

> In the digital era, individuals are increasingly profiled and grouped based on the traces they leave behind in online social networks such as Twitter and Facebook. In this paper, we develop and evaluate a novel text analysis approach for studying user identity and social roles by redefining identity as a sequence of timestamped items (e.g. tweet texts). We operationalise this idea by developing a novel text distance metric, the time-sensitive semantic edit distance (t-SED), which accounts for the temporal context across multiple traces. To evaluate this method we undertake a case study of Russian online-troll activity within US political discourse. The novel metric allows us to classify the social roles of trolls based on their traces, in this case tweets, into one of the predefined categories left-leaning, right-leaning, and news feed. We show the effectiveness of the t-SED metric to measure the similarities between tweets while accounting for the temporal context, and we use novel data visualisation techniques and qualitative analysis to uncover new empirical insights into Russian troll activity that have not been identified in previous work. Additionally, we highlight a connection with the field of Actor-Network Theory and the related hypotheses of Gabriel Tarde, and we discuss how social sequence analysis using t-SED may provide new avenues for tackling a longstanding problem in social theory: how to analyse society without separating reality into micro versus macro levels.

</details>

<details>

<summary>2019-07-23 04:30:12 - Improving Malaria Parasite Detection from Red Blood Cell using Deep Convolutional Neural Networks</summary>

- *Aimon Rahman, Hasib Zunair, M Sohel Rahman, Jesia Quader Yuki, Sabyasachi Biswas, Md Ashraful Alam, Nabila Binte Alam, M. R. C. Mahdy*

- `1907.10418v1` - [abs](http://arxiv.org/abs/1907.10418v1) - [pdf](http://arxiv.org/pdf/1907.10418v1)

> Malaria is a female anopheles mosquito-bite inflicted life-threatening disease which is considered endemic in many parts of the world. This article focuses on improving malaria detection from patches segmented from microscopic images of red blood cell smears by introducing a deep convolutional neural network. Compared to the traditional methods that use tedious hand engineering feature extraction, the proposed method uses deep learning in an end-to-end arrangement that performs both feature extraction and classification directly from the raw segmented patches of the red blood smears. The dataset used in this study was taken from National Institute of Health named NIH Malaria Dataset. The evaluation metric accuracy and loss along with 5-fold cross validation was used to compare and select the best performing architecture. To maximize the performance, existing standard pre-processing techniques from the literature has also been experimented. In addition, several other complex architectures have been implemented and tested to pick the best performing model. A holdout test has also been conducted to verify how well the proposed model generalizes on unseen data. Our best model achieves an accuracy of almost 97.77%.

</details>

<details>

<summary>2019-07-23 07:55:29 - EmoBed: Strengthening Monomodal Emotion Recognition via Training with Crossmodal Emotion Embeddings</summary>

- *Jing Han, Zixing Zhang, Zhao Ren, Björn Schuller*

- `1907.10428v1` - [abs](http://arxiv.org/abs/1907.10428v1) - [pdf](http://arxiv.org/pdf/1907.10428v1)

> Despite remarkable advances in emotion recognition, they are severely restrained from either the essentially limited property of the employed single modality, or the synchronous presence of all involved multiple modalities. Motivated by this, we propose a novel crossmodal emotion embedding framework called EmoBed, which aims to leverage the knowledge from other auxiliary modalities to improve the performance of an emotion recognition system at hand. The framework generally includes two main learning components, i. e., joint multimodal training and crossmodal training. Both of them tend to explore the underlying semantic emotion information but with a shared recognition network or with a shared emotion embedding space, respectively. In doing this, the enhanced system trained with this approach can efficiently make use of the complementary information from other modalities. Nevertheless, the presence of these auxiliary modalities is not demanded during inference. To empirically investigate the effectiveness and robustness of the proposed framework, we perform extensive experiments on the two benchmark databases RECOLA and OMG-Emotion for the tasks of dimensional emotion regression and categorical emotion classification, respectively. The obtained results show that the proposed framework significantly outperforms related baselines in monomodal inference, and are also competitive or superior to the recently reported systems, which emphasises the importance of the proposed crossmodal learning for emotion recognition.

</details>

<details>

<summary>2019-07-23 14:56:03 - Reservoir-size dependent learning in analogue neural networks</summary>

- *Xavier Porte, Louis Andreoli, Maxime Jacquot, Laurent Larger, Daniel Brunner*

- `1908.08021v1` - [abs](http://arxiv.org/abs/1908.08021v1) - [pdf](http://arxiv.org/pdf/1908.08021v1)

> The implementation of artificial neural networks in hardware substrates is a major interdisciplinary enterprise. Well suited candidates for physical implementations must combine nonlinear neurons with dedicated and efficient hardware solutions for both connectivity and training. Reservoir computing addresses the problems related with the network connectivity and training in an elegant and efficient way. However, important questions regarding impact of reservoir size and learning routines on the convergence-speed during learning remain unaddressed. Here, we study in detail the learning process of a recently demonstrated photonic neural network based on a reservoir. We use a greedy algorithm to train our neural network for the task of chaotic signals prediction and analyze the learning-error landscape. Our results unveil fundamental properties of the system's optimization hyperspace. Particularly, we determine the convergence speed of learning as a function of reservoir size and find exceptional, close to linear scaling. This linear dependence, together with our parallel diffractive coupling, represent optimal scaling conditions for our photonic neural network scheme.

</details>

<details>

<summary>2019-07-23 15:49:20 - Semantic Web for Machine Translation: Challenges and Directions</summary>

- *Diego Moussallem, Matthias Wauer, Axel-Cyrille Ngonga Ngomo*

- `1907.10676v1` - [abs](http://arxiv.org/abs/1907.10676v1) - [pdf](http://arxiv.org/pdf/1907.10676v1)

> A large number of machine translation approaches have recently been developed to facilitate the fluid migration of content across languages. However, the literature suggests that many obstacles must still be dealt with to achieve better automatic translations. One of these obstacles is lexical and syntactic ambiguity. A promising way of overcoming this problem is using Semantic Web technologies. This article is an extended abstract of our systematic review on machine translation approaches that rely on Semantic Web technologies for improving the translation of texts. Overall, we present the challenges and opportunities in the use of Semantic Web technologies in Machine Translation. Moreover, our research suggests that while Semantic Web technologies can enhance the quality of machine translation outputs for various problems, the combination of both is still in its infancy.

</details>

<details>

<summary>2019-07-23 16:15:04 - Beyond content analysis: Detecting targeted ads via distributed counting</summary>

- *Costas Iordanou, Nicolas Kourtellis, Juan Miguel Carrascosa, Claudio Soriente, Ruben Cuevas, Nikolaos Laoutaris*

- `1907.01862v2` - [abs](http://arxiv.org/abs/1907.01862v2) - [pdf](http://arxiv.org/pdf/1907.01862v2)

> Being able to check whether an online advertisement has been targeted is essential for resolving privacy controversies and implementing in practice data protection regulations like GDPR, CCPA, and COPPA. In this paper we describe the design, implementation, and deployment of an advertisement auditing system called iWnder that uses crowdsourcing to reveal in real time whether a display advertisement has been targeted or not. Crowdsourcing simplifies the detection of targeted advertising, but requires reporting to a central repository the impressions seen by different users, thereby jeopardising their privacy. We break this deadlock with a privacy preserving data sharing protocol that allows iWnder to compute global statistics required to detect targeting, while keeping the advertisements seen by individual users and their browsing history private. We conduct a simulation study to explore the effect of different parameters and a live validation to demonstrate the accuracy of our approach. Unlike previous solutions, iWnder can even detect indirect targeting, i.e., marketing campaigns that promote a product or service whose description bears no semantic overlap with its targeted audience.

</details>

<details>

<summary>2019-07-23 20:45:41 - Structured Knowledge Discovery from Massive Text Corpus</summary>

- *Chenwei Zhang*

- `1908.01837v1` - [abs](http://arxiv.org/abs/1908.01837v1) - [pdf](http://arxiv.org/pdf/1908.01837v1)

> Nowadays, with the booming development of the Internet, people benefit from its convenience due to its open and sharing nature. A large volume of natural language texts is being generated by users in various forms, such as search queries, documents, and social media posts. As the unstructured text corpus is usually noisy and messy, it becomes imperative to correctly identify and accurately annotate structured information in order to obtain meaningful insights or better understand unstructured texts. On the other hand, the existing structured information, which embodies our knowledge such as entity or concept relations, often suffers from incompleteness or quality-related issues. Given a gigantic collection of texts which offers rich semantic information, it is also important to harness the massiveness of the unannotated text corpus to expand and refine existing structured knowledge with fewer annotation efforts.   In this dissertation, I will introduce principles, models, and algorithms for effective structured knowledge discovery from the massive text corpus. We are generally interested in obtaining insights and better understanding unstructured texts with the help of structured annotations or by structure-aware modeling. Also, given the existing structured knowledge, we are interested in expanding its scale and improving its quality harnessing the massiveness of the text corpus. In particular, four problems are studied in this dissertation: Structured Intent Detection for Natural Language Understanding, Structure-aware Natural Language Modeling, Generative Structured Knowledge Expansion, and Synonym Refinement on Structured Knowledge.

</details>

<details>

<summary>2019-07-24 06:54:04 - Interpretable Classification of Time-Series Data using Efficient Enumerative Techniques</summary>

- *Sara Mohammadinejad, Jyotirmoy V. Deshmukh, Aniruddh G. Puranic, Marcell Vazquez-Chanlatte, Alexandre Donzé*

- `1907.10265v1` - [abs](http://arxiv.org/abs/1907.10265v1) - [pdf](http://arxiv.org/pdf/1907.10265v1)

> Cyber-physical system applications such as autonomous vehicles, wearable devices, and avionic systems generate a large volume of time-series data. Designers often look for tools to help classify and categorize the data. Traditional machine learning techniques for time-series data offer several solutions to solve these problems; however, the artifacts trained by these algorithms often lack interpretability. On the other hand, temporal logics, such as Signal Temporal Logic (STL) have been successfully used in the formal methods community as specifications of time-series behaviors. In this work, we propose a new technique to automatically learn temporal logic formulae that are able to cluster and classify real-valued time-series data. Previous work on learning STL formulas from data either assumes a formula-template to be given by the user, or assumes some special fragment of STL that enables exploring the formula structure in a systematic fashion. In our technique, we relax these assumptions, and provide a way to systematically explore the space of all STL formulas. As the space of all STL formulas is very large, and contains many semantically equivalent formulas, we suggest a technique to heuristically prune the space of formulas considered. Finally, we illustrate our technique on various case studies from the automotive, transportation and healthcare domain.

</details>

<details>

<summary>2019-07-24 10:12:36 - Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge</summary>

- *Ondřej Dušek, Jekaterina Novikova, Verena Rieser*

- `1901.07931v3` - [abs](http://arxiv.org/abs/1901.07931v3) - [pdf](http://arxiv.org/pdf/1901.07931v3)

> This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures -- with the majority implementing sequence-to-sequence models (seq2seq) -- as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness -- with the winning SLUG system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper.

</details>

<details>

<summary>2019-07-24 14:30:46 - SPECTECTOR: Principled Detection of Speculative Information Flows</summary>

- *Marco Guarnieri, Boris Köpf, José F. Morales, Jan Reineke, Andrés Sánchez*

- `1812.08639v2` - [abs](http://arxiv.org/abs/1812.08639v2) - [pdf](http://arxiv.org/pdf/1812.08639v2)

> Since the advent of SPECTRE, a number of countermeasures have been proposed and deployed. Rigorously reasoning about their effectiveness, however, requires a well-defined notion of security against speculative execution attacks, which has been missing until now. In this paper (1) we put forward speculative non-interference, the first semantic notion of security against speculative execution attacks, and (2) we develop SPECTECTOR, an algorithm based on symbolic execution to automatically prove speculative non-interference, or to detect violations. We implement SPECTECTOR in a tool, which we use to detect subtle leaks and optimizations opportunities in the way major compilers place SPECTRE countermeasures. A scalability analysis indicates that checking speculative non-interference does not exhibit fundamental bottlenecks beyond those inherited by symbolic execution.

</details>

<details>

<summary>2019-07-24 15:57:19 - Semantic Parsing with Dual Learning</summary>

- *Ruisheng Cao, Su Zhu, Chen Liu, Jieyu Li, Kai Yu*

- `1907.05343v2` - [abs](http://arxiv.org/abs/1907.05343v2) - [pdf](http://arxiv.org/pdf/1907.05343v2)

> Semantic parsing converts natural language queries into structured logical forms. The paucity of annotated training samples is a fundamental challenge in this field. In this work, we develop a semantic parsing framework with the dual learning algorithm, which enables a semantic parser to make full use of data (labeled and even unlabeled) through a dual-learning game. This game between a primal model (semantic parsing) and a dual model (logical form to query) forces them to regularize each other, and can achieve feedback signals from some prior-knowledge. By utilizing the prior-knowledge of logical form structures, we propose a novel reward signal at the surface and semantic levels which tends to generate complete and reasonable logical forms. Experimental results show that our approach achieves new state-of-the-art performance on ATIS dataset and gets competitive performance on Overnight dataset.

</details>

<details>

<summary>2019-07-24 17:57:39 - The Virtual Patch Clamp: Imputing C. elegans Membrane Potentials from Calcium Imaging</summary>

- *Andrew Warrington, Arthur Spencer, Frank Wood*

- `1907.11075v1` - [abs](http://arxiv.org/abs/1907.11075v1) - [pdf](http://arxiv.org/pdf/1907.11075v1)

> We develop a stochastic whole-brain and body simulator of the nematode roundworm Caenorhabditis elegans (C. elegans) and show that it is sufficiently regularizing to allow imputation of latent membrane potentials from partial calcium fluorescence imaging observations. This is the first attempt we know of to "complete the circle," where an anatomically grounded whole-connectome simulator is used to impute a time-varying "brain" state at single-cell fidelity from covariates that are measurable in practice. The sequential Monte Carlo (SMC) method we employ not only enables imputation of said latent states but also presents a strategy for learning simulator parameters via variational optimization of the noisy model evidence approximation provided by SMC. Our imputation and parameter estimation experiments were conducted on distributed systems using novel implementations of the aforementioned techniques applied to synthetic data of dimension and type representative of that which are measured in laboratories currently.

</details>

<details>

<summary>2019-07-24 21:37:29 - Visual Interaction with Deep Learning Models through Collaborative Semantic Inference</summary>

- *Sebastian Gehrmann, Hendrik Strobelt, Robert Krüger, Hanspeter Pfister, Alexander M. Rush*

- `1907.10739v1` - [abs](http://arxiv.org/abs/1907.10739v1) - [pdf](http://arxiv.org/pdf/1907.10739v1)

> Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.

</details>

<details>

<summary>2019-07-25 06:35:49 - Machine learning and semantic analysis of in-game chat for cyberbullying</summary>

- *Shane Murnion, William J. Buchanan, Adrian Smales, Gordon Russell*

- `1907.10855v1` - [abs](http://arxiv.org/abs/1907.10855v1) - [pdf](http://arxiv.org/pdf/1907.10855v1)

> One major problem with cyberbullying research is the lack of data, since researchers are traditionally forced to rely on survey data where victims and perpetrators self-report their impressions. In this paper, an automatic data collection system is presented that continuously collects in-game chat data from one of the most popular online multi-player games: World of Tanks. The data was collected and combined with other information about the players from available online data services. It presents a scoring scheme to enable identification of cyberbullying based on current research. Classification of the collected data was carried out using simple feature detection with SQL database queries and compared to classification from AI-based sentiment text analysis services that have recently become available and further against manually classified data using a custom-built classification client built for this paper. The simple SQL classification proved to be quite useful at identifying some features of toxic chat such as the use of bad language or racist sentiments, however the classification by the more sophisticated online sentiment analysis services proved to be disappointing. The results were then examined for insights into cyberbullying within this game and it was shown that it should be possible to reduce cyberbullying within the World of Tanks game by a significant factor by simply freezing the player's ability to communicate through the in-game chat function for a short period after the player is killed within a match. It was also shown that very new players are much less likely to engage in cyberbullying, suggesting that it may be a learned behaviour from other players.

</details>

<details>

<summary>2019-07-25 09:19:30 - Applying Constraint Logic Programming to SQL Semantic Analysis</summary>

- *Fernando Sáenz-Pérez*

- `1907.10914v1` - [abs](http://arxiv.org/abs/1907.10914v1) - [pdf](http://arxiv.org/pdf/1907.10914v1)

> This paper proposes the use of Constraint Logic Programming (CLP) to model SQL queries in a data-independent abstract layer by focusing on some semantic properties for signalling possible errors in such queries. First, we define a translation from SQL to Datalog, and from Datalog to CLP, so that solving this CLP program will give information about inconsistency, tautology, and possible simplifications. We use different constraint domains which are mapped to SQL types, and propose them to cooperate for improving accuracy. Our approach leverages a deductive system that includes SQL and Datalog, and we present an implementation in this system which is currently being tested in classroom, showing its advantages and differences with respect to other approaches, as well as some performance data. This paper is under consideration for acceptance in TPLP.

</details>

<details>

<summary>2019-07-25 12:28:35 - Don't Worry About the Weather: Unsupervised Condition-Dependent Domain Adaptation</summary>

- *Horia Porav, Tom Bruls, Paul Newman*

- `1907.11004v1` - [abs](http://arxiv.org/abs/1907.11004v1) - [pdf](http://arxiv.org/pdf/1907.11004v1)

> Modern models that perform system-critical tasks such as segmentation and localization exhibit good performance and robustness under ideal conditions (i.e. daytime, overcast) but performance degrades quickly and often catastrophically when input conditions change. In this work, we present a domain adaptation system that uses light-weight input adapters to pre-processes input images, irrespective of their appearance, in a way that makes them compatible with off-the-shelf computer vision tasks that are trained only on inputs with ideal conditions. No fine-tuning is performed on the off-the-shelf models, and the system is capable of incrementally training new input adapters in a self-supervised fashion, using the computer vision tasks as supervisors, when the input domain differs significantly from previously seen domains. We report large improvements in semantic segmentation and topological localization performance on two popular datasets, RobotCar and BDD.

</details>

<details>

<summary>2019-07-25 13:45:48 - Grammatical Sequence Prediction for Real-Time Neural Semantic Parsing</summary>

- *Chunyang Xiao, Christoph Teichmann, Konstantine Arkoudas*

- `1907.11049v1` - [abs](http://arxiv.org/abs/1907.11049v1) - [pdf](http://arxiv.org/pdf/1907.11049v1)

> While sequence-to-sequence (seq2seq) models achieve state-of-the-art performance in many natural language processing tasks, they can be too slow for real-time applications. One performance bottleneck is predicting the most likely next token over a large vocabulary; methods to circumvent this bottleneck are a current research topic. We focus specifically on using seq2seq models for semantic parsing, where we observe that grammars often exist which specify valid formal representations of utterance semantics. By developing a generic approach for restricting the predictions of a seq2seq model to grammatically permissible continuations, we arrive at a widely applicable technique for speeding up semantic parsing. The technique leads to a 74% speed-up on an in-house dataset with a large vocabulary, compared to the same neural model without grammatical restrictions.

</details>

<details>

<summary>2019-07-25 16:45:06 - HEIDL: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop</summary>

- *Yiwei Yang, Eser Kandogan, Yunyao Li, Walter S. Lasecki, Prithviraj Sen*

- `1907.11184v1` - [abs](http://arxiv.org/abs/1907.11184v1) - [pdf](http://arxiv.org/pdf/1907.11184v1)

> While the role of humans is increasingly recognized in machine learning community, representation of and interaction with models in current human-in-the-loop machine learning (HITL-ML) approaches are too low-level and far-removed from human's conceptual models. We demonstrate HEIDL, a prototype HITL-ML system that exposes the machine-learned model through high-level, explainable linguistic expressions formed of predicates representing semantic structure of text. In HEIDL, human's role is elevated from simply evaluating model predictions to interpreting and even updating the model logic directly by enabling interaction with rule predicates themselves. Raising the currency of interaction to such semantic levels calls for new interaction paradigms between humans and machines that result in improved productivity for text analytics model development process. Moreover, by involving humans in the process, the human-machine co-created models generalize better to unseen data as domain experts are able to instill their expertise by extrapolating from what has been learned by automated algorithms from few labelled data.

</details>

<details>

<summary>2019-07-25 22:13:24 - Probabilistic Approximate Logic and its Implementation in the Logical Imagination Engine</summary>

- *Mark-Oliver Stehr, Minyoung Kim, Carolyn L. Talcott, Merrill Knapp, Akos Vertes*

- `1907.11321v1` - [abs](http://arxiv.org/abs/1907.11321v1) - [pdf](http://arxiv.org/pdf/1907.11321v1)

> In spite of the rapidly increasing number of applications of machine learning in various domains, a principled and systematic approach to the incorporation of domain knowledge in the engineering process is still lacking and ad hoc solutions that are difficult to validate are still the norm in practice, which is of growing concern not only in mission-critical applications.   In this note, we introduce Probabilistic Approximate Logic (PALO) as a logic based on the notion of mean approximate probability to overcome conceptual and computational difficulties inherent to strictly probabilistic logics. The logic is approximate in several dimensions. Logical independence assumptions are used to obtain approximate probabilities, but by averaging over many instances of formulas a useful estimate of mean probability with known confidence can usually be obtained. To enable efficient computational inference, the logic has a continuous semantics that reflects only a subset of the structural properties of classical logic, but this imprecision can be partly compensated by richer theories obtained by classical inference or other means. Computational inference, which refers to the construction of models and validation of logical properties, is based on Stochastic Gradient Descent (SGD) and Markov Chain Monte Carlo (MCMC) techniques and hence another dimension where approximations are involved.   We also present the Logical Imagination Engine (LIME), a prototypical implementation of PALO based on TensorFlow. Albeit not limited to the biological domain, we illustrate its operation in a quite substantial bioinformatics machine learning application concerned with network synthesis and analysis in a recent DARPA project.

</details>

<details>

<summary>2019-07-25 23:10:21 - Object as Distribution</summary>

- *Li Ding, Lex Fridman*

- `1907.12929v1` - [abs](http://arxiv.org/abs/1907.12929v1) - [pdf](http://arxiv.org/pdf/1907.12929v1)

> Object detection is a critical part of visual scene understanding. The representation of the object in the detection task has important implications on the efficiency and feasibility of annotation, robustness to occlusion, pose, lighting, and other visual sources of semantic uncertainty, and effectiveness in real-world applications (e.g., autonomous driving). Popular object representations include 2D and 3D bounding boxes, polygons, splines, pixels, and voxels. Each have their strengths and weakness. In this work, we propose a new representation of objects based on the bivariate normal distribution. This distribution-based representation has the benefit of robust detection of highly-overlapping objects and the potential for improved downstream tracking and instance segmentation tasks due to the statistical representation of object edges. We provide qualitative evaluation of this representation for the object detection task and quantitative evaluation of its use in a baseline algorithm for the instance segmentation task.

</details>

<details>

<summary>2019-07-26 07:39:17 - Key Instance Selection for Unsupervised Video Object Segmentation</summary>

- *Donghyeon Cho, Sungeun Hong, Sungil Kang, Jiwon Kim*

- `1906.07851v2` - [abs](http://arxiv.org/abs/1906.07851v2) - [pdf](http://arxiv.org/pdf/1906.07851v2)

> This paper proposes key instance selection based on video saliency covering objectness and dynamics for unsupervised video object segmentation (UVOS). Our method takes frames sequentially and extracts object proposals with corresponding masks for each frame. We link objects according to their similarity until the M-th frame and then assign them unique IDs (i.e., instances). Similarity measure takes into account multiple properties such as ReID descriptor, expected trajectory, and semantic co-segmentation result. After M-th frame, we select K IDs based on video saliency and frequency of appearance; then only these key IDs are tracked through the remaining frames. Thanks to these technical contributions, our results are ranked third on the leaderboard of UVOS DAVIS challenge.

</details>

<details>

<summary>2019-07-26 08:42:38 - Semantic Deep Intermodal Feature Transfer: Transferring Feature Descriptors Between Imaging Modalities</summary>

- *Sebastian P. Kleinschmidt, Bernardo Wagner*

- `1907.11436v1` - [abs](http://arxiv.org/abs/1907.11436v1) - [pdf](http://arxiv.org/pdf/1907.11436v1)

> Under difficult environmental conditions, the view of RGB cameras may be restricted by fog, dust or difficult lighting situations. Because thermal cameras visualize thermal radiation, they are not subject to the same limitations as RGB cameras. However, because RGB and thermal imaging differ significantly in appearance, common, state-of-the-art feature descriptors are unsuitable for intermodal feature matching between these imaging modalities. As a consequence, visual maps created with an RGB camera can currently not be used for localization using a thermal camera. In this paper, we introduce the Semantic Deep Intermodal Feature Transfer (Se-DIFT), an approach for transferring image feature descriptors from the visual to the thermal spectrum and vice versa. For this purpose, we predict potential feature appearance in varying imaging modalities using a deep convolutional encoder-decoder architecture in combination with a global feature vector. Since the representation of a thermal image is not only affected by features which can be extracted from an RGB image, we introduce the global feature vector which augments the auto encoder's coding. The global feature vector contains additional information about the thermal history of a scene which is automatically extracted from external data sources. By augmenting the encoder's coding, we decrease the L1 error of the prediction by more than 7% compared to the prediction of a traditional U-Net architecture. To evaluate our approach, we match image feature descriptors detected in RGB and thermal images using Se-DIFT. Subsequently, we make a competitive comparison on the intermodal transferability of SIFT, SURF, and ORB features using our approach.

</details>

<details>

<summary>2019-07-26 13:08:36 - Bayesian Volumetric Autoregressive generative models for better semisupervised learning</summary>

- *Guilherme Pombo, Robert Gray, Tom Varsavsky, John Ashburner, Parashkev Nachev*

- `1907.11559v1` - [abs](http://arxiv.org/abs/1907.11559v1) - [pdf](http://arxiv.org/pdf/1907.11559v1)

> Deep generative models are rapidly gaining traction in medical imaging. Nonetheless, most generative architectures struggle to capture the underlying probability distributions of volumetric data, exhibit convergence problems, and offer no robust indices of model uncertainty. By comparison, the autoregressive generative model PixelCNN can be extended to volumetric data with relative ease, it readily attempts to learn the true underlying probability distribution and it still admits a Bayesian reformulation that provides a principled framework for reasoning about model uncertainty. Our contributions in this paper are two fold: first, we extend PixelCNN to work with volumetric brain magnetic resonance imaging data. Second, we show that reformulating this model to approximate a deep Gaussian process yields a measure of uncertainty that improves the performance of semi-supervised learning, in particular classification performance in settings where the proportion of labelled data is low. We quantify this improvement across classification, regression, and semantic segmentation tasks, training and testing on clinical magnetic resonance brain imaging data comprising T1-weighted and diffusion-weighted sequences.

</details>

<details>

<summary>2019-07-26 15:29:46 - Multi-Stage Prediction Networks for Data Harmonization</summary>

- *Stefano B. Blumberg, Marco Palombo, Can Son Khoo, Chantal M. W. Tax, Ryutaro Tanno, Daniel C. Alexander*

- `1907.11629v1` - [abs](http://arxiv.org/abs/1907.11629v1) - [pdf](http://arxiv.org/pdf/1907.11629v1)

> In this paper, we introduce multi-task learning (MTL) to data harmonization (DH); where we aim to harmonize images across different acquisition platforms and sites. This allows us to integrate information from multiple acquisitions and improve the predictive performance and learning efficiency of the harmonization model. Specifically, we introduce the Multi Stage Prediction (MSP) Network, a MTL framework that incorporates neural networks of potentially disparate architectures, trained for different individual acquisition platforms, into a larger architecture that is refined in unison. The MSP utilizes high-level features of single networks for individual tasks, as inputs of additional neural networks to inform the final prediction, therefore exploiting redundancy across tasks to make the most of limited training data. We validate our methods on a dMRI harmonization challenge dataset, where we predict three modern platform types, from one obtained from an old scanner. We show how MTL architectures, such as the MSP, produce around 20\% improvement of patch-based mean-squared error over current state-of-the-art methods and that our MSP outperforms off-the-shelf MTL networks. Our code is available https://github.com/sbb-gh/ .

</details>

<details>

<summary>2019-07-26 17:24:12 - Learning to Groove with Inverse Sequence Transformations</summary>

- *Jon Gillick, Adam Roberts, Jesse Engel, Douglas Eck, David Bamman*

- `1905.06118v2` - [abs](http://arxiv.org/abs/1905.06118v2) - [pdf](http://arxiv.org/pdf/1905.06118v2)

> We explore models for translating abstract musical ideas (scores, rhythms) into expressive performances using Seq2Seq and recurrent Variational Information Bottleneck (VIB) models. Though Seq2Seq models usually require painstakingly aligned corpora, we show that it is possible to adapt an approach from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix (Isola et al., 2017) and Vid2Vid (Wang et al. 2018a)) to sequences, creating large volumes of paired data by performing simple transformations and training generative models to plausibly invert these transformations. Music, and drumming in particular, provides a strong test case for this approach because many common transformations (quantization, removing voices) have clear semantics, and models for learning to invert them have real-world applications. Focusing on the case of drum set players, we create and release a new dataset for this purpose, containing over 13 hours of recordings by professional drummers aligned with fine-grained timing and dynamics information. We also explore some of the creative potential of these models, including demonstrating improvements on state-of-the-art methods for Humanization (instantiating a performance from a musical score).

</details>

<details>

<summary>2019-07-26 18:14:45 - Meaning to Form: Measuring Systematicity as Information</summary>

- *Tiago Pimentel, Arya D. McCarthy, Damián E. Blasi, Brian Roark, Ryan Cotterell*

- `1906.05906v2` - [abs](http://arxiv.org/abs/1906.05906v2) - [pdf](http://arxiv.org/pdf/1906.05906v2)

> A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram \textit{gl} have any systematic relationship to the meaning of words like \textit{glisten}, \textit{gleam} and \textit{glow}? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small---despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language.

</details>

<details>

<summary>2019-07-26 18:31:06 - Short-Term Prediction and Multi-Camera Fusion on Semantic Grids</summary>

- *Lukas Hoyer, Patrick Kesper, Anna Khoreva, Volker Fischer*

- `1903.08960v2` - [abs](http://arxiv.org/abs/1903.08960v2) - [pdf](http://arxiv.org/pdf/1903.08960v2)

> An environment representation (ER) is a substantial part of every autonomous system. It introduces a common interface between perception and other system components, such as decision making, and allows downstream algorithms to deal with abstracted data without knowledge of the used sensor. In this work, we propose and evaluate a novel architecture that generates an egocentric, grid-based, predictive, and semantically-interpretable ER. In particular, we provide a proof of concept for the spatio-temporal fusion of multiple camera sequences and short-term prediction in such an ER. Our design utilizes a strong semantic segmentation network together with depth and egomotion estimates to first extract semantic information from multiple camera streams and then transform these separately into egocentric temporally-aligned bird's-eye view grids. A deep encoder-decoder network is trained to fuse a stack of these grids into a unified semantic grid representation and to predict the dynamics of its surrounding. We evaluate this representation on real-world sequences of the Cityscapes dataset and show that our architecture can make accurate predictions in complex sensor fusion scenarios and significantly outperforms a model-driven baseline in a category-based evaluation.

</details>

<details>

<summary>2019-07-26 23:28:30 - Scalable Source Code Similarity Detection in Large Code Repositories</summary>

- *F Alomari, M Harbi*

- `1907.11817v1` - [abs](http://arxiv.org/abs/1907.11817v1) - [pdf](http://arxiv.org/pdf/1907.11817v1)

> Source code similarity are increasingly used in application development to identify clones, isolate bugs, and find copy-rights violations. Similar code fragments can be very problematic due to the fact that errors in the original code must be fixed in every copy. Other maintenance changes, such as extensions or patches, must be applied multiple times. Furthermore, the diversity of coding styles and flexibility of modern languages makes it difficult and cost ineffective to manually inspect large code repositories. Therefore, detection is only feasible by automatic techniques. We present an efficient and scalable approach for similar code fragment identification based on source code control flow graphs fingerprinting. The source code is processed to generate control flow graphs that are then hashed to create a unique fingerprint of the code capturing semantics as well as syntax similarity. The fingerprints can then be efficiently stored and retrieved to perform similarity search between code fragments. Experimental results from our prototype implementation supports the validity of our approach and show its effectiveness and efficiency in comparison with other solutions.

</details>

<details>

<summary>2019-07-27 00:01:31 - Configuration Testing: Testing Configuration Values as Code and with Code</summary>

- *Tianyin Xu, Owolabi Legunsen*

- `1905.12195v2` - [abs](http://arxiv.org/abs/1905.12195v2) - [pdf](http://arxiv.org/pdf/1905.12195v2)

> This paper proposes configuration testing--evaluating configuration values (to be deployed) by exercising the code that uses the values and assessing the corresponding program behavior. We advocate that configuration values should be systematically tested like software code and that configuration testing should be a key reliability engineering practice for preventing misconfigurations from production deployment.   The essential advantage of configuration testing is to put the configuration values (to be deployed) in the context of the target software program under test. In this way, the dynamic effects of configuration values and the impact of configuration changes can be observed during testing. Configuration testing overcomes the fundamental limitations of de facto approaches to combatting misconfigurations, namely configuration validation and software testing--the former is disconnected from code logic and semantics, while the latter can hardly cover all possible configuration values and their combinations. Our preliminary results show the effectiveness of configuration testing in capturing real-world misconfigurations.   We present the principles of writing new configuration tests and the promises of retrofitting existing software tests to be configuration tests. We discuss new adequacy and quality metrics for configuration testing. We also explore regression testing techniques to enable incremental configuration testing during continuous integration and deployment in modern software systems.

</details>

<details>

<summary>2019-07-27 06:55:24 - Many could be better than all: A novel instance-oriented algorithm for Multi-modal Multi-label problem</summary>

- *Yi Zhang, Cheng Zeng, Hao Cheng, Chongjun Wang, Lei Zhang*

- `1907.11857v1` - [abs](http://arxiv.org/abs/1907.11857v1) - [pdf](http://arxiv.org/pdf/1907.11857v1)

> With the emergence of diverse data collection techniques, objects in real applications can be represented as multi-modal features. What's more, objects may have multiple semantic meanings. Multi-modal and Multi-label (MMML) problem becomes a universal phenomenon. The quality of data collected from different channels are inconsistent and some of them may not benefit for prediction. In real life, not all the modalities are needed for prediction. As a result, we propose a novel instance-oriented Multi-modal Classifier Chains (MCC) algorithm for MMML problem, which can make convince prediction with partial modalities. MCC extracts different modalities for different instances in the testing phase. Extensive experiments are performed on one real-world herbs dataset and two public datasets to validate our proposed algorithm, which reveals that it may be better to extract many instead of all of the modalities at hand.

</details>

<details>

<summary>2019-07-27 08:03:46 - Selfie: Self-supervised Pretraining for Image Embedding</summary>

- *Trieu H. Trinh, Minh-Thang Luong, Quoc V. Le*

- `1906.02940v3` - [abs](http://arxiv.org/abs/1906.02940v3) - [pdf](http://arxiv.org/pdf/1906.02940v3)

> We introduce a pretraining technique called Selfie, which stands for SELFie supervised Image Embedding. Selfie generalizes the concept of masked language modeling of BERT (Devlin et al., 2019) to continuous data, such as images, by making use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given masked-out patches in an input image, our method learns to select the correct patch, among other "distractor" patches sampled from the same image, to fill in the masked location. This classification objective sidesteps the need for predicting exact pixel values of the target patches. The pretraining architecture of Selfie includes a network of convolutional blocks to process patches followed by an attention pooling network to summarize the content of unmasked patches before predicting masked ones. During finetuning, we reuse the convolutional weights found by pretraining. We evaluate Selfie on three benchmarks (CIFAR-10, ImageNet 32 x 32, and ImageNet 224 x 224) with varying amounts of labeled data, from 5% to 100% of the training sets. Our pretraining method provides consistent improvements to ResNet-50 across all settings compared to the standard supervised training of the same network. Notably, on ImageNet 224 x 224 with 60 examples per class (5%), our method improves the mean accuracy of ResNet-50 from 35.6% to 46.7%, an improvement of 11.1 points in absolute accuracy. Our pretraining method also improves ResNet-50 training stability, especially on low data regime, by significantly lowering the standard deviation of test accuracies across different runs.

</details>

<details>

<summary>2019-07-27 09:14:43 - Multi-task Self-Supervised Learning for Human Activity Detection</summary>

- *Aaqib Saeed, Tanir Ozcelebi, Johan Lukkien*

- `1907.11879v1` - [abs](http://arxiv.org/abs/1907.11879v1) - [pdf](http://arxiv.org/pdf/1907.11879v1)

> Deep learning methods are successfully used in applications pertaining to ubiquitous computing, health, and well-being. Specifically, the area of human activity recognition (HAR) is primarily transformed by the convolutional and recurrent neural networks, thanks to their ability to learn semantic representations from raw input. However, to extract generalizable features, massive amounts of well-curated data are required, which is a notoriously challenging task; hindered by privacy issues, and annotation costs. Therefore, unsupervised representation learning is of prime importance to leverage the vast amount of unlabeled data produced by smart devices. In this work, we propose a novel self-supervised technique for feature learning from sensory data that does not require access to any form of semantic labels. We learn a multi-task temporal convolutional network to recognize transformations applied on an input signal. By exploiting these transformations, we demonstrate that simple auxiliary tasks of the binary classification result in a strong supervisory signal for extracting useful features for the downstream task. We extensively evaluate the proposed approach on several publicly available datasets for smartphone-based HAR in unsupervised, semi-supervised, and transfer learning settings. Our method achieves performance levels superior to or comparable with fully-supervised networks, and it performs significantly better than autoencoders. Notably, for the semi-supervised case, the self-supervised features substantially boost the detection rate by attaining a kappa score between 0.7-0.8 with only 10 labeled examples per class. We get similar impressive performance even if the features are transferred from a different data source. While this paper focuses on HAR as the application domain, the proposed technique is general and could be applied to a wide variety of problems in other areas.

</details>

<details>

<summary>2019-07-27 16:14:08 - Learnable Parameter Similarity</summary>

- *Guangcong Wang, Jianhuang Lai, Wenqi Liang, Guangrun Wang*

- `1907.11943v1` - [abs](http://arxiv.org/abs/1907.11943v1) - [pdf](http://arxiv.org/pdf/1907.11943v1)

> Most of the existing approaches focus on specific visual tasks while ignoring the relations between them. Estimating task relation sheds light on the learning of high-order semantic concepts, e.g., transfer learning. How to reveal the underlying relations between different visual tasks remains largely unexplored. In this paper, we propose a novel \textbf{L}earnable \textbf{P}arameter \textbf{S}imilarity (\textbf{LPS}) method that learns an effective metric to measure the similarity of second-order semantics hidden in trained models. LPS is achieved by using a second-order neural network to align high-dimensional model parameters and learning second-order similarity in an end-to-end way. In addition, we create a model set called ModelSet500 as a parameter similarity learning benchmark that contains 500 trained models. Extensive experiments on ModelSet500 validate the effectiveness of the proposed method. Code will be released at \url{https://github.com/Wanggcong/learnable-parameter-similarity}.

</details>

<details>

<summary>2019-07-27 21:51:52 - A Hybrid Neural Network Model for Commonsense Reasoning</summary>

- *Pengcheng He, Xiaodong Liu, Weizhu Chen, Jianfeng Gao*

- `1907.11983v1` - [abs](http://arxiv.org/abs/1907.11983v1) - [pdf](http://arxiv.org/pdf/1907.11983v1)

> This paper proposes a hybrid neural network (HNN) model for commonsense reasoning. An HNN consists of two component models, a masked language model and a semantic similarity model, which share a BERT-based contextual encoder but use different model-specific input and output layers. HNN obtains new state-of-the-art results on three classic commonsense reasoning tasks, pushing the WNLI benchmark to 89%, the Winograd Schema Challenge (WSC) benchmark to 75.1%, and the PDP60 benchmark to 90.0%. An ablation study shows that language models and semantic similarity models are complementary approaches to commonsense reasoning, and HNN effectively combines the strengths of both. The code and pre-trained models will be publicly available at https://github.com/namisan/mt-dnn.

</details>

<details>

<summary>2019-07-28 11:16:30 - Causal Inference by String Diagram Surgery</summary>

- *Bart Jacobs, Aleks Kissinger, Fabio Zanasi*

- `1811.08338v2` - [abs](http://arxiv.org/abs/1811.08338v2) - [pdf](http://arxiv.org/pdf/1811.08338v2)

> Extracting causal relationships from observed correlations is a growing area in probabilistic reasoning, originating with the seminal work of Pearl and others from the early 1990s. This paper develops a new, categorically oriented view based on a clear distinction between syntax (string diagrams) and semantics (stochastic matrices), connected via interpretations as structure-preserving functors. A key notion in the identification of causal effects is that of an intervention, whereby a variable is forcefully set to a particular value independent of any prior propensities. We represent the effect of such an intervention as an endofunctor which performs `string diagram surgery' within the syntactic category of string diagrams. This diagram surgery in turn yields a new, interventional distribution via the interpretation functor. While in general there is no way to compute interventional distributions purely from observed data, we show that this is possible in certain special cases using a calculational tool called comb disintegration. We demonstrate the use of this technique on a well-known toy example, where we predict the causal effect of smoking on cancer in the presence of a confounding common cause. After developing this specific example, we show this technique provides simple sufficient conditions for computing interventions which apply to a wide variety of situations considered in the causal inference literature.

</details>

<details>

<summary>2019-07-28 11:40:03 - Interpretability Beyond Classification Output: Semantic Bottleneck Networks</summary>

- *Max Losch, Mario Fritz, Bernt Schiele*

- `1907.10882v2` - [abs](http://arxiv.org/abs/1907.10882v2) - [pdf](http://arxiv.org/pdf/1907.10882v2)

> Today's deep learning systems deliver high performance based on end-to-end training. While they deliver strong performance, these systems are hard to interpret. To address this issue, we propose Semantic Bottleneck Networks (SBN): deep networks with semantically interpretable intermediate layers that all downstream results are based on. As a consequence, the analysis on what the final prediction is based on is transparent to the engineer and failure cases and modes can be analyzed and avoided by high-level reasoning. We present a case study on street scene segmentation to demonstrate the feasibility and power of SBN. In particular, we start from a well performing classic deep network which we adapt to house a SB-Layer containing task related semantic concepts (such as object-parts and materials). Importantly, we can recover state of the art performance despite a drastic dimensionality reduction from 1000s (non-semantic feature) to 10s (semantic concept) channels. Additionally we show how the activations of the SB-Layer can be used for both the interpretation of failure cases of the network as well as for confidence prediction of the resulting output. For the first time, e.g., we show interpretable segmentation results for most predictions at over 99% accuracy.

</details>

<details>

<summary>2019-07-29 08:19:02 - Time4sys2imi: A tool to formalize real-time system models under uncertainty</summary>

- *Étienne André, Jawher Jerray, Sahar Mhiri*

- `1907.13447v1` - [abs](http://arxiv.org/abs/1907.13447v1) - [pdf](http://arxiv.org/pdf/1907.13447v1)

> Time4sys is a formalism developed by Thales, realizing a graphical specification for real-time systems. However, this formalism does not allow to perform formal analyses for real-time systems. So a translation of this tool to a formalism equipped with a formal semantics is needed. We present here Time4sys2imi, a tool translating Time4sys models into parametric timed automata in the input language of IMITATOR. This translation allows not only to check the schedulability of real-time systems, but also to infer some timing constraints (e.g., deadlines, offsets) guaranteeing schedulability. We successfully applied Time4sys2imi to various examples.

</details>

<details>

<summary>2019-07-29 11:39:05 - A Distributed Approach to LARS Stream Reasoning (System paper)</summary>

- *Thomas Eiter, Paul Ogris, Konstantin Schekotihin*

- `1907.12344v1` - [abs](http://arxiv.org/abs/1907.12344v1) - [pdf](http://arxiv.org/pdf/1907.12344v1)

> Stream reasoning systems are designed for complex decision-making from possibly infinite, dynamic streams of data. Modern approaches to stream reasoning are usually performing their computations using stand-alone solvers, which incrementally update their internal state and return results as the new portions of data streams are pushed. However, the performance of such approaches degrades quickly as the rates of the input data and the complexity of decision problems are growing. This problem was already recognized in the area of stream processing, where systems became distributed in order to allocate vast computing resources provided by clouds. In this paper we propose a distributed approach to stream reasoning that can efficiently split computations among different solvers communicating their results over data streams. Moreover, in order to increase the throughput of the distributed system, we suggest an interval-based semantics for the LARS language, which enables significant reductions of network traffic. Performed evaluations indicate that the distributed stream reasoning significantly outperforms existing stand-alone LARS solvers when the complexity of decision problems and the rate of incoming data are increasing. Under consideration for acceptance in Theory and Practice of Logic Programming.

</details>

<details>

<summary>2019-07-29 13:26:03 - VIANA: Visual Interactive Annotation of Argumentation</summary>

- *Fabian Sperrle, Rita Sevastjanova, Rebecca Kehlbeck, Mennatallah El-Assady*

- `1907.12413v1` - [abs](http://arxiv.org/abs/1907.12413v1) - [pdf](http://arxiv.org/pdf/1907.12413v1)

> Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.

</details>

<details>

<summary>2019-07-29 15:52:45 - Precomputing Datalog evaluation plans in large-scale scenarios</summary>

- *Alessio Fiorentino, Nicola Leone, Marco Manna, Simona Perri, Jessica Zangari*

- `1907.12495v1` - [abs](http://arxiv.org/abs/1907.12495v1) - [pdf](http://arxiv.org/pdf/1907.12495v1)

> With the more and more growing demand for semantic Web services over large databases, an efficient evaluation of Datalog queries is arousing a renewed interest among researchers and industry experts. In this scenario, to reduce memory consumption and possibly optimize execution times, the paper proposes novel techniques to determine an optimal indexing schema for the underlying database together with suitable body-orderings for the Datalog rules. The new approach is compared with the standard execution plans implemented in DLV over widely used ontological benchmarks. The results confirm that the memory usage can be significantly reduced without paying any cost in efficiency. This paper is under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2019-07-29 22:12:13 - Tracing cultural diachronic semantic shifts in Russian using word embeddings: test sets and baselines</summary>

- *Vadim Fomin, Daria Bakshandaeva, Julia Rodina, Andrey Kutuzov*

- `1905.06837v2` - [abs](http://arxiv.org/abs/1905.06837v2) - [pdf](http://arxiv.org/pdf/1905.06837v2)

> The paper introduces manually annotated test sets for the task of tracing diachronic (temporal) semantic shifts in Russian. The two test sets are complementary in that the first one covers comparatively strong semantic changes occurring to nouns and adjectives from pre-Soviet to Soviet times, while the second one covers comparatively subtle socially and culturally determined shifts occurring in years from 2000 to 2014. Additionally, the second test set offers more granular classification of shifts degree, but is limited to only adjectives.   The introduction of the test sets allowed us to evaluate several well-established algorithms of semantic shifts detection (posing this as a classification problem), most of which have never been tested on Russian material. All of these algorithms use distributional word embedding models trained on the corresponding in-domain corpora. The resulting scores provide solid comparison baselines for future studies tackling similar tasks. We publish the datasets, code and the trained models in order to facilitate further research in automatically detecting temporal semantic shifts for Russian words, with time periods of different granularities.

</details>

<details>

<summary>2019-07-29 22:26:51 - One-to-X analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts</summary>

- *Andrey Kutuzov, Erik Velldal, Lilja Øvrelid*

- `1907.12674v1` - [abs](http://arxiv.org/abs/1907.12674v1) - [pdf](http://arxiv.org/pdf/1907.12674v1)

> We extend the well-known word analogy task to a one-to-X formulation, including one-to-none cases, when no correct answer exists. The task is cast as a relation discovery problem and applied to historical armed conflicts datasets, attempting to predict new relations of type `location:armed-group' based on data about past events. As the source of semantic information, we use diachronic word embedding models trained on English news texts. A simple technique to improve diachronic performance in such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a ready-to-use test set for one-to-X analogy evaluation on historical armed conflicts data.

</details>

<details>

<summary>2019-07-30 15:51:54 - A Review of Keyphrase Extraction</summary>

- *Eirini Papagiannopoulou, Grigorios Tsoumakas*

- `1905.05044v2` - [abs](http://arxiv.org/abs/1905.05044v2) - [pdf](http://arxiv.org/pdf/1905.05044v2)

> Keyphrase extraction is a textual information processing task concerned with the automatic extraction of representative and characteristic phrases from a document that express all the key aspects of its content. Keyphrases constitute a succinct conceptual summary of a document, which is very useful in digital information management systems for semantic indexing, faceted search, document clustering and classification. This article introduces keyphrase extraction, provides a well-structured review of the existing work, offers interesting insights on the different evaluation approaches, highlights open issues and presents a comparative experimental study of popular unsupervised techniques on five datasets.

</details>

<details>

<summary>2019-07-30 16:29:55 - Generative Reversible Data Hiding by Image to Image Translation via GANs</summary>

- *Zhuo Zhang, Guangyuan Fu, Fuqiang Di, Changlong Li, Jia Liu*

- `1905.02872v4` - [abs](http://arxiv.org/abs/1905.02872v4) - [pdf](http://arxiv.org/pdf/1905.02872v4)

> The traditional reversible data hiding technique is based on cover image modification which inevitably leaves some traces of rewriting that can be more easily analyzed and attacked by the warder. Inspired by the cover synthesis steganography based generative adversarial networks, in this paper, a novel generative reversible data hiding scheme (GRDH) by image translation is proposed. First, an image generator is used to obtain a realistic image, which is used as an input to the image-to-image translation model with CycleGAN. After image translation, a stego image with different semantic information will be obtained. The secret message and the original input image can be recovered separately by a well-trained message extractor and the inverse transform of the image translation. Experimental results have verified the effectiveness of the scheme.

</details>

<details>

<summary>2019-07-30 21:09:05 - About epistemic negation and world views in Epistemic Logic Programs</summary>

- *Stefania Costantini*

- `1907.09867v2` - [abs](http://arxiv.org/abs/1907.09867v2) - [pdf](http://arxiv.org/pdf/1907.09867v2)

> In this paper we consider Epistemic Logic Programs, which extend Answer Set Programming (ASP) with "epistemic operators" and "epistemic negation", and a recent approach to the semantics of such programs in terms of World Views. We propose some observations on the existence and number of world views. We show how to exploit an extended ASP semantics in order to: (i) provide a characterization of world views, different from existing ones; (ii) query world views and query the whole set of world views.

</details>

<details>

<summary>2019-07-30 21:38:16 - SenseFitting: Sense Level Semantic Specialization of Word Embeddings for Word Sense Disambiguation</summary>

- *Manuel Stoeckel, Sajawel Ahmed, Alexander Mehler*

- `1907.13237v1` - [abs](http://arxiv.org/abs/1907.13237v1) - [pdf](http://arxiv.org/pdf/1907.13237v1)

> We introduce a neural network-based system of Word Sense Disambiguation (WSD) for German that is based on SenseFitting, a novel method for optimizing WSD. We outperform knowledge-based WSD methods by up to 25% F1-score and produce a new state-of-the-art on the German sense-annotated dataset WebCAGe. Our method uses three feature vectors consisting of a) sense, b) gloss, and c) relational vectors to represent target senses and to compare them with the vector centroids of sample contexts. Utilizing widely available word embeddings and lexical resources, we are able to compensate for the lower resource availability of German. SenseFitting builds upon the recently introduced semantic specialization procedure Attract-Repel, and leverages sense level semantic constraints from lexical-semantic networks (e.g. GermaNet) or online social dictionaries (e.g. Wiktionary) to produce high-quality sense embeddings from pre-trained word embeddings. We evaluate our sense embeddings with a new SimLex-999 based similarity dataset, called SimSense, that we developed for this work. We achieve results that outperform current lemma-based specialization methods for German, making them comparable to results achieved for English.

</details>

<details>

<summary>2019-07-31 09:58:43 - Enriching Rare Word Representations in Neural Language Models by Embedding Matrix Augmentation</summary>

- *Yerbolat Khassanov, Zhiping Zeng, Van Tung Pham, Haihua Xu, Eng Siong Chng*

- `1904.03799v2` - [abs](http://arxiv.org/abs/1904.03799v2) - [pdf](http://arxiv.org/pdf/1904.03799v2)

> The neural language models (NLM) achieve strong generalization capability by learning the dense representation of words and using them to estimate probability distribution function. However, learning the representation of rare words is a challenging problem causing the NLM to produce unreliable probability estimates. To address this problem, we propose a method to enrich representations of rare words in pre-trained NLM and consequently improve its probability estimation performance. The proposed method augments the word embedding matrices of pre-trained NLM while keeping other parameters unchanged. Specifically, our method updates the embedding vectors of rare words using embedding vectors of other semantically and syntactically similar words. To evaluate the proposed method, we enrich the rare street names in the pre-trained NLM and use it to rescore 100-best hypotheses output from the Singapore English speech recognition system. The enriched NLM reduces the word error rate by 6% relative and improves the recognition accuracy of the rare words by 16% absolute as compared to the baseline NLM.

</details>

<details>

<summary>2019-07-31 16:40:22 - Translating Terminological Expressions in Knowledge Bases with Neural Machine Translation</summary>

- *Mihael Arcan, Daniel Torregrosa, Paul Buitelaar*

- `1709.02184v3` - [abs](http://arxiv.org/abs/1709.02184v3) - [pdf](http://arxiv.org/pdf/1709.02184v3)

> Our work presented in this paper focuses on the translation of terminological expressions represented in semantically structured resources, like ontologies or knowledge graphs. The challenge of translating ontology labels or terminological expressions documented in knowledge bases lies in the highly specific vocabulary and the lack of contextual information, which can guide a machine translation system to translate ambiguous words into the targeted domain. Due to these challenges, we evaluate the translation quality of domain-specific expressions in the medical and financial domain with statistical as well as with neural machine translation methods and experiment domain adaptation of the translation models with terminological expressions only. Furthermore, we perform experiments on the injection of external terminological expressions into the translation systems. Through these experiments, we observed a significant advantage in domain adaptation for the domain-specific resource in the medical and financial domain and the benefit of subword models over word-based neural machine translation models for terminology translation.

</details>


## 2019-08

<details>

<summary>2019-08-01 10:27:29 - MSnet: A BERT-based Network for Gendered Pronoun Resolution</summary>

- *Zili Wang*

- `1908.00308v1` - [abs](http://arxiv.org/abs/1908.00308v1) - [pdf](http://arxiv.org/pdf/1908.00308v1)

> The pre-trained BERT model achieves a remarkable state of the art across a wide range of tasks in natural language processing. For solving the gender bias in gendered pronoun resolution task, I propose a novel neural network model based on the pre-trained BERT. This model is a type of mention score classifier and uses an attention mechanism with no parameters to compute the contextual representation of entity span, and a vector to represent the triple-wise semantic similarity among the pronoun and the entities. In stage 1 of the gendered pronoun resolution task, a variant of this model, trained in the fine-tuning approach, reduced the multi-class logarithmic loss to 0.3033 in the 5-fold cross-validation of training set and 0.2795 in testing set. Besides, this variant won the 2nd place with a score at 0.17289 in stage 2 of the task. The code in this paper is available at: https://github.com/ziliwang/MSnet-for-Gendered-PronounResolution

</details>

<details>

<summary>2019-08-01 11:00:10 - Optimal Deployments of Defense Mechanisms for the Internet of Things</summary>

- *Mengmeng Ge, Jin-Hee Cho, Charles A. Kamhoua, Dong Seong Kim*

- `1908.00324v1` - [abs](http://arxiv.org/abs/1908.00324v1) - [pdf](http://arxiv.org/pdf/1908.00324v1)

> Internet of Things (IoT) devices can be exploited by the attackers as entry points to break into the IoT networks without early detection. Little work has taken hybrid approaches that combine different defense mechanisms in an optimal way to increase the security of the IoT against sophisticated attacks. In this work, we propose a novel approach to generate the strategic deployment of adaptive deception technology and the patch management solution for the IoT under a budget constraint. We use a graphical security model along with three evaluation metrics to measure the effectiveness and efficiency of the proposed defense mechanisms. We apply the multi-objective genetic algorithm (GA) to compute the {\em Pareto optimal} deployments of defense mechanisms to maximize the security and minimize the deployment cost. We present a case study to show the feasibility of the proposed approach and to provide the defenders with various ways to choose optimal deployments of defense mechanisms for the IoT. We compare the GA with the exhaustive search algorithm (ESA) in terms of the runtime complexity and performance accuracy in optimality. Our results show that the GA is much more efficient in computing a good spread of the deployments than the ESA, in proportion to the increase of the IoT devices.

</details>

<details>

<summary>2019-08-01 12:51:01 - ConCORDe-Net: Cell Count Regularized Convolutional Neural Network for Cell Detection in Multiplex Immunohistochemistry Images</summary>

- *Yeman Brhane Hagos, Priya Lakshmi Narayanan, Ayse U. Akarca, Teresa Marafioti, Yinyin Yuan*

- `1908.00907v1` - [abs](http://arxiv.org/abs/1908.00907v1) - [pdf](http://arxiv.org/pdf/1908.00907v1)

> In digital pathology, cell detection and classification are often prerequisites to quantify cell abundance and explore tissue spatial heterogeneity. However, these tasks are particularly challenging for multiplex immunohistochemistry (mIHC) images due to high levels of variability in staining, expression intensity, and inherent noise as a result of preprocessing artefacts. We proposed a deep learning method to detect and classify cells in mIHC whole-tumour slide images of breast cancer. Inspired by inception-v3, we developed Cell COunt RegularizeD Convolutional neural Network (ConCORDe-Net) which integrates conventional dice overlap and a new cell count loss function for optimizing cell detection, followed by a multi-stage convolutional neural network for cell classification. In total, 20447 cells, belonging to five cell classes were annotated by experts from 175 patches extracted from 6 whole-tumour mIHC images. These patches were randomly split into training, validation and testing sets. Using ConCORDe-Net, we obtained a cell detection F1 score of 0.873, which is the best score compared to three state of the art methods. In particular, ConCORDe-Net excels at detecting closely located and weakly stained cells compared to other methods. Incorporating cell count loss in the objective function regularizes the network to learn weak gradient boundaries and separate weakly stained cells from background artefacts. Moreover, cell classification accuracy of 96.5% was achieved. These results support that incorporating problem-specific knowledge such as cell count into deep learning-based cell detection architectures improve the robustness of the algorithm.

</details>

<details>

<summary>2019-08-01 16:02:04 - Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections</summary>

- *Mennatallah El-Assady, Rebecca Kehlbeck, Christopher Collins, Daniel Keim, Oliver Deussen*

- `1908.00475v1` - [abs](http://arxiv.org/abs/1908.00475v1) - [pdf](http://arxiv.org/pdf/1908.00475v1)

> We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.

</details>

<details>

<summary>2019-08-01 19:24:59 - Visualizing RNN States with Predictive Semantic Encodings</summary>

- *Lindsey Sawatzky, Steven Bergner, Fred Popowich*

- `1908.00588v1` - [abs](http://arxiv.org/abs/1908.00588v1) - [pdf](http://arxiv.org/pdf/1908.00588v1)

> Recurrent Neural Networks are an effective and prevalent tool used to model sequential data such as natural language text. However, their deep nature and massive number of parameters pose a challenge for those intending to study precisely how they work. We present a visual technique that gives a high level intuition behind the semantics of the hidden states within Recurrent Neural Networks. This semantic encoding allows for hidden states to be compared throughout the model independent of their internal details. The proposed technique is displayed in a proof of concept visualization tool which is demonstrated to visualize the natural language processing task of language modelling.

</details>

<details>

<summary>2019-08-01 23:46:55 - Robby is Not a Robber (anymore): On the Use of Institutions for Learning Normative Behavior</summary>

- *Stevan Tomic, Federico Pecora, Alessandro Saffiotti*

- `1908.02138v1` - [abs](http://arxiv.org/abs/1908.02138v1) - [pdf](http://arxiv.org/pdf/1908.02138v1)

> Future robots should follow human social norms in order to be useful and accepted in human society. In this paper, we leverage already existing social knowledge in human societies by capturing it in our framework through the notion of social norms. We show how norms can be used to guide a reinforcement learning agent towards achieving normative behavior and apply the same set of norms over different domains. Thus, we are able to: (1) provide a way to intuitively encode social knowledge (through norms); (2) guide learning towards normative behaviors (through an automatic norm reward system); and (3) achieve a transfer of learning by abstracting policies; Finally, (4) the method is not dependent on a particular RL algorithm. We show how our approach can be seen as a means to achieve abstract representation and learn procedural knowledge based on the declarative semantics of norms and discuss possible implications of this in some areas of cognitive science.

</details>

<details>

<summary>2019-08-02 07:50:29 - Detection of Accounting Anomalies in the Latent Space using Adversarial Autoencoder Neural Networks</summary>

- *Marco Schreyer, Timur Sattarov, Christian Schulze, Bernd Reimer, Damian Borth*

- `1908.00734v1` - [abs](http://arxiv.org/abs/1908.00734v1) - [pdf](http://arxiv.org/pdf/1908.00734v1)

> The detection of fraud in accounting data is a long-standing challenge in financial statement audits. Nowadays, the majority of applied techniques refer to handcrafted rules derived from known fraud scenarios. While fairly successful, these rules exhibit the drawback that they often fail to generalize beyond known fraud scenarios and fraudsters gradually find ways to circumvent them. In contrast, more advanced approaches inspired by the recent success of deep learning often lack seamless interpretability of the detected results. To overcome this challenge, we propose the application of adversarial autoencoder networks. We demonstrate that such artificial neural networks are capable of learning a semantic meaningful representation of real-world journal entries. The learned representation provides a holistic view on a given set of journal entries and significantly improves the interpretability of detected accounting anomalies. We show that such a representation combined with the networks reconstruction error can be utilized as an unsupervised and highly adaptive anomaly assessment. Experiments on two datasets and initial feedback received by forensic accountants underpinned the effectiveness of the approach.

</details>

<details>

<summary>2019-08-03 13:48:27 - Adversarially Trained Convolutional Neural Networks for Semantic Segmentation of Ischaemic Stroke Lesion using Multisequence Magnetic Resonance Imaging</summary>

- *Rachana Sathish, Ronnie Rajan, Anusha Vupputuri, Nirmalya Ghosh, Debdoot Sheet*

- `1908.01176v1` - [abs](http://arxiv.org/abs/1908.01176v1) - [pdf](http://arxiv.org/pdf/1908.01176v1)

> Ischaemic stroke is a medical condition caused by occlusion of blood supply to the brain tissue thus forming a lesion. A lesion is zoned into a core associated with irreversible necrosis typically located at the center of the lesion, while reversible hypoxic changes in the outer regions of the lesion are termed as the penumbra. Early estimation of core and penumbra in ischaemic stroke is crucial for timely intervention with thrombolytic therapy to reverse the damage and restore normalcy. Multisequence magnetic resonance imaging (MRI) is commonly employed for clinical diagnosis. However, a sequence singly has not been found to be sufficiently able to differentiate between core and penumbra, while a combination of sequences is required to determine the extent of the damage. The challenge, however, is that with an increase in the number of sequences, it cognitively taxes the clinician to discover symptomatic biomarkers in these images. In this paper, we present a data-driven fully automated method for estimation of core and penumbra in ischaemic lesions using diffusion-weighted imaging (DWI) and perfusion-weighted imaging (PWI) sequence maps of MRI. The method employs recent developments in convolutional neural networks (CNN) for semantic segmentation in medical images. In the absence of availability of a large amount of labeled data, the CNN is trained using an adversarial approach employing cross-entropy as a segmentation loss along with losses aggregated from three discriminators of which two employ relativistic visual Turing test. This method is experimentally validated on the ISLES-2015 dataset through three-fold cross-validation to obtain with an average Dice score of 0.82 and 0.73 for segmentation of penumbra and core respectively.

</details>

<details>

<summary>2019-08-03 18:09:56 - Word2vec to behavior: morphology facilitates the grounding of language in machines</summary>

- *David Matthews, Sam Kriegman, Collin Cappelle, Josh Bongard*

- `1908.01211v1` - [abs](http://arxiv.org/abs/1908.01211v1) - [pdf](http://arxiv.org/pdf/1908.01211v1)

> Enabling machines to respond appropriately to natural language commands could greatly expand the number of people to whom they could be of service. Recently, advances in neural network-trained word embeddings have empowered non-embodied text-processing algorithms, and suggest they could be of similar utility for embodied machines. Here we introduce a method that does so by training robots to act similarly to semantically-similar word2vec encoded commands. We show that this enables them to act appropriately, after training, to previously-unheard commands. Finally, we show that inducing such an alignment between motoric and linguistic similarities can be facilitated or hindered by the mechanical structure of the robot. This points to future, large scale methods that find and exploit relationships between action, language, and robot structure.

</details>

<details>

<summary>2019-08-05 03:45:17 - A Deep Learning Approach for Tweet Classification and Rescue Scheduling for Effective Disaster Management</summary>

- *Md. Yasin Kabir, Sanjay Madria*

- `1908.01456v1` - [abs](http://arxiv.org/abs/1908.01456v1) - [pdf](http://arxiv.org/pdf/1908.01456v1)

> It is a challenging and complex task to acquire information from different regions of a disaster-affected area in a timely fashion. The extensive spread and reach of social media and networks allow people to share information in real-time. However, the processing of social media data and gathering of valuable information require a series of operations such as (1) processing each specific tweet for a text classification, (2) possible location determination of people needing help based on tweets, and (3) priority calculations of rescue tasks based on the classification of tweets. These are three primary challenges in developing an effective rescue scheduling operation using social media data. In this paper, first, we propose a deep learning model combining attention based Bi-directional Long Short-Term Memory (BLSTM) and Convolutional Neural Network (CNN) to classify the tweets under different categories. We use pre-trained crisis word vectors and global vectors for word representation (GLoVe) for capturing semantic meaning from tweets. Next, we perform feature engineering to create an auxiliary feature map which dramatically increases the model accuracy. In our experiments using real data sets from Hurricanes Harvey and Irma, it is observed that our proposed approach performs better compared to other classification methods based on Precision, Recall, F1-score, and Accuracy, and is highly effective to determine the correct priority of a tweet. Furthermore, to evaluate the effectiveness and robustness of the proposed classification model a merged dataset comprises of 4 different datasets from CrisisNLP and another 15 different disasters data from CrisisLex are used. Finally, we develop an adaptive multitask hybrid scheduling algorithm considering resource constraints to perform an effective rescue scheduling operation considering different rescue priorities.

</details>

<details>

<summary>2019-08-05 09:40:18 - Semantic Role Labeling with Associated Memory Network</summary>

- *Chaoyu Guan, Yuhao Cheng, Hai Zhao*

- `1908.02367v1` - [abs](http://arxiv.org/abs/1908.02367v1) - [pdf](http://arxiv.org/pdf/1908.02367v1)

> Semantic role labeling (SRL) is a task to recognize all the predicate-argument pairs of a sentence, which has been in a performance improvement bottleneck after a series of latest works were presented. This paper proposes a novel syntax-agnostic SRL model enhanced by the proposed associated memory network (AMN), which makes use of inter-sentence attention of label-known associated sentences as a kind of memory to further enhance dependency-based SRL. In detail, we use sentences and their labels from train dataset as an associated memory cue to help label the target sentence. Furthermore, we compare several associated sentences selecting strategies and label merging methods in AMN to find and utilize the label of associated sentences while attending them. By leveraging the attentive memory from known training data, Our full model reaches state-of-the-art on CoNLL-2009 benchmark datasets for syntax-agnostic setting, showing a new effective research line of SRL enhancement other than exploiting external resources such as well pre-trained language models.

</details>

<details>

<summary>2019-08-05 13:26:14 - Learning to Identify Security-Related Issues Using Convolutional Neural Networks</summary>

- *David N. Palacio, Daniel McCrystal, Kevin Moran, Carlos Bernal-Cárdenas, Denys Poshyvanyk, Chris Shenefiel*

- `1908.00614v2` - [abs](http://arxiv.org/abs/1908.00614v2) - [pdf](http://arxiv.org/pdf/1908.00614v2)

> Software security is becoming a high priority for both large companies and start-ups alike due to the increasing potential for harm that vulnerabilities and breaches carry with them. However, attaining robust security assurance while delivering features requires a precarious balancing act in the context of agile development practices. One path forward to help aid development teams in securing their software products is through the design and development of security-focused automation. Ergo, we present a novel approach, called SecureReqNet, for automatically identifying whether issues in software issue tracking systems describe security-related content. Our approach consists of a two-phase neural net architecture that operates purely on the natural language descriptions of issues. The first phase of our approach learns high dimensional word embeddings from hundreds of thousands of vulnerability descriptions listed in the CVE database and issue descriptions extracted from open source projects. The second phase then utilizes the semantic ontology represented by these embeddings to train a convolutional neural network capable of predicting whether a given issue is security-related. We evaluated SecureReqNet by applying it to identify security-related issues from a dataset of thousands of issues mined from popular projects on GitLab and GitHub. In addition, we also applied our approach to identify security-related requirements from a commercial software project developed by a major telecommunication company. Our preliminary results are encouraging, with SecureReqNet achieving an accuracy of 96% on open source issues and 71.6% on industrial requirements.

</details>

<details>

<summary>2019-08-06 10:21:44 - Beyond Structural Causal Models: Causal Constraints Models</summary>

- *Tineke Blom, Stephan Bongers, Joris M. Mooij*

- `1805.06539v3` - [abs](http://arxiv.org/abs/1805.06539v3) - [pdf](http://arxiv.org/pdf/1805.06539v3)

> Structural Causal Models (SCMs) provide a popular causal modeling framework. In this work, we show that SCMs are not flexible enough to give a complete causal representation of dynamical systems at equilibrium. Instead, we propose a generalization of the notion of an SCM, that we call Causal Constraints Model (CCM), and prove that CCMs do capture the causal semantics of such systems. We show how CCMs can be constructed from differential equations and initial conditions and we illustrate our ideas further on a simple but ubiquitous (bio)chemical reaction. Our framework also allows to model functional laws, such as the ideal gas law, in a sensible and intuitive way.

</details>

<details>

<summary>2019-08-06 13:19:24 - Aligning Linguistic Words and Visual Semantic Units for Image Captioning</summary>

- *Longteng Guo, Jing Liu, Jinhui Tang, Jiangwei Li, Wei Luo, Hanqing Lu*

- `1908.02127v1` - [abs](http://arxiv.org/abs/1908.02127v1) - [pdf](http://arxiv.org/pdf/1908.02127v1)

> Image captioning attempts to generate a sentence composed of several linguistic words, which are used to describe objects, attributes, and interactions in an image, denoted as visual semantic units in this paper. Based on this view, we propose to explicitly model the object interactions in semantics and geometry based on Graph Convolutional Networks (GCNs), and fully exploit the alignment between linguistic words and visual semantic units for image captioning. Particularly, we construct a semantic graph and a geometry graph, where each node corresponds to a visual semantic unit, i.e., an object, an attribute, or a semantic (geometrical) interaction between two objects. Accordingly, the semantic (geometrical) context-aware embeddings for each unit are obtained through the corresponding GCN learning processers. At each time step, a context gated attention module takes as inputs the embeddings of the visual semantic units and hierarchically align the current word with these units by first deciding which type of visual semantic unit (object, attribute, or interaction) the current word is about, and then finding the most correlated visual semantic units under this type. Extensive experiments are conducted on the challenging MS-COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches.

</details>

<details>

<summary>2019-08-07 00:04:58 - Fast and Accurate Capitalization and Punctuation for Automatic Speech Recognition Using Transformer and Chunk Merging</summary>

- *Binh Nguyen, Vu Bao Hung Nguyen, Hien Nguyen, Pham Ngoc Phuong, The-Loc Nguyen, Quoc Truong Do, Luong Chi Mai*

- `1908.02404v1` - [abs](http://arxiv.org/abs/1908.02404v1) - [pdf](http://arxiv.org/pdf/1908.02404v1)

> In recent years, studies on automatic speech recognition (ASR) have shown outstanding results that reach human parity on short speech segments. However, there are still difficulties in standardizing the output of ASR such as capitalization and punctuation restoration for long-speech transcription. The problems obstruct readers to understand the ASR output semantically and also cause difficulties for natural language processing models such as NER, POS and semantic parsing. In this paper, we propose a method to restore the punctuation and capitalization for long-speech ASR transcription. The method is based on Transformer models and chunk merging that allows us to (1), build a single model that performs punctuation and capitalization in one go, and (2), perform decoding in parallel while improving the prediction accuracy. Experiments on British National Corpus showed that the proposed approach outperforms existing methods in both accuracy and decoding speed.

</details>

<details>

<summary>2019-08-07 06:02:17 - TinySearch -- Semantics based Search Engine using Bert Embeddings</summary>

- *Manish Patel*

- `1908.02451v1` - [abs](http://arxiv.org/abs/1908.02451v1) - [pdf](http://arxiv.org/pdf/1908.02451v1)

> Existing search engines use keyword matching or tf-idf based matching to map the query to the web-documents and rank them. They also consider other factors such as page rank, hubs-and-authority scores, knowledge graphs to make the results more meaningful. However, the existing search engines fail to capture the meaning of query when it becomes large and complex. BERT, introduced by Google in 2018, provides embeddings for words as well as sentences. In this paper, I have developed a semantics-oriented search engine using neural networks and BERT embeddings that can search for query and rank the documents in the order of the most meaningful to least meaningful. The results shows improvement over one existing search engine for complex queries for given set of documents.

</details>

<details>

<summary>2019-08-07 07:15:29 - CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</summary>

- *Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, Youngjoon Yoo*

- `1905.04899v2` - [abs](http://arxiv.org/abs/1905.04899v2) - [pdf](http://arxiv.org/pdf/1905.04899v2)

> Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .

</details>

<details>

<summary>2019-08-07 13:12:55 - Zero-Shot Audio Classification Based on Class Label Embeddings</summary>

- *Huang Xie, Tuomas Virtanen*

- `1905.01926v2` - [abs](http://arxiv.org/abs/1905.01926v2) - [pdf](http://arxiv.org/pdf/1905.01926v2)

> This paper proposes a zero-shot learning approach for audio classification based on the textual information about class labels without any audio samples from target classes. We propose an audio classification system built on the bilinear model, which takes audio feature embeddings and semantic class label embeddings as input, and measures the compatibility between an audio feature embedding and a class label embedding. We use VGGish to extract audio feature embeddings from audio recordings. We treat textual labels as semantic side information of audio classes, and use Word2Vec to generate class label embeddings. Results on the ESC-50 dataset show that the proposed system can perform zero-shot audio classification with small training dataset. It can achieve accuracy (26 % on average) better than random guess (10 %) on each audio category. Particularly, it reaches up to 39.7 % for the category of natural audio classes.

</details>

<details>

<summary>2019-08-07 13:29:11 - Structuring Autoencoders</summary>

- *Marco Rudolph, Bastian Wandt, Bodo Rosenhahn*

- `1908.02626v1` - [abs](http://arxiv.org/abs/1908.02626v1) - [pdf](http://arxiv.org/pdf/1908.02626v1)

> In this paper we propose Structuring AutoEncoders (SAE). SAEs are neural networks which learn a low dimensional representation of data which are additionally enriched with a desired structure in this low dimensional space. While traditional Autoencoders have proven to structure data naturally they fail to discover semantic structure that is hard to recognize in the raw data. The SAE solves the problem by enhancing a traditional Autoencoder using weak supervision to form a structured latent space. In the experiments we demonstrate, that the structured latent space allows for a much more efficient data representation for further tasks such as classification for sparsely labeled data, an efficient choice of data to label, and morphing between classes. To demonstrate the general applicability of our method, we show experiments on the benchmark image datasets MNIST, Fashion-MNIST, DeepFashion2 and on a dataset of 3D human shapes.

</details>

<details>

<summary>2019-08-07 22:03:41 - Optimal Continuous State POMDP Planning with Semantic Observations: A Variational Approach</summary>

- *Luke Burks, Ian Loefgren, Nisar Ahmed*

- `1807.08229v2` - [abs](http://arxiv.org/abs/1807.08229v2) - [pdf](http://arxiv.org/pdf/1807.08229v2)

> This work develops novel strategies for optimal planning with semantic observations using continuous state partially observable markov decision processes (CPOMDPs). Two major innovations are presented in relation to Gaussian mixture (GM) CPOMDP policy approximation methods. While existing methods have many desirable theoretical properties, they are unable to efficiently represent and reason over hybrid continuous-discrete probabilistic models. The first major innovation is the derivation of closed-form variational Bayes GM approximations of Point-Based Value Iteration Bellman policy backups, using softmax models of continuous-discrete semantic observation probabilities. A key benefit of this approach is that dynamic decision-making tasks can be performed with complex non-Gaussian uncertainties, while also exploiting continuous dynamic state space models (thus avoiding cumbersome and costly discretization). The second major innovation is a new clustering-based technique for mixture condensation that scales well to very large GM policy functions and belief functions. Simulation results for a target search and interception task with semantic observations show that the GM policies resulting from these innovations are more effective than those produced by other state of the art policy approximations, but require significantly less modeling overhead and online runtime cost. Additional results show the robustness of this approach to model errors and scaling to higher dimensions.

</details>

<details>

<summary>2019-08-08 06:45:05 - Graph Node Embeddings using Domain-Aware Biased Random Walks</summary>

- *Sourav Mukherjee, Tim Oates, Ryan Wright*

- `1908.02947v1` - [abs](http://arxiv.org/abs/1908.02947v1) - [pdf](http://arxiv.org/pdf/1908.02947v1)

> The recent proliferation of publicly available graph-structured data has sparked an interest in machine learning algorithms for graph data. Since most traditional machine learning algorithms assume data to be tabular, embedding algorithms for mapping graph data to real-valued vector spaces has become an active area of research. Existing graph embedding approaches are based purely on structural information and ignore any semantic information from the underlying domain. In this paper, we demonstrate that semantic information can play a useful role in computing graph embeddings. Specifically, we present a framework for devising embedding strategies aware of domain-specific interpretations of graph nodes and edges, and use knowledge of downstream machine learning tasks to identify relevant graph substructures. Using two real-life domains, we show that our framework yields embeddings that are simple to implement and yet achieve equal or greater accuracy in machine learning tasks compared to domain independent approaches.

</details>

<details>

<summary>2019-08-08 06:46:25 - SqueezeNAS: Fast neural architecture search for faster semantic segmentation</summary>

- *Albert Shaw, Daniel Hunter, Forrest Iandola, Sammy Sidhu*

- `1908.01748v2` - [abs](http://arxiv.org/abs/1908.01748v2) - [pdf](http://arxiv.org/pdf/1908.01748v2)

> For real time applications utilizing Deep Neural Networks (DNNs), it is critical that the models achieve high-accuracy on the target task and low-latency inference on the target computing platform. While Neural Architecture Search (NAS) has been effectively used to develop low-latency networks for image classification, there has been relatively little effort to use NAS to optimize DNN architectures for other vision tasks. In this work, we present what we believe to be the first proxyless hardware-aware search targeted for dense semantic segmentation. With this approach, we advance the state-of-the-art accuracy for latency-optimized networks on the Cityscapes semantic segmentation dataset. Our latency-optimized small SqueezeNAS network achieves 68.02% validation class mIOU with less than 35 ms inference times on the NVIDIA AGX Xavier. Our latency-optimized large SqueezeNAS network achieves 73.62% class mIOU with less than 100 ms inference times. We demonstrate that significant performance gains are possible by utilizing NAS to find networks optimized for both the specific task and inference hardware. We also present detailed analysis comparing our networks to recent state-of-the-art architectures.

</details>

<details>

<summary>2019-08-08 17:21:20 - Dynamic Scale Inference by Entropy Minimization</summary>

- *Dequan Wang, Evan Shelhamer, Bruno Olshausen, Trevor Darrell*

- `1908.03182v1` - [abs](http://arxiv.org/abs/1908.03182v1) - [pdf](http://arxiv.org/pdf/1908.03182v1)

> Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.

</details>

<details>

<summary>2019-08-09 04:44:12 - Using Semantic Role Knowledge for Relevance Ranking of Key Phrases in Documents: An Unsupervised Approach</summary>

- *Prateeti Mohapatra, Neelamadhav Gantayat, Gargi B. Dasgupta*

- `1908.03313v1` - [abs](http://arxiv.org/abs/1908.03313v1) - [pdf](http://arxiv.org/pdf/1908.03313v1)

> In this paper, we investigate the integration of sentence position and semantic role of words in a PageRank system to build a key phrase ranking method. We present the evaluation results of our approach on three scientific articles. We show that semantic role information, when integrated with a PageRank system, can become a new lexical feature. Our approach had an overall improvement on all the data sets over the state-of-art baseline approaches.

</details>

<details>

<summary>2019-08-09 05:07:09 - A Comprehensive Overhaul of Feature Distillation</summary>

- *Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, Jin Young Choi*

- `1904.01866v2` - [abs](http://arxiv.org/abs/1904.01866v2) - [pdf](http://arxiv.org/pdf/1904.01866v2)

> We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at https://sites.google.com/view/byeongho-heo/overhaul

</details>

<details>

<summary>2019-08-09 07:05:34 - Hyper Vision Net: Kidney Tumor Segmentation Using Coordinate Convolutional Layer and Attention Unit</summary>

- *D. Sabarinathan, M. Parisa Beham, S. M. Md. Mansoor Roomi*

- `1908.03339v1` - [abs](http://arxiv.org/abs/1908.03339v1) - [pdf](http://arxiv.org/pdf/1908.03339v1)

> KiTs19 challenge paves the way to haste the improvement of solid kidney tumor semantic segmentation methodologies. Accurate segmentation of kidney tumor in computer tomography (CT) images is a challenging task due to the non-uniform motion, similar appearance and various shape. Inspired by this fact, in this manuscript, we present a novel kidney tumor segmentation method using deep learning network termed as Hyper vision Net model. All the existing U-net models are using a modified version of U-net to segment the kidney tumor region. In the proposed architecture, we introduced supervision layers in the decoder part, and it refines even minimal regions in the output. A dataset consists of real arterial phase abdominal CT scans of 300 patients, including 45964 images has been provided from KiTs19 for training and validation of the proposed model. Compared with the state-of-the-art segmentation methods, the results demonstrate the superiority of our approach on training dice value score of 0.9552 and 0.9633 in tumor region and kidney region, respectively.

</details>

<details>

<summary>2019-08-09 15:51:47 - BERT Rediscovers the Classical NLP Pipeline</summary>

- *Ian Tenney, Dipanjan Das, Ellie Pavlick*

- `1905.05950v2` - [abs](http://arxiv.org/abs/1905.05950v2) - [pdf](http://arxiv.org/pdf/1905.05950v2)

> Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.

</details>

<details>

<summary>2019-08-09 17:04:01 - Modeling meaning: computational interpreting and understanding of natural language fragments</summary>

- *Michael Kapustin, Pavlo Kapustin*

- `1505.08149v3` - [abs](http://arxiv.org/abs/1505.08149v3) - [pdf](http://arxiv.org/pdf/1505.08149v3)

> In this introductory article we present the basics of an approach to implementing computational interpreting of natural language aiming to model the meanings of words and phrases. Unlike other approaches, we attempt to define the meanings of text fragments in a composable and computer interpretable way. We discuss models and ideas for detecting different types of semantic incomprehension and choosing the interpretation that makes most sense in a given context. Knowledge representation is designed for handling context-sensitive and uncertain / imprecise knowledge, and for easy accommodation of new information. It stores quantitative information capturing the essence of the concepts, because it is crucial for working with natural language understanding and reasoning. Still, the representation is general enough to allow for new knowledge to be learned, and even generated by the system. The article concludes by discussing some reasoning-related topics: possible approaches to generation of new abstract concepts, and describing situations and concepts in words (e.g. for specifying interpretation difficulties).

</details>

<details>

<summary>2019-08-09 22:06:06 - A Generate-Validate Approach to Answering Questions about Qualitative Relationships</summary>

- *Arindam Mitra, Chitta Baral, Aurgho Bhattacharjee, Ishan Shrivastava*

- `1908.03645v1` - [abs](http://arxiv.org/abs/1908.03645v1) - [pdf](http://arxiv.org/pdf/1908.03645v1)

> Qualitative relationships describe how increasing or decreasing one property (e.g. altitude) affects another (e.g. temperature). They are an important aspect of natural language question answering and are crucial for building chatbots or voice agents where one may enquire about qualitative relationships. Recently a dataset about question answering involving qualitative relationships has been proposed, and a few approaches to answer such questions have been explored, in the heart of which lies a semantic parser that converts the natural language input to a suitable logical form. A problem with existing semantic parsers is that they try to directly convert the input sentences to a logical form. Since the output language varies with each application, it forces the semantic parser to learn almost everything from scratch. In this paper, we show that instead of using a semantic parser to produce the logical form, if we apply the generate-validate framework i.e. generate a natural language description of the logical form and validate if the natural language description is followed from the input text, we get a better scope for transfer learning and our method outperforms the state-of-the-art by a large margin of 7.93%.

</details>

<details>

<summary>2019-08-10 03:37:18 - Distance Map Loss Penalty Term for Semantic Segmentation</summary>

- *Francesco Caliva, Claudia Iriondo, Alejandro Morales Martinez, Sharmila Majumdar, Valentina Pedoia*

- `1908.03679v1` - [abs](http://arxiv.org/abs/1908.03679v1) - [pdf](http://arxiv.org/pdf/1908.03679v1)

> Convolutional neural networks for semantic segmentation suffer from low performance at object boundaries. In medical imaging, accurate representation of tissue surfaces and volumes is important for tracking of disease biomarkers such as tissue morphology and shape features. In this work, we propose a novel distance map derived loss penalty term for semantic segmentation. We propose to use distance maps, derived from ground truth masks, to create a penalty term, guiding the network's focus towards hard-to-segment boundary regions. We investigate the effects of this penalizing factor against cross-entropy, Dice, and focal loss, among others, evaluating performance on a 3D MRI bone segmentation task from the publicly available Osteoarthritis Initiative dataset. We observe a significant improvement in the quality of segmentation, with better shape preservation at bone boundaries and areas affected by partial volume. We ultimately aim to use our loss penalty term to improve the extraction of shape biomarkers and derive metrics to quantitatively evaluate the preservation of shape.

</details>

<details>

<summary>2019-08-10 21:26:54 - Conditional Generative Adversarial Networks for Data Augmentation and Adaptation in Remotely Sensed Imagery</summary>

- *Jonathan Howe, Kyle Pula, Aaron A. Reite*

- `1908.03809v1` - [abs](http://arxiv.org/abs/1908.03809v1) - [pdf](http://arxiv.org/pdf/1908.03809v1)

> The difficulty in obtaining labeled data relevant to a given task is among the most common and well-known practical obstacles to applying deep learning techniques to new or even slightly modified domains. The data volumes required by the current generation of supervised learning algorithms typically far exceed what a human needs to learn and complete a given task. We investigate ways to expand a given labeled corpus of remote sensed imagery into a larger corpus using Generative Adversarial Networks (GANs). We then measure how these additional synthetic data affect supervised machine learning performance on an object detection task.   Our data driven strategy is to train GANs to (1) generate synthetic segmentation masks and (2) generate plausible synthetic remote sensing imagery corresponding to these segmentation masks. Run sequentially, these GANs allow the generation of synthetic remote sensing imagery complete with segmentation labels. We apply this strategy to the data set from ISPRS' 2D Semantic Labeling Contest - Potsdam, with a follow on vehicle detection task. We find that in scenarios with limited training data, augmenting the available data with such synthetically generated data can improve detector performance.

</details>

<details>

<summary>2019-08-12 08:04:42 - Evaluating Tag Recommendations for E-Book Annotation Using a Semantic Similarity Metric</summary>

- *Emanuel Lacic, Dominik Kowald, Dieter Theiler, Matthias Traub, Lucky Kuffer, Stefanie Lindstaedt, Elisabeth Lex*

- `1908.04042v1` - [abs](http://arxiv.org/abs/1908.04042v1) - [pdf](http://arxiv.org/pdf/1908.04042v1)

> In this paper, we present our work to support publishers and editors in finding descriptive tags for e-books through tag recommendations. We propose a hybrid tag recommendation system for e-books, which leverages search query terms from Amazon users and e-book metadata, which is assigned by publishers and editors. Our idea is to mimic the vocabulary of users in Amazon, who search for and review e-books, and to combine these search terms with editor tags in a hybrid tag recommendation approach. In total, we evaluate 19 tag recommendation algorithms on the review content of Amazon users, which reflects the readers' vocabulary. Our results show that we can improve the performance of tag recommender systems for e-books both concerning tag recommendation accuracy, diversity as well as a novel semantic similarity metric, which we also propose in this paper.

</details>

<details>

<summary>2019-08-12 08:24:40 - SHREWD: Semantic Hierarchy-based Relational Embeddings for Weakly-supervised Deep Hashing</summary>

- *Heikki Arponen, Tom E Bishop*

- `1908.05602v1` - [abs](http://arxiv.org/abs/1908.05602v1) - [pdf](http://arxiv.org/pdf/1908.05602v1)

> Using class labels to represent class similarity is a typical approach to training deep hashing systems for retrieval; samples from the same or different classes take binary 1 or 0 similarity values. This similarity does not model the full rich knowledge of semantic relations that may be present between data points. In this work we build upon the idea of using semantic hierarchies to form distance metrics between all available sample labels; for example cat to dog has a smaller distance than cat to guitar. We combine this type of semantic distance into a loss function to promote similar distances between the deep neural network embeddings. We also introduce an empirical Kullback-Leibler divergence loss term to promote binarization and uniformity of the embeddings. We test the resulting SHREWD method and demonstrate improvements in hierarchical retrieval scores using compact, binary hash codes instead of real valued ones, and show that in a weakly supervised hashing setting we are able to learn competitively without explicitly relying on class labels, but instead on similarities between labels.

</details>

<details>

<summary>2019-08-12 08:31:42 - A Description Logic Framework for Commonsense Conceptual Combination Integrating Typicality, Probabilities and Cognitive Heuristics</summary>

- *Antonio Lieto, Gian Luca Pozzato*

- `1811.02366v4` - [abs](http://arxiv.org/abs/1811.02366v4) - [pdf](http://arxiv.org/pdf/1811.02366v4)

> We propose a nonmonotonic Description Logic of typicality able to account for the phenomenon of concept combination of prototypical concepts. The proposed logic relies on the logic of typicality ALC TR, whose semantics is based on the notion of rational closure, as well as on the distributed semantics of probabilistic Description Logics, and is equipped with a cognitive heuristic used by humans for concept composition. We first extend the logic of typicality ALC TR by typicality inclusions whose intuitive meaning is that "there is probability p about the fact that typical Cs are Ds". As in the distributed semantics, we define different scenarios containing only some typicality inclusions, each one having a suitable probability. We then focus on those scenarios whose probabilities belong to a given and fixed range, and we exploit such scenarios in order to ascribe typical properties to a concept C obtained as the combination of two prototypical concepts. We also show that reasoning in the proposed Description Logic is EXPTIME-complete as for the underlying ALC.

</details>

<details>

<summary>2019-08-12 09:51:14 - Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition with Multimodal Training</summary>

- *Mahdi Abavisani, Hamid Reza Vaezi Joze, Vishal M. Patel*

- `1812.06145v2` - [abs](http://arxiv.org/abs/1812.06145v2) - [pdf](http://arxiv.org/pdf/1812.06145v2)

> We present an efficient approach for leveraging the knowledge from multiple modalities in training unimodal 3D convolutional neural networks (3D-CNNs) for the task of dynamic hand gesture recognition. Instead of explicitly combining multimodal information, which is commonplace in many state-of-the-art methods, we propose a different framework in which we embed the knowledge of multiple modalities in individual networks so that each unimodal network can achieve an improved performance. In particular, we dedicate separate networks per available modality and enforce them to collaborate and learn to develop networks with common semantics and better representations. We introduce a "spatiotemporal semantic alignment" loss (SSA) to align the content of the features from different networks. In addition, we regularize this loss with our proposed "focal regularization parameter" to avoid negative knowledge transfer. Experimental results show that our framework improves the test time recognition accuracy of unimodal networks, and provides the state-of-the-art performance on various dynamic hand gesture recognition datasets.

</details>

<details>

<summary>2019-08-12 11:17:15 - VISON: An Ontology-Based Approach for Software Visualization Tool Discoverability</summary>

- *Leonel Merino, Ekaterina Kozlova, Oscar Nierstrasz, Daniel Weiskopf*

- `1908.04090v1` - [abs](http://arxiv.org/abs/1908.04090v1) - [pdf](http://arxiv.org/pdf/1908.04090v1)

> Although many tools have been presented in the research literature of software visualization, there is little evidence of their adoption. To choose a suitable visualization tool, practitioners need to analyze various characteristics of tools such as their supported software concerns and level of maturity. Indeed, some tools can be prototypes for which the lifespan is expected to be short, whereas others can be fairly mature products that are maintained for a longer time. Although such characteristics are often described in papers, we conjecture that practitioners willing to adopt software visualizations require additional support to discover suitable visualization tools. In this paper, we elaborate on our efforts to provide such support. To this end, we systematically analyzed research papers in the literature of software visualization and curated a catalog of 70 available tools that employ various visualization techniques to support the analysis of multiple software concerns. We further encapsulate these characteristics in an ontology. VISON, our software visualization ontology, captures these semantics as concepts and relationships. We report on early results of usage scenarios that demonstrate how the ontology can support (i) developers to find suitable tools for particular development concerns, and (ii) researchers who propose new software visualization tools to identify a baseline tool for a controlled experiment.

</details>

<details>

<summary>2019-08-12 15:32:10 - Assessing the Quality of Scientific Papers</summary>

- *Roman Vainshtein, Gilad Katz, Bracha Shapira, Lior Rokach*

- `1908.04200v1` - [abs](http://arxiv.org/abs/1908.04200v1) - [pdf](http://arxiv.org/pdf/1908.04200v1)

> A multitude of factors are responsible for the overall quality of scientific papers, including readability, linguistic quality, fluency,semantic complexity, and of course domain-specific technical factors. These factors vary from one field of study to another. In this paper, we propose a measure and method for assessing the overall quality of the scientific papers in a particular field of study. We evaluate our method in the computer science domain, but it can be applied to other technical and scientific fields.Our method is based on the corpus linguistics technique. This technique enables the extraction of required information and knowledge associated with a specific domain. For this purpose, we have created a large corpus, consisting of papers from very high impact conferences. First, we analyze this corpus in order to extract rich domain-specific terminology and knowledge. Then we use the acquired knowledge to estimate the quality of scientific papers by applying our proposed measure. We examine our measure on high and low scientific impact test corpora. Our results show a significant difference in the measure scores of the high and low impact test corpora. Second, we develop a classifier based on our proposed measure and compare it to the baseline classifier. Our results show that the classifier based on our measure over-performed the baseline classifier. Based on the presented results the proposed measure and the technique can be used for automated assessment of scientific papers.

</details>

<details>

<summary>2019-08-13 06:05:24 - Boosted GAN with Semantically Interpretable Information for Image Inpainting</summary>

- *Ang Li, Jianzhong Qi, Rui Zhang, Ramamohanarao Kotagiri*

- `1908.04503v1` - [abs](http://arxiv.org/abs/1908.04503v1) - [pdf](http://arxiv.org/pdf/1908.04503v1)

> Image inpainting aims at restoring missing region of corrupted images, which has many applications such as image restoration and object removal. However, current GAN-based inpainting models fail to explicitly consider the semantic consistency between restored images and original images. Forexample, given a male image with image region of one eye missing, current models may restore it with a female eye. This is due to the ambiguity of GAN-based inpainting models: these models can generate many possible restorations given a missing region. To address this limitation, our key insight is that semantically interpretable information (such as attribute and segmentation information) of input images (with missing regions) can provide essential guidance for the inpainting process. Based on this insight, we propose a boosted GAN with semantically interpretable information for image inpainting that consists of an inpainting network and a discriminative network. The inpainting network utilizes two auxiliary pretrained networks to discover the attribute and segmentation information of input images and incorporates them into the inpainting process to provide explicit semantic-level guidance. The discriminative network adopts a multi-level design that can enforce regularizations not only on overall realness but also on attribute and segmentation consistency with the original images. Experimental results show that our proposed model can preserve consistency on both attribute and segmentation level, and significantly outperforms the state-of-the-art models.

</details>

<details>

<summary>2019-08-13 06:15:41 - A Survey on Ethereum Systems Security: Vulnerabilities, Attacks and Defenses</summary>

- *Huashan Chen, Marcus Pendleton, Laurent Njilla, Shouhuai Xu*

- `1908.04507v1` - [abs](http://arxiv.org/abs/1908.04507v1) - [pdf](http://arxiv.org/pdf/1908.04507v1)

> The blockchain technology is believed by many to be a game changer in many application domains, especially financial applications. While the first generation of blockchain technology (i.e., Blockchain 1.0) is almost exclusively used for cryptocurrency purposes, the second generation (i.e., Blockchain 2.0), as represented by Ethereum, is an open and decentralized platform enabling a new paradigm of computing --- Decentralized Applications (DApps) running on top of blockchains. The rich applications and semantics of DApps inevitably introduce many security vulnerabilities, which have no counterparts in pure cryptocurrency systems like Bitcoin. Since Ethereum is a new, yet complex, system, it is imperative to have a systematic and comprehensive understanding on its security from a holistic perspective, which is unavailable. To the best of our knowledge, the present survey, which can also be used as a tutorial, fills this void. In particular, we systematize three aspects of Ethereum systems security: vulnerabilities, attacks, and defenses. We draw insights into, among other things, vulnerability root causes, attack consequences, and defense capabilities, which shed light on future research directions.

</details>

<details>

<summary>2019-08-13 09:35:52 - Reverse-Engineering Satire, or "Paper on Computational Humor Accepted Despite Making Serious Advances"</summary>

- *Robert West, Eric Horvitz*

- `1901.03253v3` - [abs](http://arxiv.org/abs/1901.03253v3) - [pdf](http://arxiv.org/pdf/1901.03253v3)

> Humor is an essential human trait. Efforts to understand humor have called out links between humor and the foundations of cognition, as well as the importance of humor in social engagement. As such, it is a promising and important subject of study, with relevance for artificial intelligence and human-computer interaction. Previous computational work on humor has mostly operated at a coarse level of granularity, e.g., predicting whether an entire sentence, paragraph, document, etc., is humorous. As a step toward deep understanding of humor, we seek fine-grained models of attributes that make a given text humorous. Starting from the observation that satirical news headlines tend to resemble serious news headlines, we build and analyze a corpus of satirical headlines paired with nearly identical but serious headlines. The corpus is constructed via Unfun.me, an online game that incentivizes players to make minimal edits to satirical headlines with the goal of making other players believe the results are serious headlines. The edit operations used to successfully remove humor pinpoint the words and concepts that play a key role in making the original, satirical headline funny. Our analysis reveals that the humor tends to reside toward the end of headlines, and primarily in noun phrases, and that most satirical headlines follow a certain logical pattern, which we term false analogy. Overall, this paper deepens our understanding of the syntactic and semantic structure of satirical news headlines and provides insights for building humor-producing systems.

</details>

<details>

<summary>2019-08-13 11:58:27 - Defending Against Universal Perturbations With Shared Adversarial Training</summary>

- *Chaithanya Kumar Mummadi, Thomas Brox, Jan Hendrik Metzen*

- `1812.03705v2` - [abs](http://arxiv.org/abs/1812.03705v2) - [pdf](http://arxiv.org/pdf/1812.03705v2)

> Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of image classifiers against such adversarial perturbations, it leaves them sensitive to perturbations on a non-negligible fraction of the inputs. In this work, we show that adversarial training is more effective in preventing universal perturbations, where the same perturbation needs to fool a classifier on many inputs. Moreover, we investigate the trade-off between robustness against universal perturbations and performance on unperturbed data and propose an extension of adversarial training that handles this trade-off more gracefully. We present results for image classification and semantic segmentation to showcase that universal perturbations that fool a model hardened with adversarial training become clearly perceptible and show patterns of the target scene.

</details>

<details>

<summary>2019-08-13 15:21:37 - Semi-Supervised Learning using Differentiable Reasoning</summary>

- *Emile van Krieken, Erman Acar, Frank van Harmelen*

- `1908.04700v1` - [abs](http://arxiv.org/abs/1908.04700v1) - [pdf](http://arxiv.org/pdf/1908.04700v1)

> We introduce Differentiable Reasoning (DR), a novel semi-supervised learning technique which uses relational background knowledge to benefit from unlabeled data. We apply it to the Semantic Image Interpretation (SII) task and show that background knowledge provides significant improvement. We find that there is a strong but interesting imbalance between the contributions of updates from Modus Ponens (MP) and its logical equivalent Modus Tollens (MT) to the learning process, suggesting that our approach is very sensitive to a phenomenon called the Raven Paradox. We propose a solution to overcome this situation.

</details>

<details>

<summary>2019-08-13 17:20:51 - Fine-grained Information Status Classification Using Discourse Context-Aware Self-Attention</summary>

- *Yufang Hou*

- `1908.04755v1` - [abs](http://arxiv.org/abs/1908.04755v1) - [pdf](http://arxiv.org/pdf/1908.04755v1)

> Previous work on bridging anaphora recognition (Hou et al., 2013a) casts the problem as a subtask of learning fine-grained information status (IS). However, these systems heavily depend on many hand-crafted linguistic features. In this paper, we propose a discourse context-aware self-attention neural network model for fine-grained IS classification. On the ISNotes corpus (Markert et al., 2012), our model with the contextually-encoded word representations (BERT) (Devlin et al., 2018) achieves new state-of-the-art performances on fine-grained IS classification, obtaining a 4.1% absolute overall accuracy improvement compared to Hou et al. (2013a). More importantly, we also show an improvement of 3.9% F1 for bridging anaphora recognition without using any complex hand-crafted semantic features designed for capturing the bridging phenomenon.

</details>

<details>

<summary>2019-08-14 05:23:12 - ClustCrypt: Privacy-Preserving Clustering of Unstructured Big Data in the Cloud</summary>

- *SM Zobaed, Sahan Ahmad, Raju Gottumukkala, Mohsen Amini Salehi*

- `1908.04960v1` - [abs](http://arxiv.org/abs/1908.04960v1) - [pdf](http://arxiv.org/pdf/1908.04960v1)

> Security and confidentiality of big data stored in the cloud are important concerns for many organizations to adopt cloud services. One common approach to address the concerns is client-side encryption where data is encrypted on the client machine before being stored in the cloud. Having encrypted data in the cloud, however, limits the ability of data clustering, which is a crucial part of many data analytics applications, such as search systems. To overcome the limitation, in this paper, we present an approach named ClustCrypt for efficient topic-based clustering of encrypted unstructured big data in the cloud. ClustCrypt dynamically estimates the optimal number of clusters based on the statistical characteristics of encrypted data. It also provides clustering approach for encrypted data. We deploy ClustCrypt within the context of a secure cloud-based semantic search system (S3BD). Experimental results obtained from evaluating ClustCrypt on three datasets demonstrate on average 60% improvement on clusters' coherency. ClustCrypt also decreases the search-time overhead by up to 78% and increases the accuracy of search results by up to 35%

</details>

<details>

<summary>2019-08-14 06:40:28 - Harmonized Multimodal Learning with Gaussian Process Latent Variable Models</summary>

- *Guoli Song, Shuhui Wang, Qingming Huang, Qi Tian*

- `1908.04979v1` - [abs](http://arxiv.org/abs/1908.04979v1) - [pdf](http://arxiv.org/pdf/1908.04979v1)

> Multimodal learning aims to discover the relationship between multiple modalities. It has become an important research topic due to extensive multimodal applications such as cross-modal retrieval. This paper attempts to address the modality heterogeneity problem based on Gaussian process latent variable models (GPLVMs) to represent multimodal data in a common space. Previous multimodal GPLVM extensions generally adopt individual learning schemes on latent representations and kernel hyperparameters, which ignore their intrinsic relationship. To exploit strong complementarity among different modalities and GPLVM components, we develop a novel learning scheme called Harmonization, where latent model parameters are jointly learned from each other. Beyond the correlation fitting or intra-modal structure preservation paradigms widely used in existing studies, the harmonization is derived in a model-driven manner to encourage the agreement between modality-specific GP kernels and the similarity of latent representations. We present a range of multimodal learning models by incorporating the harmonization mechanism into several representative GPLVM-based approaches. Experimental results on four benchmark datasets show that the proposed models outperform the strong baselines for cross-modal retrieval tasks, and that the harmonized multimodal learning method is superior in discovering semantically consistent latent representation.

</details>

<details>

<summary>2019-08-14 12:58:23 - Visual and Semantic Prototypes-Jointly Guided CNN for Generalized Zero-shot Learning</summary>

- *Chuanxing Geng, Lue Tao, Songcan Chen*

- `1908.03983v2` - [abs](http://arxiv.org/abs/1908.03983v2) - [pdf](http://arxiv.org/pdf/1908.03983v2)

> In the process of exploring the world, the curiosity constantly drives humans to cognize new things. Supposing you are a zoologist, for a presented animal image, you can recognize it immediately if you know its class. Otherwise, you would more likely attempt to cognize it by exploiting the side-information (e.g., semantic information, etc.) you have accumulated. Inspired by this, this paper decomposes the generalized zero-shot learning (G-ZSL) task into an open set recognition (OSR) task and a zero-shot learning (ZSL) task, where OSR recognizes seen classes (if we have seen (or known) them) and rejects unseen classes (if we have never seen (or known) them before), while ZSL identifies the unseen classes rejected by the former. Simultaneously, without violating OSR's assumptions (only known class knowledge is available in training), we also first attempt to explore a new generalized open set recognition (G-OSR) by introducing the accumulated side-information from known classes to OSR. For G-ZSL, such a decomposition effectively solves the class overfitting problem with easily misclassifying unseen classes as seen classes. The problem is ubiquitous in most existing G-ZSL methods. On the other hand, for G-OSR, introducing such semantic information of known classes not only improves the recognition performance but also endows OSR with the cognitive ability of unknown classes. Specifically, a visual and semantic prototypes-jointly guided convolutional neural network (VSG-CNN) is proposed to fulfill these two tasks (G-ZSL and G-OSR) in a unified end-to-end learning framework. Extensive experiments on benchmark datasets demonstrate the advantages of our learning framework.

</details>

<details>

<summary>2019-08-14 16:20:26 - Enhancing Clinical Concept Extraction with Contextual Embeddings</summary>

- *Yuqi Si, Jingqi Wang, Hua Xu, Kirk Roberts*

- `1902.08691v4` - [abs](http://arxiv.org/abs/1902.08691v4) - [pdf](http://arxiv.org/pdf/1902.08691v4)

> Neural network-based representations ("embeddings") have dramatically advanced natural language processing (NLP) tasks, including clinical NLP tasks such as concept extraction. Recently, however, more advanced embedding methods and representations (e.g., ELMo, BERT) have further pushed the state-of-the-art in NLP, yet there are no common best practices for how to integrate these representations into clinical tasks. The purpose of this study, then, is to explore the space of possible options in utilizing these new models for clinical concept extraction, including comparing these to traditional word embedding methods (word2vec, GloVe, fastText). Both off-the-shelf open-domain embeddings and pre-trained clinical embeddings from MIMIC-III are evaluated. We explore a battery of embedding methods consisting of traditional word embeddings and contextual embeddings, and compare these on four concept extraction corpora: i2b2 2010, i2b2 2012, SemEval 2014, and SemEval 2015. We also analyze the impact of the pre-training time of a large language model like ELMo or BERT on the extraction performance. Last, we present an intuitive way to understand the semantic information encoded by contextual embeddings. Contextual embeddings pre-trained on a large clinical corpus achieves new state-of-the-art performances across all concept extraction tasks. The best-performing model outperforms all state-of-the-art methods with respective F1-measures of 90.25, 93.18 (partial), 80.74, and 81.65. We demonstrate the potential of contextual embeddings through the state-of-the-art performance these methods achieve on clinical concept extraction. Additionally, we demonstrate contextual embeddings encode valuable semantic information not accounted for in traditional word representations.

</details>

<details>

<summary>2019-08-14 17:30:02 - Learning elementary structures for 3D shape generation and matching</summary>

- *Theo Deprelle, Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry*

- `1908.04725v2` - [abs](http://arxiv.org/abs/1908.04725v2) - [pdf](http://arxiv.org/pdf/1908.04725v2)

> We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shape. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.

</details>

<details>

<summary>2019-08-14 17:42:11 - Local Unsupervised Learning for Image Analysis</summary>

- *Leopold Grinberg, John Hopfield, Dmitry Krotov*

- `1908.08993v1` - [abs](http://arxiv.org/abs/1908.08993v1) - [pdf](http://arxiv.org/pdf/1908.08993v1)

> Local Hebbian learning is believed to be inferior in performance to end-to-end training using a backpropagation algorithm. We question this popular belief by designing a local algorithm that can learn convolutional filters at scale on large image datasets. These filters combined with patch normalization and very steep non-linearities result in a good classification accuracy for shallow networks trained locally, as opposed to end-to-end. The filters learned by our algorithm contain both orientation selective units and unoriented color units, resembling the responses of pyramidal neurons located in the cytochrome oxidase 'interblob' and 'blob' regions in the primary visual cortex of primates. It is shown that convolutional networks with patch normalization significantly outperform standard convolutional networks on the task of recovering the original classes when shadows are superimposed on top of standard CIFAR-10 images. Patch normalization approximates the retinal adaptation to the mean light intensity, important for human vision. We also demonstrate a successful transfer of learned representations between CIFAR-10 and ImageNet 32x32 datasets. All these results taken together hint at the possibility that local unsupervised training might be a powerful tool for learning general representations (without specifying the task) directly from unlabeled data.

</details>

<details>

<summary>2019-08-15 01:35:36 - SFSegNet: Parse Freehand Sketches using Deep Fully Convolutional Networks</summary>

- *Junkun Jiang, Ruomei Wang, Shujin Lin, Fei Wang*

- `1908.05389v1` - [abs](http://arxiv.org/abs/1908.05389v1) - [pdf](http://arxiv.org/pdf/1908.05389v1)

> Parsing sketches via semantic segmentation is attractive but challenging, because (i) free-hand drawings are abstract with large variances in depicting objects due to different drawing styles and skills; (ii) distorting lines drawn on the touchpad make sketches more difficult to be recognized; (iii) the high-performance image segmentation via deep learning technologies needs enormous annotated sketch datasets during the training stage. In this paper, we propose a Sketch-target deep FCN Segmentation Network(SFSegNet) for automatic free-hand sketch segmentation, labeling each sketch in a single object with multiple parts. SFSegNet has an end-to-end network process between the input sketches and the segmentation results, composed of 2 parts: (i) a modified deep Fully Convolutional Network(FCN) using a reweighting strategy to ignore background pixels and classify which part each pixel belongs to; (ii) affine transform encoders that attempt to canonicalize the shaking strokes. We train our network with the dataset that consists of 10,000 annotated sketches, to find an extensively applicable model to segment stokes semantically in one ground truth. Extensive experiments are carried out and segmentation results show that our method outperforms other state-of-the-art networks.

</details>

<details>

<summary>2019-08-15 03:50:18 - Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards</summary>

- *Yuqing Song, Shizhe Chen, Yida Zhao, Qin Jin*

- `1908.05407v1` - [abs](http://arxiv.org/abs/1908.05407v1) - [pdf](http://arxiv.org/pdf/1908.05407v1)

> Generating image descriptions in different languages is essential to satisfy users worldwide. However, it is prohibitively expensive to collect large-scale paired image-caption dataset for every target language which is critical for training descent image captioning models. Previous works tackle the unpaired cross-lingual image captioning problem through a pivot language, which is with the help of paired image-caption data in the pivot language and pivot-to-target machine translation models. However, such language-pivoted approach suffers from inaccuracy brought by the pivot-to-target translation, including disfluency and visual irrelevancy errors. In this paper, we propose to generate cross-lingual image captions with self-supervised rewards in the reinforcement learning framework to alleviate these two types of errors. We employ self-supervision from mono-lingual corpus in the target language to provide fluency reward, and propose a multi-level visual semantic matching model to provide both sentence-level and concept-level visual relevancy rewards. We conduct extensive experiments for unpaired cross-lingual image captioning in both English and Chinese respectively on two widely used image caption corpora. The proposed approach achieves significant performance improvement over state-of-the-art methods.

</details>

<details>

<summary>2019-08-15 10:27:32 - Bayesian Generative Models for Knowledge Transfer in MRI Semantic Segmentation Problems</summary>

- *Anna Kuzina, Evgenii Egorov, Evgeny Burnaev*

- `1908.05480v1` - [abs](http://arxiv.org/abs/1908.05480v1) - [pdf](http://arxiv.org/pdf/1908.05480v1)

> Automatic segmentation methods based on deep learning have recently demonstrated state-of-the-art performance, outperforming the ordinary methods. Nevertheless, these methods are inapplicable for small datasets, which are very common in medical problems. To this end, we propose a knowledge transfer method between diseases via the Generative Bayesian Prior network. Our approach is compared to a pre-train approach and random initialization and obtains the best results in terms of Dice Similarity Coefficient metric for the small subsets of the Brain Tumor Segmentation 2018 database (BRATS2018).

</details>

<details>

<summary>2019-08-15 10:58:57 - A Multivariate Model for Representing Semantic Non-compositionality</summary>

- *Meghdad Farahmand*

- `1908.05490v1` - [abs](http://arxiv.org/abs/1908.05490v1) - [pdf](http://arxiv.org/pdf/1908.05490v1)

> Semantically non-compositional phrases constitute an intriguing research topic in Natural Language Processing. Semantic non-compositionality --the situation when the meaning of a phrase cannot be derived from the meaning of its components, is the main characteristic of such phrases, however, they bear other characteristics such as high statistical association and non-substitutability. In this work, we present a model for identifying non-compositional phrases that takes into account all of these characteristics. We show that the presented model remarkably outperforms the existing models of identifying non-compositional phrases that mostly focus only on one of these characteristics.

</details>

<details>

<summary>2019-08-15 13:51:12 - Hamming Sentence Embeddings for Information Retrieval</summary>

- *Felix Hamann, Nadja Kurz, Adrian Ulges*

- `1908.05541v1` - [abs](http://arxiv.org/abs/1908.05541v1) - [pdf](http://arxiv.org/pdf/1908.05541v1)

> In retrieval applications, binary hashes are known to offer significant improvements in terms of both memory and speed. We investigate the compression of sentence embeddings using a neural encoder-decoder architecture, which is trained by minimizing reconstruction error. Instead of employing the original real-valued embeddings, we use latent representations in Hamming space produced by the encoder for similarity calculations.   In quantitative experiments on several benchmarks for semantic similarity tasks, we show that our compressed hamming embeddings yield a comparable performance to uncompressed embeddings (Sent2Vec, InferSent, Glove-BoW), at compression ratios of up to 256:1. We further demonstrate that our model strongly decorrelates input features, and that the compressor generalizes well when pre-trained on Wikipedia sentences. We publish the source code on Github and all experimental results.

</details>

<details>

<summary>2019-08-15 17:39:47 - A reference model for interaction semantics</summary>

- *Johannes Reich, Tizian Schröder*

- `1801.04185v2` - [abs](http://arxiv.org/abs/1801.04185v2) - [pdf](http://arxiv.org/pdf/1801.04185v2)

> In this article, we introduce a reference model for interaction semantics among communicating discrete systems to guide the discourse on interoperability.   The necessary set of unifying concepts is small and comprises essentially the notion of discrete systems interacting by exchanging information. It is based on a simple, but nevertheless complete classification of system interactions with respect to information transport and processing. Information transport can only be uni- or bidirectional and information processing is subclassified along the binary dimensions of state, determinism and synchronicity.   For interactions with bidirectional information flow we are able to define a criterion for a layered structure of systems: we name a bidirectional interaction "horizontal" if all interacting systems behave the same with respect to state, determinism and synchronicity and we name it "vertical" --- providing a semantic direction --- if there is a behavioral asymmetry between the interacting systems with respect to these properties.   It is shown that horizontal interactions are essentially stateful, asynchronous and nondeterministic and are described by protocols. Vertical interactions are essentially top-down-usage, described by object models or operations, and bottom-up-observation, described by anonymous events.   The reference model thereby helps us to understand the significant relationships that are created between interacting discrete systems by their interactions and guides us on how to talk about discrete system interoperability.   To show its conceptual power, we apply the reference model to assess several other architectural models, communication technologies and so called software design or architectural styles like SOA and REST.

</details>

<details>

<summary>2019-08-15 19:56:17 - Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers</summary>

- *Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, Chinmay Hegde*

- `1904.08489v2` - [abs](http://arxiv.org/abs/1904.08489v2) - [pdf](http://arxiv.org/pdf/1904.08489v2)

> Deep neural networks have been shown to exhibit an intriguing vulnerability to adversarial input images corrupted with imperceptible perturbations. However, the majority of adversarial attacks assume global, fine-grained control over the image pixel space. In this paper, we consider a different setting: what happens if the adversary could only alter specific attributes of the input image? These would generate inputs that might be perceptibly different, but still natural-looking and enough to fool a classifier. We propose a novel approach to generate such `semantic' adversarial examples by optimizing a particular adversarial loss over the range-space of a parametric conditional generative model. We demonstrate implementations of our attacks on binary classifiers trained on face images, and show that such natural-looking semantic adversarial examples exist. We evaluate the effectiveness of our attack on synthetic and real data, and present detailed comparisons with existing attack methods. We supplement our empirical results with theoretical bounds that demonstrate the existence of such parametric adversarial examples.

</details>

<details>

<summary>2019-08-16 04:24:38 - Tag2Pix: Line Art Colorization Using Text Tag With SECat and Changing Loss</summary>

- *Hyunsu Kim, Ho Young Jhoo, Eunhyeok Park, Sungjoo Yoo*

- `1908.05840v1` - [abs](http://arxiv.org/abs/1908.05840v1) - [pdf](http://arxiv.org/pdf/1908.05840v1)

> Line art colorization is expensive and challenging to automate. A GAN approach is proposed, called Tag2Pix, of line art colorization which takes as input a grayscale line art and color tag information and produces a quality colored image. First, we present the Tag2Pix line art colorization dataset. A generator network is proposed which consists of convolutional layers to transform the input line art, a pre-trained semantic extraction network, and an encoder for input color information. The discriminator is based on an auxiliary classifier GAN to classify the tag information as well as genuineness. In addition, we propose a novel network structure called SECat, which makes the generator properly colorize even small features such as eyes, and also suggest a novel two-step training method where the generator and discriminator first learn the notion of object and shape and then, based on the learned notion, learn colorization, such as where and how to place which color. We present both quantitative and qualitative evaluations which prove the effectiveness of the proposed method.

</details>

<details>

<summary>2019-08-16 04:29:52 - A Reliable IoT-Based Embedded Health Care System for Diabetic Patients</summary>

- *Zeyad A. Al-Odat, Sudarshan K. Srinivasan, Eman M. Al-Qtiemat, Sana Shuja*

- `1908.06086v1` - [abs](http://arxiv.org/abs/1908.06086v1) - [pdf](http://arxiv.org/pdf/1908.06086v1)

> This paper introduces a reliable health care system for diabetic patients based on the Internet of Things technology. A diabetic health care system with a hardware implementation is presented. The proposed work employs Alaris 8100 infusion pump, Keil LPC-1768 board, and IoT-cloud to monitor the diabetic patients. The security of diabetic data over the cloud and the communication channel between health care system components are considered as part of the main contributions of this work. Moreover, an easy way to control and monitor the diabetic insulin pump is implemented. The \mbox{patient\textquotesingle s} records are stored in the cloud using the Keil board that is connected to the infusion pump. The reliability of the proposed scheme is accomplished by testing the system for five performance characteristics (availability, confidentiality, integrity, authentication, and authorization). The Kiel board is embedded with Ethernet port and Cortex-M3 micro-controller that controls the insulin infusion pump. The secure hash algorithm and secure socket shell are employed to achieve the reliability components of the proposed scheme. The results show that the proposed design is reliable, secure and authentic according to different test experiments and a case study of the Markov model. Moreover, a 99.3\% availability probability has been achieved after analyzing the case study.

</details>

<details>

<summary>2019-08-16 12:38:05 - How Sequence-to-Sequence Models Perceive Language Styles?</summary>

- *Ruozi Huang, Mi Zhang, Xudong Pan, Beina Sheng*

- `1908.05947v1` - [abs](http://arxiv.org/abs/1908.05947v1) - [pdf](http://arxiv.org/pdf/1908.05947v1)

> Style is ubiquitous in our daily language uses, while what is language style to learning machines? In this paper, by exploiting the second-order statistics of semantic vectors of different corpora, we present a novel perspective on this question via style matrix, i.e. the covariance matrix of semantic vectors, and explain for the first time how Sequence-to-Sequence models encode style information innately in its semantic vectors. As an application, we devise a learning-free text style transfer algorithm, which explicitly constructs a pair of transfer operators from the style matrices for style transfer. Moreover, our algorithm is also observed to be flexible enough to transfer out-of-domain sentences. Extensive experimental evidence justifies the informativeness of style matrix and the competitive performance of our proposed style transfer algorithm with the state-of-the-art methods.

</details>

<details>

<summary>2019-08-16 15:42:29 - Wi-Fringe: Leveraging Text Semantics in WiFi CSI-Based Device-Free Named Gesture Recognition</summary>

- *Md Tamzeed Islam, Shahriar Nirjon*

- `1908.06803v1` - [abs](http://arxiv.org/abs/1908.06803v1) - [pdf](http://arxiv.org/pdf/1908.06803v1)

> The lack of adequate training data is one of the major hurdles in WiFi-based activity recognition systems. In this paper, we propose Wi-Fringe, which is a WiFi CSI-based device-free human gesture recognition system that recognizes named gestures, i.e., activities and gestures that have a semantically meaningful name in English language, as opposed to arbitrary free-form gestures. Given a list of activities (only their names in English text), along with zero or more training examples (WiFi CSI values) per activity, Wi-Fringe is able to detect all activities at runtime. In other words, a subset of activities that Wi-Fringe detects do not require any training examples at all.

</details>

<details>

<summary>2019-08-16 17:11:32 - Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints</summary>

- *Ning Yu, Larry Davis, Mario Fritz*

- `1811.08180v3` - [abs](http://arxiv.org/abs/1811.08180v3) - [pdf](http://arxiv.org/pdf/1811.08180v3)

> Recent advances in Generative Adversarial Networks (GANs) have shown increasing success in generating photorealistic images. But they also raise challenges to visual forensics and model attribution. We present the first study of learning GAN fingerprints towards image attribution and using them to classify an image as real or GAN-generated. For GAN-generated images, we further identify their sources. Our experiments show that (1) GANs carry distinct model fingerprints and leave stable fingerprints in their generated images, which support image attribution; (2) even minor differences in GAN training can result in different fingerprints, which enables fine-grained model authentication; (3) fingerprints persist across different image frequencies and patches and are not biased by GAN artifacts; (4) fingerprint finetuning is effective in immunizing against five types of adversarial image perturbations; and (5) comparisons also show our learned fingerprints consistently outperform several baselines in a variety of setups.

</details>

<details>

<summary>2019-08-16 20:25:13 - Shallow Domain Adaptive Embeddings for Sentiment Analysis</summary>

- *Prathusha K Sarma, Yingyu Liang, William A Sethares*

- `1908.06082v1` - [abs](http://arxiv.org/abs/1908.06082v1) - [pdf](http://arxiv.org/pdf/1908.06082v1)

> This paper proposes a way to improve the performance of existing algorithms for text classification in domains with strong language semantics. We propose a domain adaptation layer learns weights to combine a generic and a domain specific (DS) word embedding into a domain adapted (DA) embedding. The DA word embeddings are then used as inputs to a generic encoder + classifier framework to perform a downstream task such as classification. This adaptation layer is particularly suited to datasets that are modest in size, and which are, therefore, not ideal candidates for (re)training a deep neural network architecture. Results on binary and multi-class classification tasks using popular encoder architectures, including current state-of-the-art methods (with and without the shallow adaptation layer) show the effectiveness of the proposed approach.

</details>

<details>

<summary>2019-08-17 18:17:28 - Shift-of-Perspective Identification Within Legal Cases</summary>

- *Gathika Ratnayaka, Thejan Rupasinghe, Nisansa de Silva, Viraj Salaka Gamage, Menuka Warushavithana, Amal Shehan Perera*

- `1906.02430v4` - [abs](http://arxiv.org/abs/1906.02430v4) - [pdf](http://arxiv.org/pdf/1906.02430v4)

> Arguments, counter-arguments, facts, and evidence obtained via documents related to previous court cases are of essential need for legal professionals. Therefore, the process of automatic information extraction from documents containing legal opinions related to court cases can be considered to be of significant importance. This study is focused on the identification of sentences in legal opinion texts which convey different perspectives on a certain topic or entity. We combined several approaches based on semantic analysis, open information extraction, and sentiment analysis to achieve our objective. Then, our methodology was evaluated with the help of human judges. The outcomes of the evaluation demonstrate that our system is successful in detecting situations where two sentences deliver different opinions on the same topic or entity. The proposed methodology can be used to facilitate other information extraction tasks related to the legal domain. One such task is the automated detection of counter arguments for a given argument. Another is the identification of opponent parties in a court case.

</details>

<details>

<summary>2019-08-17 23:08:30 - Hybrid Deep Network for Anomaly Detection</summary>

- *Trong Nguyen Nguyen, Jean Meunier*

- `1908.06347v1` - [abs](http://arxiv.org/abs/1908.06347v1) - [pdf](http://arxiv.org/pdf/1908.06347v1)

> In this paper, we propose a deep convolutional neural network (CNN) for anomaly detection in surveillance videos. The model is adapted from a typical auto-encoder working on video patches under the perspective of sparse combination learning. Our CNN focuses on (unsupervisedly) learning common characteristics of normal events with the emphasis of their spatial locations (by supervised losses). To our knowledge, this is the first work that directly adapts the patch position as the target of a classification sub-network. The model is capable to provide a score of anomaly assessment for each video frame. Our experiments were performed on 4 benchmark datasets with various anomalous events and the obtained results were competitive with state-of-the-art studies.

</details>

<details>

<summary>2019-08-18 05:46:39 - DECT-MULTRA: Dual-Energy CT Image Decomposition With Learned Mixed Material Models and Efficient Clustering</summary>

- *Zhipeng Li, Saiprasad Ravishankar, Yong Long, Jeffrey A. Fessler*

- `1901.00106v2` - [abs](http://arxiv.org/abs/1901.00106v2) - [pdf](http://arxiv.org/pdf/1901.00106v2)

> Dual energy computed tomography (DECT) imaging plays an important role in advanced imaging applications due to its material decomposition capability. Image-domain decomposition operates directly on CT images using linear matrix inversion, but the decomposed material images can be severely degraded by noise and artifacts. This paper proposes a new method dubbed DECT-MULTRA for image-domain DECT material decomposition that combines conventional penalized weighted-least squares (PWLS) estimation with regularization based on a mixed union of learned transforms (MULTRA) model. Our proposed approach pre-learns a union of common-material sparsifying transforms from patches extracted from all the basis materials, and a union of cross-material sparsifying transforms from multi-material patches. The common-material transforms capture the common properties among different material images, while the cross-material transforms capture the cross-dependencies. The proposed PWLS formulation is optimized efficiently by alternating between an image update step and a sparse coding and clustering step, with both of these steps having closed-form solutions. The effectiveness of our method is validated with both XCAT phantom and clinical head data. The results demonstrate that our proposed method provides superior material image quality and decomposition accuracy compared to other competing methods.

</details>

<details>

<summary>2019-08-18 13:02:02 - Generative Adversarial Networks for Extreme Learned Image Compression</summary>

- *Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, Luc Van Gool*

- `1804.02958v3` - [abs](http://arxiv.org/abs/1804.02958v3) - [pdf](http://arxiv.org/pdf/1804.02958v3)

> We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.

</details>

<details>

<summary>2019-08-19 06:40:52 - Deep neural network or dermatologist?</summary>

- *Kyle Young, Gareth Booth, Becks Simpson, Reuben Dutton, Sally Shrapnel*

- `1908.06612v1` - [abs](http://arxiv.org/abs/1908.06612v1) - [pdf](http://arxiv.org/pdf/1908.06612v1)

> Deep learning techniques have proven high accuracy for identifying melanoma in digitised dermoscopic images. A strength is that these methods are not constrained by features that are pre-defined by human semantics. A down-side is that it is difficult to understand the rationale of the model predictions and to identify potential failure modes. This is a major barrier to adoption of deep learning in clinical practice. In this paper we ask if two existing local interpretability methods, Grad-CAM and Kernel SHAP, can shed light on convolutional neural networks trained in the context of melanoma detection. Our contributions are (i) we first explore the domain space via a reproducible, end-to-end learning framework that creates a suite of 30 models, all trained on a publicly available data set (HAM10000), (ii) we next explore the reliability of GradCAM and Kernel SHAP in this context via some basic sanity check experiments (iii) finally, we investigate a random selection of models from our suite using GradCAM and Kernel SHAP. We show that despite high accuracy, the models will occasionally assign importance to features that are not relevant to the diagnostic task. We also show that models of similar accuracy will produce different explanations as measured by these methods. This work represents first steps in bridging the gap between model accuracy and interpretability in the domain of skin cancer classification.

</details>

<details>

<summary>2019-08-19 07:14:41 - Cross-modal Zero-shot Hashing</summary>

- *Xuanwu Liu, Zhao Li, Jun Wang, Guoxian Yu, Carlotta Domeniconi, Xiangliang Zhang*

- `1908.07388v1` - [abs](http://arxiv.org/abs/1908.07388v1) - [pdf](http://arxiv.org/pdf/1908.07388v1)

> Hashing has been widely studied for big data retrieval due to its low storage cost and fast query speed. Zero-shot hashing (ZSH) aims to learn a hashing model that is trained using only samples from seen categories, but can generalize well to samples of unseen categories. ZSH generally uses category attributes to seek a semantic embedding space to transfer knowledge from seen categories to unseen ones. As a result, it may perform poorly when labeled data are insufficient. ZSH methods are mainly designed for single-modality data, which prevents their application to the widely spread multi-modal data. On the other hand, existing cross-modal hashing solutions assume that all the modalities share the same category labels, while in practice the labels of different data modalities may be different. To address these issues, we propose a general Cross-modal Zero-shot Hashing (CZHash) solution to effectively leverage unlabeled and labeled multi-modality data with different label spaces. CZHash first quantifies the composite similarity between instances using label and feature information. It then defines an objective function to achieve deep feature learning compatible with the composite similarity preserving, category attribute space learning, and hashing coding function learning. CZHash further introduces an alternative optimization procedure to jointly optimize these learning objectives. Experiments on benchmark multi-modal datasets show that CZHash significantly outperforms related representative hashing approaches both on effectiveness and adaptability.

</details>

<details>

<summary>2019-08-19 10:06:53 - Reducing the Effort for Systematic Reviews in Software Engineering</summary>

- *Francesco Osborne, Henry Muccini, Patricia Lago, Enrico Motta*

- `1908.06676v1` - [abs](http://arxiv.org/abs/1908.06676v1) - [pdf](http://arxiv.org/pdf/1908.06676v1)

> Context. Systematic Reviews (SRs) are means for collecting and synthesizing evidence from the identification and analysis of relevant studies from multiple sources. To this aim, they use a well-defined methodology meant to mitigate the risks of biases and ensure repeatability for later updates. SRs, however, involve significant effort. Goal. The goal of this paper is to introduce a novel methodology that reduces the amount of manual tedious tasks involved in SRs while taking advantage of the value provided by human expertise. Method. Starting from current methodologies for SRs, we replaced the steps of keywording and data extraction with an automatic methodology for generating a domain ontology and classifying the primary studies. This methodology has been applied in the Software Engineering sub-area of Software Architecture and evaluated by human annotators. Results. The result is a novel Expert-Driven Automatic Methodology, EDAM, for assisting researchers in performing SRs. EDAM combines ontology-learning techniques and semantic technologies with the human-in-the-loop. The first (thanks to automation) fosters scalability, objectivity, reproducibility and granularity of the studies; the second allows tailoring to the specific focus of the study at hand and knowledge reuse from domain experts. We evaluated EDAM on the field of Software Architecture against six senior researchers. As a result, we found that the performance of the senior researchers in classifying papers was not statistically significantly different from EDAM. Conclusions. Thanks to automation of the less-creative steps in SRs, our methodology allows researchers to skip the tedious tasks of keywording and manually classifying primary studies, thus freeing effort for the analysis and the discussion.

</details>

<details>

<summary>2019-08-19 12:53:49 - DeepGCNs: Can GCNs Go as Deep as CNNs?</summary>

- *Guohao Li, Matthias Müller, Ali Thabet, Bernard Ghanem*

- `1904.03751v2` - [abs](http://arxiv.org/abs/1904.03751v2) - [pdf](http://arxiv.org/pdf/1904.03751v2)

> Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.

</details>

<details>

<summary>2019-08-19 15:54:04 - The Strengths and Behavioral Quirks of Java Bytecode Decompilers</summary>

- *Nicolas Harrand, César Soto-Valero, Martin Monperrus, Benoit Baudry*

- `1908.06895v1` - [abs](http://arxiv.org/abs/1908.06895v1) - [pdf](http://arxiv.org/pdf/1908.06895v1)

> During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers.   We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total).   Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. Even the highest ranking decompiler in this study produces syntactically correct output for 84% of classes of our dataset and semantically equivalent code output for 78% of classes.

</details>

<details>

<summary>2019-08-19 17:26:13 - Why So Down? The Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics</summary>

- *Alexandre Salle, Aline Villavicencio*

- `1908.06941v1` - [abs](http://arxiv.org/abs/1908.06941v1) - [pdf](http://arxiv.org/pdf/1908.06941v1)

> In distributional semantics, the pointwise mutual information ($\mathit{PMI}$) weighting of the cooccurrence matrix performs far better than raw counts. There is, however, an issue with unobserved pair cooccurrences as $\mathit{PMI}$ goes to negative infinity. This problem is aggravated by unreliable statistics from finite corpora which lead to a large number of such pairs. A common practice is to clip negative $\mathit{PMI}$ ($\mathit{\texttt{-} PMI}$) at $0$, also known as Positive $\mathit{PMI}$ ($\mathit{PPMI}$). In this paper, we investigate alternative ways of dealing with $\mathit{\texttt{-} PMI}$ and, more importantly, study the role that negative information plays in the performance of a low-rank, weighted factorization of different $\mathit{PMI}$ matrices. Using various semantic and syntactic tasks as probes into models which use either negative or positive $\mathit{PMI}$ (or both), we find that most of the encoded semantics and syntax come from positive $\mathit{PMI}$, in contrast to $\mathit{\texttt{-} PMI}$ which contributes almost exclusively syntactic information. Our findings deepen our understanding of distributional semantics, while also introducing novel $PMI$ variants and grounding the popular $PPMI$ measure.

</details>

<details>

<summary>2019-08-19 19:02:14 - Topic Augmented Generator for Abstractive Summarization</summary>

- *Melissa Ailem, Bowen Zhang, Fei Sha*

- `1908.07026v1` - [abs](http://arxiv.org/abs/1908.07026v1) - [pdf](http://arxiv.org/pdf/1908.07026v1)

> Steady progress has been made in abstractive summarization with attention-based sequence-to-sequence learning models. In this paper, we propose a new decoder where the output summary is generated by conditioning on both the input text and the latent topics of the document. The latent topics, identified by a topic model such as LDA, reveals more global semantic information that can be used to bias the decoder to generate words. In particular, they enable the decoder to have access to additional word co-occurrence statistics captured at document corpus level. We empirically validate the advantage of the proposed approach on both the CNN/Daily Mail and the WikiHow datasets. Concretely, we attain strongly improved ROUGE scores when compared to state-of-the-art models.

</details>

<details>

<summary>2019-08-20 03:22:57 - A Symbolic Neural Network Representation and its Application to Understanding, Verifying, and Patching Networks</summary>

- *Matthew Sotoudeh, Aditya V. Thakur*

- `1908.06223v2` - [abs](http://arxiv.org/abs/1908.06223v2) - [pdf](http://arxiv.org/pdf/1908.06223v2)

> Analysis and manipulation of trained neural networks is a challenging and important problem. We propose a symbolic representation for piecewise-linear neural networks and discuss its efficient computation. With this representation, one can translate the problem of analyzing a complex neural network into that of analyzing a finite set of affine functions. We demonstrate the use of this representation for three applications. First, we apply the symbolic representation to computing weakest preconditions on network inputs, which we use to exactly visualize the advisories made by a network meant to operate an aircraft collision avoidance system. Second, we use the symbolic representation to compute strongest postconditions on the network outputs, which we use to perform bounded model checking on standard neural network controllers. Finally, we show how the symbolic representation can be combined with a new form of neural network to perform patching; i.e., correct user-specified behavior of the network.

</details>

<details>

<summary>2019-08-20 03:50:22 - Globally-Aware Multiple Instance Classifier for Breast Cancer Screening</summary>

- *Yiqiu Shen, Nan Wu, Jason Phang, Jungkyu Park, Gene Kim, Linda Moy, Kyunghyun Cho, Krzysztof J. Geras*

- `1906.02846v2` - [abs](http://arxiv.org/abs/1906.02846v2) - [pdf](http://arxiv.org/pdf/1906.02846v2)

> Deep learning models designed for visual classification tasks on natural images have become prevalent in medical image analysis. However, medical images differ from typical natural images in many ways, such as significantly higher resolutions and smaller regions of interest. Moreover, both the global structure and local details play important roles in medical image analysis tasks. To address these unique properties of medical images, we propose a neural network that is able to classify breast cancer lesions utilizing information from both a global saliency map and multiple local patches. The proposed model outperforms the ResNet-based baseline and achieves radiologist-level performance in the interpretation of screening mammography. Although our model is trained only with image-level labels, it is able to generate pixel-level saliency maps that provide localization of possible malignant findings.

</details>

<details>

<summary>2019-08-20 09:51:24 - Addressing Model Vulnerability to Distributional Shifts over Image Transformation Sets</summary>

- *Riccardo Volpi, Vittorio Murino*

- `1903.11900v2` - [abs](http://arxiv.org/abs/1903.11900v2) - [pdf](http://arxiv.org/pdf/1903.11900v2)

> We are concerned with the vulnerability of computer vision models to distributional shifts. We formulate a combinatorial optimization problem that allows evaluating the regions in the image space where a given model is more vulnerable, in terms of image transformations applied to the input, and face it with standard search algorithms. We further embed this idea in a training procedure, where we define new data augmentation rules according to the image transformations that the current model is most vulnerable to, over iterations. An empirical evaluation on classification and semantic segmentation problems suggests that the devised algorithm allows to train models that are more robust against content-preserving image manipulations and, in general, against distributional shifts.

</details>

<details>

<summary>2019-08-20 12:49:12 - Style Transformer: Unpaired Text Style Transfer without Disentangled Latent Representation</summary>

- *Ning Dai, Jianze Liang, Xipeng Qiu, Xuanjing Huang*

- `1905.05621v3` - [abs](http://arxiv.org/abs/1905.05621v3) - [pdf](http://arxiv.org/pdf/1905.05621v3)

> Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difficult to completely strip the style information from the semantics for a sentence. 2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation.

</details>

<details>

<summary>2019-08-20 18:07:37 - Phrase Localization Without Paired Training Examples</summary>

- *Josiah Wang, Lucia Specia*

- `1908.07553v1` - [abs](http://arxiv.org/abs/1908.07553v1) - [pdf](http://arxiv.org/pdf/1908.07553v1)

> Localizing phrases in images is an important part of image understanding and can be useful in many applications that require mappings between textual and visual information. Existing work attempts to learn these mappings from examples of phrase-image region correspondences (strong supervision) or from phrase-image pairs (weak supervision). We postulate that such paired annotations are unnecessary, and propose the first method for the phrase localization problem where neither training procedure nor paired, task-specific data is required. Our method is simple but effective: we use off-the-shelf approaches to detect objects, scenes and colours in images, and explore different approaches to measure semantic similarity between the categories of detected visual elements and words in phrases. Experiments on two well-known phrase localization datasets show that this approach surpasses all weakly supervised methods by a large margin and performs very competitively to strongly supervised methods, and can thus be considered a strong baseline to the task. The non-paired nature of our method makes it applicable to any domain and where no paired phrase localization annotation is available.

</details>

<details>

<summary>2019-08-20 20:10:05 - From Text to Sound: A Preliminary Study on Retrieving Sound Effects to Radio Stories</summary>

- *Songwei Ge, Curtis Xuan, Ruihua Song, Chao Zou, Wei Liu, Jin Zhou*

- `1908.07590v1` - [abs](http://arxiv.org/abs/1908.07590v1) - [pdf](http://arxiv.org/pdf/1908.07590v1)

> Sound effects play an essential role in producing high-quality radio stories but require enormous labor cost to add. In this paper, we address the problem of automatically adding sound effects to radio stories with a retrieval-based model. However, directly implementing a tag-based retrieval model leads to high false positives due to the ambiguity of story contents. To solve this problem, we introduce a retrieval-based framework hybridized with a semantic inference model which helps to achieve robust retrieval results. Our model relies on fine-designed features extracted from the context of candidate triggers. We collect two story dubbing datasets through crowdsourcing to analyze the setting of adding sound effects and to train and test our proposed methods. We further discuss the importance of each feature and introduce several heuristic rules for the trade-off between precision and recall. Together with the text-to-speech technology, our results reveal a promising automatic pipeline on producing high-quality radio stories.

</details>

<details>

<summary>2019-08-20 23:35:41 - Sparse Generative Adversarial Network</summary>

- *Shahin Mahdizadehaghdam, Ashkan Panahi, Hamid Krim*

- `1908.08930v1` - [abs](http://arxiv.org/abs/1908.08930v1) - [pdf](http://arxiv.org/pdf/1908.08930v1)

> We propose a new approach to Generative Adversarial Networks (GANs) to achieve an improved performance with additional robustness to its so-called and well recognized mode collapse. We first proceed by mapping the desired data onto a frame-based space for a sparse representation to lift any limitation of small support features prior to learning the structure. To that end we start by dividing an image into multiple patches and modifying the role of the generative network from producing an entire image, at once, to creating a sparse representation vector for each image patch. We synthesize an entire image by multiplying generated sparse representations to a pre-trained dictionary and assembling the resulting patches. This approach restricts the output of the generator to a particular structure, obtained by imposing a Union of Subspaces (UoS) model to the original training data, leading to more realistic images, while maintaining a desired diversity. To further regularize GANs in generating high-quality images and to avoid the notorious mode-collapse problem, we introduce a third player in GANs, called reconstructor. This player utilizes an auto-encoding scheme to ensure that first, the input-output relation in the generator is injective and second each real image corresponds to some input noise. We present a number of experiments, where the proposed algorithm shows a remarkably higher inception score compared to the equivalent conventional GANs.

</details>

<details>

<summary>2019-08-21 04:03:32 - Classification of Functioning, Disability, and Health for Children and Youth: ICF-CY Self Care (SCADI Dataset) Using Predictive Analytics</summary>

- *Avishek Choudhury, Christopher Greene*

- `1901.00756v3` - [abs](http://arxiv.org/abs/1901.00756v3) - [pdf](http://arxiv.org/pdf/1901.00756v3)

> The International Classification of Functioning, Disability, and Health for Children and Youth (ICF-CY) is a scaffold for designating and systematizing data on functioning and disability. It offers a standard semantic and a theoretical foundation for the demarcation and extent of wellbeing and infirmity. The multidimensional layout of ICF-CY comprehends a plethora of information with about 1400 categories making it difficult to analyze. Our research proposes a predictive model that classify self-care problems on Self-Care Activities Dataset based on the ICF- CY. The data used in this study resides 206 attributes of 70 children with motor and physical disability. Our study implements, compare and analyze Random Forest, Support vector machine, Naive Bayes, Hoeffding tree, and Lazy locally weighted learning using two-tailed T-test at 95% confidence interval. Boruta algorithm involved in the study minimizes the data dimensionality to advocate the minimal-optimal set of predictors. Random forest gave the best classification accuracy of 84.75%; root mean squared error of 0.18 and receiver operating characteristic of 0.99. Predictive analytics can simplify the usage of ICF-CY by automating the classification process of disability, functioning, and health.

</details>

<details>

<summary>2019-08-21 04:05:54 - Copy-Enhanced Heterogeneous Information Learning for Dialogue State Tracking</summary>

- *Qingbin Liu, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao*

- `1908.07705v1` - [abs](http://arxiv.org/abs/1908.07705v1) - [pdf](http://arxiv.org/pdf/1908.07705v1)

> Dialogue state tracking (DST) is an essential component in task-oriented dialogue systems, which estimates user goals at every dialogue turn. However, most previous approaches usually suffer from the following problems. Many discriminative models, especially end-to-end (E2E) models, are difficult to extract unknown values that are not in the candidate ontology; previous generative models, which can extract unknown values from utterances, degrade the performance due to ignoring the semantic information of pre-defined ontology. Besides, previous generative models usually need a hand-crafted list to normalize the generated values. How to integrate the semantic information of pre-defined ontology and dialogue text (heterogeneous texts) to generate unknown values and improve performance becomes a severe challenge. In this paper, we propose a Copy-Enhanced Heterogeneous Information Learning model with multiple encoder-decoder for DST (CEDST), which can effectively generate all possible values including unknown values by copying values from heterogeneous texts. Meanwhile, CEDST can effectively decompose the large state space into several small state spaces through multi-encoder, and employ multi-decoder to make full use of the reduced spaces to generate values. Multi-encoder-decoder architecture can significantly improve performance. Experiments show that CEDST can achieve state-of-the-art results on two datasets and our constructed datasets with many unknown values.

</details>

<details>

<summary>2019-08-21 06:22:08 - Interactive Variance Attention based Online Spoiler Detection for Time-Sync Comments</summary>

- *Wenmian Yang, Weijia Jia, Wenyuan Gao, Xiaojie Zhou, Yutao Luo*

- `1908.03451v2` - [abs](http://arxiv.org/abs/1908.03451v2) - [pdf](http://arxiv.org/pdf/1908.03451v2)

> Nowadays, time-sync comment (TSC), a new form of interactive comments, has become increasingly popular in Chinese video websites. By posting TSCs, people can easily express their feelings and exchange their opinions with others when watching online videos. However, some spoilers appear among the TSCs. These spoilers reveal crucial plots in videos that ruin people's surprise when they first watch the video. In this paper, we proposed a novel Similarity-Based Network with Interactive Variance Attention (SBN-IVA) to classify comments as spoilers or not. In this framework, we firstly extract textual features of TSCs through the word-level attentive encoder. We design Similarity-Based Network (SBN) to acquire neighbor and keyframe similarity according to semantic similarity and timestamps of TSCs. Then, we implement Interactive Variance Attention (IVA) to eliminate the impact of noise comments. Finally, we obtain the likelihood of spoiler based on the difference between the neighbor and keyframe similarity. Experiments show SBN-IVA is on average 11.2\% higher than the state-of-the-art method on F1-score in baselines.

</details>

<details>

<summary>2019-08-21 09:34:04 - ATM:Adversarial-neural Topic Model</summary>

- *Rui Wang, Deyu Zhou, Yulan He*

- `1811.00265v2` - [abs](http://arxiv.org/abs/1811.00265v2) - [pdf](http://arxiv.org/pdf/1811.00265v2)

> Topic models are widely used for thematic structure discovery in text. But traditional topic models often require dedicated inference procedures for specific tasks at hand. Also, they are not designed to generate word-level semantic representations. To address these limitations, we propose a topic modeling approach based on Generative Adversarial Nets (GANs), called Adversarial-neural Topic Model (ATM). The proposed ATM models topics with Dirichlet prior and employs a generator network to capture the semantic patterns among latent topics. Meanwhile, the generator could also produce word-level semantic representations. To illustrate the feasibility of porting ATM to tasks other than topic modeling, we apply ATM for open domain event extraction. Our experimental results on the two public corpora show that ATM generates more coherence topics, outperforming a number of competitive baselines. Moreover, ATM is able to extract meaningful events from news articles.

</details>

<details>

<summary>2019-08-21 14:14:44 - Representation Disentanglement for Multi-task Learning with application to Fetal Ultrasound</summary>

- *Qingjie Meng, Nick Pawlowski, Daniel Rueckert, Bernhard Kainz*

- `1908.07885v1` - [abs](http://arxiv.org/abs/1908.07885v1) - [pdf](http://arxiv.org/pdf/1908.07885v1)

> One of the biggest challenges for deep learning algorithms in medical image analysis is the indiscriminate mixing of image properties, e.g. artifacts and anatomy. These entangled image properties lead to a semantically redundant feature encoding for the relevant task and thus lead to poor generalization of deep learning algorithms. In this paper we propose a novel representation disentanglement method to extract semantically meaningful and generalizable features for different tasks within a multi-task learning framework. Deep neural networks are utilized to ensure that the encoded features are maximally informative with respect to relevant tasks, while an adversarial regularization encourages these features to be disentangled and minimally informative about irrelevant tasks. We aim to use the disentangled representations to generalize the applicability of deep neural networks. We demonstrate the advantages of the proposed method on synthetic data as well as fetal ultrasound images. Our experiments illustrate that our method is capable of learning disentangled internal representations. It outperforms baseline methods in multiple tasks, especially on images with new properties, e.g. previously unseen artifacts in fetal ultrasound.

</details>

<details>

<summary>2019-08-21 16:42:04 - Scene Graph Reasoning with Prior Visual Relationship for Visual Question Answering</summary>

- *Zhuoqian Yang, Zengchang Qin, Jing Yu, Yue Hu*

- `1812.09681v2` - [abs](http://arxiv.org/abs/1812.09681v2) - [pdf](http://arxiv.org/pdf/1812.09681v2)

> One of the key issues of Visual Question Answering (VQA) is to reason with semantic clues in the visual content under the guidance of the question, how to model relational semantics still remains as a great challenge. To fully capture visual semantics, we propose to reason over a structured visual representation - scene graph, with embedded objects and inter-object relationships. This shows great benefit over vanilla vector representations and implicit visual relationship learning. Based on existing visual relationship models, we propose a visual relationship encoder that projects visual relationships into a learned deep semantic space constrained by visual context and language priors. Upon the constructed graph, we propose a Scene Graph Convolutional Network (SceneGCN) to jointly reason the object properties and relational semantics for the correct answer. We demonstrate the model's effectiveness and interpretability on the challenging GQA dataset and the classical VQA 2.0 dataset, remarkably achieving state-of-the-art 54.56% accuracy on GQA compared to the existing best model.

</details>

<details>

<summary>2019-08-22 03:27:48 - Report on the First Knowledge Graph Reasoning Challenge 2018 -- Toward the eXplainable AI System</summary>

- *Takahiro Kawamura, Shusaku Egami, Koutarou Tamura, Yasunori Hokazono, Takanori Ugai, Yusuke Koyanagi, Fumihito Nishino, Seiji Okajima, Katsuhiko Murakami, Kunihiko Takamatsu, Aoi Sugiura, Shun Shiramatsu, Shawn Zhang, Kouji Kozaki*

- `1908.08184v1` - [abs](http://arxiv.org/abs/1908.08184v1) - [pdf](http://arxiv.org/pdf/1908.08184v1)

> A new challenge for knowledge graph reasoning started in 2018. Deep learning has promoted the application of artificial intelligence (AI) techniques to a wide variety of social problems. Accordingly, being able to explain the reason for an AI decision is becoming important to ensure the secure and safe use of AI techniques. Thus, we, the Special Interest Group on Semantic Web and Ontology of the Japanese Society for AI, organized a challenge calling for techniques that reason and/or estimate which characters are criminals while providing a reasonable explanation based on an open knowledge graph of a well-known Sherlock Holmes mystery story. This paper presents a summary report of the first challenge held in 2018, including the knowledge graph construction, the techniques proposed for reasoning and/or estimation, the evaluation metrics, and the results. The first prize went to an approach that formalized the problem as a constraint satisfaction problem and solved it using a lightweight formal method; the second prize went to an approach that used SPARQL and rules; the best resource prize went to a submission that constructed word embedding of characters from all sentences of Sherlock Holmes novels; and the best idea prize went to a discussion multi-agents model. We conclude this paper with the plans and issues for the next challenge in 2019.

</details>

<details>

<summary>2019-08-22 07:05:17 - BePT: A Behavior-based Process Translator for Interpreting and Understanding Process Models</summary>

- *Chen Qian, Lijie Wen, Akhil Kumar*

- `1904.09768v2` - [abs](http://arxiv.org/abs/1904.09768v2) - [pdf](http://arxiv.org/pdf/1904.09768v2)

> Sharing process models on the web has emerged as a common practice. Users can collect and share their experimental process models with others. However, some users always feel confused about the shared process models for lack of necessary guidelines or instructions. Therefore, several process translators have been proposed to explain the semantics of process models in natural language (NL). We find that previous studies suffer from information loss and generate semantically erroneous descriptions that diverge from original model behaviors. In this paper, we propose a novel process translator named BePT (Behavior-based Process Translator) based on the encoder-decoder paradigm, encoding a process model into a middle representation and decoding the representation into NL descriptions. Our theoretical analysis demonstrates that BePT satisfies behavior correctness, behavior completeness and description minimality. The qualitative and quantitative experiments show that BePT outperforms the state-of-the-art baselines.

</details>

<details>

<summary>2019-08-22 07:08:32 - motif2vec: Motif Aware Node Representation Learning for Heterogeneous Networks</summary>

- *Manoj Reddy Dareddy, Mahashweta Das, Hao Yang*

- `1908.08227v1` - [abs](http://arxiv.org/abs/1908.08227v1) - [pdf](http://arxiv.org/pdf/1908.08227v1)

> Recent years have witnessed a surge of interest in machine learning on graphs and networks with applications ranging from vehicular network design to IoT traffic management to social network recommendations. Supervised machine learning tasks in networks such as node classification and link prediction require us to perform feature engineering that is known and agreed to be the key to success in applied machine learning. Research efforts dedicated to representation learning, especially representation learning using deep learning, has shown us ways to automatically learn relevant features from vast amounts of potentially noisy, raw data. However, most of the methods are not adequate to handle heterogeneous information networks which pretty much represents most real-world data today. The methods cannot preserve the structure and semantic of multiple types of nodes and links well enough, capture higher-order heterogeneous connectivity patterns, and ensure coverage of nodes for which representations are generated. We propose a novel efficient algorithm, motif2vec that learns node representations or embeddings for heterogeneous networks. Specifically, we leverage higher-order, recurring, and statistically significant network connectivity patterns in the form of motifs to transform the original graph to motif graph(s), conduct biased random walk to efficiently explore higher order neighborhoods, and then employ heterogeneous skip-gram model to generate the embeddings. Unlike previous efforts that uses different graph meta-structures to guide the random walk, we use graph motifs to transform the original network and preserve the heterogeneity. We evaluate the proposed algorithm on multiple real-world networks from diverse domains and against existing state-of-the-art methods on multi-class node classification and link prediction tasks, and demonstrate its consistent superiority over prior work.

</details>

<details>

<summary>2019-08-22 13:02:20 - Automated Generation of Test Models from Semi-Structured Requirements</summary>

- *Jannik Fischbach, Maximilian Junker, Andreas Vogelsang, Dietmar Freudenstein*

- `1908.08810v1` - [abs](http://arxiv.org/abs/1908.08810v1) - [pdf](http://arxiv.org/pdf/1908.08810v1)

> [Context:] Model-based testing is an instrument for automated generation of test cases. It requires identifying requirements in documents, understanding them syntactically and semantically, and then translating them into a test model. One light-weight language for these test models are Cause-Effect-Graphs (CEG) that can be used to derive test cases. [Problem:] The creation of test models is laborious and we lack an automated solution that covers the entire process from requirement detection to test model creation. In addition, the majority of requirements is expressed in natural language (NL), which is hard to translate to test models automatically. [Principal Idea:] We build on the fact that not all NL requirements are equally unstructured. We found that 14 % of the lines in requirements documents of our industry partner contain "pseudo-code"-like descriptions of business rules. We apply Machine Learning to identify such semi-structured requirements descriptions and propose a rule-based approach for their translation into CEGs. [Contribution:] We make three contributions: (1) an algorithm for the automatic detection of semi-structured requirements descriptions in documents, (2) an algorithm for the automatic translation of the identified requirements into a CEG and (3) a study demonstrating that our proposed solution leads to 86 % time savings for test model creation without loss of quality.

</details>

<details>

<summary>2019-08-22 14:27:28 - SCF2 -- an Argumentation Semantics for Rational Human Judgments on Argument Acceptability: Technical Report</summary>

- *Marcos Cramer, Leendert van der Torre*

- `1908.08406v1` - [abs](http://arxiv.org/abs/1908.08406v1) - [pdf](http://arxiv.org/pdf/1908.08406v1)

> In abstract argumentation theory, many argumentation semantics have been proposed for evaluating argumentation frameworks. This paper is based on the following research question: Which semantics corresponds well to what humans consider a rational judgment on the acceptability of arguments? There are two systematic ways to approach this research question: A normative perspective is provided by the principle-based approach, in which semantics are evaluated based on their satisfaction of various normatively desirable principles. A descriptive perspective is provided by the empirical approach, in which cognitive studies are conducted to determine which semantics best predicts human judgments about arguments. In this paper, we combine both approaches to motivate a new argumentation semantics called SCF2. For this purpose, we introduce and motivate two new principles and show that no semantics from the literature satisfies both of them. We define SCF2 and prove that it satisfies both new principles. Furthermore, we discuss findings of a recent empirical cognitive study that provide additional support to SCF2.

</details>

<details>

<summary>2019-08-22 14:32:16 - Invariant Information Clustering for Unsupervised Image Classification and Segmentation</summary>

- *Xu Ji, João F. Henriques, Andrea Vedaldi*

- `1807.06653v4` - [abs](http://arxiv.org/abs/1807.06653v4) - [pdf](http://arxiv.org/pdf/1807.06653v4)

> We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC

</details>

<details>

<summary>2019-08-22 17:27:54 - Transfer Learning for Relation Extraction via Relation-Gated Adversarial Learning</summary>

- *Ningyu Zhang, Shumin Deng, Zhanlin Sun, Jiaoyan Chen, Wei Zhang, Huajun Chen*

- `1908.08507v1` - [abs](http://arxiv.org/abs/1908.08507v1) - [pdf](http://arxiv.org/pdf/1908.08507v1)

> Relation extraction aims to extract relational facts from sentences. Previous models mainly rely on manually labeled datasets, seed instances or human-crafted patterns, and distant supervision. However, the human annotation is expensive, while human-crafted patterns suffer from semantic drift and distant supervision samples are usually noisy. Domain adaptation methods enable leveraging labeled data from a different but related domain. However, different domains usually have various textual relation descriptions and different label space (the source label space is usually a superset of the target label space). To solve these problems, we propose a novel model of relation-gated adversarial learning for relation extraction, which extends the adversarial based domain adaptation. Experimental results have shown that the proposed approach outperforms previous domain adaptation methods regarding partial domain adaptation and can improve the accuracy of distance supervised relation extraction through fine-tuning.

</details>

<details>

<summary>2019-08-22 19:04:24 - Intent term selection and refinement in e-commerce queries</summary>

- *Saurav Manchanda, Mohit Sharma, George Karypis*

- `1908.08564v1` - [abs](http://arxiv.org/abs/1908.08564v1) - [pdf](http://arxiv.org/pdf/1908.08564v1)

> In e-commerce, a user tends to search for the desired product by issuing a query to the search engine and examining the retrieved results. If the search engine was successful in correctly understanding the user's query, it will return results that correspond to the products whose attributes match the terms in the query that are representative of the query's product intent. However, the search engine may fail to retrieve results that satisfy the query's product intent and thus degrading user experience due to different issues in query processing: (i) when multiple terms are present in a query it may fail to determine the relevant terms that are representative of the query's product intent, and (ii) it may suffer from vocabulary gap between the terms in the query and the product's description, i.e., terms used in the query are semantically similar but different from the terms in the product description. Hence, identifying the terms that describe the query's product intent and predicting additional terms that describe the query's product intent better than the existing query terms to the search engine is an essential task in e-commerce search. In this paper, we leverage the historical query reformulation logs of a major e-commerce retailer to develop distant-supervised approaches to solve both these problems. Our approaches exploit the fact that the significance of a term is dependent upon the context (other terms in the neighborhood) in which it is used in order to learn the importance of the term towards the query's product intent. We show that identifying and emphasizing the terms that define the query's product intent leads to a 3% improvement in ranking. Moreover, for the tasks of identifying the important terms in a query and for predicting the additional terms that represent product intent, experiments illustrate that our approaches outperform the non-contextual baselines.

</details>

<details>

<summary>2019-08-22 20:29:05 - Feedbackward Decoding for Semantic Segmentation</summary>

- *Beinan Wang, John Glossner, Daniel Iancu, Georgi N. Gaydadjiev*

- `1908.08584v1` - [abs](http://arxiv.org/abs/1908.08584v1) - [pdf](http://arxiv.org/pdf/1908.08584v1)

> We propose a novel approach for semantic segmentation that uses an encoder in the reverse direction to decode. Many semantic segmentation networks adopt a feedforward encoder-decoder architecture. Typically, an input is first downsampled by the encoder to extract high-level semantic features and continues to be fed forward through the decoder module to recover low-level spatial clues. Our method works in an alternative direction that lets information flow backward from the last layer of the encoder towards the first. The encoder performs encoding in the forward pass and the same network performs decoding in the backward pass. Therefore, the encoder itself is also the decoder. Compared to conventional encoder-decoder architectures, ours doesn't require additional layers for decoding and further reuses the encoder weights thereby reducing the total number of parameters required for processing. We show by using only the 13 convolutional layers from VGG-16 plus one tiny classification layer, our model significantly outperforms other frequently cited models that are also adapted from VGG-16. On the Cityscapes semantic segmentation benchmark, our model uses 50.0% less parameters than SegNet and achieves an 18.1% higher "IoU class" score; it uses 28.3% less parameters than DeepLab LargeFOV and the achieved "IoU class" score is 3.9% higher; it uses 89.1% fewer parameters than FCN-8s and the achieved "IoU class" score is 3.1% higher. Our code will be publicly available on Github later.

</details>

<details>

<summary>2019-08-23 11:04:48 - Toward Dialogue Modeling: A Semantic Annotation Scheme for Questions and Answers</summary>

- *Maria-Andrea Cruz-Blandón, Gosse Minnema, Aria Nourbakhsh, Maria Boritchev, Maxime Amblard*

- `1908.09921v1` - [abs](http://arxiv.org/abs/1908.09921v1) - [pdf](http://arxiv.org/pdf/1908.09921v1)

> The present study proposes an annotation scheme for classifying the content and discourse contribution of question-answer pairs. We propose detailed guidelines for using the scheme and apply them to dialogues in English, Spanish, and Dutch. Finally, we report on initial machine learning experiments for automatic annotation.

</details>

<details>

<summary>2019-08-24 00:47:39 - EyeNet: A Multi-Task Network for Off-Axis Eye Gaze Estimation and User Understanding</summary>

- *Zhengyang Wu, Srivignesh Rajendran, Tarrence van As, Joelle Zimmermann, Vijay Badrinarayanan, Andrew Rabinovich*

- `1908.09060v1` - [abs](http://arxiv.org/abs/1908.09060v1) - [pdf](http://arxiv.org/pdf/1908.09060v1)

> Eye gaze estimation and simultaneous semantic understanding of a user through eye images is a crucial component in Virtual and Mixed Reality; enabling energy efficient rendering, multi-focal displays and effective interaction with 3D content. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid blocking the user's gaze, this view-point makes drawing eye related inferences very challenging. In this work, we present EyeNet, the first single deep neural network which solves multiple heterogeneous tasks related to eye gaze estimation and semantic user understanding for an off-axis camera setting. The tasks include eye segmentation, blink detection, emotive expression classification, IR LED glints detection, pupil and cornea center estimation. To train EyeNet end-to-end we employ both hand labelled supervision and model based supervision. We benchmark all tasks on MagicEyes, a large and new dataset of 587 subjects with varying morphology, gender, skin-color, make-up and imaging conditions.

</details>

<details>

<summary>2019-08-24 15:52:57 - A framework for anomaly detection using language modeling, and its applications to finance</summary>

- *Armineh Nourbakhsh, Grace Bang*

- `1908.09156v1` - [abs](http://arxiv.org/abs/1908.09156v1) - [pdf](http://arxiv.org/pdf/1908.09156v1)

> In the finance sector, studies focused on anomaly detection are often associated with time-series and transactional data analytics. In this paper, we lay out the opportunities for applying anomaly and deviation detection methods to text corpora and challenges associated with them. We argue that language models that use distributional semantics can play a significant role in advancing these studies in novel directions, with new applications in risk identification, predictive modeling, and trend analysis.

</details>

<details>

<summary>2019-08-26 00:43:32 - Using LSTMs to Model the Java Programming Language</summary>

- *Brendon Boldt*

- `1908.11685v1` - [abs](http://arxiv.org/abs/1908.11685v1) - [pdf](http://arxiv.org/pdf/1908.11685v1)

> Recurrent neural networks (RNNs), specifically long-short term memory networks (LSTMs), can model natural language effectively. This research investigates the ability for these same LSTMs to perform next "word" prediction on the Java programming language. Java source code from four different repositories undergoes a transformation that preserves the logical structure of the source code and removes the code's various specificities such as variable names and literal values. Such datasets and an additional English language corpus are used to train and test standard LSTMs' ability to predict the next element in a sequence. Results suggest that LSTMs can effectively model Java code achieving perplexities under 22 and accuracies above 0.47, which is an improvement over LSTM's performance on the English language which demonstrated a perplexity of 85 and an accuracy of 0.27. This research can have applicability in other areas such as syntactic template suggestion and automated bug patching.

</details>

<details>

<summary>2019-08-26 09:08:12 - Learning Disentangled Representations via Independent Subspaces</summary>

- *Maren Awiszus, Hanno Ackermann, Bodo Rosenhahn*

- `1908.08989v1` - [abs](http://arxiv.org/abs/1908.08989v1) - [pdf](http://arxiv.org/pdf/1908.08989v1)

> Image generating neural networks are mostly viewed as black boxes, where any change in the input can have a number of globally effective changes on the output. In this work, we propose a method for learning disentangled representations to allow for localized image manipulations. We use face images as our example of choice. Depending on the image region, identity and other facial attributes can be modified. The proposed network can transfer parts of a face such as shape and color of eyes, hair, mouth, etc.~directly between persons while all other parts of the face remain unchanged. The network allows to generate modified images which appear like realistic images. Our model learns disentangled representations by weak supervision. We propose a localized resnet autoencoder optimized using several loss functions including a loss based on the semantic segmentation, which we interpret as masks, and a loss which enforces disentanglement by decomposition of the latent space into statistically independent subspaces. We evaluate the proposed solution w.r.t. disentanglement and generated image quality. Convincing results are demonstrated using the CelebA dataset.

</details>

<details>

<summary>2019-08-26 10:07:00 - A Methodology to Select Topology Generators for WANET Simulations (Extended Version)</summary>

- *Michael O'Sullivan, Leonardo Aniello, Vladimiro Sassone*

- `1908.09577v1` - [abs](http://arxiv.org/abs/1908.09577v1) - [pdf](http://arxiv.org/pdf/1908.09577v1)

> Many academic and industrial research works on WANETs rely on simulations, at least in the first stages, to obtain preliminary results to be subsequently validated in real settings. Topology generators (TG) are commonly used to generate the initial placement of nodes in artificial WANET topologies, where those simulations take place. The significance of these experiments heavily depends on the representativeness of artificial topologies. Indeed, if they were not drawn fairly, obtained results would apply only to a subset of possible configurations, hence they would lack of the appropriate generality required to port them to the real world. Although using many TGs could mitigate this issue by generating topologies in several different ways, that would entail a significant additional effort. Hence, the problem arises of what TGs to choose, among a number of available generators, to maximise the representativeness of generated topologies and reduce the number of TGs to use.   In this paper, we address that problem by investigating the presence of bias in the initial placement of nodes in artificial WANET topologies produced by different TGs. We propose a methodology to assess such bias and introduce two metrics to quantify the diversity of the topologies generated by a TG with respect to all the available TGs, which can be used to select what TGs to use. We carry out experiments on three well-known TGs, namely BRITE, NPART and GT-ITM. Obtained results show that using the artificial networks produced by a single TG can introduce bias.

</details>

<details>

<summary>2019-08-26 11:56:24 - Unifying Unsupervised Domain Adaptation and Zero-Shot Visual Recognition</summary>

- *Qian Wang, Penghui Bu, Toby P. Breckon*

- `1903.10601v2` - [abs](http://arxiv.org/abs/1903.10601v2) - [pdf](http://arxiv.org/pdf/1903.10601v2)

> Unsupervised domain adaptation aims to transfer knowledge from a source domain to a target domain so that the target domain data can be recognized without any explicit labelling information for this domain. One limitation of the problem setting is that testing data, despite having no labels, from the target domain is needed during training, which prevents the trained model being directly applied to classify unseen test instances. We formulate a new cross-domain classification problem arising from real-world scenarios where labelled data is available for a subset of classes (known classes) in the target domain, and we expect to recognize new samples belonging to any class (known and unseen classes) once the model is learned. This is a generalized zero-shot learning problem where the side information comes from the source domain in the form of labelled samples instead of class-level semantic representations commonly used in traditional zero-shot learning. We present a unified domain adaptation framework for both unsupervised and zero-shot learning conditions. Our approach learns a joint subspace from source and target domains so that the projections of both data in the subspace can be domain invariant and easily separable. We use the supervised locality preserving projection (SLPP) as the enabling technique and conduct experiments under both unsupervised and zero-shot learning conditions, achieving state-of-the-art results on three domain adaptation benchmark datasets: Office-Caltech, Office31 and Office-Home.

</details>

<details>

<summary>2019-08-26 12:33:44 - TACAM: Topic And Context Aware Argument Mining</summary>

- *Michael Fromm, Evgeniy Faerman, Thomas Seidl*

- `1906.00923v2` - [abs](http://arxiv.org/abs/1906.00923v2) - [pdf](http://arxiv.org/pdf/1906.00923v2)

> In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task.

</details>

<details>

<summary>2019-08-26 13:09:37 - Low-Resource Name Tagging Learned with Weakly Labeled Data</summary>

- *Yixin Cao, Zikun Hu, Tat-Seng Chua, Zhiyuan Liu, Heng Ji*

- `1908.09659v1` - [abs](http://arxiv.org/abs/1908.09659v1) - [pdf](http://arxiv.org/pdf/1908.09659v1)

> Name tagging in low-resource languages or domains suffers from inadequate training data. Existing work heavily relies on additional information, while leaving those noisy annotations unexplored that extensively exist on the web. In this paper, we propose a novel neural model for name tagging solely based on weakly labeled (WL) data, so that it can be applied in any low-resource settings. To take the best advantage of all WL sentences, we split them into high-quality and noisy portions for two modules, respectively: (1) a classification module focusing on the large portion of noisy data can efficiently and robustly pretrain the tag classifier by capturing textual context semantics; and (2) a costly sequence labeling module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling to achieve global optimum. Two modules are combined via shared parameters. Extensive experiments involving five low-resource languages and fine-grained food domain demonstrate our superior performance (6% and 7.8% F1 gains on average) as well as efficiency.

</details>

<details>

<summary>2019-08-26 19:02:39 - Slither: A Static Analysis Framework For Smart Contracts</summary>

- *Josselin Feist, Gustavo Grieco, Alex Groce*

- `1908.09878v1` - [abs](http://arxiv.org/abs/1908.09878v1) - [pdf](http://arxiv.org/pdf/1908.09878v1)

> This paper describes Slither, a static analysis framework designed to provide rich information about Ethereum smart contracts. It works by converting Solidity smart contracts into an intermediate representation called SlithIR. SlithIR uses Static Single Assignment (SSA) form and a reduced instruction set to ease implementation of analyses while preserving semantic information that would be lost in transforming Solidity to bytecode. Slither allows for the application of commonly used program analysis techniques like dataflow and taint tracking. Our framework has four main use cases: (1) automated detection of vulnerabilities, (2) automated detection of code optimization opportunities, (3) improvement of the user's understanding of the contracts, and (4) assistance with code review.   In this paper, we present an overview of Slither, detail the design of its intermediate representation, and evaluate its capabilities on real-world contracts. We show that Slither's bug detection is fast, accurate, and outperforms other static analysis tools at finding issues in Ethereum smart contracts in terms of speed, robustness, and balance of detection and false positives. We compared tools using a large dataset of smart contracts and manually reviewed results for 1000 of the most used contracts.

</details>

<details>

<summary>2019-08-26 19:56:08 - Does BERT agree? Evaluating knowledge of structure dependence through agreement relations</summary>

- *Geoff Bacon, Terry Regier*

- `1908.09892v1` - [abs](http://arxiv.org/abs/1908.09892v1) - [pdf](http://arxiv.org/pdf/1908.09892v1)

> Learning representations that accurately model semantics is an important goal of natural language processing research. Many semantic phenomena depend on syntactic structure. Recent work examines the extent to which state-of-the-art models for pre-training representations, such as BERT, capture such structure-dependent phenomena, but is largely restricted to one phenomenon in English: number agreement between subjects and verbs. We evaluate BERT's sensitivity to four types of structure-dependent agreement relations in a new semi-automatically curated dataset across 26 languages. We show that both the single-language and multilingual BERT models capture syntax-sensitive agreement patterns well in general, but we also highlight the specific linguistic contexts in which their performance degrades.

</details>

<details>

<summary>2019-08-27 04:12:12 - Neural-Davidsonian Semantic Proto-role Labeling</summary>

- *Rachel Rudinger, Adam Teichert, Ryan Culkin, Sheng Zhang, Benjamin Van Durme*

- `1804.07976v3` - [abs](http://arxiv.org/abs/1804.07976v3) - [pdf](http://arxiv.org/pdf/1804.07976v3)

> We present a model for semantic proto-role labeling (SPRL) using an adapted bidirectional LSTM encoding strategy that we call "Neural-Davidsonian": predicate-argument structure is represented as pairs of hidden states corresponding to predicate and argument head tokens of the input sequence. We demonstrate: (1) state-of-the-art results in SPRL, and (2) that our network naturally shares parameters between attributes, allowing for learning new attribute types with limited added supervision.

</details>

<details>

<summary>2019-08-27 08:50:17 - Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</summary>

- *Nils Reimers, Iryna Gurevych*

- `1908.10084v1` - [abs](http://arxiv.org/abs/1908.10084v1) - [pdf](http://arxiv.org/pdf/1908.10084v1)

> BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.   In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.   We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.

</details>

<details>

<summary>2019-08-27 11:19:48 - Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation</summary>

- *Loïc Vial, Benjamin Lecouteux, Didier Schwab*

- `1905.05677v3` - [abs](http://arxiv.org/abs/1905.05677v3) - [pdf](http://arxiv.org/pdf/1905.05677v3)

> In this article, we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation, by exploiting the semantic relationships between senses such as synonymy, hypernymy and hyponymy, in order to compress the sense vocabulary of Princeton WordNet, and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database. We propose two different methods that greatly reduces the size of neural WSD models, with the benefit of improving their coverage without additional training data, and without impacting their precision. In addition to our method, we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperform the state of the art on all WSD evaluation tasks.

</details>

<details>

<summary>2019-08-27 14:21:32 - Data Augmentation using Random Image Cropping and Patching for Deep CNNs</summary>

- *Ryo Takahashi, Takashi Matsubara, Kuniaki Uehara*

- `1811.09030v2` - [abs](http://arxiv.org/abs/1811.09030v2) - [pdf](http://arxiv.org/pdf/1811.09030v2)

> Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage similar to label smoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of $2.19\%$ on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet and an image-caption retrieval task using Microsoft COCO.

</details>

<details>

<summary>2019-08-27 14:22:56 - Task-assisted Motion Planning in Partially Observable Domains</summary>

- *Antony Thomas, Sunny Amatya, Fulvio Mastrogiovanni, Marco Baglietto*

- `1908.10227v1` - [abs](http://arxiv.org/abs/1908.10227v1) - [pdf](http://arxiv.org/pdf/1908.10227v1)

> We present an integrated Task-Motion Planning framework for robot navigation in belief space. Autonomous robots operating in real world complex scenarios require planning in the discrete (task) space and the continuous (motion) space. To this end, we propose a framework for integrating belief space reasoning within a hybrid task planner. The expressive power of PDDL+ combined with heuristic-driven semantic attachments performs the propagated and posterior belief estimates while planning. The underlying methodology for the development of the combined hybrid planner is discussed, providing suggestions for improvements and future work. Furthermore we validate key aspects of our approach using a realistic scenario in simulation.

</details>

<details>

<summary>2019-08-27 17:13:46 - Physics-Based Rendering for Improving Robustness to Rain</summary>

- *Shirsendu Sukanta Halder, Jean-François Lalonde, Raoul de Charette*

- `1908.10335v1` - [abs](http://arxiv.org/abs/1908.10335v1) - [pdf](http://arxiv.org/pdf/1908.10335v1)

> To improve the robustness to rain, we present a physically-based rain rendering pipeline for realistically inserting rain into clear weather images. Our rendering relies on a physical particle simulator, an estimation of the scene lighting and an accurate rain photometric modeling to augment images with arbitrary amount of realistic rain or fog. We validate our rendering with a user study, proving our rain is judged 40% more realistic that state-of-the-art. Using our generated weather augmented Kitti and Cityscapes dataset, we conduct a thorough evaluation of deep object detection and semantic segmentation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection and 60% for semantic segmentation. Furthermore, we show refining existing networks with our augmented images improves the robustness of both object detection and semantic segmentation algorithms. We experiment on nuScenes and measure an improvement of 15% for object detection and 35% for semantic segmentation compared to original rainy performance. Augmented databases and code are available on the project page.

</details>

<details>

<summary>2019-08-27 17:23:11 - Implementing Ranking-Based Semantics in ConArg: a Preliminary Report</summary>

- *Stafano Bistarelli, Francesco Faloci, Carlo Taticchi*

- `1908.07784v2` - [abs](http://arxiv.org/abs/1908.07784v2) - [pdf](http://arxiv.org/pdf/1908.07784v2)

> ConArg is a suite of tools that offers a wide series of applications for dealing with argumentation problems. In this work, we present the advances we made in implementing a ranking-based semantics, based on computational choice power indexes, within ConArg. Such kind of semantics represents a method for sorting the arguments of an abstract argumentation framework, according to some preference relation. The ranking-based semantics we implement relies on Shapley, Banzhaf, Deegan-Packel and Johnston power index, transferring well know properties from computational social choice to argumentation framework ranking-based semantics.

</details>

<details>

<summary>2019-08-27 20:43:49 - A survey of cross-lingual features for zero-shot cross-lingual semantic parsing</summary>

- *Jingfeng Yang, Federico Fancellu, Bonnie Webber*

- `1908.10461v1` - [abs](http://arxiv.org/abs/1908.10461v1) - [pdf](http://arxiv.org/pdf/1908.10461v1)

> The availability of corpora to train semantic parsers in English has lead to significant advances in the field. Unfortunately, for languages other than English, annotation is scarce and so are developed parsers. We then ask: could a parser trained in English be applied to language that it hasn't been trained on? To answer this question we explore zero-shot cross-lingual semantic parsing where we train an available coarse-to-fine semantic parser (Liu et al., 2018) using cross-lingual word embeddings and universal dependencies in English and test it on Italian, German and Dutch. Results on the Parallel Meaning Bank - a multilingual semantic graphbank, show that Universal Dependency features significantly boost performance when used in conjunction with other lexical features but modelling the UD structure directly when encoding the input does not.

</details>

<details>

<summary>2019-08-28 05:57:00 - iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images</summary>

- *Syed Waqas Zamir, Aditya Arora, Akshita Gupta, Salman Khan, Guolei Sun, Fahad Shahbaz Khan, Fan Zhu, Ling Shao, Gui-Song Xia, Xiang Bai*

- `1905.12886v2` - [abs](http://arxiv.org/abs/1905.12886v2) - [pdf](http://arxiv.org/pdf/1905.12886v2)

> Existing Earth Vision datasets are either suitable for semantic segmentation or object detection. In this work, we introduce the first benchmark dataset for instance segmentation in aerial imagery that combines instance-level object detection and pixel-level segmentation tasks. In comparison to instance segmentation in natural scenes, aerial images present unique challenges e.g., a huge number of instances per image, large object-scale variations and abundant tiny objects. Our large-scale and densely annotated Instance Segmentation in Aerial Images Dataset (iSAID) comes with 655,451 object instances for 15 categories across 2,806 high-resolution images. Such precise per-pixel annotations for each instance ensure accurate localization that is essential for detailed scene analysis. Compared to existing small-scale aerial image based instance segmentation datasets, iSAID contains 15$\times$ the number of object categories and 5$\times$ the number of instances. We benchmark our dataset using two popular instance segmentation approaches for natural images, namely Mask R-CNN and PANet. In our experiments we show that direct application of off-the-shelf Mask R-CNN and PANet on aerial images provide suboptimal instance segmentation results, thus requiring specialized solutions from the research community. The dataset is publicly available at: https://captain-whu.github.io/iSAID/index.html

</details>

<details>

<summary>2019-08-28 11:47:39 - Exploiting Multiple Embeddings for Chinese Named Entity Recognition</summary>

- *Canwen Xu, Feiyang Wang, Jialong Han, Chenliang Li*

- `1908.10657v1` - [abs](http://arxiv.org/abs/1908.10657v1) - [pdf](http://arxiv.org/pdf/1908.10657v1)

> Identifying the named entities mentioned in text would enrich many semantic applications at the downstream level. However, due to the predominant usage of colloquial language in microblogs, the named entity recognition (NER) in Chinese microblogs experience significant performance deterioration, compared with performing NER in formal Chinese corpus. In this paper, we propose a simple yet effective neural framework to derive the character-level embeddings for NER in Chinese text, named ME-CNER. A character embedding is derived with rich semantic information harnessed at multiple granularities, ranging from radical, character to word levels. The experimental results demonstrate that the proposed approach achieves a large performance improvement on Weibo dataset and comparable performance on MSRA news dataset with lower computational cost against the existing state-of-the-art alternatives.

</details>

<details>

<summary>2019-08-28 13:40:43 - Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension</summary>

- *Todor Mihaylov, Anette Frank*

- `1908.10721v1` - [abs](http://arxiv.org/abs/1908.10721v1) - [pdf](http://arxiv.org/pdf/1908.10721v1)

> In this work, we propose to use linguistic annotations as a basis for a \textit{Discourse-Aware Semantic Self-Attention} encoder that we employ for reading comprehension on long narrative texts. We extract relations between discourse units, events and their arguments as well as coreferring mentions, using available annotation tools. Our empirical evaluation shows that the investigated structures improve the overall performance, especially intra-sentential and cross-sentential discourse relations, sentence-internal semantic role relations, and long-distance coreference relations. We show that dedicating self-attention heads to intra-sentential relations and relations connecting neighboring sentences is beneficial for finding answers to questions in longer contexts. Our findings encourage the use of discourse-semantic annotations to enhance the generalization capacity of self-attention models for reading comprehension.

</details>

<details>

<summary>2019-08-28 14:47:32 - A Semantic Schema for Data Quality Management in a Multi-Tenant Data Platform</summary>

- *Ning Zhou, Sandra Garcia Esparza, Lars Marius Garshol*

- `1908.10754v1` - [abs](http://arxiv.org/abs/1908.10754v1) - [pdf](http://arxiv.org/pdf/1908.10754v1)

> Schibsted Media Group is a global marketplace company with presence in more than 20 countries. It is undergoing a digital transformation to convert data silos to a multi-tenant system based on a common data platform. Good data quality based on a common schema on the semantic level is essential for building successful data-driven products across marketplaces. To solve this challenge, we developed the data quality tooling based on a semantic schema management system to support schema evolution with versioning, testing and transformation. It can monitor the data quality requirements for different applications and handle incoming data consisting of multiple schema versions. Today the system is operating in production and processes over one billion events per day for over 100 applications.

</details>

<details>

<summary>2019-08-28 15:07:07 - DoPa: A Comprehensive CNN Detection Methodology against Physical Adversarial Attacks</summary>

- *Zirui Xu, Fuxun Yu, Xiang Chen*

- `1905.08790v4` - [abs](http://arxiv.org/abs/1905.08790v4) - [pdf](http://arxiv.org/pdf/1905.08790v4)

> Recently, Convolutional Neural Networks (CNNs) demonstrate a considerable vulnerability to adversarial attacks, which can be easily misled by adversarial perturbations. With more aggressive methods proposed, adversarial attacks can be also applied to the physical world, causing practical issues to various CNN powered applications. To secure CNNs, adversarial attack detection is considered as the most critical approach. However, most existing works focus on superficial patterns and merely search a particular method to differentiate the adversarial inputs and natural inputs, ignoring the analysis of CNN inner vulnerability. Therefore, they can only target to specific physical adversarial attacks, lacking expected versatility to different attacks. To address this issue, we propose DoPa -- a comprehensive CNN detection methodology for various physical adversarial attacks. By interpreting the CNN's vulnerability, we find that non-semantic adversarial perturbations can activate CNN with significantly abnormal activations and even overwhelm other semantic input patterns' activations. Therefore, we add a self-verification stage to analyze the semantics of distinguished activation patterns, which improves the CNN recognition process. We apply such a detection methodology into both image and audio CNN recognition scenarios. Experiments show that DoPa can achieve an average rate of 90% success for image attack detection and 92% success for audio attack detection.   Announcement:[The original DoPa draft on arXiv was modified and submitted to a conference already, while this short abstract was submitted only for a presentation at the KDD 2019 AIoT Workshop.]

</details>

<details>

<summary>2019-08-28 15:17:33 - Data Augmentation with Atomic Templates for Spoken Language Understanding</summary>

- *Zijian Zhao, Su Zhu, Kai Yu*

- `1908.10770v1` - [abs](http://arxiv.org/abs/1908.10770v1) - [pdf](http://arxiv.org/pdf/1908.10770v1)

> Spoken Language Understanding (SLU) converts user utterances into structured semantic representations. Data sparsity is one of the main obstacles of SLU due to the high cost of human annotation, especially when domain changes or a new domain comes. In this work, we propose a data augmentation method with atomic templates for SLU, which involves minimum human efforts. The atomic templates produce exemplars for fine-grained constituents of semantic representations. We propose an encoder-decoder model to generate the whole utterance from atomic exemplars. Moreover, the generator could be transferred from source domains to help a new domain which has little data. Experimental results show that our method achieves significant improvements on DSTC 2\&3 dataset which is a domain adaptation setting of SLU.

</details>

<details>

<summary>2019-08-28 15:29:38 - Multitask Learning for Large-scale Semantic Change Detection</summary>

- *Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, Yann Gousseau*

- `1810.08452v2` - [abs](http://arxiv.org/abs/1810.08452v2) - [pdf](http://arxiv.org/pdf/1810.08452v2)

> Change detection is one of the main problems in remote sensing, and is essential to the accurate processing and understanding of the large scale Earth observation data available through programs such as Sentinel and Landsat. Most of the recently proposed change detection methods bring deep learning to this context, but openly available change detection datasets are still very scarce, which limits the methods that can be proposed and tested. In this paper we present the first large scale high resolution semantic change detection (HRSCD) dataset, which enables the usage of deep learning methods for semantic change detection. The dataset contains coregistered RGB image pairs, pixel-wise change information and land cover information. We then propose several methods using fully convolutional neural networks to perform semantic change detection. Most notably, we present a network architecture that performs change detection and land cover mapping simultaneously, while using the predicted land cover information to help to predict changes. We also describe a sequential training scheme that allows this network to be trained without setting a hyperparameter that balances different loss functions and achieves the best overall results.

</details>

<details>

<summary>2019-08-28 19:32:00 - SpatialNLI: A Spatial Domain Natural Language Interface to Databases Using Spatial Comprehension</summary>

- *Jingjing Li, Wenlu Wang, Wei-Shinn Ku, Yingtao Tian, Haixun Wang*

- `1908.10917v1` - [abs](http://arxiv.org/abs/1908.10917v1) - [pdf](http://arxiv.org/pdf/1908.10917v1)

> A natural language interface (NLI) to databases is an interface that translates a natural language question to a structured query that is executable by database management systems (DBMS). However, an NLI that is trained in the general domain is hard to apply in the spatial domain due to the idiosyncrasy and expressiveness of the spatial questions. Inspired by the machine comprehension model, we propose a spatial comprehension model that is able to recognize the meaning of spatial entities based on the semantics of the context. The spatial semantics learned from the spatial comprehension model is then injected to the natural language question to ease the burden of capturing the spatial-specific semantics. With our spatial comprehension model and information injection, our NLI for the spatial domain, named SpatialNLI, is able to capture the semantic structure of the question and translate it to the corresponding syntax of an executable query accurately. We also experimentally ascertain that SpatialNLI outperforms state-of-the-art methods.

</details>

<details>

<summary>2019-08-28 22:18:03 - Leveraging Structural and Semantic Correspondence for Attribute-Oriented Aspect Sentiment Discovery</summary>

- *Zhe Zhang, Munindar P. Singh*

- `1908.10970v1` - [abs](http://arxiv.org/abs/1908.10970v1) - [pdf](http://arxiv.org/pdf/1908.10970v1)

> Opinionated text often involves attributes such as authorship and location that influence the sentiments expressed for different aspects. We posit that structural and semantic correspondence is both prevalent in opinionated text, especially when associated with attributes, and crucial in accurately revealing its latent aspect and sentiment structure. However, it is not recognized by existing approaches.   We propose Trait, an unsupervised probabilistic model that discovers aspects and sentiments from text and associates them with different attributes. To this end, Trait infers and leverages structural and semantic correspondence using a Markov Random Field. We show empirically that by incorporating attributes explicitly Trait significantly outperforms state-of-the-art baselines both by generating attribute profiles that accord with our intuitions, as shown via visualization, and yielding topics of greater semantic cohesion.

</details>

<details>

<summary>2019-08-29 00:21:32 - Don't paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing</summary>

- *Jonathan Herzig, Jonathan Berant*

- `1908.09940v2` - [abs](http://arxiv.org/abs/1908.09940v2) - [pdf](http://arxiv.org/pdf/1908.09940v2)

> A major hurdle on the road to conversational interfaces is the difficulty in collecting data that maps language utterances to logical forms. One prominent approach for data collection has been to automatically generate pseudo-language paired with logical forms, and paraphrase the pseudo-language to natural language through crowdsourcing (Wang et al., 2015). However, this data collection procedure often leads to low performance on real data, due to a mismatch between the true distribution of examples and the distribution induced by the data collection procedure. In this paper, we thoroughly analyze two sources of mismatch in this process: the mismatch in logical form distribution and the mismatch in language distribution between the true and induced distributions. We quantify the effects of these mismatches, and propose a new data collection approach that mitigates them. Assuming access to unlabeled utterances from the true distribution, we combine crowdsourcing with a paraphrase model to detect correct logical forms for the unlabeled utterances. On two datasets, our method leads to 70.6 accuracy on average on the true distribution, compared to 51.3 in paraphrasing-based data collection.

</details>

<details>

<summary>2019-08-29 09:47:48 - Probing Representations Learned by Multimodal Recurrent and Transformer Models</summary>

- *Jindřich Libovický, Pranava Madhyastha*

- `1908.11125v1` - [abs](http://arxiv.org/abs/1908.11125v1) - [pdf](http://arxiv.org/pdf/1908.11125v1)

> Recent literature shows that large-scale language modeling provides excellent reusable sentence representations with both recurrent and self-attentive architectures. However, there has been less clarity on the commonalities and differences in the representational properties induced by the two architectures. It also has been shown that visual information serves as one of the means for grounding sentence representations. In this paper, we present a meta-study assessing the representational quality of models where the training signal is obtained from different modalities, in particular, language modeling, image features prediction, and both textual and multimodal machine translation. We evaluate textual and visual features of sentence representations obtained using predominant approaches on image retrieval and semantic textual similarity. Our experiments reveal that on moderate-sized datasets, a sentence counterpart in a target language or visual modality provides much stronger training signal for sentence representation than language modeling. Importantly, we observe that while the Transformer models achieve superior machine translation quality, representations from the recurrent neural network based models perform significantly better over tasks focused on semantic relevance.

</details>

<details>

<summary>2019-08-29 10:14:40 - KBSET -- Knowledge-Based Support for Scholarly Editing and Text Processing</summary>

- *Jana Kittelmann, Christoph Wernhard*

- `1908.11135v1` - [abs](http://arxiv.org/abs/1908.11135v1) - [pdf](http://arxiv.org/pdf/1908.11135v1)

> KBSET supports a practical workflow for scholarly editing, based on using LaTeX with dedicated commands for semantics-oriented markup and a Prolog-implemented core system. Prolog plays there various roles: as query language and access mechanism for large Semantic Web fact bases, as data representation of structured documents and as a workflow model for advanced application tasks. The core system includes a LaTeX parser and a facility for the identification of named entities. We also sketch future perspectives of this approach to scholarly editing based on techniques of computational logic.

</details>

<details>

<summary>2019-08-29 13:22:14 - A Method for Estimating the Proximity of Vector Representation Groups in Multidimensional Space. On the Example of the Paraphrase Task</summary>

- *Artem Artemov, Boris Alekseev*

- `1908.09341v2` - [abs](http://arxiv.org/abs/1908.09341v2) - [pdf](http://arxiv.org/pdf/1908.09341v2)

> The following paper presents a method of comparing two sets of vectors. The method can be applied in all tasks, where it is necessary to measure the closeness of two objects presented as sets of vectors. It may be applicable when we compare the meanings of two sentences as part of the problem of paraphrasing. This is the problem of measuring semantic similarity of two sentences (group of words). The existing methods are not sensible for the word order or syntactic connections in the considered sentences. The method appears to be advantageous because it neither presents a group of words as one scalar value, nor does it try to show the closeness through an aggregation vector, which is mean for the set of vectors. Instead of that we measure the cosine of the angle as the mean for the first group vectors projections (the context) on one side and each vector of the second group on the other side. The similarity of two sentences defined by these means does not lose any semantic characteristics and takes account of the words traits. The method was verified on the comparison of sentence pairs in Russian.

</details>

<details>

<summary>2019-08-29 13:29:36 - Global Reasoning over Database Structures for Text-to-SQL Parsing</summary>

- *Ben Bogin, Matt Gardner, Jonathan Berant*

- `1908.11214v1` - [abs](http://arxiv.org/abs/1908.11214v1) - [pdf](http://arxiv.org/pdf/1908.11214v1)

> State-of-the-art semantic parsers rely on auto-regressive decoding, emitting one symbol at a time. When tested against complex databases that are unobserved at training time (zero-shot), the parser often struggles to select the correct set of database constants in the new database, due to the local nature of decoding. In this work, we propose a semantic parser that globally reasons about the structure of the output query to make a more contextually-informed selection of database constants. We use message-passing through a graph neural network to softly select a subset of database constants for the output query, conditioned on the question. Moreover, we train a model to rank queries based on the global alignment of database constants to question words. We apply our techniques to the current state-of-the-art model for Spider, a zero-shot semantic parsing dataset with complex databases, increasing accuracy from 39.4% to 47.4%.

</details>

<details>

<summary>2019-08-29 13:31:38 - Asymmetric Non-local Neural Networks for Semantic Segmentation</summary>

- *Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, Xiang Bai*

- `1908.07678v5` - [abs](http://arxiv.org/abs/1908.07678v5) - [pdf](http://arxiv.org/pdf/1908.07678v5)

> The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: https://github.com/MendelXu/ANN.git.

</details>

<details>

<summary>2019-08-29 15:05:52 - Grounded Agreement Games: Emphasizing Conversational Grounding in Visual Dialogue Settings</summary>

- *David Schlangen*

- `1908.11279v1` - [abs](http://arxiv.org/abs/1908.11279v1) - [pdf](http://arxiv.org/pdf/1908.11279v1)

> Where early work on dialogue in Computational Linguistics put much emphasis on dialogue structure and its relation to the mental states of the dialogue participants (e.g., Allen 1979, Grosz & Sidner 1986), current work mostly reduces dialogue to the task of producing at any one time a next utterance; e.g. in neural chatbot or Visual Dialogue settings. As a methodological decision, this is sound: Even the longest journey is a sequence of steps. It becomes detrimental, however, when the tasks and datasets from which dialogue behaviour is to be learned are tailored too much to this framing of the problem. In this short note, we describe a family of settings which still allow to keep dialogues simple, but add a constraint that makes participants care about reaching mutual understanding. In such agreement games, there is a secondary, but explicit goal besides the task level goal, and that is to reach mutual understanding about whether the task level goal has been reached. As we argue, this naturally triggers meta-semantic interaction and mutual engagement, and hence leads to richer data from which to induce models.

</details>

<details>

<summary>2019-08-29 15:50:12 - Exploiting Temporality for Semi-Supervised Video Segmentation</summary>

- *Radu Sibechi, Olaf Booij, Nora Baka, Peter Bloem*

- `1908.11309v1` - [abs](http://arxiv.org/abs/1908.11309v1) - [pdf](http://arxiv.org/pdf/1908.11309v1)

> In recent years, there has been remarkable progress in supervised image segmentation. Video segmentation is less explored, despite the temporal dimension being highly informative. Semantic labels, e.g. that cannot be accurately detected in the current frame, may be inferred by incorporating information from previous frames. However, video segmentation is challenging due to the amount of data that needs to be processed and, more importantly, the cost involved in obtaining ground truth annotations for each frame. In this paper, we tackle the issue of label scarcity by using consecutive frames of a video, where only one frame is annotated. We propose a deep, end-to-end trainable model which leverages temporal information in order to make use of easy to acquire unlabeled data. Our network architecture relies on a novel interconnection of two components: a fully convolutional network to model spatial information and temporal units that are employed at intermediate levels of the convolutional network in order to propagate information through time. The main contribution of this work is the guidance of the temporal signal through the network. We show that only placing a temporal module between the encoder and decoder is suboptimal (baseline). Our extensive experiments on the CityScapes dataset indicate that the resulting model can leverage unlabeled temporal frames and significantly outperform both the frame-by-frame image segmentation and the baseline approach.

</details>

<details>

<summary>2019-08-29 16:00:25 - Memorizing All for Implicit Discourse Relation Recognition</summary>

- *Hongxiao Bai, Hai Zhao, Junhan Zhao*

- `1908.11317v1` - [abs](http://arxiv.org/abs/1908.11317v1) - [pdf](http://arxiv.org/pdf/1908.11317v1)

> Implicit discourse relation recognition is a challenging task due to the absence of the necessary informative clue from explicit connectives. The prediction of relations requires a deep understanding of the semantic meanings of sentence pairs. As implicit discourse relation recognizer has to carefully tackle the semantic similarity of the given sentence pairs and the severe data sparsity issue exists in the meantime, it is supposed to be beneficial from mastering the entire training data. Thus in this paper, we propose a novel memory mechanism to tackle the challenges for further performance improvement. The memory mechanism is adequately memorizing information by pairing representations and discourse relations of all training instances, which right fills the slot of the data-hungry issue in the current implicit discourse relation recognizer. Our experiments show that our full model with memorizing the entire training set reaches new state-of-the-art against strong baselines, which especially for the first time exceeds the milestone of 60% accuracy in the 4-way task.

</details>

<details>

<summary>2019-08-29 16:17:53 - Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling</summary>

- *Angel Daza, Anette Frank*

- `1908.11326v1` - [abs](http://arxiv.org/abs/1908.11326v1) - [pdf](http://arxiv.org/pdf/1908.11326v1)

> We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages.

</details>

<details>

<summary>2019-08-29 16:32:35 - Task-Guided Pair Embedding in Heterogeneous Network</summary>

- *Chanyoung Park, Donghyun Kim, Qi Zhu, Jiawei Han, Hwanjo Yu*

- `1906.01546v3` - [abs](http://arxiv.org/abs/1906.01546v3) - [pdf](http://arxiv.org/pdf/1906.01546v3)

> Many real-world tasks solved by heterogeneous network embedding methods can be cast as modeling the likelihood of pairwise relationship between two nodes. For example, the goal of author identification task is to model the likelihood of a paper being written by an author (paper-author pairwise relationship). Existing task-guided embedding methods are node-centric in that they simply measure the similarity between the node embeddings to compute the likelihood of a pairwise relationship between two nodes. However, we claim that for task-guided embeddings, it is crucial to focus on directly modeling the pairwise relationship. In this paper, we propose a novel task-guided pair embedding framework in heterogeneous network, called TaPEm, that directly models the relationship between a pair of nodes that are related to a specific task (e.g., paper-author relationship in author identification). To this end, we 1) propose to learn a pair embedding under the guidance of its associated context path, i.e., a sequence of nodes between the pair, and 2) devise the pair validity classifier to distinguish whether the pair is valid with respect to the specific task at hand. By introducing pair embeddings that capture the semantics behind the pairwise relationships, we are able to learn the fine-grained pairwise relationship between two nodes, which is paramount for task-guided embedding methods. Extensive experiments on author identification task demonstrate that TaPEm outperforms the state-of-the-art methods, especially for authors with few publication records.

</details>

<details>

<summary>2019-08-29 17:54:53 - Diverse Image Synthesis from Semantic Layouts via Conditional IMLE</summary>

- *Ke Li, Tianhao Zhang, Jitendra Malik*

- `1811.12373v2` - [abs](http://arxiv.org/abs/1811.12373v2) - [pdf](http://arxiv.org/pdf/1811.12373v2)

> Most existing methods for conditional image synthesis are only able to generate a single plausible image for any given input, or at best a fixed number of plausible images. In this paper, we focus on the problem of generating images from semantic segmentation maps and present a simple new method that can generate an arbitrary number of images with diverse appearance for the same semantic layout. Unlike most existing approaches which adopt the GAN framework, our method is based on the recently introduced Implicit Maximum Likelihood Estimation (IMLE) framework. Compared to the leading approach, our method is able to generate more diverse images while producing fewer artifacts despite using the same architecture. The learned latent space also has sensible structure despite the lack of supervision that encourages such behaviour. Videos and code are available at https://people.eecs.berkeley.edu/~ke.li/projects/imle/scene_layouts/.

</details>

<details>

<summary>2019-08-29 19:57:25 - Feature2Vec: Distributional semantic modelling of human property knowledge</summary>

- *Steven Derby, Paul Miller, Barry Devereux*

- `1908.11439v1` - [abs](http://arxiv.org/abs/1908.11439v1) - [pdf](http://arxiv.org/pdf/1908.11439v1)

> Feature norm datasets of human conceptual knowledge, collected in surveys of human volunteers, yield highly interpretable models of word meaning and play an important role in neurolinguistic research on semantic cognition. However, these datasets are limited in size due to practical obstacles associated with exhaustively listing properties for a large number of words. In contrast, the development of distributional modelling techniques and the availability of vast text corpora have allowed researchers to construct effective vector space models of word meaning over large lexicons. However, this comes at the cost of interpretable, human-like information about word meaning. We propose a method for mapping human property knowledge onto a distributional semantic space, which adapts the word2vec architecture to the task of modelling concept features. Our approach gives a measure of concept and feature affinity in a single semantic space, which makes for easy and efficient ranking of candidate human-derived semantic properties for arbitrary words. We compare our model with a previous approach, and show that it performs better on several evaluation tasks. Finally, we discuss how our method could be used to develop efficient sampling techniques to extend existing feature norm datasets in a reliable way.

</details>

<details>

<summary>2019-08-29 20:16:09 - Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites</summary>

- *Alexey Tikhonov, Viacheslav Shibaev, Aleksander Nagaev, Aigul Nugmanova, Ivan P. Yamshchikov*

- `1908.06809v2` - [abs](http://arxiv.org/abs/1908.06809v2) - [pdf](http://arxiv.org/pdf/1908.06809v2)

> This paper shows that standard assessment methodology for style transfer has several significant problems. First, the standard metrics for style accuracy and semantics preservation vary significantly on different re-runs. Therefore one has to report error margins for the obtained results. Second, starting with certain values of bilingual evaluation understudy (BLEU) between input and output and accuracy of the sentiment transfer the optimization of these two standard metrics diverge from the intuitive goal of the style transfer task. Finally, due to the nature of the task itself, there is a specific dependence between these two metrics that could be easily manipulated. Under these circumstances, we suggest taking BLEU between input and human-written reformulations into consideration for benchmarks. We also propose three new architectures that outperform state of the art in terms of this metric.

</details>

<details>

<summary>2019-08-30 01:47:24 - TGG: Transferable Graph Generation for Zero-shot and Few-shot Learning</summary>

- *Chenrui Zhang, Xiaoqing Lyu, Zhi Tang*

- `1908.11503v1` - [abs](http://arxiv.org/abs/1908.11503v1) - [pdf](http://arxiv.org/pdf/1908.11503v1)

> Zero-shot and few-shot learning aim to improve generalization to unseen concepts, which are promising in many realistic scenarios. Due to the lack of data in unseen domain, relation modeling between seen and unseen domains is vital for knowledge transfer in these tasks. Most existing methods capture seen-unseen relation implicitly via semantic embedding or feature generation, resulting in inadequate use of relation and some issues remain (e.g. domain shift). To tackle these challenges, we propose a Transferable Graph Generation (TGG) approach, in which the relation is modeled and utilized explicitly via graph generation. Specifically, our proposed TGG contains two main components: (1) Graph generation for relation modeling. An attention-based aggregate network and a relation kernel are proposed, which generate instance-level graph based on a class-level prototype graph and visual features. Proximity information aggregating is guided by a multi-head graph attention mechanism, where seen and unseen features synthesized by GAN are revised as node embeddings. The relation kernel further generates edges with GCN and graph kernel method, to capture instance-level topological structure while tackling data imbalance and noise. (2) Relation propagation for relation utilization. A dual relation propagation approach is proposed, where relations captured by the generated graph are separately propagated from the seen and unseen subgraphs. The two propagations learn from each other in a dual learning fashion, which performs as an adaptation way for mitigating domain shift. All components are jointly optimized with a meta-learning strategy, and our TGG acts as an end-to-end framework unifying conventional zero-shot, generalized zero-shot and few-shot learning. Extensive experiments demonstrate that it consistently surpasses existing methods of the above three fields by a significant margin.

</details>

<details>

<summary>2019-08-30 06:38:55 - Detect Camouflaged Spam Content via StoneSkipping: Graph and Text Joint Embedding for Chinese Character Variation Representation</summary>

- *Zhuoren Jiang, Zhe Gao, Guoxiu He, Yangyang Kang, Changlong Sun, Qiong Zhang, Luo Si, Xiaozhong Liu*

- `1908.11561v1` - [abs](http://arxiv.org/abs/1908.11561v1) - [pdf](http://arxiv.org/pdf/1908.11561v1)

> The task of Chinese text spam detection is very challenging due to both glyph and phonetic variations of Chinese characters. This paper proposes a novel framework to jointly model Chinese variational, semantic, and contextualized representations for Chinese text spam detection task. In particular, a Variation Family-enhanced Graph Embedding (VFGE) algorithm is designed based on a Chinese character variation graph. The VFGE can learn both the graph embeddings of the Chinese characters (local) and the latent variation families (global). Furthermore, an enhanced bidirectional language model, with a combination gate function and an aggregation learning function, is proposed to integrate the graph and text information while capturing the sequential information. Extensive experiments have been conducted on both SMS and review datasets, to show the proposed method outperforms a series of state-of-the-art models for Chinese spam detection.

</details>

<details>

<summary>2019-08-30 09:30:38 - Cross-domain Aspect Category Transfer and Detection via Traceable Heterogeneous Graph Representation Learning</summary>

- *Zhuoren Jiang, Jian Wang, Lujun Zhao, Changlong Sun, Yao Lu, Xiaozhong Liu*

- `1908.11610v1` - [abs](http://arxiv.org/abs/1908.11610v1) - [pdf](http://arxiv.org/pdf/1908.11610v1)

> Aspect category detection is an essential task for sentiment analysis and opinion mining. However, the cost of categorical data labeling, e.g., label the review aspect information for a large number of product domains, can be inevitable but unaffordable. In this study, we propose a novel problem, cross-domain aspect category transfer and detection, which faces three challenges: various feature spaces, different data distributions, and diverse output spaces. To address these problems, we propose an innovative solution, Traceable Heterogeneous Graph Representation Learning (THGRL). Unlike prior text-based aspect detection works, THGRL explores latent domain aspect category connections via massive user behavior information on a heterogeneous graph. Moreover, an innovative latent variable "Walker Tracer" is introduced to characterize the global semantic/aspect dependencies and capture the informative vertexes on the random walk paths. By using THGRL, we project different domains' feature spaces into a common one, while allowing data distributions and output spaces stay differently. Experiment results show that the proposed method outperforms a series of state-of-the-art baseline models.

</details>

<details>

<summary>2019-08-30 15:21:28 - Latent Part-of-Speech Sequences for Neural Machine Translation</summary>

- *Xuewen Yang, Yingru Liu, Dongliang Xie, Xin Wang, Niranjan Balasubramanian*

- `1908.11782v1` - [abs](http://arxiv.org/abs/1908.11782v1) - [pdf](http://arxiv.org/pdf/1908.11782v1)

> Learning target side syntactic structure has been shown to improve Neural Machine Translation (NMT). However, incorporating syntax through latent variables introduces additional complexity in inference, as the models need to marginalize over the latent syntactic structures. To avoid this, models often resort to greedy search which only allows them to explore a limited portion of the latent space. In this work, we introduce a new latent variable model, LaSyn, that captures the co-dependence between syntax and semantics, while allowing for effective and efficient inference over the latent space. LaSyn decouples direct dependence between successive latent variables, which allows its decoder to exhaustively search through the latent syntactic choices, while keeping decoding speed proportional to the size of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based NMT system and design a neural expectation maximization algorithm that we regularize with part-of-speech information as the latent sequences. Evaluations on four different MT tasks show that incorporating target side syntax with LaSyn improves both translation quality, and also provides an opportunity to improve diversity.

</details>

<details>

<summary>2019-08-30 15:47:15 - Dense Dilated Convolutions Merging Network for Semantic Mapping of Remote Sensing Images</summary>

- *Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-Børre Salberg*

- `1908.11799v1` - [abs](http://arxiv.org/abs/1908.11799v1) - [pdf](http://arxiv.org/pdf/1908.11799v1)

> We propose a network for semantic mapping called the Dense Dilated Convolutions Merging Network (DDCM-Net) to provide a deep learning approach that can recognize multi-scale and complex shaped objects with similar color and textures, such as buildings, surfaces/roads, and trees in very high resolution remote sensing images. The proposed DDCM-Net consists of dense dilated convolutions merged with varying dilation rates. This can effectively enlarge the kernels' receptive fields, and, more importantly, obtain fused local and global context information to promote surrounding discriminative capability. We demonstrate the effectiveness of the proposed DDCM-Net on the publicly available ISPRS Potsdam dataset and achieve a performance of 92.3% F1-score and 86.0% mean intersection over union accuracy by only using the RGB bands, without any post-processing. We also show results on the ISPRS Vaihingen dataset, where the DDCM-Net trained with IRRG bands, also obtained better mapping accuracy (89.8% F1-score) than previous state-of-the-art approaches.

</details>

<details>

<summary>2019-08-30 16:18:26 - Learning Rich Representations For Structured Visual Prediction Tasks</summary>

- *Mohammadreza Mostajabi*

- `1908.11820v1` - [abs](http://arxiv.org/abs/1908.11820v1) - [pdf](http://arxiv.org/pdf/1908.11820v1)

> We describe an approach to learning rich representations for images, that enables simple and effective predictors in a range of vision tasks involving spatially structured maps. Our key idea is to map small image elements to feature representations extracted from a sequence of nested regions of increasing spatial extent. These regions are obtained by "zooming out" from the pixel/superpixel all the way to scene-level resolution, and hence we call these zoom-out features. Applied to semantic segmentation and other structured prediction tasks, our approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead image elements are classified by a feedforward multilayer network with skip-layer connections spanning the zoom-out levels. When used in conjunction with modern neural architectures such as ResNet, DenseNet and NASNet (to which it is complementary) our approach achieves competitive accuracy on segmentation benchmarks.   In addition, we propose an approach for learning category-level semantic segmentation purely from image-level classification tag. It exploits localization cues that emerge from training a modified zoom-out architecture tailored for classification tasks, to drive a weakly supervised process that automatically labels a sparse, diverse training set of points likely to belong to classes of interest. Finally, we introduce data-driven regularization functions for the supervised training of CNNs. Our innovation takes the form of a regularizer derived by learning an autoencoder over the set of annotations. This approach leverages an improved representation of label space to inform extraction of features from images

</details>

<details>

<summary>2019-08-30 19:10:13 - On the Contributions of Visual and Textual Supervision in Low-Resource Semantic Speech Retrieval</summary>

- *Ankita Pasad, Bowen Shi, Herman Kamper, Karen Livescu*

- `1904.10947v2` - [abs](http://arxiv.org/abs/1904.10947v2) - [pdf](http://arxiv.org/pdf/1904.10947v2)

> Recent work has shown that speech paired with images can be used to learn semantically meaningful speech representations even without any textual supervision. In real-world low-resource settings, however, we often have access to some transcribed speech. We study whether and how visual grounding is useful in the presence of varying amounts of textual supervision. In particular, we consider the task of semantic speech retrieval in a low-resource setting. We use a previously studied data set and task, where models are trained on images with spoken captions and evaluated on human judgments of semantic relevance. We propose a multitask learning approach to leverage both visual and textual modalities, with visual supervision in the form of keyword probabilities from an external tagger. We find that visual grounding is helpful even in the presence of textual supervision, and we analyze this effect over a range of sizes of transcribed data sets. With ~5 hours of transcribed speech, we obtain 23% higher average precision when also using visual supervision.

</details>

<details>

<summary>2019-08-30 23:27:06 - Automatically Inferring Gender Associations from Language</summary>

- *Serina Chang, Kathleen McKeown*

- `1909.00091v1` - [abs](http://arxiv.org/abs/1909.00091v1) - [pdf](http://arxiv.org/pdf/1909.00091v1)

> In this paper, we pose the question: do people talk about women and men in different ways? We introduce two datasets and a novel integration of approaches for automatically inferring gender associations from language, discovering coherent word clusters, and labeling the clusters for the semantic concepts they represent. The datasets allow us to compare how people write about women and men in two different settings - one set draws from celebrity news and the other from student reviews of computer science professors. We demonstrate that there are large-scale differences in the ways that people talk about women and men and that these differences vary across domains. Human evaluations show that our methods significantly outperform strong baselines.

</details>

<details>

<summary>2019-08-31 01:20:00 - Your Smart Home Can't Keep a Secret: Towards Automated Fingerprinting of IoT Traffic with Neural Networks</summary>

- *Shuaike Dong, Zhou Li, Di Tang, Jiongyi Chen, Menghan Sun, Kehuan Zhang*

- `1909.00104v1` - [abs](http://arxiv.org/abs/1909.00104v1) - [pdf](http://arxiv.org/pdf/1909.00104v1)

> The IoT (Internet of Things) technology has been widely adopted in recent years and has profoundly changed the people's daily lives. However, in the meantime, such a fast-growing technology has also introduced new privacy issues, which need to be better understood and measured. In this work, we look into how private information can be leaked from network traffic generated in the smart home network. Although researchers have proposed techniques to infer IoT device types or user behaviors under clean experiment setup, the effectiveness of such approaches become questionable in the complex but realistic network environment, where common techniques like Network Address and Port Translation (NAPT) and Virtual Private Network (VPN) are enabled. Traffic analysis using traditional methods (e.g., through classical machine-learning models) is much less effective under those settings, as the features picked manually are not distinctive any more. In this work, we propose a traffic analysis framework based on sequence-learning techniques like LSTM and leveraged the temporal relations between packets for the attack of device identification. We evaluated it under different environment settings (e.g., pure-IoT and noisy environment with multiple non-IoT devices). The results showed our framework was able to differentiate device types with a high accuracy. This result suggests IoT network communications pose prominent challenges to users' privacy, even when they are protected by encryption and morphed by the network gateway. As such, new privacy protection methods on IoT traffic need to be developed towards mitigating this new issue.

</details>

<details>

<summary>2019-08-31 02:10:32 - Behavior Gated Language Models</summary>

- *Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan*

- `1909.00107v1` - [abs](http://arxiv.org/abs/1909.00107v1) - [pdf](http://arxiv.org/pdf/1909.00107v1)

> Most current language modeling techniques only exploit co-occurrence, semantic and syntactic information from the sequence of words. However, a range of information such as the state of the speaker and dynamics of the interaction might be useful. In this work we derive motivation from psycholinguistics and propose the addition of behavioral information into the context of language modeling. We propose the augmentation of language models with an additional module which analyzes the behavioral state of the current context. This behavioral information is used to gate the outputs of the language model before the final word prediction output. We show that the addition of behavioral context in language models achieves lower perplexities on behavior-rich datasets. We also confirm the validity of the proposed models on a variety of model architectures and improve on previous state-of-the-art models with generic domain Penn Treebank Corpus.

</details>

<details>

<summary>2019-08-31 10:43:06 - Improving Multi-Head Attention with Capsule Networks</summary>

- *Shuhao Gu, Yang Feng*

- `1909.00188v1` - [abs](http://arxiv.org/abs/1909.00188v1) - [pdf](http://arxiv.org/pdf/1909.00188v1)

> Multi-head attention advances neural machine translation by working out multiple versions of attention in different subspaces, but the neglect of semantic overlapping between subspaces increases the difficulty of translation and consequently hinders the further improvement of translation performance. In this paper, we employ capsule networks to comb the information from the multiple heads of the attention so that similar information can be clustered and unique information can be reserved. To this end, we adopt two routing mechanisms of Dynamic Routing and EM Routing, to fulfill the clustering and separating. We conducted experiments on Chinese-to-English and English-to-German translation tasks and got consistent improvements over the strong Transformer baseline.

</details>

<details>

<summary>2019-08-31 20:07:25 - Generating Classical Chinese Poems from Vernacular Chinese</summary>

- *Zhichao Yang, Pengshan Cai, Yansong Feng, Fei Li, Weijiang Feng, Elena Suet-Ying Chiu, Hong Yu*

- `1909.00279v1` - [abs](http://arxiv.org/abs/1909.00279v1) - [pdf](http://arxiv.org/pdf/1909.00279v1)

> Classical Chinese poetry is a jewel in the treasure house of Chinese culture. Previous poem generation models only allow users to employ keywords to interfere the meaning of generated poems, leaving the dominion of generation to the model. In this paper, we propose a novel task of generating classical Chinese poems from vernacular, which allows users to have more control over the semantic of generated poems. We adapt the approach of unsupervised machine translation (UMT) to our task. We use segmentation-based padding and reinforcement learning to address under-translation and over-translation respectively. According to experiments, our approach significantly improve the perplexity and BLEU compared with typical UMT models. Furthermore, we explored guidelines on how to write the input vernacular to generate better poems. Human evaluation showed our approach can generate high-quality poems which are comparable to amateur poems.

</details>

<details>

<summary>2019-08-31 22:50:42 - Second-order Non-local Attention Networks for Person Re-identification</summary>

- *Bryan, Xia, Yuan Gong, Yizhe Zhang, Christian Poellabauer*

- `1909.00295v1` - [abs](http://arxiv.org/abs/1909.00295v1) - [pdf](http://arxiv.org/pdf/1909.00295v1)

> Recent efforts have shown promising results for person re-identification by designing part-based architectures to allow a neural network to learn discriminative representations from semantically coherent parts. Some efforts use soft attention to reallocate distant outliers to their most similar parts, while others adjust part granularity to incorporate more distant positions for learning the relationships. Others seek to generalize part-based methods by introducing a dropout mechanism on consecutive regions of the feature map to enhance distant region relationships. However, only few prior efforts model the distant or non-local positions of the feature map directly for the person re-ID task. In this paper, we propose a novel attention mechanism to directly model long-range relationships via second-order feature statistics. When combined with a generalized DropBlock module, our method performs equally to or better than state-of-the-art results for mainstream person re-identification datasets, including Market1501, CUHK03, and DukeMTMC-reID.

</details>


## 2019-09

<details>

<summary>2019-09-01 09:14:52 - Cross-Lingual Machine Reading Comprehension</summary>

- *Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu*

- `1909.00361v1` - [abs](http://arxiv.org/abs/1909.00361v1) - [pdf](http://arxiv.org/pdf/1909.00361v1)

> Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data. In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task, which is straightforward to adopt. However, to accurately align the answer into another language is difficult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in a bilingual context, and then utilize the learned knowledge to improve reading comprehension performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and significant improvements over various state-of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. Resources available: https://github.com/ymcui/Cross-Lingual-MRC

</details>

<details>

<summary>2019-09-02 09:16:06 - Enriching Medcial Terminology Knowledge Bases via Pre-trained Language Model and Graph Convolutional Network</summary>

- *Jiaying Zhang, Zhixing Zhang, Huanhuan Zhang, Zhiyuan Ma, Yangming Zhou, Ping He*

- `1909.00615v1` - [abs](http://arxiv.org/abs/1909.00615v1) - [pdf](http://arxiv.org/pdf/1909.00615v1)

> Enriching existing medical terminology knowledge bases (KBs) is an important and never-ending work for clinical research because new terminology alias may be continually added and standard terminologies may be newly renamed. In this paper, we propose a novel automatic terminology enriching approach to supplement a set of terminologies to KBs. Specifically, terminology and entity characters are first fed into pre-trained language model to obtain semantic embedding. The pre-trained model is used again to initialize the terminology and entity representations, then they are further embedded through graph convolutional network to gain structure embedding. Afterwards, both semantic and structure embeddings are combined to measure the relevancy between the terminology and the entity. Finally, the optimal alignment is achieved based on the order of relevancy between the terminology and all the entities in the KB. Experimental results on clinical indicator terminology KB, collected from 38 top-class hospitals of Shanghai Hospital Development Center, show that our proposed approach outperforms baseline methods and can effectively enrich the KB.

</details>

<details>

<summary>2019-09-02 09:19:33 - Reinforcement Learning-based Automatic Diagnosis of Acute Appendicitis in Abdominal CT</summary>

- *Walid Abdullah Al, Il Dong Yun, Kyong Joon Lee*

- `1909.00617v1` - [abs](http://arxiv.org/abs/1909.00617v1) - [pdf](http://arxiv.org/pdf/1909.00617v1)

> Acute appendicitis characterized by a painful inflammation of the vermiform appendix is one of the most common surgical emergencies. Localizing the appendix is challenging due to its unclear anatomy amidst the complex colon-structure as observed in the conventional CT views, resulting in a time-consuming diagnosis. End-to-end learning of a convolutional neural network (CNN) is also not likely to be useful because of the negligible size of the appendix compared with the abdominal CT volume. With no prior computational approaches to the best of our knowledge, we propose the first computerized automation for acute appendicitis diagnosis. In our approach, we utilize a reinforcement learning agent deployed in the lower abdominal region to obtain the appendix location first to reduce the search space for diagnosis. Then, we obtain the classification scores (i.e., the likelihood of acute appendicitis) for the local neighborhood around the localized position, using a CNN trained only on a small appendix patch per volume. From the spatial representation of the resultant scores, we finally define a region of low-entropy (RLE) to choose the optimal diagnosis score, which helps improve the classification accuracy showing robustness even under high appendix localization error cases. In our experiment with 319 abdominal CT volumes, the proposed RLE-based decision with prior localization showed significant improvement over the standard CNN-based diagnosis approaches.

</details>

<details>

<summary>2019-09-02 09:23:48 - Design and Results of the Second International Competition on Computational Models of Argumentation</summary>

- *Sarah A. Gaggl, Thomas Linsbichler, Marco Maratea, Stefan Woltran*

- `1909.00621v1` - [abs](http://arxiv.org/abs/1909.00621v1) - [pdf](http://arxiv.org/pdf/1909.00621v1)

> Argumentation is a major topic in the study of Artificial Intelligence. Since the first edition in 2015, advancements in solving (abstract) argumentation frameworks are assessed in competition events, similar to other closely related problem solving technologies. In this paper, we report about the design and results of the Second International Competition on Computational Models of Argumentation, which has been jointly organized by TU Dresden (Germany), TU Wien (Austria), and the University of Genova (Italy), in affiliation with the 2017 International Workshop on Theory and Applications of Formal Argumentation. This second edition maintains some of the design choices made in the first event, e.g. the I/O formats, the basic reasoning problems, and the organization into tasks and tracks. At the same time, it introduces significant novelties, e.g. three additional prominent semantics, and an instance selection stage for classifying instances according to their empirical hardness.

</details>

<details>

<summary>2019-09-02 12:38:58 - The Semantic Asset Administration Shell</summary>

- *Sebastian R. Bader, Maria Maleshkova*

- `1909.00690v1` - [abs](http://arxiv.org/abs/1909.00690v1) - [pdf](http://arxiv.org/pdf/1909.00690v1)

> The disruptive potential of the upcoming digital transformations for the industrial manufacturing domain have led to several reference frameworks and numerous standardization approaches. On the other hand, the Semantic Web community has made significant contributions in the field, for instance on data and service description, integration of heterogeneous sources and devices, and AI techniques in distributed systems. These two streams of work are, however, mostly unrelated and only briefly regard each others requirements, practices and terminology. We contribute to closing this gap by providing the Semantic Asset Administration Shell, an RDF-based representation of the Industrie 4.0 Component. We provide an ontology for the latest data model specification, created a RML mapping, supply resources to validate the RDF entities and introduce basic reasoning on the Asset Administration Shell data model. Furthermore, we discuss the different assumptions and presentation patterns, and analyze the implications of a semantic representation on the original data. We evaluate the thereby created overheads, and conclude that the semantic lifting is manageable, also for restricted or embedded devices, and therefore meets the needs of Industrie 4.0 scenarios.

</details>

<details>

<summary>2019-09-02 13:15:24 - Learned Semantic Multi-Sensor Depth Map Fusion</summary>

- *Denys Rozumnyi, Ian Cherabier, Marc Pollefeys, Martin R. Oswald*

- `1909.00703v1` - [abs](http://arxiv.org/abs/1909.00703v1) - [pdf](http://arxiv.org/pdf/1909.00703v1)

> Volumetric depth map fusion based on truncated signed distance functions has become a standard method and is used in many 3D reconstruction pipelines. In this paper, we are generalizing this classic method in multiple ways: 1) Semantics: Semantic information enriches the scene representation and is incorporated into the fusion process. 2) Multi-Sensor: Depth information can originate from different sensors or algorithms with very different noise and outlier statistics which are considered during data fusion. 3) Scene denoising and completion: Sensors can fail to recover depth for certain materials and light conditions, or data is missing due to occlusions. Our method denoises the geometry, closes holes and computes a watertight surface for every semantic class. 4) Learning: We propose a neural network reconstruction method that unifies all these properties within a single powerful framework. Our method learns sensor or algorithm properties jointly with semantic depth fusion and scene completion and can also be used as an expert system, e.g. to unify the strengths of various photometric stereo algorithms. Our approach is the first to unify all these properties. Experimental evaluations on both synthetic and real data sets demonstrate clear improvements.

</details>

<details>

<summary>2019-09-02 20:15:28 - Simple Attention-Based Representation Learning for Ranking Short Social Media Posts</summary>

- *Peng Shi, Jinfeng Rao, Jimmy Lin*

- `1811.01013v2` - [abs](http://arxiv.org/abs/1811.01013v2) - [pdf](http://arxiv.org/pdf/1811.01013v2)

> This paper explores the problem of ranking short social media posts with respect to user queries using neural networks. Instead of starting with a complex architecture, we proceed from the bottom up and examine the effectiveness of a simple, word-level Siamese architecture augmented with attention-based mechanisms for capturing semantic "soft" matches between query and post tokens. Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models not only achieve better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals, but are also much faster. Implementations of our samCNN (Simple Attention-based Matching CNN) models are shared with the community to support future work.

</details>

<details>

<summary>2019-09-03 03:06:46 - Transfer Fine-Tuning: A BERT Case Study</summary>

- *Yuki Arase, Junichi Tsujii*

- `1909.00931v1` - [abs](http://arxiv.org/abs/1909.00931v1) - [pdf](http://arxiv.org/pdf/1909.00931v1)

> A semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of tasks crucial for research on natural language understanding. Recently, BERT realized a breakthrough in sentence representation learning (Devlin et al., 2019), which is broadly transferable to various NLP tasks. While BERT's performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated model exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning.

</details>

<details>

<summary>2019-09-03 08:27:37 - From Textual Information Sources to Linked Data in the Agatha Project</summary>

- *Paulo Quaresma, Vitor Beires Nogueira, Kashyap Raiyani, Roy Bayot, Teresa Gonçalves*

- `1909.05359v1` - [abs](http://arxiv.org/abs/1909.05359v1) - [pdf](http://arxiv.org/pdf/1909.05359v1)

> Automatic reasoning about textual information is a challenging task in modern Natural Language Processing (NLP) systems. In this work we describe our proposal for representing and reasoning about Portuguese documents by means of Linked Data like ontologies and thesauri. Our approach resorts to a specialized pipeline of natural language processing (part-of-speech tagger, named entity recognition, semantic role labeling) to populate an ontology for the domain of criminal investigations. The provided architecture and ontology are language independent. Although some of the NLP modules are language dependent, they can be built using adequate AI methodologies.

</details>

<details>

<summary>2019-09-03 10:48:42 - STaDA: Style Transfer as Data Augmentation</summary>

- *Xu Zheng, Tejo Chalasani, Koustav Ghosal, Sebastian Lutz, Aljosa Smolic*

- `1909.01056v1` - [abs](http://arxiv.org/abs/1909.01056v1) - [pdf](http://arxiv.org/pdf/1909.01056v1)

> The success of training deep Convolutional Neural Networks (CNNs) heavily depends on a significant amount of labelled data. Recent research has found that neural style transfer algorithms can apply the artistic style of one image to another image without changing the latter's high-level semantic content, which makes it feasible to employ neural style transfer as a data augmentation method to add more variation to the training dataset. The contribution of this paper is a thorough evaluation of the effectiveness of the neural style transfer as a data augmentation method for image classification tasks. We explore the state-of-the-art neural style transfer algorithms and apply them as a data augmentation method on Caltech 101 and Caltech 256 dataset, where we found around 2% improvement from 83% to 85% of the image classification accuracy with VGG16, compared with traditional data augmentation strategies. We also combine this new method with conventional data augmentation approaches to further improve the performance of image classification. This work shows the potential of neural style transfer in computer vision field, such as helping us to reduce the difficulty of collecting sufficient labelled data and improve the performance of generic image-based deep learning algorithms.

</details>

<details>

<summary>2019-09-03 12:04:39 - ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth Image</summary>

- *Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari*

- `1909.01106v1` - [abs](http://arxiv.org/abs/1909.01106v1) - [pdf](http://arxiv.org/pdf/1909.01106v1)

> We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space. To transfer information between the geometric and semantic branches of the network, we introduce paths between them concatenating features at corresponding network layers. Motivated by the limited amount of training samples from real scenes, an interesting attribute of our architecture is the capacity to supplement the existing dataset by generating a new training dataset with high quality, realistic scenes that even includes occlusion and real noise. We build the new dataset by sampling the features directly from latent space which generates a pair of partial volumetric surface and completed volumetric semantic surface. Moreover, we utilize multiple discriminators to increase the accuracy and realism of the reconstructions. We demonstrate the benefits of our approach on standard benchmarks for the two most common completion tasks: semantic 3D scene completion and 3D object completion.

</details>

<details>

<summary>2019-09-03 16:40:24 - PolyResponse: A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking</summary>

- *Matthew Henderson, Ivan Vulić, Iñigo Casanueva, Paweł Budzianowski, Daniela Gerz, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen, Nikola Mrkšić, Pei-Hao Su*

- `1909.01296v1` - [abs](http://arxiv.org/abs/1909.01296v1) - [pdf](http://arxiv.org/pdf/1909.01296v1)

> We present PolyResponse, a conversational search engine that supports task-oriented dialogue. It is a retrieval-based approach that bypasses the complex multi-component design of traditional task-oriented dialogue systems and the use of explicit semantics in the form of task-specific ontologies. The PolyResponse engine is trained on hundreds of millions of examples extracted from real conversations: it learns what responses are appropriate in different conversational contexts. It then ranks a large index of text and visual responses according to their similarity to the given context, and narrows down the list of relevant entities during the multi-turn conversation. We introduce a restaurant search and booking system powered by the PolyResponse engine, currently available in 8 different languages.

</details>

<details>

<summary>2019-09-03 21:20:28 - Interpretable Word Embeddings via Informative Priors</summary>

- *Miriam Hurtado Bodell, Martin Arvidsson, Måns Magnusson*

- `1909.01459v1` - [abs](http://arxiv.org/abs/1909.01459v1) - [pdf](http://arxiv.org/pdf/1909.01459v1)

> Word embeddings have demonstrated strong performance on NLP tasks. However, lack of interpretability and the unsupervised nature of word embeddings have limited their use within computational social science and digital humanities. We propose the use of informative priors to create interpretable and domain-informed dimensions for probabilistic word embeddings. Experimental results show that sensible priors can capture latent semantic concepts better than or on-par with the current state of the art, while retaining the simplicity and generalizability of using priors.

</details>

<details>

<summary>2019-09-04 09:23:50 - ParaQG: A System for Generating Questions and Answers from Paragraphs</summary>

- *Vishwajeet Kumar, Sivaanandh Muneeswaran, Ganesh Ramakrishnan, Yuan-Fang Li*

- `1909.01642v1` - [abs](http://arxiv.org/abs/1909.01642v1) - [pdf](http://arxiv.org/pdf/1909.01642v1)

> Generating syntactically and semantically valid and relevant questions from paragraphs is useful with many applications. Manual generation is a labour-intensive task, as it requires the reading, parsing and understanding of long passages of text. A number of question generation models based on sequence-to-sequence techniques have recently been proposed. Most of them generate questions from sentences only, and none of them is publicly available as an easy-to-use service. In this paper, we demonstrate ParaQG, a Web-based system for generating questions from sentences and paragraphs. ParaQG incorporates a number of novel functionalities to make the question generation process user-friendly. It provides an interactive interface for a user to select answers with visual insights on generation of questions. It also employs various faceted views to group similar questions as well as filtering techniques to eliminate unanswerable questions

</details>

<details>

<summary>2019-09-04 11:37:26 - SAO WMT19 Test Suite: Machine Translation of Audit Reports</summary>

- *Tereza Vojtěchová, Michal Novák, Miloš Klouček, Ondřej Bojar*

- `1909.01701v1` - [abs](http://arxiv.org/abs/1909.01701v1) - [pdf](http://arxiv.org/pdf/1909.01701v1)

> This paper describes a machine translation test set of documents from the auditing domain and its use as one of the "test suites" in the WMT19 News Translation Task for translation directions involving Czech, English and German.   Our evaluation suggests that current MT systems optimized for the general news domain can perform quite well even in the particular domain of audit reports. The detailed manual evaluation however indicates that deep factual knowledge of the domain is necessary. For the naked eye of a non-expert, translations by many systems seem almost perfect and automatic MT evaluation with one reference is practically useless for considering these details.   Furthermore, we show on a sample document from the domain of agreements that even the best systems completely fail in preserving the semantics of the agreement, namely the identity of the parties.

</details>

<details>

<summary>2019-09-04 14:33:08 - Transport-Based Neural Style Transfer for Smoke Simulations</summary>

- *Byungsoo Kim, Vinicius C. Azevedo, Markus Gross, Barbara Solenthaler*

- `1905.07442v2` - [abs](http://arxiv.org/abs/1905.07442v2) - [pdf](http://arxiv.org/pdf/1905.07442v2)

> Artistically controlling fluids has always been a challenging task. Optimization techniques rely on approximating simulation states towards target velocity or density field configurations, which are often handcrafted by artists to indirectly control smoke dynamics. Patch synthesis techniques transfer image textures or simulation features to a target flow field. However, these are either limited to adding structural patterns or augmenting coarse flows with turbulent structures, and hence cannot capture the full spectrum of different styles and semantically complex structures. In this paper, we propose the first Transport-based Neural Style Transfer (TNST) algorithm for volumetric smoke data. Our method is able to transfer features from natural images to smoke simulations, enabling general content-aware manipulations ranging from simple patterns to intricate motifs. The proposed algorithm is physically inspired, since it computes the density transport from a source input smoke to a desired target configuration. Our transport-based approach allows direct control over the divergence of the stylization velocity field by optimizing incompressible and irrotational potentials that transport smoke towards stylization. Temporal consistency is ensured by transporting and aligning subsequent stylized velocities, and 3D reconstructions are computed by seamlessly merging stylizations from different camera viewpoints.

</details>

<details>

<summary>2019-09-04 17:23:54 - Mixture Content Selection for Diverse Sequence Generation</summary>

- *Jaemin Cho, Minjoon Seo, Hannaneh Hajishirzi*

- `1909.01953v1` - [abs](http://arxiv.org/abs/1909.01953v1) - [pdf](http://arxiv.org/pdf/1909.01953v1)

> Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state of the art model. Our code is publicly available at https://github.com/clovaai/FocusSeq2Seq.

</details>

<details>

<summary>2019-09-04 19:09:49 - DCGANs for Realistic Breast Mass Augmentation in X-ray Mammography</summary>

- *Basel Alyafi, Oliver Diaz, Robert Marti*

- `1909.02062v1` - [abs](http://arxiv.org/abs/1909.02062v1) - [pdf](http://arxiv.org/pdf/1909.02062v1)

> Early detection of breast cancer has a major contribution to curability, and using mammographic images, this can be achieved non-invasively. Supervised deep learning, the dominant CADe tool currently, has played a great role in object detection in computer vision, but it suffers from a limiting property: the need of a large amount of labelled data. This becomes stricter when it comes to medical datasets which require high-cost and time-consuming annotations. Furthermore, medical datasets are usually imbalanced, a condition that often hinders classifiers performance. The aim of this paper is to learn the distribution of the minority class to synthesise new samples in order to improve lesion detection in mammography. Deep Convolutional Generative Adversarial Networks (DCGANs) can efficiently generate breast masses. They are trained on increasing-size subsets of one mammographic dataset and used to generate diverse and realistic breast masses. The effect of including the generated images and/or applying horizontal and vertical flipping is tested in an environment where a 1:10 imbalanced dataset of masses and normal tissue patches is classified by a fully-convolutional network. A maximum of ~ 0:09 improvement of F1 score is reported by using DCGANs along with flipping augmentation over using the original images. We show that DCGANs can be used for synthesising photo-realistic breast mass patches with considerable diversity. It is demonstrated that appending synthetic images in this environment, along with flipping, outperforms the traditional augmentation method of flipping solely, offering faster improvements as a function of the training set size.

</details>

<details>

<summary>2019-09-04 20:37:30 - Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering</summary>

- *Soravit Changpinyo, Bo Pang, Piyush Sharma, Radu Soricut*

- `1909.02097v1` - [abs](http://arxiv.org/abs/1909.02097v1) - [pdf](http://arxiv.org/pdf/1909.02097v1)

> Object detection plays an important role in current solutions to vision and language tasks like image captioning and visual question answering. However, popular models like Faster R-CNN rely on a costly process of annotating ground-truths for both the bounding boxes and their corresponding semantic labels, making it less amenable as a primitive task for transfer learning. In this paper, we examine the effect of decoupling box proposal and featurization for down-stream tasks. The key insight is that this allows us to leverage a large amount of labeled annotations that were previously unavailable for standard object detection benchmarks. Empirically, we demonstrate that this leads to effective transfer learning and improved image captioning and visual question answering models, as measured on publicly available benchmarks.

</details>

<details>

<summary>2019-09-04 23:37:25 - KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning</summary>

- *Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang Ren*

- `1909.02151v1` - [abs](http://arxiv.org/abs/1909.02151v1) - [pdf](http://arxiv.org/pdf/1909.02151v1)

> Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.

</details>

<details>

<summary>2019-09-05 01:13:41 - Identifying and Explaining Discriminative Attributes</summary>

- *Armins Stepanjans, André Freitas*

- `1909.05363v1` - [abs](http://arxiv.org/abs/1909.05363v1) - [pdf](http://arxiv.org/pdf/1909.05363v1)

> Identifying what is at the center of the meaning of a word and what discriminates it from other words is a fundamental natural language inference task. This paper describes an explicit word vector representation model (WVM) to support the identification of discriminative attributes. A core contribution of the paper is a quantitative and qualitative comparative analysis of different types of data sources and Knowledge Bases in the construction of explainable and explicit WVMs: (i) knowledge graphs built from dictionary definitions, (ii) entity-attribute-relationships graphs derived from images and (iii) commonsense knowledge graphs. Using a detailed quantitative and qualitative analysis, we demonstrate that these data sources have complementary semantic aspects, supporting the creation of explicit semantic vector spaces. The explicit vector spaces are evaluated using the task of discriminative attribute identification, showing comparable performance to the state-of-the-art systems in the task (F1-score = 0.69), while delivering full model transparency and explainability.

</details>

<details>

<summary>2019-09-05 02:41:56 - A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding</summary>

- *Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen, Ting Liu*

- `1909.02188v1` - [abs](http://arxiv.org/abs/1909.02188v1) - [pdf](http://arxiv.org/pdf/1909.02188v1)

> Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two tasks are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel framework for SLU to better incorporate the intent information, which further guides the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the error propagation, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our model achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task.

</details>

<details>

<summary>2019-09-05 03:26:43 - MoralStrength: Exploiting a Moral Lexicon and Embedding Similarity for Moral Foundations Prediction</summary>

- *Oscar Araque, Lorenzo Gatti, Kyriaki Kalimeri*

- `1904.08314v2` - [abs](http://arxiv.org/abs/1904.08314v2) - [pdf](http://arxiv.org/pdf/1904.08314v2)

> Moral rhetoric plays a fundamental role in how we perceive and interpret the information we receive, greatly influencing our decision-making process. Especially when it comes to controversial social and political issues, our opinions and attitudes are hardly ever based on evidence alone. The Moral Foundations Dictionary (MFD) was developed to operationalize moral values in the text. In this study, we present MoralStrength, a lexicon of approximately 1,000 lemmas, obtained as an extension of the Moral Foundations Dictionary, based on WordNet synsets. Moreover, for each lemma it provides with a crowdsourced numeric assessment of Moral Valence, indicating the strength with which a lemma is expressing the specific value. We evaluated the predictive potentials of this moral lexicon, defining three utilization approaches of increased complexity, ranging from lemmas' statistical properties to a deep learning approach of word embeddings based on semantic similarity. Logistic regression models trained on the features extracted from MoralStrength, significantly outperformed the current state-of-the-art, reaching an F1-score of 87.6% over the previous 62.4% (p-value<0.01), and an average F1-Score of 86.25% over six different datasets. Such findings pave the way for further research, allowing for an in-depth understanding of moral narratives in text for a wide range of social issues.

</details>

<details>

<summary>2019-09-05 05:48:51 - A Better Way to Attend: Attention with Trees for Video Question Answering</summary>

- *Hongyang Xue, Wenqing Chu, Zhou Zhao, Deng Cai*

- `1909.02218v1` - [abs](http://arxiv.org/abs/1909.02218v1) - [pdf](http://arxiv.org/pdf/1909.02218v1)

> We propose a new attention model for video question answering. The main idea of the attention models is to locate on the most informative parts of the visual data. The attention mechanisms are quite popular these days. However, most existing visual attention mechanisms regard the question as a whole. They ignore the word-level semantics where each word can have different attentions and some words need no attention. Neither do they consider the semantic structure of the sentences. Although the Extended Soft Attention (E-SA) model for video question answering leverages the word-level attention, it performs poorly on long question sentences. In this paper, we propose the heterogeneous tree-structured memory network (HTreeMN) for video question answering. Our proposed approach is based upon the syntax parse trees of the question sentences. The HTreeMN treats the words differently where the \textit{visual} words are processed with an attention module and the \textit{verbal} ones not. It also utilizes the semantic structure of the sentences by combining the neighbors based on the recursive structure of the parse trees. The understandings of the words and the videos are propagated and merged from leaves to the root. Furthermore, we build a hierarchical attention mechanism to distill the attended features. We evaluate our approach on two datasets. The experimental results show the superiority of our HTreeMN model over the other attention models especially on complex questions. Our code is available on github.   Our code is available at https://github.com/ZJULearning/TreeAttention

</details>

<details>

<summary>2019-09-05 06:02:03 - Variational Semi-supervised Aspect-term Sentiment Analysis via Transformer</summary>

- *Xingyi Cheng, Weidi Xu, Taifeng Wang, Wei Chu*

- `1810.10437v3` - [abs](http://arxiv.org/abs/1810.10437v3) - [pdf](http://arxiv.org/pdf/1810.10437v3)

> Aspect-term sentiment analysis (ATSA) is a longstanding challenge in natural language understanding. It requires fine-grained semantical reasoning about a target entity appeared in the text. As manual annotation over the aspects is laborious and time-consuming, the amount of labeled data is limited for supervised learning. This paper proposes a semi-supervised method for the ATSA problem by using the Variational Autoencoder based on Transformer (VAET), which models the latent distribution via variational inference. By disentangling the latent representation into the aspect-specific sentiment and the lexical context, our method induces the underlying sentiment prediction for the unlabeled data, which then benefits the ATSA classifier. Our method is classifier agnostic, i.e., the classifier is an independent module and various advanced supervised models can be integrated. Experimental results are obtained on the SemEval 2014 task 4 and show that our method is effective with four classical classifiers. The proposed method outperforms two general semisupervised methods and achieves state-of-the-art performance.

</details>

<details>

<summary>2019-09-05 12:04:19 - Text Summarization with Pretrained Encoders</summary>

- *Yang Liu, Mirella Lapata*

- `1908.08345v2` - [abs](http://arxiv.org/abs/1908.08345v2) - [pdf](http://arxiv.org/pdf/1908.08345v2)

> Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings. Our code is available at https://github.com/nlpyang/PreSumm

</details>

<details>

<summary>2019-09-05 15:41:53 - Stack-VS: Stacked Visual-Semantic Attention for Image Caption Generation</summary>

- *Wei Wei, Ling Cheng, Xianling Mao, Guangyou Zhou, Feida Zhu*

- `1909.02489v1` - [abs](http://arxiv.org/abs/1909.02489v1) - [pdf](http://arxiv.org/pdf/1909.02489v1)

> Recently, automatic image caption generation has been an important focus of the work on multimodal translation task. Existing approaches can be roughly categorized into two classes, i.e., top-down and bottom-up, the former transfers the image information (called as visual-level feature) directly into a caption, and the later uses the extracted words (called as semanticlevel attribute) to generate a description. However, previous methods either are typically based one-stage decoder or partially utilize part of visual-level or semantic-level information for image caption generation. In this paper, we address the problem and propose an innovative multi-stage architecture (called as Stack-VS) for rich fine-gained image caption generation, via combining bottom-up and top-down attention models to effectively handle both visual-level and semantic-level information of an input image. Specifically, we also propose a novel well-designed stack decoder model, which is constituted by a sequence of decoder cells, each of which contains two LSTM-layers work interactively to re-optimize attention weights on both visual-level feature vectors and semantic-level attribute embeddings for generating a fine-gained image caption. Extensive experiments on the popular benchmark dataset MSCOCO show the significant improvements on different evaluation metrics, i.e., the improvements on BLEU-4/CIDEr/SPICE scores are 0.372, 1.226 and 0.216, respectively, as compared to the state-of-the-arts.

</details>

<details>

<summary>2019-09-06 03:31:40 - Port-Hamiltonian Approach to Neural Network Training</summary>

- *Stefano Massaroli, Michael Poli, Federico Califano, Angela Faragasso, Jinkyoo Park, Atsushi Yamashita, Hajime Asama*

- `1909.02702v1` - [abs](http://arxiv.org/abs/1909.02702v1) - [pdf](http://arxiv.org/pdf/1909.02702v1)

> Neural networks are discrete entities: subdivided into discrete layers and parametrized by weights which are iteratively optimized via difference equations. Recent work proposes networks with layer outputs which are no longer quantized but are solutions of an ordinary differential equation (ODE); however, these networks are still optimized via discrete methods (e.g. gradient descent). In this paper, we explore a different direction: namely, we propose a novel framework for learning in which the parameters themselves are solutions of ODEs. By viewing the optimization process as the evolution of a port-Hamiltonian system, we can ensure convergence to a minimum of the objective function. Numerical experiments have been performed to show the validity and effectiveness of the proposed methods.

</details>

<details>

<summary>2019-09-06 06:20:15 - Learning Programmatic Idioms for Scalable Semantic Parsing</summary>

- *Srinivasan Iyer, Alvin Cheung, Luke Zettlemoyer*

- `1904.09086v2` - [abs](http://arxiv.org/abs/1904.09086v2) - [pdf](http://arxiv.org/pdf/1904.09086v2)

> Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2\% BLEU score while reducing training time by more than 50\%. This improved speed enables us to scale up the model by training on an extended training set that is 5$\times$ larger, to further move up the SOTA by an additional 2.3\% BLEU and 0.9\% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.

</details>

<details>

<summary>2019-09-06 07:40:27 - Video Interpolation and Prediction with Unsupervised Landmarks</summary>

- *Kevin J. Shih, Aysegul Dundar, Animesh Garg, Robert Pottorf, Andrew Tao, Bryan Catanzaro*

- `1909.02749v1` - [abs](http://arxiv.org/abs/1909.02749v1) - [pdf](http://arxiv.org/pdf/1909.02749v1)

> Prediction and interpolation for long-range video data involves the complex task of modeling motion trajectories for each visible object, occlusions and dis-occlusions, as well as appearance changes due to viewpoint and lighting. Optical flow based techniques generalize but are suitable only for short temporal ranges. Many methods opt to project the video frames to a low dimensional latent space, achieving long-range predictions. However, these latent representations are often non-interpretable, and therefore difficult to manipulate. This work poses video prediction and interpolation as unsupervised latent structure inference followed by a temporal prediction in this latent space. The latent representations capture foreground semantics without explicit supervision such as keypoints or poses. Further, as each landmark can be mapped to a coordinate indicating where a semantic part is positioned, we can reliably interpolate within the coordinate domain to achieve predictable motion interpolation. Given an image decoder capable of mapping these landmarks back to the image domain, we are able to achieve high-quality long-range video interpolation and extrapolation by operating on the landmark representation space.

</details>

<details>

<summary>2019-09-06 07:47:20 - Context-aware Deep Model for Entity Recommendation in Search Engine at Alibaba</summary>

- *Qianghuai Jia, Ningyu Zhang, Nengwei Hua*

- `1909.04493v1` - [abs](http://arxiv.org/abs/1909.04493v1) - [pdf](http://arxiv.org/pdf/1909.04493v1)

> Entity recommendation, providing search users with an improved experience via assisting them in finding related entities for a given query, has become an indispensable feature of today's search engines. Existing studies typically only consider the queries with explicit entities. They usually fail to handle complex queries that without entities, such as "what food is good for cold weather", because their models could not infer the underlying meaning of the input text. In this work, we believe that contexts convey valuable evidence that could facilitate the semantic modeling of queries, and take them into consideration for entity recommendation. In order to better model the semantics of queries and entities, we learn the representation of queries and entities jointly with attentive deep neural networks. We evaluate our approach using large-scale, real-world search logs from a widely used commercial Chinese search engine. Our system has been deployed in ShenMa Search Engine and you can fetch it in UC Browser of Alibaba. Results from online A/B test suggest that the impression efficiency of click-through rate increased by 5.1% and page view increased by 5.5%.

</details>

<details>

<summary>2019-09-06 08:22:28 - Effective Search of Logical Forms for Weakly Supervised Knowledge-Based Question Answering</summary>

- *Tao Shen, Xiubo Geng, Tao Qin, Guodong Long, Jing Jiang, Daxin Jiang*

- `1909.02762v1` - [abs](http://arxiv.org/abs/1909.02762v1) - [pdf](http://arxiv.org/pdf/1909.02762v1)

> Many algorithms for Knowledge-Based Question Answering (KBQA) depend on semantic parsing, which translates a question to its logical form. When only weak supervision is provided, it is usually necessary to search valid logical forms for model training. However, a complex question typically involves a huge search space, which creates two main problems: 1) the solutions limited by computation time and memory usually reduce the success rate of the search, and 2) spurious logical forms in the search results degrade the quality of training data. These two problems lead to a poorly-trained semantic parsing model. In this work, we propose an effective search method for weakly supervised KBQA based on operator prediction for questions. With search space constrained by predicted operators, sufficient search paths can be explored, more valid logical forms can be derived, and operators possibly causing spurious logical forms can be avoided. As a result, a larger proportion of questions in a weakly supervised training set are equipped with logical forms, and fewer spurious logical forms are generated. Such high-quality training data directly contributes to a better semantic parsing model. Experimental results on one of the largest KBQA datasets (i.e., CSQA) verify the effectiveness of our approach: improving the precision from 67% to 72% and the recall from 67% to 72% in terms of the overall score.

</details>

<details>

<summary>2019-09-06 09:02:30 - Revisiting Semantic Representation and Tree Search for Similar Question Retrieval</summary>

- *Tong Guo, Huilin Gao*

- `1908.08326v8` - [abs](http://arxiv.org/abs/1908.08326v8) - [pdf](http://arxiv.org/pdf/1908.08326v8)

> This paper studies the performances of BERT combined with tree structure in short sentence ranking task. In retrieval-based question answering system, we retrieve the most similar question of the query question by ranking all the questions in datasets. If we want to rank all the sentences by neural rankers, we need to score all the sentence pairs. However it consumes large amount of time. So we design a specific tree for searching and combine deep model to solve this problem. We fine-tune BERT on the training data to get semantic vector or sentence embeddings on the test data. We use all the sentence embeddings of test data to build our tree based on k-means and do beam search at predicting time when given a sentence as query. We do the experiments on the semantic textual similarity dataset, Quora Question Pairs, and process the dataset for sentence ranking. Experimental results show that our methods outperform the strong baseline. Our tree accelerate the predicting speed by 500%-1000% without losing too much ranking accuracy.

</details>

<details>

<summary>2019-09-06 17:56:01 - Deep learning with sentence embeddings pre-trained on biomedical corpora improves the performance of finding similar sentences in electronic medical records</summary>

- *Qingyu Chen, Jingcheng Du, Sun Kim, W. John Wilbur, Zhiyong Lu*

- `1909.03044v1` - [abs](http://arxiv.org/abs/1909.03044v1) - [pdf](http://arxiv.org/pdf/1909.03044v1)

> Capturing sentence semantics plays a vital role in a range of text mining applications. Despite continuous efforts on the development of related datasets and models in the general domain, both datasets and models are limited in biomedical and clinical domains. The BioCreative/OHNLP organizers have made the first attempt to annotate 1,068 sentence pairs from clinical notes and have called for a community effort to tackle the Semantic Textual Similarity (BioCreative/OHNLP STS) challenge. We developed models using traditional machine learning and deep learning approaches. For the post challenge, we focus on two models: the Random Forest and the Encoder Network. We applied sentence embeddings pre-trained on PubMed abstracts and MIMIC-III clinical notes and updated the Random Forest and the Encoder Network accordingly. The official results demonstrated our best submission was the ensemble of eight models. It achieved a Person correlation coefficient of 0.8328, the highest performance among 13 submissions from 4 teams. For the post challenge, the performance of both Random Forest and the Encoder Network was improved; in particular, the correlation of the Encoder Network was improved by ~13%. During the challenge task, no end-to-end deep learning models had better performance than machine learning models that take manually-crafted features. In contrast, with the sentence embeddings pre-trained on biomedical corpora, the Encoder Network now achieves a correlation of ~0.84, which is higher than the original best model. The ensembled model taking the improved versions of the Random Forest and Encoder Network as inputs further increased performance to 0.8528. Deep learning models with sentence embeddings pre-trained on biomedical corpora achieve the highest performance on the test set.

</details>

<details>

<summary>2019-09-06 20:14:24 - Learning to Represent Bilingual Dictionaries</summary>

- *Muhao Chen, Yingtao Tian, Haochen Chen, Kai-Wei Chang, Steven Skiena, Carlo Zaniolo*

- `1808.03726v3` - [abs](http://arxiv.org/abs/1808.03726v3) - [pdf](http://arxiv.org/pdf/1808.03726v3)

> Bilingual word embeddings have been widely used to capture the similarity of lexical semantics in different human languages. However, many applications, such as cross-lingual semantic search and question answering, can be largely benefited from the cross-lingual correspondence between sentences and lexicons. To bridge this gap, we propose a neural embedding model that leverages bilingual dictionaries. The proposed model is trained to map the literal word definitions to the cross-lingual target words, for which we explore with different sentence encoding techniques. To enhance the learning process on limited resources, our model adopts several critical learning strategies, including multi-task learning on different bridges of languages, and joint learning of the dictionary model with a bilingual word embedding model. Experimental evaluation focuses on two applications. The results of the cross-lingual reverse dictionary retrieval task show our model's promising ability of comprehending bilingual concepts based on descriptions, and highlight the effectiveness of proposed learning strategies in improving performance. Meanwhile, our model effectively addresses the bilingual paraphrase identification problem and significantly outperforms previous approaches.

</details>

<details>

<summary>2019-09-07 12:26:11 - Unsupervised Image Regression for Heterogeneous Change Detection</summary>

- *Luigi T. Luppino, Filippo M. Bianchi, Gabriele Moser, Stian N. Anfinsen*

- `1909.05948v1` - [abs](http://arxiv.org/abs/1909.05948v1) - [pdf](http://arxiv.org/pdf/1909.05948v1)

> Change detection in heterogeneous multitemporal satellite images is an emerging and challenging topic in remote sensing. In particular, one of the main challenges is to tackle the problem in an unsupervised manner. In this paper we propose an unsupervised framework for bitemporal heterogeneous change detection based on the comparison of affinity matrices and image regression. First, our method quantifies the similarity of affinity matrices computed from co-located image patches in the two images. This is done to automatically identify pixels that are likely to be unchanged. With the identified pixels as pseudo-training data, we learn a transformation to map the first image to the domain of the other image, and vice versa. Four regression methods are selected to carry out the transformation: Gaussian process regression, support vector regression, random forest regression, and a recently proposed kernel regression method called homogeneous pixel transformation. To evaluate the potentials and limitations of our framework, and also the benefits and disadvantages of each regression method, we perform experiments on two real data sets. The results indicate that the comparison of the affinity matrices can already be considered a change detection method by itself. However, image regression is shown to improve the results obtained by the previous step alone and produces accurate change detection maps despite of the heterogeneity of the multitemporal input data. Notably, the random forest regression approach excels by achieving similar accuracy as the other methods, but with a significantly lower computational cost and with fast and robust tuning of hyperparameters.

</details>

<details>

<summary>2019-09-07 15:23:37 - Semantic Role Labeling with Iterative Structure Refinement</summary>

- *Chunchuan Lyu, Shay B. Cohen, Ivan Titov*

- `1909.03285v1` - [abs](http://arxiv.org/abs/1909.03285v1) - [pdf](http://arxiv.org/pdf/1909.03285v1)

> Modern state-of-the-art Semantic Role Labeling (SRL) methods rely on expressive sentence encoders (e.g., multi-layer LSTMs) but tend to model only local (if any) interactions between individual argument labeling decisions. This contrasts with earlier work and also with the intuition that the labels of individual arguments are strongly interdependent. We model interactions between argument labeling decisions through {\it iterative refinement}. Starting with an output produced by a factorized model, we iteratively refine it using a refinement network. Instead of modeling arbitrary interactions among roles and words, we encode prior knowledge about the SRL problem by designing a restricted network architecture capturing non-local interactions. This modeling choice prevents overfitting and results in an effective model, outperforming strong factorized baseline models on all 7 CoNLL-2009 languages, and achieving state-of-the-art results on 5 of them, including English.

</details>

<details>

<summary>2019-09-07 18:24:57 - Relationships from Entity Stream</summary>

- *Martin Andrews, Sam Witteveen*

- `1909.03315v1` - [abs](http://arxiv.org/abs/1909.03315v1) - [pdf](http://arxiv.org/pdf/1909.03315v1)

> Relational reasoning is a central component of intelligent behavior, but has proven difficult for neural networks to learn. The Relation Network (RN) module was recently proposed by DeepMind to solve such problems, and demonstrated state-of-the-art results on a number of datasets. However, the RN module scales quadratically in the size of the input, since it calculates relationship factors between every patch in the visual field, including those that do not correspond to entities. In this paper, we describe an architecture that enables relationships to be determined from a stream of entities obtained by an attention mechanism over the input field. The model is trained end-to-end, and demonstrates equivalent performance with greater interpretability while requiring only a fraction of the model parameters of the original RN module.

</details>

<details>

<summary>2019-09-07 20:04:52 - Adversarial Approximate Inference for Speech to Electroglottograph Conversion</summary>

- *Prathosh A. P., Varun Srivastava, Mayank Mishra*

- `1903.12248v2` - [abs](http://arxiv.org/abs/1903.12248v2) - [pdf](http://arxiv.org/pdf/1903.12248v2)

> Speech produced by human vocal apparatus conveys substantial non-semantic information including the gender of the speaker, voice quality, affective state, abnormalities in the vocal apparatus etc. Such information is attributed to the properties of the voice source signal, which is usually estimated from the speech signal. However, most of the source estimation techniques depend heavily on the goodness of the model assumptions and are prone to noise. A popular alternative is to indirectly obtain the source information through the Electroglottographic (EGG) signal that measures the electrical admittance around the vocal folds using dedicated hardware. In this paper, we address the problem of estimating the EGG signal directly from the speech signal, devoid of any hardware. Sampling from the intractable conditional distribution of the EGG signal given the speech signal is accomplished through optimization of an evidence lower bound. This is constructed via minimization of the KL-divergence between the true and the approximated posteriors of a latent variable learned using a deep neural auto-encoder that serves an informative prior. We demonstrate the efficacy of the method at generating the EGG signal by conducting several experiments on datasets comprising multiple speakers, voice qualities, noise settings and speech pathologies. The proposed method is evaluated on many benchmark metrics and is found to agree with the gold standard while proving better than the state-of-the-art algorithms on a few tasks such as epoch extraction.

</details>

<details>

<summary>2019-09-08 13:40:16 - Multi-Modal Three-Stream Network for Action Recognition</summary>

- *Muhammad Usman Khalid, Jie Yu*

- `1909.03466v1` - [abs](http://arxiv.org/abs/1909.03466v1) - [pdf](http://arxiv.org/pdf/1909.03466v1)

> Human action recognition in video is an active yet challenging research topic due to high variation and complexity of data. In this paper, a novel video based action recognition framework utilizing complementary cues is proposed to handle this complex problem. Inspired by the successful two stream networks for action classification, additional pose features are studied and fused to enhance understanding of human action in a more abstract and semantic way. Towards practices, not only ground truth poses but also noisy estimated poses are incorporated in the framework with our proposed pre-processing module. The whole framework and each cue are evaluated on varied benchmarking datasets as JHMDB, sub-JHMDB and Penn Action. Our results outperform state-of-the-art performance on these datasets and show the strength of complementary cues.

</details>

<details>

<summary>2019-09-08 16:14:31 - Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks</summary>

- *Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, Yang Liu*

- `1909.03496v1` - [abs](http://arxiv.org/abs/1909.03496v1) - [pdf](http://arxiv.org/pdf/1909.03496v1)

> Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68\% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module.

</details>

<details>

<summary>2019-09-09 03:36:42 - Transfer Reward Learning for Policy Gradient-Based Text Generation</summary>

- *James O' Neill, Danushka Bollegala*

- `1909.03622v1` - [abs](http://arxiv.org/abs/1909.03622v1) - [pdf](http://arxiv.org/pdf/1909.03622v1)

> Task-specific scores are often used to optimize for and evaluate the performance of conditional text generation systems. However, such scores are non-differentiable and cannot be used in the standard supervised learning paradigm. Hence, policy gradient methods are used since the gradient can be computed without requiring a differentiable objective.   However, we argue that current n-gram overlap based measures that are used as rewards can be improved by using model-based rewards transferred from tasks that directly compare the similarity of sentence pairs. These reward models either output a score of sentence-level syntactic and semantic similarity between entire predicted and target sentences as the expected return, or for intermediate phrases as segmented accumulative rewards.   We demonstrate that using a \textit{Transferable Reward Learner} leads to improved results on semantical evaluation measures in policy-gradient models for image captioning tasks. Our InferSent actor-critic model improves over a BLEU trained actor-critic model on MSCOCO when evaluated on a Word Mover's Distance similarity measure by 6.97 points, also improving on a Sliding Window Cosine Similarity measure by 10.48 points. Similar performance improvements are also obtained on the smaller Flickr-30k dataset, demonstrating the general applicability of the proposed transfer learning method.

</details>

<details>

<summary>2019-09-09 06:21:27 - SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair</summary>

- *Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, Martin Monperrus*

- `1901.01808v3` - [abs](http://arxiv.org/abs/1901.01808v3) - [pdf](http://arxiv.org/pdf/1901.01808v3)

> This paper presents a novel end-to-end approach to program repair based on sequence-to-sequence learning. We devise, implement, and evaluate a system, called SequenceR, for fixing bugs based on sequence-to-sequence learning on source code. This approach uses the copy mechanism to overcome the unlimited vocabulary problem that occurs with big code. Our system is data-driven; we train it on 35,578 samples, carefully curated from commits to open-source repositories. We evaluate it on 4,711 independent real bug fixes, as well on the Defects4J benchmark used in program repair research. SequenceR is able to perfectly predict the fixed line for 950/4711 testing samples, and find correct patches for 14 bugs in Defects4J. It captures a wide range of repair operators without any domain-specific top-down design.

</details>

<details>

<summary>2019-09-09 07:18:30 - DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing</summary>

- *Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang, Chunhong Pan*

- `1909.03669v1` - [abs](http://arxiv.org/abs/1909.03669v1) - [pdf](http://arxiv.org/pdf/1909.03669v1)

> Point cloud processing is very challenging, as the diverse shapes formed by irregular points are often indistinguishable. A thorough grasp of the elusive shape requires sufficiently contextual semantic information, yet few works devote to this. Here we propose DensePoint, a general architecture to learn densely contextual representation for point cloud processing. Technically, it extends regular grid CNN to irregular point configuration by generalizing a convolution operator, which holds the permutation invariance of points, and achieves efficient inductive learning of local patterns. Architecturally, it finds inspiration from dense connection mode, to repeatedly aggregate multi-level and multi-scale semantics in a deep hierarchy. As a result, densely contextual information along with rich semantics, can be acquired by DensePoint in an organic manner, making it highly effective. Extensive experiments on challenging benchmarks across four tasks, as well as thorough model analysis, verify DensePoint achieves the state of the arts.

</details>

<details>

<summary>2019-09-09 07:49:44 - Table2answer: Read the database and answer without SQL</summary>

- *Tong Guo, Huilin Gao*

- `1902.04260v8` - [abs](http://arxiv.org/abs/1902.04260v8) - [pdf](http://arxiv.org/pdf/1902.04260v8)

> Semantic parsing is the task of mapping natural language to logic form. In question answering, semantic parsing can be used to map the question to logic form and execute the logic form to get the answer. One key problem for semantic parsing is the hard label work. We study this problem in another way: we do not use the logic form any more. Instead we only use the schema and answer info. We think that the logic form step can be injected into the deep model. The reason why we think removing the logic form step is possible is that human can do the task without explicit logic form. We use BERT-based model and do the experiment in the WikiSQL dataset, which is a large natural language to SQL dataset. Our experimental evaluations that show that our model can achieves the baseline results in WikiSQL dataset.

</details>

<details>

<summary>2019-09-09 09:14:39 - Mumford-Shah Loss Functional for Image Segmentation with Deep Learning</summary>

- *Boah Kim, Jong Chul Ye*

- `1904.02872v2` - [abs](http://arxiv.org/abs/1904.02872v2) - [pdf](http://arxiv.org/pdf/1904.02872v2)

> Recent state-of-the-art image segmentation algorithms are mostly based on deep neural networks, thanks to their high performance and fast computation time. However, these methods are usually trained in a supervised manner, which requires large number of high quality ground-truth segmentation masks. On the other hand, classical image segmentation approaches such as level-set methods are formulated in a self-supervised manner by minimizing energy functions such as Mumford-Shah functional, so they are still useful to help generation of segmentation masks without labels. Unfortunately, these algorithms are usually computationally expensive and often have limitation in semantic segmentation. In this paper, we propose a novel loss function based on Mumford-Shah functional that can be used in deep-learning based image segmentation without or with small labeled data. This loss function is based on the observation that the softmax layer of deep neural networks has striking similarity to the characteristic function in the Mumford-Shah functional. We show that the new loss function enables semi-supervised and unsupervised segmentation. In addition, our loss function can be also used as a regularized function to enhance supervised semantic segmentation algorithms. Experimental results on multiple datasets demonstrate the effectiveness of the proposed method.

</details>

<details>

<summary>2019-09-09 11:03:10 - Towards Multimodal Emotion Recognition in German Speech Events in Cars using Transfer Learning</summary>

- *Deniz Cevher, Sebastian Zepf, Roman Klinger*

- `1909.02764v2` - [abs](http://arxiv.org/abs/1909.02764v2) - [pdf](http://arxiv.org/pdf/1909.02764v2)

> The recognition of emotions by humans is a complex process which considers multiple interacting signals such as facial expressions and both prosody and semantic content of utterances. Commonly, research on automatic recognition of emotions is, with few exceptions, limited to one modality. We describe an in-car experiment for emotion recognition from speech interactions for three modalities: the audio signal of a spoken interaction, the visual signal of the driver's face, and the manually transcribed content of utterances of the driver. We use off-the-shelf tools for emotion detection in audio and face and compare that to a neural transfer learning approach for emotion recognition from text which utilizes existing resources from other domains. We see that transfer learning enables models based on out-of-domain corpora to perform well. This method contributes up to 10 percentage points in F1, with up to 76 micro-average F1 across the emotions joy, annoyance and insecurity. Our findings also indicate that off-the-shelf-tools analyzing face and audio are not ready yet for emotion detection in in-car speech interactions without further adjustments.

</details>

<details>

<summary>2019-09-09 11:26:35 - Picture What you Read</summary>

- *Ignazio Gallo, Shah Nawaz, Alessandro Calefati, Riccardo La Grassa, Nicola Landro*

- `1909.05663v1` - [abs](http://arxiv.org/abs/1909.05663v1) - [pdf](http://arxiv.org/pdf/1909.05663v1)

> Visualization refers to our ability to create an image in our head based on the text we read or the words we hear. It is one of the many skills that makes reading comprehension possible. Convolutional Neural Networks (CNN) are an excellent tool for recognizing and classifying text documents. In addition, it can generate images conditioned on natural language. In this work, we utilize CNNs capabilities to generate realistic images representative of the text illustrating the semantic concept. We conducted various experiments to highlight the capacity of the proposed model to generate representative images of the text descriptions used as input to the proposed model.

</details>

<details>

<summary>2019-09-09 14:37:15 - Dynamic Filtering with Large Sampling Field for ConvNets</summary>

- *Jialin Wu, Dai Li, Yu Yang, Chandrajit Bajaj, Xiangyang Ji*

- `1803.07624v3` - [abs](http://arxiv.org/abs/1803.07624v3) - [pdf](http://arxiv.org/pdf/1803.07624v3)

> We propose a dynamic filtering strategy with large sampling field for ConvNets (LS-DFN), where the position-specific kernels learn from not only the identical position but also multiple sampled neighbor regions. During sampling, residual learning is introduced to ease training and an attention mechanism is applied to fuse features from different samples. Such multiple samples enlarge the kernels' receptive fields significantly without requiring more parameters. While LS-DFN inherits the advantages of DFN, namely avoiding feature map blurring by position-wise kernels while keeping translation invariance, it also efficiently alleviates the overfitting issue caused by much more parameters than normal CNNs. Our model is efficient and can be trained end-to-end via standard back-propagation. We demonstrate the merits of our LS-DFN on both sparse and dense prediction tasks involving object detection, semantic segmentation, and flow estimation. Our results show LS-DFN enjoys stronger recognition abilities in object detection and semantic segmentation tasks on VOC benchmark and sharper responses in flow estimation on FlyingChairs dataset compared to strong baselines.

</details>

<details>

<summary>2019-09-09 17:56:33 - Multimodal Word Distributions</summary>

- *Ben Athiwaratkun, Andrew Gordon Wilson*

- `1704.08424v2` - [abs](http://arxiv.org/abs/1704.08424v2) - [pdf](http://arxiv.org/pdf/1704.08424v2)

> Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.

</details>

<details>

<summary>2019-09-09 19:04:21 - Adversarial Policy Gradient for Deep Learning Image Augmentation</summary>

- *Kaiyang Cheng, Claudia Iriondo, Francesco Calivá, Justin Krogue, Sharmila Majumdar, Valentina Pedoia*

- `1909.04108v1` - [abs](http://arxiv.org/abs/1909.04108v1) - [pdf](http://arxiv.org/pdf/1909.04108v1)

> The use of semantic segmentation for masking and cropping input images has proven to be a significant aid in medical imaging classification tasks by decreasing the noise and variance of the training dataset. However, implementing this approach with classical methods is challenging: the cost of obtaining a dense segmentation is high, and the precise input area that is most crucial to the classification task is difficult to determine a-priori. We propose a novel joint-training deep reinforcement learning framework for image augmentation. A segmentation network, weakly supervised with policy gradient optimization, acts as an agent, and outputs masks as actions given samples as states, with the goal of maximizing reward signals from the classification network. In this way, the segmentation network learns to mask unimportant imaging features. Our method, Adversarial Policy Gradient Augmentation (APGA), shows promising results on Stanford's MURA dataset and on a hip fracture classification task with an increase in global accuracy of up to 7.33% and improved performance over baseline methods in 9/10 tasks evaluated. We discuss the broad applicability of our joint training strategy to a variety of medical imaging tasks.

</details>

<details>

<summary>2019-09-09 19:29:17 - General Fragment Model for Information Artifacts</summary>

- *Sandro Rama Fiorini, Wallas Sousa dos Santos, Rodrigo Costa Mesquita, Guilherme Ferreira Lima, Marcio F. Moreno*

- `1909.04117v1` - [abs](http://arxiv.org/abs/1909.04117v1) - [pdf](http://arxiv.org/pdf/1909.04117v1)

> The use of semantic descriptions in data intensive domains require a systematic model for linking semantic descriptions with their manifestations in fragments of heterogeneous information and data objects. Such information heterogeneity requires a fragment model that is general enough to support the specification of anchors from conceptual models to multiple types of information artifacts. While diverse proposals of anchoring models exist in the literature, they are usually focused in audiovisual information. We propose a generalized fragment model that can be instantiated to different kinds of information artifacts. Our objective is to systematize the way in which fragments and anchors can be described in conceptual models, without committing to a specific vocabulary.

</details>

<details>

<summary>2019-09-09 20:01:51 - Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?</summary>

- *Lyan Verwimp, Jerome R. Bellegarda*

- `1909.04130v1` - [abs](http://arxiv.org/abs/1909.04130v1) - [pdf](http://arxiv.org/pdf/1909.04130v1)

> Natural language processing (NLP) tasks tend to suffer from a paucity of suitably annotated training data, hence the recent success of transfer learning across a wide variety of them. The typical recipe involves: (i) training a deep, possibly bidirectional, neural network with an objective related to language modeling, for which training data is plentiful; and (ii) using the trained network to derive contextual representations that are far richer than standard linear word embeddings such as word2vec, and thus result in important gains. In this work, we wonder whether the opposite perspective is also true: can contextual representations trained for different NLP tasks improve language modeling itself? Since language models (LMs) are predominantly locally optimized, other NLP tasks may help them make better predictions based on the entire semantic fabric of a document. We test the performance of several types of pre-trained embeddings in neural LMs, and we investigate whether it is possible to make the LM more aware of global semantic information through embeddings pre-trained with a domain classification model. Initial experiments suggest that as long as the proper objective criterion is used during training, pre-trained embeddings are likely to be beneficial for neural language modeling.

</details>

<details>

<summary>2019-09-09 20:34:32 - Detection and Classification of Breast Cancer Metastates Based on U-Net</summary>

- *Lin Xu, Cheng Xu, Yi Tong, Yu Chun Su*

- `1909.04141v1` - [abs](http://arxiv.org/abs/1909.04141v1) - [pdf](http://arxiv.org/pdf/1909.04141v1)

> This paper presents U-net based breast cancer metastases detection and classification in lymph nodes, as well as patient-level classification based on metastases detection. The whole pipeline can be divided into five steps: preprocessing and data argumentation, patch-based segmentation, post processing, slide-level classification, and patient-level classification. In order to reduce overfitting and speedup convergence, we applied batch normalization and dropout into U-Net. The final Kappa score reaches 0.902 on training data.

</details>

<details>

<summary>2019-09-09 21:20:36 - Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs</summary>

- *Bailin Wang, Ivan Titov, Mirella Lapata*

- `1909.04165v1` - [abs](http://arxiv.org/abs/1909.04165v1) - [pdf](http://arxiv.org/pdf/1909.04165v1)

> Semantic parsing aims to map natural language utterances onto machine interpretable meaning representations, aka programs whose execution against a real-world environment produces a denotation. Weakly-supervised semantic parsers are trained on utterance-denotation pairs treating programs as latent. The task is challenging due to the large search space and spuriousness of programs which may execute to the correct answer but do not generalize to unseen examples. Our goal is to instill an inductive bias in the parser to help it distinguish between spurious and correct programs. We capitalize on the intuition that correct programs would likely respect certain structural constraints were they to be aligned to the question (e.g., program fragments are unlikely to align to overlapping text spans) and propose to model alignments as structured latent variables. In order to make the latent-alignment framework tractable, we decompose the parsing task into (1) predicting a partial "abstract program" and (2) refining it while modeling structured alignments with differential dynamic programming. We obtain state-of-the-art performance on the WIKITABLEQUESTIONS and WIKISQL datasets. When compared to a standard attention baseline, we observe that the proposed structured-alignment mechanism is highly beneficial.

</details>

<details>

<summary>2019-09-10 00:29:37 - Syntax-aware Multilingual Semantic Role Labeling</summary>

- *Shexia He, Zuchao Li, Hai Zhao*

- `1909.00310v3` - [abs](http://arxiv.org/abs/1909.00310v3) - [pdf](http://arxiv.org/pdf/1909.00310v3)

> Recently, semantic role labeling (SRL) has earned a series of success with even higher performance improvements, which can be mainly attributed to syntactic integration and enhanced word representation. However, most of these efforts focus on English, while SRL on multiple languages more than English has received relatively little attention so that is kept underdevelopment. Thus this paper intends to fill the gap on multilingual SRL with special focus on the impact of syntax and contextualized word representation. Unlike existing work, we propose a novel method guided by syntactic rule to prune arguments, which enables us to integrate syntax into multilingual SRL model simply and effectively. We present a unified SRL model designed for multiple languages together with the proposed uniform syntax enhancement. Our model achieves new state-of-the-art results on the CoNLL-2009 benchmarks of all seven languages. Besides, we pose a discussion on the syntactic role among different languages and verify the effectiveness of deep enhanced representation for multilingual SRL.

</details>

<details>

<summary>2019-09-10 01:44:21 - From Pixels to Buildings: End-to-end Probabilistic Deep Networks for Large-scale Semantic Mapping</summary>

- *Kaiyu Zheng, Andrzej Pronobis*

- `1812.11866v8` - [abs](http://arxiv.org/abs/1812.11866v8) - [pdf](http://arxiv.org/pdf/1812.11866v8)

> We introduce TopoNets, end-to-end probabilistic deep networks for modeling semantic maps with structure reflecting the topology of large-scale environments. TopoNets build a unified deep network spanning multiple levels of abstraction and spatial scales, from pixels representing geometry of local places to high-level descriptions of semantics of buildings. To this end, TopoNets leverage complex spatial relations expressed in terms of arbitrary, dynamic graphs. We demonstrate how TopoNets can be used to perform end-to-end semantic mapping from partial sensory observations and noisy topological relations discovered by a robot exploring large-scale office spaces. Thanks to their probabilistic nature and generative properties, TopoNets extend the problem of semantic mapping beyond classification. We show that TopoNets successfully perform uncertain reasoning about yet unexplored space and detect novel and incongruent environment configurations unknown to the robot. Our implementation of TopoNets achieves real-time, tractable and exact inference, which makes these new deep models a promising, practical solution to mobile robot spatial understanding at scale.

</details>

<details>

<summary>2019-09-10 01:55:22 - Unsupervised Paraphrasing by Simulated Annealing</summary>

- *Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song*

- `1909.03588v2` - [abs](http://arxiv.org/abs/1909.03588v2) - [pdf](http://arxiv.org/pdf/1909.03588v2)

> Unsupervised paraphrase generation is a promising and important research topic in natural language processing. We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases. Then, UPSA searches the sentence space towards this objective by performing a sequence of local editing. Our method is unsupervised and does not require parallel corpora for training, so it could be easily applied to different domains. We evaluate our approach on a variety of benchmark datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter. Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations. Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA.

</details>

<details>

<summary>2019-09-10 05:46:39 - Multimodal Embeddings from Language Models</summary>

- *Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan*

- `1909.04302v1` - [abs](http://arxiv.org/abs/1909.04302v1) - [pdf](http://arxiv.org/pdf/1909.04302v1)

> Word embeddings such as ELMo have recently been shown to model word semantics with greater efficacy through contextualized learning on large-scale language corpora, resulting in significant improvement in state of the art across many natural language tasks. In this work we integrate acoustic information into contextualized lexical embeddings through the addition of multimodal inputs to a pretrained bidirectional language model. The language model is trained on spoken language that includes text and audio modalities. The resulting representations from this model are multimodal and contain paralinguistic information which can modify word meanings and provide affective information. We show that these multimodal embeddings can be used to improve over previous state of the art multimodal models in emotion recognition on the CMU-MOSEI dataset.

</details>

<details>

<summary>2019-09-10 06:02:15 - Bayesian Relational Memory for Semantic Visual Navigation</summary>

- *Yi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari, Yuandong Tian*

- `1909.04306v1` - [abs](http://arxiv.org/abs/1909.04306v1) - [pdf](http://arxiv.org/pdf/1909.04306v1)

> We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards. BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We develop a BRM agent consisting of a BRM module for producing sub-goals and a goal-conditioned locomotion module for control. When testing in unseen environments, the BRM agent outperforms baselines that do not explicitly utilize the probabilistic relational memory structure

</details>

<details>

<summary>2019-09-10 06:17:06 - Learning to Disentangle Robust and Vulnerable Features for Adversarial Detection</summary>

- *Byunggill Joe, Sung Ju Hwang, Insik Shin*

- `1909.04311v1` - [abs](http://arxiv.org/abs/1909.04311v1) - [pdf](http://arxiv.org/pdf/1909.04311v1)

> Although deep neural networks have shown promising performances on various tasks, even achieving human-level performance on some, they are shown to be susceptible to incorrect predictions even with imperceptibly small perturbations to an input. There exists a large number of previous works which proposed to defend against such adversarial attacks either by robust inference or detection of adversarial inputs. Yet, most of them cannot effectively defend against whitebox attacks where an adversary has a knowledge of the model and defense. More importantly, they do not provide a convincing reason why the generated adversarial inputs successfully fool the target models. To address these shortcomings of the existing approaches, we hypothesize that the adversarial inputs are tied to latent features that are susceptible to adversarial perturbation, which we call vulnerable features. Then based on this intuition, we propose a minimax game formulation to disentangle the latent features of each instance into robust and vulnerable ones, using variational autoencoders with two latent spaces. We thoroughly validate our model for both blackbox and whitebox attacks on MNIST, Fashion MNIST5, and Cat & Dog datasets, whose results show that the adversarial inputs cannot bypass our detector without changing its semantics, in which case the attack has failed.

</details>

<details>

<summary>2019-09-10 07:37:06 - Increasing the Generalisation Capacity of Conditional VAEs</summary>

- *Alexej Klushyn, Nutan Chen, Botond Cseke, Justin Bayer, Patrick van der Smagt*

- `1908.08750v2` - [abs](http://arxiv.org/abs/1908.08750v2) - [pdf](http://arxiv.org/pdf/1908.08750v2)

> We address the problem of one-to-many mappings in supervised learning, where a single instance has many different solutions of possibly equal cost. The framework of conditional variational autoencoders describes a class of methods to tackle such structured-prediction tasks by means of latent variables. We propose to incentivise informative latent representations for increasing the generalisation capacity of conditional variational autoencoders. To this end, we modify the latent variable model by defining the likelihood as a function of the latent variable only and introduce an expressive multimodal prior to enable the model for capturing semantically meaningful features of the data. To validate our approach, we train our model on the Cornell Robot Grasping dataset, and modified versions of MNIST and Fashion-MNIST obtaining results that show a significantly higher generalisation capability.

</details>

<details>

<summary>2019-09-10 10:12:01 - Attesting Biases and Discrimination using Language Semantics</summary>

- *Xavier Ferrer Aran, Jose M. Such, Natalia Criado*

- `1909.04386v1` - [abs](http://arxiv.org/abs/1909.04386v1) - [pdf](http://arxiv.org/pdf/1909.04386v1)

> AI agents are increasingly deployed and used to make automated decisions that affect our lives on a daily basis. It is imperative to ensure that these systems embed ethical principles and respect human values. We focus on how we can attest to whether AI agents treat users fairly without discriminating against particular individuals or groups through biases in language. In particular, we discuss human unconscious biases, how they are embedded in language, and how AI systems inherit those biases by learning from and processing human language. Then, we outline a roadmap for future research to better understand and attest problematic AI biases derived from language.

</details>

<details>

<summary>2019-09-10 10:23:24 - Verifying MITL formulae on Timed Automata considering a Continuous Time Semantics</summary>

- *Claudio Menghi, Marcello Bersani, Matteo Rossi, Pierluigi San Pietro*

- `1806.08684v2` - [abs](http://arxiv.org/abs/1806.08684v2) - [pdf](http://arxiv.org/pdf/1806.08684v2)

> Timed Automata (TA) is de facto a standard modelling formalism to represent systems when the interest is the analysis of their behaviour as time progresses. This modelling formalism is mostly used for checking whether the behaviours of a system satisfy a set of properties of interest. Even if efficient model-checkers for Timed Automata exist, these tools are not easily configurable. First, they are not designed to easily allow adding new Timed Automata constructs, such as new synchronization mechanisms or communication procedures, but they assume a fixed set of Timed Automata constructs. Second, they usually do not support the full Metric Interval Temporal Logic (MITL) and rely on a precise semantics for the logic in which the property of interest is specified which cannot be easily modified and customized. Finally, they do not easily allow using different solvers that may speed up verification in different contexts. This paper presents a novel technique to perform model checking of full Metric Interval Temporal Logic (MITL) properties on TA. The technique relies on the translation of both the TA and the MITL formula into an intermediate Constraint LTL over clocks (CLTLoc) formula which is verified through an available decision procedure. The technique is flexible since the intermediate logic allows the encoding of new semantics as well as new TA constructs, by just adding new CLTLoc formulae. Furthermore, our technique is not bound to a specific solver as the intermediate CLTLoc formula can be verified using different procedures.

</details>

<details>

<summary>2019-09-10 10:23:49 - Neural Attentive Bag-of-Entities Model for Text Classification</summary>

- *Ikuya Yamada, Hiroyuki Shindo*

- `1909.01259v2` - [abs](http://arxiv.org/abs/1909.01259v2) - [pdf](http://arxiv.org/pdf/1909.01259v2)

> This study proposes a Neural Attentive Bag-of-Entities model, which is a neural network model that performs text classification using entities in a knowledge base. Entities provide unambiguous and relevant semantic signals that are beneficial for capturing semantics in texts. We combine simple high-recall entity detection based on a dictionary, to detect entities in a document, with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities. We tested the effectiveness of our model using two standard text classification datasets (i.e., the 20 Newsgroups and R8 datasets) and a popular factoid question answering dataset based on a trivia quiz game. As a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec.

</details>

<details>

<summary>2019-09-10 11:14:39 - Compositional Liveness-Preserving Conformance Testing of Timed I/O Automata -- Technical Report</summary>

- *Lars Luthmann, Hendrik Göttmann, Malte Lochau*

- `1909.03703v2` - [abs](http://arxiv.org/abs/1909.03703v2) - [pdf](http://arxiv.org/pdf/1909.03703v2)

> I/O conformance testing theories (e.g., ioco) are concerned with formally defining when observable output behaviors of an implementation conform to those permitted by a specification. Thereupon, several real-time extensions of ioco, usually called tioco, have been proposed, further taking into account permitted delays between actions. In this paper, we propose an improved version of tioco, called live timed ioco (ltioco), tackling various weaknesses of existing definitions. Here, a reasonable adaptation of quiescence (i.e., observable absence of any outputs) to real-time behaviors has to be done with care: ltioco therefore distinguishes safe outputs being allowed to happen, from live outputs being enforced to happen within a certain time period thus inducing two different facets of quiescence. Furthermore, tioco is frequently defined on Timed I/O Labeled Transition Systems (TIOLTS), a semantic model of Timed I/O Automata (TIOA) which is infinitely branching and thus infeasible for practical testing tools. Instead, we extend the theory of zone graphs to enable ltioco testing on a finite semantic model of TIOA. Finally, we investigate compositionality of ltioco with respect to parallel composition including a proper treatment of silent transitions.

</details>

<details>

<summary>2019-09-10 12:54:57 - Extracting and Learning a Dependency-Enhanced Type Lexicon for Dutch</summary>

- *Konstantinos Kogkalidis*

- `1909.02955v2` - [abs](http://arxiv.org/abs/1909.02955v2) - [pdf](http://arxiv.org/pdf/1909.02955v2)

> This thesis is concerned with type-logical grammars and their practical applicability as tools of reasoning about sentence syntax and semantics. The focal point is narrowed to Dutch, a language exhibiting a large degree of word order variability. In order to overcome difficulties arising as a result of that variability, the thesis explores and expands upon a type grammar based on Multiplicative Intuitionistic Linear Logic, agnostic to word order but enriched with decorations that aim to reduce its proof-theoretic complexity. An algorithm for the conversion of dependency-annotated sentences into type sequences is then implemented, populating the type logic with concrete, data-driven lexical types. Two experiments are ran on the resulting grammar instantiation. The first pertains to the learnability of the type-assignment process by a neural architecture. A novel application of a self-attentive sequence transduction model is proposed; contrary to established practices, it constructs types inductively by internalizing the type-formation syntax, thus exhibiting generalizability beyond a pre-specified type vocabulary. The second revolves around a deductive parsing system that can resolve structural ambiguities by consulting both word and type information; preliminary results suggest both excellent computational efficiency and performance.

</details>

<details>

<summary>2019-09-10 13:01:27 - Learning review representations from user and product level information for spam detection</summary>

- *Chunyuan Yuan, Wei Zhou, Qianwen Ma, Shangwen Lv, Jizhong Han, Songlin Hu*

- `1909.04455v1` - [abs](http://arxiv.org/abs/1909.04455v1) - [pdf](http://arxiv.org/pdf/1909.04455v1)

> Opinion spam has become a widespread problem in social media, where hired spammers write deceptive reviews to promote or demote products to mislead the consumers for profit or fame. Existing works mainly focus on manually designing discrete textual or behavior features, which cannot capture complex semantics of reviews. Although recent works apply deep learning methods to learn review-level semantic features, their models ignore the impact of the user-level and product-level information on learning review semantics and the inherent user-review-product relationship information. In this paper, we propose a Hierarchical Fusion Attention Network (HFAN) to automatically learn the semantics of reviews from the user and product level. Specifically, we design a multi-attention unit to extract user(product)-related review information. Then, we use orthogonal decomposition and fusion attention to learn a user, review, and product representation from the review information. Finally, we take the review as a relation between user and product entity and apply TransH to jointly encode this relationship into review representation. Experimental results obtained more than 10\% absolute precision improvement over the state-of-the-art performances on four real-world datasets, which show the effectiveness and versatility of the model.

</details>

<details>

<summary>2019-09-10 13:39:52 - Hierarchy Parsing for Image Captioning</summary>

- *Ting Yao, Yingwei Pan, Yehao Li, Tao Mei*

- `1909.03918v2` - [abs](http://arxiv.org/abs/1909.03918v2) - [pdf](http://arxiv.org/pdf/1909.03918v2)

> It is always well believed that parsing an image into constituent visual patterns would be helpful for understanding and representing an image. Nevertheless, there has not been evidence in support of the idea on describing an image with a natural-language utterance. In this paper, we introduce a new design to model a hierarchy from instance level (segmentation), region level (detection) to the whole image to delve into a thorough image understanding for captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture that novelly integrates hierarchical structure into image encoder. Technically, an image decomposes into a set of regions and some of the regions are resolved into finer ones. Each region then regresses to an instance, i.e., foreground of the region. Such process naturally builds a hierarchal tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the hierarchal structure and enhance all the instance-level, region-level and image-level features. Our HIP is appealing in view that it is pluggable to any neural captioning models. Extensive experiments on COCO image captioning dataset demonstrate the superiority of HIP. More remarkably, HIP plus a top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1% to 127.2% on COCO Karpathy test split. When further endowing instance-level and region-level features from HIP with semantic relation learnt through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.

</details>

<details>

<summary>2019-09-10 14:05:28 - Countering Language Drift via Visual Grounding</summary>

- *Jason Lee, Kyunghyun Cho, Douwe Kiela*

- `1909.04499v1` - [abs](http://arxiv.org/abs/1909.04499v1) - [pdf](http://arxiv.org/pdf/1909.04499v1)

> Emergent multi-agent communication protocols are very different from natural language and not easily interpretable by humans. We find that agents that were initially pretrained to produce natural language can also experience detrimental language drift: when a non-linguistic reward is used in a goal-based task, e.g. some scalar success metric, the communication protocol may easily and radically diverge from natural language. We recast translation as a multi-agent communication game and examine auxiliary training constraints for their effectiveness in mitigating language drift. We show that a combination of syntactic (language model likelihood) and semantic (visual grounding) constraints gives the best communication performance, allowing pre-trained agents to retain English syntax while learning to accurately convey the intended meaning.

</details>

<details>

<summary>2019-09-10 18:39:26 - Neural Embedding Allocation: Distributed Representations of Topic Models</summary>

- *Kamrun Naher Keya, Yannis Papanikolaou, James R. Foulds*

- `1909.04702v1` - [abs](http://arxiv.org/abs/1909.04702v1) - [pdf](http://arxiv.org/pdf/1909.04702v1)

> Word embedding models such as the skip-gram learn vector representations of words' semantic relationships, and document embedding models learn similar representations for documents. On the other hand, topic models provide latent representations of the documents' topical themes. To get the benefits of these representations simultaneously, we propose a unifying algorithm, called neural embedding allocation (NEA), which deconstructs topic models into interpretable vector-space embeddings of words, topics, documents, authors, and so on, by learning neural embeddings to mimic the topic models. We showcase NEA's effectiveness and generality on LDA, author-topic models and the recently proposed mixed membership skip gram topic model and achieve better performance with the embeddings compared to several state-of-the-art models. Furthermore, we demonstrate that using NEA to smooth out the topics improves coherence scores over the original topic models when the number of topics is large.

</details>

<details>

<summary>2019-09-10 20:41:57 - End-to-End Boundary Aware Networks for Medical Image Segmentation</summary>

- *Ali Hatamizadeh, Demetri Terzopoulos, Andriy Myronenko*

- `1908.08071v2` - [abs](http://arxiv.org/abs/1908.08071v2) - [pdf](http://arxiv.org/pdf/1908.08071v2)

> Fully convolutional neural networks (CNNs) have proven to be effective at representing and classifying textural information, thus transforming image intensity into output class masks that achieve semantic image segmentation. In medical image analysis, however, expert manual segmentation often relies on the boundaries of anatomical structures of interest. We propose boundary aware CNNs for medical image segmentation. Our networks are designed to account for organ boundary information, both by providing a special network edge branch and edge-aware loss terms, and they are trainable end-to-end. We validate their effectiveness on the task of brain tumor segmentation using the BraTS 2018 dataset. Our experiments reveal that our approach yields more accurate segmentation results, which makes it promising for more extensive application to medical image segmentation.

</details>

<details>

<summary>2019-09-10 23:30:26 - Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization</summary>

- *Siyao Li, Deren Lei, Pengda Qin, William Yang Wang*

- `1909.00141v2` - [abs](http://arxiv.org/abs/1909.00141v2) - [pdf](http://arxiv.org/pdf/1909.00141v2)

> Deep reinforcement learning (RL) has been a commonly-used strategy for the abstractive summarization task to address both the exposure bias and non-differentiable task issues. However, the conventional reward Rouge-L simply looks for exact n-grams matches between candidates and annotated references, which inevitably makes the generated sentences repetitive and incoherent. In this paper, instead of Rouge-L, we explore the practicability of utilizing the distributional semantics to measure the matching degrees. With distributional semantics, sentence-level evaluation can be obtained, and semantically-correct phrases can also be generated without being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language.

</details>

<details>

<summary>2019-09-11 06:43:01 - A Comparative Study of Some Central Notions of ASPIC+ and DeLP</summary>

- *Alejandro J. Garcia, Henry Prakken, Guillermo R. Simari*

- `1909.02810v2` - [abs](http://arxiv.org/abs/1909.02810v2) - [pdf](http://arxiv.org/pdf/1909.02810v2)

> This paper formally compares some central notions from two well-known formalisms for rule-based argumentation, DeLP and ASPIC+. The comparisons especially focus on intuitive adequacy and inter-translatability, consistency, and closure properties. As for differences in the definitions of arguments and attack, it turns out that DeLP's definitions are intuitively appealing but that they may not fully comply with Caminada and Amgoud's rationality postulates of strict closure and indirect consistency. For some special cases, the DeLP definitions are shown to fare better than ASPIC+. Next, it is argued that there are reasons to consider a variant of DeLP with grounded semantics, since in some examples its current notion of warrant arguably has counterintuitive consequences and may lead to sets of warranted arguments that are not admissible. Finally, under some minimality and consistency assumptions on ASPIC+ arguments, a one-to-many correspondence between ASPIC+ arguments and DeLP arguments is identified in such a way that if the DeLP warranting procedure is changed to grounded semantics, then DeLP notion of warrant and ASPIC+'s notion of justification are equivalent. This result is proven for three alternative definitions of attack.

</details>

<details>

<summary>2019-09-11 08:31:08 - Unsupervised speech representation learning using WaveNet autoencoders</summary>

- *Jan Chorowski, Ron J. Weiss, Samy Bengio, Aäron van den Oord*

- `1901.08810v2` - [abs](http://arxiv.org/abs/1901.08810v2) - [pdf](http://arxiv.org/pdf/1901.08810v2)

> We consider the task of unsupervised extraction of meaningful latent representations of speech by applying autoencoding neural networks to speech waveforms. The goal is to learn a representation able to capture high level semantic content from the signal, e.g.\ phoneme identities, while being invariant to confounding low level details in the signal such as the underlying pitch contour or background noise. Since the learned representation is tuned to contain only phonetic content, we resort to using a high capacity WaveNet decoder to infer information discarded by the encoder from previous samples. Moreover, the behavior of autoencoder models depends on the kind of constraint that is applied to the latent representation. We compare three variants: a simple dimensionality reduction bottleneck, a Gaussian Variational Autoencoder (VAE), and a discrete Vector Quantized VAE (VQ-VAE). We analyze the quality of learned representations in terms of speaker independence, the ability to predict phonetic content, and the ability to accurately reconstruct individual spectrogram frames. Moreover, for discrete encodings extracted using the VQ-VAE, we measure the ease of mapping them to phonemes. We introduce a regularization scheme that forces the representations to focus on the phonetic content of the utterance and report performance comparable with the top entries in the ZeroSpeech 2017 unsupervised acoustic unit discovery task.

</details>

<details>

<summary>2019-09-11 08:55:09 - How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations</summary>

- *Betty van Aken, Benjamin Winter, Alexander Löser, Felix A. Gers*

- `1909.04925v1` - [abs](http://arxiv.org/abs/1909.04925v1) - [pdf](http://arxiv.org/pdf/1909.04925v1)

> Bidirectional Encoder Representations from Transformers (BERT) reach state-of-the-art results in a variety of Natural Language Processing tasks. However, understanding of their internal functioning is still insufficient and unsatisfactory. In order to better understand BERT and other Transformer-based models, we present a layer-wise analysis of BERT's hidden states. Unlike previous research, which mainly focuses on explaining Transformer models by their attention weights, we argue that hidden states contain equally valuable information. Specifically, our analysis focuses on models fine-tuned on the task of Question Answering (QA) as an example of a complex downstream task. We inspect how QA models transform token vectors in order to find the correct answer. To this end, we apply a set of general and QA-specific probing tasks that reveal the information stored in each representation layer. Our qualitative analysis of hidden state visualizations provides additional insights into BERT's reasoning process. Our results show that the transformations within BERT go through phases that are related to traditional pipeline tasks. The system can therefore implicitly incorporate task-specific information into its token representations. Furthermore, our analysis reveals that fine-tuning has little impact on the models' semantic abilities and that prediction errors can be recognized in the vector representations of even early layers.

</details>

<details>

<summary>2019-09-11 10:03:54 - Convolutional Analysis Operator Learning: Acceleration and Convergence</summary>

- *Il Yong Chun, Jeffrey A. Fessler*

- `1802.05584v7` - [abs](http://arxiv.org/abs/1802.05584v7) - [pdf](http://arxiv.org/pdf/1802.05584v7)

> Convolutional operator learning is gaining attention in many signal processing and computer vision applications. Learning kernels has mostly relied on so-called patch-domain approaches that extract and store many overlapping patches across training signals. Due to memory demands, patch-domain methods have limitations when learning kernels from large datasets -- particularly with multi-layered structures, e.g., convolutional neural networks -- or when applying the learned kernels to high-dimensional signal recovery problems. The so-called convolution approach does not store many overlapping patches, and thus overcomes the memory problems particularly with careful algorithmic designs; it has been studied within the "synthesis" signal model, e.g., convolutional dictionary learning. This paper proposes a new convolutional analysis operator learning (CAOL) framework that learns an analysis sparsifying regularizer with the convolution perspective, and develops a new convergent Block Proximal Extrapolated Gradient method using a Majorizer (BPEG-M) to solve the corresponding block multi-nonconvex problems. To learn diverse filters within the CAOL framework, this paper introduces an orthogonality constraint that enforces a tight-frame filter condition, and a regularizer that promotes diversity between filters. Numerical experiments show that, with sharp majorizers, BPEG-M significantly accelerates the CAOL convergence rate compared to the state-of-the-art block proximal gradient (BPG) method. Numerical experiments for sparse-view computational tomography show that a convolutional sparsifying regularizer learned via CAOL significantly improves reconstruction quality compared to a conventional edge-preserving regularizer. Using more and wider kernels in a learned regularizer better preserves edges in reconstructed images.

</details>

<details>

<summary>2019-09-11 12:42:08 - Jointly embedding the local and global relations of heterogeneous graph for rumor detection</summary>

- *Chunyuan Yuan, Qianwen Ma, Wei Zhou, Jizhong Han, Songlin Hu*

- `1909.04465v2` - [abs](http://arxiv.org/abs/1909.04465v2) - [pdf](http://arxiv.org/pdf/1909.04465v2)

> The development of social media has revolutionized the way people communicate, share information and make decisions, but it also provides an ideal platform for publishing and spreading rumors. Existing rumor detection methods focus on finding clues from text content, user profiles, and propagation patterns. However, the local semantic relation and global structural information in the message propagation graph have not been well utilized by previous works.   In this paper, we present a novel global-local attention network (GLAN) for rumor detection, which jointly encodes the local semantic and global structural information. We first generate a better integrated representation for each source tweet by fusing the semantic information of related retweets with the attention mechanism. Then, we model the global relationships among all source tweets, retweets, and users as a heterogeneous graph to capture the rich structural information for rumor detection. We conduct experiments on three real-world datasets, and the results demonstrate that GLAN significantly outperforms the state-of-the-art models in both rumor detection and early detection scenarios.

</details>

<details>

<summary>2019-09-11 19:52:54 - VideoBERT: A Joint Model for Video and Language Representation Learning</summary>

- *Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid*

- `1904.01766v2` - [abs](http://arxiv.org/abs/1904.01766v2) - [pdf](http://arxiv.org/pdf/1904.01766v2)

> Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.

</details>

<details>

<summary>2019-09-12 02:47:47 - Neural Semantic Parsing in Low-Resource Settings with Back-Translation and Meta-Learning</summary>

- *Yibo Sun, Duyu Tang, Nan Duan, Yeyun Gong, Xiaocheng Feng, Bing Qin, Daxin Jiang*

- `1909.05438v1` - [abs](http://arxiv.org/abs/1909.05438v1) - [pdf](http://arxiv.org/pdf/1909.05438v1)

> Neural semantic parsing has achieved impressive results in recent years, yet its success relies on the availability of large amounts of supervised data. Our goal is to learn a neural semantic parser when only prior knowledge about a limited number of simple rules is available, without access to either annotated programs or execution results. Our approach is initialized by rules, and improved in a back-translation paradigm using generated question-program pairs from the semantic parser and the question generator. A phrase table with frequent mapping patterns is automatically derived, also updated as training progresses, to measure the quality of generated instances. We train the model with model-agnostic meta-learning to guarantee the accuracy and stability on examples covered by rules, and meanwhile acquire the versatility to generalize well on examples uncovered by rules. Results on three benchmark datasets with different domains and programs show that our approach incrementally improves the accuracy. On WikiSQL, our best model is comparable to the SOTA system learned from denotations.

</details>

<details>

<summary>2019-09-12 04:21:41 - Visualizing Trends of Key Roles in News Articles</summary>

- *Chen Xia, Haoxiang Zhang, Jacob Moghtader, Allen Wu, Kai-Wei Chang*

- `1909.05449v1` - [abs](http://arxiv.org/abs/1909.05449v1) - [pdf](http://arxiv.org/pdf/1909.05449v1)

> There are tons of news articles generated every day reflecting the activities of key roles such as people, organizations and political parties. Analyzing these key roles allows us to understand the trends in news. In this paper, we present a demonstration system that visualizes the trend of key roles in news articles based on natural language processing techniques. Specifically, we apply a semantic role labeler and the dynamic word embedding technique to understand relationships between key roles in the news across different time periods and visualize the trends of key role and news topics change over time.

</details>

<details>

<summary>2019-09-12 06:21:55 - Catalog of Formalized Application Integration Patterns</summary>

- *Daniel Ritter, Stefanie Rinderle-Ma, Marco Montali, Andrey Rivkin, Aman Sinha*

- `1807.03197v3` - [abs](http://arxiv.org/abs/1807.03197v3) - [pdf](http://arxiv.org/pdf/1807.03197v3)

> Enterprise application integration (EAI) solutions are the centrepiece of current enterprise IT architectures (e.g., cloud and mobile computing, business networks), however, require the formalization of their building blocks, represented by integration patterns, verification and optimization. This work serves as an instructive pattern formalization catalog that leads to the formalization of all currently known integration patterns. Therefore, we explain the classification of the underlying requirements of the pattern semantics and formalize representative patterns from the different categories, by realizing them in timed db-net. In this way, the catalog will allow for the addition of future patterns by assigning them to a category and applying the described formalism.

</details>

<details>

<summary>2019-09-12 07:02:03 - Core Semantic First: A Top-down Approach for AMR Parsing</summary>

- *Deng Cai, Wai Lam*

- `1909.04303v2` - [abs](http://arxiv.org/abs/1909.04303v2) - [pdf](http://arxiv.org/pdf/1909.04303v2)

> We introduce a novel scheme for parsing a piece of text into its Abstract Meaning Representation (AMR): Graph Spanning based Parsing (GSP). One novel characteristic of GSP is that it constructs a parse graph incrementally in a top-down fashion. Starting from the root, at each step, a new node and its connections to existing nodes will be jointly predicted. The output graph spans the nodes by the distance to the root, following the intuition of first grasping the main ideas then digging into more details. The \textit{core semantic first} principle emphasizes capturing the main ideas of a sentence, which is of great interest. We evaluate our model on the latest AMR sembank and achieve the state-of-the-art performance in the sense that no heuristic graph re-categorization is adopted. More importantly, the experiments show that our parser is especially good at obtaining the core semantics.

</details>

<details>

<summary>2019-09-12 08:04:48 - A Sketch-Based System for Semantic Parsing</summary>

- *Zechang Li, Yuxuan Lai, Yuxi Xie, Yansong Feng, Dongyan Zhao*

- `1909.00574v2` - [abs](http://arxiv.org/abs/1909.00574v2) - [pdf](http://arxiv.org/pdf/1909.00574v2)

> This paper presents our semantic parsing system for the evaluation task of open domain semantic parsing in NLPCC 2019. Many previous works formulate semantic parsing as a sequence-to-sequence(seq2seq) problem. Instead, we treat the task as a sketch-based problem in a coarse-to-fine(coarse2fine) fashion. The sketch is a high-level structure of the logical form exclusive of low-level details such as entities and predicates. In this way, we are able to optimize each part individually. Specifically, we decompose the process into three stages: the sketch classification determines the high-level structure while the entity labeling and the matching network fill in missing details. Moreover, we adopt the seq2seq method to evaluate logical form candidates from an overall perspective. The co-occurrence relationship between predicates and entities contribute to the reranking as well. Our submitted system achieves the exactly matching accuracy of 82.53% on full test set and 47.83% on hard test subset, which is the 3rd place in NLPCC 2019 Shared Task 2. After optimizations for parameters, network structure and sampling, the accuracy reaches 84.47% on full test set and 63.08% on hard test subset(Our code and data are available at https://github.com/zechagl/NLPCC2019-Semantic-Parsing).

</details>

<details>

<summary>2019-09-12 14:45:43 - NSURL-2019 Shared Task 8: Semantic Question Similarity in Arabic</summary>

- *Haitham Seelawi, Ahmad Mustafa, Hesham Al-Bataineh, Wael Farhan, Hussein T. Al-Natsheh*

- `1909.09691v1` - [abs](http://arxiv.org/abs/1909.09691v1) - [pdf](http://arxiv.org/pdf/1909.09691v1)

> Question semantic similarity (Q2Q) is a challenging task that is very useful in many NLP applications, such as detecting duplicate questions and question answering systems. In this paper, we present the results and findings of the shared task (Semantic Question Similarity in Arabic). The task was organized as part of the first workshop on NLP Solutions for Under Resourced Languages (NSURL 2019) The goal of the task is to predict whether two questions are semantically similar or not, even if they are phrased differently. A total of 9 teams participated in the task. The datasets created for this task are made publicly available to support further research on Arabic Q2Q.

</details>

<details>

<summary>2019-09-12 16:25:05 - Style-aware Neural Model with Application in Authorship Attribution</summary>

- *Fereshteh Jafariakinabad, Kien A. Hua*

- `1909.06194v1` - [abs](http://arxiv.org/abs/1909.06194v1) - [pdf](http://arxiv.org/pdf/1909.06194v1)

> Writing style is a combination of consistent decisions associated with a specific author at different levels of language production, including lexical, syntactic, and structural. In this paper, we introduce a style-aware neural model to encode document information from three stylistic levels and evaluate it in the domain of authorship attribution. First, we propose a simple way to jointly encode syntactic and lexical representations of sentences. Subsequently, we employ an attention-based hierarchical neural network to encode the syntactic and semantic structure of sentences in documents while rewarding the sentences which contribute more to capturing the writing style. Our experimental results, based on four benchmark datasets, reveal the benefits of encoding document information from all three stylistic levels when compared to the baseline methods in the literature.

</details>

<details>

<summary>2019-09-12 16:28:56 - HapPenIng: Happen, Predict, Infer -- Event Series Completion in a Knowledge Graph</summary>

- *Simon Gottschalk, Elena Demidova*

- `1909.06219v1` - [abs](http://arxiv.org/abs/1909.06219v1) - [pdf](http://arxiv.org/pdf/1909.06219v1)

> Event series, such as the Wimbledon Championships and the US presidential elections, represent important happenings in key societal areas including sports, culture and politics. However, semantic reference sources, such as Wikidata, DBpedia and EventKG knowledge graphs, provide only an incomplete event series representation. In this paper we target the problem of event series completion in a knowledge graph. We address two tasks: 1) prediction of sub-event relations, and 2) inference of real-world events that happened as a part of event series and are missing in the knowledge graph. To address these problems, our proposed supervised HapPenIng approach leverages structural features of event series. HapPenIng does not require any external knowledge - the characteristics making it unique in the context of event inference. Our experimental evaluation demonstrates that HapPenIng outperforms the baselines by 44 and 52 percentage points in terms of precision for the sub-event prediction and the inference tasks, correspondingly.

</details>

<details>

<summary>2019-09-12 17:36:45 - Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation</summary>

- *Suraj Nair, Chelsea Finn*

- `1909.05829v1` - [abs](http://arxiv.org/abs/1909.05829v1) - [pdf](http://arxiv.org/pdf/1909.05829v1)

> Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves nearly a 200% performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes. Project page: https://sites.google.com/stanford.edu/hvf

</details>

<details>

<summary>2019-09-13 00:06:34 - Graph Representations for Higher-Order Logic and Theorem Proving</summary>

- *Aditya Paliwal, Sarah Loos, Markus Rabe, Kshitij Bansal, Christian Szegedy*

- `1905.10006v2` - [abs](http://arxiv.org/abs/1905.10006v2) - [pdf](http://arxiv.org/pdf/1905.10006v2)

> This paper presents the first use of graph neural networks (GNNs) for higher-order proof search and demonstrates that GNNs can improve upon state-of-the-art results in this domain. Interactive, higher-order theorem provers allow for the formalization of most mathematical theories and have been shown to pose a significant challenge for deep learning. Higher-order logic is highly expressive and, even though it is well-structured with a clearly defined grammar and semantics, there still remains no well-established method to convert formulas into graph-based representations. In this paper, we consider several graphical representations of higher-order logic and evaluate them against the HOList benchmark for higher-order theorem proving.

</details>

<details>

<summary>2019-09-13 02:43:36 - Leveraging 2-hop Distant Supervision from Table Entity Pairs for Relation Extraction</summary>

- *Xiang Deng, Huan Sun*

- `1909.06007v1` - [abs](http://arxiv.org/abs/1909.06007v1) - [pdf](http://arxiv.org/pdf/1909.06007v1)

> Distant supervision (DS) has been widely used to automatically construct (noisy) labeled data for relation extraction (RE). Given two entities, distant supervision exploits sentences that directly mention them for predicting their semantic relation. We refer to this strategy as 1-hop DS, which unfortunately may not work well for long-tail entities with few supporting sentences. In this paper, we introduce a new strategy named 2-hop DS to enhance distantly supervised RE, based on the observation that there exist a large number of relational tables on the Web which contain entity pairs that share common relations. We refer to such entity pairs as anchors for each other, and collect all sentences that mention the anchor entity pairs of a given target entity pair to help relation prediction. We develop a new neural RE method REDS2 in the multi-instance learning paradigm, which adopts a hierarchical model structure to fuse information respectively from 1-hop DS and 2-hop DS. Extensive experimental results on a benchmark dataset show that REDS2 can consistently outperform various baselines across different settings by a substantial margin.

</details>

<details>

<summary>2019-09-13 05:59:40 - SANVis: Visual Analytics for Understanding Self-Attention Networks</summary>

- *Cheonbok Park, Inyoup Na, Yongjang Jo, Sungbok Shin, Jaehyo Yoo, Bum Chul Kwon, Jian Zhao, Hyungjong Noh, Yeonsoo Lee, Jaegul Choo*

- `1909.09595v1` - [abs](http://arxiv.org/abs/1909.09595v1) - [pdf](http://arxiv.org/pdf/1909.09595v1)

> Attention networks, a deep neural network architecture inspired by humans' attention mechanism, have seen significant success in image captioning, machine translation, and many other applications. Recently, they have been further evolved into an advanced approach called multi-head self-attention networks, which can encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether multi-head attention. Meanwhile, the increased model complexity prevents users from easily understanding and manipulating the inner workings of models. To tackle the challenges, we present a visual analytics system called SANVis, which helps users understand the behaviors and the characteristics of multi-head self-attention networks. Using a state-of-the-art self-attention model called Transformer, we demonstrate usage scenarios of SANVis in machine translation tasks. Our system is available at http://short.sanvis.org

</details>

<details>

<summary>2019-09-13 14:25:06 - V2: Fast Detection of Configuration Drift in Python</summary>

- *Eric Horton, Chris Parnin*

- `1909.06251v1` - [abs](http://arxiv.org/abs/1909.06251v1) - [pdf](http://arxiv.org/pdf/1909.06251v1)

> Code snippets are prevalent, but are hard to reuse because they often lack an accompanying environment configuration. Most are not actively maintained, allowing for drift between the most recent possible configuration and the code snippet as the snippet becomes out-of-date over time. Recent work has identified the problem of validating and detecting out-of-date code snippets as the most important consideration for code reuse. However, determining if a snippet is correct, but simply out-of-date, is a non-trivial task. In the best case, breaking changes are well documented, allowing developers to manually determine when a code snippet contains an out-of-date API usage. In the worst case, determining if and when a breaking change was made requires an exhaustive search through previous dependency versions.   We present V2, a strategy for determining if a code snippet is out-of-date by detecting discrete instances of configuration drift, where the snippet uses an API which has since undergone a breaking change. Each instance of configuration drift is classified by a failure encountered during validation and a configuration patch, consisting of dependency version changes, which fixes the underlying fault. V2 uses feedback-directed search to explore the possible configuration space for a code snippet, reducing the number of potential environment configurations that need to be validated. When run on a corpus of public Python snippets from prior research, V2 identifies 248 instances of configuration drift.

</details>

<details>

<summary>2019-09-13 14:49:38 - Semantic and Visual Similarities for Efficient Knowledge Transfer in CNN Training</summary>

- *Lucas Pascal, Xavier Bost, Benoît Huet*

- `1909.12916v1` - [abs](http://arxiv.org/abs/1909.12916v1) - [pdf](http://arxiv.org/pdf/1909.12916v1)

> In recent years, representation learning approaches have disrupted many multimedia computing tasks. Among those approaches, deep convolutional neural networks (CNNs) have notably reached human level expertise on some constrained image classification tasks. Nonetheless, training CNNs from scratch for new task or simply new data turns out to be complex and time-consuming. Recently, transfer learning has emerged as an effective methodology for adapting pre-trained CNNs to new data and classes, by only retraining the last classification layer. This paper focuses on improving this process, in order to better transfer knowledge between CNN architectures for faster trainings in the case of fine tuning for image classification. This is achieved by combining and transfering supplementary weights, based on similarity considerations between source and target classes. The study includes a comparison between semantic and content-based similarities, and highlights increased initial performances and training speed, along with superior long term performances when limited training samples are available.

</details>

<details>

<summary>2019-09-13 15:20:41 - Risk-Aware Planning by Confidence Estimation using Deep Learning-Based Perception</summary>

- *Maymoonah Toubeh, Pratap Tokekar*

- `1910.00101v1` - [abs](http://arxiv.org/abs/1910.00101v1) - [pdf](http://arxiv.org/pdf/1910.00101v1)

> This work proposes the use of Bayesian approximations of uncertainty from deep learning in a robot planner, showing that this produces more cautious actions in safety-critical scenarios. The case study investigated is motivated by a setup where an aerial robot acts as a "scout" for a ground robot. This is useful when the below area is unknown or dangerous, with applications in space exploration, military, or search-and-rescue. Images taken from the aerial view are used to provide a less obstructed map to guide the navigation of the robot on the ground. Experiments are conducted using a deep learning semantic image segmentation, followed by a path planner based on the resulting cost map, to provide an empirical analysis of the proposed method. A comparison with similar approaches is presented to portray the usefulness of certain techniques, or variations within a technique, in similar experimental settings. The method is analyzed to assess the impact of variations in the uncertainty extraction, as well as the absence of an uncertainty metric, on the overall system with the use of a defined metric which measures surprise to the planner. The analysis is performed on multiple datasets, showing a similar trend of lower surprise when uncertainty information is incorporated in the planning, given threshold values of the hyperparameters in the uncertainty extraction have been met. We find that taking uncertainty into account leads to paths that could be 18% less risky on an average.

</details>

<details>

<summary>2019-09-13 17:56:40 - That's C, baby. C!</summary>

- *Roberto Bagnara*

- `1909.06353v1` - [abs](http://arxiv.org/abs/1909.06353v1) - [pdf](http://arxiv.org/pdf/1909.06353v1)

> Hardly a week goes by at BUGSENG without having to explain to someone that almost any piece of C text, considered in isolation, means absolutely nothing. The belief that C text has meaning in itself is so common, also among seasoned C practitioners, that I thought writing a short paper on the subject was a good time investment. The problem is due to the fact that the semantics of the C programming language is not fully defined: non-definite behavior, predefined macros, different library implementations, peculiarities of the translation process, . . . : all these contribute to the fact that no meaning can be assigned to source code unless full details about the build are available. The paper starts with an exercise that admits a solution. The existence of this solution will hopefully convince anyone that, in general, unless the toolchain and the build procedure are fully known, no meaning can be assigned to any nontrivial piece of C code.

</details>

<details>

<summary>2019-09-13 17:59:03 - Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering</summary>

- *Shiyue Zhang, Mohit Bansal*

- `1909.06356v1` - [abs](http://arxiv.org/abs/1909.06356v1) - [pdf](http://arxiv.org/pdf/1909.06356v1)

> Text-based Question Generation (QG) aims at generating natural and relevant questions that can be answered by a given answer in some context. Existing QG models suffer from a "semantic drift" problem, i.e., the semantics of the model-generated question drifts away from the given context and answer. In this paper, we first propose two semantics-enhanced rewards obtained from downstream question paraphrasing and question answering tasks to regularize the QG model to generate semantically valid questions. Second, since the traditional evaluation metrics (e.g., BLEU) often fall short in evaluating the quality of generated questions, we propose a QA-based evaluation method which measures the QG model's ability to mimic human annotators in generating QA training data. Experiments show that our method achieves the new state-of-the-art performance w.r.t. traditional metrics, and also performs best on our QA-based evaluation metrics. Further, we investigate how to use our QG model to augment QA datasets and enable semi-supervised QA. We propose two ways to generate synthetic QA pairs: generate new questions from existing articles or collect QA pairs from new articles. We also propose two empirically effective strategies, a data filter and mixing mini-batch training, to properly use the QG-generated data for QA. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles.

</details>

<details>

<summary>2019-09-13 19:16:53 - Learning Household Task Knowledge from WikiHow Descriptions</summary>

- *Yilun Zhou, Julie A. Shah, Steven Schockaert*

- `1909.06414v1` - [abs](http://arxiv.org/abs/1909.06414v1) - [pdf](http://arxiv.org/pdf/1909.06414v1)

> Commonsense procedural knowledge is important for AI agents and robots that operate in a human environment. While previous attempts at constructing procedural knowledge are mostly rule- and template-based, recent advances in deep learning provide the possibility of acquiring such knowledge directly from natural language sources. As a first step in this direction, we propose a model to learn embeddings for tasks, as well as the individual steps that need to be taken to solve them, based on WikiHow articles. We learn these embeddings such that they are predictive of both step relevance and step ordering. We also experiment with the use of integer programming for inferring consistent global step orderings from noisy pairwise predictions.

</details>

<details>

<summary>2019-09-14 03:51:46 - NeMo: a toolkit for building AI applications using Neural Modules</summary>

- *Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, Patrice Castonguay, Mariya Popova, Jocelyn Huang, Jonathan M. Cohen*

- `1909.09577v1` - [abs](http://arxiv.org/abs/1909.09577v1) - [pdf](http://arxiv.org/pdf/1909.09577v1)

> NeMo (Neural Modules) is a Python framework-agnostic toolkit for creating AI applications through re-usability, abstraction, and composition. NeMo is built around neural modules, conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations. NeMo makes it easy to combine and re-use these building blocks while providing a level of semantic correctness checking via its neural type system. The toolkit comes with extendable collections of pre-built modules for automatic speech recognition and natural language processing. Furthermore, NeMo provides built-in support for distributed training and mixed precision on latest NVIDIA GPUs. NeMo is open-source https://github.com/NVIDIA/NeMo

</details>

<details>

<summary>2019-09-14 14:30:25 - Adversarial Examples Versus Cloud-based Detectors: A Black-box Empirical Study</summary>

- *Xurong Li, Shouling Ji, Meng Han, Juntao Ji, Zhenyu Ren, Yushan Liu, Chunming Wu*

- `1901.01223v4` - [abs](http://arxiv.org/abs/1901.01223v4) - [pdf](http://arxiv.org/pdf/1901.01223v4)

> Deep learning has been broadly leveraged by major cloud providers, such as Google, AWS and Baidu, to offer various computer vision related services including image classification, object identification, illegal image detection, etc. While recent works extensively demonstrated that deep learning classification models are vulnerable to adversarial examples, cloud-based image detection models, which are more complicated than classifiers, may also have similar security concern but not get enough attention yet. In this paper, we mainly focus on the security issues of real-world cloud-based image detectors. Specifically, (1) based on effective semantic segmentation, we propose four attacks to generate semantics-aware adversarial examples via only interacting with black-box APIs; and (2) we make the first attempt to conduct an extensive empirical study of black-box attacks against real-world cloud-based image detectors. Through the comprehensive evaluations on five major cloud platforms: AWS, Azure, Google Cloud, Baidu Cloud, and Alibaba Cloud, we demonstrate that our image processing based attacks can reach a success rate of approximately 100%, and the semantic segmentation based attacks have a success rate over 90% among different detection services, such as violence, politician, and pornography detection. We also proposed several possible defense strategies for these security challenges in the real-life situation.

</details>

<details>

<summary>2019-09-14 15:42:30 - Subword Semantic Hashing for Intent Classification on Small Datasets</summary>

- *Kumar Shridhar, Ayushman Dash, Amit Sahu, Gustav Grund Pihlgren, Pedro Alonso, Vinaychandran Pondenkandath, Gyorgy Kovacs, Foteini Simistira, Marcus Liwicki*

- `1810.07150v3` - [abs](http://arxiv.org/abs/1810.07150v3) - [pdf](http://arxiv.org/pdf/1810.07150v3)

> In this paper, we introduce the use of Semantic Hashing as embedding for the task of Intent Classification and achieve state-of-the-art performance on three frequently used benchmarks. Intent Classification on a small dataset is a challenging task for data-hungry state-of-the-art Deep Learning based systems. Semantic Hashing is an attempt to overcome such a challenge and learn robust text classification. Current word embedding based are dependent on vocabularies. One of the major drawbacks of such methods is out-of-vocabulary terms, especially when having small training datasets and using a wider vocabulary. This is the case in Intent Classification for chatbots, where typically small datasets are extracted from internet communication. Two problems arise by the use of internet communication. First, such datasets miss a lot of terms in the vocabulary to use word embeddings efficiently. Second, users frequently make spelling errors. Typically, the models for intent classification are not trained with spelling errors and it is difficult to think about ways in which users will make mistakes. Models depending on a word vocabulary will always face such issues. An ideal classifier should handle spelling errors inherently. With Semantic Hashing, we overcome these challenges and achieve state-of-the-art results on three datasets: AskUbuntu, Chatbot, and Web Application. Our benchmarks are available online: https://github.com/kumar-shridhar/Know-Your-Intent

</details>

<details>

<summary>2019-09-14 17:25:03 - Joint Wasserstein Autoencoders for Aligning Multimodal Embeddings</summary>

- *Shweta Mahajan, Teresa Botschen, Iryna Gurevych, Stefan Roth*

- `1909.06635v1` - [abs](http://arxiv.org/abs/1909.06635v1) - [pdf](http://arxiv.org/pdf/1909.06635v1)

> One of the key challenges in learning joint embeddings of multiple modalities, e.g. of images and text, is to ensure coherent cross-modal semantics that generalize across datasets. We propose to address this through joint Gaussian regularization of the latent representations. Building on Wasserstein autoencoders (WAEs) to encode the input in each domain, we enforce the latent embeddings to be similar to a Gaussian prior that is shared across the two domains, ensuring compatible continuity of the encoded semantic representations of images and texts. Semantic alignment is achieved through supervision from matching image-text pairs. To show the benefits of our semi-supervised representation, we apply it to cross-modal retrieval and phrase localization. We not only achieve state-of-the-art accuracy, but significantly better generalization across datasets, owing to the semantic continuity of the latent space.

</details>

<details>

<summary>2019-09-14 21:49:09 - 3D Kidneys and Kidney Tumor Semantic Segmentation using Boundary-Aware Networks</summary>

- *Andriy Myronenko, Ali Hatamizadeh*

- `1909.06684v1` - [abs](http://arxiv.org/abs/1909.06684v1) - [pdf](http://arxiv.org/pdf/1909.06684v1)

> Automated segmentation of kidneys and kidney tumors is an important step in quantifying the tumor's morphometrical details to monitor the progression of the disease and accurately compare decisions regarding the kidney tumor treatment. Manual delineation techniques are often tedious, error-prone and require expert knowledge for creating unambiguous representation of kidneys and kidney tumors segmentation. In this work, we propose an end-to-end boundary aware fully Convolutional Neural Networks (CNNs) for reliable kidney and kidney tumor semantic segmentation from arterial phase abdominal 3D CT scans. We propose a segmentation network consisting of an encoder-decoder architecture that specifically accounts for organ and tumor edge information by devising a dedicated boundary branch supervised by edge-aware loss terms. We have evaluated our model on 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge dataset and our method has achieved dice scores of 0.9742 and 0.8103 for kidney and tumor repetitively and an overall composite dice score of 0.8923.

</details>

<details>

<summary>2019-09-14 23:15:20 - Beyond BLEU: Training Neural Machine Translation with Semantic Similarity</summary>

- *John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, Graham Neubig*

- `1909.06694v1` - [abs](http://arxiv.org/abs/1909.06694v1) - [pdf](http://arxiv.org/pdf/1909.06694v1)

> While most neural machine translation (NMT) systems are still trained using maximum likelihood estimation, recent work has demonstrated that optimizing systems to directly improve evaluation metrics such as BLEU can substantially improve final translation accuracy. However, training with BLEU has some limitations: it doesn't assign partial credit, it has a limited range of output values, and it can penalize semantically correct hypotheses if they differ lexically from the reference. In this paper, we introduce an alternative reward function for optimizing NMT systems that is based on recent work in semantic similarity. We evaluate on four disparate languages translated to English, and find that training with our proposed metric results in better translations as evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU.

</details>

<details>

<summary>2019-09-15 19:10:16 - Wasserstein Diffusion Tikhonov Regularization</summary>

- *Alex Tong Lin, Yonatan Dukler, Wuchen Li, Guido Montufar*

- `1909.06860v1` - [abs](http://arxiv.org/abs/1909.06860v1) - [pdf](http://arxiv.org/pdf/1909.06860v1)

> We propose regularization strategies for learning discriminative models that are robust to in-class variations of the input data. We use the Wasserstein-2 geometry to capture semantically meaningful neighborhoods in the space of images, and define a corresponding input-dependent additive noise data augmentation model. Expanding and integrating the augmented loss yields an effective Tikhonov-type Wasserstein diffusion smoothness regularizer. This approach allows us to apply high levels of regularization and train functions that have low variability within classes but remain flexible across classes. We provide efficient methods for computing the regularizer at a negligible cost in comparison to training with adversarial data augmentation. Initial experiments demonstrate improvements in generalization performance under adversarial perturbations and also large in-class variations of the input data.

</details>

<details>

<summary>2019-09-15 19:18:13 - Putting the Horse Before the Cart:A Generator-Evaluator Framework for Question Generation from Text</summary>

- *Vishwajeet Kumar, Ganesh Ramakrishnan, Yuan-Fang Li*

- `1808.04961v5` - [abs](http://arxiv.org/abs/1808.04961v5) - [pdf](http://arxiv.org/pdf/1808.04961v5)

> Automatic question generation (QG) is a useful yet challenging task in NLP. Recent neural network-based approaches represent the state-of-the-art in this task. In this work, we attempt to strengthen them significantly by adopting a holistic and novel generator-evaluator framework that directly optimizes objectives that reward semantics and structure. The {\it generator} is a sequence-to-sequence model that incorporates the {\it structure} and {\it semantics} of the question being generated. The generator predicts an answer in the passage that the question can pivot on. Employing the copy and coverage mechanisms, it also acknowledges other contextually important (and possibly rare) keywords in the passage that the question needs to conform to, while not redundantly repeating words. The {\it evaluator} model evaluates and assigns a reward to each predicted question based on its conformity to the {\it structure} of ground-truth questions. We propose two novel QG-specific reward functions for text conformity and answer conformity of the generated question. The evaluator also employs structure-sensitive rewards based on evaluation measures such as BLEU, GLEU, and ROUGE-L, which are suitable for QG. In contrast, most of the previous works only optimize the cross-entropy loss, which can induce inconsistencies between training (objective) and testing (evaluation) measures. Our evaluation shows that our approach significantly outperforms state-of-the-art systems on the widely-used SQuAD benchmark as per both automatic and human evaluation.

</details>

<details>

<summary>2019-09-16 11:56:58 - GAN-Tree: An Incrementally Learned Hierarchical Generative Framework for Multi-Modal Data Distributions</summary>

- *Jogendra Nath Kundu, Maharshi Gor, Dakshit Agrawal, R. Venkatesh Babu*

- `1908.03919v3` - [abs](http://arxiv.org/abs/1908.03919v3) - [pdf](http://arxiv.org/pdf/1908.03919v3)

> Despite the remarkable success of generative adversarial networks, their performance seems less impressive for diverse training sets, requiring learning of discontinuous mapping functions. Though multi-mode prior or multi-generator models have been proposed to alleviate this problem, such approaches may fail depending on the empirically chosen initial mode components. In contrast to such bottom-up approaches, we present GAN-Tree, which follows a hierarchical divisive strategy to address such discontinuous multi-modal data. Devoid of any assumption on the number of modes, GAN-Tree utilizes a novel mode-splitting algorithm to effectively split the parent mode to semantically cohesive children modes, facilitating unsupervised clustering. Further, it also enables incremental addition of new data modes to an already trained GAN-Tree, by updating only a single branch of the tree structure. As compared to prior approaches, the proposed framework offers a higher degree of flexibility in choosing a large variety of mutually exclusive and exhaustive tree nodes called GAN-Set. Extensive experiments on synthetic and natural image datasets including ImageNet demonstrate the superiority of GAN-Tree against the prior state-of-the-arts.

</details>

<details>

<summary>2019-09-16 14:17:04 - A Convolutional Transformation Network for Malware Classification</summary>

- *Duc-Ly Vu, Trong-Kha Nguyen, Tam V. Nguyen, Tu N. Nguyen, Fabio Massacci, Phu H. Phung*

- `1909.07227v1` - [abs](http://arxiv.org/abs/1909.07227v1) - [pdf](http://arxiv.org/pdf/1909.07227v1)

> Modern malware evolves various detection avoidance techniques to bypass the state-of-the-art detection methods. An emerging trend to deal with this issue is the combination of image transformation and machine learning techniques to classify and detect malware. However, existing works in this field only perform simple image transformation methods that limit the accuracy of the detection. In this paper, we introduce a novel approach to classify malware by using a deep network on images transformed from binary samples. In particular, we first develop a novel hybrid image transformation method to convert binaries into color images that convey the binary semantics. The images are trained by a deep convolutional neural network that later classifies the test inputs into benign or malicious categories. Through the extensive experiments, our proposed method surpasses all baselines and achieves 99.14% in terms of accuracy on the testing set.

</details>

<details>

<summary>2019-09-16 17:22:25 - Multilingual Neural Machine Translation for Zero-Resource Languages</summary>

- *Surafel M. Lakew, Marcello Federico, Matteo Negri, Marco Turchi*

- `1909.07342v1` - [abs](http://arxiv.org/abs/1909.07342v1) - [pdf](http://arxiv.org/pdf/1909.07342v1)

> In recent years, Neural Machine Translation (NMT) has been shown to be more effective than phrase-based statistical methods, thus quickly becoming the state of the art in machine translation (MT). However, NMT systems are limited in translating low-resourced languages, due to the significant amount of parallel data that is required to learn useful mappings between languages. In this work, we show how the so-called multilingual NMT can help to tackle the challenges associated with low-resourced language translation. The underlying principle of multilingual NMT is to force the creation of hidden representations of words in a shared semantic space across multiple languages, thus enabling a positive parameter transfer across languages. Along this direction, we present multilingual translation experiments with three languages (English, Italian, Romanian) covering six translation directions, utilizing both recurrent neural networks and transformer (or self-attentive) neural networks. We then focus on the zero-shot translation problem, that is how to leverage multi-lingual data in order to learn translation directions that are not covered by the available training material. To this aim, we introduce our recently proposed iterative self-training method, which incrementally improves a multilingual NMT on a zero-shot direction by just relying on monolingual data. Our results on TED talks data show that multilingual NMT outperforms conventional bilingual NMT, that the transformer NMT outperforms recurrent NMT, and that zero-shot NMT outperforms conventional pivoting methods and even matches the performance of a fully-trained bilingual system.

</details>

<details>

<summary>2019-09-16 20:18:38 - Visualizing How Embeddings Generalize</summary>

- *Xiaotong Liu, Hong Xuan, Zeyu Zhang, Abby Stylianou, Robert Pless*

- `1909.07464v1` - [abs](http://arxiv.org/abs/1909.07464v1) - [pdf](http://arxiv.org/pdf/1909.07464v1)

> Deep metric learning is often used to learn an embedding function that captures the semantic differences within a dataset. A key factor in many problem domains is how this embedding generalizes to new classes of data. In observing many triplet selection strategies for Metric Learning, we find that the best performance consistently arises from approaches that focus on a few, well selected triplets.We introduce visualization tools to illustrate how an embedding generalizes beyond measuring accuracy on validation data, and we illustrate the behavior of a range of triplet selection strategies.

</details>

<details>

<summary>2019-09-17 07:33:25 - Incremental Learning Techniques for Semantic Segmentation</summary>

- *Umberto Michieli, Pietro Zanuttigh*

- `1907.13372v4` - [abs](http://arxiv.org/abs/1907.13372v4) - [pdf](http://arxiv.org/pdf/1907.13372v4)

> Deep learning architectures exhibit a critical drop of performance due to catastrophic forgetting when they are required to incrementally learn new tasks. Contemporary incremental learning frameworks focus on image classification and object detection while in this work we formally introduce the incremental learning problem for semantic segmentation in which a pixel-wise labeling is considered. To tackle this task we propose to distill the knowledge of the previous model to retain the information about previously learned classes, whilst updating the current model to learn the new ones. We propose various approaches working both on the output logits and on intermediate features. In opposition to some recent frameworks, we do not store any image from previously learned classes and only the last model is needed to preserve high accuracy on these classes. The experimental evaluation on the Pascal VOC2012 dataset shows the effectiveness of the proposed approaches.

</details>

<details>

<summary>2019-09-17 08:33:12 - Proceedings 35th International Conference on Logic Programming (Technical Communications)</summary>

- *Bart Bogaerts, Esra Erdem, Paul Fodor, Andrea Formisano, Giovambattista Ianni, Daniela Inclezan, German Vidal, Alicia Villanueva, Marina De Vos, Fangkai Yang*

- `1909.07646v1` - [abs](http://arxiv.org/abs/1909.07646v1) - [pdf](http://arxiv.org/pdf/1909.07646v1)

> Since the first conference held in Marseille in 1982, ICLP has been the premier international event for presenting research in logic programming. Contributions are sought in all areas of logic programming, including but not restricted to:   Foundations: Semantics, Formalisms, Nonmonotonic reasoning, Knowledge representation.   Languages: Concurrency, Objects, Coordination, Mobility, Higher Order, Types, Modes, Assertions, Modules, Meta-programming, Logic-based domain-specific languages, Programming Techniques.   Declarative programming: Declarative program development, Analysis, Type and mode inference, Partial evaluation, Abstract interpretation, Transformation, Validation, Verification, Debugging, Profiling, Testing, Execution visualization   Implementation: Virtual machines, Compilation, Memory management, Parallel/distributed execution, Constraint handling rules, Tabling, Foreign interfaces, User interfaces.   Related Paradigms and Synergies: Inductive and Co-inductive Logic Programming, Constraint Logic Programming, Answer Set Programming, Interaction with SAT, SMT and CSP solvers, Logic programming techniques for type inference and theorem proving, Argumentation, Probabilistic Logic Programming, Relations to object-oriented and Functional programming.   Applications: Databases, Big Data, Data integration and federation, Software engineering, Natural language processing, Web and Semantic Web, Agents, Artificial intelligence, Computational life sciences, Education, Cybersecurity, and Robotics.

</details>

<details>

<summary>2019-09-17 08:57:07 - Controllable Length Control Neural Encoder-Decoder via Reinforcement Learning</summary>

- *Junyi Bian, Baojun Lin, Ke Zhang, Zhaohui Yan, Hong Tang, Yonghe Zhang*

- `1909.09492v1` - [abs](http://arxiv.org/abs/1909.09492v1) - [pdf](http://arxiv.org/pdf/1909.09492v1)

> Controlling output length in neural language generation is valuable in many scenarios, especially for the tasks that have length constraints. A model with stronger length control capacity can produce sentences with more specific length, however, it usually sacrifices semantic accuracy of the generated sentences. Here, we denote a concept of Controllable Length Control (CLC) for the trade-off between length control capacity and semantic accuracy of the language generation model. More specifically, CLC is to alter length control capacity of the model so as to generate sentence with corresponding quality. This is meaningful in real applications when length control capacity and outputs quality are requested with different priorities, or to overcome unstability of length control during model training. In this paper, we propose two reinforcement learning (RL) methods to adjust the trade-off between length control capacity and semantic accuracy of length control models. Results show that our RL methods improve scores across a wide range of target lengths and achieve the goal of CLC. Additionally, two models LenMC and LenLInit modified on previous length-control models are proposed to obtain better performance in summarization task while still maintain the ability to control length.

</details>

<details>

<summary>2019-09-17 10:18:14 - Multi-Task Learning for Automotive Foggy Scene Understanding via Domain Adaptation to an Illumination-Invariant Representation</summary>

- *Naif Alshammari, Samet Akçay, Toby P. Breckon*

- `1909.07697v1` - [abs](http://arxiv.org/abs/1909.07697v1) - [pdf](http://arxiv.org/pdf/1909.07697v1)

> Joint scene understanding and segmentation for automotive applications is a challenging problem in two key aspects:- (1) classifying every pixel in the entire scene and (2) performing this task under unstable weather and illumination changes (e.g. foggy weather), which results in poor outdoor scene visibility. This poor outdoor scene visibility leads to a non-optimal performance of deep convolutional neural network-based scene understanding and segmentation. In this paper, we propose an efficient end-to-end contemporary automotive semantic scene understanding approach under foggy weather conditions, employing domain adaptation and illumination-invariant image per-transformation. As a multi-task pipeline, our proposed model provides:- (1) transferring images from extreme to clear-weather condition using domain transfer approach and (2) semantically segmenting a scene using a competitive encoder-decoder convolutional neural network (CNN) with dense connectivity, skip connections and fusion-based techniques. We evaluate our approach on challenging foggy datasets, including synthetic dataset (Foggy Cityscapes) as well as real-world datasets (Foggy Zurich and Foggy Driving). By incorporating RGB, depth, and illumination-invariant information, our approach outperforms the state-of-the-art within automotive scene understanding, under foggy weather condition.

</details>

<details>

<summary>2019-09-17 12:07:22 - Course Concept Expansion in MOOCs with External Knowledge and Interactive Game</summary>

- *Jifan Yu, Chenyu Wang, Gan Luo, Lei Hou, Juanzi Li, Jie Tang, Zhiyuan Liu*

- `1909.07739v1` - [abs](http://arxiv.org/abs/1909.07739v1) - [pdf](http://arxiv.org/pdf/1909.07739v1)

> As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users. Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments. In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods. The source code and datasets have been published.

</details>

<details>

<summary>2019-09-17 15:05:31 - Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model</summary>

- *Prashanth Vijayaraghavan, Deb Roy*

- `1909.07873v1` - [abs](http://arxiv.org/abs/1909.07873v1) - [pdf](http://arxiv.org/pdf/1909.07873v1)

> Recently, generating adversarial examples has become an important means of measuring robustness of a deep learning model. Adversarial examples help us identify the susceptibilities of the model and further counter those vulnerabilities by applying adversarial training techniques. In natural language domain, small perturbations in the form of misspellings or paraphrases can drastically change the semantics of the text. We propose a reinforcement learning based approach towards generating adversarial examples in black-box settings. We demonstrate that our method is able to fool well-trained models for (a) IMDB sentiment classification task and (b) AG's news corpus news categorization task with significantly high success rates. We find that the adversarial examples generated are semantics-preserving perturbations to the original text.

</details>

<details>

<summary>2019-09-17 16:46:58 - Say Anything: Automatic Semantic Infelicity Detection in L2 English Indefinite Pronouns</summary>

- *Ella Rabinovich, Julia Watson, Barend Beekhuizen, Suzanne Stevenson*

- `1909.07928v1` - [abs](http://arxiv.org/abs/1909.07928v1) - [pdf](http://arxiv.org/pdf/1909.07928v1)

> Computational research on error detection in second language speakers has mainly addressed clear grammatical anomalies typical to learners at the beginner-to-intermediate level. We focus instead on acquisition of subtle semantic nuances of English indefinite pronouns by non-native speakers at varying levels of proficiency. We first lay out theoretical, linguistically motivated hypotheses, and supporting empirical evidence on the nature of the challenges posed by indefinite pronouns to English learners. We then suggest and evaluate an automatic approach for detection of atypical usage patterns, demonstrating that deep learning architectures are promising for this task involving nuanced semantic anomalies.

</details>

<details>

<summary>2019-09-17 19:21:11 - Revealing the Importance of Semantic Retrieval for Machine Reading at Scale</summary>

- *Yixin Nie, Songhe Wang, Mohit Bansal*

- `1909.08041v1` - [abs](http://arxiv.org/abs/1909.08041v1) - [pdf](http://arxiv.org/pdf/1909.08041v1)

> Machine Reading at Scale (MRS) is a challenging task in which a system is given an input query and is asked to produce a precise output by "reading" information from a large knowledge base. The task has gained popularity with its natural combination of information retrieval (IR) and machine comprehension (MC). Advancements in representation learning have led to separated progress in both IR and MC; however, very few studies have examined the relationship and combined design of retrieval and comprehension at different levels of granularity, for development of MRS systems. In this work, we give general guidelines on system design for MRS by proposing a simple yet effective pipeline system with special consideration on hierarchical semantic retrieval at both paragraph and sentence level, and their potential effects on the downstream task. The system is evaluated on both fact verification and open-domain multihop QA, achieving state-of-the-art results on the leaderboard test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of semantic retrieval, we present ablation and analysis studies to quantify the contribution of neural retrieval modules at both paragraph-level and sentence-level, and illustrate that intermediate semantic retrieval modules are vital for not only effectively filtering upstream information and thus saving downstream computation, but also for shaping upstream data distribution and providing better data for downstream modeling. Code/data made publicly available at: https://github.com/easonnie/semanticRetrievalMRS

</details>

<details>

<summary>2019-09-18 06:59:51 - Exploiting Partial Knowledge in Declarative Domain-Specific Heuristics for ASP</summary>

- *Richard Taupe, Konstantin Schekotihin, Peter Schüller, Antonius Weinzierl, Gerhard Friedrich*

- `1909.08231v1` - [abs](http://arxiv.org/abs/1909.08231v1) - [pdf](http://arxiv.org/pdf/1909.08231v1)

> Domain-specific heuristics are an important technique for solving combinatorial problems efficiently. We propose a novel semantics for declarative specifications of domain-specific heuristics in Answer Set Programming (ASP). Decision procedures that are based on a partial solution are a frequent ingredient of existing domain-specific heuristics, e.g., for placing an item that has not been placed yet in bin packing. Therefore, in our novel semantics negation as failure and aggregates in heuristic conditions are evaluated on a partial solver state. State-of-the-art solvers do not allow such a declarative specification. Our implementation in the lazy-grounding ASP system Alpha supports heuristic directives under this semantics. By that, we also provide the first implementation for incorporating declaratively specified domain-specific heuristics in a lazy-grounding setting. Experiments confirm that the combination of ASP solving with lazy grounding and our novel heuristics can be a vital ingredient for solving industrial-size problems.

</details>

<details>

<summary>2019-09-18 07:06:51 - Strong Equivalence for LPMLN Programs</summary>

- *Joohyung Lee, Man Luo*

- `1909.08998v1` - [abs](http://arxiv.org/abs/1909.08998v1) - [pdf](http://arxiv.org/pdf/1909.08998v1)

> LPMLN is a probabilistic extension of answer set programs with the weight scheme adapted from Markov Logic. We study the concept of strong equivalence in LPMLN, which is a useful mathematical tool for simplifying a part of an LPMLN program without looking at the rest of it. We show that the verification of strong equivalence in LPMLN can be reduced to equivalence checking in classical logic via a reduct and choice rules as well as to equivalence checking under the "soft" logic of here-and-there. The result allows us to leverage an answer set solver for LPMLN strong equivalence checking. The study also suggests us a few reformulations of the LPMLN semantics using choice rules, the logic of here-and-there, and classical logic.

</details>

<details>

<summary>2019-09-18 07:07:42 - Extended Magic for Negation: Efficient Demand-Driven Evaluation of Stratified Datalog with Precise Complexity Guarantees</summary>

- *K. Tuncay Tekle, Yanhong A. Liu*

- `1909.08246v1` - [abs](http://arxiv.org/abs/1909.08246v1) - [pdf](http://arxiv.org/pdf/1909.08246v1)

> Given a set of Datalog rules, facts, and a query, answers to the query can be inferred bottom-up starting from the facts or top-down starting from the query. For efficiency, top-down evaluation is extended with memoization of inferred facts, and bottom-up evaluation is performed after transformations to make rules driven by the demand from the query. Prior work has shown their precise complexity analysis and relationships. However, when Datalog is extended with even stratified negation, which has a simple and universally accepted semantics, transformations to make rules demand-driven may result in non-stratified negation, which has had many complex semantics and evaluation methods.   This paper presents (1) a simple extension to demand transformation, a transformation to make rules demand-driven for Datalog without negation, to support stratified negation, and (2) a simple extension to an optimal bottom-up evaluation method for Datalog with stratified negation, to handle non-stratified negation in the resulting rules. We show that the method provides precise complexity guarantees. It is also optimal in that only facts needed for top-down evaluation of the query are inferred and each firing of a rule to infer such a fact takes worst-case constant time. We extend the precise relationship between top-down evaluation and demand-driven bottom-up evaluation to Datalog with stratified negation. Finally, we show experimental results for performance, as well as applications to previously challenging examples.

</details>

<details>

<summary>2019-09-18 07:08:44 - BigData Applications from Graph Analytics to Machine Learning by Aggregates in Recursion</summary>

- *Ariyam Das, Youfu Li, Jin Wang, Mingda Li, Carlo Zaniolo*

- `1909.08249v1` - [abs](http://arxiv.org/abs/1909.08249v1) - [pdf](http://arxiv.org/pdf/1909.08249v1)

> In the past, the semantic issues raised by the non-monotonic nature of aggregates often prevented their use in the recursive statements of logic programs and deductive databases. However, the recently introduced notion of Pre-mappability (PreM) has shown that, in key applications of interest, aggregates can be used in recursion to optimize the perfect-model semantics of aggregate-stratified programs. Therefore we can preserve the declarative formal semantics of such programs while achieving a highly efficient operational semantics that is conducive to scalable implementations on parallel and distributed platforms. In this paper, we show that with PreM, a wide spectrum of classical algorithms of practical interest, ranging from graph analytics and dynamic programming based optimization problems to data mining and machine learning applications can be concisely expressed in declarative languages by using aggregates in recursion. Our examples are also used to show that PreM can be checked using simple techniques and templatized verification strategies. A wide range of advanced BigData applications can now be expressed declaratively in logic-based languages, including Datalog, Prolog, and even SQL, while enabling their execution with superior performance and scalability.

</details>

<details>

<summary>2019-09-18 07:09:07 - Natural Language Generation for Non-Expert Users</summary>

- *Van Duc Nguyen, Tran Cao Son, Enrico Pontelli*

- `1909.08250v1` - [abs](http://arxiv.org/abs/1909.08250v1) - [pdf](http://arxiv.org/pdf/1909.08250v1)

> Motivated by the difficulty in presenting computational results, especially when the results are a collection of atoms in a logical language, to users, who are not proficient in computer programming and/or the logical representation of the results, we propose a system for automatic generation of natural language descriptions for applications targeting mainstream users. Differently from many earlier systems with the same aim, the proposed system does not employ templates for the generation task. It assumes that there exist some natural language sentences in the application domain and uses this repository for the natural language description. It does not require, however, a large corpus as it is often required in machine learning approaches. The systems consist of two main components. The first one aims at analyzing the sentences and constructs a Grammatical Framework (GF) for given sentences and is implemented using the Stanford parser and an answer set program. The second component is for sentence construction and relies on GF Library. The paper includes two use cases to demostrate the capability of the system. As the sentence construction is done via GF, the paper includes a use case evaluation showing that the proposed system could also be utilized in addressing a challenge to create an abstract Wikipedia, which is recently discussed in the BlueSky session of the 2018 International Semantic Web Conference.

</details>

<details>

<summary>2019-09-18 10:55:46 - A Lexical, Syntactic, and Semantic Perspective for Understanding Style in Text</summary>

- *Gaurav Verma, Balaji Vasan Srinivasan*

- `1909.08349v1` - [abs](http://arxiv.org/abs/1909.08349v1) - [pdf](http://arxiv.org/pdf/1909.08349v1)

> With a growing interest in modeling inherent subjectivity in natural language, we present a linguistically-motivated process to understand and analyze the writing style of individuals from three perspectives: lexical, syntactic, and semantic. We discuss the stylistically expressive elements within each of these levels and use existing methods to quantify the linguistic intuitions related to some of these elements. We show that such a multi-level analysis is useful for developing a well-knit understanding of style - which is independent of the natural language task at hand, and also demonstrate its value in solving three downstream tasks: authors' style analysis, authorship attribution, and emotion prediction. We conduct experiments on a variety of datasets, comprising texts from social networking sites, user reviews, legal documents, literary books, and newswire. The results on the aforementioned tasks and datasets illustrate that such a multi-level understanding of style, which has been largely ignored in recent works, models style-related subjectivity in text and can be leveraged to improve performance on multiple downstream tasks both qualitatively and quantitatively.

</details>

<details>

<summary>2019-09-18 11:14:53 - Subword ELMo</summary>

- *Jiangtong Li, Hai Zhao, Zuchao Li, Wei Bi, Xiaojiang Liu*

- `1909.08357v1` - [abs](http://arxiv.org/abs/1909.08357v1) - [pdf](http://arxiv.org/pdf/1909.08357v1)

> Embedding from Language Models (ELMo) has shown to be effective for improving many natural language processing (NLP) tasks, and ELMo takes character information to compose word representation to train language models.However, the character is an insufficient and unnatural linguistic unit for word representation.Thus we introduce Embedding from Subword-aware Language Models (ESuLMo) which learns word representation from subwords using unsupervised segmentation over words.We show that ESuLMo can enhance four benchmark NLP tasks more effectively than ELMo, including syntactic dependency parsing, semantic role labeling, implicit discourse relation recognition and textual entailment, which brings a meaningful improvement over ELMo.

</details>

<details>

<summary>2019-09-18 15:16:38 - Deep Learning Assisted Heuristic Tree Search for the Container Pre-marshalling Problem</summary>

- *André Hottung, Shunji Tanaka, Kevin Tierney*

- `1709.09972v2` - [abs](http://arxiv.org/abs/1709.09972v2) - [pdf](http://arxiv.org/pdf/1709.09972v2)

> The container pre-marshalling problem (CPMP) is concerned with the re-ordering of containers in container terminals during off-peak times so that containers can be quickly retrieved when the port is busy. The problem has received significant attention in the literature and is addressed by a large number of exact and heuristic methods. Existing methods for the CPMP heavily rely on problem-specific components (e.g., proven lower bounds) that need to be developed by domain experts with knowledge of optimization techniques and a deep understanding of the problem at hand. With the goal to automate the costly and time-intensive design of heuristics for the CPMP, we propose a new method called Deep Learning Heuristic Tree Search (DLTS). It uses deep neural networks to learn solution strategies and lower bounds customized to the CPMP solely through analyzing existing (near-) optimal solutions to CPMP instances. The networks are then integrated into a tree search procedure to decide which branch to choose next and to prune the search tree. DLTS produces the highest quality heuristic solutions to the CPMP to date with gaps to optimality below 2% on real-world sized instances.

</details>

<details>

<summary>2019-09-18 16:58:01 - Oracle-Supported Dynamic Exploit Generation for Smart Contracts</summary>

- *Haijun Wang, Yi Li, Shang-Wei Lin, Cyrille Artho, Lei Ma, Yang Liu*

- `1909.06605v2` - [abs](http://arxiv.org/abs/1909.06605v2) - [pdf](http://arxiv.org/pdf/1909.06605v2)

> Despite the high stakes involved in smart contracts, they are often developed in an undisciplined manner, leaving the security and reliability of blockchain transactions at risk. In this paper, we introduce ContraMaster: an oracle-supported dynamic exploit generation framework for smart contracts. Existing approaches mutate only single transactions; ContraMaster exceeds these by mutating the transaction sequences. ContraMaster uses data-flow, control-flow, and the dynamic contract state to guide its mutations. It then monitors the executions of target contract programs, and validates the results against a general-purpose semantic test oracle to discover vulnerabilities. Being a dynamic technique, it guarantees that each discovered vulnerability is a violation of the test oracle and is able to generate the attack script to exploit this vulnerability. In contrast to rule-based approaches, ContraMaster has not shown any false positives, and it easily generalizes to unknown types of vulnerabilities (e.g., logic errors). We evaluate ContraMaster on 218 vulnerable smart contracts. The experimental results confirm its practical applicability and advantages over the state-of-the-art techniques, and also reveal three new types of attacks.

</details>

<details>

<summary>2019-09-18 17:35:15 - A prototype for a serious digital game to teach linguistic ontologies</summary>

- *Diana Medina, Grissa Maturana, Fernán Villa, Carlos Mario Zapata*

- `1909.07371v2` - [abs](http://arxiv.org/abs/1909.07371v2) - [pdf](http://arxiv.org/pdf/1909.07371v2)

> The objective of ontologies is to increase the compression of a given domain by eliminating interpretation problems. Among kinds of ontologies are linguistics ontologies which are ontologies used to simplify the interface between domain knowledge and linguistic components. Digital games have received increasing interest from educators in recent years for their potential to enhance the language learning and linguistic learning experience. Within the literature are games to teach ontologies of a specific domain, and games that use ontologies to facilitate the understanding of a given domain. Other educational games teach linguistics or vocabulary in contexts in which language is useful and meaningful. Although games help to understand difficult topics, the use of games that seek to meet the learning objectives of linguistics is not very popular and those focused on teaching linguistic ontologies are scarce. To solve the lack of the recreational resource for teaching linguistics in this document a prototype of a digital game called onto-ling is proposed. The goal is for the player to learn the relationship between concepts according to semantics, types of concepts and relationships through a game of levels.

</details>

<details>

<summary>2019-09-19 01:34:52 - Multi-sense Definition Modeling using Word Sense Decompositions</summary>

- *Ruimin Zhu, Thanapon Noraset, Alisa Liu, Wenxin Jiang, Doug Downey*

- `1909.09483v1` - [abs](http://arxiv.org/abs/1909.09483v1) - [pdf](http://arxiv.org/pdf/1909.09483v1)

> Word embeddings capture syntactic and semantic information about words. Definition modeling aims to make the semantic content in each embedding explicit, by outputting a natural language definition based on the embedding. However, existing definition models are limited in their ability to generate accurate definitions for different senses of the same word. In this paper, we introduce a new method that enables definition modeling for multiple senses. We show how a Gumble-Softmax approach outperforms baselines at matching sense-specific embeddings to definitions during training. In experiments, our multi-sense definition model improves recall over a state-of-the-art single-sense definition model by a factor of three, without harming precision.

</details>

<details>

<summary>2019-09-19 04:15:02 - Made for Each Other: Broad-coverage Semantic Structures Meet Preposition Supersenses</summary>

- *Jakob Prange, Nathan Schneider, Omri Abend*

- `1909.08796v1` - [abs](http://arxiv.org/abs/1909.08796v1) - [pdf](http://arxiv.org/pdf/1909.08796v1)

> Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) is a typologically-informed, broad-coverage semantic annotation scheme that describes coarse-grained predicate-argument structure but currently lacks semantic roles. We argue that lexicon-free annotation of the semantic roles marked by prepositions, as formulated by Schneider et al. (2018b), is complementary and suitable for integration within UCCA. We show empirically for English that the schemes, though annotated independently, are compatible and can be combined in a single semantic graph. A comparison of several approaches to parsing the integrated representation lays the groundwork for future research on this task.

</details>

<details>

<summary>2019-09-19 07:59:18 - PTracer: A Linux Kernel Patch Trace Bot</summary>

- *Yang Wen, Jicheng Cao, Shengyu Cheng*

- `1903.03610v4` - [abs](http://arxiv.org/abs/1903.03610v4) - [pdf](http://arxiv.org/pdf/1903.03610v4)

> We present PTracer, a Linux kernel patch trace bot based on an improved PatchNet. PTracer continuously monitors new patches in the git repository of the mainline Linux kernel, filters out unconcerned ones, classifies the rest as bug-fixing or non bug-fixing patches, and reports bug-fixing patches to the kernel experts of commercial operating systems. We use the patches in February 2019 of the mainline Linux kernel to perform the test. As a result, PTracer recommended 151 patches to CGEL kernel experts out of 5,142, and 102 of which were accepted. PTracer has been successfully applied to a commercial operating system and has the advantages of improving software quality and saving labor cost.

</details>

<details>

<summary>2019-09-19 08:39:00 - Procedural Reasoning Networks for Understanding Multimodal Procedures</summary>

- *Mustafa Sercan Amac, Semih Yagcioglu, Aykut Erdem, Erkut Erdem*

- `1909.08859v1` - [abs](http://arxiv.org/abs/1909.08859v1) - [pdf](http://arxiv.org/pdf/1909.08859v1)

> This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.

</details>

<details>

<summary>2019-09-19 10:16:21 - A Split-and-Recombine Approach for Follow-up Query Analysis</summary>

- *Qian Liu, Bei Chen, Haoyan Liu, Lei Fang, Jian-Guang Lou, Bin Zhou, Dongmei Zhang*

- `1909.08905v1` - [abs](http://arxiv.org/abs/1909.08905v1) - [pdf](http://arxiv.org/pdf/1909.08905v1)

> Context-dependent semantic parsing has proven to be an important yet challenging task. To leverage the advances in context-independent semantic parsing, we propose to perform follow-up query analysis, aiming to restate context-dependent natural language queries with contextual information. To accomplish the task, we propose STAR, a novel approach with a well-designed two-phase process. It is parser-independent and able to handle multifarious follow-up scenarios in different domains. Experiments on the FollowUp dataset show that STAR outperforms the state-of-the-art baseline by a large margin of nearly 8%. The superiority on parsing results verifies the feasibility of follow-up query analysis. We also explore the extensibility of STAR on the SQA dataset, which is very promising.

</details>

<details>

<summary>2019-09-19 11:58:18 - Deep Contextualized Pairwise Semantic Similarity for Arabic Language Questions</summary>

- *Hesham Al-Bataineh, Wael Farhan, Ahmad Mustafa, Haitham Seelawi, Hussein T. Al-Natsheh*

- `1909.09490v1` - [abs](http://arxiv.org/abs/1909.09490v1) - [pdf](http://arxiv.org/pdf/1909.09490v1)

> Question semantic similarity is a challenging and active research problem that is very useful in many NLP applications, such as detecting duplicate questions in community question answering platforms such as Quora. Arabic is considered to be an under-resourced language, has many dialects, and rich in morphology. Combined together, these challenges make identifying semantically similar questions in Arabic even more difficult. In this paper, we introduce a novel approach to tackle this problem, and test it on two benchmarks; one for Modern Standard Arabic (MSA), and another for the 24 major Arabic dialects. We are able to show that our new system outperforms state-of-the-art approaches by achieving 93% F1-score on the MSA benchmark and 82% on the dialectical one. This is achieved by utilizing contextualized word representations (ELMo embeddings) trained on a text corpus containing MSA and dialectic sentences. This in combination with a pairwise fine-grained similarity layer, helps our question-to-question similarity model to generalize predictions on different dialects while being trained only on question-to-question MSA data.

</details>

<details>

<summary>2019-09-19 13:24:29 - Analysing Neural Language Models: Contextual Decomposition Reveals Default Reasoning in Number and Gender Assignment</summary>

- *Jaap Jumelet, Willem Zuidema, Dieuwke Hupkes*

- `1909.08975v1` - [abs](http://arxiv.org/abs/1909.08975v1) - [pdf](http://arxiv.org/pdf/1909.08975v1)

> Extensive research has recently shown that recurrent neural language models are able to process a wide range of grammatical phenomena. How these models are able to perform these remarkable feats so well, however, is still an open question. To gain more insight into what information LSTMs base their decisions on, we propose a generalisation of Contextual Decomposition (GCD). In particular, this setup enables us to accurately distil which part of a prediction stems from semantic heuristics, which part truly emanates from syntactic cues and which part arise from the model biases themselves instead. We investigate this technique on tasks pertaining to syntactic agreement and co-reference resolution and discover that the model strongly relies on a default reasoning effect to perform these tasks.

</details>

<details>

<summary>2019-09-19 14:19:26 - Evidence-based lean logic profiles for conceptual data modelling languages</summary>

- *Pablo Rubén Fillottrani, C. Maria Keet*

- `1809.03001v2` - [abs](http://arxiv.org/abs/1809.03001v2) - [pdf](http://arxiv.org/pdf/1809.03001v2)

> Multiple logic-based reconstructions of conceptual data modelling languages such as EER, UML Class Diagrams, and ORM exist. They mainly cover various fragments of the languages and none are formalised such that the logic applies simultaneously for all three modelling language families as unifying mechanism. This hampers interchangeability, interoperability, and tooling support. In addition, due to the lack of a systematic design process of the logic used for the formalisation, hidden choices permeate the formalisations that have rendered them incompatible. We aim to address these problems, first, by structuring the logic design process in a methodological way. We generalise and extend the DSL design process to apply to logic language design more generally and, in particular, by incorporating an ontological analysis of language features in the process. Second, we specify minimal logic profiles availing of this extended process, including the ontological commitments embedded in the languages, of evidence gathered of language feature usage, and of computational complexity insights from Description Logics (DL). The profiles characterise the essential logic structure needed to handle the semantics of conceptual models, therewith enabling the development of interoperability tools. There is no known DL language that matches exactly the features of those profiles and the common core is small (in the tractable DL $\mathcal{ALNI}$). Although hardly any inconsistencies can be derived with the profiles, it is promising for scalable runtime use of conceptual data models.

</details>

<details>

<summary>2019-09-19 15:29:27 - Semantic Relatedness Based Re-ranker for Text Spotting</summary>

- *Ahmed Sabir, Francesc Moreno-Noguer, Lluís Padró*

- `1909.07950v2` - [abs](http://arxiv.org/abs/1909.07950v2) - [pdf](http://arxiv.org/pdf/1909.07950v2)

> Applications such as textual entailment, plagiarism detection or document clustering rely on the notion of semantic similarity, and are usually approached with dimension reduction techniques like LDA or with embedding-based neural approaches. We present a scenario where semantic similarity is not enough, and we devise a neural approach to learn semantic relatedness. The scenario is text spotting in the wild, where a text in an image (e.g. street sign, advertisement or bus destination) must be identified and recognized. Our goal is to improve the performance of vision systems by leveraging semantic information. Our rationale is that the text to be spotted is often related to the image context in which it appears (word pairs such as Delta-airplane, or quarters-parking are not similar, but are clearly related). We show how learning a word-to-word or word-to-sentence relatedness score can improve the performance of text spotting systems up to 2.9 points, outperforming other measures in a benchmark dataset.

</details>

<details>

<summary>2019-09-19 16:10:15 - Look, Read and Enrich. Learning from Scientific Figures and their Captions</summary>

- *Jose Manuel Gomez-Perez, Raul Ortega*

- `1909.09070v1` - [abs](http://arxiv.org/abs/1909.09070v1) - [pdf](http://arxiv.org/pdf/1909.09070v1)

> Compared to natural images, understanding scientific figures is particularly hard for machines. However, there is a valuable source of information in scientific literature that until now has remained untapped: the correspondence between a figure and its caption. In this paper we investigate what can be learnt by looking at a large number of figures and reading their captions, and introduce a figure-caption correspondence learning task that makes use of our observations. Training visual and language networks without supervision other than pairs of unconstrained figures and captions is shown to successfully solve this task. We also show that transferring lexical and semantic knowledge from a knowledge graph significantly enriches the resulting features. Finally, we demonstrate the positive impact of such features in other tasks involving scientific text and figures, like multi-modal classification and machine comprehension for question answering, outperforming supervised baselines and ad-hoc approaches.

</details>

<details>

<summary>2019-09-19 22:45:21 - HyperLearn: A Distributed Approach for Representation Learning in Datasets With Many Modalities</summary>

- *Devanshu Arya, Stevan Rudinac, Marcel Worring*

- `1909.09252v1` - [abs](http://arxiv.org/abs/1909.09252v1) - [pdf](http://arxiv.org/pdf/1909.09252v1)

> Multimodal datasets contain an enormous amount of relational information, which grows exponentially with the introduction of new modalities. Learning representations in such a scenario is inherently complex due to the presence of multiple heterogeneous information channels. These channels can encode both (a) inter-relations between the items of different modalities and (b) intra-relations between the items of the same modality. Encoding multimedia items into a continuous low-dimensional semantic space such that both types of relations are captured and preserved is extremely challenging, especially if the goal is a unified end-to-end learning framework. The two key challenges that need to be addressed are: 1) the framework must be able to merge complex intra and inter relations without losing any valuable information and 2) the learning model should be invariant to the addition of new and potentially very different modalities. In this paper, we propose a flexible framework which can scale to data streams from many modalities. To that end we introduce a hypergraph-based model for data representation and deploy Graph Convolutional Networks to fuse relational information within and across modalities. Our approach provides an efficient solution for distributing otherwise extremely computationally expensive or even unfeasible training processes across multiple-GPUs, without any sacrifices in accuracy. Moreover, adding new modalities to our model requires only an additional GPU unit keeping the computational time unchanged, which brings representation learning to truly multimodal datasets. We demonstrate the feasibility of our approach in the experiments on multimedia datasets featuring second, third and fourth order relations.

</details>

<details>

<summary>2019-09-20 00:00:41 - Towards Multimodal Understanding of Passenger-Vehicle Interactions in Autonomous Vehicles: Intent/Slot Recognition Utilizing Audio-Visual Data</summary>

- *Eda Okur, Shachi H Kumar, Saurav Sahay, Lama Nachman*

- `1909.13714v1` - [abs](http://arxiv.org/abs/1909.13714v1) - [pdf](http://arxiv.org/pdf/1909.13714v1)

> Understanding passenger intents from spoken interactions and car's vision (both inside and outside the vehicle) are important building blocks towards developing contextual dialog systems for natural interactions in autonomous vehicles (AV). In this study, we continued exploring AMIE (Automated-vehicle Multimodal In-cabin Experience), the in-cabin agent responsible for handling certain multimodal passenger-vehicle interactions. When the passengers give instructions to AMIE, the agent should parse such commands properly considering available three modalities (language/text, audio, video) and trigger the appropriate functionality of the AV system. We had collected a multimodal in-cabin dataset with multi-turn dialogues between the passengers and AMIE using a Wizard-of-Oz scheme via realistic scavenger hunt game. In our previous explorations, we experimented with various RNN-based models to detect utterance-level intents (set destination, change route, go faster, go slower, stop, park, pull over, drop off, open door, and others) along with intent keywords and relevant slots (location, position/direction, object, gesture/gaze, time-guidance, person) associated with the action to be performed in our AV scenarios. In this recent work, we propose to discuss the benefits of multimodal understanding of in-cabin utterances by incorporating verbal/language input (text and speech embeddings) together with the non-verbal/acoustic and visual input from inside and outside the vehicle (i.e., passenger gestures and gaze from in-cabin video stream, referred objects outside of the vehicle from the road view camera stream). Our experimental results outperformed text-only baselines and with multimodality, we achieved improved performances for utterance-level intent detection and slot filling.

</details>

<details>

<summary>2019-09-20 02:08:24 - Making Code Re-randomization Practical with MARDU</summary>

- *Christopher Jelesnianski, Jinwoo Yom, Changwoo Min, Yeongjin Jang*

- `1909.09294v1` - [abs](http://arxiv.org/abs/1909.09294v1) - [pdf](http://arxiv.org/pdf/1909.09294v1)

> Defense techniques such as Data Execution Prevention (DEP) and Address Space Layout Randomization (ASLR) were the early role models preventing primitive code injection and return-oriented programming (ROP) attacks. Notably, these techniques did so in an elegant and utilitarian manner, keeping performance and scalability in the forefront, making them one of the few widely-adopted defense techniques. As code re-use has evolved in complexity from JIT-ROP, to BROP and data-only attacks, defense techniques seem to have tunneled on defending at all costs, losing-their-way in pragmatic defense design. Some fail to provide comprehensive coverage, being too narrow in scope, while others provide unrealistic overheads leaving users willing to take their chances to maintain performance expectations.   We present Mardu, an on-demand system-wide re-randomization technique that improves re-randomization and refocuses efforts to simultaneously embrace key characteristics of defense techniques: security, performance, and scalability. Our code sharing with diversification is achieved by implementing reactive and scalable, rather than continuous or one-time diversification while the use of hardware supported eXecute-only Memory (XoM) and shadow stack prevent memory disclosure; entwining and enabling code sharing further minimizes needed tracking, patching costs, and memory overhead. Mardu's evaluation shows performance and scalability to have low average overhead in both compute-intensive (5.5% on SPEC) and real-world applications (4.4% on NGINX). With this design, Mardu demonstrates that strong and scalable security guarantees are possible to achieve at a practical cost to encourage deployment.

</details>

<details>

<summary>2019-09-20 10:21:55 - Learning to Create Sentence Semantic Relation Graphs for Multi-Document Summarization</summary>

- *Diego Antognini, Boi Faltings*

- `1909.12231v1` - [abs](http://arxiv.org/abs/1909.12231v1) - [pdf](http://arxiv.org/pdf/1909.12231v1)

> Linking facts across documents is a challenging task, as the language used to express the same information in a sentence can vary significantly, which complicates the task of multi-document summarization. Consequently, existing approaches heavily rely on hand-crafted features, which are domain-dependent and hard to craft, or additional annotated data, which is costly to gather. To overcome these limitations, we present a novel method, which makes use of two types of sentence embeddings: universal embeddings, which are trained on a large unrelated corpus, and domain-specific embeddings, which are learned during training.   To this end, we develop SemSentSum, a fully data-driven model able to leverage both types of sentence embeddings by building a sentence semantic relation graph. SemSentSum achieves competitive results on two types of summary, consisting of 665 bytes and 100 words. Unlike other state-of-the-art models, neither hand-crafted features nor additional annotated data are necessary, and the method is easily adaptable for other tasks. To our knowledge, we are the first to use multiple sentence embeddings for the task of multi-document summarization.

</details>

<details>

<summary>2019-09-20 13:44:38 - Generalization to Novel Objects using Prior Relational Knowledge</summary>

- *Varun Kumar Vijay, Abhinav Ganesh, Hanlin Tang, Arjun Bansal*

- `1906.11315v2` - [abs](http://arxiv.org/abs/1906.11315v2) - [pdf](http://arxiv.org/pdf/1906.11315v2)

> To solve tasks in new environments involving objects unseen during training, agents must reason over prior information about those objects and their relations. We introduce the Prior Knowledge Graph network, an architecture for combining prior information, structured as a knowledge graph, with a symbolic parsing of the visual scene, and demonstrate that this approach is able to apply learned relations to novel objects whereas the baseline algorithms fail. Ablation experiments show that the agents ground the knowledge graph relations to semantically-relevant behaviors. In both a Sokoban game and the more complex Pacman environment, our network is also more sample efficient than the baselines, reaching the same performance in 5-10x fewer episodes. Once the agents are trained with our approach, we can manipulate agent behavior by modifying the knowledge graph in semantically meaningful ways. These results suggest that our network provides a framework for agents to reason over structured knowledge graphs while still leveraging gradient based learning approaches.

</details>

<details>

<summary>2019-09-20 13:50:09 - Detecting Fault Injection Attacks with Runtime Verification</summary>

- *Ali Kassem, Yliès Falcone*

- `1907.03309v2` - [abs](http://arxiv.org/abs/1907.03309v2) - [pdf](http://arxiv.org/pdf/1907.03309v2)

> Fault injections are increasingly used to attack/test secure applications. In this paper, we define formal models of runtime monitors that can detect fault injections that result in test inversion attacks and arbitrary jumps in the control flow. Runtime verification monitors offer several advantages. The code implementing a monitor is small compared to the entire application code. Monitors have a formal semantics; and we prove that they effectively detect attacks. Each monitor is a module dedicated to detecting an attack and can be deployed as needed to secure the application. A monitor can run separately from the application or it can be ``weaved'' inside the application. Our monitors have been validated by detecting simulated attacks on a program that verifies a user PIN.

</details>

<details>

<summary>2019-09-20 15:08:51 - Natural Language Processing via LDA Topic Model in Recommendation Systems</summary>

- *Hamed Jelodar, Yongli Wang, Mahdi Rabbani, SeyedValyAllah Ayobi*

- `1909.09551v1` - [abs](http://arxiv.org/abs/1909.09551v1) - [pdf](http://arxiv.org/pdf/1909.09551v1)

> Today, Internet is one of the widest available media worldwide. Recommendation systems are increasingly being used in various applications such as movie recommendation, mobile recommendation, article recommendation and etc. Collaborative Filtering (CF) and Content-Based (CB) are Well-known techniques for building recommendation systems. Topic modeling based on LDA, is a powerful technique for semantic mining and perform topic extraction. In the past few years, many articles have been published based on LDA technique for building recommendation systems. In this paper, we present taxonomy of recommendation systems and applications based on LDA. In addition, we utilize LDA and Gibbs sampling algorithms to evaluate ISWC and WWW conference publications in computer science. Our study suggest that the recommendation systems based on LDA could be effective in building smart recommendation system in online communities.

</details>

<details>

<summary>2019-09-21 07:56:09 - Visuallly Grounded Generation of Entailments from Premises</summary>

- *Somaye Jafaritazehjani, Albert Gatt, Marc Tanti*

- `1909.09788v1` - [abs](http://arxiv.org/abs/1909.09788v1) - [pdf](http://arxiv.org/pdf/1909.09788v1)

> Natural Language Inference (NLI) is the task of determining the semantic relationship between a premise and a hypothesis. In this paper, we focus on the {\em generation} of hypotheses from premises in a multimodal setting, to generate a sentence (hypothesis) given an image and/or its description (premise) as the input. The main goals of this paper are (a) to investigate whether it is reasonable to frame NLI as a generation task; and (b) to consider the degree to which grounding textual premises in visual information is beneficial to generation. We compare different neural architectures, showing through automatic and human evaluation that entailments can indeed be generated successfully. We also show that multimodal models outperform unimodal models in this task, albeit marginally.

</details>

<details>

<summary>2019-09-21 21:57:38 - Generating Timelines by Modeling Semantic Change</summary>

- *Guy D. Rosin, Kira Radinsky*

- `1909.09907v1` - [abs](http://arxiv.org/abs/1909.09907v1) - [pdf](http://arxiv.org/pdf/1909.09907v1)

> Though languages can evolve slowly, they can also react strongly to dramatic world events. By studying the connection between words and events, it is possible to identify which events change our vocabulary and in what way. In this work, we tackle the task of creating timelines - records of historical "turning points", represented by either words or events, to understand the dynamics of a target word. Our approach identifies these points by leveraging both static and time-varying word embeddings to measure the influence of words and events. In addition to quantifying changes, we show how our technique can help isolate semantic changes. Our qualitative and quantitative evaluations show that we are able to capture this semantic change and event influence.

</details>

<details>

<summary>2019-09-22 01:38:08 - Techniques and Applications for Crawling, Ingesting and Analyzing Blockchain Data</summary>

- *Evan Brinckman, Andrey Kuehlkamp, Jarek Nabrzyski, Ian J. Taylor*

- `1909.09925v1` - [abs](http://arxiv.org/abs/1909.09925v1) - [pdf](http://arxiv.org/pdf/1909.09925v1)

> As the public Ethereum network surpasses half a billion transactions and enterprise Blockchain systems becoming highly capable of meeting the demands of global deployments, production Blockchain applications are fast becoming commonplace across a diverse range of business and scientific verticals. In this paper, we reflect on work we have been conducting recently surrounding the ingestion, retrieval and analysis of Blockchain data. We describe the scaling and semantic challenges when extracting Blockchain data in a way that preserves the original metadata of each transaction by cross referencing the Smart Contract interface with the on-chain data. We then discuss a scientific use case in the area of Scientific workflows by describing how we can harvest data from tasks and dependencies in a generic way. We then discuss how crawled public blockchain data can be analyzed using two unsupervised machine learning algorithms, which are designed to identify outlier accounts or smart contracts in the system. We compare and contrast the two machine learning methods and cross correlate with public Websites to illustrate the effectiveness such approaches.

</details>

<details>

<summary>2019-09-22 13:17:39 - Tag-based Semantic Features for Scene Image Classification</summary>

- *Chiranjibi Sitaula, Yong Xiang, Anish Basnet, Sunil Aryal, Xuequan Lu*

- `1909.09999v1` - [abs](http://arxiv.org/abs/1909.09999v1) - [pdf](http://arxiv.org/pdf/1909.09999v1)

> The existing image feature extraction methods are primarily based on the content and structure information of images, and rarely consider the contextual semantic information. Regarding some types of images such as scenes and objects, the annotations and descriptions of them available on the web may provide reliable contextual semantic information for feature extraction. In this paper, we introduce novel semantic features of an image based on the annotations and descriptions of its similar images available on the web. Specifically, we propose a new method which consists of two consecutive steps to extract our semantic features. For each image in the training set, we initially search the top $k$ most similar images from the internet and extract their annotations/descriptions (e.g., tags or keywords). The annotation information is employed to design a filter bank for each image category and generate filter words (codebook). Finally, each image is represented by the histogram of the occurrences of filter words in all categories. We evaluate the performance of the proposed features in scene image classification on three commonly-used scene image datasets (i.e., MIT-67, Scene15 and Event8). Our method typically produces a lower feature dimension than existing feature extraction methods. Experimental results show that the proposed features generate better classification accuracies than vision based and tag based features, and comparable results to deep learning based features.

</details>

<details>

<summary>2019-09-22 14:28:15 - Transductive Auxiliary Task Self-Training for Neural Multi-Task Models</summary>

- *Johannes Bjerva, Katharina Kann, Isabelle Augenstein*

- `1908.06136v2` - [abs](http://arxiv.org/abs/1908.06136v2) - [pdf](http://arxiv.org/pdf/1908.06136v2)

> Multi-task learning and self-training are two common ways to improve a machine learning model's performance in settings with limited training data. Drawing heavily on ideas from those two approaches, we suggest transductive auxiliary task self-training: training a multi-task model on (i) a combination of main and auxiliary task training data, and (ii) test instances with auxiliary task labels which a single-task version of the model has previously generated. We perform extensive experiments on 86 combinations of languages and tasks. Our results are that, on average, transductive auxiliary task self-training improves absolute accuracy by up to 9.56% over the pure multi-task model for dependency relation tagging and by up to 13.03% for semantic tagging.

</details>

<details>

<summary>2019-09-22 18:01:36 - Inducing Constituency Trees through Neural Machine Translation</summary>

- *Phu Mon Htut, Kyunghyun Cho, Samuel R. Bowman*

- `1909.10056v1` - [abs](http://arxiv.org/abs/1909.10056v1) - [pdf](http://arxiv.org/pdf/1909.10056v1)

> Latent tree learning(LTL) methods learn to parse sentences using only indirect supervision from a downstream task. Recent advances in latent tree learning have made it possible to recover moderately high quality tree structures by training with language modeling or auto-encoding objectives. In this work, we explore the hypothesis that decoding in machine translation, as a conditional language modeling task, will produce better tree structures since it offers a similar training signal as language modeling, but with more semantic signal. We adapt two existing latent-tree language models--PRPN andON-LSTM--for use in translation. We find that they indeed recover trees that are better in F1 score than those seen in language modeling on WSJ test set, while maintaining strong translation quality. We observe that translation is a better objective than language modeling for inducing trees, marking the first success at latent tree learning using a machine translation objective. Additionally, our findings suggest that, although translation provides better signal for inducing trees than language modeling, translation models can perform well without exploiting the latent tree structure.

</details>

<details>

<summary>2019-09-22 22:39:09 - User-Guided Clustering in Heterogeneous Information Networks via Motif-Based Comprehensive Transcription</summary>

- *Yu Shi, Xinwei He, Naijing Zhang, Carl Yang, Jiawei Han*

- `1811.11320v3` - [abs](http://arxiv.org/abs/1811.11320v3) - [pdf](http://arxiv.org/pdf/1811.11320v3)

> Heterogeneous information networks (HINs) with rich semantics are ubiquitous in real-world applications. For a given HIN, many reasonable clustering results with distinct semantic meaning can simultaneously exist. User-guided clustering is hence of great practical value for HINs where users provide labels to a small portion of nodes. To cater to a broad spectrum of user guidance evidenced by different expected clustering results, carefully exploiting the signals residing in the data is potentially useful. Meanwhile, as one type of complex networks, HINs often encapsulate higher-order interactions that reflect the interlocked nature among nodes and edges. Network motifs, sometimes referred to as meta-graphs, have been used as tools to capture such higher-order interactions and reveal the many different semantics. We therefore approach the problem of user-guided clustering in HINs with network motifs. In this process, we identify the utility and importance of directly modeling higher-order interactions without collapsing them to pairwise interactions. To achieve this, we comprehensively transcribe the higher-order interaction signals to a series of tensors via motifs and propose the MoCHIN model based on joint non-negative tensor factorization. This approach applies to arbitrarily many, arbitrary forms of HIN motifs. An inference algorithm with speed-up methods is also proposed to tackle the challenge that tensor size grows exponentially as the number of nodes in a motif increases. We validate the effectiveness of the proposed method on two real-world datasets and three tasks, and MoCHIN outperforms all baselines in three evaluation tasks under three different metrics. Additional experiments demonstrated the utility of motifs and the benefit of directly modeling higher-order information especially when user guidance is limited.

</details>

<details>

<summary>2019-09-23 05:29:04 - Automatic Short Answer Grading via Multiway Attention Networks</summary>

- *Tiaoqiao Liu, Wenbiao Ding, Zhiwei Wang, Jiliang Tang, Gale Yan Huang, Zitao Liu*

- `1909.10166v1` - [abs](http://arxiv.org/abs/1909.10166v1) - [pdf](http://arxiv.org/pdf/1909.10166v1)

> Automatic short answer grading (ASAG), which autonomously score student answers according to reference answers, provides a cost-effective and consistent approach to teaching professionals and can reduce their monotonous and tedious grading workloads. However, ASAG is a very challenging task due to two reasons: (1) student answers are made up of free text which requires a deep semantic understanding; and (2) the questions are usually open-ended and across many domains in K-12 scenarios. In this paper, we propose a generalized end-to-end ASAG learning framework which aims to (1) autonomously extract linguistic information from both student and reference answers; and (2) accurately model the semantic relations between free-text student and reference answers in open-ended domain. The proposed ASAG model is evaluated on a large real-world K-12 dataset and can outperform the state-of-the-art baselines in terms of various evaluation metrics.

</details>

<details>

<summary>2019-09-23 05:56:13 - Syntax-Aware Aspect-Level Sentiment Classification with Proximity-Weighted Convolution Network</summary>

- *Chen Zhang, Qiuchi Li, Dawei Song*

- `1909.10171v1` - [abs](http://arxiv.org/abs/1909.10171v1) - [pdf](http://arxiv.org/pdf/1909.10171v1)

> It has been widely accepted that Long Short-Term Memory (LSTM) network, coupled with attention mechanism and memory module, is useful for aspect-level sentiment classification. However, existing approaches largely rely on the modelling of semantic relatedness of an aspect with its context words, while to some extent ignore their syntactic dependencies within sentences. Consequently, this may lead to an undesirable result that the aspect attends on contextual words that are descriptive of other aspects. In this paper, we propose a proximity-weighted convolution network to offer an aspect-specific syntax-aware representation of contexts. In particular, two ways of determining proximity weight are explored, namely position proximity and dependency proximity. The representation is primarily abstracted by a bidirectional LSTM architecture and further enhanced by a proximity-weighted convolution. Experiments conducted on the SemEval 2014 benchmark demonstrate the effectiveness of our proposed approach compared with a range of state-of-the-art models.

</details>

<details>

<summary>2019-09-23 11:24:52 - Predicting Landscapes from Environmental Conditions Using Generative Networks</summary>

- *Christian Requena-Mesa, Markus Reichstein, Miguel Mahecha, Basil Kraft, Joachim Denzler*

- `1909.10296v1` - [abs](http://arxiv.org/abs/1909.10296v1) - [pdf](http://arxiv.org/pdf/1909.10296v1)

> Landscapes are meaningful ecological units that strongly depend on the environmental conditions. Such dependencies between landscapes and the environment have been noted since the beginning of Earth sciences and cast into conceptual models describing the interdependencies of climate, geology, vegetation and geomorphology. Here, we ask whether landscapes, as seen from space, can be statistically predicted from pertinent environmental conditions. To this end we adapted a deep learning generative model in order to establish the relationship between the environmental conditions and the view of landscapes from the Sentinel-2 satellite. We trained a conditional generative adversarial network to generate multispectral imagery given a set of climatic, terrain and anthropogenic predictors. The generated imagery of the landscapes share many characteristics with the real one. Results based on landscape patch metrics, indicative of landscape composition and structure, show that the proposed generative model creates landscapes that are more similar to the targets than the baseline models while overall reflectance and vegetation cover are predicted better. We demonstrate that for many purposes the generated landscapes behave as real with immediate application for global change studies. We envision the application of machine learning as a tool to forecast the effects of climate change on the spatial features of landscapes, while we assess its limitations and breaking points.

</details>

<details>

<summary>2019-09-23 14:35:23 - GNTeam at 2018 n2c2: Feature-augmented BiLSTM-CRF for drug-related entity recognition in hospital discharge summaries</summary>

- *Maksim Belousov, Nikola Milosevic, Ghada Alfattni, Haifa Alrdahi, Goran Nenadic*

- `1909.10390v1` - [abs](http://arxiv.org/abs/1909.10390v1) - [pdf](http://arxiv.org/pdf/1909.10390v1)

> Monitoring the administration of drugs and adverse drug reactions are key parts of pharmacovigilance. In this paper, we explore the extraction of drug mentions and drug-related information (reason for taking a drug, route, frequency, dosage, strength, form, duration, and adverse events) from hospital discharge summaries through deep learning that relies on various representations for clinical named entity recognition. This work was officially part of the 2018 n2c2 shared task, and we use the data supplied as part of the task. We developed two deep learning architecture based on recurrent neural networks and pre-trained language models. We also explore the effect of augmenting word representations with semantic features for clinical named entity recognition. Our feature-augmented BiLSTM-CRF model performed with F1-score of 92.67% and ranked 4th for entity extraction sub-task among submitted systems to n2c2 challenge. The recurrent neural networks that use the pre-trained domain-specific word embeddings and a CRF layer for label optimization perform drug, adverse event and related entities extraction with micro-averaged F1-score of over 91%. The augmentation of word vectors with semantic features extracted using available clinical NLP toolkits can further improve the performance. Word embeddings that are pre-trained on a large unannotated corpus of relevant documents and further fine-tuned to the task perform rather well. However, the augmentation of word embeddings with semantic features can help improve the performance (primarily by boosting precision) of drug-related named entity recognition from electronic health records.

</details>

<details>

<summary>2019-09-23 15:14:56 - Biomedical Mention Disambiguation using a Deep Learning Approach</summary>

- *Chih-Hsuan Wei, Kyubum Lee, Robert Leaman, Zhiyong Lu*

- `1909.10416v1` - [abs](http://arxiv.org/abs/1909.10416v1) - [pdf](http://arxiv.org/pdf/1909.10416v1)

> Automatically locating named entities in natural language text - named entity recognition - is an important task in the biomedical domain. Many named entity mentions are ambiguous between several bioconcept types, however, causing text spans to be annotated as more than one type when simultaneously recognizing multiple entity types. The straightforward solution is a rule-based approach applying a priority order based on the precision of each entity tagger (from highest to lowest). While this method is straightforward and useful, imprecise disambiguation remains a significant source of error. We address this issue by generating a partially labeled corpus of ambiguous concept mentions. We first collect named entity mentions from multiple human-curated databases (e.g. CTDbase, gene2pubmed), then correlate them with the text mined span from PubTator to provide the context where the mention appears. Our corpus contains more than 3 million concept mentions that ambiguous between one or more concept types in PubTator (about 3% of all mentions). We approached this task as a classification problem and developed a deep learning-based method which uses the semantics of the span being classified and the surrounding words to identify the most likely bioconcept type. More specifically, we develop a convolutional neural network (CNN) and along short-term memory (LSTM) network to respectively handle the semantic syntax features, then concatenate these within a fully connected layer for final classification. The priority ordering rule-based approach demonstrated F1-scores of 71.29% (micro-averaged) and 41.19% (macro-averaged), while the new disambiguation method demonstrated F1-scores of 91.94% (micro-averaged) and 85.42% (macro-averaged), a very substantial increase.

</details>

<details>

<summary>2019-09-23 17:25:50 - Formalism for Supporting the Development of Verifiably Safe Medical Guidelines with Statecharts</summary>

- *Chunhui Guo, Zhicheng Fu, Zhenyu Zhang, Shangping Ren, Lui Sha*

- `1909.10493v1` - [abs](http://arxiv.org/abs/1909.10493v1) - [pdf](http://arxiv.org/pdf/1909.10493v1)

> Improving the effectiveness and safety of patient care is the ultimate objective for medical cyber-physical systems. Many medical best practice guidelines exist, but most of the existing guidelines in handbooks are difficult for medical staff to remember and apply clinically. Furthermore, although the guidelines have gone through clinical validations, validations by medical professionals alone do not provide guarantees for the safety of medical cyber-physical systems. Hence, formal verification is also needed. The paper presents the formal semantics for a framework that we developed to support the development of verifiably safe medical guidelines.   The framework allows computer scientists to work together with medical professionals to transform medical best practice guidelines into executable statechart models, Yakindu in particular, so that medical functionalities and properties can be quickly prototyped and validated. Existing formal verification technologies, UPPAAL timed automata in particular, is integrated into the framework to provide formal verification capabilities to verify safety properties. However, some components used/built into the framework, such as the open-source Yakindu statecharts as well as the transformation rules from statecharts to timed automata, do not have built-in semantics. The ambiguity becomes unavoidable unless formal semantics is defined for the framework, which is what the paper is to present.

</details>

<details>

<summary>2019-09-23 17:26:35 - Semantic RL with Action Grammars: Data-Efficient Learning of Hierarchical Task Abstractions</summary>

- *Robert Tjarko Lange, Aldo Faisal*

- `1907.12477v2` - [abs](http://arxiv.org/abs/1907.12477v2) - [pdf](http://arxiv.org/pdf/1907.12477v2)

> Hierarchical Reinforcement Learning algorithms have successfully been applied to temporal credit assignment problems with sparse reward signals. However, state-of-the-art algorithms require manual specification of sub-task structures, a sample inefficient exploration phase or lack semantic interpretability. Humans, on the other hand, efficiently detect hierarchical sub-structures induced by their surroundings. It has been argued that this inference process universally applies to language, logical reasoning as well as motor control. Therefore, we propose a cognitive-inspired Reinforcement Learning architecture which uses grammar induction to identify sub-goal policies. By treating an on-policy trajectory as a sentence sampled from the policy-conditioned language of the environment, we identify hierarchical constituents with the help of unsupervised grammatical inference. The resulting set of temporal abstractions is called action grammar (Pastra & Aloimonos, 2012) and unifies symbolic and connectionist approaches to Reinforcement Learning. It can be used to facilitate efficient imitation, transfer and online learning.

</details>

<details>

<summary>2019-09-23 23:31:54 - Scalable Simulation of Realistic Volume Fraction Red Blood Cell Flows through Vascular Networks</summary>

- *Libin Lu, Matthew J. Morse, Abtin Rahimian, Georg Stadler, Denis Zorin*

- `1909.11085v1` - [abs](http://arxiv.org/abs/1909.11085v1) - [pdf](http://arxiv.org/pdf/1909.11085v1)

> High-resolution blood flow simulations have potential for developing better understanding biophysical phenomena at the microscale, such as vasodilation, vasoconstriction and overall vascular resistance. To this end, we present a scalable platform for the simulation of red blood cell (RBC) flows through complex capillaries by modeling the physical system as a viscous fluid with immersed deformable particles. We describe a parallel boundary integral equation solver for general elliptic partial differential equations, which we apply to Stokes flow through blood vessels. We also detail a parallel collision avoiding algorithm to ensure RBCs and the blood vessel remain contact-free. We have scaled our code on Stampede2 at the Texas Advanced Computing Center up to 34,816 cores. Our largest simulation enforces a contact-free state between four billion surface elements and solves for three billion degrees of freedom on one million RBCs and a blood vessel composed from two million patches.

</details>

<details>

<summary>2019-09-23 23:40:59 - Generating Geological Facies Models with Fidelity to Diversity and Statistics of Training Images using Improved Generative Adversarial Networks</summary>

- *Lingchen Zhu, Tuanfeng Zhang*

- `1909.10652v1` - [abs](http://arxiv.org/abs/1909.10652v1) - [pdf](http://arxiv.org/pdf/1909.10652v1)

> This paper presents a methodology and workflow that overcome the limitations of the conventional Generative Adversarial Networks (GANs) for geological facies modeling. It attempts to improve the training stability and guarantee the diversity of the generated geology through interpretable latent vectors. The resulting samples are ensured to have the equal probability (or an unbiased distribution) as from the training dataset. This is critical when applying GANs to generate unbiased and representative geological models that can be further used to facilitate objective uncertainty evaluation and optimal decision-making in oil field exploration and development.   We proposed and implemented a new variant of GANs called Info-WGAN for the geological facies modeling that combines Information Maximizing Generative Adversarial Network (InfoGAN) with Wasserstein distance and Gradient Penalty (GP) for learning interpretable latent codes as well as generating stable and unbiased distribution from the training data. Different from the original GAN design, InfoGAN can use the training images with full, partial, or no labels to perform disentanglement of the complex sedimentary types exhibited in the training dataset to achieve the variety and diversity of the generated samples. This is accomplished by adding additional categorical variables that provide disentangled semantic representations besides the mere randomized latent vector used in the original GANs. By such means, a regularization term is used to maximize the mutual information between such latent categorical codes and the generated geological facies in the loss function.   Furthermore, the resulting unbiased sampling by Info-WGAN makes the data conditioning much easier than the conventional GANs in geological modeling because of the variety and diversity as well as the equal probability of the unconditional sampling by the generator.

</details>

<details>

<summary>2019-09-24 06:03:35 - Situating Sentence Embedders with Nearest Neighbor Overlap</summary>

- *Lucy H. Lin, Noah A. Smith*

- `1909.10724v1` - [abs](http://arxiv.org/abs/1909.10724v1) - [pdf](http://arxiv.org/pdf/1909.10724v1)

> As distributed approaches to natural language semantics have developed and diversified, embedders for linguistic units larger than words have come to play an increasingly important role. To date, such embedders have been evaluated using benchmark tasks (e.g., GLUE) and linguistic probes. We propose a comparative approach, nearest neighbor overlap (N2O), that quantifies similarity between embedders in a task-agnostic manner. N2O requires only a collection of examples and is simple to understand: two embedders are more similar if, for the same set of inputs, there is greater overlap between the inputs' nearest neighbors. Though applicable to embedders of texts of any size, we focus on sentence embedders and use N2O to show the effects of different design choices and architectures.

</details>

<details>

<summary>2019-09-24 06:13:53 - Universal Semi-Supervised Semantic Segmentation</summary>

- *Tarun Kalluri, Girish Varma, Manmohan Chandraker, C V Jawahar*

- `1811.10323v3` - [abs](http://arxiv.org/abs/1811.10323v3) - [pdf](http://arxiv.org/pdf/1811.10323v3)

> In recent years, the need for semantic segmentation has arisen across several different applications and environments. However, the expense and redundancy of annotation often limits the quantity of labels available for training in any domain, while deployment is easier if a single model works well across domains. In this paper, we pose the novel problem of universal semi-supervised semantic segmentation and propose a solution framework, to meet the dual needs of lower annotation and deployment costs. In contrast to counterpoints such as fine tuning, joint training or unsupervised domain adaptation, universal semi-supervised segmentation ensures that across all domains: (i) a single model is deployed, (ii) unlabeled data is used, (iii) performance is improved, (iv) only a few labels are needed and (v) label spaces may differ. To address this, we minimize supervised as well as within and cross-domain unsupervised losses, introducing a novel feature alignment objective based on pixel-aware entropy regularization for the latter. We demonstrate quantitative advantages over other approaches on several combinations of segmentation datasets across different geographies (Germany, England, India) and environments (outdoors, indoors), as well as qualitative insights on the aligned representations.

</details>

<details>

<summary>2019-09-24 06:49:42 - Deep Metric Learning using Similarities from Nonlinear Rank Approximations</summary>

- *Konstantin Schall, Kai Uwe Barthel, Nico Hezel, Klaus Jung*

- `1909.09427v2` - [abs](http://arxiv.org/abs/1909.09427v2) - [pdf](http://arxiv.org/pdf/1909.09427v2)

> In recent years, deep metric learning has achieved promising results in learning high dimensional semantic feature embeddings where the spatial relationships of the feature vectors match the visual similarities of the images. Similarity search for images is performed by determining the vectors with the smallest distances to a query vector. However, high retrieval quality does not depend on the actual distances of the feature vectors, but rather on the ranking order of the feature vectors from similar images. In this paper, we introduce a metric learning algorithm that focuses on identifying and modifying those feature vectors that most strongly affect the retrieval quality. We compute normalized approximated ranks and convert them to similarities by applying a nonlinear transfer function. These similarities are used in a newly proposed loss function that better contracts similar and disperses dissimilar samples. Experiments demonstrate significant improvement over existing deep feature embedding methods on the CUB-200-2011, Cars196, and Stanford Online Products data sets for all embedding sizes.

</details>

<details>

<summary>2019-09-24 06:53:16 - Deep Aggregation of Regional Convolutional Activations for Content Based Image Retrieval</summary>

- *Konstantin Schall, Kai Uwe Barthel, Nico Hezel, Klaus Jung*

- `1909.09420v2` - [abs](http://arxiv.org/abs/1909.09420v2) - [pdf](http://arxiv.org/pdf/1909.09420v2)

> One of the key challenges of deep learning based image retrieval remains in aggregating convolutional activations into one highly representative feature vector. Ideally, this descriptor should encode semantic, spatial and low level information. Even though off-the-shelf pre-trained neural networks can already produce good representations in combination with aggregation methods, appropriate fine tuning for the task of image retrieval has shown to significantly boost retrieval performance. In this paper, we present a simple yet effective supervised aggregation method built on top of existing regional pooling approaches. In addition to the maximum activation of a given region, we calculate regional average activations of extracted feature maps. Subsequently, weights for each of the pooled feature vectors are learned to perform a weighted aggregation to a single feature vector. Furthermore, we apply our newly proposed NRA loss function for deep metric learning to fine tune the backbone neural network and to learn the aggregation weights. Our method achieves state-of-the-art results for the INRIA Holidays data set and competitive results for the Oxford Buildings and Paris data sets while reducing the training time significantly.

</details>

<details>

<summary>2019-09-24 09:37:05 - Neural Poetry: Learning to Generate Poems using Syllables</summary>

- *Andrea Zugarini, Stefano Melacci, Marco Maggini*

- `1908.08861v2` - [abs](http://arxiv.org/abs/1908.08861v2) - [pdf](http://arxiv.org/pdf/1908.08861v2)

> Motivated by the recent progresses on machine learning-based models that learn artistic styles, in this paper we focus on the problem of poem generation. This is a challenging task in which the machine has to capture the linguistic features that strongly characterize a certain poet, as well as the semantics of the poet's production, that are influenced by his personal experiences and by his literary background. Since poetry is constructed using syllables, that regulate the form and structure of poems, we propose a syllable-based neural language model, and we describe a poem generation mechanism that is designed around the poet style, automatically selecting the most representative generations. The poetic work of a target author is usually not enough to successfully train modern deep neural networks, so we propose a multi-stage procedure that exploits non-poetic works of the same author, and also other publicly available huge corpora to learn syntax and grammar of the target language. We focus on the Italian poet Dante Alighieri, widely famous for his Divine Comedy. A quantitative and qualitative experimental analysis of the generated tercets is reported, where we included expert judges with strong background in humanistic studies. The generated tercets are frequently considered to be real by a generic population of judges, with relative difference of 56.25\% with respect to the ones really authored by Dante, and expert judges perceived Dante's style and rhymes in the generated text.

</details>

<details>

<summary>2019-09-24 13:49:14 - Understanding Semantics from Speech Through Pre-training</summary>

- *Pengwei Wang, Liangchen Wei, Yong Cao, Jinghui Xie, Yuji Cao, Zaiqing Nie*

- `1909.10924v1` - [abs](http://arxiv.org/abs/1909.10924v1) - [pdf](http://arxiv.org/pdf/1909.10924v1)

> End-to-end Spoken Language Understanding (SLU) is proposed to infer the semantic meaning directly from audio features without intermediate text representation. Although the acoustic model component of an end-to-end SLU system can be pre-trained with Automatic Speech Recognition (ASR) targets, the SLU component can only learn semantic features from limited task-specific training data. In this paper, for the first time we propose to do large-scale unsupervised pre-training for the SLU component of an end-to-end SLU system, so that the SLU component may preserve semantic features from massive unlabeled audio data. As the output of the acoustic model component, i.e. phoneme posterior sequences, has much different characteristic from text sequences, we propose a novel pre-training model called BERT-PLM, which stands for Bidirectional Encoder Representations from Transformers through Permutation Language Modeling. BERT-PLM trains the SLU component on unlabeled data through a regression objective equivalent to the partial permutation language modeling objective, while leverages full bi-directional context information with BERT networks. The experiment results show that our approach out-perform the state-of-the-art end-to-end systems with over 12.5% error reduction.

</details>

<details>

<summary>2019-09-24 16:18:01 - Posture and sequence recognition for Bharatanatyam dance performances using machine learning approach</summary>

- *Tanwi Mallick, Partha Pratim Das, Arun Kumar Majumdar*

- `1909.11023v1` - [abs](http://arxiv.org/abs/1909.11023v1) - [pdf](http://arxiv.org/pdf/1909.11023v1)

> Understanding the underlying semantics of performing arts like dance is a challenging task. Dance is multimedia in nature and spans over time as well as space. Capturing and analyzing the multimedia content of the dance is useful for the preservation of cultural heritage, to build video recommendation systems, to assist learners to use tutoring systems. To develop an application for dance, three aspects of dance analysis need to be addressed: 1) Segmentation of the dance video to find the representative action elements, 2) Matching or recognition of the detected action elements, and 3) Recognition of the dance sequences formed by combining a number of action elements under certain rules. This paper attempts to solve three fundamental problems of dance analysis for understanding the underlying semantics of dance forms. Our focus is on an Indian Classical Dance (ICD) form known as Bharatanatyam. As dance is driven by music, we use the music as well as motion information for key posture extraction. Next, we recognize the key postures using machine learning as well as deep learning techniques. Finally, the dance sequence is recognized using the Hidden Markov Model (HMM). We capture the multi-modal data of Bharatanatyam dance using Kinect and build an annotated data set for research in ICD.

</details>

<details>

<summary>2019-09-24 16:52:18 - Assessing the Lexico-Semantic Relational Knowledge Captured by Word and Concept Embeddings</summary>

- *Ronald Denaux, Jose Manuel Gomez-Perez*

- `1909.11042v1` - [abs](http://arxiv.org/abs/1909.11042v1) - [pdf](http://arxiv.org/pdf/1909.11042v1)

> Deep learning currently dominates the benchmarks for various NLP tasks and, at the basis of such systems, words are frequently represented as embeddings --vectors in a low dimensional space-- learned from large text corpora and various algorithms have been proposed to learn both word and concept embeddings. One of the claimed benefits of such embeddings is that they capture knowledge about semantic relations. Such embeddings are most often evaluated through tasks such as predicting human-rated similarity and analogy which only test a few, often ill-defined, relations. In this paper, we propose a method for (i) reliably generating word and concept pair datasets for a wide number of relations by using a knowledge graph and (ii) evaluating to what extent pre-trained embeddings capture those relations. We evaluate the approach against a proprietary and a public knowledge graph and analyze the results, showing which lexico-semantic relational knowledge is captured by current embedding learning approaches.

</details>

<details>

<summary>2019-09-24 21:56:53 - A Visual Analytics Framework for Adversarial Text Generation</summary>

- *Brandon Laughlin, Christopher Collins, Karthik Sankaranarayanan, Khalil El-Khatib*

- `1909.11202v1` - [abs](http://arxiv.org/abs/1909.11202v1) - [pdf](http://arxiv.org/pdf/1909.11202v1)

> This paper presents a framework which enables a user to more easily make corrections to adversarial texts. While attack algorithms have been demonstrated to automatically build adversaries, changes made by the algorithms can often have poor semantics or syntax. Our framework is designed to facilitate human intervention by aiding users in making corrections. The framework extends existing attack algorithms to work within an evolutionary attack process paired with a visual analytics loop. Using an interactive dashboard a user is able to review the generation process in real time and receive suggestions from the system for edits to be made. The adversaries can be used to both diagnose robustness issues within a single classifier or to compare various classifier options. With the weaknesses identified, the framework can also be used as a first step in mitigating adversarial threats. The framework can be used as part of further research into defense methods in which the adversarial examples are used to evaluate new countermeasures. We demonstrate the framework with a word swapping attack for the task of sentiment classification.

</details>

<details>

<summary>2019-09-24 22:10:22 - Architecture and evolution of semantic networks in mathematics texts</summary>

- *Nicolas H. Christianson, Ann Sizemore Blevins, Danielle S. Bassett*

- `1908.04911v2` - [abs](http://arxiv.org/abs/1908.04911v2) - [pdf](http://arxiv.org/pdf/1908.04911v2)

> Knowledge is a network of interconnected concepts. Yet, precisely how the topological structure of knowledge constrains its acquisition remains unknown, hampering the development of learning enhancement strategies. Here we study the topological structure of semantic networks reflecting mathematical concepts and their relations in college-level linear algebra texts. We hypothesize that these networks will exhibit structural order, reflecting the logical sequence of topics that ensures accessibility. We find that the networks exhibit strong core-periphery architecture, where a dense core of concepts presented early is complemented with a sparse periphery presented evenly throughout the exposition; the latter is composed of many small modules each reflecting more narrow domains. Using tools from applied topology, we find that the expositional evolution of the semantic networks produces and subsequently fills knowledge gaps, and that the density of these gaps tracks negatively with community ratings of each textbook. Broadly, our study lays the groundwork for future efforts developing optimal design principles for textbook exposition and teaching in a classroom setting.

</details>

<details>

<summary>2019-09-24 23:48:28 - Fooling Network Interpretation in Image Classification</summary>

- *Akshayvarun Subramanya, Vipin Pillai, Hamed Pirsiavash*

- `1812.02843v2` - [abs](http://arxiv.org/abs/1812.02843v2) - [pdf](http://arxiv.org/pdf/1812.02843v2)

> Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process.

</details>

<details>

<summary>2019-09-25 05:16:15 - Question Answering is a Format; When is it Useful?</summary>

- *Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi, Alon Talmor, Sewon Min*

- `1909.11291v1` - [abs](http://arxiv.org/abs/1909.11291v1) - [pdf](http://arxiv.org/pdf/1909.11291v1)

> Recent years have seen a dramatic expansion of tasks and datasets posed as question answering, from reading comprehension, semantic role labeling, and even machine translation, to image and video understanding. With this expansion, there are many differing views on the utility and definition of "question answering" itself. Some argue that its scope should be narrow, or broad, or that it is overused in datasets today. In this opinion piece, we argue that question answering should be considered a format which is sometimes useful for studying particular phenomena, not a phenomenon or task in itself. We discuss when a task is correctly described as question answering, and when a task is usefully posed as question answering, instead of using some other format.

</details>

<details>

<summary>2019-09-25 10:12:05 - Beyond image classification: zooplankton identification with deep vector space embeddings</summary>

- *Ketil Malde, Hyeongji Kim*

- `1909.11380v1` - [abs](http://arxiv.org/abs/1909.11380v1) - [pdf](http://arxiv.org/pdf/1909.11380v1)

> Zooplankton images, like many other real world data types, have intrinsic properties that make the design of effective classification systems difficult. For instance, the number of classes encountered in practical settings is potentially very large, and classes can be ambiguous or overlap. In addition, the choice of taxonomy often differs between researchers and between institutions. Although high accuracy has been achieved in benchmarks using standard classifier architectures, biases caused by an inflexible classification scheme can have profound effects when the output is used in ecosystem assessments and monitoring.   Here, we propose using a deep convolutional network to construct a vector embedding of zooplankton images. The system maps (embeds) each image into a high-dimensional Euclidean space so that distances between vectors reflect semantic relationships between images. We show that the embedding can be used to derive classifications with comparable accuracy to a specific classifier, but that it simultaneously reveals important structures in the data. Furthermore, we apply the embedding to new classes previously unseen by the system, and evaluate its classification performance in such cases.   Traditional neural network classifiers perform well when the classes are clearly defined a priori and have sufficiently large labeled data sets available. For practical cases in ecology as well as in many other fields this is not the case, and we argue that the vector embedding method presented here is a more appropriate approach.

</details>

<details>

<summary>2019-09-25 12:01:36 - A Survey of Binary Code Similarity</summary>

- *Irfan Ul Haq, Juan Caballero*

- `1909.11424v1` - [abs](http://arxiv.org/abs/1909.11424v1) - [pdf](http://arxiv.org/pdf/1909.11424v1)

> Binary code similarity approaches compare two or more pieces of binary code to identify their similarities and differences. The ability to compare binary code enables many real-world applications on scenarios where source code may not be available such as patch analysis, bug search, and malware detection and analysis. Over the past 20 years numerous binary code similarity approaches have been proposed, but the research area has not yet been systematically analyzed. This paper presents a first survey of binary code similarity. It analyzes 61 binary code similarity approaches, which are systematized on four aspects: (1) the applications they enable, (2) their approach characteristics, (3) how the approaches are implemented, and (4) the benchmarks and methodologies used to evaluate them. In addition, the survey discusses the scope and origins of the area, its evolution over the past two decades, and the challenges that lie ahead.

</details>

<details>

<summary>2019-09-25 14:20:57 - Synthetic Data for Deep Learning</summary>

- *Sergey I. Nikolenko*

- `1909.11512v1` - [abs](http://arxiv.org/abs/1909.11512v1) - [pdf](http://arxiv.org/pdf/1909.11512v1)

> Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.

</details>

<details>

<summary>2019-09-25 17:17:30 - Explicitly disentangling image content from translation and rotation with spatial-VAE</summary>

- *Tristan Bepler, Ellen D. Zhong, Kotaro Kelley, Edward Brignole, Bonnie Berger*

- `1909.11663v1` - [abs](http://arxiv.org/abs/1909.11663v1) - [pdf](http://arxiv.org/pdf/1909.11663v1)

> Given an image dataset, we are often interested in finding data generative factors that encode semantic content independently from pose variables such as rotation and translation. However, current disentanglement approaches do not impose any specific structure on the learned latent representations. We propose a method for explicitly disentangling image rotation and translation from other unstructured latent factors in a variational autoencoder (VAE) framework. By formulating the generative model as a function of the spatial coordinate, we make the reconstruction error differentiable with respect to latent translation and rotation parameters. This formulation allows us to train a neural network to perform approximate inference on these latent variables while explicitly constraining them to only represent rotation and translation. We demonstrate that this framework, termed spatial-VAE, effectively learns latent representations that disentangle image rotation and translation from content and improves reconstruction over standard VAEs on several benchmark datasets, including applications to modeling continuous 2-D views of proteins from single particle electron microscopy and galaxies in astronomical images.

</details>

<details>

<summary>2019-09-25 21:45:33 - Generating Logical Forms from Graph Representations of Text and Entities</summary>

- *Peter Shaw, Philip Massey, Angelica Chen, Francesco Piccinno, Yasemin Altun*

- `1905.08407v3` - [abs](http://arxiv.org/abs/1905.08407v3) - [pdf](http://arxiv.org/pdf/1905.08407v3)

> Structured information about entities is critical for many semantic parsing tasks. We present an approach that uses a Graph Neural Network (GNN) architecture to incorporate information about relevant entities and their relations during parsing. Combined with a decoder copy mechanism, this approach provides a conceptually simple mechanism to generate logical forms with entities. We demonstrate that this approach is competitive with the state-of-the-art across several tasks without pre-training, and outperforms existing approaches when combined with BERT pre-training.

</details>

<details>

<summary>2019-09-26 00:14:22 - Pre-train, Interact, Fine-tune: A Novel Interaction Representation for Text Classification</summary>

- *Jianming Zheng, Fei Cai, Honghui Chen, Maarten de Rijke*

- `1909.11824v1` - [abs](http://arxiv.org/abs/1909.11824v1) - [pdf](http://arxiv.org/pdf/1909.11824v1)

> Text representation can aid machines in understanding text. Previous work on text representation often focuses on the so-called forward implication, i.e., preceding words are taken as the context of later words for creating representations, thus ignoring the fact that the semantics of a text segment is a product of the mutual implication of words in the text: later words contribute to the meaning of preceding words. We introduce the concept of interaction and propose a two-perspective interaction representation, that encapsulates a local and a global interaction representation. Here, a local interaction representation is one that interacts among words with parent-children relationships on the syntactic trees and a global interaction interpretation is one that interacts among all the words in a sentence. We combine the two interaction representations to develop a Hybrid Interaction Representation (HIR).   Inspired by existing feature-based and fine-tuning-based pretrain-finetuning approaches to language models, we integrate the advantages of feature-based and fine-tuning-based methods to propose the Pre-train, Interact, Fine-tune (PIF) architecture.   We evaluate our proposed models on five widely-used datasets for text classification tasks. Our ensemble method, outperforms state-of-the-art baselines with improvements ranging from 2.03% to 3.15% in terms of error rate. In addition, we find that, the improvements of PIF against most state-of-the-art methods is not affected by increasing of the length of the text.

</details>

<details>

<summary>2019-09-26 00:51:09 - Adversarial Deep Embedded Clustering: on a better trade-off between Feature Randomness and Feature Drift</summary>

- *Nairouz Mrabah, Mohamed Bouguessa, Riadh Ksantini*

- `1909.11832v1` - [abs](http://arxiv.org/abs/1909.11832v1) - [pdf](http://arxiv.org/pdf/1909.11832v1)

> Clustering using deep autoencoders has been thoroughly investigated in recent years. Current approaches rely on simultaneously learning embedded features and clustering the data points in the latent space. Although numerous deep clustering approaches outperform the shallow models in achieving favorable results on several high-semantic datasets, a critical weakness of such models has been overlooked. In the absence of concrete supervisory signals, the embedded clustering objective function may distort the latent space by learning from unreliable pseudo-labels. Thus, the network can learn non-representative features, which in turn undermines the discriminative ability, yielding worse pseudo-labels. In order to alleviate the effect of random discriminative features, modern autoencoder-based clustering papers propose to use the reconstruction loss for pretraining and as a regularizer during the clustering phase. Nevertheless, a clustering-reconstruction trade-off can cause the \textit{Feature Drift} phenomena. In this paper, we propose ADEC (Adversarial Deep Embedded Clustering) a novel autoencoder-based clustering model, which addresses a dual problem, namely, \textit{Feature Randomness} and \textit{Feature Drift}, using adversarial training. We empirically demonstrate the suitability of our model on handling these problems using benchmark real datasets. Experimental results validate that our model outperforms state-of-the-art autoencoder-based clustering methods.

</details>

<details>

<summary>2019-09-26 02:33:07 - Mathematical Reasoning in Latent Space</summary>

- *Dennis Lee, Christian Szegedy, Markus N. Rabe, Sarah M. Loos, Kshitij Bansal*

- `1909.11851v1` - [abs](http://arxiv.org/abs/1909.11851v1) - [pdf](http://arxiv.org/pdf/1909.11851v1)

> We design and conduct a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. We can compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether a statement can be rewritten by other theorems. Predicting the embedding of a formula generated by some rewrite rule is naturally viewed as approximate reasoning in the latent space. In order to measure the effectiveness of this reasoning, we perform approximate deduction sequences in the latent space and use the resulting embedding to inform the semantic features of the corresponding formal statement (which is obtained by performing the corresponding rewrite sequence using real formulas). Our experiments show that graph neural networks can make non-trivial predictions about the rewrite-success of statements, even when they propagate predicted latent representations for several steps. Since our corpus of mathematical formulas includes a wide variety of mathematical disciplines, this experiment is a strong indicator for the feasibility of deduction in latent space in general.

</details>

<details>

<summary>2019-09-26 06:01:42 - Unsupervised Image Translation using Adversarial Networks for Improved Plant Disease Recognition</summary>

- *Haseeb Nazki, Sook Yoon, Alvaro Fuentes, Dong Sun Park*

- `1909.11915v1` - [abs](http://arxiv.org/abs/1909.11915v1) - [pdf](http://arxiv.org/pdf/1909.11915v1)

> Acquisition of data in task-specific applications of machine learning like plant disease recognition is a costly endeavor owing to the requirements of professional human diligence and time constraints. In this paper, we present a simple pipeline that uses GANs in an unsupervised image translation environment to improve learning with respect to the data distribution in a plant disease dataset, reducing the partiality introduced by acute class imbalance and hence shifting the classification decision boundary towards better performance. The empirical analysis of our method is demonstrated on a limited dataset of 2789 tomato plant disease images, highly corrupted with an imbalance in the 9 disease categories. First, we extend the state of the art for the GAN-based image-to-image translation method by enhancing the perceptual quality of the generated images and preserving the semantics. We introduce AR-GAN, where in addition to the adversarial loss, our synthetic image generator optimizes on Activation Reconstruction loss (ARL) function that optimizes feature activations against the natural image. We present visually more compelling synthetic images in comparison to most prominent existing models and evaluate the performance of our GAN framework in terms of various datasets and metrics. Second, we evaluate the performance of a baseline convolutional neural network classifier for improved recognition using the resulting synthetic samples to augment our training set and compare it with the classical data augmentation scheme. We observe a significant improvement in classification accuracy (+5.2%) using generated synthetic samples as compared to (+0.8%) increase using classic augmentation in an equal class distribution environment.

</details>

<details>

<summary>2019-09-26 07:16:34 - SMoTherSpectre: exploiting speculative execution through port contention</summary>

- *Atri Bhattacharyya, Alexandra Sandulescu, Matthias Neugschwandtner, Alessandro Sorniotti, Babak Falsafi, Mathias Payer, Anil Kurmus*

- `1903.01843v3` - [abs](http://arxiv.org/abs/1903.01843v3) - [pdf](http://arxiv.org/pdf/1903.01843v3)

> Spectre, Meltdown, and related attacks have demonstrated that kernels, hypervisors, trusted execution environments, and browsers are prone to information disclosure through micro-architectural weaknesses. However, it remains unclear as to what extent other applications, in particular those that do not load attacker-provided code, may be impacted. It also remains unclear as to what extent these attacks are reliant on cache-based side channels.   We introduce SMoTherSpectre, a speculative code-reuse attack that leverages port-contention in simultaneously multi-threaded processors (SMoTher) as a side channel to leak information from a victim process. SMoTher is a fine-grained side channel that detects contention based on a single victim instruction. To discover real-world gadgets, we describe a methodology and build a tool that locates SMoTher-gadgets in popular libraries. In an evaluation on glibc, we found hundreds of gadgets that can be used to leak information. Finally, we demonstrate proof-of-concept attacks against the OpenSSH server, creating oracles for determining four host key bits, and against an application performing encryption using the OpenSSL library, creating an oracle which can differentiate a bit of the plaintext through gadgets in libcrypto and glibc.

</details>

<details>

<summary>2019-09-26 07:56:07 - BERT-Based Multi-Head Selection for Joint Entity-Relation Extraction</summary>

- *Weipeng Huang, Xingyi Cheng, Taifeng Wang, Wei Chu*

- `1908.05908v2` - [abs](http://arxiv.org/abs/1908.05908v2) - [pdf](http://arxiv.org/pdf/1908.05908v2)

> In this paper, we report our method for the Information Extraction task in 2019 Language and Intelligence Challenge. We incorporate BERT into the multi-head selection framework for joint entity-relation extraction. This model extends existing approaches from three perspectives. First, BERT is adopted as a feature extraction layer at the bottom of the multi-head selection framework. We further optimize BERT by introducing a semantic-enhanced task during BERT pre-training. Second, we introduce a large-scale Baidu Baike corpus for entity recognition pre-training, which is of weekly supervised learning since there is no actual named entity label. Third, soft label embedding is proposed to effectively transmit information between entity recognition and relation extraction. Combining these three contributions, we enhance the information extracting ability of the multi-head selection model and achieve F1-score 0.876 on testset-1 with a single model. By ensembling four variants of our model, we finally achieve F1 score 0.892 (1st place) on testset-1 and F1 score 0.8924 (2nd place) on testset-2.

</details>

<details>

<summary>2019-09-26 09:56:23 - MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance</summary>

- *Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, Steffen Eger*

- `1909.02622v2` - [abs](http://arxiv.org/abs/1909.02622v2) - [pdf](http://arxiv.org/pdf/1909.02622v2)

> A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a metric that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that metrics combining contextualized representations with a distance measure perform the best. Such metrics also demonstrate strong generalization capability across tasks. For ease-of-use we make our metrics available as web service.

</details>

<details>

<summary>2019-09-26 14:13:39 - MinWikiSplit: A Sentence Splitting Corpus with Minimal Propositions</summary>

- *Christina Niklaus, Andre Freitas, Siegfried Handschuh*

- `1909.12131v1` - [abs](http://arxiv.org/abs/1909.12131v1) - [pdf](http://arxiv.org/pdf/1909.12131v1)

> We compiled a new sentence splitting corpus that is composed of 203K pairs of aligned complex source and simplified target sentences. Contrary to previously proposed text simplification corpora, which contain only a small number of split examples, we present a dataset where each input sentence is broken down into a set of minimal propositions, i.e. a sequence of sound, self-contained utterances with each of them presenting a minimal semantic unit that cannot be further decomposed into meaningful propositions. This corpus is useful for developing sentence splitting approaches that learn how to transform sentences with a complex linguistic structure into a fine-grained representation of short sentences that present a simple and more regular structure which is easier to process for downstream applications and thus facilitates and improves their performance.

</details>

<details>

<summary>2019-09-26 14:18:09 - Semantic Change and Emerging Tropes In a Large Corpus of New High German Poetry</summary>

- *Thomas Haider, Steffen Eger*

- `1909.12136v1` - [abs](http://arxiv.org/abs/1909.12136v1) - [pdf](http://arxiv.org/pdf/1909.12136v1)

> Due to its semantic succinctness and novelty of expression, poetry is a great test bed for semantic change analysis. However, so far there is a scarcity of large diachronic corpora. Here, we provide a large corpus of German poetry which consists of about 75k poems with more than 11 million tokens, with poems ranging from the 16th to early 20th century. We then track semantic change in this corpus by investigating the rise of tropes (`love is magic') over time and detecting change points of meaning, which we find to occur particularly within the German Romantic period. Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry.

</details>

<details>

<summary>2019-09-26 14:21:33 - DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German</summary>

- *Christina Niklaus, Matthias Cetto, Andre Freitas, Siegfried Handschuh*

- `1909.12140v1` - [abs](http://arxiv.org/abs/1909.12140v1) - [pdf](http://arxiv.org/pdf/1909.12140v1)

> We introduce DisSim, a discourse-aware sentence splitting framework for English and German whose goal is to transform syntactically complex sentences into an intermediate representation that presents a simple and more regular structure which is easier to process for downstream semantic applications. For this purpose, we turn input sentences into a two-layered semantic hierarchy in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the coherence structure of the input and, hence, its interpretability for downstream tasks.

</details>

<details>

<summary>2019-09-26 17:25:57 - Relationship Explainable Multi-objective Reinforcement Learning with Semantic Explainability Generation</summary>

- *Huixin Zhan, Yongcan Cao*

- `1909.12268v1` - [abs](http://arxiv.org/abs/1909.12268v1) - [pdf](http://arxiv.org/pdf/1909.12268v1)

> Solving multi-objective optimization problems is important in various applications where users are interested in obtaining optimal policies subject to multiple, yet often conflicting objectives. A typical approach to obtain optimal policies is to first construct a loss function that is based on the scalarization of individual objectives, and then find the optimal policy that minimizes the loss. However, optimizing the scalarized (and weighted) loss does not necessarily provide guarantee of high performance on each possibly conflicting objective because it is challenging to assign the right weights without knowing the relationship among these objectives. Moreover, the effectiveness of these gradient descent algorithms is limited by the agent's ability to explain their decisions and actions to human users. The purpose of this study is two-fold. First, we propose a vector value function based multi-objective reinforcement learning (V2f-MORL) approach that seeks to quantify the inter-objective relationship via reinforcement learning (RL) when the impact of one objective on others is unknown a prior. In particular, we construct one actor and multiple critics that can co-learn the policy and inter-objective relationship matrix (IORM), quantifying the impact of objectives on each other, in an iterative way. Second, we provide a semantic representation that can uncover the trade-off of decision policies made by users to reconcile conflicting objectives based on the proposed V2f-MORL approach for the explainability of the generated behaviors subject to given optimization objectives. We demonstrate the effectiveness of the proposed approach via a MuJoCo based robotics case study.

</details>

<details>

<summary>2019-09-26 19:03:54 - Coin_flipper at eHealth-KD Challenge 2019: Voting LSTMs for Key Phrases and Semantic Relation Identification Applied to Spanish eHealth Texts</summary>

- *Neus Català, Mario Martin*

- `1909.12339v1` - [abs](http://arxiv.org/abs/1909.12339v1) - [pdf](http://arxiv.org/pdf/1909.12339v1)

> This paper describes our approach presented for the eHealth-KD 2019 challenge. Our participation was aimed at testing how far we could go using generic tools for Text-Processing but, at the same time, using common optimization techniques in the field of Data Mining. The architecture proposed for both tasks of the challenge is a standard stacked 2-layer bi-LSTM. The main particularities of our approach are: (a) The use of a surrogate function of F1 as loss function to close the gap between the minimization function and the evaluation metric, and (b) The generation of an ensemble of models for generating predictions by majority vote. Our system ranked second with an F1 score of 62.18% in the main task by a narrow margin with the winner that scored 63.94%.

</details>

<details>

<summary>2019-09-27 05:44:38 - StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</summary>

- *Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, Luo Si*

- `1908.04577v3` - [abs](http://arxiv.org/abs/1908.04577v3) - [pdf](http://arxiv.org/pdf/1908.04577v3)

> Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.

</details>

<details>

<summary>2019-09-27 06:36:39 - Learnable Tree Filter for Structure-preserving Feature Transform</summary>

- *Lin Song, Yanwei Li, Zeming Li, Gang Yu, Hongbin Sun, Jian Sun, Nanning Zheng*

- `1909.12513v1` - [abs](http://arxiv.org/abs/1909.12513v1) - [pdf](http://arxiv.org/pdf/1909.12513v1)

> Learning discriminative global features plays a vital role in semantic segmentation. And most of the existing methods adopt stacks of local convolutions or non-local blocks to capture long-range context. However, due to the absence of spatial structure preservation, these operators ignore the object details when enlarging receptive fields. In this paper, we propose the learnable tree filter to form a generic tree filtering module that leverages the structural property of minimal spanning tree to model long-range dependencies while preserving the details. Furthermore, we propose a highly efficient linear-time algorithm to reduce resource consumption. Thus, the designed modules can be plugged into existing deep neural networks conveniently. To this end, tree filtering modules are embedded to formulate a unified framework for semantic segmentation. We conduct extensive ablation studies to elaborate on the effectiveness and efficiency of the proposed method. Specifically, it attains better performance with much less overhead compared with the classic PSP block and Non-local operation under the same backbone. Our approach is proved to achieve consistent improvements on several benchmarks without bells-and-whistles. Code and models are available at https://github.com/StevenGrove/TreeFilter-Torch.

</details>

<details>

<summary>2019-09-27 14:16:30 - EDUCE: Explaining model Decisions through Unsupervised Concepts Extraction</summary>

- *Diane Bouchacourt, Ludovic Denoyer*

- `1905.11852v2` - [abs](http://arxiv.org/abs/1905.11852v2) - [pdf](http://arxiv.org/pdf/1905.11852v2)

> Providing explanations along with predictions is crucial in some text processing tasks. Therefore, we propose a new self-interpretable model that performs output prediction and simultaneously provides an explanation in terms of the presence of particular concepts in the input. To do so, our model's prediction relies solely on a low-dimensional binary representation of the input, where each feature denotes the presence or absence of concepts. The presence of a concept is decided from an excerpt i.e. a small sequence of consecutive words in the text. Relevant concepts for the prediction task at hand are automatically defined by our model, avoiding the need for concept-level annotations. To ease interpretability, we enforce that for each concept, the corresponding excerpts share similar semantics and are differentiable from each others. We experimentally demonstrate the relevance of our approach on text classification and multi-sentiment analysis tasks.

</details>

<details>

<summary>2019-09-27 16:10:06 - Improving Semantic Parsing with Neural Generator-Reranker Architecture</summary>

- *Huseyin A. Inan, Gaurav Singh Tomar, Huapu Pan*

- `1909.12764v1` - [abs](http://arxiv.org/abs/1909.12764v1) - [pdf](http://arxiv.org/pdf/1909.12764v1)

> Semantic parsing is the problem of deriving machine interpretable meaning representations from natural language utterances. Neural models with encoder-decoder architectures have recently achieved substantial improvements over traditional methods. Although neural semantic parsers appear to have relatively high recall using large beam sizes, there is room for improvement with respect to one-best precision. In this work, we propose a generator-reranker architecture for semantic parsing. The generator produces a list of potential candidates and the reranker, which consists of a pre-processing step for the candidates followed by a novel critic network, reranks these candidates based on the similarity between each candidate and the input sentence. We show the advantages of this approach along with how it improves the parsing performance through extensive analysis. We experiment our model on three semantic parsing datasets (GEO, ATIS, and OVERNIGHT). The overall architecture achieves the state-of-the-art results in all three datasets.

</details>

<details>

<summary>2019-09-27 18:40:23 - Automatically Learning Data Augmentation Policies for Dialogue Tasks</summary>

- *Tong Niu, Mohit Bansal*

- `1909.12868v1` - [abs](http://arxiv.org/abs/1909.12868v1) - [pdf](http://arxiv.org/pdf/1909.12868v1)

> Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image's semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the controller to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy's required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies.

</details>

<details>

<summary>2019-09-27 23:27:10 - MGBPv2: Scaling Up Multi-Grid Back-Projection Networks</summary>

- *Pablo Navarrete Michelini, Wenbin Chen, Hanwen Liu, Dan Zhu*

- `1909.12983v1` - [abs](http://arxiv.org/abs/1909.12983v1) - [pdf](http://arxiv.org/pdf/1909.12983v1)

> Here, we describe our solution for the AIM-2019 Extreme Super-Resolution Challenge, where we won the 1st place in terms of perceptual quality (MOS) similar to the ground truth and achieved the 5th place in terms of high-fidelity (PSNR). To tackle this challenge, we introduce the second generation of MultiGrid BackProjection networks (MGBPv2) whose major modifications make the system scalable and more general than its predecessor. It combines the scalability of the multigrid algorithm and the performance of iterative backprojections. In its original form, MGBP is limited to a small number of parameters due to a strongly recursive structure. In MGBPv2, we make full use of the multigrid recursion from the beginning of the network; we allow different parameters in every module of the network; we simplify the main modules; and finally, we allow adjustments of the number of network features based on the scale of operation. For inference tasks, we introduce an overlapping patch approach to further allow processing of very large images (e.g. 8K). Our training strategies make use of a multiscale loss, combining distortion and/or perception losses on the output as well as downscaled output images. The final system can balance between high quality and high performance.

</details>

<details>

<summary>2019-09-28 08:30:03 - Feature Fusion Detector for Semantic Cognition of Remote Sensing</summary>

- *Wei Zhou, Yiying Li*

- `1909.13047v1` - [abs](http://arxiv.org/abs/1909.13047v1) - [pdf](http://arxiv.org/pdf/1909.13047v1)

> The value of remote sensing images is of vital importance in many areas and needs to be refined by some cognitive approaches. The remote sensing detection is an appropriate way to achieve the semantic cognition. However, such detection is a challenging issue for scale diversity, diversity of views, small objects, sophisticated light and shadow backgrounds. In this article, inspired by the state-of-the-art detection framework FPN, we propose a novel approach for constructing a feature fusion module that optimizes feature context utilization in detection, calling our system LFFN for Layer-weakening Feature Fusion Network. We explore the inherent relevance of different layers to the final decision, and the incentives of higher-level features to lower-level features. More importantly, we explore the characteristics of different backbone networks in the mining of basic features and the correlation utilization of convolutional channels, and call our upgraded version as advanced LFFN. Based on experiments on the remote sensing dataset from Google Earth, our LFFN has proved effective and practical for the semantic cognition of remote sensing, achieving 89% mAP which is 4.1% higher than that of FPN. Moreover, in terms of the generalization performance, LFFN achieves 79.9% mAP on VOC 2007 and achieves 73.0% mAP on VOC 2012 test, and advacned LFFN obtains the mAP values of 80.7% and 74.4% on VOC 2007 and 2012 respectively, outperforming the comparable state-of-the-art SSD and Faster R-CNN models.

</details>

<details>

<summary>2019-09-28 11:54:15 - W-RNN: News text classification based on a Weighted RNN</summary>

- *Dan Wang, Jibing Gong, Yaxi Song*

- `1909.13077v1` - [abs](http://arxiv.org/abs/1909.13077v1) - [pdf](http://arxiv.org/pdf/1909.13077v1)

> Most of the information is stored as text, so text mining is regarded as having high commercial potential. Aiming at the semantic constraint problem of classification methods based on sparse representation, we propose a weighted recurrent neural network (W-RNN), which can fully extract text serialization semantic information. For the problem that the feature high dimensionality and unclear semantic relationship in text data representation, we first utilize the word vector to represent the vocabulary in the text and use Recurrent Neural Network (RNN) to extract features of the serialized text data. The word vector is then automatically weighted and summed using the intermediate output of the word vector to form the text representation vector. Finally, the neural network is used for classification. W-RNN is verified on the news dataset and proves that W-RNN is superior to other four baseline methods in Precision, Recall, F1 and loss values, which is suitable for text classification.

</details>

<details>

<summary>2019-09-28 21:32:55 - Generalized Zero-shot ICD Coding</summary>

- *Congzheng Song, Shanghang Zhang, Najmeh Sadoughi, Pengtao Xie, Eric Xing*

- `1909.13154v1` - [abs](http://arxiv.org/abs/1909.13154v1) - [pdf](http://arxiv.org/pdf/1909.13154v1)

> The International Classification of Diseases (ICD) is a list of classification codes for the diagnoses. Automatic ICD coding is in high demand as the manual coding can be labor-intensive and error-prone. It is a multi-label text classification task with extremely long-tailed label distribution, making it difficult to perform fine-grained classification on both frequent and zero-shot codes at the same time. In this paper, we propose a latent feature generation framework for generalized zero-shot ICD coding, where we aim to improve the prediction on codes that have no labeled data without compromising the performance on seen codes. Our framework generates pseudo features conditioned on the ICD code descriptions and exploits the ICD code hierarchical structure. To guarantee the semantic consistency between the generated features and real features, we reconstruct the keywords in the input documents that are related to the conditioned ICD codes. To the best of our knowledge, this works represents the first one that proposes an adversarial generative model for the generalized zero-shot learning on multi-label text classification. Extensive experiments demonstrate the effectiveness of our approach. On the public MIMIC-III dataset, our methods improve the F1 score from nearly 0 to 20.91% for the zero-shot codes, and increase the AUC score by 3% (absolute improvement) from previous state of the art. We also show that the framework improves the performance on few-shot codes.

</details>

<details>

<summary>2019-09-29 01:28:58 - Meta-Graph Based HIN Spectral Embedding: Methods, Analyses, and Insights</summary>

- *Carl Yang, Yichen Feng, Pan Li, Yu Shi, Jiawei Han*

- `1910.00004v1` - [abs](http://arxiv.org/abs/1910.00004v1) - [pdf](http://arxiv.org/pdf/1910.00004v1)

> In this work, we propose to study the utility of different meta-graphs, as well as how to simultaneously leverage multiple meta-graphs for HIN embedding in an unsupervised manner. Motivated by prolific research on homogeneous networks, especially spectral graph theory, we firstly conduct a systematic empirical study on the spectrum and embedding quality of different meta-graphs on multiple HINs, which leads to an efficient method of meta-graph assessment. It also helps us to gain valuable insight into the higher-order organization of HINs and indicates a practical way of selecting useful embedding dimensions. Further, we explore the challenges of combining multiple meta-graphs to capture the multi-dimensional semantics in HIN through reasoning from mathematical geometry and arrive at an embedding compression method of autoencoder with $\ell_{2,1}$-loss, which finds the most informative meta-graphs and embeddings in an end-to-end unsupervised manner. Finally, empirical analysis suggests a unified workflow to close the gap between our meta-graph assessment and combination methods. To the best of our knowledge, this is the first research effort to provide rich theoretical and empirical analyses on the utility of meta-graphs and their combinations, especially regarding HIN embedding. Extensive experimental comparisons with various state-of-the-art neural network based embedding methods on multiple real-world HINs demonstrate the effectiveness and efficiency of our framework in finding useful meta-graphs and generating high-quality HIN embeddings.

</details>

<details>

<summary>2019-09-29 02:56:24 - Gated Task Interaction Framework for Multi-task Sequence Tagging</summary>

- *Isaac K. E. Ampomah, Sally McClean, Zhiwei Lin, Glenn Hawe*

- `1909.13193v1` - [abs](http://arxiv.org/abs/1909.13193v1) - [pdf](http://arxiv.org/pdf/1909.13193v1)

> Recent studies have shown that neural models can achieve high performance on several sequence labelling/tagging problems without the explicit use of linguistic features such as part-of-speech (POS) tags. These models are trained only using the character-level and the word embedding vectors as inputs. Others have shown that linguistic features can improve the performance of neural models on tasks such as chunking and named entity recognition (NER). However, the change in performance depends on the degree of semantic relatedness between the linguistic features and the target task; in some instances, linguistic features can have a negative impact on performance. This paper presents an approach to jointly learn these linguistic features along with the target sequence labelling tasks with a new multi-task learning (MTL) framework called Gated Tasks Interaction (GTI) network for solving multiple sequence tagging tasks. The GTI network exploits the relations between the multiple tasks via neural gate modules. These gate modules control the flow of information between the different tasks. Experiments on benchmark datasets for chunking and NER show that our framework outperforms other competitive baselines trained with and without external training resources.

</details>

<details>

<summary>2019-09-29 16:43:30 - Lifelong Neural Topic Learning in Contextualized Autoregressive Topic Models of Language via Informative Transfers</summary>

- *Yatin Chaudhary, Pankaj Gupta, Thomas Runkler*

- `1909.13315v1` - [abs](http://arxiv.org/abs/1909.13315v1) - [pdf](http://arxiv.org/pdf/1909.13315v1)

> Topic models such as LDA, DocNADE, iDocNADEe have been popular in document analysis. However, the traditional topic models have several limitations including: (1) Bag-of-words (BoW) assumption, where they ignore word ordering, (2) Data sparsity, where the application of topic models is challenging due to limited word co-occurrences, leading to incoherent topics and (3) No Continuous Learning framework for topic learning in lifelong fashion, exploiting historical knowledge (or latent topics) and minimizing catastrophic forgetting. This thesis focuses on addressing the above challenges within neural topic modeling framework. We propose: (1) Contextualized topic model that combines a topic and a language model and introduces linguistic structures (such as word ordering, syntactic and semantic features, etc.) in topic modeling, (2) A novel lifelong learning mechanism into neural topic modeling framework to demonstrate continuous learning in sequential document collections and minimizing catastrophic forgetting. Additionally, we perform a selective data augmentation to alleviate the need for complete historical corpora during data hallucination or replay.

</details>

<details>

<summary>2019-09-29 18:01:00 - Recent Advances in End-to-End Spoken Language Understanding</summary>

- *Natalia Tomashenko, Antoine Caubriere, Yannick Esteve, Antoine Laurent, Emmanuel Morin*

- `1909.13332v1` - [abs](http://arxiv.org/abs/1909.13332v1) - [pdf](http://arxiv.org/pdf/1909.13332v1)

> This work investigates spoken language understanding (SLU) systems in the scenario when the semantic information is extracted directly from the speech signal by means of a single end-to-end neural network model. Two SLU tasks are considered: named entity recognition (NER) and semantic slot filling (SF). For these tasks, in order to improve the model performance, we explore various techniques including speaker adaptation, a modification of the connectionist temporal classification (CTC) training criterion, and sequential pretraining.

</details>

<details>

<summary>2019-09-29 21:33:46 - Unfolding the Structure of a Document using Deep Learning</summary>

- *Muhammad Mahbubur Rahman, Tim Finin*

- `1910.03678v1` - [abs](http://arxiv.org/abs/1910.03678v1) - [pdf](http://arxiv.org/pdf/1910.03678v1)

> Understanding and extracting of information from large documents, such as business opportunities, academic articles, medical documents and technical reports, poses challenges not present in short documents. Such large documents may be multi-themed, complex, noisy and cover diverse topics. We describe a framework that can analyze large documents and help people and computer systems locate desired information in them. We aim to automatically identify and classify different sections of documents and understand their purpose within the document. A key contribution of our research is modeling and extracting the logical and semantic structure of electronic documents using deep learning techniques. We evaluate the effectiveness and robustness of our framework through extensive experiments on two collections: more than one million scholarly articles from arXiv and a collection of requests for proposal documents from government sources.

</details>

<details>

<summary>2019-09-30 02:08:47 - Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History</summary>

- *Yiheng Zhou, Yulia Tsvetkov, Alan W Black, Zhou Yu*

- `1909.13425v1` - [abs](http://arxiv.org/abs/1909.13425v1) - [pdf](http://arxiv.org/pdf/1909.13425v1)

> We study non-collaborative dialogs, where two agents have a conflict of interest but must strategically communicate to reach an agreement (e.g., negotiation). This setting poses new challenges for modeling dialog history because the dialog's outcome relies not only on the semantic intent, but also on tactics that convey the intent. We propose to model both semantic and tactic history using finite state transducers (FSTs). Unlike RNN, FSTs can explicitly represent dialog history through all the states traversed, facilitating interpretability of dialog structure. We train FSTs on a set of strategies and tactics used in negotiation dialogs. The trained FSTs show plausible tactic structure and can be generalized to other non-collaborative domains (e.g., persuasion). We evaluate the FSTs by incorporating them in an automated negotiating system that attempts to sell products and a persuasion system that persuades people to donate to a charity. Experiments show that explicitly modeling both semantic and tactic history is an effective way to improve both dialog policy planning and generation performance.

</details>

<details>

<summary>2019-09-30 05:03:25 - Improving Textual Network Learning with Variational Homophilic Embeddings</summary>

- *Wenlin Wang, Chenyang Tao, Zhe Gan, Guoyin Wang, Liqun Chen, Xinyuan Zhang, Ruiyi Zhang, Qian Yang, Ricardo Henao, Lawrence Carin*

- `1909.13456v1` - [abs](http://arxiv.org/abs/1909.13456v1) - [pdf](http://arxiv.org/pdf/1909.13456v1)

> The performance of many network learning applications crucially hinges on the success of network embedding algorithms, which aim to encode rich network information into low-dimensional vertex-based vector representations. This paper considers a novel variational formulation of network embeddings, with special focus on textual networks. Different from most existing methods that optimize a discriminative objective, we introduce Variational Homophilic Embedding (VHE), a fully generative model that learns network embeddings by modeling the semantic (textual) information with a variational autoencoder, while accounting for the structural (topology) information through a novel homophilic prior design. Homophilic vertex embeddings encourage similar embedding vectors for related (connected) vertices. The proposed VHE promises better generalization for downstream tasks, robustness to incomplete observations, and the ability to generalize to unseen vertices. Extensive experiments on real-world networks, for multiple tasks, demonstrate that the proposed method consistently achieves superior performance relative to competing state-of-the-art approaches.

</details>

<details>

<summary>2019-09-30 08:35:04 - Multi-Modal Attention Network Learning for Semantic Source Code Retrieval</summary>

- *Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu, Philip S. Yu*

- `1909.13516v1` - [abs](http://arxiv.org/abs/1909.13516v1) - [pdf](http://arxiv.org/pdf/1909.13516v1)

> Code retrieval techniques and tools have been playing a key role in facilitating software developers to retrieve existing code fragments from available open-source repositories given a user query. Despite the existing efforts in improving the effectiveness of code retrieval, there are still two main issues hindering them from being used to accurately retrieve satisfiable code fragments from large-scale repositories when answering complicated queries. First, the existing approaches only consider shallow features of source code such as method names and code tokens, but ignoring structured features such as abstract syntax trees (ASTs) and control-flow graphs (CFGs) of source code, which contains rich and well-defined semantics of source code. Second, although the deep learning-based approach performs well on the representation of source code, it lacks the explainability, making it hard to interpret the retrieval results and almost impossible to understand which features of source code contribute more to the final results.   To tackle the two aforementioned issues, this paper proposes MMAN, a novel Multi-Modal Attention Network for semantic source code retrieval. A comprehensive multi-modal representation is developed for representing unstructured and structured features of source code, with one LSTM for the sequential tokens of code, a Tree-LSTM for the AST of code and a GGNN (Gated Graph Neural Network) for the CFG of code. Furthermore, a multi-modal attention fusion layer is applied to assign weights to different parts of each modality of source code and then integrate them into a single hybrid representation. Comprehensive experiments and analysis on a large-scale real-world dataset show that our proposed model can accurately retrieve code snippets and outperforms the state-of-the-art methods.

</details>

<details>

<summary>2019-09-30 10:55:00 - Spread-gram: A spreading-activation schema of network structural learning</summary>

- *Jie Bai, Linjing Li, Daniel Zeng*

- `1909.13581v1` - [abs](http://arxiv.org/abs/1909.13581v1) - [pdf](http://arxiv.org/pdf/1909.13581v1)

> Network representation learning has exploded recently. However, existing studies usually reconstruct networks as sequences or matrices, which may cause information bias or sparsity problem during model training. Inspired by a cognitive model of human memory, we propose a network representation learning scheme. In this scheme, we learn node embeddings by adjusting the proximity of nodes traversing the spreading structure of the network. Our proposed method shows a significant improvement in multiple analysis tasks based on various real-world networks, ranging from semantic networks to protein interaction networks, international trade networks, human behavior networks, etc. In particular, our model can effectively discover the hierarchical structures in networks. The well-organized model training speeds up the convergence to only a small number of iterations, and the training time is linear with respect to the edge numbers.

</details>

<details>

<summary>2019-09-30 11:44:54 - FixMiner: Mining Relevant Fix Patterns for Automated Program Repair</summary>

- *Anil Koyuncu, Kui Liu, Tegawendé F. Bissyandé, Dongsun Kim, Jacques Klein, Martin Monperrus, Yves Le Traon*

- `1810.01791v2` - [abs](http://arxiv.org/abs/1810.01791v2) - [pdf](http://arxiv.org/pdf/1810.01791v2)

> Patching is a common activity in software development. It is generally performed on a source code base to address bugs or add new functionalities. In this context, given the recurrence of bugs across projects, the associated similar patches can be leveraged to extract generic fix actions. While the literature includes various approaches leveraging similarity among patches to guide program repair, these approaches often do not yield fix patterns that are tractable and reusable as actionable input to APR systems. In this paper, we propose a systematic and automated approach to mining relevant and actionable fix patterns based on an iterative clustering strategy applied to atomic changes within patches. The goal of FixMiner is thus to infer separate and reusable fix patterns that can be leveraged in other patch generation systems. Our technique, FixMiner, leverages Rich Edit Script which is a specialized tree structure of the edit scripts that captures the AST-level context of the code changes. FixMiner uses different tree representations of Rich Edit Scripts for each round of clustering to identify similar changes. These are abstract syntax trees, edit actions trees, and code context trees. We have evaluated FixMiner on thousands of software patches collected from open source projects. Preliminary results show that we are able to mine accurate patterns, efficiently exploiting change information in Rich Edit Scripts. We further integrated the mined patterns to an automated program repair prototype, PARFixMiner, with which we are able to correctly fix 26 bugs of the Defects4J benchmark. Beyond this quantitative performance, we show that the mined fix patterns are sufficiently relevant to produce patches with a high probability of correctness: 81% of PARFixMiner's generated plausible patches are correct.

</details>

<details>

<summary>2019-09-30 14:02:53 - Retrieval-based Goal-Oriented Dialogue Generation</summary>

- *Ana Valeria Gonzalez, Isabelle Augenstein, Anders Søgaard*

- `1909.13717v1` - [abs](http://arxiv.org/abs/1909.13717v1) - [pdf](http://arxiv.org/pdf/1909.13717v1)

> Most research on dialogue has focused either on dialogue generation for openended chit chat or on state tracking for goal-directed dialogue. In this work, we explore a hybrid approach to goal-oriented dialogue generation that combines retrieval from past history with a hierarchical, neural encoder-decoder architecture. We evaluate this approach in the customer support domain using the Multiwoz dataset (Budzianowski et al., 2018). We show that adding this retrieval step to a hierarchical, neural encoder-decoder architecture leads to significant improvements, including responses that are rated more appropriate and fluent by human evaluators. Finally, we compare our retrieval-based model to various semantically conditioned models explicitly using past dialog act information, and find that our proposed model is competitive with the current state of the art (Chen et al., 2019), while not requiring explicit labels about past machine acts.

</details>

<details>

<summary>2019-09-30 14:20:06 - Cross-Modal Subspace Learning with Scheduled Adaptive Margin Constraints</summary>

- *David Semedo, João Magalhães*

- `1909.13733v1` - [abs](http://arxiv.org/abs/1909.13733v1) - [pdf](http://arxiv.org/pdf/1909.13733v1)

> Cross-modal embeddings, between textual and visual modalities, aim to organise multimodal instances by their semantic correlations. State-of-the-art approaches use maximum-margin methods, based on the hinge-loss, to enforce a constant margin m, to separate projections of multimodal instances from different categories. In this paper, we propose a novel scheduled adaptive maximum-margin (SAM) formulation that infers triplet-specific constraints during training, therefore organising instances by adaptively enforcing inter-category and inter-modality correlations. This is supported by a scheduled adaptive margin function, that is smoothly activated, replacing a static margin by an adaptively inferred one reflecting triplet-specific semantic correlations while accounting for the incremental learning behaviour of neural networks to enforce category cluster formation and enforcement. Experiments on widely used datasets show that our model improved upon state-of-the-art approaches, by achieving a relative improvement of up to ~12.5% over the second best method, thus confirming the effectiveness of our scheduled adaptive margin formulation.

</details>

<details>

<summary>2019-09-30 16:03:11 - Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel Textual Transfer</summary>

- *Richard Yuanzhe Pang, Kevin Gimpel*

- `1810.11878v2` - [abs](http://arxiv.org/abs/1810.11878v2) - [pdf](http://arxiv.org/pdf/1810.11878v2)

> We consider the problem of automatically generating textual paraphrases with modified attributes or properties, focusing on the setting without parallel data (Hu et al., 2017; Shen et al., 2017). This setting poses challenges for evaluation. We show that the metric of post-transfer classification accuracy is insufficient on its own, and propose additional metrics based on semantic preservation and fluency as well as a way to combine them into a single overall score. We contribute new loss functions and training strategies to address the different metrics. Semantic preservation is addressed by adding a cyclic consistency loss and a loss based on paraphrase pairs, while fluency is improved by integrating losses based on style-specific language models. We experiment with a Yelp sentiment dataset and a new literature dataset that we propose, using multiple models that extend prior work (Shen et al., 2017). We demonstrate that our metrics correlate well with human judgments, at both the sentence-level and system-level. Automatic and manual evaluation also show large improvements over the baseline method of Shen et al. (2017). We hope that our proposed metrics can speed up system development for new textual transfer tasks while also encouraging the community to address our three complementary aspects of transfer quality.

</details>

<details>

<summary>2019-09-30 16:34:33 - Deep Neural Networks Ensemble for Detecting Medication Mentions in Tweets</summary>

- *Davy Weissenbacher, Abeed Sarker, Ari Klein, Karen O'Connor, Arjun Magge Ranganatha, Graciela Gonzalez-Hernandez*

- `1904.05308v2` - [abs](http://arxiv.org/abs/1904.05308v2) - [pdf](http://arxiv.org/pdf/1904.05308v2)

> Objective: After years of research, Twitter posts are now recognized as an important source of patient-generated data, providing unique insights into population health. A fundamental step to incorporating Twitter data in pharmacoepidemiological research is to automatically recognize medication mentions in tweets. Given that lexical searches for medication names may fail due to misspellings or ambiguity with common words, we propose a more advanced method to recognize them. Methods: We present Kusuri, an Ensemble Learning classifier, able to identify tweets mentioning drug products and dietary supplements. Kusuri ("medication" in Japanese) is composed of two modules. First, four different classifiers (lexicon-based, spelling-variant-based, pattern-based and one based on a weakly-trained neural network) are applied in parallel to discover tweets potentially containing medication names. Second, an ensemble of deep neural networks encoding morphological, semantical and long-range dependencies of important words in the tweets discovered is used to make the final decision. Results: On a balanced (50-50) corpus of 15,005 tweets, Kusuri demonstrated performances close to human annotators with 93.7% F1-score, the best score achieved thus far on this corpus. On a corpus made of all tweets posted by 113 Twitter users (98,959 tweets, with only 0.26% mentioning medications), Kusuri obtained 76.3% F1-score. There is not a prior drug extraction system that compares running on such an extremely unbalanced dataset. Conclusion: The system identifies tweets mentioning drug names with performance high enough to ensure its usefulness and ready to be integrated in larger natural language processing systems.

</details>

<details>

<summary>2019-09-30 17:15:57 - The Universal Decompositional Semantics Dataset and Decomp Toolkit</summary>

- *Aaron Steven White, Elias Stengel-Eskin, Siddharth Vashishtha, Venkata Govindarajan, Dee Ann Reisinger, Tim Vieira, Keisuke Sakaguchi, Sheng Zhang, Francis Ferraro, Rachel Rudinger, Kyle Rawlins, Benjamin Van Durme*

- `1909.13851v1` - [abs](http://arxiv.org/abs/1909.13851v1) - [pdf](http://arxiv.org/pdf/1909.13851v1)

> We present the Universal Decompositional Semantics (UDS) dataset (v1.0), which is bundled with the Decomp toolkit (v0.1). UDS1.0 unifies five high-quality, decompositional semantics-aligned annotation sets within a single semantic graph specification---with graph structures defined by the predicative patterns produced by the PredPatt tool and real-valued node and edge attributes constructed using sophisticated normalization procedures. The Decomp toolkit provides a suite of Python 3 tools for querying UDS graphs using SPARQL. Both UDS1.0 and Decomp0.1 are publicly available at http://decomp.io.

</details>

<details>

<summary>2019-09-30 17:26:51 - INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps</summary>

- *Wei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey Clausse, Maximilian Naumann, Julius Kummerle, Hendrik Konigshof, Christoph Stiller, Arnaud de La Fortelle, Masayoshi Tomizuka*

- `1910.03088v1` - [abs](http://arxiv.org/abs/1910.03088v1) - [pdf](http://arxiv.org/pdf/1910.03088v1)

> Behavior-related research areas such as motion prediction/planning, representation/imitation learning, behavior modeling/generation, and algorithm testing, require support from high-quality motion datasets containing interactive driving scenarios with different driving cultures. In this paper, we present an INTERnational, Adversarial and Cooperative moTION dataset (INTERACTION dataset) in interactive driving scenarios with semantic maps. Five features of the dataset are highlighted. 1) The interactive driving scenarios are diverse, including urban/highway/ramp merging and lane changes, roundabouts with yield/stop signs, signalized intersections, intersections with one/two/all-way stops, etc. 2) Motion data from different countries and different continents are collected so that driving preferences and styles in different cultures are naturally included. 3) The driving behavior is highly interactive and complex with adversarial and cooperative motions of various traffic participants. Highly complex behavior such as negotiations, aggressive/irrational decisions and traffic rule violations are densely contained in the dataset, while regular behavior can also be found from cautious car-following, stop, left/right/U-turn to rational lane-change and cycling and pedestrian crossing, etc. 4) The levels of criticality span wide, from regular safe operations to dangerous, near-collision maneuvers. Real collision, although relatively slight, is also included. 5) Maps with complete semantic information are provided with physical layers, reference lines, lanelet connections and traffic rules. The data is recorded from drones and traffic cameras. Statistics of the dataset in terms of number of entities and interaction density are also provided, along with some utilization examples in a variety of behavior-related research areas. The dataset can be downloaded via https://interaction-dataset.com.

</details>

<details>

<summary>2019-09-30 18:32:03 - Interpreting Adversarial Examples by Activation Promotion and Suppression</summary>

- *Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu Zhao, Quanfu Fan, Chuang Gan, Xue Lin*

- `1904.02057v2` - [abs](http://arxiv.org/abs/1904.02057v2) - [pdf](http://arxiv.org/pdf/1904.02057v2)

> It is widely known that convolutional neural networks (CNNs) are vulnerable to adversarial examples: images with imperceptible perturbations crafted to fool classifiers. However, interpretability of these perturbations is less explored in the literature. This work aims to better understand the roles of adversarial perturbations and provide visual explanations from pixel, image and network perspectives. We show that adversaries have a promotion-suppression effect (PSE) on neurons' activations and can be primarily categorized into three types: i) suppression-dominated perturbations that mainly reduce the classification score of the true label, ii) promotion-dominated perturbations that focus on boosting the confidence of the target label, and iii) balanced perturbations that play a dual role in suppression and promotion. We also provide image-level interpretability of adversarial examples. This links PSE of pixel-level perturbations to class-specific discriminative image regions localized by class activation mapping (Zhou et al. 2016). Further, we examine the adversarial effect through network dissection (Bau et al. 2017), which offers concept-level interpretability of hidden units. We show that there exists a tight connection between the units' sensitivity to adversarial attacks and their interpretability on semantic concepts. Lastly, we provide some new insights from our interpretation to improve the adversarial robustness of networks.

</details>

<details>

<summary>2019-09-30 18:58:03 - Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations</summary>

- *Po-Yao Huang, Xiaojun Chang, Alexander Hauptmann*

- `1910.00058v1` - [abs](http://arxiv.org/abs/1910.00058v1) - [pdf](http://arxiv.org/pdf/1910.00058v1)

> With the aim of promoting and understanding the multilingual version of image search, we leverage visual object detection and propose a model with diverse multi-head attention to learn grounded multilingual multimodal representations. Specifically, our model attends to different types of textual semantics in two languages and visual objects for fine-grained alignments between sentences and images. We introduce a new objective function which explicitly encourages attention diversity to learn an improved visual-semantic embedding space. We evaluate our model in the German-Image and English-Image matching tasks on the Multi30K dataset, and in the Semantic Textual Similarity task with the English descriptions of visual content. Results show that our model yields a significant performance gain over other methods in all of the three tasks.

</details>


## 2019-10

<details>

<summary>2019-10-01 00:47:31 - Specializing Word Embeddings (for Parsing) by Information Bottleneck</summary>

- *Xiang Lisa Li, Jason Eisner*

- `1910.00163v1` - [abs](http://arxiv.org/abs/1910.00163v1) - [pdf](http://arxiv.org/pdf/1910.00163v1)

> Pre-trained word embeddings like ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck (VIB) method to nonlinearly compress these embeddings, keeping only the information that helps a discriminative parser. We compress each word embedding to either a discrete tag or a continuous vector. In the discrete version, our automatically compressed tags form an alternative tag set: we show experimentally that our tags capture most of the information in traditional POS tag annotations, but our tag sequences can be parsed more accurately at the same level of tag granularity. In the continuous version, we show experimentally that moderately compressing the word embeddings by our method yields a more accurate parser in 8 of 9 languages, unlike simple dimensionality reduction.

</details>

<details>

<summary>2019-10-01 04:07:15 - Writing habits and telltale neighbors: analyzing clinical concept usage patterns with sublanguage embeddings</summary>

- *Denis Newman-Griffis, Eric Fosler-Lussier*

- `1910.00192v1` - [abs](http://arxiv.org/abs/1910.00192v1) - [pdf](http://arxiv.org/pdf/1910.00192v1)

> Natural language processing techniques are being applied to increasingly diverse types of electronic health records, and can benefit from in-depth understanding of the distinguishing characteristics of medical document types. We present a method for characterizing the usage patterns of clinical concepts among different document types, in order to capture semantic differences beyond the lexical level. By training concept embeddings on clinical documents of different types and measuring the differences in their nearest neighborhood structures, we are able to measure divergences in concept usage while correcting for noise in embedding learning. Experiments on the MIMIC-III corpus demonstrate that our approach captures clinically-relevant differences in concept usage and provides an intuitive way to explore semantic characteristics of clinical document collections.

</details>

<details>

<summary>2019-10-01 09:23:35 - Robust Semantic Parsing with Adversarial Learning for Domain Generalization</summary>

- *Gabriel Marzinotto, Geraldine Damnati, Frédéric Béchet, Benoit Favre*

- `1910.06700v1` - [abs](http://arxiv.org/abs/1910.06700v1) - [pdf](http://arxiv.org/pdf/1910.06700v1)

> This paper addresses the issue of generalization for Semantic Parsing in an adversarial framework. Building models that are more robust to inter-document variability is crucial for the integration of Semantic Parsing technologies in real applications. The underlying question throughout this study is whether adversarial learning can be used to train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations.We propose to perform Semantic Parsing with a domain classification adversarial task without explicit knowledge of the domain. The strategy is first evaluated on a French corpus of encyclopedic documents, annotated with FrameNet, in an information retrieval perspective, then on PropBank Semantic Role Labeling task on the CoNLL-2005 benchmark. We show that adversarial learning increases all models generalization capabilities both on in and out-of-domain data.

</details>

<details>

<summary>2019-10-01 09:39:07 - Bad Form: Comparing Context-Based and Form-Based Few-Shot Learning in Distributional Semantic Models</summary>

- *Jeroen Van Hautte, Guy Emerson, Marek Rei*

- `1910.00275v1` - [abs](http://arxiv.org/abs/1910.00275v1) - [pdf](http://arxiv.org/pdf/1910.00275v1)

> Word embeddings are an essential component in a wide range of natural language processing applications. However, distributional semantic models are known to struggle when only a small number of context sentences are available. Several methods have been proposed to obtain higher-quality vectors for these words, leveraging both this context information and sometimes the word forms themselves through a hybrid approach. We show that the current tasks do not suffice to evaluate models that use word-form information, as such models can easily leverage word forms in the training data that are related to word forms in the test data. We introduce 3 new tasks, allowing for a more balanced comparison between models. Furthermore, we show that hyperparameters that have largely been ignored in previous work can consistently improve the performance of both baseline and advanced models, achieving a new state of the art on 4 out of 6 tasks.

</details>

<details>

<summary>2019-10-01 10:28:32 - Distance-Based Approaches to Repair Semantics in Ontology-based Data Access</summary>

- *César Prouté, Bruno Yun, Madalina Croitoru*

- `1910.00293v1` - [abs](http://arxiv.org/abs/1910.00293v1) - [pdf](http://arxiv.org/pdf/1910.00293v1)

> In the presence of inconsistencies, repair techniques thrive to restore consistency by reasoning with several repairs. However, since the number of repairs can be large, standard inconsistent tolerant semantics usually yield few answers. In this paper, we use the notion of syntactic distance between repairs following the intuition that it can allow us to cluster some repairs "close" to each other. In this way, we propose a generic framework to answer queries in a more personalise fashion.

</details>

<details>

<summary>2019-10-01 12:18:42 - Towards French Smart Building Code: Compliance Checking Based on Semantic Rules</summary>

- *Nicolas Bus, Ana Roxin, Guillaume Picinbono, Muhammad Fahad*

- `1910.00334v1` - [abs](http://arxiv.org/abs/1910.00334v1) - [pdf](http://arxiv.org/pdf/1910.00334v1)

> Manually checking models for compliance against building regulation is a time-consuming task for architects and construction engineers. There is thus a need for algorithms that process information from construction projects and report non-compliant elements. Still automated code-compliance checking raises several obstacles. Building regulations are usually published as human readable texts and their content is often ambiguous or incomplete. Also, the vocabulary used for expressing such regulations is very different from the vocabularies used to express Building Information Models (BIM). Furthermore, the high level of details associated to BIM-contained geometries induces complex calculations. Finally, the level of complexity of the IFC standard also hinders the automation of IFC processing tasks. Model chart, formal rules and pre-processors approach allows translating construction regulations into semantic queries. We further demonstrate the usefulness of this approach through several use cases. We argue our approach is a step forward in bridging the gap between regulation texts and automated checking algorithms. Finally with the recent building ontology BOT recommended by the W3C Linked Building Data Community Group, we identify perspectives for standardizing and extending our approach.

</details>

<details>

<summary>2019-10-01 13:26:04 - Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings</summary>

- *Gregor Wiedemann, Steffen Remus, Avi Chawla, Chris Biemann*

- `1909.10430v2` - [abs](http://arxiv.org/abs/1909.10430v2) - [pdf](http://arxiv.org/pdf/1909.10430v2)

> Contextualized word embeddings (CWE) such as provided by ELMo (Peters et al., 2018), Flair NLP (Akbik et al., 2018), or BERT (Devlin et al., 2019) are a major recent innovation in NLP. CWEs provide semantic vector representations of words depending on their respective context. Their advantage over static word embeddings has been shown for a number of tasks, such as text classification, sequence tagging, or machine translation. Since vectors of the same word type can vary depending on the respective context, they implicitly provide a model for word sense disambiguation (WSD). We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs. We compare the performance of different CWE models for the task and can report improvements above the current state of the art for two standard WSD benchmark datasets. We further show that the pre-trained BERT model is able to place polysemic words into distinct 'sense' regions of the embedding space, while ELMo and Flair NLP do not seem to possess this ability.

</details>

<details>

<summary>2019-10-01 14:31:19 - Reverse Engineering x86 Processor Microcode</summary>

- *Philipp Koppe, Benjamin Kollenda, Marc Fyrbiak, Christian Kison, Robert Gawlik, Christof Paar, Thorsten Holz*

- `1910.00948v1` - [abs](http://arxiv.org/abs/1910.00948v1) - [pdf](http://arxiv.org/pdf/1910.00948v1)

> Microcode is an abstraction layer on top of the physical components of a CPU and present in most general-purpose CPUs today. In addition to facilitate complex and vast instruction sets, it also provides an update mechanism that allows CPUs to be patched in-place without requiring any special hardware. While it is well-known that CPUs are regularly updated with this mechanism, very little is known about its inner workings given that microcode and the update mechanism are proprietary and have not been throughly analyzed yet.   In this paper, we reverse engineer the microcode semantics and inner workings of its update mechanism of conventional COTS CPUs on the example of AMD's K8 and K10 microarchitectures. Furthermore, we demonstrate how to develop custom microcode updates. We describe the microcode semantics and additionally present a set of microprograms that demonstrate the possibilities offered by this technology. To this end, our microprograms range from CPU-assisted instrumentation to microcoded Trojans that can even be reached from within a web browser and enable remote code execution and cryptographic implementation attacks.

</details>

<details>

<summary>2019-10-01 14:56:08 - Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation</summary>

- *Ivan Donadello, Luciano Serafini*

- `1910.00462v1` - [abs](http://arxiv.org/abs/1910.00462v1) - [pdf](http://arxiv.org/pdf/1910.00462v1)

> Semantic Image Interpretation is the task of extracting a structured semantic description from images. This requires the detection of visual relationships: triples (subject,relation,object) describing a semantic relation between a subject and an object. A pure supervised approach to visual relationship detection requires a complete and balanced training set for all the possible combinations of (subject, relation, object). However, such training sets are not available and would require a prohibitive human effort. This implies the ability of predicting triples which do not appear in the training set. This problem is called zero-shot learning. State-of-the-art approaches to zero-shot learning exploit similarities among relationships in the training set or external linguistic knowledge. In this paper, we perform zero-shot learning by using Logic Tensor Networks, a novel Statistical Relational Learning framework that exploits both the similarities with other seen relationships and background knowledge, expressed with logical constraints between subjects, relations and objects. The experiments on the Visual Relationship Dataset show that the use of logical constraints outperforms the current methods. This implies that background knowledge can be used to alleviate the incompleteness of training sets.

</details>

<details>

<summary>2019-10-01 17:43:34 - Understanding Early Word Learning in Situated Artificial Agents</summary>

- *Felix Hill, Stephen Clark, Karl Moritz Hermann, Phil Blunsom*

- `1710.09867v2` - [abs](http://arxiv.org/abs/1710.09867v2) - [pdf](http://arxiv.org/pdf/1710.09867v2)

> Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome challenges that infants face when learning their first words. While it is notable that models with no meaningful prior knowledge overcome these obstacles, researchers currently lack a clear understanding of how they do so, a problem that we attempt to address in this paper. For maximum control and generality, we focus on a simple neural network-based language learning agent, trained via policy-gradient methods, which can interpret single-word instructions in a simulated 3D world. Whilst the goal is not to explicitly model infant word learning, we take inspiration from experimental paradigms in developmental psychology and apply some of these to the artificial agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel method for visualising semantic representations in the agent.

</details>

<details>

<summary>2019-10-02 00:20:23 - Comparing Deep Learning Models for Multi-cell Classification in Liquid-based Cervical Cytology Images</summary>

- *Sudhir Sornapudi, G. T. Brown, Zhiyun Xue, Rodney Long, Lisa Allen, Sameer Antani*

- `1910.00722v1` - [abs](http://arxiv.org/abs/1910.00722v1) - [pdf](http://arxiv.org/pdf/1910.00722v1)

> Liquid-based cytology (LBC) is a reliable automated technique for the screening of Papanicolaou (Pap) smear data. It is an effective technique for collecting a majority of the cervical cells and aiding cytopathologists in locating abnormal cells. Most methods published in the research literature rely on accurate cell segmentation as a prior, which remains challenging due to a variety of factors, e.g., stain consistency, presence of clustered cells, etc. We propose a method for automatic classification of cervical slide images through generation of labeled cervical patch data and extracting deep hierarchical features by fine-tuning convolution neural networks, as well as a novel graph-based cell detection approach for cellular level evaluation. The results show that the proposed pipeline can classify images of both single cell and overlapping cells. The VGG-19 model is found to be the best at classifying the cervical cytology patch data with 95 % accuracy under precision-recall curve.

</details>

<details>

<summary>2019-10-02 06:54:46 - TFLMS: Large Model Support in TensorFlow by Graph Rewriting</summary>

- *Tung D. Le, Haruki Imai, Yasushi Negishi, Kiyokuni Kawachiya*

- `1807.02037v2` - [abs](http://arxiv.org/abs/1807.02037v2) - [pdf](http://arxiv.org/pdf/1807.02037v2)

> While accelerators such as GPUs have limited memory, deep neural networks are becoming larger and will not fit with the memory limitation of accelerators for training. We propose an approach to tackle this problem by rewriting the computational graph of a neural network, in which swap-out and swap-in operations are inserted to temporarily store intermediate results on CPU memory. In particular, we first revise the concept of a computational graph by defining a concrete semantics for variables in a graph. We then formally show how to derive swap-out and swap-in operations from an existing graph and present rules to optimize the graph. To realize our approach, we developed a module in TensorFlow, named TFLMS. TFLMS is published as a pull request in the TensorFlow repository for contributing to the TensorFlow community. With TFLMS, we were able to train ResNet-50 and 3DUnet with 4.7x and 2x larger batch size, respectively. In particular, we were able to train 3DUNet using images of size of $192^3$ for image segmentation, which, without TFLMS, had been done only by dividing the images to smaller images, which affects the accuracy.

</details>

<details>

<summary>2019-10-02 08:22:03 - Abstractive Dialog Summarization with Semantic Scaffolds</summary>

- *Lin Yuan, Zhou Yu*

- `1910.00825v1` - [abs](http://arxiv.org/abs/1910.00825v1) - [pdf](http://arxiv.org/pdf/1910.00825v1)

> The demand for abstractive dialog summary is growing in real-world applications. For example, customer service center or hospitals would like to summarize customer service interaction and doctor-patient interaction. However, few researchers explored abstractive summarization on dialogs due to the lack of suitable datasets. We propose an abstractive dialog summarization dataset based on MultiWOZ. If we directly apply previous state-of-the-art document summarization methods on dialogs, there are two significant drawbacks: the informative entities such as restaurant names are difficult to preserve, and the contents from different dialog domains are sometimes mismatched. To address these two drawbacks, we propose Scaffold Pointer Network (SPNet)to utilize the existing annotation on speaker role, semantic slot and dialog domain. SPNet incorporates these semantic scaffolds for dialog summarization. Since ROUGE cannot capture the two drawbacks mentioned, we also propose a new evaluation metric that considers critical informative entities in the text. On MultiWOZ, our proposed SPNet outperforms state-of-the-art abstractive summarization methods on all the automatic and human evaluation metrics.

</details>

<details>

<summary>2019-10-02 12:39:53 - Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU</summary>

- *Andrea Vanzo, Emanuele Bastianelli, Oliver Lemon*

- `1910.00912v1` - [abs](http://arxiv.org/abs/1910.00912v1) - [pdf](http://arxiv.org/pdf/1910.00912v1)

> We present a new neural architecture for wide-coverage Natural Language Understanding in Spoken Dialogue Systems. We develop a hierarchical multi-task architecture, which delivers a multi-layer representation of sentence meaning (i.e., Dialogue Acts and Frame-like structures). The architecture is a hierarchy of self-attention mechanisms and BiLSTM encoders followed by CRF tagging layers. We describe a variety of experiments, showing that our approach obtains promising results on a dataset annotated with Dialogue Acts and Frame Semantics. Moreover, we demonstrate its applicability to a different, publicly available NLU dataset annotated with domain-specific intents and corresponding semantic roles, providing overall performance higher than state-of-the-art tools such as RASA, Dialogflow, LUIS, and Watson. For example, we show an average 4.45% improvement in entity tagging F-score over Rasa, Dialogflow and LUIS.

</details>

<details>

<summary>2019-10-02 13:05:48 - A CCG-based Compositional Semantics and Inference System for Comparatives</summary>

- *Izumi Haruta, Koji Mineshima, Daisuke Bekki*

- `1910.00930v1` - [abs](http://arxiv.org/abs/1910.00930v1) - [pdf](http://arxiv.org/pdf/1910.00930v1)

> Comparative constructions play an important role in natural language inference. However, attempts to study semantic representations and logical inferences for comparatives from the computational perspective are not well developed, due to the complexity of their syntactic structures and inference patterns. In this study, using a framework based on Combinatory Categorial Grammar (CCG), we present a compositional semantics that maps various comparative constructions in English to semantic representations and introduces an inference system that effectively handles logical inference with comparatives, including those involving numeral adjectives, antonyms, and quantification. We evaluate the performance of our system on the FraCaS test suite and show that the system can handle a variety of complex logical inferences with comparatives.

</details>

<details>

<summary>2019-10-02 14:27:11 - Tree-Structured Semantic Encoder with Knowledge Sharing for Domain Adaptation in Natural Language Generation</summary>

- *Bo-Hsiang Tseng, Paweł Budzianowski, Yen-Chen Wu, Milica Gašić*

- `1910.06719v1` - [abs](http://arxiv.org/abs/1910.06719v1) - [pdf](http://arxiv.org/pdf/1910.06719v1)

> Domain adaptation in natural language generation (NLG) remains challenging because of the high complexity of input semantics across domains and limited data of a target domain. This is particularly the case for dialogue systems, where we want to be able to seamlessly include new domains into the conversation. Therefore, it is crucial for generation models to share knowledge across domains for the effective adaptation from one domain to another. In this study, we exploit a tree-structured semantic encoder to capture the internal structure of complex semantic representations required for multi-domain dialogues in order to facilitate knowledge sharing across domains. In addition, a layer-wise attention mechanism between the tree encoder and the decoder is adopted to further improve the model's capability. The automatic evaluation results show that our model outperforms previous methods in terms of the BLEU score and the slot error rate, in particular when the adaptation data is limited. In subjective evaluation, human judges tend to prefer the sentences generated by our model, rating them more highly on informativeness and naturalness than other systems.

</details>

<details>

<summary>2019-10-02 14:38:24 - Prediction Error Meta Classification in Semantic Segmentation: Detection via Aggregated Dispersion Measures of Softmax Probabilities</summary>

- *Matthias Rottmann, Pascal Colling, Thomas-Paul Hack, Robin Chan, Fabian Hüger, Peter Schlicht, Hanno Gottschalk*

- `1811.00648v2` - [abs](http://arxiv.org/abs/1811.00648v2) - [pdf](http://arxiv.org/pdf/1811.00648v2)

> We present a method that "meta" classifies whether seg-ments predicted by a semantic segmentation neural networkintersect with the ground truth. For this purpose, we employ measures of dispersion for predicted pixel-wise class probability distributions, like classification entropy, that yield heat maps of the input scene's size. We aggregate these dispersion measures segment-wise and derive metrics that are well-correlated with the segment-wise IoU of prediction and ground truth. This procedure yields an almost plug and play post-processing tool to rate the prediction quality of semantic segmentation networks on segment level. This is especially relevant for monitoring neural networks in online applications like automated driving or medical imaging where reliability is of utmost importance. In our tests, we use publicly available state-of-the-art networks trained on the Cityscapes dataset and the BraTS2017 dataset and analyze the predictive power of different metrics as well as different sets of metrics. To this end, we compute logistic LASSO regression fits for the task of classifying IoU=0 vs. IoU>0 per segment and obtain AUROC values of up to 91.55%. We complement these tests with linear regression fits to predict the segment-wise IoU and obtain prediction standard deviations of down to 0.130 as well as $R^2$ values of up to 84.15%. We show that these results clearly outperform standard approaches.

</details>

<details>

<summary>2019-10-03 01:26:31 - Learning Point Embeddings from Shape Repositories for Few-Shot Segmentation</summary>

- *Gopal Sharma, Evangelos Kalogerakis, Subhransu Maji*

- `1910.01269v1` - [abs](http://arxiv.org/abs/1910.01269v1) - [pdf](http://arxiv.org/pdf/1910.01269v1)

> User generated 3D shapes in online repositories contain rich information about surfaces, primitives, and their geometric relations, often arranged in a hierarchy. We present a framework for learning representations of 3D shapes that reflect the information present in this meta data and show that it leads to improved generalization for semantic segmentation tasks. Our approach is a point embedding network that generates a vectorial representation of the 3D points such that it reflects the grouping hierarchy and tag data. The main challenge is that the data is noisy and highly variable. To this end, we present a tree-aware metric-learning approach and demonstrate that such learned embeddings offer excellent transfer to semantic segmentation tasks, especially when training data is limited. Our approach reduces the relative error by $10.2\%$ with $8$ training examples, by $11.72\%$ with $120$ training examples on the ShapeNet semantic segmentation benchmark, in comparison to the network trained from scratch. By utilizing tag data the relative error is reduced by $12.8\%$ with $8$ training examples, in comparison to the network trained from scratch. These improvements come at no additional labeling cost as the meta data is freely available.

</details>

<details>

<summary>2019-10-03 01:28:31 - Can Automated Program Repair Refine Fault Localization?</summary>

- *Yiling Lou, Ali Ghanbari, Xia Li, Lingming Zhang, Dan Hao, Lu Zhang*

- `1910.01270v1` - [abs](http://arxiv.org/abs/1910.01270v1) - [pdf](http://arxiv.org/pdf/1910.01270v1)

> Software bugs are prevalent in modern software systems and notoriously hard to debug manually. Therefore, a large body of research efforts have been dedicated to automated software debugging, including both automated fault localization and program repair. However, the existing fault localization techniques are usually ineffective on real-world software systems while even the most advanced program repair techniques can only fix a small ratio of real-world bugs. Although fault localization and program repair are inherently connected, we observe that in the literature their only connection is that program repair techniques usually use off-the-shelf fault localization techniques (e.g., Ochiai) to determine the potential candidate statements/elements for patching. In this work, we explore their connection in the other direction, i.e., can program repair in turn help with fault localization? In this way,we not only open a new dimension for more powerful fault localization, but also extend the application scope of program repair to all possible bugs (not only the bugs that can be directly automatically fixed).We have designed ProFL, a simplistic approach using patch-execution results (from program repair) as the feedback information for fault localization. The experimental results on the widely used Defects4J benchmark show that the basic ProFL can already localize 161 of the 395 studied bugs within Top-1, while state-of-the-art spectrum and mutation based fault localization techniques at most localize 117 within Top-1. We also demonstrate ProFL's effectiveness under different settings. Lastly, we show that ProFL can further boost state-of-the-art fault localization via both unsupervised and supervised learning.

</details>

<details>

<summary>2019-10-03 01:51:17 - Extracting UMLS Concepts from Medical Text Using General and Domain-Specific Deep Learning Models</summary>

- *Kathleen C. Fraser, Isar Nejadgholi, Berry De Bruijn, Muqun Li, Astha LaPlante, Khaldoun Zine El Abidine*

- `1910.01274v1` - [abs](http://arxiv.org/abs/1910.01274v1) - [pdf](http://arxiv.org/pdf/1910.01274v1)

> Entity recognition is a critical first step to a number of clinical NLP applications, such as entity linking and relation extraction. We present the first attempt to apply state-of-the-art entity recognition approaches on a newly released dataset, MedMentions. This dataset contains over 4000 biomedical abstracts, annotated for UMLS semantic types. In comparison to existing datasets, MedMentions contains a far greater number of entity types, and thus represents a more challenging but realistic scenario in a real-world setting. We explore a number of relevant dimensions, including the use of contextual versus non-contextual word embeddings, general versus domain-specific unsupervised pre-training, and different deep learning architectures. We contrast our results against the well-known i2b2 2010 entity recognition dataset, and propose a new method to combine general and domain-specific information. While producing a state-of-the-art result for the i2b2 2010 task (F1 = 0.90), our results on MedMentions are significantly lower (F1 = 0.63), suggesting there is still plenty of opportunity for improvement on this new data.

</details>

<details>

<summary>2019-10-03 06:13:59 - Eradicating Attacks on the Internal Network with Internal Network Policy</summary>

- *Yehuda Afek, Anat Bremler-Barr, Alon Noy*

- `1910.00975v2` - [abs](http://arxiv.org/abs/1910.00975v2) - [pdf](http://arxiv.org/pdf/1910.00975v2)

> In this paper we present three attacks on private internal networks behind a NAT and a corresponding new protection mechanism, Internal Network Policy, to mitigate a wide range of attacks that penetrate internal networks behind a NAT. In the attack scenario, a victim is tricked to visit the attacker's website, which contains a malicious script that lets the attacker access the victim's internal network in different ways, including opening a port in the NAT or sending a sophisticated request to local devices. The first attack utilizes DNS Rebinding in a particular way, while the other two demonstrate different methods of attacking the network, based on application security vulnerabilities. Following the attacks, we provide a new browser security policy, Internal Network Policy (INP), which protects against these types of vulnerabilities and attacks. This policy is implemented in the browser just like Same Origin Policy (SOP) and prevents malicious access to internal resources by external entities.

</details>

<details>

<summary>2019-10-03 14:08:57 - Generalization Bounds for Convolutional Neural Networks</summary>

- *Shan Lin, Jingwei Zhang*

- `1910.01487v1` - [abs](http://arxiv.org/abs/1910.01487v1) - [pdf](http://arxiv.org/pdf/1910.01487v1)

> Convolutional neural networks (CNNs) have achieved breakthrough performances in a wide range of applications including image classification, semantic segmentation, and object detection. Previous research on characterizing the generalization ability of neural networks mostly focuses on fully connected neural networks (FNNs), regarding CNNs as a special case of FNNs without taking into account the special structure of convolutional layers. In this work, we propose a tighter generalization bound for CNNs by exploiting the sparse and permutation structure of its weight matrices. As the generalization bound relies on the spectral norm of weight matrices, we further study spectral norms of three commonly used convolution operations including standard convolution, depthwise convolution, and pointwise convolution. Theoretical and experimental results both demonstrate that our bounds for CNNs are tighter than existing bounds.

</details>

<details>

<summary>2019-10-03 15:42:43 - DIRE: A Neural Approach to Decompiled Identifier Naming</summary>

- *Jeremy Lacomis, Pengcheng Yin, Edward J. Schwartz, Miltiadis Allamanis, Claire Le Goues, Graham Neubig, Bogdan Vasilescu*

- `1909.09029v2` - [abs](http://arxiv.org/abs/1909.09029v2) - [pdf](http://arxiv.org/pdf/1909.09029v2)

> The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.

</details>

<details>

<summary>2019-10-04 16:01:22 - PPGAN: Privacy-preserving Generative Adversarial Network</summary>

- *Yi Liu, Jialiang Peng, James J. Q Yu, Yi Wu*

- `1910.02007v1` - [abs](http://arxiv.org/abs/1910.02007v1) - [pdf](http://arxiv.org/pdf/1910.02007v1)

> Generative Adversarial Network (GAN) and its variants serve as a perfect representation of the data generation model, providing researchers with a large amount of high-quality generated data. They illustrate a promising direction for research with limited data availability. When GAN learns the semantic-rich data distribution from a dataset, the density of the generated distribution tends to concentrate on the training data. Due to the gradient parameters of the deep neural network contain the data distribution of the training samples, they can easily remember the training samples. When GAN is applied to private or sensitive data, for instance, patient medical records, as private information may be leakage. To address this issue, we propose a Privacy-preserving Generative Adversarial Network (PPGAN) model, in which we achieve differential privacy in GANs by adding well-designed noise to the gradient during the model learning procedure. Besides, we introduced the Moments Accountant strategy in the PPGAN training process to improve the stability and compatibility of the model by controlling privacy loss. We also give a mathematical proof of the differential privacy discriminator. Through extensive case studies of the benchmark datasets, we demonstrate that PPGAN can generate high-quality synthetic data while retaining the required data available under a reasonable privacy budget.

</details>

<details>

<summary>2019-10-04 17:28:20 - TechNet: Technology Semantic Network Based on Patent Data</summary>

- *Serhad Sarica, Jianxi Luo, Kristin L. Wood*

- `1906.00411v4` - [abs](http://arxiv.org/abs/1906.00411v4) - [pdf](http://arxiv.org/pdf/1906.00411v4)

> The growing developments in general semantic networks, knowledge graphs and ontology databases have motivated us to build a large-scale comprehensive semantic network of technology-related data for engineering knowledge discovery, technology search and retrieval, and artificial intelligence for engineering design and innovation. Specially, we constructed a technology semantic network (TechNet) that covers the elemental concepts in all domains of technology and their semantic associations by mining the complete U.S. patent database from 1976. To derive the TechNet, natural language processing techniques were utilized to extract terms from massive patent texts and recent word embedding algorithms were employed to vectorize such terms and establish their semantic relationships. We report and evaluate the TechNet for retrieving terms and their pairwise relevance that is meaningful from a technology and engineering design perspective. The TechNet may serve as an infrastructure to support a wide range of applications, e.g., technical text summaries, search query predictions, relational knowledge discovery, and design ideation support, in the context of engineering and technology, and complement or enrich existing semantic databases. To enable such applications, the TechNet is made public via an online interface and APIs for public users to retrieve technology-related terms and their relevancies.

</details>

<details>

<summary>2019-10-05 07:49:12 - On the Limits of Learning to Actively Learn Semantic Representations</summary>

- *Omri Koshorek, Gabriel Stanovsky, Yichu Zhou, Vivek Srikumar, Jonathan Berant*

- `1910.02228v1` - [abs](http://arxiv.org/abs/1910.02228v1) - [pdf](http://arxiv.org/pdf/1910.02228v1)

> One of the goals of natural language understanding is to develop models that map sentences into meaning representations. However, training such models requires expensive annotation of complex structures, which hinders their adoption. Learning to actively-learn (LTAL) is a recent paradigm for reducing the amount of labeled data by learning a policy that selects which samples should be labeled. In this work, we examine LTAL for learning semantic representations, such as QA-SRL. We show that even an oracle policy that is allowed to pick examples that maximize performance on the test set (and constitutes an upper bound on the potential of LTAL), does not substantially improve performance compared to a random policy. We investigate factors that could explain this finding and show that a distinguishing characteristic of successful applications of LTAL is the interaction between optimization and the oracle policy selection process. In successful applications of LTAL, the examples selected by the oracle policy do not substantially depend on the optimization procedure, while in our setup the stochastic nature of optimization strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving data efficiency in learning semantic meaning representations is limited.

</details>

<details>

<summary>2019-10-06 05:06:38 - Design and Use of Loop-Transformation Pragmas</summary>

- *Michael Kruse, Hal Finkel*

- `1910.02375v1` - [abs](http://arxiv.org/abs/1910.02375v1) - [pdf](http://arxiv.org/pdf/1910.02375v1)

> Adding a pragma directive into the source code is arguably easier than rewriting it, for instance for loop unrolling. Moreover, if the application is maintained for multiple platforms, their difference in performance characteristics may require different code transformations. Code transformation directives allow replacing the directives depending on the platform, i.e. separation of code semantics and its performance optimization.   In this paper, we explore the design space (syntax and semantics) of adding such directive into a future OpenMP specification. Using a prototype implementation in Clang, we demonstrate the usefulness of such directives on a few benchmarks.

</details>

<details>

<summary>2019-10-06 14:14:04 - A Short Remark on Analogical Reasoning</summary>

- *Karl Schlechta*

- `1910.02453v1` - [abs](http://arxiv.org/abs/1910.02453v1) - [pdf](http://arxiv.org/pdf/1910.02453v1)

> We discuss the problem of defining a logic for analogical reasoning, and sketch a solution in the style of the semantics for Counterfactual Conditionals, Preferential Structures, etc.

</details>

<details>

<summary>2019-10-06 17:08:33 - Unsupervised Projection Networks for Generative Adversarial Networks</summary>

- *Daiyaan Arfeen, Jesse Zhang*

- `1910.00579v2` - [abs](http://arxiv.org/abs/1910.00579v2) - [pdf](http://arxiv.org/pdf/1910.00579v2)

> We propose the use of unsupervised learning to train projection networks that project onto the latent space of an already trained generator. We apply our method to a trained StyleGAN, and use our projection network to perform image super-resolution and clustering of images into semantically identifiable groups.

</details>

<details>

<summary>2019-10-06 18:30:35 - FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System</summary>

- *Bowen Yu, Claudio T. Silva*

- `1908.00681v2` - [abs](http://arxiv.org/abs/1908.00681v2) - [pdf](http://arxiv.org/pdf/1908.00681v2)

> Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.

</details>

<details>

<summary>2019-10-07 08:04:19 - Towards a Data Centric Approach for the Design and Verification of Cryptographic Protocols</summary>

- *Luca Arnaboldi, Roberto Metere*

- `1910.02656v1` - [abs](http://arxiv.org/abs/1910.02656v1) - [pdf](http://arxiv.org/pdf/1910.02656v1)

> We propose MetaCP, a Meta Cryptography Protocol verification tool, as an automated tool simplifying the design of security protocols through a graphical interface. The graphical interface can be seen as a modern editor of a non-relational database whose data are protocols. The information of protocols are stored in XML, enjoying a fixed format and syntax aiming to contain all required information to specify any kind of protocol. This XML can be seen as an almost semanticless language, where different plugins confer strict semantics modelling the protocol into a variety of back-end verification languages. In this paper, we showcase the effectiveness of this novel approach by demonstrating how easy MetaCP makes it to design and verify a protocol going from the graphical design to formally verified protocol using a Tamarin prover plugin. Whilst similar approaches have been proposed in the past, most famously the AVISPA Tool, no previous approach provides such as small learning curve and ease of use even for non security professionals, combined with the flexibility to integrate with the state of the art verification tools.

</details>

<details>

<summary>2019-10-07 08:42:45 - Interpretable Disentanglement of Neural Networks by Extracting Class-Specific Subnetwork</summary>

- *Yulong Wang, Xiaolin Hu, Hang Su*

- `1910.02673v1` - [abs](http://arxiv.org/abs/1910.02673v1) - [pdf](http://arxiv.org/pdf/1910.02673v1)

> We propose a novel perspective to understand deep neural networks in an interpretable disentanglement form. For each semantic class, we extract a class-specific functional subnetwork from the original full model, with compressed structure while maintaining comparable prediction performance. The structure representations of extracted subnetworks display a resemblance to their corresponding class semantic similarities. We also apply extracted subnetworks in visual explanation and adversarial example detection tasks by merely replacing the original full model with class-specific subnetworks. Experiments demonstrate that this intuitive operation can effectively improve explanation saliency accuracy for gradient-based explanation methods, and increase the detection rate for confidence score-based adversarial example detection methods.

</details>

<details>

<summary>2019-10-07 10:36:40 - Push it to the Limit: Discover Edge-Cases in Image Data with Autoencoders</summary>

- *Ilja Manakov, Volker Tresp*

- `1910.02713v1` - [abs](http://arxiv.org/abs/1910.02713v1) - [pdf](http://arxiv.org/pdf/1910.02713v1)

> In this paper, we focus on the problem of identifying semantic factors of variation in large image datasets. By training a convolutional Autoencoder on the image data, we create encodings, which describe each datapoint at a higher level of abstraction than pixel-space. We then apply Principal Component Analysis to the encodings to disentangle the factors of variation in the data. Sorting the dataset according to the values of individual principal components, we find that samples at the high and low ends of the distribution often share specific semantic characteristics. We refer to these groups of samples as semantic groups. When applied to real-world data, this method can help discover unwanted edge-cases.

</details>

<details>

<summary>2019-10-07 11:38:28 - MaskParse@Deskin at SemEval-2019 Task 1: Cross-lingual UCCA Semantic Parsing using Recursive Masked Sequence Tagging</summary>

- *Gabriel Marzinotto, Johannes Heinecke, Geraldine Damnati*

- `1910.02733v1` - [abs](http://arxiv.org/abs/1910.02733v1) - [pdf](http://arxiv.org/pdf/1910.02733v1)

> This paper describes our recursive system for SemEval-2019 \textit{ Task 1: Cross-lingual Semantic Parsing with UCCA}. Each recursive step consists of two parts. We first perform semantic parsing using a sequence tagger to estimate the probabilities of the UCCA categories in the sentence. Then, we apply a decoding policy which interprets these probabilities and builds the graph nodes. Parsing is done recursively, we perform a first inference on the sentence to extract the main scenes and links and then we recursively apply our model on the sentence using a masking feature that reflects the decisions made in previous steps. Process continues until the terminal nodes are reached. We choose a standard neural tagger and we focused on our recursive parsing strategy and on the cross lingual transfer problem to develop a robust model for the French language, using only few training samples.

</details>

<details>

<summary>2019-10-07 11:41:14 - Adapting a FrameNet Semantic Parser for Spoken Language Understanding Using Adversarial Learning</summary>

- *Gabriel Marzinotto, Geraldine Damnati, Frédéric Béchet*

- `1910.02734v1` - [abs](http://arxiv.org/abs/1910.02734v1) - [pdf](http://arxiv.org/pdf/1910.02734v1)

> This paper presents a new semantic frame parsing model, based on Berkeley FrameNet, adapted to process spoken documents in order to perform information extraction from broadcast contents. Building upon previous work that had shown the effectiveness of adversarial learning for domain generalization in the context of semantic parsing of encyclopedic written documents, we propose to extend this approach to elocutionary style generalization. The underlying question throughout this study is whether adversarial learning can be used to combine data from different sources and train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations as well as automatic speech recognition errors. The proposed strategy is evaluated on a French corpus of encyclopedic written documents and a smaller corpus of radio podcast transcriptions, both annotated with a FrameNet paradigm. We show that adversarial learning increases all models generalization capabilities both on manual and automatic speech transcription as well as on encyclopedic data.

</details>

<details>

<summary>2019-10-07 14:06:38 - Semantic Preserving Generative Adversarial Models</summary>

- *Shahar Harel, Meir Maor, Amir Ronen*

- `1910.02804v1` - [abs](http://arxiv.org/abs/1910.02804v1) - [pdf](http://arxiv.org/pdf/1910.02804v1)

> We introduce generative adversarial models in which the discriminator is replaced by a calibrated (non-differentiable) classifier repeatedly enhanced by domain relevant features. The role of the classifier is to prove that the actual and generated data differ over a controlled semantic space. We demonstrate that such models have the ability to generate objects with strong guarantees on their properties in a wide range of domains. They require less data than ordinary GANs, provide natural stopping conditions, uncover important properties of the data, and enhance transfer learning. Our techniques can be combined with standard generative models. We demonstrate the usefulness of our approach by applying it to several unrelated domains: generating good locations for cellular antennae, molecule generation preserving key chemical properties, and generating and extrapolating lines from very few data points. Intriguing open problems are presented as well.

</details>

<details>

<summary>2019-10-07 16:44:32 - Correlations between Word Vector Sets</summary>

- *Vitalii Zhelezniak, April Shen, Daniel Busbridge, Aleksandar Savkov, Nils Hammerla*

- `1910.02902v1` - [abs](http://arxiv.org/abs/1910.02902v1) - [pdf](http://arxiv.org/pdf/1910.02902v1)

> Similarity measures based purely on word embeddings are comfortably competing with much more sophisticated deep learning and expert-engineered systems on unsupervised semantic textual similarity (STS) tasks. In contrast to commonly used geometric approaches, we treat a single word embedding as e.g. 300 observations from a scalar random variable. Using this paradigm, we first illustrate that similarities derived from elementary pooling operations and classic correlation coefficients yield excellent results on standard STS benchmarks, outperforming many recently proposed methods while being much faster and trivial to implement. Next, we demonstrate how to avoid pooling operations altogether and compare sets of word embeddings directly via correlation operators between reproducing kernel Hilbert spaces. Just like cosine similarity is used to compare individual word vectors, we introduce a novel application of the centered kernel alignment (CKA) as a natural generalisation of squared cosine similarity for sets of word vectors. Likewise, CKA is very easy to implement and enjoys very strong empirical results.

</details>

<details>

<summary>2019-10-07 19:11:52 - Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness</summary>

- *Luke S. Snyder, Yi-Shan Lin, Morteza Karimzadeh, Dan Goldwasser, David S. Ebert*

- `1908.02588v2` - [abs](http://arxiv.org/abs/1908.02588v2) - [pdf](http://arxiv.org/pdf/1908.02588v2)

> Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users' knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework's effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system.

</details>

<details>

<summary>2019-10-07 23:46:49 - Capturing Argument Interaction in Semantic Role Labeling with Capsule Networks</summary>

- *Xinchi Chen, Chunchuan Lyu, Ivan Titov*

- `1910.03136v1` - [abs](http://arxiv.org/abs/1910.03136v1) - [pdf](http://arxiv.org/pdf/1910.03136v1)

> Semantic role labeling (SRL) involves extracting propositions (i.e. predicates and their typed arguments) from natural language sentences. State-of-the-art SRL models rely on powerful encoders (e.g., LSTMs) and do not model non-local interaction between arguments. We propose a new approach to modeling these interactions while maintaining efficient inference. Specifically, we use Capsule Networks: each proposition is encoded as a tuple of \textit{capsules}, one capsule per argument type (i.e. role). These tuples serve as embeddings of entire propositions. In every network layer, the capsules interact with each other and with representations of words in the sentence. Each iteration results in updated proposition embeddings and updated predictions about the SRL structure. Our model substantially outperforms the non-refinement baseline model on all 7 CoNLL-2019 languages and achieves state-of-the-art results on 5 languages (including English) for dependency SRL. We analyze the types of mistakes corrected by the refinement procedure. For example, each role is typically (but not always) filled with at most one argument. Whereas enforcing this approximate constraint is not useful with the modern SRL system, iterative procedure corrects the mistakes by capturing this intuition in a flexible and context-sensitive way.

</details>

<details>

<summary>2019-10-08 13:33:48 - Linguistically Informed Relation Extraction and Neural Architectures for Nested Named Entity Recognition in BioNLP-OST 2019</summary>

- *Usama Yaseen, Pankaj Gupta, Hinrich Schütze*

- `1910.03385v1` - [abs](http://arxiv.org/abs/1910.03385v1) - [pdf](http://arxiv.org/pdf/1910.03385v1)

> Named Entity Recognition (NER) and Relation Extraction (RE) are essential tools in distilling knowledge from biomedical literature. This paper presents our findings from participating in BioNLP Shared Tasks 2019. We addressed Named Entity Recognition including nested entities extraction, Entity Normalization and Relation Extraction. Our proposed approach of Named Entities can be generalized to different languages and we have shown it's effectiveness for English and Spanish text. We investigated linguistic features, hybrid loss including ranking and Conditional Random Fields (CRF), multi-task objective and token-level ensembling strategy to improve NER. We employed dictionary based fuzzy and semantic search to perform Entity Normalization. Finally, our RE system employed Support Vector Machine (SVM) with linguistic features.   Our NER submission (team:MIC-CIS) ranked first in BB-2019 norm+NER task with standard error rate (SER) of 0.7159 and showed competitive performance on PharmaCo NER task with F1-score of 0.8662. Our RE system ranked first in the SeeDev-binary Relation Extraction Task with F1-score of 0.3738.

</details>

<details>

<summary>2019-10-08 16:25:45 - Predicting Auction Price of Vehicle License Plate with Deep Recurrent Neural Network</summary>

- *Vinci Chow*

- `1701.08711v5` - [abs](http://arxiv.org/abs/1701.08711v5) - [pdf](http://arxiv.org/pdf/1701.08711v5)

> In Chinese societies, superstition is of paramount importance, and vehicle license plates with desirable numbers can fetch very high prices in auctions. Unlike other valuable items, license plates are not allocated an estimated price before auction. I propose that the task of predicting plate prices can be viewed as a natural language processing (NLP) task, as the value depends on the meaning of each individual character on the plate and its semantics. I construct a deep recurrent neural network (RNN) to predict the prices of vehicle license plates in Hong Kong, based on the characters on a plate. I demonstrate the importance of having a deep network and of retraining. Evaluated on 13 years of historical auction prices, the deep RNN's predictions can explain over 80 percent of price variations, outperforming previous models by a significant margin. I also demonstrate how the model can be extended to become a search engine for plates and to provide estimates of the expected price distribution.

</details>

<details>

<summary>2019-10-09 04:38:34 - Generative Question Refinement with Deep Reinforcement Learning in Retrieval-based QA System</summary>

- *Ye Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, Philip S. Yu*

- `1908.05604v3` - [abs](http://arxiv.org/abs/1908.05604v3) - [pdf](http://arxiv.org/pdf/1908.05604v3)

> In real-world question-answering (QA) systems, ill-formed questions, such as wrong words, ill word order, and noisy expressions, are common and may prevent the QA systems from understanding and answering them accurately. In order to eliminate the effect of ill-formed questions, we approach the question refinement task and propose a unified model, QREFINE, to refine the ill-formed questions to well-formed question. The basic idea is to learn a Seq2Seq model to generate a new question from the original one. To improve the quality and retrieval performance of the generated questions, we make two major improvements: 1) To better encode the semantics of ill-formed questions, we enrich the representation of questions with character embedding and the recent proposed contextual word embedding such as BERT, besides the traditional context-free word embeddings; 2) To make it capable to generate desired questions, we train the model with deep reinforcement learning techniques that considers an appropriate wording of the generation as an immediate reward and the correlation between generated question and answer as time-delayed long-term rewards. Experimental results on real-world datasets show that the proposed QREFINE method can generate refined questions with more readability but fewer mistakes than the original questions provided by users. Moreover, the refined questions also significantly improve the accuracy of answer retrieval.

</details>

<details>

<summary>2019-10-09 16:11:17 - Patch Refinement -- Localized 3D Object Detection</summary>

- *Johannes Lehner, Andreas Mitterecker, Thomas Adler, Markus Hofmarcher, Bernhard Nessler, Sepp Hochreiter*

- `1910.04093v1` - [abs](http://arxiv.org/abs/1910.04093v1) - [pdf](http://arxiv.org/pdf/1910.04093v1)

> We introduce Patch Refinement a two-stage model for accurate 3D object detection and localization from point cloud data. Patch Refinement is composed of two independently trained Voxelnet-based networks, a Region Proposal Network (RPN) and a Local Refinement Network (LRN). We decompose the detection task into a preliminary Bird's Eye View (BEV) detection step and a local 3D detection step. Based on the proposed BEV locations by the RPN, we extract small point cloud subsets ("patches"), which are then processed by the LRN, which is less limited by memory constraints due to the small area of each patch. Therefore, we can apply encoding with a higher voxel resolution locally. The independence of the LRN enables the use of additional augmentation techniques and allows for an efficient, regression focused training as it uses only a small fraction of each scene. Evaluated on the KITTI 3D object detection benchmark, our submission from January 28, 2019, outperformed all previous entries on all three difficulties of the class car, using only 50 % of the available training data and only LiDAR information.

</details>

<details>

<summary>2019-10-09 18:10:41 - SemanticLock: An authentication method for mobile devices using semantically-linked images</summary>

- *Ilesanmi Olade, Haining Liang, Charles Fleming*

- `1806.11361v3` - [abs](http://arxiv.org/abs/1806.11361v3) - [pdf](http://arxiv.org/pdf/1806.11361v3)

> We introduce SemanticLock, a single factor graphical authentication solution for mobile devices. SemanticLock uses a set of graphical images as password tokens that construct a semantically memorable story representing the user`s password. A familiar and quick action of dragging or dropping the images into their respective positions either in a \textit{continous flow} or in \textit{discrete} movements on the the touchscreen is what is required to use our solution.   The authentication strength of the SemanticLock is based on the large number of possible semantic constructs derived from the positioning of the image tokens and the type of images selected. Semantic Lock has a high resistance to smudge attacks and it equally exhibits a higher level of memorability due to its graphical paradigm.   In a three weeks user study with 21 participants comparing SemanticLock against other authentication systems, we discovered that SemanticLock outperformed the PIN and matched the PATTERN both on speed, memorability, user acceptance and usability. Furthermore, qualitative test also show that SemanticLock was rated more superior in like-ability. SemanticLock was also evaluated while participants walked unencumbered and walked encumbered carrying "everyday" items to analyze the effects of such activities on its usage.

</details>

<details>

<summary>2019-10-09 18:34:49 - Relation-Aware Graph Attention Network for Visual Question Answering</summary>

- *Linjie Li, Zhe Gan, Yu Cheng, Jingjing Liu*

- `1903.12314v3` - [abs](http://arxiv.org/abs/1903.12314v3) - [pdf](http://arxiv.org/pdf/1903.12314v3)

> In order to answer semantically-complicated questions about an image, a Visual Question Answering (VQA) model needs to fully understand the visual scene in the image, especially the interactive dynamics between different objects. We propose a Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. Two types of visual object relations are explored: (i) Explicit Relations that represent geometric positions and semantic interactions between objects; and (ii) Implicit Relations that capture the hidden dynamics between image regions. Experiments demonstrate that ReGAT outperforms prior state-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further show that ReGAT is compatible to existing VQA architectures, and can be used as a generic relation encoder to boost the model performance for VQA.

</details>

<details>

<summary>2019-10-10 02:58:00 - Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding</summary>

- *Wenqiang Liu, Hongyun Cai, Xu Cheng, Sifa Xie, Yipeng Yu, Hanyu Zhang*

- `1910.03891v2` - [abs](http://arxiv.org/abs/1910.03891v2) - [pdf](http://arxiv.org/pdf/1910.03891v2)

> The goal of representation learning of knowledge graph is to encode both entities and relations into a low-dimensional embedding spaces. Many recent works have demonstrated the benefits of knowledge graph embedding on knowledge graph completion task, such as relation extraction. However, we observe that: 1) existing method just take direct relations between entities into consideration and fails to express high-order structural relationship between entities; 2) these methods just leverage relation triples of KGs while ignoring a large number of attribute triples that encoding rich semantic information. To overcome these limitations, this paper propose a novel knowledge graph embedding method, named KANE, which is inspired by the recent developments of graph convolutional networks (GCN). KANE can capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner under the graph convolutional networks framework. Empirical results on three datasets show that KANE significantly outperforms seven state-of-arts methods. Further analysis verify the efficiency of our method and the benefits brought by the attention mechanism.

</details>

<details>

<summary>2019-10-11 05:24:46 - Communication-based Evaluation for Natural Language Generation</summary>

- *Benjamin Newman, Reuben Cohn-Gordon, Christopher Potts*

- `1909.07290v2` - [abs](http://arxiv.org/abs/1909.07290v2) - [pdf](http://arxiv.org/pdf/1909.07290v2)

> Natural language generation (NLG) systems are commonly evaluated using n-gram overlap measures (e.g. BLEU, ROUGE). These measures do not directly capture semantics or speaker intentions, and so they often turn out to be misaligned with our true goals for NLG. In this work, we argue instead for communication-based evaluations: assuming the purpose of an NLG system is to convey information to a reader/listener, we can directly evaluate its effectiveness at this task using the Rational Speech Acts model of pragmatic language use. We illustrate with a color reference dataset that contains descriptions in pre-defined quality categories, showing that our method better aligns with these quality categories than do any of the prominent n-gram overlap methods.

</details>

<details>

<summary>2019-10-11 10:40:42 - Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base</summary>

- *Tao Shen, Xiubo Geng, Tao Qin, Daya Guo, Duyu Tang, Nan Duan, Guodong Long, Daxin Jiang*

- `1910.05069v1` - [abs](http://arxiv.org/abs/1910.05069v1) - [pdf](http://arxiv.org/pdf/1910.05069v1)

> We consider the problem of conversational question answering over a large-scale knowledge base. To handle huge entity vocabulary of a large-scale knowledge base, recent neural semantic parsing based approaches usually decompose the task into several subtasks and then solve them sequentially, which leads to following issues: 1) errors in earlier subtasks will be propagated and negatively affect downstream ones; and 2) each subtask cannot naturally share supervision signals with others. To tackle these issues, we propose an innovative multi-task learning framework where a pointer-equipped semantic parsing model is designed to resolve coreference in conversations, and naturally empower joint learning with a novel type-aware entity detection model. The proposed framework thus enables shared supervisions and alleviates the effect of error propagation. Experiments on a large-scale conversational question answering dataset containing 1.6M question answering pairs over 12.8M entities show that the proposed framework improves overall F1 score from 67% to 79% compared with previous state-of-the-art work.

</details>

<details>

<summary>2019-10-11 11:02:45 - Incorporating Fine-grained Events in Stock Movement Prediction</summary>

- *Deli Chen, Yanyan Zou, Keiko Harimoto, Ruihan Bao, Xuancheng Ren, Xu Sun*

- `1910.05078v1` - [abs](http://arxiv.org/abs/1910.05078v1) - [pdf](http://arxiv.org/pdf/1910.05078v1)

> Considering event structure information has proven helpful in text-based stock movement prediction. However, existing works mainly adopt the coarse-grained events, which loses the specific semantic information of diverse event types. In this work, we propose to incorporate the fine-grained events in stock movement prediction. Firstly, we propose a professional finance event dictionary built by domain experts and use it to extract fine-grained events automatically from finance news. Then we design a neural model to combine finance news with fine-grained event structure and stock trade data to predict the stock movement. Besides, in order to improve the generalizability of the proposed method, we design an advanced model that uses the extracted fine-grained events as the distant supervised label to train a multi-task framework of event extraction and stock prediction. The experimental results show that our method outperforms all the baselines and has good generalizability.

</details>

<details>

<summary>2019-10-11 17:22:19 - Learning Analogy-Preserving Sentence Embeddings for Answer Selection</summary>

- *Aissatou Diallo, Markus Zopf, Johannes Fürnkranz*

- `1910.05315v1` - [abs](http://arxiv.org/abs/1910.05315v1) - [pdf](http://arxiv.org/pdf/1910.05315v1)

> Answer selection aims at identifying the correct answer for a given question from a set of potentially correct answers. Contrary to previous works, which typically focus on the semantic similarity between a question and its answer, our hypothesis is that question-answer pairs are often in analogical relation to each other. Using analogical inference as our use case, we propose a framework and a neural network architecture for learning dedicated sentence embeddings that preserve analogical properties in the semantic space. We evaluate the proposed method on benchmark datasets for answer selection and demonstrate that our sentence embeddings indeed capture analogical properties better than conventional embeddings, and that analogy-based question answering outperforms a comparable similarity-based technique.

</details>

<details>

<summary>2019-10-11 19:56:47 - Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study</summary>

- *Ziyu Yao, Yu Su, Huan Sun, Wen-tau Yih*

- `1910.05389v1` - [abs](http://arxiv.org/abs/1910.05389v1) - [pdf](http://arxiv.org/pdf/1910.05389v1)

> As a promising paradigm, interactive semantic parsing has shown to improve both semantic parsing accuracy and user confidence in the results. In this paper, we propose a new, unified formulation of the interactive semantic parsing problem, where the goal is to design a model-based intelligent agent. The agent maintains its own state as the current predicted semantic parse, decides whether and where human intervention is needed, and generates a clarification question in natural language. A key part of the agent is a world model: it takes a percept (either an initial question or subsequent feedback from the user) and transitions to a new state. We then propose a simple yet remarkably effective instantiation of our framework, demonstrated on two text-to-SQL datasets (WikiSQL and Spider) with different state-of-the-art base semantic parsers. Compared to an existing interactive semantic parsing approach that treats the base parser as a black box, our approach solicits less user feedback but yields higher run-time accuracy.

</details>

<details>

<summary>2019-10-12 11:51:24 - Generating Human Action Videos by Coupling 3D Game Engines and Probabilistic Graphical Models</summary>

- *César Roberto de Souza, Adrien Gaidon, Yohann Cabon, Naila Murray, Antonio Manuel López*

- `1910.06699v1` - [abs](http://arxiv.org/abs/1910.06699v1) - [pdf](http://arxiv.org/pdf/1910.06699v1)

> Deep video action recognition models have been highly successful in recent years but require large quantities of manually annotated data, which are expensive and laborious to obtain. In this work, we investigate the generation of synthetic training data for video action recognition, as synthetic data have been successfully used to supervise models for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation, physics models and other components of modern game engines. With this model we generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for "Procedural Human Action Videos". PHAV contains a total of 39,982 videos, with more than 1,000 examples for each of 35 action categories. Our video generation approach is not limited to existing motion capture sequences: 14 of these 35 categories are procedurally defined synthetic actions. In addition, each video is represented with 6 different data modalities, including RGB, optical flow and pixel-level semantic labels. These modalities are generated almost simultaneously using the Multiple Render Targets feature of modern GPUs. In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers action classes from multiple datasets) representation learning architecture that is able to simultaneously learn from synthetic and real video datasets, even when their action categories differ. Our experiments on the UCF-101 and HMDB-51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance. Our approach also significantly outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.

</details>

<details>

<summary>2019-10-12 17:06:07 - SmokEng: Towards Fine-grained Classification of Tobacco-related Social Media Text</summary>

- *Kartikey Pant, Venkata Himakar Yanamandra, Alok Debnath, Radhika Mamidi*

- `1910.05598v1` - [abs](http://arxiv.org/abs/1910.05598v1) - [pdf](http://arxiv.org/pdf/1910.05598v1)

> Contemporary datasets on tobacco consumption focus on one of two topics, either public health mentions and disease surveillance, or sentiment analysis on topical tobacco products and services. However, two primary considerations are not accounted for, the language of the demographic affected and a combination of the topics mentioned above in a fine-grained classification mechanism. In this paper, we create a dataset of 3144 tweets, which are selected based on the presence of colloquial slang related to smoking and analyze it based on the semantics of the tweet. Each class is created and annotated based on the content of the tweets such that further hierarchical methods can be easily applied.   Further, we prove the efficacy of standard text classification methods on this dataset, by designing experiments which do both binary as well as multi-class classification. Our experiments tackle the identification of either a specific topic (such as tobacco product promotion), a general mention (cigarettes and related products) or a more fine-grained classification. This methodology paves the way for further analysis, such as understanding sentiment or style, which makes this dataset a vital contribution to both disease surveillance and tobacco use research.

</details>

<details>

<summary>2019-10-13 06:03:39 - Bayesian Neural Decoding Using A Diversity-Encouraging Latent Representation Learning Method</summary>

- *Tian Chen, Lingge Li, Gabriel Elias, Norbert Fortin, Babak Shahbaba*

- `1910.05695v1` - [abs](http://arxiv.org/abs/1910.05695v1) - [pdf](http://arxiv.org/pdf/1910.05695v1)

> It is well established that temporal organization is critical to memory, and that the ability to temporally organize information is fundamental to many perceptual, cognitive, and motor processes. While our understanding of how the brain processes the spatial context of memories has advanced considerably, our understanding of their temporal organization lags far behind. In this paper, we propose a new approach for elucidating the neural basis of complex behaviors and temporal organization of memories. More specifically, we focus on neural decoding - the prediction of behavioral or experimental conditions based on observed neural data. In general, this is a challenging classification problem, which is of immense interest in neuroscience. Our goal is to develop a new framework that not only improves the overall accuracy of decoding, but also provides a clear latent representation of the decoding process. To accomplish this, our approach uses a Variational Auto-encoder (VAE) model with a diversity-encouraging prior based on determinantal point processes (DPP) to improve latent representation learning by avoiding redundancy in the latent space. We apply our method to data collected from a novel rat experiment that involves presenting repeated sequences of odors at a single port and testing the rats' ability to identify each odor. We show that our method leads to substantially higher accuracy rate for neural decoding and allows to discover novel biological phenomena by providing a clear latent representation of the decoding process.

</details>

<details>

<summary>2019-10-13 06:20:58 - Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks</summary>

- *Chen Zhang, Qiuchi Li, Dawei Song*

- `1909.03477v2` - [abs](http://arxiv.org/abs/1909.03477v2) - [pdf](http://arxiv.org/pdf/1909.03477v2)

> Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neural Networks (CNNs) are widely applied for aspect-based sentiment classification. However, these models lack a mechanism to account for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we propose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models, and further demonstrate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.

</details>

<details>

<summary>2019-10-14 04:59:05 - High Granular Operator Spaces, and Less-Contaminated General Rough Mereologies</summary>

- *A. Mani*

- `1811.06560v4` - [abs](http://arxiv.org/abs/1811.06560v4) - [pdf](http://arxiv.org/pdf/1811.06560v4)

> Granular operator spaces and variants had been introduced and used in theoretical investigations on the foundations of general rough sets by the present author over the last few years. In this research, higher order versions of these are presented uniformly as partial algebraic systems. They are also adapted for practical applications when the data is representable by data table-like structures according to a minimalist schema for avoiding contamination. Issues relating to valuations used in information systems or tables are also addressed. The concept of contamination introduced and studied by the present author across a number of her papers, concerns mixing up of information across semantic domains (or domains of discourse). Rough inclusion functions (\textsf{RIF}s), variants, and numeric functions often have a direct or indirect role in contaminating algorithms. Some solutions that seek to replace or avoid them have been proposed and investigated by the present author in some of her earlier papers. Because multiple kinds of solution are of interest to the contamination problem, granular generalizations of RIFs are proposed, and investigated. Interesting representation results are proved and a core algebraic strategy for generalizing Skowron-Polkowski style of rough mereology (though for a very different purpose) is formulated. A number of examples have been added to illustrate key parts of the proposal in higher order variants of granular operator spaces. Further algorithms grounded in mereological nearness, suited for decision-making in human-machine interaction contexts, are proposed by the present author. Applications of granular \textsf{RIF}s to partial/soft solutions of the inverse problem are also invented in this paper.

</details>

<details>

<summary>2019-10-14 06:26:01 - Characterizing Deep Learning Training Workloads on Alibaba-PAI</summary>

- *Mengdi Wang, Chen Meng, Guoping Long, Chuan Wu, Jun Yang, Wei Lin, Yangqing Jia*

- `1910.05930v1` - [abs](http://arxiv.org/abs/1910.05930v1) - [pdf](http://arxiv.org/pdf/1910.05930v1)

> Modern deep learning models have been exploited in various domains, including computer vision (CV), natural language processing (NLP), search and recommendation. In practical AI clusters, workloads training these models are run using software frameworks such as TensorFlow, Caffe, PyTorch and CNTK. One critical issue for efficiently operating practical AI clouds, is to characterize the computing and data transfer demands of these workloads, and more importantly, the training performance given the underlying software framework and hardware configurations. In this paper, we characterize deep learning training workloads from Platform of Artificial Intelligence (PAI) in Alibaba. We establish an analytical framework to investigate detailed execution time breakdown of various workloads using different training architectures, to identify performance bottleneck. Results show that weight/gradient communication during training takes almost 62% of the total execution time among all our workloads on average. The computation part, involving both GPU computing and memory access, are not the biggest bottleneck based on collective behavior of the workloads. We further evaluate attainable performance of the workloads on various potential software/hardware mappings, and explore implications on software architecture selection and hardware configurations. We identify that 60% of PS/Worker workloads can be potentially sped up when ported to the AllReduce architecture exploiting the high-speed NVLink for GPU interconnect, and on average 1.7X speedup can be achieved when Ethernet bandwidth is upgraded from 25 Gbps to 100 Gbps.

</details>

<details>

<summary>2019-10-14 07:14:53 - Inpatient2Vec: Medical Representation Learning for Inpatients</summary>

- *Ying Wang, Xiao Xu, Tao Jin, Xiang Li, Guotong Xie, Jianmin Wang*

- `1904.08558v2` - [abs](http://arxiv.org/abs/1904.08558v2) - [pdf](http://arxiv.org/pdf/1904.08558v2)

> Representation learning (RL) plays an important role in extracting proper representations from complex medical data for various analyzing tasks, such as patient grouping, clinical endpoint prediction and medication recommendation. Medical data can be divided into two typical categories, outpatient and inpatient, that have different data characteristics. However, few of existing RL methods are specially designed for inpatients data, which have strong temporal relations and consistent diagnosis. In addition, for unordered medical activity set, existing medical RL methods utilize a simple pooling strategy, which would result in indistinguishable contributions among the activities for learning. In this work, weproposeInpatient2Vec, anovelmodel for learning three kinds of representations for inpatient, including medical activity, hospital day and diagnosis. A multi-layer self-attention mechanism with two training tasks is designed to capture the inpatient data characteristics and process the unordered set. Using a real-world dataset, we demonstrate that the proposed approach outperforms the competitive baselines on semantic similarity measurement and clinical events prediction tasks.

</details>

<details>

<summary>2019-10-14 09:55:39 - BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding</summary>

- *Timo I. Denk, Christian Reisswig*

- `1909.04948v2` - [abs](http://arxiv.org/abs/1909.04948v2) - [pdf](http://arxiv.org/pdf/1909.04948v2)

> For understanding generic documents, information like font sizes, column layout, and generally the positioning of words may carry semantic information that is crucial for solving a downstream document intelligence task. Our novel BERTgrid, which is based on Chargrid by Katti et al. (2018), represents a document as a grid of contextualized word piece embedding vectors, thereby making its spatial structure and semantics accessible to the processing neural network. The contextualized embedding vectors are retrieved from a BERT language model. We use BERTgrid in combination with a fully convolutional network on a semantic instance segmentation task for extracting fields from invoices. We demonstrate its performance on tabulated line item and document header field extraction.

</details>

<details>

<summary>2019-10-14 19:56:53 - Adaptive Masked Proxies for Few-Shot Segmentation</summary>

- *Mennatullah Siam, Boris Oreshkin, Martin Jagersand*

- `1902.11123v5` - [abs](http://arxiv.org/abs/1902.11123v5) - [pdf](http://arxiv.org/pdf/1902.11123v5)

> Deep learning has thrived by training on large-scale datasets. However, in robotics applications sample efficiency is critical. We propose a novel adaptive masked proxies method that constructs the final segmentation layer weights from few labelled samples. It utilizes multi-resolution average pooling on base embeddings masked with the label to act as a positive proxy for the new class, while fusing it with the previously learned class signatures. Our method is evaluated on PASCAL-$5^i$ dataset and outperforms the state-of-the-art in the few-shot semantic segmentation. Unlike previous methods, our approach does not require a second branch to estimate parameters or prototypes, which enables it to be used with 2-stream motion and appearance based segmentation networks. We further propose a novel setup for evaluating continual learning of object segmentation which we name incremental PASCAL (iPASCAL) where our method outperforms the baseline method. Our code is publicly available at https://github.com/MSiam/AdaptiveMaskedProxies.

</details>

<details>

<summary>2019-10-14 21:22:54 - Restoration of marker occluded hematoxylin and eosin stained whole slide histology images using generative adversarial networks</summary>

- *Bairavi Venkatesh, Tosha Shah, Antong Chen, Soheil Ghafurian*

- `1910.06428v1` - [abs](http://arxiv.org/abs/1910.06428v1) - [pdf](http://arxiv.org/pdf/1910.06428v1)

> It is common for pathologists to annotate specific regions of the tissue, such as tumor, directly on the glass slide with markers. Although this practice was helpful prior to the advent of histology whole slide digitization, it often occludes important details which are increasingly relevant to immuno-oncology due to recent advancements in digital pathology imaging techniques. The current work uses a generative adversarial network with cycle loss to remove these annotations while still maintaining the underlying structure of the tissue by solving an image-to-image translation problem. We train our network on up to 300 whole slide images with marker inks and show that 70% of the corrected image patches are indistinguishable from originally uncontaminated image tissue to a human expert. This portion increases 97% when we replace the human expert with a deep residual network. We demonstrated the fidelity of the method to the original image by calculating the correlation between image gradient magnitudes. We observed a revival of up to 94,000 nuclei per slide in our dataset, the majority of which were located on tissue border.

</details>

<details>

<summary>2019-10-15 01:23:04 - On Constructing a Knowledge Base of Chinese Criminal Cases</summary>

- *Xiaohan Wu, Benjamin L. Liebman, Rachel E. Stern, Margaret E. Roberts, Amarnath Gupta*

- `1910.07494v1` - [abs](http://arxiv.org/abs/1910.07494v1) - [pdf](http://arxiv.org/pdf/1910.07494v1)

> We are developing a knowledge base over Chinese judicial decision documents to facilitate landscape analyses of Chinese Criminal Cases. We view judicial decision documents as a mixed-granularity semi-structured text where different levels of the text carry different semantic constructs and entailments. We use a combination of context-sensitive grammar, dependency parsing and discourse analysis to extract a formal and interpretable representation of these documents. Our knowledge base is developed by constructing associations between different elements of these documents. The interpretability is contributed in part by our formal representation of the Chinese criminal laws, also as semi-structured documents. The landscape analyses utilize these two representations and enable a law researcher to ask legal pattern analysis queries.

</details>

<details>

<summary>2019-10-15 02:40:29 - Hierarchical Semantic Correspondence Learning for Post-Discharge Patient Mortality Prediction</summary>

- *Shaika Chowdhury, Chenwei Zhang, Philip S. Yu, Yuan Luo*

- `1910.06492v1` - [abs](http://arxiv.org/abs/1910.06492v1) - [pdf](http://arxiv.org/pdf/1910.06492v1)

> Predicting patient mortality is an important and challenging problem in the healthcare domain, especially for intensive care unit (ICU) patients. Electronic health notes serve as a rich source for learning patient representations, that can facilitate effective risk assessment. However, a large portion of clinical notes are unstructured and also contain domain specific terminologies, from which we need to extract structured information. In this paper, we introduce an embedding framework to learn semantically-plausible distributed representations of clinical notes that exploits the semantic correspondence between the unstructured texts and their corresponding structured knowledge, known as semantic frame, in a hierarchical fashion. Our approach integrates text modeling and semantic correspondence learning into a single model that comprises 1) an unstructured embedding module that makes use of self-similarity matrix representations in order to inject structural regularities of different segments inherent in clinical texts to promote local coherence, 2) a structured embedding module to embed the semantic frames (e.g., UMLS semantic types) with deep ConvNet and 3) a hierarchical semantic correspondence module that embeds by enhancing the interactions between text-semantic frame embedding pairs at multiple levels (i.e., words, sentence, note). Evaluations on multiple embedding benchmarks on post discharge intensive care patient mortality prediction tasks demonstrate its effectiveness compared to approaches that do not exploit the semantic interactions between structured and unstructured information present in clinical notes.

</details>

<details>

<summary>2019-10-15 06:11:03 - When Deep Learning Met Code Search</summary>

- *Jose Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, Satish Chandra*

- `1905.03813v4` - [abs](http://arxiv.org/abs/1905.03813v4) - [pdf](http://arxiv.org/pdf/1905.03813v4)

> There have been multiple recent proposals on using deep neural networks for code search using natural language. Common across these proposals is the idea of $\mathit{embedding}$ code and natural language queries, into real vectors and then using vector distance to approximate semantic correlation between code and the query. Multiple approaches exist for learning these embeddings, including $\mathit{unsupervised}$ techniques, which rely only on a corpus of code examples, and $\mathit{supervised}$ techniques, which use an $\mathit{aligned}$ corpus of paired code and natural language descriptions. The goal of this supervision is to produce embeddings that are more similar for a query and the corresponding desired code snippet. Clearly, there are choices in whether to use supervised techniques at all, and if one does, what sort of network and training to use for supervision. This paper is the first to evaluate these choices systematically. To this end, we assembled implementations of state-of-the-art techniques to run on a common platform, training and evaluation corpora. To explore the design space in network complexity, we also introduced a new design point that is a $\mathit{minimal}$ supervision extension to an existing unsupervised technique. Our evaluation shows that: 1. adding supervision to an existing unsupervised technique can improve performance, though not necessarily by much; 2. simple networks for supervision can be more effective that more sophisticated sequence-based networks for code search; 3. while it is common to use docstrings to carry out supervision, there is a sizeable gap between the effectiveness of docstrings and a more query-appropriate supervision corpus.   The evaluation dataset is now available at arXiv:1908.09804

</details>

<details>

<summary>2019-10-15 07:44:54 - Text2Math: End-to-end Parsing Text into Math Expressions</summary>

- *Yanyan Zou, Wei Lu*

- `1910.06571v1` - [abs](http://arxiv.org/abs/1910.06571v1) - [pdf](http://arxiv.org/pdf/1910.06571v1)

> We propose Text2Math, a model for semantically parsing text into math expressions. The model can be used to solve different math related problems including arithmetic word problems and equation parsing problems. Unlike previous approaches, we tackle the problem from an end-to-end structured prediction perspective where our algorithm aims to predict the complete math expression at once as a tree structure, where minimal manual efforts are involved in the process. Empirical results on benchmark datasets demonstrate the efficacy of our approach.

</details>

<details>

<summary>2019-10-15 08:10:25 - Learning Navigation Subroutines from Egocentric Videos</summary>

- *Ashish Kumar, Saurabh Gupta, Jitendra Malik*

- `1905.12612v2` - [abs](http://arxiv.org/abs/1905.12612v2) - [pdf](http://arxiv.org/pdf/1905.12612v2)

> Planning at a higher level of abstraction instead of low level torques improves the sample efficiency in reinforcement learning, and computational efficiency in classical planning. We propose a method to learn such hierarchical abstractions, or subroutines from egocentric video data of experts performing tasks. We learn a self-supervised inverse model on small amounts of random interaction data to pseudo-label the expert egocentric videos with agent actions. Visuomotor subroutines are acquired from these pseudo-labeled videos by learning a latent intent-conditioned policy that predicts the inferred pseudo-actions from the corresponding image observations. We demonstrate our proposed approach in context of navigation, and show that we can successfully learn consistent and diverse visuomotor subroutines from passive egocentric videos. We demonstrate the utility of our acquired visuomotor subroutines by using them as is for exploration, and as sub-policies in a hierarchical RL framework for reaching point goals and semantic goals. We also demonstrate behavior of our subroutines in the real world, by deploying them on a real robotic platform. Project website: https://ashishkumar1993.github.io/subroutines/.

</details>

<details>

<summary>2019-10-15 08:43:47 - FacTweet: Profiling Fake News Twitter Accounts</summary>

- *Bilal Ghanem, Simone Paolo Ponzetto, Paolo Rosso*

- `1910.06592v1` - [abs](http://arxiv.org/abs/1910.06592v1) - [pdf](http://arxiv.org/pdf/1910.06592v1)

> We present an approach to detect fake news in Twitter at the account level using a neural recurrent model and a variety of different semantic and stylistic features. Our method extracts a set of features from the timelines of news Twitter accounts by reading their posts as chunks, rather than dealing with each tweet independently. We show the experimental benefits of modeling latent stylistic signatures of mixed fake and real news with a sequential model over a wide range of strong baselines.

</details>

<details>

<summary>2019-10-15 21:28:13 - Iterative Delexicalization for Improved Spoken Language Understanding</summary>

- *Avik Ray, Yilin Shen, Hongxia Jin*

- `1910.07060v1` - [abs](http://arxiv.org/abs/1910.07060v1) - [pdf](http://arxiv.org/pdf/1910.07060v1)

> Recurrent neural network (RNN) based joint intent classification and slot tagging models have achieved tremendous success in recent years for building spoken language understanding and dialog systems. However, these models suffer from poor performance for slots which often encounter large semantic variability in slot values after deployment (e.g. message texts, partial movie/artist names). While greedy delexicalization of slots in the input utterance via substring matching can partly improve performance, it often produces incorrect input. Moreover, such techniques cannot delexicalize slots with out-of-vocabulary slot values not seen at training. In this paper, we propose a novel iterative delexicalization algorithm, which can accurately delexicalize the input, even with out-of-vocabulary slot values. Based on model confidence of the current delexicalized input, our algorithm improves delexicalization in every iteration to converge to the best input having the highest confidence. We show on benchmark and in-house datasets that our algorithm can greatly improve parsing performance for RNN based models, especially for out-of-distribution slot values.

</details>

<details>

<summary>2019-10-15 22:58:19 - Explainable Semantic Mapping for First Responders</summary>

- *Jean Oh, Martial Hebert, Hae-Gon Jeon, Xavier Perez, Chia Dai, Yeeho Song*

- `1910.07093v1` - [abs](http://arxiv.org/abs/1910.07093v1) - [pdf](http://arxiv.org/pdf/1910.07093v1)

> One of the key challenges in the semantic mapping problem in postdisaster environments is how to analyze a large amount of data efficiently with minimal supervision. To address this challenge, we propose a deep learning-based semantic mapping tool consisting of three main ideas. First, we develop a frugal semantic segmentation algorithm that uses only a small amount of labeled data. Next, we investigate on the problem of learning to detect a new class of object using just a few training examples. Finally, we develop an explainable cost map learning algorithm that can be quickly trained to generate traversability cost maps using only raw sensor data such as aerial-view imagery. This paper presents an overview of the proposed idea and the lessons learned.

</details>

<details>

<summary>2019-10-16 10:54:46 - On the Relation between Weak Completion Semantics and Answer Set Semantics</summary>

- *Emmanuelle-Anna Dietz Saldanha, Jorge Fandinno*

- `1910.07278v1` - [abs](http://arxiv.org/abs/1910.07278v1) - [pdf](http://arxiv.org/pdf/1910.07278v1)

> The Weak Completion Semantics (WCS) is a computational cognitive theory that has shown to be successful in modeling episodes of human reasoning. As the WCS is a recently developed logic programming approach, this paper investigates the correspondence of the WCS with respect to the well-established Answer Set Semantics (ASP). The underlying three-valued logic of both semantics is different and their models are evaluated with respect to different program transformations. We first illustrate these differences by the formal representation of some examples of a well-known psychological experiment, the suppression task. After that, we will provide a translation from logic programs understood under the WCS into logic programs understood under the ASP. In particular, we will show that logic programs under the WCS can be represented as logic programs under the ASP by means of a definition completion, where all defined atoms in a program must be false when their definitions are false.

</details>

<details>

<summary>2019-10-16 14:20:19 - A Pilot Study for Chinese SQL Semantic Parsing</summary>

- *Qingkai Min, Yuefeng Shi, Yue Zhang*

- `1909.13293v2` - [abs](http://arxiv.org/abs/1909.13293v2) - [pdf](http://arxiv.org/pdf/1909.13293v2)

> The task of semantic parsing is highly useful for dialogue and question answering systems. Many datasets have been proposed to map natural language text into SQL, among which the recent Spider dataset provides cross-domain samples with multiple tables and complex queries. We build a Spider dataset for Chinese, which is currently a low-resource language in this task area. Interesting research questions arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and word-based encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL.

</details>

<details>

<summary>2019-10-16 15:44:09 - Bridging the Knowledge Gap: Enhancing Question Answering with World and Domain Knowledge</summary>

- *Travis R. Goodwin, Dina Demner-Fushman*

- `1910.07429v1` - [abs](http://arxiv.org/abs/1910.07429v1) - [pdf](http://arxiv.org/pdf/1910.07429v1)

> In this paper we present OSCAR (Ontology-based Semantic Composition Augmented Regularization), a method for injecting task-agnostic knowledge from an Ontology or knowledge graph into a neural network during pretraining. We evaluated the impact of including OSCAR when pretraining BERT with Wikipedia articles by measuring the performance when fine-tuning on two question answering tasks involving world knowledge and causal reasoning and one requiring domain (healthcare) knowledge and obtained 33:3%, 18:6%, and 4% improved accuracy compared to pretraining BERT without OSCAR and obtaining new state-of-the-art results on two of the tasks.

</details>

<details>

<summary>2019-10-17 00:14:33 - How Different Are Different diff Algorithms in Git?</summary>

- *Yusuf Sulistyo Nugroho, Hideaki Hata, Kenichi Matsumoto*

- `1902.02467v4` - [abs](http://arxiv.org/abs/1902.02467v4) - [pdf](http://arxiv.org/pdf/1902.02467v4)

> Automatic identification of the differences between two versions of a file is a common and basic task in several applications of mining code repositories. Git, a version control system, has a diff utility and users can select algorithms of diff from the default algorithm Myers to the advanced Histogram algorithm. From our systematic mapping, we identified three popular applications of diff in recent studies. On the impact on code churn metrics in 14 Java projects, we obtained different values in 1.7% to 8.2% commits based on the different diff algorithms. Regarding bug-introducing change identification, we found 6.0% and 13.3% in the identified bug-fix commits had different results of bug-introducing changes from 10 Java projects. For patch application, we found that the Histogram is more suitable than Myers for providing the changes of code, from our manual analysis. Thus, we strongly recommend using the Histogram algorithm when mining Git repositories to consider differences in source code.

</details>

<details>

<summary>2019-10-17 10:23:19 - Multimodal Differential Network for Visual Question Generation</summary>

- *Badri N. Patro, Sandeep Kumar, Vinod K. Kurmi, Vinay P. Namboodiri*

- `1808.03986v2` - [abs](http://arxiv.org/abs/1808.03986v2) - [pdf](http://arxiv.org/pdf/1808.03986v2)

> Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).

</details>

<details>

<summary>2019-10-17 12:09:11 - Topical Keyphrase Extraction with Hierarchical Semantic Networks</summary>

- *Yoo yeon Sung, Seoung Bum Kim*

- `1910.07848v1` - [abs](http://arxiv.org/abs/1910.07848v1) - [pdf](http://arxiv.org/pdf/1910.07848v1)

> Topical keyphrase extraction is used to summarize large collections of text documents. However, traditional methods cannot properly reflect the intrinsic semantics and relationships of keyphrases because they rely on a simple term-frequency-based process. Consequently, these methods are not effective in obtaining significant contextual knowledge. To resolve this, we propose a topical keyphrase extraction method based on a hierarchical semantic network and multiple centrality network measures that together reflect the hierarchical semantics of keyphrases. We conduct experiments on real data to examine the practicality of the proposed method and to compare its performance with that of existing topical keyphrase extraction methods. The results confirm that the proposed method outperforms state-of-the-art topical keyphrase extraction methods in terms of the representativeness of the selected keyphrases for each topic. The proposed method can effectively reflect intrinsic keyphrase semantics and interrelationships.

</details>

<details>

<summary>2019-10-17 13:27:25 - Predicting origin-destination ride-sourcing demand with a spatio-temporal encoder-decoder residual multi-graph convolutional network</summary>

- *Jintao Ke, Xiaoran Qin, Hai Yang, Zhengfei Zheng, Zheng Zhu, Jieping Ye*

- `1910.09103v1` - [abs](http://arxiv.org/abs/1910.09103v1) - [pdf](http://arxiv.org/pdf/1910.09103v1)

> With the rapid development of mobile-internet technologies, on-demand ride-sourcing services have become increasingly popular and largely reshaped the way people travel. Demand prediction is one of the most fundamental components in supply-demand management systems of ride-sourcing platforms. With accurate short-term prediction for origin-destination (OD) demand, the platforms make precise and timely decisions on real-time matching, idle vehicle reallocations and ride-sharing vehicle routing, etc. Compared to zone-based demand prediction that has been examined by many previous studies, OD-based demand prediction is more challenging. This is mainly due to the complicated spatial and temporal dependencies among demand of different OD pairs. To overcome this challenge, we propose the Spatio-Temporal Encoder-Decoder Residual Multi-Graph Convolutional network (ST-ED-RMGC), a novel deep learning model for predicting ride-sourcing demand of various OD pairs. Firstly, the model constructs OD graphs, which utilize adjacent matrices to characterize the non-Euclidean pair-wise geographical and semantic correlations among different OD pairs. Secondly, based on the constructed graphs, a residual multi-graph convolutional (RMGC) network is designed to encode the contextual-aware spatial dependencies, and a long-short term memory (LSTM) network is used to encode the temporal dependencies, into a dense vector space. Finally, we reuse the RMGC networks to decode the compressed vector back to OD graphs and predict the future OD demand. Through extensive experiments on the for-hire-vehicles datasets in Manhattan, New York City, we show that our proposed deep learning framework outperforms the state-of-arts by a significant margin.

</details>

<details>

<summary>2019-10-17 14:03:07 - Adversarial Framing for Image and Video Classification</summary>

- *Konrad Zolna, Michal Zajac, Negar Rostamzadeh, Pedro O. Pinheiro*

- `1812.04599v3` - [abs](http://arxiv.org/abs/1812.04599v3) - [pdf](http://arxiv.org/pdf/1812.04599v3)

> Neural networks are prone to adversarial attacks. In general, such attacks deteriorate the quality of the input by either slightly modifying most of its pixels, or by occluding it with a patch. In this paper, we propose a method that keeps the image unchanged and only adds an adversarial framing on the border of the image. We show empirically that our method is able to successfully attack state-of-the-art methods on both image and video classification problems. Notably, the proposed method results in a universal attack which is very fast at test time. Source code can be found at https://github.com/zajaczajac/adv_framing .

</details>

<details>

<summary>2019-10-17 16:07:49 - CLN2INV: Learning Loop Invariants with Continuous Logic Networks</summary>

- *Gabriel Ryan, Justin Wong, Jianan Yao, Ronghui Gu, Suman Jana*

- `1909.11542v3` - [abs](http://arxiv.org/abs/1909.11542v3) - [pdf](http://arxiv.org/pdf/1909.11542v3)

> Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT) for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous truth values that allows CLNs to be trained efficiently. We use CLNs to implement a new inference system for loop invariants, CLN2INV, that significantly outperforms existing approaches on the popular Code2Inv dataset. CLN2INV is the first tool to solve all 124 theoretically solvable problems in the Code2Inv dataset. Moreover, CLN2INV takes only 1.1 seconds on average for each problem, which is 40 times faster than existing approaches. We further demonstrate that CLN2INV can even learn 12 significantly more complex loop invariants than the ones required for the Code2Inv dataset.

</details>

<details>

<summary>2019-10-17 17:37:58 - The lexical and grammatical sources of neg-raising inferences</summary>

- *Hannah Youngeun An, Aaron Steven White*

- `1908.05253v3` - [abs](http://arxiv.org/abs/1908.05253v3) - [pdf](http://arxiv.org/pdf/1908.05253v3)

> We investigate neg(ation)-raising inferences, wherein negation on a predicate can be interpreted as though in that predicate's subordinate clause. To do this, we collect a large-scale dataset of neg-raising judgments for effectively all English clause-embedding verbs and develop a model to jointly induce the semantic types of verbs and their subordinate clauses and the relationship of these types to neg-raising inferences. We find that some neg-raising inferences are attributable to properties of particular predicates, while others are attributable to subordinate clause structure.

</details>

<details>

<summary>2019-10-17 22:55:29 - SetExpan: Corpus-Based Set Expansion via Context Feature Selection and Rank Ensemble</summary>

- *Jiaming Shen, Zeqiu Wu, Dongming Lei, Jingbo Shang, Xiang Ren, Jiawei Han*

- `1910.08192v1` - [abs](http://arxiv.org/abs/1910.08192v1) - [pdf](http://arxiv.org/pdf/1910.08192v1)

> Corpus-based set expansion (i.e., finding the "complete" set of entities belonging to the same semantic class, based on a given corpus and a tiny set of seeds) is a critical task in knowledge discovery. It may facilitate numerous downstream applications, such as information extraction, taxonomy induction, question answering, and web search. To discover new entities in an expanded set, previous approaches either make one-time entity ranking based on distributional similarity, or resort to iterative pattern-based bootstrapping. The core challenge for these methods is how to deal with noisy context features derived from free-text corpora, which may lead to entity intrusion and semantic drifting. In this study, we propose a novel framework, SetExpan, which tackles this problem, with two techniques: (1) a context feature selection method that selects clean context features for calculating entity-entity distributional similarity, and (2) a ranking-based unsupervised ensemble method for expanding entity set based on denoised context features. Experiments on three datasets show that SetExpan is robust and outperforms previous state-of-the-art methods in terms of mean average precision.

</details>

<details>

<summary>2019-10-17 23:34:04 - Are Nearby Neighbors Relatives?: Testing Deep Music Embeddings</summary>

- *Jaehun Kim, Julián Urbano, Cynthia C. S. Liem, Alan Hanjalic*

- `1904.07154v3` - [abs](http://arxiv.org/abs/1904.07154v3) - [pdf](http://arxiv.org/pdf/1904.07154v3)

> Deep neural networks have frequently been used to directly learn representations useful for a given task from raw input data. In terms of overall performance metrics, machine learning solutions employing deep representations frequently have been reported to greatly outperform those using hand-crafted feature representations. At the same time, they may pick up on aspects that are predominant in the data, yet not actually meaningful or interpretable. In this paper, we therefore propose a systematic way to test the trustworthiness of deep music representations, considering musical semantics. The underlying assumption is that in case a deep representation is to be trusted, distance consistency between known related points should be maintained both in the input audio space and corresponding latent deep space. We generate known related points through semantically meaningful transformations, both considering imperceptible and graver transformations. Then, we examine within- and between-space distance consistencies, both considering audio space and latent embedded space, the latter either being a result of a conventional feature extractor or a deep encoder. We illustrate how our method, as a complement to task-specific performance, provides interpretable insight into what a network may have captured from training data signals.

</details>

<details>

<summary>2019-10-18 05:10:33 - BOBBY2: Buffer Based Robust High-Speed Object Tracking</summary>

- *Keifer Lee, Jun Jet Tai, Swee King Phang*

- `1910.08263v1` - [abs](http://arxiv.org/abs/1910.08263v1) - [pdf](http://arxiv.org/pdf/1910.08263v1)

> In this work, a novel high-speed single object tracker that is robust against non-semantic distractor exemplars is introduced; dubbed BOBBY2. It incorporates a novel exemplar buffer module that sparsely caches the target's appearance across time, enabling it to adapt to potential target deformation. As for training, an augmented ImageNet-VID dataset was used in conjunction with the one cycle policy, enabling it to reach convergence with less than 2 epoch worth of data. For validation, the model was benchmarked on the GOT-10k dataset and on an additional small, albeit challenging custom UAV dataset collected with the TU-3 UAV. We demonstrate that the exemplar buffer is capable of providing redundancies in case of unintended target drifts, a desirable trait in any middle to long term tracking. Even when the buffer is predominantly filled with distractors instead of valid exemplars, BOBBY2 is capable of maintaining a near-optimal level of accuracy. BOBBY2 manages to achieve a very competitive result on the GOT-10k dataset and to a lesser degree on the challenging custom TU-3 dataset, without fine-tuning, demonstrating its generalizability. In terms of speed, BOBBY2 utilizes a stripped down AlexNet as feature extractor with 63% less parameters than a vanilla AlexNet, thus being able to run at a competitive 85 FPS.

</details>

<details>

<summary>2019-10-18 06:17:07 - Estimator Vectors: OOV Word Embeddings based on Subword and Context Clue Estimates</summary>

- *Raj Patel, Carlotta Domeniconi*

- `1910.10491v1` - [abs](http://arxiv.org/abs/1910.10491v1) - [pdf](http://arxiv.org/pdf/1910.10491v1)

> Semantic representations of words have been successfully extracted from unlabeled corpuses using neural network models like word2vec. These representations are generally high quality and are computationally inexpensive to train, making them popular. However, these approaches generally fail to approximate out of vocabulary (OOV) words, a task humans can do quite easily, using word roots and context clues. This paper proposes a neural network model that learns high quality word representations, subword representations, and context clue representations jointly. Learning all three types of representations together enhances the learning of each, leading to enriched word vectors, along with strong estimates for OOV words, via the combination of the corresponding context clue and subword embeddings. Our model, called Estimator Vectors (EV), learns strong word embeddings and is competitive with state of the art methods for OOV estimation.

</details>

<details>

<summary>2019-10-18 15:20:38 - Towards Learning Cross-Modal Perception-Trace Models</summary>

- *Achim Rettinger, Viktoria Bogdanova, Philipp Niemann*

- `1910.08549v1` - [abs](http://arxiv.org/abs/1910.08549v1) - [pdf](http://arxiv.org/pdf/1910.08549v1)

> Representation learning is a key element of state-of-the-art deep learning approaches. It enables to transform raw data into structured vector space embeddings. Such embeddings are able to capture the distributional semantics of their context, e.g. by word windows on natural language sentences, graph walks on knowledge graphs or convolutions on images. So far, this context is manually defined, resulting in heuristics which are solely optimized for computational performance on certain tasks like link-prediction. However, such heuristic models of context are fundamentally different to how humans capture information. For instance, when reading a multi-modal webpage (i) humans do not perceive all parts of a document equally: Some words and parts of images are skipped, others are revisited several times which makes the perception trace highly non-sequential; (ii) humans construct meaning from a document's content by shifting their attention between text and image, among other things, guided by layout and design elements. In this paper we empirically investigate the difference between human perception and context heuristics of basic embedding models. We conduct eye tracking experiments to capture the underlying characteristics of human perception of media documents containing a mixture of text and images. Based on that, we devise a prototypical computational perception-trace model, called CMPM. We evaluate empirically how CMPM can improve a basic skip-gram embedding approach. Our results suggest, that even with a basic human-inspired computational perception model, there is a huge potential for improving embeddings since such a model does inherently capture multiple modalities, as well as layout and design elements.

</details>

<details>

<summary>2019-10-18 16:11:31 - Concept Pointer Network for Abstractive Summarization</summary>

- *Wang Wenbo, Gao Yang, Huang Heyan, Zhou Yuxiang*

- `1910.08486v1` - [abs](http://arxiv.org/abs/1910.08486v1) - [pdf](http://arxiv.org/pdf/1910.08486v1)

> A quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details. Inspired by the popular pointer generator sequence-to-sequence model, this paper presents a concept pointer network for improving these aspects of abstractive summarization. The network leverages knowledge-based, context-aware conceptualizations to derive an extended set of candidate concepts. The model then points to the most appropriate choice using both the concept set and original source text. This joint approach generates abstractive summaries with higher-level semantic concepts. The training model is also optimized in a way that adapts to different data, which is based on a novel method of distantly-supervised learning guided by reference summaries and testing set. Overall, the proposed approach provides statistically significant improvements over several state-of-the-art models on both the DUC-2004 and Gigaword datasets. A human evaluation of the model's abstractive abilities also supports the quality of the summaries produced within this framework.

</details>

<details>

<summary>2019-10-18 17:22:15 - Movienet: A Movie Multilayer Network Model using Visual and Textual Semantic Cues</summary>

- *Youssef Mourchid, Benjamin Renoust, Olivier Roupin, Le Van, Hocine Cherifi, Mohammed El Hassouni*

- `1910.09368v1` - [abs](http://arxiv.org/abs/1910.09368v1) - [pdf](http://arxiv.org/pdf/1910.09368v1)

> Discovering content and stories in movies is one of the most important concepts in multimedia content research studies. Network models have proven to be an efficient choice for this purpose. When an audience watches a movie, they usually compare the characters and the relationships between them. For this reason, most of the models developed so far are based on social networks analysis. They focus essentially on the characters at play. By analyzing characters' interactions, we can obtain a broad picture of the narration's content. Other works have proposed to exploit semantic elements such as scenes, dialogues, etc. However, they are always captured from a single facet. Motivated by these limitations, we introduce in this work a multilayer network model to capture the narration of a movie based on its script, its subtitles, and the movie content. After introducing the model and the extraction process from the raw data, we perform a comparative analysis of the whole 6-movie cycle of the Star Wars saga. Results demonstrate the effectiveness of the proposed framework for video content representation and analysis.

</details>

<details>

<summary>2019-10-19 03:32:16 - An Improved Historical Embedding without Alignment</summary>

- *Xiaofei Xu, Ke Deng, Fei Hu, Li Li*

- `1910.08692v1` - [abs](http://arxiv.org/abs/1910.08692v1) - [pdf](http://arxiv.org/pdf/1910.08692v1)

> Many words have evolved in meaning as a result of cultural and social change. Understanding such changes is crucial for modelling language and cultural evolution. Low-dimensional embedding methods have shown promise in detecting words' meaning change by encoding them into dense vectors. However, when exploring semantic change of words over time, these methods require the alignment of word embeddings across different time periods. This process is computationally expensive, prohibitively time consuming and suffering from contextual variability. In this paper, we propose a new and scalable method for encoding words from different time periods into one dense vector space. This can greatly improve performance when it comes to identifying words that have changed in meaning over time. We evaluated our method on dataset from Google Books N-gram. Our method outperformed three other popular methods in terms of the number of words correctly identified to have changed in meaning. Additionally, we provide an intuitive visualization of the semantic evolution of some words extracted by our method

</details>

<details>

<summary>2019-10-19 06:59:16 - Microservices based Framework to Support Interoperable IoT Applications for Enhanced Data Analytics</summary>

- *Sajjad Ali, Muhammad Aslam Jarwar, Ilyoung Chong*

- `1910.08713v1` - [abs](http://arxiv.org/abs/1910.08713v1) - [pdf](http://arxiv.org/pdf/1910.08713v1)

> Internet of things is growing with a large number of diverse objects which generate billions of data streams by sensing, actuating and communicating. Management of heterogeneous IoT objects with existing approaches and processing of myriads of data from these objects using monolithic services have become major challenges in developing effective IoT applications. The heterogeneity can be resolved by providing interoperability with semantic virtualization of objects. Moreover, monolithic services can be substituted with modular microservices. This article presents an architecture that enables the development of IoT applications using semantically interoperable microservices and virtual objects. The proposed framework supports analytic features with knowledge-driven and data-driven techniques to provision intelligent services on top of interoperable microservices in Web Objects enabled IoT environment. The knowledge-driven aspects are supported with reasoning on semantic ontology models and the data-driven aspects are realized with machine learning pipeline. The development of service functionalities is supported with microservices to enhance modularity and reusability. To evaluate the proposed framework a proof of concept implementation with a use case is discussed.

</details>

<details>

<summary>2019-10-19 20:05:44 - Natural Question Generation with Reinforcement Learning Based Graph-to-Sequence Model</summary>

- *Yu Chen, Lingfei Wu, Mohammed J. Zaki*

- `1910.08832v1` - [abs](http://arxiv.org/abs/1910.08832v1) - [pdf](http://arxiv.org/pdf/1910.08832v1)

> Natural question generation (QG) aims to generate questions from a passage and an answer. In this paper, we propose a novel reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator where a novel Bidirectional Gated Graph Neural Network is proposed to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the generation of syntactically and semantically valid text. The proposed model outperforms previous state-of-the-art methods by a large margin on the SQuAD dataset.

</details>

<details>

<summary>2019-10-19 21:39:03 - XL-Editor: Post-editing Sentences with XLNet</summary>

- *Yong-Siang Shih, Wei-Cheng Chang, Yiming Yang*

- `1910.10479v1` - [abs](http://arxiv.org/abs/1910.10479v1) - [pdf](http://arxiv.org/pdf/1910.10479v1)

> While neural sequence generation models achieve initial success for many NLP applications, the canonical decoding procedure with left-to-right generation order (i.e., autoregressive) in one-pass can not reflect the true nature of human revising a sentence to obtain a refined result. In this work, we propose XL-Editor, a novel training framework that enables state-of-the-art generalized autoregressive pretraining methods, XLNet specifically, to revise a given sentence by the variable-length insertion probability. Concretely, XL-Editor can (1) estimate the probability of inserting a variable-length sequence into a specific position of a given sentence; (2) execute post-editing operations such as insertion, deletion, and replacement based on the estimated variable-length insertion probability; (3) complement existing sequence-to-sequence models to refine the generated sequences. Empirically, we first demonstrate better post-editing capabilities of XL-Editor over XLNet on the text insertion and deletion tasks, which validates the effectiveness of our proposed framework. Furthermore, we extend XL-Editor to the unpaired text style transfer task, where transferring the target style onto a given sentence can be naturally viewed as post-editing the sentence into the target style. XL-Editor achieves significant improvement in style transfer accuracy and also maintains coherent semantic of the original sentence, showing the broad applicability of our method.

</details>

<details>

<summary>2019-10-20 03:57:00 - Deep Music Analogy Via Latent Representation Disentanglement</summary>

- *Ruihan Yang, Dingsu Wang, Ziyu Wang, Tianyao Chen, Junyan Jiang, Gus Xia*

- `1906.03626v4` - [abs](http://arxiv.org/abs/1906.03626v4) - [pdf](http://arxiv.org/pdf/1906.03626v4)

> Analogy-making is a key method for computer algorithms to generate both natural and creative music pieces. In general, an analogy is made by partially transferring the music abstractions, i.e., high-level representations and their relationships, from one piece to another; however, this procedure requires disentangling music representations, which usually takes little effort for musicians but is non-trivial for computers. Three sub-problems arise: extracting latent representations from the observation, disentangling the representations so that each part has a unique semantic interpretation, and mapping the latent representations back to actual music. In this paper, we contribute an explicitly-constrained variational autoencoder (EC$^2$-VAE) as a unified solution to all three sub-problems. We focus on disentangling the pitch and rhythm representations of 8-beat music clips conditioned on chords. In producing music analogies, this model helps us to realize the imaginary situation of "what if" a piece is composed using a different pitch contour, rhythm pattern, or chord progression by borrowing the representations from other pieces. Finally, we validate the proposed disentanglement method using objective measurements and evaluate the analogy examples by a subjective study.

</details>

<details>

<summary>2019-10-20 07:16:29 - Leveraging Hierarchical Representations for Preserving Privacy and Utility in Text</summary>

- *Oluwaseyi Feyisetan, Tom Diethe, Thomas Drake*

- `1910.08917v1` - [abs](http://arxiv.org/abs/1910.08917v1) - [pdf](http://arxiv.org/pdf/1910.08917v1)

> Guaranteeing a certain level of user privacy in an arbitrary piece of text is a challenging issue. However, with this challenge comes the potential of unlocking access to vast data stores for training machine learning models and supporting data driven decisions. We address this problem through the lens of dx-privacy, a generalization of Differential Privacy to non Hamming distance metrics. In this work, we explore word representations in Hyperbolic space as a means of preserving privacy in text. We provide a proof satisfying dx-privacy, then we define a probability distribution in Hyperbolic space and describe a way to sample from it in high dimensions. Privacy is provided by perturbing vector representations of words in high dimensional Hyperbolic space to obtain a semantic generalization. We conduct a series of experiments to demonstrate the tradeoff between privacy and utility. Our privacy experiments illustrate protections against an authorship attribution algorithm while our utility experiments highlight the minimal impact of our perturbations on several downstream machine learning models. Compared to the Euclidean baseline, we observe > 20x greater guarantees on expected privacy against comparable worst case statistics.

</details>

<details>

<summary>2019-10-20 09:11:34 - Representation, Justification and Explanation in a Value Driven Agent: An Argumentation-Based Approach</summary>

- *Beishui Liao, Michael Anderson, Susan Leigh Anderson*

- `1812.05362v2` - [abs](http://arxiv.org/abs/1812.05362v2) - [pdf](http://arxiv.org/pdf/1812.05362v2)

> Ethical and explainable artificial intelligence is an interdisciplinary research area involving computer science, philosophy, logic, the social sciences, etc. For an ethical autonomous system, the ability to justify and explain its decision making is a crucial aspect of transparency and trustworthiness. This paper takes a Value Driven Agent (VDA) as an example, explicitly representing implicit knowledge of a machine learning-based autonomous agent and using this formalism to justify and explain the decisions of the agent. For this purpose, we introduce a novel formalism to describe the intrinsic knowledge and solutions of a VDA in each situation. Based on this formalism, we formulate an approach to justify and explain the decision-making process of a VDA, in terms of a typical argumentation formalism, Assumption-based Argumentation (ABA). As a result, a VDA in a given situation is mapped onto an argumentation framework in which arguments are defined by the notion of deduction. Justified actions with respect to semantics from argumentation correspond to solutions of the VDA. The acceptance (rejection) of arguments and their premises in the framework provides an explanation for why an action was selected (or not). Furthermore, we go beyond the existing version of VDA, considering not only practical reasoning, but also epistemic reasoning, such that the inconsistency of knowledge of the VDA can be identified, handled and explained.

</details>

<details>

<summary>2019-10-20 14:20:59 - Semantic Graph Parsing with Recurrent Neural Network DAG Grammars</summary>

- *Federico Fancellu, Sorcha Gilroy, Adam Lopez, Mirella Lapata*

- `1910.00051v2` - [abs](http://arxiv.org/abs/1910.00051v2) - [pdf](http://arxiv.org/pdf/1910.00051v2)

> Semantic parses are directed acyclic graphs (DAGs), so semantic parsing should be modeled as graph prediction. But predicting graphs presents difficult technical challenges, so it is simpler and more common to predict the linearized graphs found in semantic parsing datasets using well-understood sequence models. The cost of this simplicity is that the predicted strings may not be well-formed graphs. We present recurrent neural network DAG grammars, a graph-aware sequence model that ensures only well-formed graphs while sidestepping many difficulties in graph prediction. We test our model on the Parallel Meaning Bank---a multilingual semantic graphbank. Our approach yields competitive results in English and establishes the first results for German, Italian and Dutch.

</details>

<details>

<summary>2019-10-21 00:00:17 - Semantics for Global and Local Interpretation of Deep Neural Networks</summary>

- *Jindong Gu, Volker Tresp*

- `1910.09085v1` - [abs](http://arxiv.org/abs/1910.09085v1) - [pdf](http://arxiv.org/pdf/1910.09085v1)

> Deep neural networks (DNNs) with high expressiveness have achieved state-of-the-art performance in many tasks. However, their distributed feature representations are difficult to interpret semantically. In this work, human-interpretable semantic concepts are associated with vectors in feature space. The association process is mathematically formulated as an optimization problem. The semantic vectors obtained from the optimal solution are applied to interpret deep neural networks globally and locally. The global interpretations are useful to understand the knowledge learned by DNNs. The interpretation of local behaviors can help to understand individual decisions made by DNNs better. The empirical experiments demonstrate how to use identified semantics to interpret the existing DNNs.

</details>

<details>

<summary>2019-10-21 07:35:46 - Semantic Graph Convolutional Network for Implicit Discourse Relation Classification</summary>

- *Yingxue Zhang, Ping Jian, Fandong Meng, Ruiying Geng, Wei Cheng, Jie Zhou*

- `1910.09183v1` - [abs](http://arxiv.org/abs/1910.09183v1) - [pdf](http://arxiv.org/pdf/1910.09183v1)

> Implicit discourse relation classification is of great importance for discourse parsing, but remains a challenging problem due to the absence of explicit discourse connectives communicating these relations. Modeling the semantic interactions between the two arguments of a relation has proven useful for detecting implicit discourse relations. However, most previous approaches model such semantic interactions from a shallow interactive level, which is inadequate on capturing enough semantic information. In this paper, we propose a novel and effective Semantic Graph Convolutional Network (SGCN) to enhance the modeling of inter-argument semantics on a deeper interaction level for implicit discourse relation classification. We first build an interaction graph over representations of the two arguments, and then automatically extract in-depth semantic interactive information through graph convolution. Experimental results on the English corpus PDTB and the Chinese corpus CDTB both demonstrate the superiority of our model to previous state-of-the-art systems.

</details>

<details>

<summary>2019-10-21 11:59:01 - PiBooster: A Light-Weight Approach to Performance Improvements in Page Table Management for Paravirtual Virtual-Machines</summary>

- *Zhi Zhang, Yueqiang Cheng*

- `1910.09277v1` - [abs](http://arxiv.org/abs/1910.09277v1) - [pdf](http://arxiv.org/pdf/1910.09277v1)

> In paravirtualization, the page table management components of the guest operating systems are properly patched for the security guarantees of the hypervisor. However, none of them pay enough attentions to the performance improvements, which results in two noticeable performance issues. First, such security patches exacerbate the problem that the execution paths of the guest page table (de)allocations become extremely long, which would consequently increase the latencies of process creations and exits. Second, the patches introduce many additional IOTLB flushes, leading to extra IOTLB misses, and the misses would have negative impacts on I/O performance of all peripheral devices. In this paper, we propose PiBooster, a novel lightweight approach for improving the performance in page table management. First, PiBooster shortens the execution paths of the page table (de)allocations by the PiBooster cache, which maintains dedicated buffers for serving page table (de)allocations. Second, PiBooster eliminates the additional IOTLB misses with a fine-grained validation scheme, which performs page table and DMA validations separately, instead of doing both together. We implement a prototype on Xen with Linux as the guest kernel. We do small modifications on Xen (166 SLoC) and Linux kernel (350 SLoC). We evaluate the I/O performance in both micro and macro ways. The micro experiment results indicate that PiBooster is able to completely eliminate the additional IOTLB flushes in the workload-stable environments, and effectively reduces (de)allocation time of the page table by 47% on average. The macro benchmarks show that the latencies of the process creations and exits are expectedly reduced by 16% on average. Moreover, the SPECINT,lmbench and netperf results indicate that PiBooster has no negative performance impacts on CPU computation, network I/O, and disk I/O.

</details>

<details>

<summary>2019-10-21 12:43:25 - MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning</summary>

- *Dominik Müller, Frank Kramer*

- `1910.09308v1` - [abs](http://arxiv.org/abs/1910.09308v1) - [pdf](http://arxiv.org/pdf/1910.09308v1)

> The increased availability and usage of modern medical imaging induced a strong need for automatic medical image segmentation. Still, current image segmentation platforms do not provide the required functionalities for plain setup of medical image segmentation pipelines. Already implemented pipelines are commonly standalone software, optimized on a specific public data set. Therefore, this paper introduces the open-source Python library MIScnn. The aim of MIScnn is to provide an intuitive API allowing fast building of medical image segmentation pipelines including data I/O, preprocessing, data augmentation, patch-wise analysis, metrics, a library with state-of-the-art deep learning models and model utilization like training, prediction, as well as fully automatic evaluation (e.g. cross-validation). Similarly, high configurability and multiple open interfaces allow full pipeline customization. Running a cross-validation with MIScnn on the Kidney Tumor Segmentation Challenge 2019 data set (multi-class semantic segmentation with 300 CT scans) resulted into a powerful predictor based on the standard 3D U-Net model. With this experiment, we could show that the MIScnn framework enables researchers to rapidly set up a complete medical image segmentation pipeline by using just a few lines of code. The source code for MIScnn is available in the Git repository: https://github.com/frankkramer-lab/MIScnn.

</details>

<details>

<summary>2019-10-21 13:34:12 - Improving Word Representations: A Sub-sampled Unigram Distribution for Negative Sampling</summary>

- *Wenxiang Jiao, Irwin King, Michael R. Lyu*

- `1910.09362v1` - [abs](http://arxiv.org/abs/1910.09362v1) - [pdf](http://arxiv.org/pdf/1910.09362v1)

> Word2Vec is the most popular model for word representation and has been widely investigated in literature. However, its noise distribution for negative sampling is decided by empirical trials and the optimality has always been ignored. We suggest that the distribution is a sub-optimal choice, and propose to use a sub-sampled unigram distribution for better negative sampling. Our contributions include: (1) proposing the concept of semantics quantification and deriving a suitable sub-sampling rate for the proposed distribution adaptive to different training corpora; (2) demonstrating the advantages of our approach in both negative sampling and noise contrastive estimation by extensive evaluation tasks; and (3) proposing a semantics weighted model for the MSR sentence completion task, resulting in considerable improvements. Our work not only improves the quality of word vectors but also benefits current understanding of Word2Vec.

</details>

<details>

<summary>2019-10-21 14:23:14 - A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis</summary>

- *Jorge Agnese, Jonathan Herrera, Haicheng Tao, Xingquan Zhu*

- `1910.09399v1` - [abs](http://arxiv.org/abs/1910.09399v1) - [pdf](http://arxiv.org/pdf/1910.09399v1)

> Text-to-image synthesis refers to computational methods which translate human written textual descriptions, in the form of keywords or sentences, into images with similar semantic meaning to the text. In earlier research, image synthesis relied mainly on word to image correlation analysis combined with supervised methods to find best alignment of the visual content matching to the text. Recent progress in deep learning (DL) has brought a new set of unsupervised deep learning methods, particularly deep generative models which are able to generate realistic visual images using suitably trained neural network models. In this paper, we review the most recent development in the text-to-image synthesis research domain. Our survey first introduces image synthesis and its challenges, and then reviews key concepts such as generative adversarial networks (GANs) and deep convolutional encoder-decoder neural networks (DCNN). After that, we propose a taxonomy to summarize GAN based text-to-image synthesis into four major categories: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs. We elaborate the main objective of each group, and further review typical GAN architectures in each group. The taxonomy and the review outline the techniques and the evolution of different approaches, and eventually provide a clear roadmap to summarize the list of contemporaneous solutions that utilize GANs and DCNNs to generate enthralling results in categories such as human faces, birds, flowers, room interiors, object reconstruction from edge maps (games) etc. The survey will conclude with a comparison of the proposed solutions, challenges that remain unresolved, and future developments in the text-to-image synthesis domain.

</details>

<details>

<summary>2019-10-21 15:25:27 - Zero-shot Learning via Simultaneous Generating and Learning</summary>

- *Hyeonwoo Yu, Beomhee Lee*

- `1910.09446v1` - [abs](http://arxiv.org/abs/1910.09446v1) - [pdf](http://arxiv.org/pdf/1910.09446v1)

> To overcome the absence of training data for unseen classes, conventional zero-shot learning approaches mainly train their model on seen datapoints and leverage the semantic descriptions for both seen and unseen classes. Beyond exploiting relations between classes of seen and unseen, we present a deep generative model to provide the model with experience about both seen and unseen classes. Based on the variational auto-encoder with class-specific multi-modal prior, the proposed method learns the conditional distribution of seen and unseen classes. In order to circumvent the need for samples of unseen classes, we treat the non-existing data as missing examples. That is, our network aims to find optimal unseen datapoints and model parameters, by iteratively following the generating and learning strategy. Since we obtain the conditional generative model for both seen and unseen classes, classification as well as generation can be performed directly without any off-the-shell classifiers. In experimental results, we demonstrate that the proposed generating and learning strategy makes the model achieve the outperforming results compared to that trained only on the seen classes, and also to the several state-of-the-art methods.

</details>

<details>

<summary>2019-10-21 20:26:54 - GANspection</summary>

- *Hammad A. Ayyubi*

- `1910.09638v1` - [abs](http://arxiv.org/abs/1910.09638v1) - [pdf](http://arxiv.org/pdf/1910.09638v1)

> Generative Adversarial Networks (GANs) have been used extensively and quite successfully for unsupervised learning. As GANs don't approximate an explicit probability distribution, it's an interesting study to inspect the latent space representations learned by GANs. The current work seeks to push the boundaries of such inspection methods to further understand in more detail the manifold being learned by GANs. Various interpolation and extrapolation techniques along with vector arithmetic is used to understand the learned manifold. We show through experiments that GANs indeed learn a data probability distribution rather than memorize images/data. Further, we prove that GANs encode semantically relevant information in the learned probability distribution. The experiments have been performed on two publicly available datasets - Large Scale Scene Understanding (LSUN) and CelebA.

</details>

<details>

<summary>2019-10-22 01:02:51 - Embedded Bayesian Network Classifiers</summary>

- *David Heckerman, Chris Meek*

- `1910.09715v1` - [abs](http://arxiv.org/abs/1910.09715v1) - [pdf](http://arxiv.org/pdf/1910.09715v1)

> Low-dimensional probability models for local distribution functions in a Bayesian network include decision trees, decision graphs, and causal independence models. We describe a new probability model for discrete Bayesian networks, which we call an embedded Bayesian network classifier or EBNC. The model for a node $Y$ given parents $\bf X$ is obtained from a (usually different) Bayesian network for $Y$ and $\bf X$ in which $\bf X$ need not be the parents of $Y$. We show that an EBNC is a special case of a softmax polynomial regression model. Also, we show how to identify a non-redundant set of parameters for an EBNC, and describe an asymptotic approximation for learning the structure of Bayesian networks that contain EBNCs. Unlike the decision tree, decision graph, and causal independence models, we are unaware of a semantic justification for the use of these models. Experiments are needed to determine whether the models presented in this paper are useful in practice.

</details>

<details>

<summary>2019-10-22 01:09:17 - PFirewall: Semantics-Aware Customizable Data Flow Control for Home Automation Systems</summary>

- *Haotian Chi, Qiang Zeng, Xiaojiang Du, Lannan Luo*

- `1910.07987v3` - [abs](http://arxiv.org/abs/1910.07987v3) - [pdf](http://arxiv.org/pdf/1910.07987v3)

> Emerging Internet of Thing (IoT) platforms provide a convenient solution for integrating heterogeneous IoT devices and deploying home automation applications. However, serious privacy threats arise as device data now flow out to the IoT platforms, which may be subject to various attacks. We observe two privacy-unfriendly practices in emerging home automation systems: first, the majority of data flowed to the platform are superfluous in the sense that they do not trigger any home automation; second, home owners currently have nearly zero control over their data.   We present PFirewall, a customizable data-flow control system to enhance user privacy. PFirewall analyzes the automation apps to extract their semantics, which are automatically transformed into data-minimization policies; these policies only send minimized data flows to the platform for app execution, such that the ability of attackers to infer user privacy is significantly impaired. In addition, PFirewall provides capabilities and interfaces for users to define and enforce customizable policies based on individual privacy preferences. PFirewall adopts an elegant man-in-the-middle design, transparently executing data minimization and user-defined policies to process raw data flows and mediating the processed data between IoT devices and the platform (via the hub), without requiring modifications of the platform or IoT devices. We implement PFirewall to work with two popular platforms: SmartThings and openHAB, and set up two real-world testbeds to evaluate its performance. The evaluation results show that PFirewall is very effective: it reduces IoT data sent to the platform by 97% and enforces user defined policies successfully.

</details>

<details>

<summary>2019-10-22 03:06:37 - Free-Form Image Inpainting with Gated Convolution</summary>

- *Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas Huang*

- `1806.03589v2` - [abs](http://arxiv.org/abs/1806.03589v2) - [pdf](http://arxiv.org/pdf/1806.03589v2)

> We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting

</details>

<details>

<summary>2019-10-22 05:49:54 - Self-Correction for Human Parsing</summary>

- *Peike Li, Yunqiu Xu, Yunchao Wei, Yi Yang*

- `1910.09777v1` - [abs](http://arxiv.org/abs/1910.09777v1) - [pdf](http://arxiv.org/pdf/1910.09777v1)

> Labeling pixel-level masks for fine-grained semantic segmentation tasks, e.g. human parsing, remains a challenging task. The ambiguous boundary between different semantic parts and those categories with similar appearance usually are confusing, leading to unexpected noises in ground truth masks. To tackle the problem of learning with label noises, this work introduces a purification strategy, called Self-Correction for Human Parsing (SCHP), to progressively promote the reliability of the supervised labels as well as the learned models. In particular, starting from a model trained with inaccurate annotations as initialization, we design a cyclically learning scheduler to infer more reliable pseudo-masks by iteratively aggregating the current learned model with the former optimal one in an online manner. Besides, those correspondingly corrected labels can in turn to further boost the model performance. In this way, the models and the labels will reciprocally become more robust and accurate during the self-correction learning cycles. Benefiting from the superiority of SCHP, we achieve the best performance on two popular single-person human parsing benchmarks, including LIP and Pascal-Person-Part datasets. Our overall system ranks 1st in CVPR2019 LIP Challenge. Code is available at https://github.com/PeikeLi/Self-Correction-Human-Parsing.

</details>

<details>

<summary>2019-10-22 08:39:30 - CBOWRA: A Representation Learning Approach for Medication Anomaly Detection</summary>

- *Liang Zhao, Zhiyuan Ma, Yangming Zhou, Kai Wang, Shengping Liu, Ju Gao*

- `1908.07147v2` - [abs](http://arxiv.org/abs/1908.07147v2) - [pdf](http://arxiv.org/pdf/1908.07147v2)

> Electronic health record is an important source for clinical researches and applications, and errors inevitably occur in the data, which could lead to severe damages to both patients and hospital services. One of such error is the mismatches between diagnoses and prescriptions, which we address as 'medication anomaly' in the paper, and clinicians used to manually identify and correct them. With the development of machine learning techniques, researchers are able to train specific model for the task, but the process still requires expert knowledge to construct proper features, and few semantic relations are considered. In this paper, we propose a simple, yet effective detection method that tackles the problem by detecting the semantic inconsistency between diagnoses and prescriptions. Unlike traditional outlier or anomaly detection, the scheme uses continuous bag of words to construct the semantic connection between specific central words and their surrounding context. The detection of medication anomaly is transformed into identifying the least possible central word based on given context. To help distinguish the anomaly from normal context, we also incorporate a ranking accumulation strategy. The experiments were conducted on two real hospital electronic medical records, and the topN accuracy of the proposed method increased by 3.91 to 10.91% and 0.68 to 2.13% on the datasets, respectively, which is highly competitive to other traditional machine learning-based approaches.

</details>

<details>

<summary>2019-10-22 13:05:11 - Double Supervised Network with Attention Mechanism for Scene Text Recognition</summary>

- *Yuting Gao, Zheng Huang, Yuchen Dai, Cheng Xu, Kai Chen, Jie Tuo*

- `1808.00677v3` - [abs](http://arxiv.org/abs/1808.00677v3) - [pdf](http://arxiv.org/pdf/1808.00677v3)

> In this paper, we propose Double Supervised Network with Attention Mechanism (DSAN), a novel end-to-end trainable framework for scene text recognition. It incorporates one text attention module during feature extraction which enforces the model to focus on text regions and the whole framework is supervised by two branches. One supervision branch comes from context-level modelling and another comes from one extra supervision enhancement branch which aims at tackling inexplicit semantic information at character level. These two supervisions can benefit each other and yield better performance. The proposed approach can recognize text in arbitrary length and does not need any predefined lexicon. Our method outperforms the current state-of-the-art methods on three text recognition benchmarks: IIIT5K, ICDAR2013 and SVT reaching accuracy 88.6%, 92.3% and 84.1% respectively which suggests the effectiveness of the proposed method.

</details>

<details>

<summary>2019-10-22 15:09:02 - Language-guided Semantic Mapping and Mobile Manipulation in Partially Observable Environments</summary>

- *Siddharth Patki, Ethan Fahnestock, Thomas M. Howard, Matthew R. Walter*

- `1910.10034v1` - [abs](http://arxiv.org/abs/1910.10034v1) - [pdf](http://arxiv.org/pdf/1910.10034v1)

> Recent advances in data-driven models for grounded language understanding have enabled robots to interpret increasingly complex instructions. Two fundamental limitations of these methods are that most require a full model of the environment to be known a priori, and they attempt to reason over a world representation that is flat and unnecessarily detailed, which limits scalability. Recent semantic mapping methods address partial observability by exploiting language as a sensor to infer a distribution over topological, metric and semantic properties of the environment. However, maintaining a distribution over highly detailed maps that can support grounding of diverse instructions is computationally expensive and hinders real-time human-robot collaboration. We propose a novel framework that learns to adapt perception according to the task in order to maintain compact distributions over semantic maps. Experiments with a mobile manipulator demonstrate more efficient instruction following in a priori unknown environments.

</details>

<details>

<summary>2019-10-22 15:47:56 - Attacking Optical Flow</summary>

- *Anurag Ranjan, Joel Janai, Andreas Geiger, Michael J. Black*

- `1910.10053v1` - [abs](http://arxiv.org/abs/1910.10053v1) - [pdf](http://arxiv.org/pdf/1910.10053v1)

> Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.

</details>

<details>

<summary>2019-10-23 05:02:15 - TCT: A Cross-supervised Learning Method for Multimodal Sequence Representation</summary>

- *Wubo Li, Wei Zou, Xiangang Li*

- `1911.05186v1` - [abs](http://arxiv.org/abs/1911.05186v1) - [pdf](http://arxiv.org/pdf/1911.05186v1)

> Multimodalities provide promising performance than unimodality in most tasks. However, learning the semantic of the representations from multimodalities efficiently is extremely challenging. To tackle this, we propose the Transformer based Cross-modal Translator (TCT) to learn unimodal sequence representations by translating from other related multimodal sequences on a supervised learning method. Combined TCT with Multimodal Transformer Network (MTN), we evaluate MTN-TCT on the video-grounded dialogue which uses multimodality. The proposed method reports new state-of-the-art performance on video-grounded dialogue which indicates representations learned by TCT are more semantics compared to directly use unimodality.

</details>

<details>

<summary>2019-10-23 05:50:04 - Deep Classification Network for Monocular Depth Estimation</summary>

- *Azeez Oluwafemi, Yang Zou, B. V. K. Vijaya Kumar*

- `1910.10369v1` - [abs](http://arxiv.org/abs/1910.10369v1) - [pdf](http://arxiv.org/pdf/1910.10369v1)

> Monocular Depth Estimation is usually treated as a supervised and regression problem when it actually is very similar to semantic segmentation task since they both are fundamentally pixel-level classification tasks. We applied depth increments that increases with depth in discretizing depth values and then applied Deeplab v2 and the result was higher accuracy. We were able to achieve a state-of-the-art result on the KITTI dataset and outperformed existing architecture by an 8% margin.

</details>

<details>

<summary>2019-10-23 06:32:16 - Network2Vec Learning Node Representation Based on Space Mapping in Networks</summary>

- *Huang Zhenhua, Wang Zhenyu, Zhang Rui, Zhao Yangyang, Xie Xiaohui, Sharad Mehrotra*

- `1910.10379v1` - [abs](http://arxiv.org/abs/1910.10379v1) - [pdf](http://arxiv.org/pdf/1910.10379v1)

> Complex networks represented as node adjacency matrices constrains the application of machine learning and parallel algorithms. To address this limitation, network embedding (i.e., graph representation) has been intensively studied to learn a fixed-length vector for each node in an embedding space, where the node properties in the original graph are preserved. Existing methods mainly focus on learning embedding vectors to preserve nodes proximity, i.e., nodes next to each other in the graph space should also be closed in the embedding space, but do not enforce algebraic statistical properties to be shared between the embedding space and graph space. In this work, we propose a lightweight model, entitled Network2Vec, to learn network embedding on the base of semantic distance mapping between the graph space and embedding space. The model builds a bridge between the two spaces leveraging the property of group homomorphism. Experiments on different learning tasks, including node classification, link prediction, and community visualization, demonstrate the effectiveness and efficiency of the new embedding method, which improves the state-of-the-art model by 19% in node classification and 7% in link prediction tasks at most. In addition, our method is significantly faster, consuming only a fraction of the time used by some famous methods.

</details>

<details>

<summary>2019-10-23 14:15:42 - Learning Priors in High-frequency Domain for Inverse Imaging Reconstruction</summary>

- *Zhuonan He, Jinjie Zhou, Dong Liang, Yuhao Wang, Qiegen Liu*

- `1910.11148v1` - [abs](http://arxiv.org/abs/1910.11148v1) - [pdf](http://arxiv.org/pdf/1910.11148v1)

> Ill-posed inverse problems in imaging remain an active research topic in several decades, with new approaches constantly emerging. Recognizing that the popular dictionary learning and convolutional sparse coding are both essentially modeling the high-frequency component of an image, which convey most of the semantic information such as texture details, in this work we propose a novel multi-profile high-frequency transform-guided denoising autoencoder as prior (HF-DAEP). To achieve this goal, we first extract a set of multi-profile high-frequency components via a specific transformation and add the artificial Gaussian noise to these high-frequency components as training samples. Then, as the high-frequency prior information is learned, we incorporate it into classical iterative reconstruction process by proximal gradient descent technique. Preliminary results on highly under-sampled magnetic resonance imaging and sparse-view computed tomography reconstruction demonstrate that the proposed method can efficiently reconstruct feature details and present advantages over state-of-the-arts.

</details>

<details>

<summary>2019-10-23 17:57:11 - Correction of Automatic Speech Recognition with Transformer Sequence-to-sequence Model</summary>

- *Oleksii Hrinchuk, Mariya Popova, Boris Ginsburg*

- `1910.10697v1` - [abs](http://arxiv.org/abs/1910.10697v1) - [pdf](http://arxiv.org/pdf/1910.10697v1)

> In this work, we introduce a simple yet efficient post-processing model for automatic speech recognition (ASR). Our model has Transformer-based encoder-decoder architecture which "translates" ASR model output into grammatically and semantically correct text. We investigate different strategies for regularizing and optimizing the model and show that extensive data augmentation and the initialization with pre-trained weights are required to achieve good performance. On the LibriSpeech benchmark, our method demonstrates significant improvement in word error rate over the baseline acoustic model with greedy decoding, especially on much noisier dev-other and test-other portions of the evaluation dataset. Our model also outperforms baseline with 6-gram language model re-scoring and approaches the performance of re-scoring with Transformer-XL neural language model.

</details>

<details>

<summary>2019-10-23 18:24:55 - A Novel Approach for Automatic Bengali Question Answering System using Semantic Similarity Analysis</summary>

- *Arijit Das, Jaydeep Mandal, Zargham Danial, Alok Ranjan Pal, Diganta Saha*

- `1910.10758v1` - [abs](http://arxiv.org/abs/1910.10758v1) - [pdf](http://arxiv.org/pdf/1910.10758v1)

> Finding the semantically accurate answer is one of the key challenges in advanced searching. In contrast to keyword-based searching, the meaning of a question or query is important here and answers are ranked according to relevance. It is very natural that there is almost no common word between the question sentence and the answer sentence. In this paper, an approach is described to find out the semantically relevant answers in the Bengali dataset. In the first part of the algorithm, a set of statistical parameters like frequency, index, part-of-speech (POS), etc. is matched between a question and the probable answers. In the second phase, entropy and similarity are calculated in different modules. Finally, a sense score is generated to rank the answers. The algorithm is tested on a repository containing a total of 275000 sentences. This Bengali repository is a product of Technology Development for Indian Languages (TDIL) project sponsored by Govt. of India and provided by the Language Research Unit of Indian Statistical Institute, Kolkata. The shallow parser, developed by the LTRC group of IIIT Hyderabad is used for POS tagging. The actual answer is ranked as 1st in 82.3% cases. The actual answer is ranked within 1st to 5th in 90.0% cases. The accuracy of the system is coming as 97.32% and precision of the system is coming as 98.14% using confusion matrix. The challenges and pitfalls of the work are reported at last in this paper.

</details>

<details>

<summary>2019-10-23 20:23:22 - Low Shot Learning with Untrained Neural Networks for Imaging Inverse Problems</summary>

- *Oscar Leong, Wesam Sakla*

- `1910.10797v1` - [abs](http://arxiv.org/abs/1910.10797v1) - [pdf](http://arxiv.org/pdf/1910.10797v1)

> Employing deep neural networks as natural image priors to solve inverse problems either requires large amounts of data to sufficiently train expressive generative models or can succeed with no data via untrained neural networks. However, very few works have considered how to interpolate between these no- to high-data regimes. In particular, how can one use the availability of a small amount of data (even $5-25$ examples) to one's advantage in solving these inverse problems and can a system's performance increase as the amount of data increases as well? In this work, we consider solving linear inverse problems when given a small number of examples of images that are drawn from the same distribution as the image of interest. Comparing to untrained neural networks that use no data, we show how one can pre-train a neural network with a few given examples to improve reconstruction results in compressed sensing and semantic image recovery problems such as colorization. Our approach leads to improved reconstruction as the amount of available data increases and is on par with fully trained generative models, while requiring less than $1 \%$ of the data needed to train a generative model.

</details>

<details>

<summary>2019-10-23 20:23:44 - Context-endcoding for neural network based skull stripping in magnetic resonance imaging</summary>

- *Zhen Liu, Borui Xiao, Yuemeng Li, Yong Fan*

- `1910.10798v1` - [abs](http://arxiv.org/abs/1910.10798v1) - [pdf](http://arxiv.org/pdf/1910.10798v1)

> Skull stripping is usually the first step for most brain analysisprocess in magnetic resonance images. A lot of deep learn-ing neural network based methods have been developed toachieve higher accuracy. Since the 3D deep learning modelssuffer from high computational cost and are subject to GPUmemory limit challenge, a variety of 2D deep learning meth-ods have been developed. However, existing 2D deep learn-ing methods are not equipped to effectively capture 3D se-mantic information that is needed to achieve higher accuracy.In this paper, we propose a context-encoding method to em-power the 2D network to capture the 3D context information.For the context-encoding method, firstly we encode the 2Dfeatures of original 2D network, secondly we encode the sub-volume of 3D MRI images, finally we fuse the encoded 2Dfeatures and 3D features with semantic encoding classifica-tion loss. To get computational efficiency, although we en-code the sub-volume of 3D MRI images instead of buildinga 3D neural network, extensive experiments on three bench-mark Datasets demonstrate our method can achieve superioraccuracy to state-of-the-art alternative methods with the dicescore 99.6% on NFBS and 99.09 % on LPBA40 and 99.17 %on OASIS.

</details>

<details>

<summary>2019-10-23 22:20:22 - Intranet Security using a LAN Packet Sniffer to Monitor Traffic</summary>

- *Henry N. Ogbu, Moses Adah Agana*

- `1910.10827v1` - [abs](http://arxiv.org/abs/1910.10827v1) - [pdf](http://arxiv.org/pdf/1910.10827v1)

> This paper was designed to provide Intranet traffic monitoring by sniffing the packets at the local Area Network (LAN) server end to provide security and control. It was implemented using five computer systems configured with static Internet Protocol (IP) addresses used in monitoring the IP traffic on the network by capturing and analyzing live packets from various sources and destinations in the network. The LAN was deployed on windows 8 with a D-link 16-port switch, category 6 Ethernet cable and other LAN devices. The IP traffics were captured and analyzed using Wireshark Version 2.0.3. Four network instructions were used in the analysis of the IP traffic and the results displayed the IP and Media Access Control (MAC) address sources and destinations of the frames, Ethernet, IP addresses, User Datagram Protocol (UDP) and Hypertext Transfer Protocol (HTTP). The outcome can aid network administrators to control Intranet access and provide security.

</details>

<details>

<summary>2019-10-23 23:55:23 - Relation Module for Non-answerable Prediction on Question Answering</summary>

- *Kevin Huang, Yun Tang, Jing Huang, Xiaodong He, Bowen Zhou*

- `1910.10843v1` - [abs](http://arxiv.org/abs/1910.10843v1) - [pdf](http://arxiv.org/pdf/1910.10843v1)

> Machine reading comprehension(MRC) has attracted significant amounts of research attention recently, due to an increase of challenging reading comprehension datasets. In this paper, we aim to improve a MRC model's ability to determine whether a question has an answer in a given context (e.g. the recently proposed SQuAD 2.0 task). Our solution is a relation module that is adaptable to any MRC model. The relation module consists of both semantic extraction and relational information. We first extract high level semantics as objects from both question and context with multi-head self-attentive pooling. These semantic objects are then passed to a relation network, which generates relationship scores for each object pair in a sentence. These scores are used to determine whether a question is non-answerable. We test the relation module on the SQuAD 2.0 dataset using both BiDAF and BERT models as baseline readers. We obtain 1.8% gain of F1 on top of the BiDAF reader, and 1.0% on top of the BERT base model. These results show the effectiveness of our relation module on MRC

</details>

<details>

<summary>2019-10-23 23:56:32 - Universal Text Representation from BERT: An Empirical Study</summary>

- *Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, Bing Xiang*

- `1910.07973v2` - [abs](http://arxiv.org/abs/1910.07973v2) - [pdf](http://arxiv.org/pdf/1910.07973v2)

> We present a systematic investigation of layer-wise BERT activations for general-purpose text representations to understand what linguistic information they capture and how transferable they are across different tasks. Sentence-level embeddings are evaluated against two state-of-the-art models on downstream and probing tasks from SentEval, while passage-level embeddings are evaluated on four question-answering (QA) datasets under a learning-to-rank problem setting. Embeddings from the pre-trained BERT model perform poorly in semantic similarity and sentence surface information probing tasks. Fine-tuning BERT on natural language inference data greatly improves the quality of the embeddings. Combining embeddings from different BERT layers can further boost performance. BERT embeddings outperform BM25 baseline significantly on factoid QA datasets at the passage level, but fail to perform better than BM25 on non-factoid datasets. For all QA datasets, there is a gap between embedding-based method and in-domain fine-tuned BERT (we report new state-of-the-art results on two datasets), which suggests deep interactions between question and answer pairs are critical for those hard tasks.

</details>

<details>

<summary>2019-10-24 00:22:43 - GF + MMT = GLF -- From Language to Semantics through LF</summary>

- *Michael Kohlhase, Jan Frederik Schaefer*

- `1910.10849v1` - [abs](http://arxiv.org/abs/1910.10849v1) - [pdf](http://arxiv.org/pdf/1910.10849v1)

> These days, vast amounts of knowledge are available online, most of it in written form. Search engines help us access this knowledge, but aggregating, relating and reasoning with it is still a predominantly human effort. One of the key challenges for automated reasoning based on natural-language texts is the need to extract meaning (semantics) from texts. Natural language understanding (NLU) systems describe the conversion from a set of natural language utterances to terms in a particular logic. Tools for the co-development of grammar and target logic are currently largely missing.   We will describe the Grammatical Logical Framework (GLF), a combination of two existing frameworks, in which large parts of a symbolic, rule-based NLU system can be developed and implemented: the Grammatical Framework (GF) and MMT. GF is a tool for syntactic analysis, generation, and translation with complex natural language grammars and MMT can be used to specify logical systems and to represent knowledge in them. Combining these tools is possible, because they are based on compatible logical frameworks: Martin-L\"of type theory and LF. The flexibility of logical frameworks is needed, as NLU research has not settled on a particular target logic for meaning representation. Instead, new logics are developed all the time to handle various language phenomena. GLF allows users to develop the logic and the language parsing components in parallel, and to connect them for experimentation with the entire pipeline.

</details>

<details>

<summary>2019-10-24 01:13:12 - Panoptic-DeepLab</summary>

- *Bowen Cheng, Maxwell D. Collins, Yukun Zhu, Ting Liu, Thomas S. Huang, Hartwig Adam, Liang-Chieh Chen*

- `1910.04751v3` - [abs](http://arxiv.org/abs/1910.04751v3) - [pdf](http://arxiv.org/pdf/1910.04751v3)

> We present Panoptic-DeepLab, a bottom-up and single-shot approach for panoptic segmentation. Our Panoptic-DeepLab is conceptually simple and delivers state-of-the-art results. In particular, we adopt the dual-ASPP and dual-decoder structures specific to semantic, and instance segmentation, respectively. The semantic segmentation branch is the same as the typical design of any semantic segmentation model (e.g., DeepLab), while the instance segmentation branch is class-agnostic, involving a simple instance center regression. Our single Panoptic-DeepLab sets the new state-of-art at all three Cityscapes benchmarks, reaching 84.2% mIoU, 39.0% AP, and 65.5% PQ on test set, and advances results on the other challenging Mapillary Vistas.

</details>

<details>

<summary>2019-10-24 03:00:53 - Low-Resource Sequence Labeling via Unsupervised Multilingual Contextualized Representations</summary>

- *Zuyi Bao, Rui Huang, Chen Li, Kenny Q. Zhu*

- `1910.10893v1` - [abs](http://arxiv.org/abs/1910.10893v1) - [pdf](http://arxiv.org/pdf/1910.10893v1)

> Previous work on cross-lingual sequence labeling tasks either requires parallel data or bridges the two languages through word-byword matching. Such requirements and assumptions are infeasible for most languages, especially for languages with large linguistic distances, e.g., English and Chinese. In this work, we propose a Multilingual Language Model with deep semantic Alignment (MLMA) to generate language-independent representations for cross-lingual sequence labeling. Our methods require only monolingual corpora with no bilingual resources at all and take advantage of deep contextualized representations. Experimental results show that our approach achieves new state-of-the-art NER and POS performance across European languages, and is also effective on distant language pairs such as English and Chinese.

</details>

<details>

<summary>2019-10-24 05:05:39 - A Hybrid Semantic Parsing Approach for Tabular Data Analysis</summary>

- *Yan Gao, Jian-Guang Lou, Dongmei Zhang*

- `1910.10363v2` - [abs](http://arxiv.org/abs/1910.10363v2) - [pdf](http://arxiv.org/pdf/1910.10363v2)

> This paper presents a novel approach to translating natural language questions to SQL queries for given tables, which meets three requirements as a real-world data analysis application: cross-domain, multilingualism and enabling quick-start. Our proposed approach consists of: (1) a novel data abstraction step before the parser to make parsing table-agnosticism; (2) a set of semantic rules for parsing abstracted data-analysis questions to intermediate logic forms as tree derivations to reduce the search space; (3) a neural-based model as a local scoring function on a span-based semantic parser for structured optimization and efficient inference. Experiments show that our approach outperforms state-of-the-art algorithms on a large open benchmark dataset WikiSQL. We also achieve promising results on a small dataset for more complex queries in both English and Chinese, which demonstrates our language expansion and quick-start ability.

</details>

<details>

<summary>2019-10-24 09:23:02 - Adversarial Feature Alignment: Avoid Catastrophic Forgetting in Incremental Task Lifelong Learning</summary>

- *Xin Yao, Tianchi Huang, Chenglei Wu, Rui-Xiao Zhang, Lifeng Sun*

- `1910.10986v1` - [abs](http://arxiv.org/abs/1910.10986v1) - [pdf](http://arxiv.org/pdf/1910.10986v1)

> Human beings are able to master a variety of knowledge and skills with ongoing learning. By contrast, dramatic performance degradation is observed when new tasks are added to an existing neural network model. This phenomenon, termed as \emph{Catastrophic Forgetting}, is one of the major roadblocks that prevent deep neural networks from achieving human-level artificial intelligence. Several research efforts, e.g. \emph{Lifelong} or \emph{Continual} learning algorithms, have been proposed to tackle this problem. However, they either suffer from an accumulating drop in performance as the task sequence grows longer, or require to store an excessive amount of model parameters for historical memory, or cannot obtain competitive performance on the new tasks. In this paper, we focus on the incremental multi-task image classification scenario. Inspired by the learning process of human students, where they usually decompose complex tasks into easier goals, we propose an adversarial feature alignment method to avoid catastrophic forgetting. In our design, both the low-level visual features and high-level semantic features serve as soft targets and guide the training process in multiple stages, which provide sufficient supervised information of the old tasks and help to reduce forgetting. Due to the knowledge distillation and regularization phenomenons, the proposed method gains even better performance than finetuning on the new tasks, which makes it stand out from other methods. Extensive experiments in several typical lifelong learning scenarios demonstrate that our method outperforms the state-of-the-art methods in both accuracies on new tasks and performance preservation on old tasks.

</details>

<details>

<summary>2019-10-24 09:41:44 - Vision-Infused Deep Audio Inpainting</summary>

- *Hang Zhou, Ziwei Liu, Xudong Xu, Ping Luo, Xiaogang Wang*

- `1910.10997v1` - [abs](http://arxiv.org/abs/1910.10997v1) - [pdf](http://arxiv.org/pdf/1910.10997v1)

> Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, \ie synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI). Code, models, dataset and video results are available at https://hangz-nju-cuhk.github.io/projects/AudioInpainting

</details>

<details>

<summary>2019-10-24 10:33:13 - Question Answering over Knowledge Graphs via Structural Query Patterns</summary>

- *Weiguo Zheng, Mei Zhang*

- `1910.09760v2` - [abs](http://arxiv.org/abs/1910.09760v2) - [pdf](http://arxiv.org/pdf/1910.09760v2)

> Natural language question answering over knowledge graphs is an important and interesting task as it enables common users to gain accurate answers in an easy and intuitive manner. However, it remains a challenge to bridge the gap between unstructured questions and structured knowledge graphs. To address the problem, a natural discipline is building a structured query to represent the input question. Searching the structured query over the knowledge graph can produce answers to the question. Distinct from the existing methods that are based on semantic parsing or templates, we propose an effective approach powered by a novel notion, structural query pattern, in this paper. Given an input question, we first generate its query sketch that is compatible with the underlying structure of the knowledge graph. Then, we complete the query graph by labeling the nodes and edges under the guidance of the structural query pattern. Finally, answers can be retrieved by executing the constructed query graph over the knowledge graph. Evaluations on three question answering benchmarks show that our proposed approach outperforms state-of-the-art methods significantly.

</details>

<details>

<summary>2019-10-24 15:05:01 - Syntax-Enhanced Self-Attention-Based Semantic Role Labeling</summary>

- *Yue Zhang, Rui Wang, Luo Si*

- `1910.11204v1` - [abs](http://arxiv.org/abs/1910.11204v1) - [pdf](http://arxiv.org/pdf/1910.11204v1)

> As a fundamental NLP task, semantic role labeling (SRL) aims to discover the semantic roles for each predicate within one sentence. This paper investigates how to incorporate syntactic knowledge into the SRL task effectively. We present different approaches of encoding the syntactic information derived from dependency trees of different quality and representations; we propose a syntax-enhanced self-attention model and compare it with other two strong baseline methods; and we conduct experiments with newly published deep contextualized word representations as well. The experiment results demonstrate that with proper incorporation of the high quality syntactic information, our model achieves a new state-of-the-art performance for the Chinese SRL task on the CoNLL-2009 dataset.

</details>

<details>

<summary>2019-10-24 17:17:10 - ÚFAL MRPipe at MRP 2019: UDPipe Goes Semantic in the Meaning Representation Parsing Shared Task</summary>

- *Milan Straka, Jana Straková*

- `1910.11295v1` - [abs](http://arxiv.org/abs/1910.11295v1) - [pdf](http://arxiv.org/pdf/1910.11295v1)

> We present a system description of our contribution to the CoNLL 2019 shared task, Cross-Framework Meaning Representation Parsing (MRP 2019). The proposed architecture is our first attempt towards a semantic parsing extension of the UDPipe 2.0, a lemmatization, POS tagging and dependency parsing pipeline.   For the MRP 2019, which features five formally and linguistically different approaches to meaning representation (DM, PSD, EDS, UCCA and AMR), we propose a uniform, language and framework agnostic graph-to-graph neural network architecture. Without any knowledge about the graph structure, and specifically without any linguistically or framework motivated features, our system implicitly models the meaning representation graphs.   After fixing a human error (we used earlier incorrect version of provided test set analyses), our submission would score third in the competition evaluation. The source code of our system is available at https://github.com/ufal/mrpipe-conll2019.

</details>

<details>

<summary>2019-10-24 17:24:43 - Identifying Unknown Instances for Autonomous Driving</summary>

- *Kelvin Wong, Shenlong Wang, Mengye Ren, Ming Liang, Raquel Urtasun*

- `1910.11296v1` - [abs](http://arxiv.org/abs/1910.11296v1) - [pdf](http://arxiv.org/pdf/1910.11296v1)

> In the past few years, we have seen great progress in perception algorithms, particular through the use of deep learning. However, most existing approaches focus on a few categories of interest, which represent only a small fraction of the potential categories that robots need to handle in the real-world. Thus, identifying objects from unknown classes remains a challenging yet crucial task. In this paper, we develop a novel open-set instance segmentation algorithm for point clouds which can segment objects from both known and unknown classes in a holistic way. Our method uses a deep convolutional neural network to project points into a category-agnostic embedding space in which they can be clustered into instances irrespective of their semantics. Experiments on two large-scale self-driving datasets validate the effectiveness of our proposed method.

</details>

<details>

<summary>2019-10-24 17:56:04 - Seeing What a GAN Cannot Generate</summary>

- *David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, Antonio Torralba*

- `1910.11626v1` - [abs](http://arxiv.org/abs/1910.11626v1) - [pdf](http://arxiv.org/pdf/1910.11626v1)

> Despite the success of Generative Adversarial Networks (GANs), mode collapse remains a serious issue during GAN training. To date, little work has focused on understanding and quantifying which modes have been dropped by a model. In this work, we visualize mode collapse at both the distribution level and the instance level. First, we deploy a semantic segmentation network to compare the distribution of segmented objects in the generated images with the target distribution in the training set. Differences in statistics reveal object classes that are omitted by a GAN. Second, given the identified omitted object classes, we visualize the GAN's omissions directly. In particular, we compare specific differences between individual photos and their approximate inversions by a GAN. To this end, we relax the problem of inversion and solve the tractable problem of inverting a GAN layer instead of the entire generator. Finally, we use this framework to analyze several recent GANs trained on multiple datasets and identify their typical failure cases.

</details>

<details>

<summary>2019-10-25 02:25:34 - L2RS: A Learning-to-Rescore Mechanism for Automatic Speech Recognition</summary>

- *Yuanfeng Song, Di Jiang, Xuefang Zhao, Qian Xu, Raymond Chi-Wing Wong, Lixin Fan, Qiang Yang*

- `1910.11496v1` - [abs](http://arxiv.org/abs/1910.11496v1) - [pdf](http://arxiv.org/pdf/1910.11496v1)

> Modern Automatic Speech Recognition (ASR) systems primarily rely on scores from an Acoustic Model (AM) and a Language Model (LM) to rescore the N-best lists. With the abundance of recent natural language processing advances, the information utilized by current ASR for evaluating the linguistic and semantic legitimacy of the N-best hypotheses is rather limited. In this paper, we propose a novel Learning-to-Rescore (L2RS) mechanism, which is specialized for utilizing a wide range of textual information from the state-of-the-art NLP models and automatically deciding their weights to rescore the N-best lists for ASR systems. Specifically, we incorporate features including BERT sentence embedding, topic vector, and perplexity scores produced by n-gram LM, topic modeling LM, BERT LM and RNNLM to train a rescoring model. We conduct extensive experiments based on a public dataset, and experimental results show that L2RS outperforms not only traditional rescoring methods but also its deep neural network counterparts by a substantial improvement of 20.67% in terms of NDCG@10. L2RS paves the way for developing more effective rescoring models for ASR.

</details>

<details>

<summary>2019-10-25 09:17:20 - Hyperspherical Prototype Networks</summary>

- *Pascal Mettes, Elise van der Pol, Cees G. M. Snoek*

- `1901.10514v3` - [abs](http://arxiv.org/abs/1901.10514v3) - [pdf](http://arxiv.org/pdf/1901.10514v3)

> This paper introduces hyperspherical prototype networks, which unify classification and regression with prototypes on hyperspherical output spaces. For classification, a common approach is to define prototypes as the mean output vector over training examples per class. Here, we propose to use hyperspheres as output spaces, with class prototypes defined a priori with large margin separation. We position prototypes through data-independent optimization, with an extension to incorporate priors from class semantics. By doing so, we do not require any prototype updating, we can handle any training size, and the output dimensionality is no longer constrained to the number of classes. Furthermore, we generalize to regression, by optimizing outputs as an interpolation between two prototypes on the hypersphere. Since both tasks are now defined by the same loss function, they can be jointly trained for multi-task problems. Experimentally, we show the benefit of hyperspherical prototype networks for classification, regression, and their combination over other prototype methods, softmax cross-entropy, and mean squared error approaches.

</details>

<details>

<summary>2019-10-25 15:28:08 - Automatic Driver Identification from In-Vehicle Network Logs</summary>

- *Mina Remeli, Szilvia Lestyan, Gergely Acs, Gergely Biczok*

- `1911.09508v1` - [abs](http://arxiv.org/abs/1911.09508v1) - [pdf](http://arxiv.org/pdf/1911.09508v1)

> Data generated by cars is growing at an unprecedented scale. As cars gradually become part of the Internet of Things (IoT) ecosystem, several stakeholders discover the value of in-vehicle network logs containing the measurements of the multitude of sensors deployed within the car. This wealth of data is also expected to be exploitable by third parties for the purpose of profiling drivers in order to provide personalized, valueadded services. Although several prior works have successfully demonstrated the feasibility of driver re-identification using the in-vehicle network data captured on the vehicle's CAN (Controller Area Network) bus, they inferred the identity of the driver only from known sensor signals (such as the vehicle's speed, brake pedal position, steering wheel angle, etc.) extracted from the CAN messages. However, car manufacturers intentionally do not reveal exact signal location and semantics within CAN logs. We show that the inference of driver identity is possible even with off-the-shelf machine learning techniques without reverse-engineering the CAN protocol. We demonstrate our approach on a dataset of 33 drivers and show that a driver can be re-identified and distinguished from other drivers with an accuracy of 75-85%.

</details>

<details>

<summary>2019-10-25 17:04:17 - Sapphire: A Configurable Crypto-Processor for Post-Quantum Lattice-based Protocols</summary>

- *Utsav Banerjee, Tenzin S. Ukyab, Anantha P. Chandrakasan*

- `1910.07557v2` - [abs](http://arxiv.org/abs/1910.07557v2) - [pdf](http://arxiv.org/pdf/1910.07557v2)

> Public key cryptography protocols, such as RSA and elliptic curve cryptography, will be rendered insecure by Shor's algorithm when large-scale quantum computers are built. Cryptographers are working on quantum-resistant algorithms, and lattice-based cryptography has emerged as a prime candidate. However, high computational complexity of these algorithms makes it challenging to implement lattice-based protocols on low-power embedded devices. To address this challenge, we present Sapphire - a lattice cryptography processor with configurable parameters. Efficient sampling, with a SHA-3-based PRNG, provides two orders of magnitude energy savings; a single-port RAM-based number theoretic transform memory architecture is proposed, which provides 124k-gate area savings; while a low-power modular arithmetic unit accelerates polynomial computations. Our test chip was fabricated in TSMC 40nm low-power CMOS process, with the Sapphire cryptographic core occupying 0.28 mm2 area consisting of 106k logic gates and 40.25 KB SRAM. Sapphire can be programmed with custom instructions for polynomial arithmetic and sampling, and it is coupled with a low-power RISC-V micro-processor to demonstrate NIST Round 2 lattice-based CCA-secure key encapsulation and signature protocols Frodo, NewHope, qTESLA, CRYSTALS-Kyber and CRYSTALS-Dilithium, achieving up to an order of magnitude improvement in performance and energy-efficiency compared to state-of-the-art hardware implementations. All key building blocks of Sapphire are constant-time and secure against timing and simple power analysis side-channel attacks. We also discuss how masking-based DPA countermeasures can be implemented on the Sapphire core without any changes to the hardware.

</details>

<details>

<summary>2019-10-25 23:01:43 - Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning</summary>

- *Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, Karol Hausman*

- `1910.11956v1` - [abs](http://arxiv.org/abs/1910.11956v1) - [pdf](http://arxiv.org/pdf/1910.11956v1)

> We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two-phase approach consists of an imitation learning stage that produces goal-conditioned hierarchical policies, and a reinforcement learning phase that finetunes these policies for task performance. Our method, while not necessarily perfect at imitation learning, is very amenable to further improvement via environment interaction, allowing it to scale to challenging long-horizon tasks. We simplify the long-horizon policy learning problem by using a novel data-relabeling algorithm for learning goal-conditioned hierarchical policies, where the low-level only acts for a fixed number of steps, regardless of the goal achieved. While we rely on demonstration data to bootstrap policy learning, we do not assume access to demonstrations of every specific tasks that is being solved, and instead leverage unstructured and unsegmented demonstrations of semantically meaningful behaviors that are not only less burdensome to provide, but also can greatly facilitate further improvement using reinforcement learning. We demonstrate the effectiveness of our method on a number of multi-stage, long-horizon manipulation tasks in a challenging kitchen simulation environment. Videos are available at https://relay-policy-learning.github.io/

</details>

<details>

<summary>2019-10-26 09:04:45 - SUPER Learning: A Supervised-Unsupervised Framework for Low-Dose CT Image Reconstruction</summary>

- *Zhipeng Li, Siqi Ye, Yong Long, Saiprasad Ravishankar*

- `1910.12024v1` - [abs](http://arxiv.org/abs/1910.12024v1) - [pdf](http://arxiv.org/pdf/1910.12024v1)

> Recent years have witnessed growing interest in machine learning-based models and techniques for low-dose X-ray CT (LDCT) imaging tasks. The methods can typically be categorized into supervised learning methods and unsupervised or model-based learning methods. Supervised learning methods have recently shown success in image restoration tasks. However, they often rely on large training sets. Model-based learning methods such as dictionary or transform learning do not require large or paired training sets and often have good generalization properties, since they learn general properties of CT image sets. Recent works have shown the promising reconstruction performance of methods such as PWLS-ULTRA that rely on clustering the underlying (reconstructed) image patches into a learned union of transforms. In this paper, we propose a new Supervised-UnsuPERvised (SUPER) reconstruction framework for LDCT image reconstruction that combines the benefits of supervised learning methods and (unsupervised) transform learning-based methods such as PWLS-ULTRA that involve highly image-adaptive clustering. The SUPER model consists of several layers, each of which includes a deep network learned in a supervised manner and an unsupervised iterative method that involves image-adaptive components. The SUPER reconstruction algorithms are learned in a greedy manner from training data. The proposed SUPER learning methods dramatically outperform both the constituent supervised learning-based networks and iterative algorithms for LDCT, and use much fewer iterations in the iterative reconstruction modules.

</details>

<details>

<summary>2019-10-26 13:18:54 - Variational Student: Learning Compact and Sparser Networks in Knowledge Distillation Framework</summary>

- *Srinidhi Hegde, Ranjitha Prasad, Ramya Hebbalaguppe, Vishwajith Kumar*

- `1910.12061v1` - [abs](http://arxiv.org/abs/1910.12061v1) - [pdf](http://arxiv.org/pdf/1910.12061v1)

> The holy grail in deep neural network research is porting the memory- and computation-intensive network models on embedded platforms with a minimal compromise in model accuracy. To this end, we propose a novel approach, termed as Variational Student, where we reap the benefits of compressibility of the knowledge distillation (KD) framework, and sparsity inducing abilities of variational inference (VI) techniques. Essentially, we build a sparse student network, whose sparsity is induced by the variational parameters found via optimizing a loss function based on VI, leveraging the knowledge learnt by an accurate but complex pre-trained teacher network. Further, for sparsity enhancement, we also employ a Block Sparse Regularizer on a concatenated tensor of teacher and student network weights. We demonstrate that the marriage of KD and the VI techniques inherits compression properties from the KD framework, and enhances levels of sparsity from the VI approach, with minimal compromise in the model accuracy. We benchmark our results on LeNet MLP and VGGNet (CNN) and illustrate a memory footprint reduction of 64x and 213x on these MLP and CNN variants, respectively, without a need to retrain the teacher network. Furthermore, in the low data regime, we observed that our method outperforms state-of-the-art Bayesian techniques in terms of accuracy.

</details>

<details>

<summary>2019-10-26 15:09:46 - Addressing Failure Prediction by Learning Model Confidence</summary>

- *Charles Corbière, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, Patrick Pérez*

- `1910.04851v2` - [abs](http://arxiv.org/abs/1910.04851v2) - [pdf](http://arxiv.org/pdf/1910.04851v2)

> Assessing reliably the confidence of a deep neural network and predicting its failures is of primary importance for the practical deployment of these models. In this paper, we propose a new target criterion for model confidence, corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time, we propose to learn TCP criterion on the training set, introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures, small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods, from MCP to Bayesian uncertainty, as well as recent approaches specifically designed for failure prediction.

</details>

<details>

<summary>2019-10-26 22:07:24 - Exploiting GAN Internal Capacity for High-Quality Reconstruction of Natural Images</summary>

- *Marcos Pividori, Guillermo L. Grinblat, Lucas C. Uzal*

- `1911.05630v1` - [abs](http://arxiv.org/abs/1911.05630v1) - [pdf](http://arxiv.org/pdf/1911.05630v1)

> Generative Adversarial Networks (GAN) have demonstrated impressive results in modeling the distribution of natural images, learning latent representations that capture semantic variations in an unsupervised basis. Beyond the generation of novel samples, it is of special interest to exploit the ability of the GAN generator to model the natural image manifold and hence generate credible changes when manipulating images. However, this line of work is conditioned by the quality of the reconstruction. Until now, only inversion to the latent space has been considered, we propose to exploit the representation in intermediate layers of the generator, and we show that this leads to increased capacity. In particular, we observe that the representation after the first dense layer, present in all state-of-the-art GAN models, is expressive enough to represent natural images with high visual fidelity. It is possible to interpolate around these images obtaining a sequence of new plausible synthetic images that cannot be generated from the latent space. Finally, as an example of potential applications that arise from this inversion mechanism, we show preliminary results in exploiting the learned representation in the attention map of the generator to obtain an unsupervised segmentation of natural images.

</details>

<details>

<summary>2019-10-27 04:53:35 - SoulMate: Short-text author linking through Multi-aspect temporal-textual embedding</summary>

- *Saeed Najafipour, Saeid Hosseini, Wen Hua, Mohammad Reza Kangavari, Xiaofang Zhou*

- `1910.12180v1` - [abs](http://arxiv.org/abs/1910.12180v1) - [pdf](http://arxiv.org/pdf/1910.12180v1)

> Linking authors of short-text contents has important usages in many applications, including Named Entity Recognition (NER) and human community detection. However, certain challenges lie ahead. Firstly, the input short-text contents are noisy, ambiguous, and do not follow the grammatical rules. Secondly, traditional text mining methods fail to effectively extract concepts through words and phrases. Thirdly, the textual contents are temporally skewed, which can affect the semantic understanding by multiple time facets. Finally, using the complementary knowledge-bases makes the results biased to the content of the external database and deviates the understanding and interpretation away from the real nature of the given short text corpus. To overcome these challenges, we devise a neural network-based temporal-textual framework that generates the tightly connected author subgraphs from microblog short-text contents. Our approach, on the one hand, computes the relevance score (edge weight) between the authors through considering a portmanteau of contents and concepts, and on the other hand, employs a stack-wise graph cutting algorithm to extract the communities of the related authors. Experimental results show that compared to other knowledge-centered competitors, our multi-aspect vector space model can achieve a higher performance in linking short-text authors. Additionally, given the author linking task, the more comprehensive the dataset is, the higher the significance of the extracted concepts will be.

</details>

<details>

<summary>2019-10-27 05:04:25 - Multi-source Domain Adaptation for Semantic Segmentation</summary>

- *Sicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer*

- `1910.12181v1` - [abs](http://arxiv.org/abs/1910.12181v1) - [pdf](http://arxiv.org/pdf/1910.12181v1)

> Simulation-to-real domain adaptation for semantic segmentation has been actively studied for various applications such as autonomous driving. Existing methods mainly focus on a single-source setting, which cannot easily handle a more practical scenario of multiple sources with different distributions. In this paper, we propose to investigate multi-source domain adaptation for semantic segmentation. Specifically, we design a novel framework, termed Multi-source Adversarial Domain Aggregation Network (MADAN), which can be trained in an end-to-end manner. First, we generate an adapted domain for each source with dynamic semantic consistency while aligning at the pixel-level cycle-consistently towards the target. Second, we propose sub-domain aggregation discriminator and cross-domain cycle discriminator to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and target domain while training the segmentation network. Extensive experiments from synthetic GTA and SYNTHIA to real Cityscapes and BDDS datasets demonstrate that the proposed MADAN model outperforms state-of-the-art approaches. Our source code is released at: https://github.com/Luodian/MADAN.

</details>

<details>

<summary>2019-10-27 06:56:43 - Look-up and Adapt: A One-shot Semantic Parser</summary>

- *Zhichu Lu, Forough Arabshahi, Igor Labutov, Tom Mitchell*

- `1910.12197v1` - [abs](http://arxiv.org/abs/1910.12197v1) - [pdf](http://arxiv.org/pdf/1910.12197v1)

> Computing devices have recently become capable of interacting with their end users via natural language. However, they can only operate within a limited "supported" domain of discourse and fail drastically when faced with an out-of-domain utterance, mainly due to the limitations of their semantic parser. In this paper, we propose a semantic parser that generalizes to out-of-domain examples by learning a general strategy for parsing an unseen utterance through adapting the logical forms of seen utterances, instead of learning to generate a logical form from scratch. Our parser maintains a memory consisting of a representative subset of the seen utterances paired with their logical forms. Given an unseen utterance, our parser works by looking up a similar utterance from the memory and adapting its logical form until it fits the unseen utterance. Moreover, we present a data generation strategy for constructing utterance-logical form pairs from different domains. Our results show an improvement of up to 68.8% on one-shot parsing under two different evaluation settings compared to the baselines.

</details>

<details>

<summary>2019-10-27 17:28:00 - TreeCaps: Tree-Structured Capsule Networks for Program Source Code Processing</summary>

- *Vinoj Jayasundara, Nghi Duy Quoc Bui, Lingxiao Jiang, David Lo*

- `1910.12306v1` - [abs](http://arxiv.org/abs/1910.12306v1) - [pdf](http://arxiv.org/pdf/1910.12306v1)

> Program comprehension is a fundamental task in software development and maintenance processes. Software developers often need to understand a large amount of existing code before they can develop new features or fix bugs in existing programs. Being able to process programming language code automatically and provide summaries of code functionality accurately can significantly help developers to reduce time spent in code navigation and understanding, and thus increase productivity. Different from natural language articles, source code in programming languages often follows rigid syntactical structures and there can exist dependencies among code elements that are located far away from each other through complex control flows and data flows. Existing studies on tree-based convolutional neural networks (TBCNN) and gated graph neural networks (GGNN) are not able to capture essential semantic dependencies among code elements accurately. In this paper, we propose novel tree-based capsule networks (TreeCaps) and relevant techniques for processing program code in an automated way that encodes code syntactical structures and captures code dependencies more accurately. Based on evaluation on programs written in different programming languages, we show that our TreeCaps-based approach can outperform other approaches in classifying the functionalities of many programs.

</details>

<details>

<summary>2019-10-28 00:50:55 - What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?</summary>

- *Chenglei Si, Shuohang Wang, Min-Yen Kan, Jing Jiang*

- `1910.12391v1` - [abs](http://arxiv.org/abs/1910.12391v1) - [pdf](http://arxiv.org/pdf/1910.12391v1)

> Multiple-Choice Reading Comprehension (MCRC) requires the model to read the passage and question, and select the correct answer among the given options. Recent state-of-the-art models have achieved impressive performance on multiple MCRC datasets. However, such performance may not reflect the model's true ability of language understanding and reasoning. In this work, we adopt two approaches to investigate what BERT learns from MCRC datasets: 1) an un-readable data attack, in which we add keywords to confuse BERT, leading to a significant performance drop; and 2) an un-answerable data training, in which we train BERT on partial or shuffled input. Under un-answerable data training, BERT achieves unexpectedly high performance. Based on our experiments on the 5 key MCRC datasets - RACE, MCTest, MCScript, MCScript2.0, DREAM - we observe that 1) fine-tuned BERT mainly learns how keywords lead to correct prediction, instead of learning semantic understanding and reasoning; and 2) BERT does not need correct syntactic information to solve the task; 3) there exists artifacts in these datasets such that they can be solved even without the full context.

</details>

<details>

<summary>2019-10-28 06:27:38 - Applications of Generative Adversarial Models in Visual Search Reformulation</summary>

- *Kyle Xiao, Houdong Hu, Yan Wang*

- `1910.12460v1` - [abs](http://arxiv.org/abs/1910.12460v1) - [pdf](http://arxiv.org/pdf/1910.12460v1)

> Query reformulation is the process by which a input search query is refined by the user to match documents outside the original top-n results. On average, roughly 50% of text search queries involve some form of reformulation, and term suggestion tools are used 35% of the time when offered to users. As prevalent as text search queries are, however, such a feature has yet to be explored at scale for visual search. This is because reformulation for images presents a novel challenge to seamlessly transform visual features to match user intent within the context of a typical user session. In this paper, we present methods of semantically transforming visual queries, such as utilizing operations in the latent space of a generative adversarial model for the scenarios of fashion and product search.

</details>

<details>

<summary>2019-10-28 07:28:57 - Multi-Module System for Open Domain Chinese Question Answering over Knowledge Base</summary>

- *Yiying Yang, Xiahui He, Kaijie Zhou, Zhongyu Wei*

- `1910.12477v1` - [abs](http://arxiv.org/abs/1910.12477v1) - [pdf](http://arxiv.org/pdf/1910.12477v1)

> For the task of open domain Knowledge Based Question Answering in CCKS2019, we propose a method combining information retrieval and semantic parsing. This multi-module system extracts the topic entity and the most related relation predicate from a question and transforms it into a Sparql query statement. Our method obtained the F1 score of 70.45% on the test data.

</details>

<details>

<summary>2019-10-28 09:20:23 - Multi-sequence Cardiac MR Segmentation with Adversarial Domain Adaptation Network</summary>

- *Jiexiang Wang, Hongyu Huang, Chaoqi Chen, Wenao Ma, Yue Huang, Xinghao Ding*

- `1910.12514v1` - [abs](http://arxiv.org/abs/1910.12514v1) - [pdf](http://arxiv.org/pdf/1910.12514v1)

> Automatic and accurate segmentation of the ventricles and myocardium from multi-sequence cardiac MRI (CMR) is crucial for the diagnosis and treatment management for patients suffering from myocardial infarction (MI). However, due to the existence of domain shift among different modalities of datasets, the performance of deep neural networks drops significantly when the training and testing datasets are distinct. In this paper, we propose an unsupervised domain alignment method to explicitly alleviate the domain shifts among different modalities of CMR sequences, \emph{e.g.,} bSSFP, LGE, and T2-weighted. Our segmentation network is attention U-Net with pyramid pooling module, where multi-level feature space and output space adversarial learning are proposed to transfer discriminative domain knowledge across different datasets. Moreover, we further introduce a group-wise feature recalibration module to enforce the fine-grained semantic-level feature alignment that matching features from different networks but with the same class label. We evaluate our method on the multi-sequence cardiac MR Segmentation Challenge 2019 datasets, which contain three different modalities of MRI sequences. Extensive experimental results show that the proposed methods can obtain significant segmentation improvements compared with the baseline models.

</details>

<details>

<summary>2019-10-28 11:01:39 - Correlation Priors for Reinforcement Learning</summary>

- *Bastian Alt, Adrian Šošić, Heinz Koeppl*

- `1909.05106v2` - [abs](http://arxiv.org/abs/1909.05106v2) - [pdf](http://arxiv.org/pdf/1909.05106v2)

> Many decision-making problems naturally exhibit pronounced structures inherited from the characteristics of the underlying environment. In a Markov decision process model, for example, two distinct states can have inherently related semantics or encode resembling physical state configurations. This often implies locally correlated transition dynamics among the states. In order to complete a certain task in such environments, the operating agent usually needs to execute a series of temporally and spatially correlated actions. Though there exists a variety of approaches to capture these correlations in continuous state-action domains, a principled solution for discrete environments is missing. In this work, we present a Bayesian learning framework based on P\'olya-Gamma augmentation that enables an analogous reasoning in such cases. We demonstrate the framework on a number of common decision-making related problems, such as imitation learning, subgoal extraction, system identification and Bayesian reinforcement learning. By explicitly modeling the underlying correlation structures of these problems, the proposed approach yields superior predictive performance compared to correlation-agnostic models, even when trained on data sets that are an order of magnitude smaller in size.

</details>

<details>

<summary>2019-10-28 13:46:42 - A Comparison of Neural Network Training Methods for Text Classification</summary>

- *Anderson de Andrade*

- `1910.12674v1` - [abs](http://arxiv.org/abs/1910.12674v1) - [pdf](http://arxiv.org/pdf/1910.12674v1)

> We study the impact of neural networks in text classification. Our focus is on training deep neural networks with proper weight initialization and greedy layer-wise pretraining. Results are compared with 1-layer neural networks and Support Vector Machines. We work with a dataset of labeled messages from the Twitter microblogging service and aim to predict weather conditions. A feature extraction procedure specific for the task is proposed, which applies dimensionality reduction using Latent Semantic Analysis. Our results show that neural networks outperform Support Vector Machines with Gaussian kernels, noticing performance gains from introducing additional hidden layers with nonlinearities. The impact of using Nesterov's Accelerated Gradient in backpropagation is also studied. We conclude that deep neural networks are a reasonable approach for text classification and propose further ideas to improve performance.

</details>

<details>

<summary>2019-10-28 14:33:09 - Few-shot Video-to-Video Synthesis</summary>

- *Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, Bryan Catanzaro*

- `1910.12713v1` - [abs](http://arxiv.org/abs/1910.12713v1) - [pdf](http://arxiv.org/pdf/1910.12713v1)

> Video-to-video synthesis (vid2vid) aims at converting an input semantic video, such as videos of human poses or segmentation masks, to an output photorealistic video. While the state-of-the-art of vid2vid has advanced significantly, existing approaches share two major limitations. First, they are data-hungry. Numerous images of a target human subject or a scene are required for training. Second, a learned model has limited generalization capability. A pose-to-human vid2vid model can only synthesize poses of the single person in the training set. It does not generalize to other humans that are not in the training set. To address the limitations, we propose a few-shot vid2vid framework, which learns to synthesize videos of previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines using several large-scale video datasets including human-dancing videos, talking-head videos, and street-scene videos. The experimental results verify the effectiveness of the proposed framework in addressing the two limitations of existing vid2vid approaches.

</details>

<details>

<summary>2019-10-28 17:53:14 - Visualizing and Measuring the Geometry of BERT</summary>

- *Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, Martin Wattenberg*

- `1906.02715v2` - [abs](http://arxiv.org/abs/1906.02715v2) - [pdf](http://arxiv.org/pdf/1906.02715v2)

> Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.

</details>

<details>

<summary>2019-10-28 18:03:08 - Variational Adversarial Active Learning</summary>

- *Samarth Sinha, Sayna Ebrahimi, Trevor Darrell*

- `1904.00370v3` - [abs](http://arxiv.org/abs/1904.00370v3) - [pdf](http://arxiv.org/pdf/1904.00370v3)

> Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Unlike conventional active learning algorithms, our approach is task agnostic, i.e., it does not depend on the performance of the task for which we are trying to acquire labeled data. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled and labeled data. The mini-max game between the VAE and the adversarial network is played such that while the VAE tries to trick the adversarial network into predicting that all data points are from the labeled pool, the adversarial network learns how to discriminate between dissimilarities in the latent space. We extensively evaluate our method on various image classification and semantic segmentation benchmark datasets and establish a new state of the art on $\text{CIFAR10/100}$, $\text{Caltech-256}$, $\text{ImageNet}$, $\text{Cityscapes}$, and $\text{BDD100K}$. Our results demonstrate that our adversarial approach learns an effective low dimensional latent space in large-scale settings and provides for a computationally efficient sampling method. Our code is available at https://github.com/sinhasam/vaal.

</details>

<details>

<summary>2019-10-28 18:42:16 - Trend-responsive User Segmentation Enabling Traceable Publishing Insights. A Case Study of a Real-world Large-scale News Recommendation System</summary>

- *Joanna Misztal-Radecka, Dominik Rusiecki, Michał Żmuda, Artur Bujak*

- `1911.11070v1` - [abs](http://arxiv.org/abs/1911.11070v1) - [pdf](http://arxiv.org/pdf/1911.11070v1)

> The traditional offline approaches are no longer sufficient for building modern recommender systems in domains such as online news services, mainly due to the high dynamics of environment changes and necessity to operate on a large scale with high data sparsity. The ability to balance exploration with exploitation makes the multi-armed bandits an efficient alternative to the conventional methods, and a robust user segmentation plays a crucial role in providing the context for such online recommendation algorithms. In this work, we present an unsupervised and trend-responsive method for segmenting users according to their semantic interests, which has been integrated with a real-world system for large-scale news recommendations. The results of an online A/B test show significant improvements compared to a global-optimization algorithm on several services with different characteristics. Based on the experimental results as well as the exploration of segments descriptions and trend dynamics, we propose extensions to this approach that address particular real-world challenges for different use-cases. Moreover, we describe a method of generating traceable publishing insights facilitating the creation of content that serves the diversity of all users needs.

</details>

<details>

<summary>2019-10-29 02:14:14 - Subspace Detours: Building Transport Plans that are Optimal on Subspace Projections</summary>

- *Boris Muzellec, Marco Cuturi*

- `1905.10099v4` - [abs](http://arxiv.org/abs/1905.10099v4) - [pdf](http://arxiv.org/pdf/1905.10099v4)

> Computing optimal transport (OT) between measures in high dimensions is doomed by the curse of dimensionality. A popular approach to avoid this curse is to project input measures on lower-dimensional subspaces (1D lines in the case of sliced Wasserstein distances), solve the OT problem between these reduced measures, and settle for the Wasserstein distance between these reductions, rather than that between the original measures. This approach is however difficult to extend to the case in which one wants to compute an OT map (a Monge map) between the original measures. Since computations are carried out on lower-dimensional projections, classical map estimation techniques can only produce maps operating in these reduced dimensions. We propose in this work two methods to extrapolate, from an transport map that is optimal on a subspace, one that is nearly optimal in the entire space. We prove that the best optimal transport plan that takes such "subspace detours" is a generalization of the Knothe-Rosenblatt transport. We show that these plans can be explicitly formulated when comparing Gaussian measures (between which the Wasserstein distance is commonly referred to as the Bures or Fr\'echet distance). We provide an algorithm to select optimal subspaces given pairs of Gaussian measures, and study scenarios in which that mediating subspace can be selected using prior information. We consider applications to semantic mediation between elliptic word embeddings and domain adaptation with Gaussian mixture models.

</details>

<details>

<summary>2019-10-29 02:53:46 - PT-ResNet: Perspective Transformation-Based Residual Network for Semantic Road Image Segmentation</summary>

- *Rui Fan, Yuan Wang, Lei Qiao, Ruiwen Yao, Peng Han, Weidong Zhang, Ioannis Pitas, Ming Liu*

- `1910.13055v1` - [abs](http://arxiv.org/abs/1910.13055v1) - [pdf](http://arxiv.org/pdf/1910.13055v1)

> Semantic road region segmentation is a high-level task, which paves the way towards road scene understanding. This paper presents a residual network trained for semantic road segmentation. Firstly, we represent the projections of road disparities in the v-disparity map as a linear model, which can be estimated by optimizing the v-disparity map using dynamic programming. This linear model is then utilized to reduce the redundant information in the left and right road images. The right image is also transformed into the left perspective view, which greatly enhances the road surface similarity between the two images. Finally, the processed stereo images and their disparity maps are concatenated to create a set of 3D images, which are then utilized to train our neural network. The experimental results illustrate that our network achieves a maximum F1-measure of approximately 91.19% when analyzing the images from the KITTI road dataset.

</details>

<details>

<summary>2019-10-29 08:54:19 - CogniVal: A Framework for Cognitive Word Embedding Evaluation</summary>

- *Nora Hollenstein, Antonio de la Torre, Nicolas Langer, Ce Zhang*

- `1909.09001v2` - [abs](http://arxiv.org/abs/1909.09001v2) - [pdf](http://arxiv.org/pdf/1909.09001v2)

> An interesting method of evaluating word representations is by how much they reflect the semantic representations in the human brain. However, most, if not all, previous works only focus on small datasets and a single modality. In this paper, we present the first multi-modal framework for evaluating English word representations based on cognitive lexical semantics. Six types of word embeddings are evaluated by fitting them to 15 datasets of eye-tracking, EEG and fMRI signals recorded during language processing. To achieve a global score over all evaluation hypotheses, we apply statistical significance testing accounting for the multiple comparisons problem. This framework is easily extensible and available to include other intrinsic and extrinsic evaluation methods. We find strong correlations in the results between cognitive datasets, across recording modalities and to their performance on extrinsic NLP tasks.

</details>

<details>

<summary>2019-10-29 10:53:02 - Embedding Symbolic Knowledge into Deep Networks</summary>

- *Yaqi Xie, Ziwei Xu, Mohan S. Kankanhalli, Kuldeep S. Meel, Harold Soh*

- `1909.01161v4` - [abs](http://arxiv.org/abs/1909.01161v4) - [pdf](http://arxiv.org/pdf/1909.01161v4)

> In this work, we aim to leverage prior symbolic knowledge to improve the performance of deep models. We propose a graph embedding network that projects propositional formulae (and assignments) onto a manifold via an augmented Graph Convolutional Network (GCN). To generate semantically-faithful embeddings, we develop techniques to recognize node heterogeneity, and semantic regularization that incorporate structural constraints into the embedding. Experiments show that our approach improves the performance of models trained to perform entailment checking and visual relation prediction. Interestingly, we observe a connection between the tractability of the propositional theory representation and the ease of embedding. Future exploration of this connection may elucidate the relationship between knowledge compilation and vector representation learning.

</details>

<details>

<summary>2019-10-29 19:44:17 - Quantifying the Semantic Core of Gender Systems</summary>

- *Adina Williams, Ryan Cotterell, Lawrence Wolf-Sonkin, Damián Blasi, Hanna Wallach*

- `1910.13497v1` - [abs](http://arxiv.org/abs/1910.13497v1) - [pdf](http://arxiv.org/pdf/1910.13497v1)

> Many of the world's languages employ grammatical gender on the lexeme. For example, in Spanish, the word for 'house' (casa) is feminine, whereas the word for 'paper' (papel) is masculine. To a speaker of a genderless language, this assignment seems to exist with neither rhyme nor reason. But is the assignment of inanimate nouns to grammatical genders truly arbitrary? We present the first large-scale investigation of the arbitrariness of noun-gender assignments. To that end, we use canonical correlation analysis to correlate the grammatical gender of inanimate nouns with an externally grounded definition of their lexical semantics. We find that 18 languages exhibit a significant correlation between grammatical gender and lexical semantics.

</details>

<details>

<summary>2019-10-29 22:12:44 - A Heuristically Modified FP-Tree for Ontology Learning with Applications in Education</summary>

- *Safwan Shatnawi, Mohamed Medhat Gaber, Mihaela Cocea*

- `1910.13561v1` - [abs](http://arxiv.org/abs/1910.13561v1) - [pdf](http://arxiv.org/pdf/1910.13561v1)

> We propose a heuristically modified FP-Tree for ontology learning from text. Unlike previous research, for concept extraction, we use a regular expression parser approach widely adopted in compiler construction, i.e., deterministic finite automata (DFA). Thus, the concepts are extracted from unstructured documents. For ontology learning, we use a frequent pattern mining approach and employ a rule mining heuristic function to enhance its quality. This process does not rely on predefined lexico-syntactic patterns, thus, it is applicable for different subjects. We employ the ontology in a question-answering system for students' content-related questions. For validation, we used textbook questions/answers and questions from online course forums. Subject experts rated the quality of the system's answers on a subset of questions and their ratings were used to identify the most appropriate automatic semantic text similarity metric to use as a validation metric for all answers. The Latent Semantic Analysis was identified as the closest to the experts' ratings. We compared the use of our ontology with the use of Text2Onto for the question-answering system and found that with our ontology 80% of the questions were answered, while with Text2Onto only 28.4% were answered, thanks to the finer grained hierarchy our approach is able to produce.

</details>

<details>

<summary>2019-10-30 02:52:51 - A framework for verifying deadlock and nondeterminism in UML activity diagrams based on CSP</summary>

- *Lucas Lima, Amaury Tavares, Sidney C. Nogueira*

- `1910.13638v1` - [abs](http://arxiv.org/abs/1910.13638v1) - [pdf](http://arxiv.org/pdf/1910.13638v1)

> Deadlock and nondeterminism may become increasingly hard to detect in concurrent and distributed systems. UML activity diagrams are flowcharts that model sequential and concurrent behavior. Although the UML community widely adopts such diagrams, there is no standard approach to verify the presence of deadlock and nondeterministic behavior in activity diagrams. Nondeterminism is usually neglected in the literature even though it may be considered a very relevant property. This work proposes a framework for the automatic verification of deadlock and nondeterminism in UML activity diagrams. It introduces a compositional CSP semantics for activity diagrams that is used to automatically generate CSP specifications from UML models. These specifications are the input for the automatic verification of deadlock and nondeterministic behavior using the FDR refinement checker. We propose a plugin for the Astah modeling environment that mechanizes the translation process, and that calls FDR in the background to perform the verification of properties. The tool keeps the traceability between a diagram and its CSP specification. It parses the FDR results to highlight the diagram paths that lead to a deadlock or a nondeterministic behavior. This framework adds verification capabilities to the UML modeling tool and keeps the formal semantics transparent to the users. Therefore, the user does not need to understand or manipulate formal notations during modeling. We present the results of a case study that applies the proposed framework for the verification of models in the domain of cloud computing. We discuss future applications due to the potential of our approach.

</details>

<details>

<summary>2019-10-30 05:13:33 - Multi Modal Semantic Segmentation using Synthetic Data</summary>

- *Kartik Srivastava, Akash Kumar Singh, Guruprasad M. Hegde*

- `1910.13676v1` - [abs](http://arxiv.org/abs/1910.13676v1) - [pdf](http://arxiv.org/pdf/1910.13676v1)

> Semantic understanding of scenes in three-dimensional space (3D) is a quintessential part of robotics oriented applications such as autonomous driving as it provides geometric cues such as size, orientation and true distance of separation to objects which are crucial for taking mission critical decisions. As a first step, in this work we investigate the possibility of semantically classifying different parts of a given scene in 3D by learning the underlying geometric context in addition to the texture cues BUT in the absence of labelled real-world datasets. To this end we generate a large number of synthetic scenes, their pixel-wise labels and corresponding 3D representations using CARLA software framework. We then build a deep neural network that learns underlying category specific 3D representation and texture cues from color information of the rendered synthetic scenes. Further on we apply the learned model on different real world datasets to evaluate its performance. Our preliminary investigation of results show that the neural network is able to learn the geometric context from synthetic scenes and effectively apply this knowledge to classify each point of a 3D representation of a scene in real-world.

</details>

<details>

<summary>2019-10-30 16:00:54 - Let's FACE it. Finnish Poetry Generation with Aesthetics and Framing</summary>

- *Mika Hämäläinen, Khalid Alnajjar*

- `1910.13946v1` - [abs](http://arxiv.org/abs/1910.13946v1) - [pdf](http://arxiv.org/pdf/1910.13946v1)

> We present a creative poem generator for the morphologically rich Finnish language. Our method falls into the master-apprentice paradigm, where a computationally creative genetic algorithm teaches a BRNN model to generate poetry. We model several parts of poetic aesthetics in the fitness function of the genetic algorithm, such as sonic features, semantic coherence, imagery and metaphor. Furthermore, we justify the creativity of our method based on the FACE theory on computational creativity and take additional care in evaluating our system by automatic metrics for concepts together with human evaluation for aesthetics, framing and expressions.

</details>

<details>

<summary>2019-10-30 17:10:21 - Auto-Annotation Quality Prediction for Semi-Supervised Learning with Ensembles</summary>

- *Dror Simon, Miriam Farber, Roman Goldenberg*

- `1910.13988v1` - [abs](http://arxiv.org/abs/1910.13988v1) - [pdf](http://arxiv.org/pdf/1910.13988v1)

> Auto-annotation by ensemble of models is an efficient method of learning on unlabeled data. Wrong or inaccurate annotations generated by the ensemble may lead to performance degradation of the trained model. To deal with this problem we propose filtering the auto-labeled data using a trained model that predicts the quality of the annotation from the degree of consensus between ensemble models. Using semantic segmentation as an example, we show the advantage of the proposed auto-annotation filtering over training on data contaminated with inaccurate labels.   Moreover, our experimental results show that in the case of semantic segmentation, the performance of a state-of-the-art model can be achieved by training it with only a fraction (30$\%$) of the original manually labeled data set, and replacing the rest with the auto-annotated, quality filtered labels.

</details>

<details>

<summary>2019-10-30 17:25:24 - Fine-Grained Object Detection over Scientific Document Images with Region Embeddings</summary>

- *Ankur Goswami, Joshua McGrath, Shanan Peters, Theodoros Rekatsinas*

- `1910.12462v2` - [abs](http://arxiv.org/abs/1910.12462v2) - [pdf](http://arxiv.org/pdf/1910.12462v2)

> We study the problem of object detection over scanned images of scientific documents. We consider images that contain objects of varying aspect ratios and sizes and range from coarse elements such as tables and figures to fine elements such as equations and section headers. We find that current object detectors fail to produce properly localized region proposals over such page objects. We revisit the original R-CNN model and present a method for generating fine-grained proposals over document elements. We also present a region embedding model that uses the convolutional maps of a proposal's neighbors as context to produce an embedding for each proposal. This region embedding is able to capture the semantic relationships between a target region and its surrounding context. Our end-to-end model produces an embedding for each proposal, then classifies each proposal by using a multi-head attention model that attends to the most important neighbors of a proposal. To evaluate our model, we collect and annotate a dataset of publications from heterogeneous journals. We show that our model, referred to as Attentive-RCNN, yields a 17% mAP improvement compared to standard object detection models.

</details>

<details>

<summary>2019-10-30 17:43:04 - SegSort: Segmentation by Discriminative Sorting of Segments</summary>

- *Jyh-Jing Hwang, Stella X. Yu, Jianbo Shi, Maxwell D. Collins, Tien-Ju Yang, Xiao Zhang, Liang-Chieh Chen*

- `1910.06962v2` - [abs](http://arxiv.org/abs/1910.06962v2) - [pdf](http://arxiv.org/pdf/1910.06962v2)

> Almost all existing deep learning approaches for semantic segmentation tackle this task as a pixel-wise classification problem. Yet humans understand a scene not in terms of pixels, but by decomposing it into perceptual groups and structures that are the basic building blocks of recognition. This motivates us to propose an end-to-end pixel-wise metric learning approach that mimics this process. In our approach, the optimal visual representation determines the right segmentation within individual images and associates segments with the same semantic classes across images. The core visual learning problem is therefore to maximize the similarity within segments and minimize the similarity between segments. Given a model trained this way, inference is performed consistently by extracting pixel-wise embeddings and clustering, with the semantic label determined by the majority vote of its nearest neighbors from an annotated set.   As a result, we present the SegSort, as a first attempt using deep learning for unsupervised semantic segmentation, achieving $76\%$ performance of its supervised counterpart. When supervision is available, SegSort shows consistent improvements over conventional approaches based on pixel-wise softmax training. Additionally, our approach produces more precise boundaries and consistent region predictions. The proposed SegSort further produces an interpretable result, as each choice of label can be easily understood from the retrieved nearest segments.

</details>

<details>

<summary>2019-10-30 18:00:41 - Named Entity Recognition -- Is there a glass ceiling?</summary>

- *Tomasz Stanislawek, Anna Wróblewska, Alicja Wójcicka, Daniel Ziembicki, Przemyslaw Biecek*

- `1910.02403v2` - [abs](http://arxiv.org/abs/1910.02403v2) - [pdf](http://arxiv.org/pdf/1910.02403v2)

> Recent developments in Named Entity Recognition (NER) have resulted in better and better models. However, is there a glass ceiling? Do we know which types of errors are still hard or even impossible to correct? In this paper, we present a detailed analysis of the types of errors in state-of-the-art machine learning (ML) methods. Our study reveals the weak and strong points of the Stanford, CMU, FLAIR, ELMO and BERT models, as well as their shared limitations. We also introduce new techniques for improving annotation, for training processes and for checking a model's quality and stability. Presented results are based on the CoNLL 2003 data set for the English language. A new enriched semantic annotation of errors for this data set and new diagnostic data sets are attached in the supplementary materials.

</details>

<details>

<summary>2019-10-31 02:10:04 - A Comparison of Semantic Similarity Methods for Maximum Human Interpretability</summary>

- *Pinky Sitikhu, Kritish Pahi, Pujan Thapa, Subarna Shakya*

- `1910.09129v2` - [abs](http://arxiv.org/abs/1910.09129v2) - [pdf](http://arxiv.org/pdf/1910.09129v2)

> The inclusion of semantic information in any similarity measures improves the efficiency of the similarity measure and provides human interpretable results for further analysis. The similarity calculation method that focuses on features related to the text's words only, will give less accurate results. This paper presents three different methods that not only focus on the text's words but also incorporates semantic information of texts in their feature vector and computes semantic similarities. These methods are based on corpus-based and knowledge-based methods, which are: cosine similarity using tf-idf vectors, cosine similarity using word embedding and soft cosine similarity using word embedding. Among these three, cosine similarity using tf-idf vectors performed best in finding similarities between short news texts. The similar texts given by the method are easy to interpret and can be used directly in other information retrieval applications.

</details>

<details>

<summary>2019-10-31 08:52:26 - Formal Verification of Dynamic and Stochastic Behaviors for Automotive Systems</summary>

- *Li Huang, Tian Liang, Eun-Young Kang*

- `1910.14312v1` - [abs](http://arxiv.org/abs/1910.14312v1) - [pdf](http://arxiv.org/pdf/1910.14312v1)

> Formal analysis of functional and non-functional requirements is crucial in automotive systems. The behaviors of those systems often rely on complex dynamics as well as on stochastic behaviors. We have proposed a probabilistic extension of Clock Constraint Specification Language, called PrCCSL,for specification of (non)-functional requirements and proved the correctness of requirements by mapping the semantics of the specifications into UPPAAL models. Previous work is extended in this paper by including an extension of PrCCSL, called PrCCSL*, for specification of stochastic and dynamic system behaviors, as well as complex requirements related to multiple events. To formally analyze the system behaviors/requirements specified in PrCCSL*, the PrCCSL* specifications are translated into stochastic UPPAAL models for formal verification. We implement an automatic translation tool, namely ProTL, which can also perform formal analysis on PrCCSL* specifications using UPPAAL-SMC as an analysis backend. Our approach is demonstrated on two automotive systems case studies.

</details>

<details>

<summary>2019-10-31 11:38:13 - Image-Conditioned Graph Generation for Road Network Extraction</summary>

- *Davide Belli, Thomas Kipf*

- `1910.14388v1` - [abs](http://arxiv.org/abs/1910.14388v1) - [pdf](http://arxiv.org/pdf/1910.14388v1)

> Deep generative models for graphs have shown great promise in the area of drug design, but have so far found little application beyond generating graph-structured molecules. In this work, we demonstrate a proof of concept for the challenging task of road network extraction from image data. This task can be framed as image-conditioned graph generation, for which we develop the Generative Graph Transformer (GGT), a deep autoregressive model that makes use of attention mechanisms for image conditioning and the recurrent generation of graphs. We benchmark GGT on the application of road network extraction from semantic segmentation data. For this, we introduce the Toulouse Road Network dataset, based on real-world publicly-available data. We further propose the StreetMover distance: a metric based on the Sinkhorn distance for effectively evaluating the quality of road network generation. The code and dataset are publicly available.

</details>

<details>

<summary>2019-10-31 15:51:04 - Positional Attention-based Frame Identification with BERT: A Deep Learning Approach to Target Disambiguation and Semantic Frame Selection</summary>

- *Sang-Sang Tan, Jin-Cheon Na*

- `1910.14549v1` - [abs](http://arxiv.org/abs/1910.14549v1) - [pdf](http://arxiv.org/pdf/1910.14549v1)

> Semantic parsing is the task of transforming sentences from natural language into formal representations of predicate-argument structures. Under this research area, frame-semantic parsing has attracted much interest. This parsing approach leverages the lexical information defined in FrameNet to associate marked predicates or targets with semantic frames, thereby assigning semantic roles to sentence components based on pre-specified frame elements in FrameNet. In this paper, a deep neural network architecture known as Positional Attention-based Frame Identification with BERT (PAFIBERT) is presented as a solution to the frame identification subtask in frame-semantic parsing. Although the importance of this subtask is well-established, prior research has yet to find a robust solution that works satisfactorily for both in-domain and out-of-domain data. This study thus set out to improve frame identification in light of recent advancements of language modeling and transfer learning in natural language processing. The proposed method is partially empowered by BERT, a pre-trained language model that excels at capturing contextual information in texts. By combining the language representation power of BERT with a position-based attention mechanism, PAFIBERT is able to attend to target-specific contexts in sentences for disambiguating targets and associating them with the most suitable semantic frames. Under various experimental settings, PAFIBERT outperformed existing solutions by a significant margin, achieving new state-of-the-art results for both in-domain and out-of-domain benchmark test sets.

</details>

<details>

<summary>2019-10-31 16:20:33 - Visual Appearance Based Person Retrieval in Unconstrained Environment Videos</summary>

- *Hiren Galiyawala, Mehul S Raval, Shivansh Dave*

- `1910.14565v1` - [abs](http://arxiv.org/abs/1910.14565v1) - [pdf](http://arxiv.org/pdf/1910.14565v1)

> Visual appearance-based person retrieval is a challenging problem in surveillance. It uses attributes like height, cloth color, cloth type and gender to describe a human. Such attributes are known as soft biometrics. This paper proposes person retrieval from surveillance video using height, torso cloth type, torso cloth color and gender. The approach introduces an adaptive torso patch extraction and bounding box regression to improve the retrieval. The algorithm uses fine-tuned Mask R-CNN and DenseNet-169 for person detection and attribute classification respectively. The performance is analyzed on AVSS 2018 challenge II dataset and it achieves 11.35% improvement over state-of-the-art based on average Intersection over Union measure.

</details>

<details>

<summary>2019-10-31 17:51:12 - Discrete Object Generation with Reversible Inductive Construction</summary>

- *Ari Seff, Wenda Zhou, Farhan Damani, Abigail Doyle, Ryan P. Adams*

- `1907.08268v2` - [abs](http://arxiv.org/abs/1907.08268v2) - [pdf](http://arxiv.org/pdf/1907.08268v2)

> The success of generative modeling in continuous domains has led to a surge of interest in generating discrete data such as molecules, source code, and graphs. However, construction histories for these discrete objects are typically not unique and so generative models must reason about intractably large spaces in order to learn. Additionally, structured discrete domains are often characterized by strict constraints on what constitutes a valid object and generative models must respect these requirements in order to produce useful novel samples. Here, we present a generative model for discrete objects employing a Markov chain where transitions are restricted to a set of local operations that preserve validity. Building off of generative interpretations of denoising autoencoders, the Markov chain alternates between producing 1) a sequence of corrupted objects that are valid but not from the data distribution, and 2) a learned reconstruction distribution that attempts to fix the corruptions while also preserving validity. This approach constrains the generative model to only produce valid objects, requires the learner to only discover local modifications to the objects, and avoids marginalization over an unknown and potentially large space of construction histories. We evaluate the proposed approach on two highly structured discrete domains, molecules and Laman graphs, and find that it compares favorably to alternative methods at capturing distributional statistics for a host of semantically relevant metrics.

</details>

<details>

<summary>2019-10-31 19:30:54 - Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping</summary>

- *Jian Ni, Radu Florian*

- `1911.00069v1` - [abs](http://arxiv.org/abs/1911.00069v1) - [pdf](http://arxiv.org/pdf/1911.00069v1)

> Relation extraction (RE) seeks to detect and classify semantic relationships between entities, which provides useful information for many NLP applications. Since the state-of-the-art RE models require large amounts of manually annotated data and language-specific resources to achieve high accuracy, it is very challenging to transfer an RE model of a resource-rich language to a resource-poor language. In this paper, we propose a new approach for cross-lingual RE model transfer based on bilingual word embedding mapping. It projects word embeddings from a target language to a source language, so that a well-trained source-language neural network RE model can be directly applied to the target language. Experiment results show that the proposed approach achieves very good performance for a number of target languages on both in-house and open datasets, using a small bilingual dictionary with only 1K word pairs.

</details>

<details>

<summary>2019-10-31 19:50:42 - Text-to-image synthesis method evaluation based on visual patterns</summary>

- *William Lund Sommer, Alexandros Iosifidis*

- `1911.00077v1` - [abs](http://arxiv.org/abs/1911.00077v1) - [pdf](http://arxiv.org/pdf/1911.00077v1)

> A commonly used evaluation metric for text-to-image synthesis is the Inception score (IS) \cite{inceptionscore}, which has been shown to be a quality metric that correlates well with human judgment. However, IS does not reveal properties of the generated images indicating the ability of a text-to-image synthesis method to correctly convey semantics of the input text descriptions. In this paper, we introduce an evaluation metric and a visual evaluation method allowing for the simultaneous estimation of the realism, variety and semantic accuracy of generated images. The proposed method uses a pre-trained Inception network \cite{inceptionnet} to produce high dimensional representations for both real and generated images. These image representations are then visualized in a $2$-dimensional feature space defined by the t-distributed Stochastic Neighbor Embedding (t-SNE) \cite{tsne}. Visual concepts are determined by clustering the real image representations, and are subsequently used to evaluate the similarity of the generated images to the real ones by classifying them to the closest visual concept. The resulting classification accuracy is shown to be a effective gauge for the semantic accuracy of text-to-image synthesis methods.

</details>

<details>

<summary>2019-10-31 23:11:21 - Predicting the Politics of an Image Using Webly Supervised Data</summary>

- *Christopher Thomas, Adriana Kovashka*

- `1911.00147v1` - [abs](http://arxiv.org/abs/1911.00147v1) - [pdf](http://arxiv.org/pdf/1911.00147v1)

> The news media shape public opinion, and often, the visual bias they contain is evident for human observers. This bias can be inferred from how different media sources portray different subjects or topics. In this paper, we model visual political bias in contemporary media sources at scale, using webly supervised data. We collect a dataset of over one million unique images and associated news articles from left- and right-leaning news sources, and develop a method to predict the image's political leaning. This problem is particularly challenging because of the enormous intra-class visual and semantic diversity of our data. We propose a two-stage method to tackle this problem. In the first stage, the model is forced to learn relevant visual concepts that, when joined with document embeddings computed from articles paired with the images, enable the model to predict bias. In the second stage, we remove the requirement of the text domain and train a visual classifier from the features of the former model. We show this two-stage approach facilitates learning and outperforms several strong baselines. We also present extensive qualitative results demonstrating the nuances of the data.

</details>


## 2019-11

<details>

<summary>2019-11-01 00:17:40 - Weird Machines as Insecure Compilation</summary>

- *Jennifer Paykin, Eric Mertens, Mark Tullsen, Luke Maurer, Benoît Razet, Alexander Bakst, Scott Moore*

- `1911.00157v1` - [abs](http://arxiv.org/abs/1911.00157v1) - [pdf](http://arxiv.org/pdf/1911.00157v1)

> Weird machines---the computational models accessible by exploiting security vulnerabilities---arise from the difference between the model a programmer has in her head of how her program should run and the implementation that actually executes. Previous attempts to reason about or identify weird machines have viewed these models through the lens of formal computational structures such as state machines and Turing machines. But because programmers rarely think about programs in this way, it is difficult to effectively apply insights about weird machines to improve security.   We present a new view of weird machines based on techniques from programming languages theory and secure compilation. Instead of an underspecified model drawn from a programmers' head, we start with a program written in a high-level source language that enforces security properties by design. Instead of state machines to describe computation, we use the well-defined semantics of this source language and a target language, into which the source program will be compiled. Weird machines are the sets of behaviors that can be achieved by a compiled source program in the target language that cannot be achieved in the source language directly. That is, exploits are witnesses to insecure compilation.   This paper develops a framework for characterizing weird machines as insecure compilation, and illustrates the framework with examples of common exploits. We study the classes of security properties that exploits violate, the compositionality of exploits in a compiler stack, and the weird machines and mitigations that arise.

</details>

<details>

<summary>2019-11-01 04:54:54 - Read, Highlight and Summarize: A Hierarchical Neural Semantic Encoder-based Approach</summary>

- *Rajeev Bhatt Ambati, Saptarashmi Bandyopadhyay, Prasenjit Mitra*

- `1910.03177v2` - [abs](http://arxiv.org/abs/1910.03177v2) - [pdf](http://arxiv.org/pdf/1910.03177v2)

> Traditional sequence-to-sequence (seq2seq) models and other variations of the attention-mechanism such as hierarchical attention have been applied to the text summarization problem. Though there is a hierarchy in the way humans use language by forming paragraphs from sentences and sentences from words, hierarchical models have usually not worked that much better than their traditional seq2seq counterparts. This effect is mainly because either the hierarchical attention mechanisms are too sparse using hard attention or noisy using soft attention. In this paper, we propose a method based on extracting the highlights of a document; a key concept that is conveyed in a few sentences. In a typical text summarization dataset consisting of documents that are 800 tokens in length (average), capturing long-term dependencies is very important, e.g., the last sentence can be grouped with the first sentence of a document to form a summary. LSTMs (Long Short-Term Memory) proved useful for machine translation. However, they often fail to capture long-term dependencies while modeling long sequences. To address these issues, we have adapted Neural Semantic Encoders (NSE) to text summarization, a class of memory-augmented neural networks by improving its functionalities and proposed a novel hierarchical NSE that outperforms similar previous models significantly. The quality of summarization was improved by augmenting linguistic factors, namely lemma, and Part-of-Speech (PoS) tags, to each word in the dataset for improved vocabulary coverage and generalization. The hierarchical NSE model on factored dataset outperformed the state-of-the-art by nearly 4 ROUGE points. We further designed and used the first GPU-based self-critical Reinforcement Learning model.

</details>

<details>

<summary>2019-11-01 09:20:14 - Kernelized Bayesian Softmax for Text Generation</summary>

- *Ning Miao, Hao Zhou, Chengqi Zhao, Wenxian Shi, Lei Li*

- `1911.00274v1` - [abs](http://arxiv.org/abs/1911.00274v1) - [pdf](http://arxiv.org/pdf/1911.00274v1)

> Neural models for text generation require a softmax layer with proper token embeddings during the decoding phase. Most existing approaches adopt single point embedding for each token. However, a word may have multiple senses according to different context, some of which might be distinct. In this paper, we propose KerBS, a novel approach for learning better embeddings for text generation. KerBS embodies two advantages: (a) it employs a Bayesian composition of embeddings for words with multiple senses; (b) it is adaptive to semantic variances of words and robust to rare sentence context by imposing learned kernels to capture the closeness of words (senses) in the embedding space. Empirical studies show that KerBS significantly boosts the performance of several text generation tasks.

</details>

<details>

<summary>2019-11-01 12:13:45 - On the Linguistic Representational Power of Neural Machine Translation Models</summary>

- *Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, James Glass*

- `1911.00317v1` - [abs](http://arxiv.org/abs/1911.00317v1) - [pdf](http://arxiv.org/pdf/1911.00317v1)

> Despite the recent success of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. We analyze the representations learned by neural machine translation models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word-structure captured within the learned representations, an important aspect in translating morphologically-rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models learn a non-trivial amount of linguistic information. Notable findings include: i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers; (iii) Representations learned using characters are more informed about wordmorphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.

</details>

<details>

<summary>2019-11-01 14:09:44 - Air-Writing Translater: A Novel Unsupervised Domain Adaptation Method for Inertia-Trajectory Translation of In-air Handwriting</summary>

- *Songbin Xu, Yang Xue, Xin Zhang, Lianwen Jin*

- `1911.05649v1` - [abs](http://arxiv.org/abs/1911.05649v1) - [pdf](http://arxiv.org/pdf/1911.05649v1)

> As a new way of human-computer interaction, inertial sensor based in-air handwriting can provide a natural and unconstrained interaction to express more complex and richer information in 3D space. However, most of the existing in-air handwriting work is mainly focused on handwritten character recognition, which makes these work suffer from poor readability of inertial signal and lack of labeled samples. To address these two problems, we use unsupervised domain adaptation method to reconstruct the trajectory of inertial signal and generate inertial samples using online handwritten trajectories. In this paper, we propose an AirWriting Translater model to learn the bi-directional translation between trajectory domain and inertial domain in the absence of paired inertial and trajectory samples. Through semantic-level adversarial training and latent classification loss, the proposed model learns to extract domain-invariant content between inertial signal and trajectory, while preserving semantic consistency during the translation across the two domains. We carefully design the architecture, so that the proposed framework can accept inputs of arbitrary length and translate between different sampling rates. We also conduct experiments on two public datasets: 6DMG (in-air handwriting dataset) and CT (handwritten trajectory dataset), the results on the two datasets demonstrate that the proposed network successes in both Inertia-to Trajectory and Trajectory-to-Inertia translation tasks.

</details>

<details>

<summary>2019-11-01 17:24:16 - MaxSAT Evaluation 2019 -- Benchmark: Identifying Security-Critical Cyber-Physical Components in Weighted AND/OR Graphs</summary>

- *Martín Barrère, Chris Hankin, Nicolas Nicolau, Demetrios G. Eliades, Thomas Parisini*

- `1911.00516v1` - [abs](http://arxiv.org/abs/1911.00516v1) - [pdf](http://arxiv.org/pdf/1911.00516v1)

> This paper presents a MaxSAT benchmark focused on identifying critical nodes in AND/OR graphs. We use AND/OR graphs to model Industrial Control Systems (ICS) as they are able to semantically grasp intricate logical interdependencies among ICS components. However, identifying critical nodes in AND/OR graphs is an NP-complete problem. We address this problem by efficiently transforming the input AND/OR graph-based model into a weighted logical formula that is then used to build and solve a Weighted Partial MaxSAT problem. The benchmark includes 80 cases with AND/OR graphs of different size and composition as well as the optimal cost and solution for each case.

</details>

<details>

<summary>2019-11-01 21:49:49 - SHACL Constraints with Inference Rules</summary>

- *Paolo Pareti, George Konstantinidis, Timothy J. Norman, Murat Şensoy*

- `1911.00598v1` - [abs](http://arxiv.org/abs/1911.00598v1) - [pdf](http://arxiv.org/pdf/1911.00598v1)

> The Shapes Constraint Language (SHACL) has been recently introduced as a W3C recommendation to define constraints that can be validated against RDF graphs. Interactions of SHACL with other Semantic Web technologies, such as ontologies or reasoners, is a matter of ongoing research. In this paper we study the interaction of a subset of SHACL with inference rules expressed in datalog. On the one hand, SHACL constraints can be used to define a "schema" for graph datasets. On the other hand, inference rules can lead to the discovery of new facts that do not match the original schema. Given a set of SHACL constraints and a set of datalog rules, we present a method to detect which constraints could be violated by the application of the inference rules on some graph instance of the schema, and update the original schema, i.e, the set of SHACL constraints, in order to capture the new facts that can be inferred. We provide theoretical and experimental results of the various components of our approach.

</details>

<details>

<summary>2019-11-01 22:07:08 - Hierarchical Optimal Transport for Document Representation</summary>

- *Mikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, Justin Solomon*

- `1906.10827v2` - [abs](http://arxiv.org/abs/1906.10827v2) - [pdf](http://arxiv.org/pdf/1906.10827v2)

> The ability to measure similarity between documents enables intelligent summarization and analysis of large corpora. Past distances between documents suffer from either an inability to incorporate semantic similarities between words or from scalability issues. As an alternative, we introduce hierarchical optimal transport as a meta-distance between documents, where documents are modeled as distributions over topics, which themselves are modeled as distributions over words. We then solve an optimal transport problem on the smaller topic space to compute a similarity score. We give conditions on the topics under which this construction defines a distance, and we relate it to the word mover's distance. We evaluate our technique for k-NN classification and show better interpretability and scalability with comparable performance to current methods at a fraction of the cost.

</details>

<details>

<summary>2019-11-02 08:45:24 - Machine Translation Evaluation using Bi-directional Entailment</summary>

- *Rakesh Khobragade, Heaven Patel, Anand Namdev, Anish Mishra, Pushpak Bhattacharyya*

- `1911.00681v1` - [abs](http://arxiv.org/abs/1911.00681v1) - [pdf](http://arxiv.org/pdf/1911.00681v1)

> In this paper, we propose a new metric for Machine Translation (MT) evaluation, based on bi-directional entailment. We show that machine generated translation can be evaluated by determining paraphrasing with a reference translation provided by a human translator. We hypothesize, and show through experiments, that paraphrasing can be detected by evaluating entailment relationship in the forward and backward direction. Unlike conventional metrics, like BLEU or METEOR, our approach uses deep learning to determine the semantic similarity between candidate and reference translation for generating scores rather than relying upon simple n-gram overlap. We use BERT's pre-trained implementation of transformer networks, fine-tuned on MNLI corpus, for natural language inferencing. We apply our evaluation metric on WMT'14 and WMT'17 dataset to evaluate systems participating in the translation task and find that our metric has a better correlation with the human annotated score compared to the other traditional metrics at system level.

</details>

<details>

<summary>2019-11-02 14:32:08 - ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations</summary>

- *Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, Yonggang Wang*

- `1911.00720v1` - [abs](http://arxiv.org/abs/1911.00720v1) - [pdf](http://arxiv.org/pdf/1911.00720v1)

> The pre-training of text encoders normally processes text as a sequence of tokens corresponding to small text units, such as word pieces in English and characters in Chinese. It omits information carried by larger text granularity, and thus the encoders cannot easily adapt to certain combinations of characters. This leads to a loss of important semantic information, which is especially problematic for Chinese because the language does not have explicit word boundaries. In this paper, we propose ZEN, a BERT-based Chinese (Z) text encoder Enhanced by N-gram representations, where different combinations of characters are considered during training. As a result, potential word or phase boundaries are explicitly pre-trained and fine-tuned with the character encoder (BERT). Therefore ZEN incorporates the comprehensive information of both the character sequence and words or phrases it contains. Experimental results illustrated the effectiveness of ZEN on a series of Chinese NLP tasks. We show that ZEN, using less resource than other published encoders, can achieve state-of-the-art performance on most tasks. Moreover, it is shown that reasonable performance can be obtained when ZEN is trained on a small corpus, which is important for applying pre-training techniques to scenarios with limited data. The code and pre-trained models of ZEN are available at https://github.com/sinovation/zen.

</details>

<details>

<summary>2019-11-02 18:05:47 - Relations among different privacy notions</summary>

- *Jun Zhao*

- `1911.00761v1` - [abs](http://arxiv.org/abs/1911.00761v1) - [pdf](http://arxiv.org/pdf/1911.00761v1)

> We present a comprehensive view of the relations among several privacy notions: differential privacy (DP) [1], Bayesian differential privacy (BDP) [2], semantic privacy (SP) [3], and membership privacy (MP) [4]. The results are organized into two parts. In part one, we extend the notion of semantic privacy (SP) to Bayesian semantic privacy (BSP) and show its essential equivalence with Bayesian differential privacy (BDP) in the quantitative sense. We prove the relations between BDP, BSP, and SP as follows: $\epsilon$-BDP $\Longleftarrow$ $\big(\frac{1}{2}-\frac{1}{e^{\epsilon}+1}\big)$-BSP, and $\epsilon$-BDP $\Longrightarrow$ $(e^{2\epsilon}-1)$-BSP $\Longrightarrow$ $(e^{2\epsilon}-1)$-SP. In addition, we obtain a minor result $\epsilon$-DP $\Longleftarrow$ $\big(\frac{1}{2}-\frac{1}{e^{\epsilon}+1}\big)$-SP, which improves the result of Kasiviswanathan and Smith [3] stating $\epsilon$-DP $\Longleftarrow$ $\epsilon/6$-SP for $\epsilon \leq 1.35$. In part two, we establish the relations between BDP and MP. First, $\epsilon$-BDP $\Longrightarrow$ $\epsilon$-MP. Second, for a family of distributions that are downward scalable in the sense of Li et al. [4], it is shown that $\epsilon$-BDP $\Longleftarrow$ $\epsilon$-MP.

</details>

<details>

<summary>2019-11-03 02:24:39 - Enhanced Convolutional Neural Tangent Kernels</summary>

- *Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S. Du, Wei Hu, Ruslan Salakhutdinov, Sanjeev Arora*

- `1911.00809v1` - [abs](http://arxiv.org/abs/1911.00809v1) - [pdf](http://arxiv.org/pdf/1911.00809v1)

> Recent research shows that for training with $\ell_2$ loss, convolutional neural networks (CNNs) whose width (number of channels in convolutional layers) goes to infinity correspond to regression with respect to the CNN Gaussian Process kernel (CNN-GP) if only the last layer is trained, and correspond to regression with respect to the Convolutional Neural Tangent Kernel (CNTK) if all layers are trained. An exact algorithm to compute CNTK (Arora et al., 2019) yielded the finding that classification accuracy of CNTK on CIFAR-10 is within 6-7% of that of that of the corresponding CNN architecture (best figure being around 78%) which is interesting performance for a fixed kernel. Here we show how to significantly enhance the performance of these kernels using two ideas. (1) Modifying the kernel using a new operation called Local Average Pooling (LAP) which preserves efficient computability of the kernel and inherits the spirit of standard data augmentation using pixel shifts. Earlier papers were unable to incorporate naive data augmentation because of the quadratic training cost of kernel regression. This idea is inspired by Global Average Pooling (GAP), which we show for CNN-GP and CNTK is equivalent to full translation data augmentation. (2) Representing the input image using a pre-processing technique proposed by Coates et al. (2011), which uses a single convolutional layer composed of random image patches. On CIFAR-10, the resulting kernel, CNN-GP with LAP and horizontal flip data augmentation, achieves 89% accuracy, matching the performance of AlexNet (Krizhevsky et al., 2012). Note that this is the best such result we know of for a classifier that is not a trained neural network. Similar improvements are obtained for Fashion-MNIST.

</details>

<details>

<summary>2019-11-03 02:47:51 - Posing Fair Generalization Tasks for Natural Language Inference</summary>

- *Atticus Geiger, Ignacio Cases, Lauri Karttunen, Chris Potts*

- `1911.00811v1` - [abs](http://arxiv.org/abs/1911.00811v1) - [pdf](http://arxiv.org/pdf/1911.00811v1)

> Deep learning models for semantics are generally evaluated using naturalistic corpora. Adversarial methods, in which models are evaluated on new examples with known semantic properties, have begun to reveal that good performance at these naturalistic tasks can hide serious shortcomings. However, we should insist that these evaluations be fair -that the models are given data sufficient to support the requisite kinds of generalization. In this paper, we define and motivate a formal notion of fairness in this sense. We then apply these ideas to natural language inference by constructing very challenging but provably fair artificial datasets and showing that standard neural models fail to generalize in the required ways; only task-specific models that jointly compose the premise and hypothesis are able to achieve high performance, and even these models do not solve the task perfectly.

</details>

<details>

<summary>2019-11-03 07:11:58 - Low-dimensional Semantic Space: from Text to Word Embedding</summary>

- *Xiaolei Lu, Bin Ni*

- `1911.00845v1` - [abs](http://arxiv.org/abs/1911.00845v1) - [pdf](http://arxiv.org/pdf/1911.00845v1)

> This article focuses on the study of Word Embedding, a feature-learning technique in Natural Language Processing that maps words or phrases to low-dimensional vectors. Beginning with the linguistic theories concerning contextual similarities - "Distributional Hypothesis" and "Context of Situation", this article introduces two ways of numerical representation of text: One-hot and Distributed Representation. In addition, this article presents statistical-based Language Models(such as Co-occurrence Matrix and Singular Value Decomposition) as well as Neural Network Language Models (NNLM, such as Continuous Bag-of-Words and Skip-Gram). This article also analyzes how Word Embedding can be applied to the study of word-sense disambiguation and diachronic linguistics.

</details>

<details>

<summary>2019-11-03 07:21:41 - BERT-CNN: a Hierarchical Patent Classifier Based on a Pre-Trained Language Model</summary>

- *Xiaolei Lu, Bin Ni*

- `1911.06241v1` - [abs](http://arxiv.org/abs/1911.06241v1) - [pdf](http://arxiv.org/pdf/1911.06241v1)

> The automatic classification is a process of automatically assigning text documents to predefined categories. An accurate automatic patent classifier is crucial to patent inventors and patent examiners in terms of intellectual property protection, patent management, and patent information retrieval. We present BERT-CNN, a hierarchical patent classifier based on pre-trained language model by training the national patent application documents collected from the State Information Center, China. The experimental results show that BERT-CNN achieves 84.3% accuracy, which is far better than the two compared baseline methods, Convolutional Neural Networks and Recurrent Neural Networks. We didn't apply our model to the third and fourth hierarchical level of the International Patent Classification - "subclass" and "group".The visualization of the Attention Mechanism shows that BERT-CNN obtains new state-of-the-art results in representing vocabularies and semantics. This article demonstrates the practicality and effectiveness of BERT-CNN in the field of automatic patent classification.

</details>

<details>

<summary>2019-11-03 20:38:38 - MRNN: A Multi-Resolution Neural Network with Duplex Attention for Document Retrieval in the Context of Question Answering</summary>

- *Tolgahan Cakaloglu, Xiaowei Xu*

- `1911.00964v1` - [abs](http://arxiv.org/abs/1911.00964v1) - [pdf](http://arxiv.org/pdf/1911.00964v1)

> The primary goal of ad-hoc retrieval (document retrieval in the context of question answering) is to find relevant documents satisfied the information need posted in a natural language query. It requires a good understanding of the query and all the documents in a corpus, which is difficult because the meaning of natural language texts depends on the context, syntax,and semantics. Recently deep neural networks have been used to rank search results in response to a query. In this paper, we devise a multi-resolution neural network(MRNN) to leverage the whole hierarchy of representations for document retrieval. The proposed MRNN model is capable of deriving a representation that integrates representations of different levels of abstraction from all the layers of the learned hierarchical representation.Moreover, a duplex attention component is designed to refinethe multi-resolution representation so that an optimal contextfor matching the query and document can be determined. More specifically, the first attention mechanism determines optimal context from the learned multi-resolution representation for the query and document. The latter attention mechanism aims to fine-tune the representation so that the query and the relevant document are closer in proximity. The empirical study shows that MRNN with the duplex attention is significantly superior to existing models used for ad-hoc retrieval on benchmark datasets including SQuAD, WikiQA, QUASAR, and TrecQA.

</details>

<details>

<summary>2019-11-04 01:47:34 - Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems</summary>

- *Asma Ghandeharioun, Judy Hanwen Shen, Natasha Jaques, Craig Ferguson, Noah Jones, Agata Lapedriza, Rosalind Picard*

- `1906.09308v2` - [abs](http://arxiv.org/abs/1906.09308v2) - [pdf](http://arxiv.org/pdf/1906.09308v2)

> Building an open-domain conversational agent is a challenging problem. Current evaluation methods, mostly post-hoc judgments of static conversation, do not capture conversation quality in a realistic interactive context. In this paper, we investigate interactive human evaluation and provide evidence for its necessity; we then introduce a novel, model-agnostic, and dataset-agnostic method to approximate it. In particular, we propose a self-play scenario where the dialog system talks to itself and we calculate a combination of proxies such as sentiment and semantic coherence on the conversation trajectory. We show that this metric is capable of capturing the human-rated quality of a dialog model better than any automated metric known to-date, achieving a significant Pearson correlation (r>.7, p<.05). To investigate the strengths of this novel metric and interactive evaluation in comparison to state-of-the-art metrics and human evaluation of static conversations, we perform extended experiments with a set of models, including several that make novel improvements to recent hierarchical dialog generation architectures through sentiment and semantic knowledge distillation on the utterance level. Finally, we open-source the interactive evaluation platform we built and the dataset we collected to allow researchers to efficiently deploy and evaluate dialog models.

</details>

<details>

<summary>2019-11-04 04:27:29 - Metric Learning for Dynamic Text Classification</summary>

- *Jeremy Wohlwend, Ethan R. Elenberg, Samuel Altschul, Shawn Henry, Tao Lei*

- `1911.01026v1` - [abs](http://arxiv.org/abs/1911.01026v1) - [pdf](http://arxiv.org/pdf/1911.01026v1)

> Traditional text classifiers are limited to predicting over a fixed set of labels. However, in many real-world applications the label set is frequently changing. For example, in intent classification, new intents may be added over time while others are removed. We propose to address the problem of dynamic text classification by replacing the traditional, fixed-size output layer with a learned, semantically meaningful metric space. Here the distances between textual inputs are optimized to perform nearest-neighbor classification across overlapping label sets. Changing the label set does not involve removing parameters, but rather simply adding or removing support points in the metric space. Then the learned metric can be fine-tuned with only a few additional training examples. We demonstrate that this simple strategy is robust to changes in the label space. Furthermore, our results show that learning a non-Euclidean metric can improve performance in the low data regime, suggesting that further work on metric spaces may benefit low-resource research.

</details>

<details>

<summary>2019-11-04 13:19:19 - Compiling Arguments in an Argumentation Framework into Three-valued Logical Expressions</summary>

- *Sosuke Moriguchi, Kazuko Takahashi*

- `1911.01185v1` - [abs](http://arxiv.org/abs/1911.01185v1) - [pdf](http://arxiv.org/pdf/1911.01185v1)

> In this paper, we propose a new method for computing general allocators directly from completeness conditions. A general allocator is an abstraction of all complete labelings for an argumentation framework. Any complete labeling is obtained from a general allocator by assigning logical constants to variables. We proved the existence of the general allocators in our previous work. However, the construction requires us to enumerate all complete labelings for the framework, which makes the computation prohibitively slow. The method proposed in this paper enables us to compute general allocators without enumerating complete labelings. It also provides the solutions of local allocation that yield semantics for subsets of the framework. We demonstrate two applications of general allocators, stability, and a new concept for frameworks, termed arity. Moreover, the method, including local allocation, is applicable to broad extensions of frameworks, such as argumentation frameworks with set-attacks, bipolar argumentation frameworks, and abstract dialectical frameworks.

</details>

<details>

<summary>2019-11-04 14:35:59 - A Holistic Natural Language Generation Framework for the Semantic Web</summary>

- *Axel-Cyrille Ngonga Ngomo, Diego Moussallem, Lorenz Bühmann*

- `1911.01248v1` - [abs](http://arxiv.org/abs/1911.01248v1) - [pdf](http://arxiv.org/pdf/1911.01248v1)

> With the ever-growing generation of data for the Semantic Web comes an increasing demand for this data to be made available to non-semantic Web experts. One way of achieving this goal is to translate the languages of the Semantic Web into natural language. We present LD2NL, a framework for verbalizing the three key languages of the Semantic Web, i.e., RDF, OWL, and SPARQL. Our framework is based on a bottom-up approach to verbalization. We evaluated LD2NL in an open survey with 86 persons. Our results suggest that our framework can generate verbalizations that are close to natural languages and that can be easily understood by non-experts. Therewith, it enables non-domain experts to interpret Semantic Web data with more than 91\% of the accuracy of domain experts.

</details>

<details>

<summary>2019-11-04 15:51:52 - AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification</summary>

- *Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, Shanfeng Zhu*

- `1811.01727v3` - [abs](http://arxiv.org/abs/1811.01727v3) - [pdf](http://arxiv.org/pdf/1811.01727v3)

> Extreme multi-label text classification (XMTC) is an important problem in the era of big data, for tagging a given text with the most relevant multiple labels from an extremely large-scale label set. XMTC can be found in many applications, such as item categorization, web page tagging, and news annotation. Traditionally most methods used bag-of-words (BOW) as inputs, ignoring word context as well as deep semantic information. Recent attempts to overcome the problems of BOW by deep learning still suffer from 1) failing to capture the important subtext for each label and 2) lack of scalability against the huge number of labels. We propose a new label tree-based deep learning model for XMTC, called AttentionXML, with two unique features: 1) a multi-label attention mechanism with raw text as input, which allows to capture the most relevant part of text to each label; and 2) a shallow and wide probabilistic label tree (PLT), which allows to handle millions of labels, especially for "tail labels". We empirically compared the performance of AttentionXML with those of eight state-of-the-art methods over six benchmark datasets, including Amazon-3M with around 3 million labels. AttentionXML outperformed all competing methods under all experimental settings. Experimental results also show that AttentionXML achieved the best performance against tail labels among label tree-based methods. The code and datasets are available at http://github.com/yourh/AttentionXML .

</details>

<details>

<summary>2019-11-04 17:10:36 - Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations</summary>

- *Fenglin Liu, Yuanxin Liu, Xuancheng Ren, Xiaodong He, Xu Sun*

- `1905.06139v3` - [abs](http://arxiv.org/abs/1905.06139v3) - [pdf](http://arxiv.org/pdf/1905.06139v3)

> In vision-and-language grounding problems, fine-grained representations of the image are considered to be of paramount importance. Most of the current systems incorporate visual features and textual concepts as a sketch of an image. However, plainly inferred representations are usually undesirable in that they are composed of separate components, the relations of which are elusive. In this work, we aim at representing an image with a set of integrated visual regions and corresponding textual concepts, reflecting certain semantics. To this end, we build the Mutual Iterative Attention (MIA) module, which integrates correlated visual features and textual concepts, respectively, by aligning the two modalities. We evaluate the proposed approach on two representative vision-and-language grounding tasks, i.e., image captioning and visual question answering. In both tasks, the semantic-grounded image representations consistently boost the performance of the baseline models under all metrics across the board. The results demonstrate that our approach is effective and generalizes well to a wide range of models for image-related applications. (The code is available at https://github.com/fenglinliu98/MIA)

</details>

<details>

<summary>2019-11-04 18:04:55 - Relation Learning on Social Networks with Multi-Modal Graph Edge Variational Autoencoders</summary>

- *Carl Yang, Jieyu Zhang, Haonan Wang, Sha Li, Myungwan Kim, Matt Walker, Yiou Xiao, Jiawei Han*

- `1911.05465v1` - [abs](http://arxiv.org/abs/1911.05465v1) - [pdf](http://arxiv.org/pdf/1911.05465v1)

> While node semantics have been extensively explored in social networks, little research attention has been paid to profile edge semantics, i.e., social relations. Ideal edge semantics should not only show that two users are connected, but also why they know each other and what they share in common. However, relations in social networks are often hard to profile, due to noisy multi-modal signals and limited user-generated ground-truth labels.   In this work, we aim to develop a unified and principled framework that can profile user relations as edge semantics in social networks by integrating multi-modal signals in the presence of noisy and incomplete data. Our framework is also flexible towards limited or missing supervision. Specifically, we assume a latent distribution of multiple relations underlying each user link, and learn them with multi-modal graph edge variational autoencoders. We encode the network data with a graph convolutional network, and decode arbitrary signals with multiple reconstruction networks. Extensive experiments and case studies on two public DBLP author networks and two internal LinkedIn member networks demonstrate the superior effectiveness and efficiency of our proposed model.

</details>

<details>

<summary>2019-11-04 19:58:41 - Broad-Coverage Semantic Parsing as Transduction</summary>

- *Sheng Zhang, Xutai Ma, Kevin Duh, Benjamin Van Durme*

- `1909.02607v2` - [abs](http://arxiv.org/abs/1909.02607v2) - [pdf](http://arxiv.org/pdf/1909.02607v2)

> We unify different broad-coverage semantic parsing tasks under a transduction paradigm, and propose an attention-based neural framework that incrementally builds a meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the transducer can be effectively trained without relying on a pre-trained aligner. Experiments conducted on three separate broad-coverage semantic parsing tasks -- AMR, SDP and UCCA -- demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.

</details>

<details>

<summary>2019-11-05 01:46:52 - Deep Collaborative Discrete Hashing with Semantic-Invariant Structure</summary>

- *Zijian Wang, Zheng Zhang, Yadan Luo, Zi Huang*

- `1911.01565v1` - [abs](http://arxiv.org/abs/1911.01565v1) - [pdf](http://arxiv.org/pdf/1911.01565v1)

> Existing deep hashing approaches fail to fully explore semantic correlations and neglect the effect of linguistic context on visual attention learning, leading to inferior performance. This paper proposes a dual-stream learning framework, dubbed Deep Collaborative Discrete Hashing (DCDH), which constructs a discriminative common discrete space by collaboratively incorporating the shared and individual semantics deduced from visual features and semantic labels. Specifically, the context-aware representations are generated by employing the outer product of visual embeddings and semantic encodings. Moreover, we reconstruct the labels and introduce the focal loss to take advantage of frequent and rare concepts. The common binary code space is built on the joint learning of the visual representations attended by language, the semantic-invariant structure construction and the label distribution correction. Extensive experiments demonstrate the superiority of our method.

</details>

<details>

<summary>2019-11-05 02:44:38 - Program Synthesis and Semantic Parsing with Learned Code Idioms</summary>

- *Richard Shin, Miltiadis Allamanis, Marc Brockschmidt, Oleksandr Polozov*

- `1906.10816v4` - [abs](http://arxiv.org/abs/1906.10816v4) - [pdf](http://arxiv.org/pdf/1906.10816v4)

> Program synthesis of general-purpose source code from natural language specifications is challenging due to the need to reason about high-level patterns in the target program and low-level implementation details at the same time. In this work, we present PATOIS, a system that allows a neural program synthesizer to explicitly interleave high-level and low-level reasoning at every generation step. It accomplishes this by automatically mining common code idioms from a given corpus, incorporating them into the underlying language for neural synthesis, and training a tree-based neural synthesizer to use these idioms during code generation. We evaluate PATOIS on two complex semantic parsing datasets and show that using learned code idioms improves the synthesizer's accuracy.

</details>

<details>

<summary>2019-11-05 03:43:14 - Improving Bidirectional Decoding with Dynamic Target Semantics in Neural Machine Translation</summary>

- *Yong Shan, Yang Feng, Jinchao Zhang, Fandong Meng, Wen Zhang*

- `1911.01597v1` - [abs](http://arxiv.org/abs/1911.01597v1) - [pdf](http://arxiv.org/pdf/1911.01597v1)

> Generally, Neural Machine Translation models generate target words in a left-to-right (L2R) manner and fail to exploit any future (right) semantics information, which usually produces an unbalanced translation. Recent works attempt to utilize the right-to-left (R2L) decoder in bidirectional decoding to alleviate this problem. In this paper, we propose a novel \textbf{D}ynamic \textbf{I}nteraction \textbf{M}odule (\textbf{DIM}) to dynamically exploit target semantics from R2L translation for enhancing the L2R translation quality. Different from other bidirectional decoding approaches, DIM firstly extracts helpful target information through addressing and reading operations, then updates target semantics for tracking the interactive history. Additionally, we further introduce an \textbf{agreement regularization} term into the training objective to narrow the gap between L2R and R2L translations. Experimental results on NIST Chinese$\Rightarrow$English and WMT'16 English$\Rightarrow$Romanian translation tasks show that our system achieves significant improvements over baseline systems, which also reaches comparable results compared to the state-of-the-art Transformer model with much fewer parameters of it.

</details>

<details>

<summary>2019-11-05 04:23:20 - A Review of Automated Speech and Language Features for Assessment of Cognitive and Thought Disorders</summary>

- *Rohit Voleti, Julie M. Liss, Visar Berisha*

- `1906.01157v2` - [abs](http://arxiv.org/abs/1906.01157v2) - [pdf](http://arxiv.org/pdf/1906.01157v2)

> It is widely accepted that information derived from analyzing speech (the acoustic signal) and language production (words and sentences) serves as a useful window into the health of an individual's cognitive ability. In fact, most neuropsychological testing batteries have a component related to speech and language where clinicians elicit speech from patients for subjective evaluation across a broad set of dimensions. With advances in speech signal processing and natural language processing, there has been recent interest in developing tools to detect more subtle changes in cognitive-linguistic function. This work relies on extracting a set of features from recorded and transcribed speech for objective assessments of speech and language, early diagnosis of neurological disease, and tracking of disease after diagnosis. With an emphasis on cognitive and thought disorders, in this paper we provide a review of existing speech and language features used in this domain, discuss their clinical application, and highlight their advantages and disadvantages. Broadly speaking, the review is split into two categories: language features based on natural language processing and speech features based on speech signal processing. Within each category, we consider features that aim to measure complementary dimensions of cognitive-linguistics, including language diversity, syntactic complexity, semantic coherence, and timing. We conclude the review with a proposal of new research directions to further advance the field.

</details>

<details>

<summary>2019-11-05 13:22:05 - OntoScene, A Logic-based Scene Interpreter: Implementation and Application in the Rock Art Domain</summary>

- *Daniela Briola, Viviana Mascardi, Massimiliano Gioseffi*

- `1911.04863v1` - [abs](http://arxiv.org/abs/1911.04863v1) - [pdf](http://arxiv.org/pdf/1911.04863v1)

> We present OntoScene, a framework aimed at understanding the semantics of visual scenes starting from the semantics of their elements and the spatial relations holding between them. OntoScene exploits ontologies for representing knowledge and Prolog for specifying the interpretation rules that domain experts may adopt, and for implementing the SceneInterpreter engine. Ontologies allow the designer to formalize the domain in a reusable way, and make the system modular and interoperable with existing multiagent systems, while Prolog provides a solid basis to define complex rules of interpretation in a way that can be affordable even for people with no background in Computational Logics. The domain selected for experimenting OntoScene is that of prehistoric rock art, which provides us with a fascinating and challenging testbed. Under consideration in Theory and Practice of Logic Programming (TPLP)

</details>

<details>

<summary>2019-11-05 15:41:27 - Semantic Image Synthesis with Spatially-Adaptive Normalization</summary>

- *Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu*

- `1903.07291v2` - [abs](http://arxiv.org/abs/1903.07291v2) - [pdf](http://arxiv.org/pdf/1903.07291v2)

> We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE .

</details>

<details>

<summary>2019-11-05 20:26:04 - An Affective Situation Labeling System from Psychological Behaviors in Emotion Recognition</summary>

- *Byung Hyung Kim, Sungho Jo*

- `1911.01158v2` - [abs](http://arxiv.org/abs/1911.01158v2) - [pdf](http://arxiv.org/pdf/1911.01158v2)

> This paper presents a computational framework for providing affective labels to real-life situations, called A-Situ. We first define an affective situation, as a specific arrangement of affective entities relevant to emotion elicitation in a situation. Then, the affective situation is represented as a set of labels in the valence-arousal emotion space. Based on physiological behaviors in response to a situation, the proposed framework quantifies the expected emotion evoked by the interaction with a stimulus event. The accumulated result in a spatiotemporal situation is represented as a polynomial curve called the affective curve, which bridges the semantic gap between cognitive and affective perception in real-world situations. We show the efficacy of the curve for reliable emotion labeling in real-world experiments, respectively concerning 1) a comparison between the results from our system and existing explicit assessments for measuring emotion, 2) physiological distinctiveness in emotional states, and 3) physiological characteristics correlated to continuous labels. The efficiency of affective curves to discriminate emotional states is evaluated through subject-dependent classification performance using bicoherence features to represent discrete affective states in the valence-arousal space. Furthermore, electroencephalography-based statistical analysis revealed the physiological correlates of the affective curves.

</details>

<details>

<summary>2019-11-05 20:45:25 - Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues</summary>

- *Or Levi, Pedram Hosseini, Mona Diab, David A. Broniatowski*

- `1910.01160v2` - [abs](http://arxiv.org/abs/1910.01160v2) - [pdf](http://arxiv.org/pdf/1910.01160v2)

> The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for social media platforms. Further to the efforts of reducing exposure to misinformation on social media, purveyors of fake news have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus satire. Previous work have studied whether fake news and satire can be distinguished based on language differences. Contrary to fake news, satire stories are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with linguistic features based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between fake news and satire. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the data with current news events, to help identify a political or social message.

</details>

<details>

<summary>2019-11-06 00:58:02 - A coupled autoencoder approach for multi-modal analysis of cell types</summary>

- *Rohan Gala, Nathan Gouwens, Zizhen Yao, Agata Budzillo, Osnat Penn, Bosiljka Tasic, Gabe Murphy, Hongkui Zeng, Uygar Sümbül*

- `1911.05663v1` - [abs](http://arxiv.org/abs/1911.05663v1) - [pdf](http://arxiv.org/pdf/1911.05663v1)

> Recent developments in high throughput profiling of individual neurons have spurred data driven exploration of the idea that there exist natural groupings of neurons referred to as cell types. The promise of this idea is that the immense complexity of brain circuits can be reduced, and effectively studied by means of interactions between cell types. While clustering of neuron populations based on a particular data modality can be used to define cell types, such definitions are often inconsistent across different characterization modalities. We pose this issue of cross-modal alignment as an optimization problem and develop an approach based on coupled training of autoencoders as a framework for such analyses. We apply this framework to a Patch-seq dataset consisting of transcriptomic and electrophysiological profiles for the same set of neurons to study consistency of representations across modalities, and evaluate cross-modal data prediction ability. We explore the problem where only a subset of neurons is characterized with more than one modality, and demonstrate that representations learned by coupled autoencoders can be used to identify types sampled only by a single modality.

</details>

<details>

<summary>2019-11-06 10:55:11 - sCompile: Critical Path Identification and Analysis for Smart Contracts</summary>

- *Jialiang Chang, Bo Gao, Hao Xiao, Jun Sun, Yan Cai, Zijiang Yang*

- `1808.00624v2` - [abs](http://arxiv.org/abs/1808.00624v2) - [pdf](http://arxiv.org/pdf/1808.00624v2)

> Ethereum smart contracts are an innovation built on top of the blockchain technology, which provides a platform for automatically executing contracts in an anonymous, distributed, and trusted way. The problem is magnified by the fact that smart contracts, unlike ordinary programs, cannot be patched easily once deployed. It is important for smart contracts to be checked against potential vulnerabilities. In this work, we propose an alternative approach to automatically identify critical program paths (with multiple function calls including inter-contract function calls) in a smart contract, rank the paths according to their criticalness, discard them if they are infeasible or otherwise present them with user friendly warnings for user inspection. We identify paths which involve monetary transaction as critical paths, and prioritize those which potentially violate important properties. For scalability, symbolic execution techniques are only applied to top ranked critical paths. Our approach has been implemented in a tool called sCompile, which has been applied to 36,099 smart contracts. The experiment results show that sCompile is efficient, i.e., 5 seconds on average for one smart contract. Furthermore, we show that many known vulnerabilities can be captured if user inspects as few as 10 program paths generated by sCompile. Lastly, sCompile discovered 224 unknown vulnerabilities with a false positive rate of 15.4% before user inspection.

</details>

<details>

<summary>2019-11-06 13:23:41 - Learning to Answer by Learning to Ask: Getting the Best of GPT-2 and BERT Worlds</summary>

- *Tassilo Klein, Moin Nabi*

- `1911.02365v1` - [abs](http://arxiv.org/abs/1911.02365v1) - [pdf](http://arxiv.org/pdf/1911.02365v1)

> Automatic question generation aims at the generation of questions from a context, with the corresponding answers being sub-spans of the given passage. Whereas, most of the methods mostly rely on heuristic rules to generate questions, more recently also neural network approaches have been proposed. In this work, we propose a variant of the self-attention Transformer network architectures model to generate meaningful and diverse questions. To this end, we propose an easy to use model consisting of the conjunction of the Transformer decoder GPT-2 model with Transformer encoder BERT for the downstream task for question answering. The model is trained in an end-to-end fashion, where the language model is trained to produce a question-answer-aware input representation that facilitates to generate an answer focused question. Our result of neural question generation from text on the SQuAD 1.1 dataset suggests that our method can produce semantically correct and diverse questions. Additionally, we assessed the performance of our proposed method for the downstream task of question answering. The analysis shows that our proposed generation & answering collaboration framework relatively improves both tasks and is particularly powerful in the semi-supervised setup. The results further suggest a robust and comparably lean pipeline facilitating question generation in the small-data regime.

</details>

<details>

<summary>2019-11-06 16:48:34 - BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling</summary>

- *Lars Maaløe, Marco Fraccaro, Valentin Liévin, Ole Winther*

- `1902.02102v3` - [abs](http://arxiv.org/abs/1902.02102v3) - [pdf](http://arxiv.org/pdf/1902.02102v3)

> With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.

</details>

<details>

<summary>2019-11-06 18:05:13 - A Spoken Dialogue System for Spatial Question Answering in a Physical Blocks World</summary>

- *Georgiy Platonov, Benjamin Kane, Aaron Gindi, Lenhart K. Schubert*

- `1911.02524v1` - [abs](http://arxiv.org/abs/1911.02524v1) - [pdf](http://arxiv.org/pdf/1911.02524v1)

> The blocks world is a classic toy domain that has long been used to build and test spatial reasoning systems. Despite its relative simplicity, tackling this domain in its full complexity requires the agent to exhibit a rich set of functional capabilities, ranging from vision to natural language understanding. There is currently a resurgence of interest in solving problems in such limited domains using modern techniques. In this work we tackle spatial question answering in a holistic way, using a vision system, speech input and output mediated by an animated avatar, a dialogue system that robustly interprets spatial queries, and a constraint solver that derives answers based on 3-D spatial modeling. The contributions of this work include a semantic parser that maps spatial questions into logical forms consistent with a general approach to meaning representation, a dialog manager based on a schema representation, and a constraint solver for spatial questions that provides answers in agreement with human perception. These and other components are integrated into a multi-modal human-computer interaction pipeline.

</details>

<details>

<summary>2019-11-06 20:19:00 - Coverage-based Outlier Explanation</summary>

- *Yue Wu, Leman Akoglu, Ian Davidson*

- `1911.02617v1` - [abs](http://arxiv.org/abs/1911.02617v1) - [pdf](http://arxiv.org/pdf/1911.02617v1)

> Outlier detection is a core task in data mining with a plethora of algorithms that have enjoyed wide scale usage. Existing algorithms are primarily focused on detection, that is the identification of outliers in a given dataset. In this paper we explore the relatively under-studied problem of the outlier explanation problem. Our goal is, given a dataset that is already divided into outliers and normal instances, explain what characterizes the outliers. We explore the novel direction of a semantic explanation that a domain expert or policy maker is able to understand. We formulate this as an optimization problem to find explanations that are both interpretable and pure. Through experiments on real-world data sets, we quantitatively show that our method can efficiently generate better explanations compared with rule-based learners.

</details>

<details>

<summary>2019-11-06 23:37:27 - A Domain-Specific Language for Verifying Software Requirement Constraints</summary>

- *Marzina Vidal, Tiago Massoni, Franklin Ramalho*

- `1911.02679v1` - [abs](http://arxiv.org/abs/1911.02679v1) - [pdf](http://arxiv.org/pdf/1911.02679v1)

> Software requirement analysis can certainly benefit from prevention and early detection of failures, in particular by some kind of automatic analysis. Formal methods offer means to represent and analyze requirements with rigorous tools, avoiding ambiguities and allowing automatic verification of requirement consistency. However, formalisms often clash in the culture or lack of skills of software analysts, making them challenging to apply. In this article, we propose a Domain-Specific Language (DSL) based on Set Theory for requirement analysts. The Graphical InvaRiant Language (GIRL) can be used to specify software requirement structural invariants, with entities and their relationships. Those invariants can then have their consistency evaluated by the Alloy Analyzer, based on a mapping semantics we provide for transforming GIRL models into Alloy specifications with no user intervention. With a prototypical language editor and transformations implemented into an Eclipse plugin, we carried out a qualitative study with requirement analysts working for a government software company in Brazil, to evaluate usability and effectiveness of the GIRL-based analysis of real software requirements. The participants were able to effectively use the underlying formal analysis, since 79 out of 80 assigned invariants were correctly modeled. While participants perceived as low the complexity of learning and using GIRL's simplest, set-based structures and relationships, the most complex logical structures, such as quantification and implication, were challenging. Furthermore, almost all post-study evaluations from the participants were positive, especially as a tool for discovering requirement inconsistencies.

</details>

<details>

<summary>2019-11-07 03:13:26 - SubCharacter Chinese-English Neural Machine Translation with Wubi encoding</summary>

- *Wei Zhang, Feifei Lin, Xiaodong Wang, Zhenshuang Liang, Zhen Huang*

- `1911.02737v1` - [abs](http://arxiv.org/abs/1911.02737v1) - [pdf](http://arxiv.org/pdf/1911.02737v1)

> Neural machine translation (NMT) is one of the best methods for understanding the differences in semantic rules between two languages. Especially for Indo-European languages, subword-level models have achieved impressive results. However, when the translation task involves Chinese, semantic granularity remains at the word and character level, so there is still need more fine-grained translation model of Chinese. In this paper, we introduce a simple and effective method for Chinese translation at the sub-character level. Our approach uses the Wubi method to translate Chinese into English; byte-pair encoding (BPE) is then applied. Our method for Chinese-English translation eliminates the need for a complicated word segmentation algorithm during preprocessing. Furthermore, our method allows for sub-character-level neural translation based on recurrent neural network (RNN) architecture, without preprocessing. The empirical results show that for Chinese-English translation tasks, our sub-character-level model has a comparable BLEU score to the subword model, despite having a much smaller vocabulary. Additionally, the small vocabulary is highly advantageous for NMT model compression.

</details>

<details>

<summary>2019-11-07 04:03:07 - PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation</summary>

- *Can Qin, Haoxuan You, Lichen Wang, C. -C. Jay Kuo, Yun Fu*

- `1911.02744v1` - [abs](http://arxiv.org/abs/1911.02744v1) - [pdf](http://arxiv.org/pdf/1911.02744v1)

> Domain Adaptation (DA) approaches achieved significant improvements in a wide range of machine learning and computer vision tasks (i.e., classification, detection, and segmentation). However, as far as we are aware, there are few methods yet to achieve domain adaptation directly on 3D point cloud data. The unique challenge of point cloud data lies in its abundant spatial geometric information, and the semantics of the whole object is contributed by including regional geometric structures. Specifically, most general-purpose DA methods that struggle for global feature alignment and ignore local geometric information are not suitable for 3D domain alignment. In this paper, we propose a novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN jointly aligns the global and local features in multi-level. For local alignment, we propose Self-Adaptive (SA) node module with an adjusted receptive field to model the discriminative local structures for aligning domains. To represent hierarchically scaled features, node-attention module is further introduced to weight the relationship of SA nodes across objects and domains. For global alignment, an adversarial-training strategy is employed to learn and align global features across domains. Since there is no common evaluation benchmark for 3D point cloud DA scenario, we build a general benchmark (i.e., PointDA-10) extracted from three popular 3D object/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification fashion. Extensive experiments on PointDA-10 illustrate the superiority of our model over the state-of-the-art general-purpose DA methods.

</details>

<details>

<summary>2019-11-07 10:59:40 - Explicit Pairwise Word Interaction Modeling Improves Pretrained Transformers for English Semantic Similarity Tasks</summary>

- *Yinan Zhang, Raphael Tang, Jimmy Lin*

- `1911.02847v1` - [abs](http://arxiv.org/abs/1911.02847v1) - [pdf](http://arxiv.org/pdf/1911.02847v1)

> In English semantic similarity tasks, classic word embedding-based approaches explicitly model pairwise "interactions" between the word representations of a sentence pair. Transformer-based pretrained language models disregard this notion, instead modeling pairwise word interactions globally and implicitly through their self-attention mechanism. In this paper, we hypothesize that introducing an explicit, constrained pairwise word interaction mechanism to pretrained language models improves their effectiveness on semantic similarity tasks. We validate our hypothesis using BERT on four tasks in semantic textual similarity and answer sentence selection. We demonstrate consistent improvements in quality by adding an explicit pairwise word interaction module to BERT.

</details>

<details>

<summary>2019-11-07 12:29:44 - Grid Saliency for Context Explanations of Semantic Segmentation</summary>

- *Lukas Hoyer, Mauricio Munoz, Prateek Katiyar, Anna Khoreva, Volker Fischer*

- `1907.13054v2` - [abs](http://arxiv.org/abs/1907.13054v2) - [pdf](http://arxiv.org/pdf/1907.13054v2)

> Recently, there has been a growing interest in developing saliency methods that provide visual explanations of network predictions. Still, the usability of existing methods is limited to image classification models. To overcome this limitation, we extend the existing approaches to generate grid saliencies, which provide spatially coherent visual explanations for (pixel-level) dense prediction networks. As the proposed grid saliency allows to spatially disentangle the object and its context, we specifically explore its potential to produce context explanations for semantic segmentation networks, discovering which context most influences the class predictions inside a target object area. We investigate the effectiveness of grid saliency on a synthetic dataset with an artificially induced bias between objects and their context as well as on the real-world Cityscapes dataset using state-of-the-art segmentation networks. Our results show that grid saliency can be successfully used to provide easily interpretable context explanations and, moreover, can be employed for detecting and localizing contextual biases present in the data.

</details>

<details>

<summary>2019-11-07 13:37:38 - Efficacy of Pixel-Level OOD Detection for Semantic Segmentation</summary>

- *Matt Angus, Krzysztof Czarnecki, Rick Salay*

- `1911.02897v1` - [abs](http://arxiv.org/abs/1911.02897v1) - [pdf](http://arxiv.org/pdf/1911.02897v1)

> The detection of out of distribution samples for image classification has been widely researched. Safety critical applications, such as autonomous driving, would benefit from the ability to localise the unusual objects causing the image to be out of distribution. This paper adapts state-of-the-art methods for detecting out of distribution images for image classification to the new task of detecting out of distribution pixels, which can localise the unusual objects. It further experimentally compares the adapted methods on two new datasets derived from existing semantic segmentation datasets using PSPNet and DeeplabV3+ architectures, as well as proposing a new metric for the task. The evaluation shows that the performance ranking of the compared methods does not transfer to the new task and every method performs significantly worse than their image-level counterparts.

</details>

<details>

<summary>2019-11-07 14:06:36 - Transformation of Dense and Sparse Text Representations</summary>

- *Wenpeng Hu, Mengyu Wang, Bing Liu, Feng Ji, Haiqing Chen, Dongyan Zhao, Jinwen Ma, Rui Yan*

- `1911.02914v1` - [abs](http://arxiv.org/abs/1911.02914v1) - [pdf](http://arxiv.org/pdf/1911.02914v1)

> Sparsity is regarded as a desirable property of representations, especially in terms of explanation. However, its usage has been limited due to the gap with dense representations. Most NLP research progresses in recent years are based on dense representations. Thus the desirable property of sparsity cannot be leveraged. Inspired by Fourier Transformation, in this paper, we propose a novel Semantic Transformation method to bridge the dense and sparse spaces, which can facilitate the NLP research to shift from dense space to sparse space or to jointly use both spaces. The key idea of the proposed approach is to use a Forward Transformation to transform dense representations to sparse representations. Then some useful operations in the sparse space can be performed over the sparse representations, and the sparse representations can be used directly to perform downstream tasks such as text classification and natural language inference. Then, a Backward Transformation can also be carried out to transform those processed sparse representations to dense representations. Experiments using classification tasks and natural language inference task show that the proposed Semantic Transformation is effective.

</details>

<details>

<summary>2019-11-07 16:21:40 - SENSE: Semantically Enhanced Node Sequence Embedding</summary>

- *Swati Rallapalli, Liang Ma, Mudhakar Srivatsa, Ananthram Swami, Heesung Kwon, Graham Bent, Christopher Simpkin*

- `1911.02970v1` - [abs](http://arxiv.org/abs/1911.02970v1) - [pdf](http://arxiv.org/pdf/1911.02970v1)

> Effectively capturing graph node sequences in the form of vector embeddings is critical to many applications. We achieve this by (i) first learning vector embeddings of single graph nodes and (ii) then composing them to compactly represent node sequences. Specifically, we propose SENSE-S (Semantically Enhanced Node Sequence Embedding - for Single nodes), a skip-gram based novel embedding mechanism, for single graph nodes that co-learns graph structure as well as their textual descriptions. We demonstrate that SENSE-S vectors increase the accuracy of multi-label classification tasks by up to 50% and link-prediction tasks by up to 78% under a variety of scenarios using real datasets. Based on SENSE-S, we next propose generic SENSE to compute composite vectors that represent a sequence of nodes, where preserving the node order is important. We prove that this approach is efficient in embedding node sequences, and our experiments on real data confirm its high accuracy in node order decoding.

</details>

<details>

<summary>2019-11-08 03:45:55 - Why Do Masked Neural Language Models Still Need Common Sense Knowledge?</summary>

- *Sunjae Kwon, Cheongwoong Kang, Jiyeon Han, Jaesik Choi*

- `1911.03024v1` - [abs](http://arxiv.org/abs/1911.03024v1) - [pdf](http://arxiv.org/pdf/1911.03024v1)

> Currently, contextualized word representations are learned by intricate neural network models, such as masked neural language models (MNLMs). The new representations significantly enhanced the performance in automated question answering by reading paragraphs. However, identifying the detailed knowledge trained in the MNLMs is difficult owing to numerous and intermingled parameters. This paper provides empirical but insightful analyses on the pretrained MNLMs with respect to common sense knowledge. First, we propose a test that measures what types of common sense knowledge do pretrained MNLMs understand. From the test, we observed that MNLMs partially understand various types of common sense knowledge but do not accurately understand the semantic meaning of relations. In addition, based on the difficulty of the question-answering task problems, we observed that pretrained MLM-based models are still vulnerable to problems that require common sense knowledge. We also experimentally demonstrated that we can elevate existing MNLM-based models by combining knowledge from an external common sense repository.

</details>

<details>

<summary>2019-11-08 07:05:20 - What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning</summary>

- *Jaejun Lee, Raphael Tang, Jimmy Lin*

- `1911.03090v1` - [abs](http://arxiv.org/abs/1911.03090v1) - [pdf](http://arxiv.org/pdf/1911.03090v1)

> Pretrained transformer-based language models have achieved state of the art across countless tasks in natural language processing. These models are highly expressive, comprising at least a hundred million parameters and a dozen layers. Recent evidence suggests that only a few of the final layers need to be fine-tuned for high quality on downstream tasks. Naturally, a subsequent research question is, "how many of the last layers do we need to fine-tune?" In this paper, we precisely answer this question. We examine two recent pretrained language models, BERT and RoBERTa, across standard tasks in textual entailment, semantic similarity, sentiment analysis, and linguistic acceptability. We vary the number of final layers that are fine-tuned, then study the resulting change in task-specific effectiveness. We show that only a fourth of the final layers need to be fine-tuned to achieve 90% of the original quality. Surprisingly, we also find that fine-tuning all layers does not always help.

</details>

<details>

<summary>2019-11-08 14:33:06 - Composing and Embedding the Words-as-Classifiers Model of Grounded Semantics</summary>

- *Daniele Moro, Stacy Black, Casey Kennington*

- `1911.03283v1` - [abs](http://arxiv.org/abs/1911.03283v1) - [pdf](http://arxiv.org/pdf/1911.03283v1)

> The words-as-classifiers model of grounded lexical semantics learns a semantic fitness score between physical entities and the words that are used to denote those entities. In this paper, we explore how such a model can incrementally perform composition and how the model can be unified with a distributional representation. For the latter, we leverage the classifier coefficients as an embedding. For composition, we leverage the underlying mechanics of three different classifier types (i.e., logistic regression, decision trees, and multi-layer perceptrons) to arrive at a several systematic approaches to composition unique to each classifier including both denotational and connotational methods of composition. We compare these approaches to each other and to prior work in a visual reference resolution task using the refCOCO dataset. Our results demonstrate the need to expand upon existing composition strategies and bring together grounded and distributional representations.

</details>

<details>

<summary>2019-11-08 15:12:36 - How Language-Neutral is Multilingual BERT?</summary>

- *Jindřich Libovický, Rudolf Rosa, Alexander Fraser*

- `1911.03310v1` - [abs](http://arxiv.org/abs/1911.03310v1) - [pdf](http://arxiv.org/pdf/1911.03310v1)

> Multilingual BERT (mBERT) provides sentence representations for 104 languages, which are useful for many multi-lingual tasks. Previous work probed the cross-linguality of mBERT using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the semantic properties of mBERT. We show that mBERT representations can be split into a language-specific component and a language-neutral component, and that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment and sentence retrieval but is not yet good enough for the more difficult task of MT quality estimation. Our work presents interesting challenges which must be solved to build better language-neutral representations, particularly for tasks requiring linguistic transfer of semantics.

</details>

<details>

<summary>2019-11-08 16:54:58 - A Good Sample is Hard to Find: Noise Injection Sampling and Self-Training for Neural Language Generation Models</summary>

- *Chris Kedzie, Kathleen McKeown*

- `1911.03373v1` - [abs](http://arxiv.org/abs/1911.03373v1) - [pdf](http://arxiv.org/pdf/1911.03373v1)

> Deep neural networks (DNN) are quickly becoming the de facto standard modeling method for many natural language generation (NLG) tasks. In order for such models to truly be useful, they must be capable of correctly generating utterances for novel meaning representations (MRs) at test time. In practice, even sophisticated DNNs with various forms of semantic control frequently fail to generate utterances faithful to the input MR. In this paper, we propose an architecture agnostic self-training method to sample novel MR/text utterance pairs to augment the original training data. Remarkably, after training on the augmented data, even simple encoder-decoder models with greedy decoding are capable of generating semantically correct utterances that are as good as state-of-the-art outputs in both automatic and human evaluations of quality.

</details>

<details>

<summary>2019-11-08 16:59:17 - Investigation of Error Simulation Techniques for Learning Dialog Policies for Conversational Error Recovery</summary>

- *Maryam Fazel-Zarandi, Longshaokan Wang, Aditya Tiwari, Spyros Matsoukas*

- `1911.03378v1` - [abs](http://arxiv.org/abs/1911.03378v1) - [pdf](http://arxiv.org/pdf/1911.03378v1)

> Training dialog policies for speech-based virtual assistants requires a plethora of conversational data. The data collection phase is often expensive and time consuming due to human involvement. To address this issue, a common solution is to build user simulators for data generation. For the successful deployment of the trained policies into real world domains, it is vital that the user simulator mimics realistic conditions. In particular, speech-based assistants are heavily affected by automatic speech recognition and language understanding errors, hence the user simulator should be able to simulate similar errors. In this paper, we review the existing error simulation methods that induce errors at audio, phoneme, text, or semantic level; and conduct detailed comparisons between the audio-level and text-level methods. In the process, we improve the existing text-level method by introducing confidence score prediction and out-of-vocabulary word mapping. We also explore the impact of audio-level and text-level methods on learning a simple clarification dialog policy to recover from errors to provide insight on future improvement for both approaches.

</details>

<details>

<summary>2019-11-08 23:25:59 - PatchNet: Hierarchical Deep Learning-Based Stable Patch Identification for the Linux Kernel</summary>

- *Thong Hoang, Julia Lawall, Yuan Tian, Richard J Oentaryo, David Lo*

- `1911.03576v1` - [abs](http://arxiv.org/abs/1911.03576v1) - [pdf](http://arxiv.org/pdf/1911.03576v1)

> Linux kernel stable versions serve the needs of users who value stability of the kernel over new features. The quality of such stable versions depends on the initiative of kernel developers and maintainers to propagate bug fixing patches to the stable versions. Thus, it is desirable to consider to what extent this process can be automated. A previous approach relies on words from commit messages and a small set of manually constructed code features. This approach, however, shows only moderate accuracy. In this paper, we investigate whether deep learning can provide a more accurate solution. We propose PatchNet, a hierarchical deep learning-based approach capable of automatically extracting features from commit messages and commit code and using them to identify stable patches. PatchNet contains a deep hierarchical structure that mirrors the hierarchical and sequential structure of commit code, making it distinctive from the existing deep learning models on source code. Experiments on 82,403 recent Linux patches confirm the superiority of PatchNet against various state-of-the-art baselines, including the one recently-adopted by Linux kernel maintainers.

</details>

<details>

<summary>2019-11-09 02:49:31 - Zero-Shot Paraphrase Generation with Multilingual Language Models</summary>

- *Yinpeng Guo, Yi Liao, Xin Jiang, Qing Zhang, Yibo Zhang, Qun Liu*

- `1911.03597v1` - [abs](http://arxiv.org/abs/1911.03597v1) - [pdf](http://arxiv.org/pdf/1911.03597v1)

> Leveraging multilingual parallel texts to automatically generate paraphrases has drawn much attention as size of high-quality paraphrase corpus is limited. Round-trip translation, also known as the pivoting method, is a typical approach to this end. However, we notice that the pivoting process involves multiple machine translation models and is likely to incur semantic drift during the two-step translations. In this paper, inspired by the Transformer-based language models, we propose a simple and unified paraphrasing model, which is purely trained on multilingual parallel data and can conduct zero-shot paraphrase generation in one step. Compared with the pivoting approach, paraphrases generated by our model is more semantically similar to the input sentence. Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences. In addition, we introduce the mechanism of denoising auto-encoder (DAE) to improve diversity and robustness of the model. Experimental results show that our model surpasses the pivoting method in terms of relevance, diversity, fluency and efficiency.

</details>

<details>

<summary>2019-11-10 02:53:06 - Semi-Heterogeneous Three-Way Joint Embedding Network for Sketch-Based Image Retrieval</summary>

- *Jianjun Lei, Yuxin Song, Bo Peng, Zhanyu Ma, Ling Shao, Yi-Zhe Song*

- `1911.04470v1` - [abs](http://arxiv.org/abs/1911.04470v1) - [pdf](http://arxiv.org/pdf/1911.04470v1)

> Sketch-based image retrieval (SBIR) is a challenging task due to the large cross-domain gap between sketches and natural images. How to align abstract sketches and natural images into a common high-level semantic space remains a key problem in SBIR. In this paper, we propose a novel semi-heterogeneous three-way joint embedding network (Semi3-Net), which integrates three branches (a sketch branch, a natural image branch, and an edgemap branch) to learn more discriminative cross-domain feature representations for the SBIR task. The key insight lies with how we cultivate the mutual and subtle relationships amongst the sketches, natural images, and edgemaps. A semi-heterogeneous feature mapping is designed to extract bottom features from each domain, where the sketch and edgemap branches are shared while the natural image branch is heterogeneous to the other branches. In addition, a joint semantic embedding is introduced to embed the features from different domains into a common high-level semantic space, where all of the three branches are shared. To further capture informative features common to both natural images and the corresponding edgemaps, a co-attention model is introduced to conduct common channel-wise feature recalibration between different domains. A hybrid-loss mechanism is designed to align the three branches, where an alignment loss and a sketch-edgemap contrastive loss are presented to encourage the network to learn invariant cross-domain representations. Experimental results on two widely used category-level datasets (Sketchy and TU-Berlin Extension) demonstrate that the proposed method outperforms state-of-the-art methods.

</details>

<details>

<summary>2019-11-10 06:04:35 - Unsupervised Annotation of Phenotypic Abnormalities via Semantic Latent Representations on Electronic Health Records</summary>

- *Jingqing Zhang, Xiaoyu Zhang, Kai Sun, Xian Yang, Chengliang Dai, Yike Guo*

- `1911.03862v1` - [abs](http://arxiv.org/abs/1911.03862v1) - [pdf](http://arxiv.org/pdf/1911.03862v1)

> The extraction of phenotype information which is naturally contained in electronic health records (EHRs) has been found to be useful in various clinical informatics applications such as disease diagnosis. However, due to imprecise descriptions, lack of gold standards and the demand for efficiency, annotating phenotypic abnormalities on millions of EHR narratives is still challenging. In this work, we propose a novel unsupervised deep learning framework to annotate the phenotypic abnormalities from EHRs via semantic latent representations. The proposed framework takes the advantage of Human Phenotype Ontology (HPO), which is a knowledge base of phenotypic abnormalities, to standardize the annotation results. Experiments have been conducted on 52,722 EHRs from MIMIC-III dataset. Quantitative and qualitative analysis have shown the proposed framework achieves state-of-the-art annotation performance and computational efficiency compared with other methods.

</details>

<details>

<summary>2019-11-10 11:24:02 - Semantic Noise Matters for Neural Natural Language Generation</summary>

- *Ondřej Dušek, David M. Howcroft, Verena Rieser*

- `1911.03905v1` - [abs](http://arxiv.org/abs/1911.03905v1) - [pdf](http://arxiv.org/pdf/1911.03905v1)

> Neural natural language generation (NNLG) systems are known for their pathological outputs, i.e. generating text which is unrelated to the input specification. In this paper, we show the impact of semantic noise on state-of-the-art NNLG models which implement different semantic control mechanisms. We find that cleaned data can improve semantic correctness by up to 97%, while maintaining fluency. We also find that the most common error is omitting information, rather than hallucination.

</details>

<details>

<summary>2019-11-11 08:03:32 - Wasserstein distances for evaluating cross-lingual embeddings</summary>

- *Georgios Balikas, Ioannis Partalas*

- `1910.11005v2` - [abs](http://arxiv.org/abs/1910.11005v2) - [pdf](http://arxiv.org/pdf/1910.11005v2)

> Word embeddings are high dimensional vector representations of words that capture their semantic similarity in the vector space. There exist several algorithms for learning such embeddings both for a single language as well as for several languages jointly. In this work we propose to evaluate collections of embeddings by adapting downstream natural language tasks to the optimal transport framework. We show how the family of Wasserstein distances can be used to solve cross-lingual document retrieval and the cross-lingual document classification problems. We argue on the advantages of this approach compared to more traditional evaluation methods of embeddings like bilingual lexical induction. Our experimental results suggest that using Wasserstein distances on these problems out-performs several strong baselines and performs on par with state-of-the-art models.

</details>

<details>

<summary>2019-11-11 12:38:12 - A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music</summary>

- *Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck*

- `1803.05428v5` - [abs](http://arxiv.org/abs/1803.05428v5) - [pdf](http://arxiv.org/pdf/1803.05428v5)

> The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.

</details>

<details>

<summary>2019-11-11 14:11:21 - Diversity by Phonetics and its Application in Neural Machine Translation</summary>

- *Abdul Rafae Khan, Jia Xu*

- `1911.04292v1` - [abs](http://arxiv.org/abs/1911.04292v1) - [pdf](http://arxiv.org/pdf/1911.04292v1)

> We introduce a powerful approach for Neural Machine Translation (NMT), whereby, during training and testing, together with the input we provide its phonetic encoding and the variants of such an encoding. This way we obtain very significant improvements up to 4 BLEU points over the state-of-the-art large-scale system. The phonetic encoding is the first part of our contribution, with a second being a theory that aims to understand the reason for this improvement. Our hypothesis states that the phonetic encoding helps NMT because it encodes a procedure to emphasize the difference between semantically diverse sentences. We conduct an empirical geometric validation of our hypothesis in support of which we obtain overwhelming evidence. Subsequently, as our third contribution and based on our theory, we develop artificial mechanisms that leverage during learning the hypothesized (and verified) effect phonetics. We achieve significant and consistent improvements overall language pairs and datasets: French-English, German-English, and Chinese-English in medium task IWSLT'17 and French-English in large task WMT'18 Bio, with up to 4 BLEU points over the state-of-the-art. Moreover, our approaches are more robust than baselines when evaluated on unknown out-of-domain test sets with up to a 5 BLEU point increase.

</details>

<details>

<summary>2019-11-11 14:20:08 - Knowledge Graph Embedding for Ecotoxicological Effect Prediction</summary>

- *Erik Bryhn Myklebust, Ernesto Jimenez-Ruiz, Jiaoyan Chen, Raoul Wolf, Knut Erik Tollefsen*

- `1907.01328v3` - [abs](http://arxiv.org/abs/1907.01328v3) - [pdf](http://arxiv.org/pdf/1907.01328v3)

> Exploring the effects a chemical compound has on a species takes a considerable experimental effort. Appropriate methods for estimating and suggesting new effects can dramatically reduce the work needed to be done by a laboratory. In this paper we explore the suitability of using a knowledge graph embedding approach for ecotoxicological effect prediction. A knowledge graph has been constructed from publicly available data sets, including a species taxonomy and chemical classification and similarity. The publicly available effect data is integrated to the knowledge graph using ontology alignment techniques. Our experimental results show that the knowledge graph based approach improves the selected baselines.

</details>

<details>

<summary>2019-11-11 15:11:25 - What the Vec? Towards Probabilistically Grounded Embeddings</summary>

- *Carl Allen, Ivana Balažević, Timothy Hospedales*

- `1805.12164v3` - [abs](http://arxiv.org/abs/1805.12164v3) - [pdf](http://arxiv.org/pdf/1805.12164v3)

> Word2Vec (W2V) and GloVe are popular, fast and efficient word embedding algorithms. Their embeddings are widely used and perform well on a variety of natural language processing tasks. Moreover, W2V has recently been adopted in the field of graph embedding, where it underpins several leading algorithms. However, despite their ubiquity and relatively simple model architecture, a theoretical understanding of what the embedding parameters of W2V and GloVe learn and why that is useful in downstream tasks has been lacking. We show that different interactions between PMI vectors reflect semantic word relationships, such as similarity and paraphrasing, that are encoded in low dimensional word embeddings under a suitable projection, theoretically explaining why embeddings of W2V and GloVe work. As a consequence, we also reveal an interesting mathematical interconnection between the considered semantic relationships themselves.

</details>

<details>

<summary>2019-11-11 16:04:55 - Attending to Entities for Better Text Understanding</summary>

- *Pengxiang Cheng, Katrin Erk*

- `1911.04361v1` - [abs](http://arxiv.org/abs/1911.04361v1) - [pdf](http://arxiv.org/pdf/1911.04361v1)

> Recent progress in NLP witnessed the development of large-scale pre-trained language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al. 2017), and in a range of end tasks, such models have achieved state-of-the-art results, approaching human performance. This demonstrates the power of the stacked self-attention architecture when paired with a sufficient number of layers and a large amount of pre-training data. However, on tasks that require complex and long-distance reasoning where surface-level cues are not enough, there is still a large gap between the pre-trained models and human performance. Strubell et al. (2018) recently showed that it is possible to inject knowledge of syntactic structure into a model through supervised self-attention. We conjecture that a similar injection of semantic knowledge, in particular, coreference information, into an existing model would improve performance on such complex problems. On the LAMBADA (Paperno et al. 2016) task, we show that a model trained from scratch with coreference as auxiliary supervision for self-attention outperforms the largest GPT-2 model, setting the new state-of-the-art, while only containing a tiny fraction of parameters compared to GPT-2. We also conduct a thorough analysis of different variants of model architectures and supervision configurations, suggesting future directions on applying similar techniques to other problems.

</details>

<details>

<summary>2019-11-11 18:13:05 - Sequence-to-Set Semantic Tagging: End-to-End Multi-label Prediction using Neural Attention for Complex Query Reformulation and Automated Text Categorization</summary>

- *Manirupa Das, Juanxi Li, Eric Fosler-Lussier, Simon Lin, Soheil Moosavinasab, Steve Rust, Yungui Huang, Rajiv Ramnath*

- `1911.04427v1` - [abs](http://arxiv.org/abs/1911.04427v1) - [pdf](http://arxiv.org/pdf/1911.04427v1)

> Novel contexts may often arise in complex querying scenarios such as in evidence-based medicine (EBM) involving biomedical literature, that may not explicitly refer to entities or canonical concept forms occurring in any fact- or rule-based knowledge source such as an ontology like the UMLS. Moreover, hidden associations between candidate concepts meaningful in the current context, may not exist within a single document, but within the collection, via alternate lexical forms. Therefore, inspired by the recent success of sequence-to-sequence neural models in delivering the state-of-the-art in a wide range of NLP tasks, we develop a novel sequence-to-set framework with neural attention for learning document representations that can effect term transfer within the corpus, for semantically tagging a large collection of documents. We demonstrate that our proposed method can be effective in both a supervised multi-label classification setup for text categorization, as well as in a unique unsupervised setting with no human-annotated document labels that uses no external knowledge resources and only corpus-derived term statistics to drive the training. Further, we show that semi-supervised training using our architecture on large amounts of unlabeled data can augment performance on the text categorization task when limited labeled data is available. Our approach to generate document encodings employing our sequence-to-set models for inference of semantic tags, gives to the best of our knowledge, the state-of-the-art for both, the unsupervised query expansion task for the TREC CDS 2016 challenge dataset when evaluated on an Okapi BM25--based document retrieval system; and also over the MLTM baseline (Soleimani et al, 2016), for both supervised and semi-supervised multi-label prediction tasks on the del.icio.us and Ohsumed datasets. We will make our code and data publicly available.

</details>

<details>

<summary>2019-11-11 19:33:34 - Attention-based Pairwise Multi-Perspective Convolutional Neural Network for Answer Selection in Question Answering</summary>

- *Jamshid Mozafari, Mohammad Ali Nematbakhsh, Afsaneh Fatemi*

- `1909.01059v3` - [abs](http://arxiv.org/abs/1909.01059v3) - [pdf](http://arxiv.org/pdf/1909.01059v3)

> Over the past few years, question answering and information retrieval systems have become widely used. These systems attempt to find the answer of the asked questions from raw text sources. A component of these systems is Answer Selection which selects the most relevant from candidate answers. Syntactic similarities were mostly used to compute the similarity, but in recent works, deep neural networks have been used, making a significant improvement in this field. In this research, a model is proposed to select the most relevant answers to the factoid question from the candidate answers. The proposed model ranks the candidate answers in terms of semantic and syntactic similarity to the question, using convolutional neural networks. In this research, Attention mechanism and Sparse feature vector use the context-sensitive interactions between questions and answer sentence. Wide convolution increases the importance of the interrogative word. Pairwise ranking is used to learn differentiable representations to distinguish positive and negative answers. Our model indicates strong performance on the TrecQA Raw beating previous state-of-the-art systems by 1.4% in MAP and 1.1% in MRR while using the benefits of no additional syntactic parsers and external tools. The results show that using context-sensitive interactions between question and answer sentences can help to find the correct answer more accurately.

</details>

<details>

<summary>2019-11-12 02:49:38 - A Syntax-aware Multi-task Learning Framework for Chinese Semantic Role Labeling</summary>

- *Qingrong Xia, Zhenghua Li, Min Zhang*

- `1911.04641v1` - [abs](http://arxiv.org/abs/1911.04641v1) - [pdf](http://arxiv.org/pdf/1911.04641v1)

> Semantic role labeling (SRL) aims to identify the predicate-argument structure of a sentence. Inspired by the strong correlation between syntax and semantics, previous works pay much attention to improve SRL performance on exploiting syntactic knowledge, achieving significant results. Pipeline methods based on automatic syntactic trees and multi-task learning (MTL) approaches using standard syntactic trees are two common research orientations. In this paper, we adopt a simple unified span-based model for both span-based and word-based Chinese SRL as a strong baseline. Besides, we present a MTL framework that includes the basic SRL module and a dependency parser module. Different from the commonly used hard parameter sharing strategy in MTL, the main idea is to extract implicit syntactic representations from the dependency parser as external inputs for the basic SRL model. Experiments on the benchmarks of Chinese Proposition Bank 1.0 and CoNLL-2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax.

</details>

<details>

<summary>2019-11-12 05:50:29 - MCPA: Program Analysis as Machine Learning</summary>

- *Marcel Böhme*

- `1911.04687v1` - [abs](http://arxiv.org/abs/1911.04687v1) - [pdf](http://arxiv.org/pdf/1911.04687v1)

> Static program analysis today takes an analytical approach which is quite suitable for a well-scoped system. Data- and control-flow is taken into account. Special cases such as pointers, procedures, and undefined behavior must be handled. A program is analyzed precisely on the statement level. However, the analytical approach is ill-equiped to handle implementations of complex, large-scale, heterogeneous software systems we see in the real world. Existing static analysis techniques that scale, trade correctness (i.e., soundness or completeness) for scalability and build on strong assumptions (e.g., language-specificity). Scalable static analysis are well-known to report errors that do *not* exist (false positives) or fail to report errors that *do* exist (false negatives). Then, how do we know the degree to which the analysis outcome is correct?   In this paper, we propose an approach to scale-oblivious greybox program analysis with bounded error which applies efficient approximation schemes (FPRAS) from the foundations of machine learning: PAC learnability. Given two parameters $\delta$ and $\epsilon$, with probability at least $(1-\delta)$, our Monte Carlo Program Analysis (MCPA) approach produces an outcome that has an average error at most $\epsilon$. The parameters $\delta>0$ and $\epsilon>0$ can be chosen arbitrarily close to zero (0) such that the program analysis outcome is said to be probably-approximately correct (PAC). We demonstrate the pertinent concepts of MCPA using three applications: $(\epsilon,\delta)$-approximate quantitative analysis, $(\epsilon,\delta)$-approximate software verification, and $(\epsilon,\delta)$-approximate patch verification.

</details>

<details>

<summary>2019-11-12 06:59:38 - Learning Multi-Sense Word Distributions using Approximate Kullback-Leibler Divergence</summary>

- *P. Jayashree, Ballijepalli Shreya, P. K. Srijith*

- `1911.06118v1` - [abs](http://arxiv.org/abs/1911.06118v1) - [pdf](http://arxiv.org/pdf/1911.06118v1)

> Learning word representations has garnered greater attention in the recent past due to its diverse text applications. Word embeddings encapsulate the syntactic and semantic regularities of sentences. Modelling word embedding as multi-sense gaussian mixture distributions, will additionally capture uncertainty and polysemy of words. We propose to learn the Gaussian mixture representation of words using a Kullback-Leibler (KL) divergence based objective function. The KL divergence based energy function provides a better distance metric which can effectively capture entailment and distribution similarity among the words. Due to the intractability of KL divergence for Gaussian mixture, we go for a KL approximation between Gaussian mixtures. We perform qualitative and quantitative experiments on benchmark word similarity and entailment datasets which demonstrate the effectiveness of the proposed approach.

</details>

<details>

<summary>2019-11-12 09:41:44 - Prediction of Missing Semantic Relations in Lexical-Semantic Network using Random Forest Classifier</summary>

- *Kévin Cousot, Mehdi Mirzapour, Waleed Ragheb*

- `1911.04759v1` - [abs](http://arxiv.org/abs/1911.04759v1) - [pdf](http://arxiv.org/pdf/1911.04759v1)

> This study focuses on the prediction of missing six semantic relations (such as is_a and has_part) between two given nodes in RezoJDM a French lexical-semantic network. The output of this prediction is a set of pairs in which the first entries are semantic relations and the second entries are the probabilities of existence of such relations. Due to the statement of the problem we choose the random forest (RF) predictor classifier approach to tackle this problem. We take for granted the existing semantic relations, for training/test dataset, gathered and validated by crowdsourcing. We describe how all of the mentioned ideas can be followed after using the node2vec approach in the feature extraction phase. We show how this approach can lead to acceptable results.

</details>

<details>

<summary>2019-11-12 12:39:31 - oo7: Low-overhead Defense against Spectre Attacks via Program Analysis</summary>

- *Guanhua Wang, Sudipta Chattopadhyay, Ivan Gotovchits, Tulika Mitra, Abhik Roychoudhury*

- `1807.05843v6` - [abs](http://arxiv.org/abs/1807.05843v6) - [pdf](http://arxiv.org/pdf/1807.05843v6)

> The Spectre vulnerability in modern processors has been widely reported. The key insight in this vulnerability is that speculative execution in processors can be misused to access the secrets. Subsequently, even though the speculatively executed instructions are squashed, the secret may linger in micro-architectural states such as cache, and can potentially be accessed by an attacker via side channels. In this paper, we propose oo7, a static analysis approach that can mitigate Spectre attacks by detecting potentially vulnerable code snippets in program binaries and protecting them against the attack by patching them. Our key contribution is to balance the concerns of effectiveness, analysis time and run-time overheads. We employ control flow extraction, taint analysis, and address analysis to detect tainted conditional branches and speculative memory accesses. oo7 can detect all fifteen purpose-built Spectre-vulnerable code patterns, whereas Microsoft compiler with Spectre mitigation option can only detect two of them. We also report the results of a large-scale study on applying oo7 to over 500 program binaries (average binary size 261 KB) from different real-world projects. We protect programs against Spectre attack by selectively inserting fences only at vulnerable conditional branches to prevent speculative execution. Our approach is experimentally observed to incur around 5.9% performance overheads on SPECint benchmarks.

</details>

<details>

<summary>2019-11-12 17:11:55 - EDUQA: Educational Domain Question Answering System using Conceptual Network Mapping</summary>

- *Abhishek Agarwal, Nikhil Sachdeva, Raj Kamal Yadav, Vishaal Udandarao, Vrinda Mittal, Anubha Gupta, Abhinav Mathur*

- `1911.05013v1` - [abs](http://arxiv.org/abs/1911.05013v1) - [pdf](http://arxiv.org/pdf/1911.05013v1)

> Most of the existing question answering models can be largely compiled into two categories: i) open domain question answering models that answer generic questions and use large-scale knowledge base along with the targeted web-corpus retrieval and ii) closed domain question answering models that address focused questioning area and use complex deep learning models. Both the above models derive answers through textual comprehension methods. Due to their inability to capture the pedagogical meaning of textual content, these models are not appropriately suited to the educational field for pedagogy. In this paper, we propose an on-the-fly conceptual network model that incorporates educational semantics. The proposed model preserves correlations between conceptual entities by applying intelligent indexing algorithms on the concept network so as to improve answer generation. This model can be utilized for building interactive conversational agents for aiding classroom learning.

</details>

<details>

<summary>2019-11-12 20:56:33 - Unsupervised Medical Image Segmentation with Adversarial Networks: From Edge Diagrams to Segmentation Maps</summary>

- *Umaseh Sivanesan, Luis H. Braga, Ranil R. Sonnadara, Kiret Dhindsa*

- `1911.05140v1` - [abs](http://arxiv.org/abs/1911.05140v1) - [pdf](http://arxiv.org/pdf/1911.05140v1)

> We develop and approach to unsupervised semantic medical image segmentation that extends previous work with generative adversarial networks. We use existing edge detection methods to construct simple edge diagrams, train a generative model to convert them into synthetic medical images, and construct a dataset of synthetic images with known segmentations using variations on extracted edge diagrams. This synthetic dataset is then used to train a supervised image segmentation model. We test our approach on a clinical dataset of kidney ultrasound images and the benchmark ISIC 2018 skin lesion dataset. We show that our unsupervised approach is more accurate than previous unsupervised methods, and performs reasonably compared to supervised image segmentation models. All code and trained models are available at https://github.com/kiretd/Unsupervised-MIseg.

</details>

<details>

<summary>2019-11-13 15:31:48 - Avoiding hashing and encouraging visual semantics in referential emergent language games</summary>

- *Daniela Mihai, Jonathon Hare*

- `1911.05546v1` - [abs](http://arxiv.org/abs/1911.05546v1) - [pdf](http://arxiv.org/pdf/1911.05546v1)

> There has been an increasing interest in the area of emergent communication between agents which learn to play referential signalling games with realistic images. In this work, we consider the signalling game setting of Havrylov and Titov and investigate the effect of the feature extractor's weights and of the task being solved on the visual semantics learned or captured by the models. We impose various augmentation to the input images and additional tasks in the game with the aim to induce visual representations which capture conceptual properties of images. Through our set of experiments, we demonstrate that communication systems which capture visual semantics can be learned in a completely self-supervised manner by playing the right types of game.

</details>

<details>

<summary>2019-11-13 16:42:58 - Video Compression With Rate-Distortion Autoencoders</summary>

- *Amirhossein Habibian, Ties van Rozendaal, Jakub M. Tomczak, Taco S. Cohen*

- `1908.05717v2` - [abs](http://arxiv.org/abs/1908.05717v2) - [pdf](http://arxiv.org/pdf/1908.05717v2)

> In this paper we present a a deep generative model for lossy video compression. We employ a model that consists of a 3D autoencoder with a discrete latent space and an autoregressive prior used for entropy coding. Both autoencoder and prior are trained jointly to minimize a rate-distortion loss, which is closely related to the ELBO used in variational autoencoders. Despite its simplicity, we find that our method outperforms the state-of-the-art learned video compression networks based on motion compensation or interpolation. We systematically evaluate various design choices, such as the use of frame-based or spatio-temporal autoencoders, and the type of autoregressive prior. In addition, we present three extensions of the basic method that demonstrate the benefits over classical approaches to compression. First, we introduce semantic compression, where the model is trained to allocate more bits to objects of interest. Second, we study adaptive compression, where the model is adapted to a domain with limited variability, e.g., videos taken from an autonomous car, to achieve superior compression on that domain. Finally, we introduce multimodal compression, where we demonstrate the effectiveness of our model in joint compression of multiple modalities captured by non-standard imaging sensors, such as quad cameras. We believe that this opens up novel video compression applications, which have not been feasible with classical codecs.

</details>

<details>

<summary>2019-11-13 18:13:07 - Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text</summary>

- *Ian Porada, Kaheer Suleman, Jackie Chi Kit Cheung*

- `1911.05689v1` - [abs](http://arxiv.org/abs/1911.05689v1) - [pdf](http://arxiv.org/pdf/1911.05689v1)

> Modeling semantic plausibility requires commonsense knowledge about the world and has been used as a testbed for exploring various knowledge representations. Previous work has focused specifically on modeling physical plausibility and shown that distributional methods fail when tested in a supervised setting. At the same time, distributional models, namely large pretrained language models, have led to improved results for many natural language understanding tasks. In this work, we show that these pretrained language models are in fact effective at modeling physical plausibility in the supervised setting. We therefore present the more difficult problem of learning to model physical plausibility directly from text. We create a training set by extracting attested events from a large corpus, and we provide a baseline for training on these attested events in a self-supervised manner and testing on a physical plausibility task. We believe results could be further improved by injecting explicit commonsense knowledge into a distributional model.

</details>

<details>

<summary>2019-11-13 19:53:21 - An In-Depth Study on Open-Set Camera Model Identification</summary>

- *Pedro Ribeiro Mendes Júnior, Luca Bondi, Paolo Bestagini, Stefano Tubaro, Anderson Rocha*

- `1904.08497v2` - [abs](http://arxiv.org/abs/1904.08497v2) - [pdf](http://arxiv.org/pdf/1904.08497v2)

> Camera model identification refers to the problem of linking a picture to the camera model used to shoot it. As this might be an enabling factor in different forensic applications to single out possible suspects (e.g., detecting the author of child abuse or terrorist propaganda material), many accurate camera model attribution methods have been developed in the literature. One of their main drawbacks, however, is the typical closed-set assumption of the problem. This means that an investigated photograph is always assigned to one camera model within a set of known ones present during investigation, i.e., training time, and the fact that the picture can come from a completely unrelated camera model during actual testing is usually ignored. Under realistic conditions, it is not possible to assume that every picture under analysis belongs to one of the available camera models. To deal with this issue, in this paper, we present the first in-depth study on the possibility of solving the camera model identification problem in open-set scenarios. Given a photograph, we aim at detecting whether it comes from one of the known camera models of interest or from an unknown one. We compare different feature extraction algorithms and classifiers specially targeting open-set recognition. We also evaluate possible open-set training protocols that can be applied along with any open-set classifier, observing that a simple of those alternatives obtains best results. Thorough testing on independent datasets shows that it is possible to leverage a recently proposed convolutional neural network as feature extractor paired with a properly trained open-set classifier aiming at solving the open-set camera model attribution problem even to small-scale image patches, improving over state-of-the-art available solutions.

</details>

<details>

<summary>2019-11-13 22:18:44 - Parsimonious Morpheme Segmentation with an Application to Enriching Word Embeddings</summary>

- *Ahmed El-Kishky, Frank Xu, Aston Zhang, Jiawei Han*

- `1908.07832v2` - [abs](http://arxiv.org/abs/1908.07832v2) - [pdf](http://arxiv.org/pdf/1908.07832v2)

> Traditionally, many text-mining tasks treat individual word-tokens as the finest meaningful semantic granularity. However, in many languages and specialized corpora, words are composed by concatenating semantically meaningful subword structures. Word-level analysis cannot leverage the semantic information present in such subword structures. With regard to word embedding techniques, this leads to not only poor embeddings for infrequent words in long-tailed text corpora but also weak capabilities for handling out-of-vocabulary words. In this paper we propose MorphMine for unsupervised morpheme segmentation. MorphMine applies a parsimony criterion to hierarchically segment words into the fewest number of morphemes at each level of the hierarchy. This leads to longer shared morphemes at each level of segmentation. Experiments show that MorphMine segments words in a variety of languages into human-verified morphemes. Additionally, we experimentally demonstrate that utilizing MorphMine morphemes to enrich word embeddings consistently improves embedding quality on a variety of of embedding evaluations and a downstream language modeling task.

</details>

<details>

<summary>2019-11-13 22:36:57 - GNNExplainer: Generating Explanations for Graph Neural Networks</summary>

- *Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec*

- `1903.03894v4` - [abs](http://arxiv.org/abs/1903.03894v4) - [pdf](http://arxiv.org/pdf/1903.03894v4)

> Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs.GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models, and explaining predictions made by GNNs remains unsolved. Here we propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1% on average. GNNExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.

</details>

<details>

<summary>2019-11-13 23:59:44 - Motion Reasoning for Goal-Based Imitation Learning</summary>

- *De-An Huang, Yu-Wei Chao, Chris Paxton, Xinke Deng, Li Fei-Fei, Juan Carlos Niebles, Animesh Garg, Dieter Fox*

- `1911.05864v1` - [abs](http://arxiv.org/abs/1911.05864v1) - [pdf](http://arxiv.org/pdf/1911.05864v1)

> We address goal-based imitation learning, where the aim is to output the symbolic goal from a third-person video demonstration. This enables the robot to plan for execution and reproduce the same goal in a completely different environment. The key challenge is that the goal of a video demonstration is often ambiguous at the level of semantic actions. The human demonstrators might unintentionally achieve certain subgoals in the demonstrations with their actions. Our main contribution is to propose a motion reasoning framework that combines task and motion planning to disambiguate the true intention of the demonstrator in the video demonstration. This allows us to robustly recognize the goals that cannot be disambiguated by previous action-based approaches. We evaluate our approach by collecting a dataset of 96 video demonstrations in a mockup kitchen environment. We show that our motion reasoning plays an important role in recognizing the actual goal of the demonstrator and improves the success rate by over 20%. We further show that by using the automatically inferred goal from the video demonstration, our robot is able to reproduce the same task in a real kitchen environment.

</details>

<details>

<summary>2019-11-14 07:45:32 - HUSE: Hierarchical Universal Semantic Embeddings</summary>

- *Pradyumna Narayana, Aniket Pednekar, Abishek Krishnamoorthy, Kazoo Sone, Sugato Basu*

- `1911.05978v1` - [abs](http://arxiv.org/abs/1911.05978v1) - [pdf](http://arxiv.org/pdf/1911.05978v1)

> There is a recent surge of interest in cross-modal representation learning corresponding to images and text. The main challenge lies in mapping images and text to a shared latent space where the embeddings corresponding to a similar semantic concept lie closer to each other than the embeddings corresponding to different semantic concepts, irrespective of the modality. Ranking losses are commonly used to create such shared latent space -- however, they do not impose any constraints on inter-class relationships resulting in neighboring clusters to be completely unrelated. The works in the domain of visual semantic embeddings address this problem by first constructing a semantic embedding space based on some external knowledge and projecting image embeddings onto this fixed semantic embedding space. These works are confined only to image domain and constraining the embeddings to a fixed space adds additional burden on learning. This paper proposes a novel method, HUSE, to learn cross-modal representation with semantic information. HUSE learns a shared latent space where the distance between any two universal embeddings is similar to the distance between their corresponding class embeddings in the semantic embedding space. HUSE also uses a classification objective with a shared classification layer to make sure that the image and text embeddings are in the same shared latent space. Experiments on UPMC Food-101 show our method outperforms previous state-of-the-art on retrieval, hierarchical precision and classification results.

</details>

<details>

<summary>2019-11-14 10:09:56 - Answering questions by learning to rank -- Learning to rank by answering questions</summary>

- *George-Sebastian Pîrtoacă, Traian Rebedea, Stefan Ruseti*

- `1909.00596v2` - [abs](http://arxiv.org/abs/1909.00596v2) - [pdf](http://arxiv.org/pdf/1909.00596v2)

> Answering multiple-choice questions in a setting in which no supporting documents are explicitly provided continues to stand as a core problem in natural language processing. The contribution of this article is two-fold. First, it describes a method which can be used to semantically rank documents extracted from Wikipedia or similar natural language corpora. Second, we propose a model employing the semantic ranking that holds the first place in two of the most popular leaderboards for answering multiple-choice questions: ARC Easy and Challenge. To achieve this, we introduce a self-attention based neural network that latently learns to rank documents by their importance related to a given question, whilst optimizing the objective of predicting the correct answer. These documents are considered relevant contexts for the underlying question. We have published the ranked documents so that they can be used off-the-shelf to improve downstream decision models.

</details>

<details>

<summary>2019-11-14 11:36:16 - Semantic Granularity Metric Learning for Visual Search</summary>

- *Dipu Manandhar, Muhammet Bastan, Kim-Hui Yap*

- `1911.06047v1` - [abs](http://arxiv.org/abs/1911.06047v1) - [pdf](http://arxiv.org/pdf/1911.06047v1)

> Deep metric learning applied to various applications has shown promising results in identification, retrieval and recognition. Existing methods often do not consider different granularity in visual similarity. However, in many domain applications, images exhibit similarity at multiple granularities with visual semantic concepts, e.g. fashion demonstrates similarity ranging from clothing of the exact same instance to similar looks/design or a common category. Therefore, training image triplets/pairs used for metric learning inherently possess different degree of information. However, the existing methods often treats them with equal importance during training. This hinders capturing the underlying granularities in feature similarity required for effective visual search.   In view of this, we propose a new deep semantic granularity metric learning (SGML) that develops a novel idea of leveraging attribute semantic space to capture different granularity of similarity, and then integrate this information into deep metric learning. The proposed method simultaneously learns image attributes and embeddings using multitask CNNs. The two tasks are not only jointly optimized but are further linked by the semantic granularity similarity mappings to leverage the correlations between the tasks. To this end, we propose a new soft-binomial deviance loss that effectively integrates the degree of information in training samples, which helps to capture visual similarity at multiple granularities. Compared to recent ensemble-based methods, our framework is conceptually elegant, computationally simple and provides better performance. We perform extensive experiments on benchmark metric learning datasets and demonstrate that our method outperforms recent state-of-the-art methods, e.g., 1-4.5\% improvement in Recall@1 over the previous state-of-the-arts [1],[2] on DeepFashion In-Shop dataset.

</details>

<details>

<summary>2019-11-14 19:52:08 - Give me (un)certainty -- An exploration of parameters that affect segmentation uncertainty</summary>

- *Katharina Hoebel, Ken Chang, Jay Patel, Praveer Singh, Jayashree Kalpathy-Cramer*

- `1911.06357v1` - [abs](http://arxiv.org/abs/1911.06357v1) - [pdf](http://arxiv.org/pdf/1911.06357v1)

> Segmentation tasks in medical imaging are inherently ambiguous: the boundary of a target structure is oftentimes unclear due to image quality and biological factors. As such, predicted segmentations from deep learning algorithms are inherently ambiguous. Additionally, "ground truth" segmentations performed by human annotators are in fact weak labels that further increase the uncertainty of outputs of supervised models developed on these manual labels. To date, most deep learning segmentation studies utilize predicted segmentations without uncertainty quantification. In contrast, we explore the use of Monte Carlo dropout U-Nets for the segmentation with additional quantification of segmentation uncertainty. We assess the utility of three measures of uncertainty (Coefficient of Variation, Mean Pairwise Dice, and Mean Voxelwise Uncertainty) for the segmentation of a less ambiguous target structure (liver) and a more ambiguous one (liver tumors). Furthermore, we assess how the utility of these measures changes with different patch sizes and cost functions. Our results suggest that models trained using larger patches and the weighted categorical cross-entropy as cost function allow the extraction of more meaningful uncertainty measures compared to smaller patches and soft dice loss. Among the three uncertainty measures Mean Pairwise Dice shows the strongest correlation with segmentation quality. Our study serves as a proof-of-concept of how uncertainty measures can be used to assess the quality of a predicted segmentation, potentially serving to flag low quality segmentations from a given model for further human review.

</details>

<details>

<summary>2019-11-15 01:06:47 - Domain-Relevant Embeddings for Medical Question Similarity</summary>

- *Clara McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, Xavier Amatriain*

- `1910.04192v2` - [abs](http://arxiv.org/abs/1910.04192v2) - [pdf](http://arxiv.org/pdf/1910.04192v2)

> The rate at which medical questions are asked online far exceeds the capacity of qualified people to answer them, and many of these questions are not unique. Identifying same-question pairs could enable questions to be answered more effectively. While many research efforts have focused on the problem of general question similarity for non-medical applications, these approaches do not generalize well to the medical domain, where medical expertise is often required to determine semantic similarity. In this paper, we show how a semi-supervised approach of pre-training a neural network on medical question-answer pairs is a particularly useful intermediate task for the ultimate goal of determining medical question similarity. While other pre-training tasks yield an accuracy below 78.7% on this task, our model achieves an accuracy of 82.6% with the same number of training examples, and an accuracy of 80.0% with a much smaller training set.

</details>

<details>

<summary>2019-11-15 07:45:35 - Leveraging Deep Graph-Based Text Representation for Sentiment Polarity Applications</summary>

- *Kayvan Bijari, Hadi Zare, Emad Kebriaei, Hadi Veisi*

- `1902.10247v3` - [abs](http://arxiv.org/abs/1902.10247v3) - [pdf](http://arxiv.org/pdf/1902.10247v3)

> Over the last few years, machine learning over graph structures has manifested a significant enhancement in text mining applications such as event detection, opinion mining, and news recommendation. One of the primary challenges in this regard is structuring a graph that encodes and encompasses the features of textual data for the effective machine learning algorithm. Besides, exploration and exploiting of semantic relations is regarded as a principal step in text mining applications. However, most of the traditional text mining methods perform somewhat poor in terms of employing such relations. In this paper, we propose a sentence-level graph-based text representation which includes stop words to consider semantic and term relations. Then, we employ a representation learning approach on the combined graphs of sentences to extract the latent and continuous features of the documents. Eventually, the learned features of the documents are fed into a deep neural network for the sentiment classification task. The experimental results demonstrate that the proposed method substantially outperforms the related sentiment analysis approaches based on several benchmark datasets. Furthermore, our method can be generalized on different datasets without any dependency on pre-trained word embeddings.

</details>

<details>

<summary>2019-11-15 13:19:12 - Data Preparation in Agriculture Through Automated Semantic Annotation -- Basis for a Wide Range of Smart Services</summary>

- *Julian Klose, Markus Schröder, Silke Becker, Ansgar Bernardi, Arno Ruckelshausen*

- `1911.06606v1` - [abs](http://arxiv.org/abs/1911.06606v1) - [pdf](http://arxiv.org/pdf/1911.06606v1)

> Modern agricultural technology and the increasing digitalisation of such processes provide a wide range of data. However, their efficient and beneficial use suffers from legitimate concerns about data sovereignty and control, format inconsistencies and different interpretations. As a proposed solution, we present Wikinormia, a collaborative platform in which interested participants can describe and discuss their own new data formats. Once a finalized vocabulary has been created, specific parsers can semantically process the raw data into three basic representations: spatial information, time series and semantic facts (agricultural knowledge graph). Thanks to publicly accessible definitions and descriptions, developers can easily gain an overview of the concepts that are relevant to them. A variety of services will then (subject to individual access rights) be able to query their data simply via a query interface and retrieve results. We have already implemented this proposed solution in a prototype in the SDSD (Smart Data - Smart Services) project and demonstrate the benefits with a range of representative services. This provides an efficient system for the cooperative, flexible digitalisation of agricultural workflows.

</details>

<details>

<summary>2019-11-15 14:21:54 - A Policy Editor for Semantic Sensor Networks</summary>

- *Paolo Pareti, George Konstantinidis, Timothy J. Norman*

- `1911.06657v1` - [abs](http://arxiv.org/abs/1911.06657v1) - [pdf](http://arxiv.org/pdf/1911.06657v1)

> An important use of sensors and actuator networks is to comply with health and safety policies in hazardous environments. In order to deal with increasingly large and dynamic environments, and to quickly react to emergencies, tools are needed to simplify the process of translating high-level policies into executable queries and rules. We present a framework to produce such tools, which uses rules to aggregate low-level sensor data, described using the Semantic Sensor Network Ontology, into more useful and actionable abstractions. Using the schema of the underlying data sources as an input, we automatically generate abstractions which are relevant to the use case at hand. In this demonstration we present a policy editor tool and a simulation on which policies can be tested.

</details>

<details>

<summary>2019-11-15 16:26:54 - Breaking the encryption scheme of the Moscow Internet voting system</summary>

- *Pierrick Gaudry, Alexander Golovnev*

- `1908.05127v2` - [abs](http://arxiv.org/abs/1908.05127v2) - [pdf](http://arxiv.org/pdf/1908.05127v2)

> In September 2019, voters for the election at the Parliament of the city of Moscow were allowed to use an Internet voting system. The source code of it had been made available for public testing. In this paper we show two successful attacks on the encryption scheme implemented in the voting system. Both attacks were sent to the developers of the system, and both issues had been fixed after that.The encryption used in this system is a variant of ElGamal over finite fields. In the first attack we show that the used key sizes are too small. We explain how to retrieve the private keys from the public keys in a matter of minutes with easily available resources.When this issue had been fixed and the new system had become available for testing, we discovered that the new implementation was not semantically secure. We demonstrate how this newly found security vulnerability can be used for counting the number of votes cast for a candidate.

</details>

<details>

<summary>2019-11-15 20:32:12 - Semantic interoperability and characterization of data provenance in computational molecular engineering</summary>

- *M. T. Horsch, C. Niethammer, G. Boccardo, P. Carbone, S. Chiacchiera, M. Chiricotto, J. D. Elliott, V. Lobaskin, P. Neumann, P. Schiffels, M. A. Seaton, I. T. Todorov, J. Vrabec, W. L. Cavalcanti*

- `1908.02335v2` - [abs](http://arxiv.org/abs/1908.02335v2) - [pdf](http://arxiv.org/pdf/1908.02335v2)

> By introducing a common representational system for metadata that describe the employed simulation workflows, diverse sources of data and platforms in computational molecular engineering, such as workflow management systems, can become interoperable at the semantic level. To achieve semantic interoperability, the present work introduces two ontologies that provide a formal specification of the entities occurring in a simulation workflow and the relations between them: The software ontology VISO is developed to represent software packages and their features, and OSMO, an ontology for simulation, modelling, and optimization, is introduced on the basis of MODA, a previously developed semi-intuitive graph notation for workflows in materials modelling. As a proof of concept, OSMO is employed to describe a use case of the TaLPas workflow management system, a scheduler and workflow optimizer for particle-based simulations.

</details>

<details>

<summary>2019-11-16 03:05:41 - Finding Social Media Trolls: Dynamic Keyword Selection Methods for Rapidly-Evolving Online Debates</summary>

- *Anqi Liu, Maya Srikanth, Nicholas Adams-Cohen, R. Michael Alvarez, Anima Anandkumar*

- `1911.05332v2` - [abs](http://arxiv.org/abs/1911.05332v2) - [pdf](http://arxiv.org/pdf/1911.05332v2)

> Online harassment is a significant social problem. Prevention of online harassment requires rapid detection of harassing, offensive, and negative social media posts. In this paper, we propose the use of word embedding models to identify offensive and harassing social media messages in two aspects: detecting fast-changing topics for more effective data collection and representing word semantics in different domains. We demonstrate with preliminary results that using the GloVe (Global Vectors for Word Representation) model facilitates the discovery of new and relevant keywords to use for data collection and trolling detection. Our paper concludes with a discussion of a research agenda to further develop and test word embedding models for identification of social media harassment and trolling.

</details>

<details>

<summary>2019-11-16 03:41:40 - Robust Reading Comprehension with Linguistic Constraints via Posterior Regularization</summary>

- *Mantong Zhou, Minlie Huang, Xiaoyan Zhu*

- `1911.06948v1` - [abs](http://arxiv.org/abs/1911.06948v1) - [pdf](http://arxiv.org/pdf/1911.06948v1)

> In spite of great advancements of machine reading comprehension (RC), existing RC models are still vulnerable and not robust to different types of adversarial examples. Neural models over-confidently predict wrong answers to semantic different adversarial examples, while over-sensitively predict wrong answers to semantic equivalent adversarial examples. Existing methods which improve the robustness of such neural models merely mitigate one of the two issues but ignore the other. In this paper, we address the over-confidence issue and the over-sensitivity issue existing in current RC models simultaneously with the help of external linguistic knowledge. We first incorporate external knowledge to impose different linguistic constraints (entity constraint, lexical constraint, and predicate constraint), and then regularize RC models through posterior regularization. Linguistic constraints induce more reasonable predictions for both semantic different and semantic equivalent adversarial examples, and posterior regularization provides an effective mechanism to incorporate these constraints. Our method can be applied to any existing neural RC models including state-of-the-art BERT models. Extensive experiments show that our method remarkably improves the robustness of base RC models, and is better to cope with these two issues simultaneously.

</details>

<details>

<summary>2019-11-16 10:39:06 - SimVODIS: Simultaneous Visual Odometry, Object Detection, and Instance Segmentation</summary>

- *Ue-Hwan Kim, Se-Ho Kim, Jong-Hwan Kim*

- `1911.05939v2` - [abs](http://arxiv.org/abs/1911.05939v2) - [pdf](http://arxiv.org/pdf/1911.05939v2)

> Intelligent agents need to understand the surrounding environment to provide meaningful services to or interact intelligently with humans. The agents should perceive geometric features as well as semantic entities inherent in the environment. Contemporary methods in general provide one type of information regarding the environment at a time, making it difficult to conduct high-level tasks. Moreover, running two types of methods and associating two resultant information requires a lot of computation and complicates the software architecture. To overcome these limitations, we propose a neural architecture that simultaneously performs both geometric and semantic tasks in a single thread: simultaneous visual odometry, object detection, and instance segmentation (SimVODIS). Training SimVODIS requires unlabeled video sequences and the photometric consistency between input image frames generates self-supervision signals. The performance of SimVODIS outperforms or matches the state-of-the-art performance in pose estimation, depth map prediction, object detection, and instance segmentation tasks while completing all the tasks in a single thread. We expect SimVODIS would enhance the autonomy of intelligent agents and let the agents provide effective services to humans.

</details>

<details>

<summary>2019-11-16 13:34:16 - Contribution au Niveau de l'Approche Indirecte à Base de Transfert dans la Traduction Automatique</summary>

- *Sadik Bessou*

- `1911.07030v1` - [abs](http://arxiv.org/abs/1911.07030v1) - [pdf](http://arxiv.org/pdf/1911.07030v1)

> In this thesis, we address several important issues concerning the morphological analysis of Arabic language applied to textual data and machine translation. First, we provided an overview on machine translation, its history and its development, then we exposed human translation techniques for eventual inspiration in machine translation, and we exposed linguistic approaches and particularly indirect transfer approaches. Finally, we presented our contributions to the resolution of morphosyntactic problems in computer linguistics as multilingual information retrieval and machine translation. As a first contribution, we developed a morphological analyzer for Arabic, and we have exploited it in the bilingual information retrieval such as a computer application of multilingual documentary. Results validation showed a statistically significant performance. In a second contribution, we proposed a list of morphosyntactic transfer rules from English to Arabic for translation in three phases: analysis, transfer, generation. We focused on the transfer phase without semantic distortion for an abstraction of English in a sufficient subset of Arabic.

</details>

<details>

<summary>2019-11-17 04:07:47 - On Incorporating Semantic Prior Knowledge in Deep Learning Through Embedding-Space Constraints</summary>

- *Damien Teney, Ehsan Abbasnejad, Anton van den Hengel*

- `1909.13471v2` - [abs](http://arxiv.org/abs/1909.13471v2) - [pdf](http://arxiv.org/pdf/1909.13471v2)

> The knowledge that humans hold about a problem often extends far beyond a set of training data and output labels. While the success of deep learning mostly relies on supervised training, important properties cannot be inferred efficiently from end-to-end annotations alone, for example causal relations or domain-specific invariances. We present a general technique to supplement supervised training with prior knowledge expressed as relations between training instances. We illustrate the method on the task of visual question answering to exploit various auxiliary annotations, including relations of equivalence and of logical entailment between questions. Existing methods to use these annotations, including auxiliary losses and data augmentation, cannot guarantee the strict inclusion of these relations into the model since they require a careful balancing against the end-to-end objective. Our method uses these relations to shape the embedding space of the model, and treats them as strict constraints on its learned representations. In the context of VQA, this approach brings significant improvements in accuracy and robustness, in particular over the common practice of incorporating the constraints as a soft regularizer. We also show that incorporating this type of prior knowledge with our method brings consistent improvements, independently from the amount of supervised data used. It demonstrates the value of an additional training signal that is otherwise difficult to extract from end-to-end annotations alone.

</details>

<details>

<summary>2019-11-17 05:40:48 - Explicit Contextual Semantics for Text Comprehension</summary>

- *Zhuosheng Zhang, Yuwei Wu, Zuchao Li, Hai Zhao*

- `1809.02794v3` - [abs](http://arxiv.org/abs/1809.02794v3) - [pdf](http://arxiv.org/pdf/1809.02794v3)

> Who did what to whom is a major focus in natural language understanding, which is right the aim of semantic role labeling (SRL) task. Despite of sharing a lot of processing characteristics and even task purpose, it is surprisingly that jointly considering these two related tasks was never formally reported in previous work. Thus this paper makes the first attempt to let SRL enhance text comprehension and inference through specifying verbal predicates and their corresponding semantic roles. In terms of deep learning models, our embeddings are enhanced by explicit contextual semantic role labels for more fine-grained semantics. We show that the salient labels can be conveniently added to existing models and significantly improve deep learning models in challenging text comprehension tasks. Extensive experiments on benchmark machine reading comprehension and inference datasets verify that the proposed semantic learning helps our system reach new state-of-the-art over strong baselines which have been enhanced by well pretrained language models from the latest progress.

</details>

<details>

<summary>2019-11-17 09:36:07 - MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning</summary>

- *Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, Liangchen Luo*

- `1911.09483v1` - [abs](http://arxiv.org/abs/1911.09483v1) - [pdf](http://arxiv.org/pdf/1911.09483v1)

> In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism. Code will be available at https://github.com/lancopku/MUSE

</details>

<details>

<summary>2019-11-17 14:58:17 - DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue</summary>

- *Xiaoze Jiang, Jing Yu, Zengchang Qin, Yingying Zhuang, Xingxing Zhang, Yue Hu, Qi Wu*

- `1911.07251v1` - [abs](http://arxiv.org/abs/1911.07251v1) - [pdf](http://arxiv.org/pdf/1911.07251v1)

> Different from Visual Question Answering task that requires to answer only one question about an image, Visual Dialogue involves multiple questions which cover a broad range of visual content that could be related to any objects, relationships or semantics. The key challenge in Visual Dialogue task is thus to learn a more comprehensive and semantic-rich image representation which may have adaptive attentions on the image for variant questions. In this research, we propose a novel model to depict an image from both visual and semantic perspectives. Specifically, the visual view helps capture the appearance-level information, including objects and their relationships, while the semantic view enables the agent to understand high-level visual semantics from the whole image to the local regions. Futhermore, on top of such multi-view image features, we propose a feature selection framework which is able to adaptively capture question-relevant information hierarchically in fine-grained level. The proposed method achieved state-of-the-art results on benchmark Visual Dialogue datasets. More importantly, we can tell which modality (visual or semantic) has more contribution in answering the current question by visualizing the gate values. It gives us insights in understanding of human cognition in Visual Dialogue.

</details>

<details>

<summary>2019-11-17 15:46:38 - Learning with Hierarchical Complement Objective</summary>

- *Hao-Yun Chen, Li-Huang Tsai, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan*

- `1911.07257v1` - [abs](http://arxiv.org/abs/1911.07257v1) - [pdf](http://arxiv.org/pdf/1911.07257v1)

> Label hierarchies widely exist in many vision-related problems, ranging from explicit label hierarchies existed in image classification to latent label hierarchies existed in semantic segmentation. Nevertheless, state-of-the-art methods often deploy cross-entropy loss that implicitly assumes class labels to be exclusive and thus independence from each other. Motivated by the fact that classes from the same parental category usually share certain similarity, we design a new training diagram called Hierarchical Complement Objective Training (HCOT) that leverages the information from label hierarchy. HCOT maximizes the probability of the ground truth class, and at the same time, neutralizes the probabilities of rest of the classes in a hierarchical fashion, making the model take advantage of the label hierarchy explicitly. The proposed HCOT is evaluated on both image classification and semantic segmentation tasks. Experimental results confirm that HCOT outperforms state-of-the-art models in CIFAR-100, ImageNet-2012, and PASCAL-Context. The study further demonstrates that HCOT can be applied on tasks with latent label hierarchies, which is a common characteristic in many machine learning tasks.

</details>

<details>

<summary>2019-11-17 23:20:23 - Exploiting Human Social Cognition for the Detection of Fake and Fraudulent Faces via Memory Networks</summary>

- *Tharindu Fernando, Clinton Fookes, Simon Denman, Sridha Sridharan*

- `1911.07844v1` - [abs](http://arxiv.org/abs/1911.07844v1) - [pdf](http://arxiv.org/pdf/1911.07844v1)

> Advances in computer vision have brought us to the point where we have the ability to synthesise realistic fake content. Such approaches are seen as a source of disinformation and mistrust, and pose serious concerns to governments around the world. Convolutional Neural Networks (CNNs) demonstrate encouraging results when detecting fake images that arise from the specific type of manipulation they are trained on. However, this success has not transitioned to unseen manipulation types, resulting in a significant gap in the line-of-defense. We propose a Hierarchical Memory Network (HMN) architecture, which is able to successfully detect faked faces by utilising knowledge stored in neural memories as well as visual cues to reason about the perceived face and anticipate its future semantic embeddings. This renders a generalisable face tampering detection framework. Experimental results demonstrate the proposed approach achieves superior performance for fake and fraudulent face detection compared to the state-of-the-art.

</details>

<details>

<summary>2019-11-18 01:44:19 - Sequence-Aware Factorization Machines for Temporal Predictive Analytics</summary>

- *Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen, Wen-Chih Peng, Xue Li, Xiaofang Zhou*

- `1911.02752v2` - [abs](http://arxiv.org/abs/1911.02752v2) - [pdf](http://arxiv.org/pdf/1911.02752v2)

> In various web applications like targeted advertising and recommender systems, the available categorical features (e.g., product type) are often of great importance but sparse. As a widely adopted solution, models based on Factorization Machines (FMs) are capable of modelling high-order interactions among features for effective sparse predictive analytics. As the volume of web-scale data grows exponentially over time, sparse predictive analytics inevitably involves dynamic and sequential features. However, existing FM-based models assume no temporal orders in the data, and are unable to capture the sequential dependencies or patterns within the dynamic features, impeding the performance and adaptivity of these methods. Hence, in this paper, we propose a novel Sequence-Aware Factorization Machine (SeqFM) for temporal predictive analytics, which models feature interactions by fully investigating the effect of sequential dependencies. As static features (e.g., user gender) and dynamic features (e.g., user interacted items) express different semantics, we innovatively devise a multi-view self-attention scheme that separately models the effect of static features, dynamic features and the mutual interactions between static and dynamic features in three different views. In SeqFM, we further map the learned representations of feature interactions to the desired output with a shared residual network. To showcase the versatility and generalizability of SeqFM, we test SeqFM in three popular application scenarios for FM-based models, namely ranking, classification and regression tasks. Extensive experimental results on six large-scale datasets demonstrate the superior effectiveness and efficiency of SeqFM.

</details>

<details>

<summary>2019-11-18 03:11:36 - Multi-task Sentence Encoding Model for Semantic Retrieval in Question Answering Systems</summary>

- *Qiang Huang, Jianhui Bu, Weijian Xie, Shengwen Yang, Weijia Wu, Liping Liu*

- `1911.07405v1` - [abs](http://arxiv.org/abs/1911.07405v1) - [pdf](http://arxiv.org/pdf/1911.07405v1)

> Question Answering (QA) systems are used to provide proper responses to users' questions automatically. Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification (PI) problem. Given a question, the aim of the task is to find the most similar question from a QA knowledge base. In this paper, we propose a Multi-task Sentence Encoding Model (MSEM) for the PI problem, wherein a connected graph is employed to depict the relation between sentences, and a multi-task learning model is applied to address both the sentence matching and sentence intent classification problem. In addition, we implement a general semantic retrieval framework that combines our proposed model and the Approximate Nearest Neighbor (ANN) technology, which enables us to find the most similar question from all available candidates very quickly during online serving. The experiments show the superiority of our proposed method as compared with the existing sentence matching models.

</details>

<details>

<summary>2019-11-18 05:37:42 - Towards Verified Stochastic Variational Inference for Probabilistic Programs</summary>

- *Wonyeol Lee, Hangyeol Yu, Xavier Rival, Hongseok Yang*

- `1907.08827v4` - [abs](http://arxiv.org/abs/1907.08827v4) - [pdf](http://arxiv.org/pdf/1907.08827v4)

> Probabilistic programming is the idea of writing models from statistics and machine learning using program notations and reasoning about these models using generic inference engines. Recently its combination with deep learning has been explored intensely, leading to the development of deep probabilistic programming languages such as Pyro. At the core of this development lie inference engines based on stochastic variational inference algorithms. When asked to find information about the posterior distribution of a model written in such a language, these algorithms convert this posterior-inference query into an optimisation problem and solve it approximately by gradient ascent. In this paper, we analyse one of the most fundamental and versatile variational inference algorithms, called score estimator, using tools from denotational semantics and program analysis. We formally express what this algorithm does on models denoted by programs, and expose implicit assumptions made by the algorithm. The violation of these assumptions may lead to an undefined optimisation objective or the loss of convergence guarantee of the optimisation process. We then describe rules for proving these assumptions, which can be automated by static program analyses. Some of our rules use nontrivial facts from continuous mathematics, and let us replace requirements about integrals in the assumptions, by conditions involving differentiation or boundedness, which are much easier to prove automatically. Following our general methodology, we have developed a static program analysis for Pyro that aims at discharging the assumption about what we call model-guide support match. Applied to the eight representative model-guide pairs from the Pyro webpage, our analysis finds a bug in one of these cases, reveals a non-standard use of an inference engine in another, and shows the assumptions are met in the remaining cases.

</details>

<details>

<summary>2019-11-18 06:38:33 - Accurate Trajectory Prediction for Autonomous Vehicles</summary>

- *Michael Diodato, Yu Li, Antonia Lovjer, Minsu Yeom, Albert Song, Yiyang Zeng, Abhay Khosla, Benedikt Schifferer, Manik Goyal, Iddo Drori*

- `1911.08568v1` - [abs](http://arxiv.org/abs/1911.08568v1) - [pdf](http://arxiv.org/pdf/1911.08568v1)

> Predicting vehicle trajectories, angle and speed is important for safe and comfortable driving. We demonstrate the best predicted angle, speed, and best performance overall winning the top three places of the ICCV 2019 Learning to Drive challenge. Our key contributions are (i) a general neural network system architecture which embeds and fuses together multiple inputs by encoding, and decodes multiple outputs using neural networks, (ii) using pre-trained neural networks for augmenting the given input data with segmentation maps and semantic information, and (iii) leveraging the form and distribution of the expected output in the model.

</details>

<details>

<summary>2019-11-18 07:52:43 - Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation</summary>

- *Chengyi Wang, Yu Wu, Shujie Liu, Zhenglu Yang, Ming Zhou*

- `1909.07575v3` - [abs](http://arxiv.org/abs/1909.07575v3) - [pdf](http://arxiv.org/pdf/1909.07575v3)

> End-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model outperforms baselines 2.2 BLEU on a large benchmark dataset.

</details>

<details>

<summary>2019-11-18 10:16:38 - Fine-Grained Static Detection of Obfuscation Transforms Using Ensemble-Learning and Semantic Reasoning</summary>

- *Ramtine Tofighi-Shirazi, Irina Mariuca Asavoae, Philippe Elbaz-Vincent*

- `1911.07523v1` - [abs](http://arxiv.org/abs/1911.07523v1) - [pdf](http://arxiv.org/pdf/1911.07523v1)

> The ability to efficiently detect the software protections used is at a prime to facilitate the selection and application of adequate deob-fuscation techniques. We present a novel approach that combines semantic reasoning techniques with ensemble learning classification for the purpose of providing a static detection framework for obfuscation transformations. By contrast to existing work, we provide a methodology that can detect multiple layers of obfuscation, without depending on knowledge of the underlying functionality of the training-set used. We also extend our work to detect constructions of obfuscation transformations, thus providing a fine-grained methodology. To that end, we provide several studies for the best practices of the use of machine learning techniques for a scalable and efficient model. According to our experimental results and evaluations on obfuscators such as Tigress and OLLVM, our models have up to 91% accuracy on state-of-the-art obfuscation transformations. Our overall accuracies for their constructions are up to 100%.

</details>

<details>

<summary>2019-11-18 11:08:11 - Multi-Task Learning of Height and Semantics from Aerial Images</summary>

- *Marcela Carvalho, Bertrand Le Saux, Pauline Trouvé-Peloux, Frédéric Champagnat, Andrés Almansa*

- `1911.07543v1` - [abs](http://arxiv.org/abs/1911.07543v1) - [pdf](http://arxiv.org/pdf/1911.07543v1)

> Aerial or satellite imagery is a great source for land surface analysis, which might yield land use maps or elevation models. In this investigation, we present a neural network framework for learning semantics and local height together. We show how this joint multi-task learning benefits to each task on the large dataset of the 2018 Data Fusion Contest. Moreover, our framework also yields an uncertainty map which allows assessing the prediction of the model. Code is available at https://github.com/marcelampc/mtl_aerial_images .

</details>

<details>

<summary>2019-11-18 13:28:37 - On the Vietnamese Name Entity Recognition: A Deep Learning Method Approach</summary>

- *Ngoc C. Lê, Ngoc-Yen Nguyen, Anh-Duong Trinh*

- `1912.01109v1` - [abs](http://arxiv.org/abs/1912.01109v1) - [pdf](http://arxiv.org/pdf/1912.01109v1)

> Named entity recognition (NER) plays an important role in text-based information retrieval. In this paper, we combine Bidirectional Long Short-Term Memory (Bi-LSTM) \cite{hochreiter1997,schuster1997} with Conditional Random Field (CRF) \cite{lafferty2001} to create a novel deep learning model for the NER problem. Each word as input of the deep learning model is represented by a Word2vec-trained vector. A word embedding set trained from about one million articles in 2018 collected through a Vietnamese news portal (baomoi.com). In addition, we concatenate a Word2Vec\cite{mikolov2013}-trained vector with semantic feature vector (Part-Of-Speech (POS) tagging, chunk-tag) and hidden syntactic feature vector (extracted by Bi-LSTM nerwork) to achieve the (so far best) result in Vietnamese NER system. The result was conducted on the data set VLSP2016 (Vietnamese Language and Speech Processing 2016 \cite{vlsp2016}) competition.

</details>

<details>

<summary>2019-11-18 14:50:31 - Using Mapping Languages for Building Legal Knowledge Graphs from XML Files</summary>

- *Ademar Crotti Junior, Fabrizio Orlandi, Declan O'Sullivan, Christian Dirschl, Quentin Reul*

- `1911.07673v1` - [abs](http://arxiv.org/abs/1911.07673v1) - [pdf](http://arxiv.org/pdf/1911.07673v1)

> This paper presents our experience on building RDF knowledge graphs for an industrial use case in the legal domain. The information contained in legal information systems are often accessed through simple keyword interfaces and presented as a simple list of hits. In order to improve search accuracy one may avail of knowledge graphs, where the semantics of the data can be made explicit. Significant research effort has been invested in the area of building knowledge graphs from semi-structured text documents, such as XML, with the prevailing approach being the use of mapping languages. In this paper, we present a semantic model for representing legal documents together with an industrial use case. We also present a set of use case requirements based on the proposed semantic model, which are used to compare and discuss the use of state-of-the-art mapping languages for building knowledge graphs for legal data.

</details>

<details>

<summary>2019-11-18 15:22:45 - An Attack on the the Encryption Scheme of the Moscow Internet Voting System</summary>

- *Alexander Golovnev*

- `1908.09170v2` - [abs](http://arxiv.org/abs/1908.09170v2) - [pdf](http://arxiv.org/pdf/1908.09170v2)

> The next Moscow City Duma elections will be held on September 8th with an option of Internet voting. Some source code of the voting system is posted online for public testing. Pierrick Gaudry recently showed that due to the relatively small length of the key, the encryption scheme could be easily broken. This issue has been fixed in the current version of the voting system. In this note we show that the new implementation of the ElGamal encryption system is not semantically secure. We also demonstrate how this newly found security vulnerability can be potentially used for counting the number of votes cast for a candidate.

</details>

<details>

<summary>2019-11-18 15:38:40 - Temporal Logics Over Finite Traces with Uncertainty (Technical Report)</summary>

- *Fabrizio M. Maggi, Marco Montali, Rafael Peñaloza*

- `1903.04940v2` - [abs](http://arxiv.org/abs/1903.04940v2) - [pdf](http://arxiv.org/pdf/1903.04940v2)

> Temporal logics over finite traces have recently seen wide application in a number of areas, from business process modelling, monitoring, and mining to planning and decision making. However, real-life dynamic systems contain a degree of uncertainty which cannot be handled with classical logics. We thus propose a new probabilistic temporal logic over finite traces using superposition semantics, where all possible evolutions are possible, until observed. We study the properties of the logic and provide automata-based mechanisms for deriving probabilistic inferences from its formulas. We then study a fragment of the logic with better computational properties. Notably, formulas in this fragment can be discovered from event log data using off-the-shelf existing declarative process discovery techniques.

</details>

<details>

<summary>2019-11-18 20:56:26 - Improving Universal Sound Separation Using Sound Classification</summary>

- *Efthymios Tzinis, Scott Wisdom, John R. Hershey, Aren Jansen, Daniel P. W. Ellis*

- `1911.07951v1` - [abs](http://arxiv.org/abs/1911.07951v1) - [pdf](http://arxiv.org/pdf/1911.07951v1)

> Deep learning approaches have recently achieved impressive performance on both audio source separation and sound classification. Most audio source separation approaches focus only on separating sources belonging to a restricted domain of source classes, such as speech and music. However, recent work has demonstrated the possibility of "universal sound separation", which aims to separate acoustic sources from an open domain, regardless of their class. In this paper, we utilize the semantic information learned by sound classifier networks trained on a vast amount of diverse sounds to improve universal sound separation. In particular, we show that semantic embeddings extracted from a sound classifier can be used to condition a separation network, providing it with useful additional information. This approach is especially useful in an iterative setup, where source estimates from an initial separation stage and their corresponding classifier-derived embeddings are fed to a second separation network. By performing a thorough hyperparameter search consisting of over a thousand experiments, we find that classifier embeddings from clean sources provide nearly one dB of SNR gain, and our best iterative models achieve a significant fraction of this oracle performance, establishing a new state-of-the-art for universal sound separation.

</details>

<details>

<summary>2019-11-19 02:38:18 - AdvSPADE: Realistic Unrestricted Attacks for Semantic Segmentation</summary>

- *Guangyu Shen, Chengzhi Mao, Junfeng Yang, Baishakhi Ray*

- `1910.02354v3` - [abs](http://arxiv.org/abs/1910.02354v3) - [pdf](http://arxiv.org/pdf/1910.02354v3)

> Due to the inherent robustness of segmentation models, traditional norm-bounded attack methods show limited effect on such type of models. In this paper, we focus on generating unrestricted adversarial examples for semantic segmentation models. We demonstrate a simple and effective method to generate unrestricted adversarial examples using conditional generative adversarial networks (CGAN) without any hand-crafted metric. The na\"ive implementation of CGAN, however, yields inferior image quality and low attack success rate. Instead, we leverage the SPADE (Spatially-adaptive denormalization) structure with an additional loss item to generate effective adversarial attacks in a single step. We validate our approach on the popular Cityscapes and ADE20K datasets, and demonstrate that our synthetic adversarial examples are not only realistic, but also improve the attack success rate by up to 41.0\% compared with the state of the art adversarial attack methods including PGD.

</details>

<details>

<summary>2019-11-19 07:06:22 - In Search of Credible News</summary>

- *Momchil Hardalov, Ivan Koychev, Preslav Nakov*

- `1911.08125v1` - [abs](http://arxiv.org/abs/1911.08125v1) - [pdf](http://arxiv.org/pdf/1911.08125v1)

> We study the problem of finding fake online news. This is an important problem as news of questionable credibility have recently been proliferating in social media at an alarming scale. As this is an understudied problem, especially for languages other than English, we first collect and release to the research community three new balanced credible vs. fake news datasets derived from four online sources. We then propose a language-independent approach for automatically distinguishing credible from fake news, based on a rich feature set. In particular, we use linguistic (n-gram), credibility-related (capitalization, punctuation, pronoun use, sentiment polarity), and semantic (embeddings and DBPedia data) features. Our experiments on three different testsets show that our model can distinguish credible from fake news with very high accuracy.

</details>

<details>

<summary>2019-11-19 07:40:32 - Neural Snowball for Few-Shot Relation Learning</summary>

- *Tianyu Gao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen Lin, Leyu Lin, Maosong Sun*

- `1908.11007v2` - [abs](http://arxiv.org/abs/1908.11007v2) - [pdf](http://arxiv.org/pdf/1908.11007v2)

> Knowledge graphs typically undergo open-ended growth of new relations. This cannot be well handled by relation extraction that focuses on pre-defined relations with sufficient training data. To address new relations with few-shot instances, we propose a novel bootstrapping approach, Neural Snowball, to learn new relations by transferring semantic knowledge about existing relations. More specifically, we use Relational Siamese Networks (RSN) to learn the metric of relational similarities between instances based on existing relations and their labeled data. Afterwards, given a new relation and its few-shot instances, we use RSN to accumulate reliable instances from unlabeled corpora; these instances are used to train a relation classifier, which can further identify new facts of the new relation. The process is conducted iteratively like a snowball. Experiments show that our model can gather high-quality instances for better few-shot relation learning and achieves significant improvement compared to baselines. Codes and datasets are released on https://github.com/thunlp/Neural-Snowball.

</details>

<details>

<summary>2019-11-19 08:03:11 - Tell Me What They're Holding: Weakly-supervised Object Detection with Transferable Knowledge from Human-object Interaction</summary>

- *Daesik Kim, Gyujeong Lee, Jisoo Jeong, Nojun Kwak*

- `1911.08141v1` - [abs](http://arxiv.org/abs/1911.08141v1) - [pdf](http://arxiv.org/pdf/1911.08141v1)

> In this work, we introduce a novel weakly supervised object detection (WSOD) paradigm to detect objects belonging to rare classes that have not many examples using transferable knowledge from human-object interactions (HOI). While WSOD shows lower performance than full supervision, we mainly focus on HOI as the main context which can strongly supervise complex semantics in images. Therefore, we propose a novel module called RRPN (relational region proposal network) which outputs an object-localizing attention map only with human poses and action verbs. In the source domain, we fully train an object detector and the RRPN with full supervision of HOI. With transferred knowledge about localization map from the trained RRPN, a new object detector can learn unseen objects with weak verbal supervision of HOI without bounding box annotations in the target domain. Because the RRPN is designed as an add-on type, we can apply it not only to the object detection but also to other domains such as semantic segmentation. The experimental results on HICO-DET dataset show the possibility that the proposed method can be a cheap alternative for the current supervised object detection paradigm. Moreover, qualitative results demonstrate that our model can properly localize unseen objects on HICO-DET and V-COCO datasets.

</details>

<details>

<summary>2019-11-19 10:07:18 - Deep and Dense Sarcasm Detection</summary>

- *Devin Pelser, Hugh Murrell*

- `1911.07474v2` - [abs](http://arxiv.org/abs/1911.07474v2) - [pdf](http://arxiv.org/pdf/1911.07474v2)

> Recent work in automated sarcasm detection has placed a heavy focus on context and meta-data. Whilst certain utterances indeed require background knowledge and commonsense reasoning, previous works have only explored shallow models for capturing the lexical, syntactic and semantic cues present within a text. In this paper, we propose a deep 56 layer network, implemented with dense connectivity to model the isolated utterance and extract richer features therein. We compare our approach against recent state-of-the-art architectures which make considerable use of extrinsic information, and demonstrate competitive results whilst using only the local features of the text. Further, we provide an analysis of the dependency of prior convolution outputs in generating the final feature maps. Finally a case study is presented, supporting that our approach accurately classifies additional uses of clear sarcasm, which a standard CNN misclassifies.

</details>

<details>

<summary>2019-11-19 16:29:59 - A model for predicting price polarity of real estate properties using information of real estate market websites</summary>

- *Vladimir Vargas-Calderón, Jorge E. Camargo*

- `1911.08382v1` - [abs](http://arxiv.org/abs/1911.08382v1) - [pdf](http://arxiv.org/pdf/1911.08382v1)

> This paper presents a model that uses the information that sellers publish in real estate market websites to predict whether a property has higher or lower price than the average price of its similar properties. The model learns the correlation between price and information (text descriptions and features) of real estate properties through automatic identification of latent semantic content given by a machine learning model based on doc2vec and xgboost. The proposed model was evaluated with a data set of 57,516 publications of real estate properties collected from 2016 to 2018 of Bogot\'a city. Results show that the accuracy of a classifier that involves text descriptions is slightly higher than a classifier that only uses features of the real estate properties, as text descriptions tends to contain detailed information about the property.

</details>

<details>

<summary>2019-11-19 16:36:54 - Multi-Resolution 3D CNN for MRI Brain Tumor Segmentation and Survival Prediction</summary>

- *Mehdi Amian, Mohammadreza Soltaninejad*

- `1911.08388v1` - [abs](http://arxiv.org/abs/1911.08388v1) - [pdf](http://arxiv.org/pdf/1911.08388v1)

> In this study, an automated three dimensional (3D) deep segmentation approach for detecting gliomas in 3D pre-operative MRI scans is proposed. Then, a classi-fication algorithm based on random forests, for survival prediction is presented. The objective is to segment the glioma area and produce segmentation labels for its different sub-regions, i.e. necrotic and the non-enhancing tumor core, the peri-tumoral edema, and enhancing tumor. The proposed deep architecture for the segmentation task encompasses two parallel streamlines with two different reso-lutions. One deep convolutional neural network is to learn local features of the input data while the other one is set to have a global observation on whole image. Deemed to be complementary, the outputs of each stream are then merged to pro-vide an ensemble complete learning of the input image. The proposed network takes the whole image as input instead of patch-based approaches in order to con-sider the semantic features throughout the whole volume. The algorithm is trained on BraTS 2019 which included 335 training cases, and validated on 127 unseen cases from the validation dataset using a blind testing approach. The proposed method was also evaluated on the BraTS 2019 challenge test dataset of 166 cases. The results show that the proposed methods provide promising segmentations as well as survival prediction. The mean Dice overlap measures of automatic brain tumor segmentation for validation set were 0.84, 0.74 and 0.71 for the whole tu-mor, core and enhancing tumor, respectively. The corresponding results for the challenge test dataset were 0.82, 0.72, and 0.70, respectively. The overall accura-cy of the proposed model for the survival prediction task is %52 for the valida-tion and %49 for the test dataset.

</details>

<details>

<summary>2019-11-19 22:44:40 - Adverse Childhood Experiences Ontology for Mental Health Surveillance, Research, and Evaluation: Advanced Knowledge Representation and Semantic Web Techniques</summary>

- *Jon Hael Brenas, Eun Kyong Shin, Arash Shaban-Nejad*

- `1912.05530v1` - [abs](http://arxiv.org/abs/1912.05530v1) - [pdf](http://arxiv.org/pdf/1912.05530v1)

> Background: Adverse Childhood Experiences (ACEs), a set of negative events and processes that a person might encounter during childhood and adolescence, have been proven to be linked to increased risks of a multitude of negative health outcomes and conditions when children reach adulthood and beyond.   Objective: To better understand the relationship between ACEs and their relevant risk factors with associated health outcomes and to eventually design and implement preventive interventions, access to an integrated coherent dataset is needed. Therefore, we implemented a formal ontology as a resource to allow the mental health community to facilitate data integration and knowledge modeling and to improve ACEs surveillance and research.   Methods: We use advanced knowledge representation and Semantic Web tools and techniques to implement the ontology. The current implementation of the ontology is expressed in the description logic ALCRIQ(D), a sublogic of Web Ontology Language (OWL 2).   Results: The ACEs Ontology has been implemented and made available to the mental health community and the public via the BioPortal repository. Moreover, multiple use-case scenarios have been introduced to showcase and evaluate the usability of the ontology in action. The ontology was created to be used by major actors in the ACEs community with different applications, from the diagnosis of individuals and predicting potential negative outcomes that they might encounter to the prevention of ACEs in a population and designing interventions and policies.   Conclusions: The ACEs Ontology provides a uniform and reusable semantic network and an integrated knowledge structure for mental health practitioners and researchers to improve ACEs surveillance and evaluation.

</details>

<details>

<summary>2019-11-20 00:18:44 - Sibling Neural Estimators: Improving Iterative Image Decoding with Gradient Communication</summary>

- *Ankur Mali, Alexander G. Ororbia, Clyde Lee Giles*

- `1911.08478v1` - [abs](http://arxiv.org/abs/1911.08478v1) - [pdf](http://arxiv.org/pdf/1911.08478v1)

> For lossy image compression, we develop a neural-based system which learns a nonlinear estimator for decoding from quantized representations. The system links two recurrent networks that \help" each other reconstruct same target image patches using complementary portions of spatial context that communicate via gradient signals. This dual agent system builds upon prior work that proposed the iterative refinement algorithm for recurrent neural network (RNN)based decoding which improved image reconstruction compared to standard decoding techniques. Our approach, which works with any encoder, neural or non-neural, This system progressively reduces image patch reconstruction error over a fixed number of steps. Experiment with variants of RNN memory cells, with and without future information, find that our model consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe as much as a 1:64 decibel (dB) gain over JPEG, a 1:46 dB gain over JPEG 2000, a 1:34 dB gain over the GOOG neural baseline, 0:36 over E2E (a modern competitive neural compression model), and 0:37 over a single iterative neural decoder.

</details>

<details>

<summary>2019-11-20 00:27:24 - Unsupervised Learning of Object Keypoints for Perception and Control</summary>

- *Tejas Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew Zisserman, Volodymyr Mnih*

- `1906.11883v2` - [abs](http://arxiv.org/abs/1906.11883v2) - [pdf](http://arxiv.org/pdf/1906.11883v2)

> The study of object representations in computer vision has primarily focused on developing representations that are useful for image classification, object detection, or semantic segmentation as downstream tasks. In this work we aim to learn object representations that are useful for control and reinforcement learning (RL). To this end, we introduce Transporter, a neural network architecture for discovering concise geometric object representations in terms of keypoints or image-space coordinates. Our method learns from raw video frames in a fully unsupervised manner, by transporting learnt image features between video frames using a keypoint bottleneck. The discovered keypoints track objects and object parts across long time-horizons more accurately than recent similar methods. Furthermore, consistent long-term tracking enables two notable results in control domains -- (1) using the keypoint co-ordinates and corresponding image features as inputs enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations drastically reduces the search space, enabling deep exploration (leading to states unreachable through random action exploration) without any extrinsic rewards.

</details>

<details>

<summary>2019-11-20 00:43:07 - Generate (non-software) Bugs to Fool Classifiers</summary>

- *Hiromu Yakura, Youhei Akimoto, Jun Sakuma*

- `1911.08644v1` - [abs](http://arxiv.org/abs/1911.08644v1) - [pdf](http://arxiv.org/pdf/1911.08644v1)

> In adversarial attacks intended to confound deep learning models, most studies have focused on limiting the magnitude of the modification so that humans do not notice the attack. On the other hand, during an attack against autonomous cars, for example, most drivers would not find it strange if a small insect image were placed on a stop sign, or they may overlook it. In this paper, we present a systematic approach to generate natural adversarial examples against classification models by employing such natural-appearing perturbations that imitate a certain object or signal. We first show the feasibility of this approach in an attack against an image classifier by employing generative adversarial networks that produce image patches that have the appearance of a natural object to fool the target model. We also introduce an algorithm to optimize placement of the perturbation in accordance with the input image, which makes the generation of adversarial examples fast and likely to succeed. Moreover, we experimentally show that the proposed approach can be extended to the audio domain, for example, to generate perturbations that sound like the chirping of birds to fool a speech classifier.

</details>

<details>

<summary>2019-11-20 00:48:36 - Co-Attention Hierarchical Network: Generating Coherent Long Distractors for Reading Comprehension</summary>

- *Xiaorui Zhou, Senlin Luo, Yunfang Wu*

- `1911.08648v1` - [abs](http://arxiv.org/abs/1911.08648v1) - [pdf](http://arxiv.org/pdf/1911.08648v1)

> In reading comprehension, generating sentence-level distractors is a significant task, which requires a deep understanding of the article and question. The traditional entity-centered methods can only generate word-level or phrase-level distractors. Although recently proposed neural-based methods like sequence-to-sequence (Seq2Seq) model show great potential in generating creative text, the previous neural methods for distractor generation ignore two important aspects. First, they didn't model the interactions between the article and question, making the generated distractors tend to be too general or not relevant to question context. Second, they didn't emphasize the relationship between the distractor and article, making the generated distractors not semantically relevant to the article and thus fail to form a set of meaningful options. To solve the first problem, we propose a co-attention enhanced hierarchical architecture to better capture the interactions between the article and question, thus guide the decoder to generate more coherent distractors. To alleviate the second problem, we add an additional semantic similarity loss to push the generated distractors more relevant to the article. Experimental results show that our model outperforms several strong baselines on automatic metrics, achieving state-of-the-art performance. Further human evaluation indicates that our generated distractors are more coherent and more educative compared with those distractors generated by baselines.

</details>

<details>

<summary>2019-11-20 03:45:34 - SSAH: Semi-supervised Adversarial Deep Hashing with Self-paced Hard Sample Generation</summary>

- *Sheng Jin, Shangchen Zhou, Yao Liu, Chao Chen, Xiaoshuai Sun, Hongxun Yao, Xiansheng Hua*

- `1911.08688v1` - [abs](http://arxiv.org/abs/1911.08688v1) - [pdf](http://arxiv.org/pdf/1911.08688v1)

> Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data, which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However, existing GAN-based methods treat image generations and hashing learning as two isolated processes, leading to generation ineffectiveness. Besides, most works fail to exploit the semantic information in unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace Adversarial Hashing method, named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (A-Net) and a hashing network (H-Net). To improve the quality of generative images, first, the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second, we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones, we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve state-of-the-art models on both the widely-used hashing datasets and fine-grained datasets.

</details>

<details>

<summary>2019-11-20 06:53:07 - Pan-Cancer Diagnostic Consensus Through Searching Archival Histopathology Images Using Artificial Intelligence</summary>

- *Shivam Kalra, H. R. Tizhoosh, Sultaan Shah, Charles Choi, Savvas Damaskinos, Amir Safarpoor, Sobhan Shafiei, Morteza Babaie, Phedias Diamandis, Clinton JV Campbell, Liron Pantanowitz*

- `1911.08736v1` - [abs](http://arxiv.org/abs/1911.08736v1) - [pdf](http://arxiv.org/pdf/1911.08736v1)

> The emergence of digital pathology has opened new horizons for histopathology and cytology. Artificial-intelligence algorithms are able to operate on digitized slides to assist pathologists with diagnostic tasks. Whereas machine learning involving classification and segmentation methods have obvious benefits for image analysis in pathology, image search represents a fundamental shift in computational pathology. Matching the pathology of new patients with already diagnosed and curated cases offers pathologist a novel approach to improve diagnostic accuracy through visual inspection of similar cases and computational majority vote for consensus building. In this study, we report the results from searching the largest public repository (The Cancer Genome Atlas [TCGA] program by National Cancer Institute, USA) of whole slide images from almost 11,000 patients depicting different types of malignancies. For the first time, we successfully indexed and searched almost 30,000 high-resolution digitized slides constituting 16 terabytes of data comprised of 20 million 1000x1000 pixels image patches. The TCGA image database covers 25 anatomic sites and contains 32 cancer subtypes. High-performance storage and GPU power were employed for experimentation. The results were assessed with conservative "majority voting" to build consensus for subtype diagnosis through vertical search and demonstrated high accuracy values for both frozen sections slides (e.g., bladder urothelial carcinoma 93%, kidney renal clear cell carcinoma 97%, and ovarian serous cystadenocarcinoma 99%) and permanent histopathology slides (e.g., prostate adenocarcinoma 98%, skin cutaneous melanoma 99%, and thymoma 100%). The key finding of this validation study was that computational consensus appears to be possible for rendering diagnoses if a sufficiently large number of searchable cases are available for each cancer subtype.

</details>

<details>

<summary>2019-11-20 07:16:16 - SemanticZ at SemEval-2016 Task 3: Ranking Relevant Answers in Community Question Answering Using Semantic Similarity Based on Fine-tuned Word Embeddings</summary>

- *Todor Mihaylov, Preslav Nakov*

- `1911.08743v1` - [abs](http://arxiv.org/abs/1911.08743v1) - [pdf](http://arxiv.org/pdf/1911.08743v1)

> We describe our system for finding good answers in a community forum, as defined in SemEval-2016, Task 3 on Community Question Answering. Our approach relies on several semantic similarity features based on fine-tuned word embeddings and topics similarities. In the main Subtask C, our primary submission was ranked third, with a MAP of 51.68 and accuracy of 69.94. In Subtask A, our primary submission was also third, with MAP of 77.58 and accuracy of 73.39.

</details>

<details>

<summary>2019-11-20 07:34:49 - Yottixel -- An Image Search Engine for Large Archives of Histopathology Whole Slide Images</summary>

- *S. Kalra, C. Choi, S. Shah, L. Pantanowitz, H. R. Tizhoosh*

- `1911.08748v1` - [abs](http://arxiv.org/abs/1911.08748v1) - [pdf](http://arxiv.org/pdf/1911.08748v1)

> With the emergence of digital pathology, searching for similar images in large archives has gained considerable attention. Image retrieval can provide pathologists with unprecedented access to the evidence embodied in already diagnosed and treated cases from the past. This paper proposes a search engine specialized for digital pathology, called Yottixel, a portmanteau for "one yotta pixel," alluding to the big-data nature of histopathology images. The most impressive characteristic of Yottixel is its ability to represent whole slide images (WSIs) in a compact manner. Yottixel can perform millions of searches in real-time with a high search accuracy and low storage profile. Yottixel uses an intelligent indexing algorithm capable of representing WSIs with a mosaic of patches by converting them into a small number of methodically extracted barcodes, called "Bunch of Barcodes" (BoB), the most prominent performance enabler of Yottixel. The performance of the prototype platform is qualitatively tested using 300 WSIs from the University of Pittsburgh Medical Center (UPMC) and 2,020 WSIs from The Cancer Genome Atlas Program (TCGA) provided by the National Cancer Institute. Both datasets amount to more than 4,000,000 patches of 1000x1000 pixels. We report three sets of experiments that show that Yottixel can accurately retrieve organs and malignancies, and its semantic ordering shows good agreement with the subjective evaluation of human observers.

</details>

<details>

<summary>2019-11-20 07:36:45 - Unsupervised Domain Adaptation by Optical Flow Augmentation in Semantic Segmentation</summary>

- *Oluwafemi Azeez*

- `1911.09652v1` - [abs](http://arxiv.org/abs/1911.09652v1) - [pdf](http://arxiv.org/pdf/1911.09652v1)

> It is expensive to generate real-life image labels and there is a domain gap between real-life and simulated images, hence a model trained on the latter cannot adapt to the former. Solving this can totally eliminate the need for labeling real-life datasets completely. Class balanced self-training is one of the existing techniques that attempt to reduce the domain gap. Moreover, augmenting RGB with flow maps has improved performance in simple semantic segmentation and geometry is preserved across domains. Hence, by augmenting images with dense optical flow map, domain adaptation in semantic segmentation can be improved.

</details>

<details>

<summary>2019-11-20 08:29:10 - Paraphrasing Verbs for Noun Compound Interpretation</summary>

- *Preslav Nakov*

- `1911.08762v1` - [abs](http://arxiv.org/abs/1911.08762v1) - [pdf](http://arxiv.org/pdf/1911.08762v1)

> An important challenge for the automatic analysis of English written text is the abundance of noun compounds: sequences of nouns acting as a single noun. In our view, their semantics is best characterized by the set of all possible paraphrasing verbs, with associated weights, e.g., malaria mosquito is carry (23), spread (16), cause (12), transmit (9), etc. Using Amazon's Mechanical Turk, we collect paraphrasing verbs for 250 noun-noun compounds previously proposed in the linguistic literature, thus creating a valuable resource for noun compound interpretation. Using these verbs, we further construct a dataset of pairs of sentences representing a special kind of textual entailment task, where a binary decision is to be made about whether an expression involving a verb and two nouns can be transformed into a noun compound, while preserving the sentence meaning.

</details>

<details>

<summary>2019-11-20 08:53:57 - Towards FAIR protocols and workflows: The OpenPREDICT case study</summary>

- *Remzi Celebi, Joao Rebelo Moreira, Ahmed A. Hassan, Sandeep Ayyar, Lars Ridder, Tobias Kuhn, Michel Dumontier*

- `1911.09531v1` - [abs](http://arxiv.org/abs/1911.09531v1) - [pdf](http://arxiv.org/pdf/1911.09531v1)

> It is essential for the advancement of science that scientists and researchers share, reuse and reproduce workflows and protocols used by others. The FAIR principles are a set of guidelines that aim to maximize the value and usefulness of research data, and emphasize a number of important points regarding the means by which digital objects are found and reused by others. The question of how to apply these principles not just to the static input and output data but also to the dynamic workflows and protocols that consume and produce them is still under debate and poses a number of challenges. In this paper we describe our inclusive and overarching approach to apply the FAIR principles to workflows and protocols and demonstrate its benefits. We apply and evaluate our approach on a case study that consists of making the PREDICT workflow, a highly cited drug repurposing workflow, open and FAIR. This includes FAIRification of the involved datasets, as well as applying semantic technologies to represent and store data about the detailed versions of the general protocol, of the concrete workflow instructions, and of their execution traces. A semantic model was proposed to better address these specific requirements and were evaluated by answering competency questions. This semantic model consists of classes and relations from a number of existing ontologies, including Workflow4ever, PROV, EDAM, and BPMN. This allowed us then to formulate and answer new kinds of competency questions. Our evaluation shows the high degree to which our FAIRified OpenPREDICT workflow now adheres to the FAIR principles and the practicality and usefulness of being able to answer our new competency questions.

</details>

<details>

<summary>2019-11-20 10:15:07 - Solving Online Threat Screening Games using Constrained Action Space Reinforcement Learning</summary>

- *Sanket Shah, Arunesh Sinha, Pradeep Varakantham, Andrew Perrault, Milind Tambe*

- `1911.08799v1` - [abs](http://arxiv.org/abs/1911.08799v1) - [pdf](http://arxiv.org/pdf/1911.08799v1)

> Large-scale screening for potential threats with limited resources and capacity for screening is a problem of interest at airports, seaports, and other ports of entry. Adversaries can observe screening procedures and arrive at a time when there will be gaps in screening due to limited resource capacities. To capture this game between ports and adversaries, this problem has been previously represented as a Stackelberg game, referred to as a Threat Screening Game (TSG). Given the significant complexity associated with solving TSGs and uncertainty in arrivals of customers, existing work has assumed that screenees arrive and are allocated security resources at the beginning of the time window. In practice, screenees such as airport passengers arrive in bursts correlated with flight time and are not bound by fixed time windows. To address this, we propose an online threat screening model in which screening strategy is determined adaptively as a passenger arrives while satisfying a hard bound on acceptable risk of not screening a threat. To solve the online problem with a hard bound on risk, we formulate it as a Reinforcement Learning (RL) problem with constraints on the action space (hard bound on risk). We provide a novel way to efficiently enforce linear inequality constraints on the action output in Deep Reinforcement Learning. We show that our solution allows us to significantly reduce screenee wait time while guaranteeing a bound on risk.

</details>

<details>

<summary>2019-11-20 11:14:07 - Zero-Shot Semantic Parsing for Instructions</summary>

- *Ofer Givoli, Roi Reichart*

- `1911.08827v1` - [abs](http://arxiv.org/abs/1911.08827v1) - [pdf](http://arxiv.org/pdf/1911.08827v1)

> We consider a zero-shot semantic parsing task: parsing instructions into compositional logical forms, in domains that were not seen during training. We present a new dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application's initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction. We introduce a new training algorithm that aims to train a semantic parser on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain. We integrate our algorithm into the floating parser of Pasupat and Liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation. Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser.

</details>

<details>

<summary>2019-11-20 11:23:17 - A Conditional Perspective for Iterated Belief Contraction</summary>

- *Kai Sauerwald, Gabriele Kern-Isberner, Christoph Beierle*

- `1911.08833v1` - [abs](http://arxiv.org/abs/1911.08833v1) - [pdf](http://arxiv.org/pdf/1911.08833v1)

> According to Boutillier, Darwiche, Pearl and others, principles for iterated revision can be characterised in terms of changing beliefs about conditionals. For iterated contraction a similar formulation is not known. This is especially because for iterated belief change the connection between revision and contraction via the Levi and Harper identity is not straightforward, and therefore, characterisation results do not transfer easily between iterated revision and contraction. In this article, we develop an axiomatisation of iterated contraction in terms of changing conditional beliefs. We prove that the new set of postulates conforms semantically to the class of operators like the ones given by Konieczny and Pino P\'erez for iterated contraction.

</details>

<details>

<summary>2019-11-20 11:29:47 - Table-Of-Contents generation on contemporary documents</summary>

- *Najah-Imane Bentabet, Rémi Juge, Sira Ferradans*

- `1911.08836v1` - [abs](http://arxiv.org/abs/1911.08836v1) - [pdf](http://arxiv.org/pdf/1911.08836v1)

> The generation of precise and detailed Table-Of-Contents (TOC) from a document is a problem of major importance for document understanding and information extraction. Despite its importance, it is still a challenging task, especially for non-standardized documents with rich layout information such as commercial documents. In this paper, we present a new neural-based pipeline for TOC generation applicable to any searchable document. Unlike previous methods, we do not use semantic labeling nor assume the presence of parsable TOC pages in the document. Moreover, we analyze the influence of using external knowledge encoded as a template. We empirically show that this approach is only useful in a very low resource environment. Finally, we propose a new domain-specific data set that sheds some light on the difficulties of TOC generation in real-world documents. The proposed method shows better performance than the state-of-the-art on a public data set and on the newly released data set.

</details>

<details>

<summary>2019-11-20 19:25:16 - On First-Order Model-Based Reasoning</summary>

- *Maria Paola Bonacina, Ulrich Furbach, Viorica Sofronie-Stokkermans*

- `1502.02535v4` - [abs](http://arxiv.org/abs/1502.02535v4) - [pdf](http://arxiv.org/pdf/1502.02535v4)

> Reasoning semantically in first-order logic is notoriously a challenge. This paper surveys a selection of semantically-guided or model-based methods that aim at meeting aspects of this challenge. For first-order logic we touch upon resolution-based methods, tableaux-based methods, DPLL-inspired methods, and we give a preview of a new method called SGGS, for Semantically-Guided Goal-Sensitive reasoning. For first-order theories we highlight hierarchical and locality-based methods, concluding with the recent Model-Constructing satisfiability calculus.

</details>

<details>

<summary>2019-11-20 19:49:27 - ID-aware Quality for Set-based Person Re-identification</summary>

- *Xinshao Wang, Elyor Kodirov, Yang Hua, Neil M. Robertson*

- `1911.09143v1` - [abs](http://arxiv.org/abs/1911.09143v1) - [pdf](http://arxiv.org/pdf/1911.09143v1)

> Set-based person re-identification (SReID) is a matching problem that aims to verify whether two sets are of the same identity (ID). Existing SReID models typically generate a feature representation per image and aggregate them to represent the set as a single embedding. However, they can easily be perturbed by noises--perceptually/semantically low quality images--which are inevitable due to imperfect tracking/detection systems, or overfit to trivial images. In this work, we present a novel and simple solution to this problem based on ID-aware quality that measures the perceptual and semantic quality of images guided by their ID information. Specifically, we propose an ID-aware Embedding that consists of two key components: (1) Feature learning attention that aims to learn robust image embeddings by focusing on 'medium' hard images. This way it can prevent overfitting to trivial images, and alleviate the influence of outliers. (2) Feature fusion attention is to fuse image embeddings in the set to obtain the set-level embedding. It ignores noisy information and pays more attention to discriminative images to aggregate more discriminative information. Experimental results on four datasets show that our method outperforms state-of-the-art approaches despite the simplicity of our approach.

</details>

<details>

<summary>2019-11-21 02:09:16 - ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</summary>

- *Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, Haifeng Wang*

- `1907.12412v2` - [abs](http://arxiv.org/abs/1907.12412v2) - [pdf](http://arxiv.org/pdf/1907.12412v2)

> Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.

</details>

<details>

<summary>2019-11-21 02:30:31 - Semantic Segmentation of Thigh Muscle using 2.5D Deep Learning Network Trained with Limited Datasets</summary>

- *Hasnine Haque, Masahiro Hashimoto, Nozomu Uetake, Masahiro Jinzaki*

- `1911.09249v1` - [abs](http://arxiv.org/abs/1911.09249v1) - [pdf](http://arxiv.org/pdf/1911.09249v1)

> Purpose: We propose a 2.5D deep learning neural network (DLNN) to automatically classify thigh muscle into 11 classes and evaluate its classification accuracy over 2D and 3D DLNN when trained with limited datasets. Enables operator invariant quantitative assessment of the thigh muscle volume change with respect to the disease progression. Materials and methods: Retrospective datasets consist of 48 thigh volume (TV) cropped from CT DICOM images. Cropped volumes were aligned with femur axis and resample in 2 mm voxel-spacing. Proposed 2.5D DLNN consists of three 2D U-Net trained with axial, coronal and sagittal muscle slices respectively. A voting algorithm was used to combine the output of U-Nets to create final segmentation. 2.5D U-Net was trained on PC with 38 TV and the remaining 10 TV were used to evaluate segmentation accuracy of 10 classes within Thigh. The result segmentation of both left and right thigh were de-cropped to original CT volume space. Finally, segmentation accuracies were compared between proposed DLNN and 2D/3D U-Net. Results: Average segmentation DSC score accuracy of all classes with 2.5D U-Net as 91.18% and Average Surface distance (ASD) accuracy as 0.84 mm. We found, mean DSC score for 2D U-Net was 3.3% lower than the that of 2.5D U-Net and mean DSC score of 3D U-Net was 5.7% lower than that of 2.5D U-Net when trained with same datasets. Conclusion: We achieved a faster computationally efficient and automatic segmentation of thigh muscle into 11 classes with reasonable accuracy. Enables quantitative evaluation of muscle atrophy with disease progression.

</details>

<details>

<summary>2019-11-21 03:52:50 - Attention-Informed Mixed-Language Training for Zero-shot Cross-lingual Task-oriented Dialogue Systems</summary>

- *Zihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng Xu, Pascale Fung*

- `1911.09273v1` - [abs](http://arxiv.org/abs/1911.09273v1) - [pdf](http://arxiv.org/pdf/1911.09273v1)

> Recently, data-driven task-oriented dialogue systems have achieved promising performance in English. However, developing dialogue systems that support low-resource languages remains a long-standing challenge due to the absence of high-quality data. In order to circumvent the expensive and time-consuming data collection, we introduce Attention-Informed Mixed-Language Training (MLT), a novel zero-shot adaptation method for cross-lingual task-oriented dialogue systems. It leverages very few task-related parallel word pairs to generate code-switching sentences for learning the inter-lingual semantics across languages. Instead of manually selecting the word pairs, we propose to extract source words based on the scores computed by the attention layer of a trained English task-related model and then generate word pairs using existing bilingual dictionaries. Furthermore, intensive experiments with different cross-lingual embeddings demonstrate the effectiveness of our approach. Finally, with very few word pairs, our model achieves significant zero-shot adaptation performance improvements in both cross-lingual dialogue state tracking and natural language understanding (i.e., intent detection and slot filling) tasks compared to the current state-of-the-art approaches, which utilize a much larger amount of bilingual data.

</details>

<details>

<summary>2019-11-21 08:30:43 - Exploring Self-Supervised Regularization for Supervised and Semi-Supervised Learning</summary>

- *Phi Vu Tran*

- `1906.10343v2` - [abs](http://arxiv.org/abs/1906.10343v2) - [pdf](http://arxiv.org/pdf/1906.10343v2)

> Recent advances in semi-supervised learning have shown tremendous potential in overcoming a major barrier to the success of modern machine learning algorithms: access to vast amounts of human-labeled training data. Previous algorithms based on consistency regularization can harness the abundance of unlabeled data to produce impressive results on a number of semi-supervised benchmarks, approaching the performance of strong supervised baselines using only a fraction of the available labeled data. In this work, we challenge the long-standing success of consistency regularization by introducing self-supervised regularization as the basis for combining semantic feature representations from unlabeled data. We perform extensive comparative experiments to demonstrate the effectiveness of self-supervised regularization for supervised and semi-supervised image classification on SVHN, CIFAR-10, and CIFAR-100 benchmark datasets. We present two main results: (1) models augmented with self-supervised regularization significantly improve upon traditional supervised classifiers without the need for unlabeled data; (2) together with unlabeled data, our models yield semi-supervised performance competitive with, and in many cases exceeding, prior state-of-the-art consistency baselines. Lastly, our models have the practical utility of being efficiently trained end-to-end and require no additional hyper-parameters to tune for optimal performance beyond the standard set for training neural networks. Reference code and data are available at https://github.com/vuptran/sesemi

</details>

<details>

<summary>2019-11-21 09:20:24 - Schemaless Queries over Document Tables with Dependencies</summary>

- *Mustafa Canim, Cristina Cornelio, Arun Iyengar, Ryan Musa, Mariano Rodrigez Muro*

- `1911.09356v1` - [abs](http://arxiv.org/abs/1911.09356v1) - [pdf](http://arxiv.org/pdf/1911.09356v1)

> Unstructured enterprise data such as reports, manuals and guidelines often contain tables. The traditional way of integrating data from these tables is through a two-step process of table detection/extraction and mapping the table layouts to an appropriate schema. This can be an expensive process. In this paper we show that by using semantic technologies (RDF/SPARQL and database dependencies) paired with a simple but powerful way to transform tables with non-relational layouts, it is possible to offer query answering services over these tables with minimal manual work or domain-specific mappings. Our method enables users to exploit data in tables embedded in documents with little effort, not only for simple retrieval queries, but also for structured queries that require joining multiple interrelated tables.

</details>

<details>

<summary>2019-11-21 10:13:13 - The Next 700 Relational Program Logics</summary>

- *Kenji Maillard, Catalin Hritcu, Exequiel Rivas, Antoine Van Muylder*

- `1907.05244v3` - [abs](http://arxiv.org/abs/1907.05244v3) - [pdf](http://arxiv.org/pdf/1907.05244v3)

> We propose the first framework for defining relational program logics for arbitrary monadic effects. The framework is embedded within a relational dependent type theory and is highly expressive. At the semantic level, we provide an algebraic presentation of relational specifications as a class of relative monads, and link computations and specifications by introducing relational effect observations, which map pairs of monadic computations to relational specifications in a way that respects the algebraic structure. For an arbitrary relational effect observation, we generically define the core of a sound relational program logic, and explain how to complete it to a full-fledged logic for the monadic effect at hand. We show that this generic framework can be used to define relational program logics for effects as diverse as state, input-output, nondeterminism, and discrete probabilities. We, moreover, show that by instantiating our framework with state and unbounded iteration we can embed a variant of Benton's Relational Hoare Logic, and also sketch how to reconstruct Relational Hoare Type Theory. Finally, we identify and overcome conceptual challenges that prevented previous relational program logics from properly dealing with control effects, and are the first to provide a relational program logic for exceptions.

</details>

<details>

<summary>2019-11-21 11:02:48 - Assessing Cyber-Physical Security in Industrial Control Systems</summary>

- *Martín Barrère, Chris Hankin, Demetrios G. Eliades, Nicolas Nicolau, Thomas Parisini*

- `1911.09404v1` - [abs](http://arxiv.org/abs/1911.09404v1) - [pdf](http://arxiv.org/pdf/1911.09404v1)

> Over the last years, Industrial Control Systems (ICS) have become increasingly exposed to a wide range of cyber-physical threats. Efficient models and techniques able to capture their complex structure and identify critical cyber-physical components are therefore essential. AND/OR graphs have proven very useful in this context as they are able to semantically grasp intricate logical interdependencies among ICS components. However, identifying critical nodes in AND/OR graphs is an NP-complete problem. In addition, ICS settings normally involve various cyber and physical security measures that simultaneously protect multiple ICS components in overlapping manners, which makes this problem even harder. In this paper, we present an extended security metric based on AND/OR hypergraphs which efficiently identifies the set of critical ICS components and security measures that should be compromised, with minimum cost (effort) for an attacker, in order to disrupt the operation of vital ICS assets. Our approach relies on MAX-SAT techniques, which we have incorporated in META4ICS, a Java-based security metric analyser for ICS. We also provide a thorough performance evaluation that shows the feasibility of our method. Finally, we illustrate our methodology through a case study in which we analyse the security posture of a realistic Water Transport Network (WTN).

</details>

<details>

<summary>2019-11-21 13:06:28 - Certain Answers to a SPARQL Query over a Knowledge Base (extended version)</summary>

- *Julien Corman, Guohui Xiao*

- `1911.02668v3` - [abs](http://arxiv.org/abs/1911.02668v3) - [pdf](http://arxiv.org/pdf/1911.02668v3)

> Ontology-Mediated Query Answering (OMQA) is a well-established framework to answer queries over an RDFS or OWL Knowledge Base (KB). OMQA was originally designed for unions of conjunctive queries (UCQs), and based on certain answers. More recently, OMQA has been extended to SPARQL queries, but to our knowledge, none of the efforts made in this direction (either in the literature, or the so-called SPARQL entailment regimes) is able to capture both certain answers for UCQs and the standard interpretation of SPARQL over a plain graph. We formalize these as requirements to be met by any semantics aiming at conciliating certain answers and SPARQL answers, and define three additional requirements, which generalize to KBs some basic properties of SPARQL answers. Then we show that a semantics can be defined that satisfies all requirements for SPARQL queries with SELECT, UNION, and OPTIONAL, and for DLs with the canonical model property. We also investigate combined complexity for query answering under such a semantics over DL-Lite R KBs. In particular, we show for different fragments of SPARQL that known upper-bounds for query answering over a plain graph are matched.

</details>

<details>

<summary>2019-11-21 14:09:33 - What Do You Mean `Why?': Resolving Sluices in Conversations</summary>

- *Victor Petrén Bach Hansen, Anders Søgaard*

- `1911.09478v1` - [abs](http://arxiv.org/abs/1911.09478v1) - [pdf](http://arxiv.org/pdf/1911.09478v1)

> In conversation, we often ask one-word questions such as `Why?' or `Who?'. Such questions are typically easy for humans to answer, but can be hard for computers, because their resolution requires retrieving both the right semantic frames and the right arguments from context. This paper introduces the novel ellipsis resolution task of resolving such one-word questions, referred to as sluices in linguistics. We present a crowd-sourced dataset containing annotations of sluices from over 4,000 dialogues collected from conversational QA datasets, as well as a series of strong baseline architectures.

</details>

<details>

<summary>2019-11-21 18:32:23 - Story Realization: Expanding Plot Events into Sentences</summary>

- *Prithviraj Ammanabrolu, Ethan Tien, Wesley Cheung, Zhaochen Luo, William Ma, Lara J. Martin, Mark O. Riedl*

- `1909.03480v2` - [abs](http://arxiv.org/abs/1909.03480v2) - [pdf](http://arxiv.org/pdf/1909.03480v2)

> Neural network based approaches to automated story plot generation attempt to learn how to generate novel plots from a corpus of natural language plot summaries. Prior work has shown that a semantic abstraction of sentences called events improves neural plot generation and and allows one to decompose the problem into: (1) the generation of a sequence of events (event-to-event) and (2) the transformation of these events into natural language sentences (event-to-sentence). However, typical neural language generation approaches to event-to-sentence can ignore the event details and produce grammatically-correct but semantically-unrelated sentences. We present an ensemble-based model that generates natural language guided by events.We provide results---including a human subjects study---for a full end-to-end automated story generation system showing that our method generates more coherent and plausible stories than baseline approaches.

</details>

<details>

<summary>2019-11-21 20:48:53 - Detecting semantic anomalies</summary>

- *Faruk Ahmed, Aaron Courville*

- `1908.04388v3` - [abs](http://arxiv.org/abs/1908.04388v3) - [pdf](http://arxiv.org/pdf/1908.04388v3)

> We critically appraise the recent interest in out-of-distribution (OOD) detection and question the practical relevance of existing benchmarks. While the currently prevalent trend is to consider different datasets as OOD, we argue that out-distributions of practical interest are ones where the distinction is semantic in nature for a specified context, and that evaluative tasks should reflect this more closely. Assuming a context of object recognition, we recommend a set of benchmarks, motivated by practical applications. We make progress on these benchmarks by exploring a multi-task learning based approach, showing that auxiliary objectives for improved semantic awareness result in improved semantic anomaly detection, with accompanying generalization benefits.

</details>

<details>

<summary>2019-11-21 21:42:38 - An Innovative Approach to Addressing Childhood Obesity: A Knowledge-Based Infrastructure for Supporting Multi-Stakeholder Partnership Decision-Making in Quebec, Canada</summary>

- *Nii Antiaye Addy, Arash Shaban-Nejad, David L. Buckeridge, Laurette Dubé*

- `1911.09763v1` - [abs](http://arxiv.org/abs/1911.09763v1) - [pdf](http://arxiv.org/pdf/1911.09763v1)

> The purpose of this paper is to describe and analyze the development of a knowledge-based infrastructure to support MSP decision-making processes. The paper emerged from a study to define specifications for a knowledge-based infrastructure to provide decision support for community-level MSPs in the Canadian province of Quebec. As part of the study, a process assessment was conducted to understand the needs of communities as they collect, organize, and analyze data to make decisions about their priorities. The result of this process is a portrait, which is an epidemiological profile of health and nutrition in their community. Portraits inform strategic planning and development of interventions and are used to assess the impact of interventions. Our key findings indicate ambiguities and disagreement among MSP decision-makers regarding causal relationships between actions and outcomes, and the relevant data needed for making decisions. MSP decision-makers expressed a desire for easy-to-use tools that facilitate the collection, organization, synthesis, and analysis of data, to enable decision-making in a timely manner. Findings inform conceptual modeling and ontological analysis to capture the domain knowledge and specify relationships between actions and outcomes. This modeling and analysis provide the foundation for an ontology, encoded using OWL 2 Web Ontology Language. The ontology is developed to provide semantic support for the MSP process, defining objectives, strategies, actions, indicators, and data sources. In the future, software interacting with the ontology can facilitate interactive browsing by decision-makers in the MSP in the form of concepts, instances, relationships, and axioms. Our ontology also facilitates the integration and interpretation of community data and can help in managing semantic interoperability between different knowledge sources.

</details>

<details>

<summary>2019-11-22 00:20:31 - Infusing Knowledge into the Textual Entailment Task Using Graph Convolutional Networks</summary>

- *Pavan Kapanipathi, Veronika Thost, Siva Sankalp Patel, Spencer Whitehead, Ibrahim Abdelaziz, Avinash Balakrishnan, Maria Chang, Kshitij Fadnis, Chulaka Gunasekara, Bassem Makni, Nicholas Mattei, Kartik Talamadupula, Achille Fokoue*

- `1911.02060v2` - [abs](http://arxiv.org/abs/1911.02060v2) - [pdf](http://arxiv.org/pdf/1911.02060v2)

> Textual entailment is a fundamental task in natural language processing. Most approaches for solving the problem use only the textual content present in training data. A few approaches have shown that information from external knowledge sources like knowledge graphs (KGs) can add value, in addition to the textual content, by providing background knowledge that may be critical for a task. However, the proposed models do not fully exploit the information in the usually large and noisy KGs, and it is not clear how it can be effectively encoded to be useful for entailment. We present an approach that complements text-based entailment models with information from KGs by (1) using Personalized PageR- ank to generate contextual subgraphs with reduced noise and (2) encoding these subgraphs using graph convolutional networks to capture KG structure. Our technique extends the capability of text models exploiting structural and semantic information found in KGs. We evaluate our approach on multiple textual entailment datasets and show that the use of external knowledge helps improve prediction accuracy. This is particularly evident in the challenging BreakingNLI dataset, where we see an absolute improvement of 5-20% over multiple text-based entailment models.

</details>

<details>

<summary>2019-11-22 01:36:06 - Making High-Performance Robots Safe and Easy to Use for an Introduction to Computing</summary>

- *Joseph Spitzer, Joydeep Biswas, Arjun Guha*

- `1909.03110v2` - [abs](http://arxiv.org/abs/1909.03110v2) - [pdf](http://arxiv.org/pdf/1909.03110v2)

> Robots are a popular platform for introducing computing and artificial intelligence to novice programmers. However, programming state-of-the-art robots is very challenging, and requires knowledge of concurrency, operation safety, and software engineering skills, which can take years to teach. In this paper, we present an approach to introducing computing that allows students to safely and easily program high-performance robots. We develop a platform for students to program RoboCup Small Size League robots using JavaScript. The platform 1) ensures physical safety at several levels of abstraction, 2) allows students to program robots using the JavaScript in the browser, without the need to install software, and 3) presents a simplified JavaScript semantics that shields students from confusing language features. We discuss our experience running a week-long workshop using this platform, and analyze over 3,000 student-written program revisions to provide empirical evidence that our approach does help students.

</details>

<details>

<summary>2019-11-22 04:14:31 - A Discrete CVAE for Response Generation on Short-Text Conversation</summary>

- *Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Guodong Zhou, Shuming Shi*

- `1911.09845v1` - [abs](http://arxiv.org/abs/1911.09845v1) - [pdf](http://arxiv.org/pdf/1911.09845v1)

> Neural conversation models such as encoder-decoder models are easy to generate bland and generic responses. Some researchers propose to use the conditional variational autoencoder(CVAE) which maximizes the lower bound on the conditional log-likelihood on a continuous latent variable. With different sampled la-tent variables, the model is expected to generate diverse responses. Although the CVAE-based models have shown tremendous potential, their improvement of generating high-quality responses is still unsatisfactory. In this paper, we introduce a discrete latent variable with an explicit semantic meaning to improve the CVAE on short-text conversation. A major advantage of our model is that we can exploit the semantic distance between the latent variables to maintain good diversity between the sampled latent variables. Accordingly, we pro-pose a two-stage sampling approach to enable efficient diverse variable selection from a large latent space assumed in the short-text conversation task. Experimental results indicate that our model outperforms various kinds of generation models under both automatic and human evaluations and generates more diverse and in-formative responses.

</details>

<details>

<summary>2019-11-22 06:38:42 - Neuron Interaction Based Representation Composition for Neural Machine Translation</summary>

- *Jian Li, Xing Wang, Baosong Yang, Shuming Shi, Michael R. Lyu, Zhaopeng Tu*

- `1911.09877v1` - [abs](http://arxiv.org/abs/1911.09877v1) - [pdf](http://arxiv.org/pdf/1911.09877v1)

> Recent NLP studies reveal that substantial linguistic information can be attributed to single neurons, i.e., individual dimensions of the representation vectors. We hypothesize that modeling strong interactions among neurons helps to better capture complex information by composing the linguistic properties embedded in individual neurons. Starting from this intuition, we propose a novel approach to compose representations learned by different components in neural machine translation (e.g., multi-layer networks or multi-head attention), based on modeling strong interactions among neurons in the representation vectors. Specifically, we leverage bilinear pooling to model pairwise multiplicative interactions among individual neurons, and a low-rank approximation to make the model computationally feasible. We further propose extended bilinear pooling to incorporate first-order representations. Experiments on WMT14 English-German and English-French translation tasks show that our model consistently improves performances over the SOTA Transformer baseline. Further analyses demonstrate that our approach indeed captures more syntactic and semantic information as expected.

</details>

<details>

<summary>2019-11-22 11:12:48 - Instance Cross Entropy for Deep Metric Learning</summary>

- *Xinshao Wang, Elyor Kodirov, Yang Hua, Neil Robertson*

- `1911.09976v1` - [abs](http://arxiv.org/abs/1911.09976v1) - [pdf](http://arxiv.org/pdf/1911.09976v1)

> Loss functions play a crucial role in deep metric learning thus a variety of them have been proposed. Some supervise the learning process by pairwise or tripletwise similarity constraints while others take advantage of structured similarity information among multiple data points. In this work, we approach deep metric learning from a novel perspective. We propose instance cross entropy (ICE) which measures the difference between an estimated instance-level matching distribution and its ground-truth one. ICE has three main appealing properties. Firstly, similar to categorical cross entropy (CCE), ICE has clear probabilistic interpretation and exploits structured semantic similarity information for learning supervision. Secondly, ICE is scalable to infinite training data as it learns on mini-batches iteratively and is independent of the training set size. Thirdly, motivated by our relative weight analysis, seamless sample reweighting is incorporated. It rescales samples' gradients to control the differentiation degree over training examples instead of truncating them by sample mining. In addition to its simplicity and intuitiveness, extensive experiments on three real-world benchmarks demonstrate the superiority of ICE.

</details>

<details>

<summary>2019-11-22 12:20:44 - Anaphora Resolution in Dialogue Systems for South Asian Languages</summary>

- *Vinay Annam, Nikhil Koditala, Radhika Mamidi*

- `1911.09994v1` - [abs](http://arxiv.org/abs/1911.09994v1) - [pdf](http://arxiv.org/pdf/1911.09994v1)

> Anaphora resolution is a challenging task which has been the interest of NLP researchers for a long time. Traditional resolution techniques like eliminative constraints and weighted preferences were successful in many languages. However, they are ineffective in free word order languages like most SouthAsian languages.Heuristic and rule-based techniques were typical in these languages, which are constrained to context and domain.In this paper, we venture a new strategy us-ing neural networks for resolving anaphora in human-human dialogues. The architecture chiefly consists of three components, a shallow parser for extracting features, a feature vector generator which produces the word embed-dings, and a neural network model which will predict the antecedent mention of an anaphora.The system has been trained and tested on Telugu conversation corpus we generated. Given the advantage of the semantic information in word embeddings and appending actor, gender, number, person and part of plural features the model has reached an F1-score of 86.

</details>

<details>

<summary>2019-11-22 15:51:08 - HAL: Improved Text-Image Matching by Mitigating Visual Semantic Hubs</summary>

- *Fangyu Liu, Rongtian Ye, Xun Wang, Shuaipeng Li*

- `1911.10097v1` - [abs](http://arxiv.org/abs/1911.10097v1) - [pdf](http://arxiv.org/pdf/1911.10097v1)

> The hubness problem widely exists in high-dimensional embedding space and is a fundamental source of error for cross-modal matching tasks. In this work, we study the emergence of hubs in Visual Semantic Embeddings (VSE) with application to text-image matching. We analyze the pros and cons of two widely adopted optimization objectives for training VSE and propose a novel hubness-aware loss function (HAL) that addresses previous methods' defects. Unlike (Faghri et al.2018) which simply takes the hardest sample within a mini-batch, HAL takes all samples into account, using both local and global statistics to scale up the weights of "hubs". We experiment our method with various configurations of model architectures and datasets. The method exhibits exceptionally good robustness and brings consistent improvement on the task of text-image matching across all settings. Specifically, under the same model architectures as (Faghri et al. 2018) and (Lee at al. 2018), by switching only the learning objective, we report a maximum R@1improvement of 7.4% on MS-COCO and 8.3% on Flickr30k.

</details>

<details>

<summary>2019-11-22 16:14:21 - Comparison of UNet, ENet, and BoxENet for Segmentation of Mast Cells in Scans of Histological Slices</summary>

- *Alexander Karimov, Artem Razumov, Ruslana Manbatchurina, Ksenia Simonova, Irina Donets, Anastasia Vlasova, Yulia Khramtsova, Konstantin Ushenin*

- `1909.06840v3` - [abs](http://arxiv.org/abs/1909.06840v3) - [pdf](http://arxiv.org/pdf/1909.06840v3)

> Deep neural networks show high accuracy in theproblem of semantic and instance segmentation of biomedicaldata. However, this approach is computationally expensive. Thecomputational cost may be reduced with network simplificationafter training or choosing the proper architecture, which providessegmentation with less accuracy but does it much faster. In thepresent study, we analyzed the accuracy and performance ofUNet and ENet architectures for the problem of semantic imagesegmentation. In addition, we investigated the ENet architecture by replacing of some convolution layers with box-convolutionlayers. The analysis performed on the original dataset consisted of histology slices with mast cells. These cells provide a region forsegmentation with different types of borders, which vary fromclearly visible to ragged. ENet was less accurate than UNet byonly about 1-2%, but ENet performance was 8-15 times faster than UNet one.

</details>

<details>

<summary>2019-11-22 16:48:10 - CRUR: Coupled-Recurrent Unit for Unification, Conceptualization and Context Capture for Language Representation -- A Generalization of Bi Directional LSTM</summary>

- *Chiranjib Sur*

- `1911.10132v1` - [abs](http://arxiv.org/abs/1911.10132v1) - [pdf](http://arxiv.org/pdf/1911.10132v1)

> In this work we have analyzed a novel concept of sequential binding based learning capable network based on the coupling of recurrent units with Bayesian prior definition. The coupling structure encodes to generate efficient tensor representations that can be decoded to generate efficient sentences and can describe certain events. These descriptions are derived from structural representations of visual features of images and media. An elaborated study of the different types of coupling recurrent structures are studied and some insights of their performance are provided. Supervised learning performance for natural language processing is judged based on statistical evaluations, however, the truth is perspective, and in this case the qualitative evaluations reveal the real capability of the different architectural strengths and variations. Bayesian prior definition of different embedding helps in better characterization of the sentences based on the natural language structure related to parts of speech and other semantic level categorization in a form which is machine interpret-able and inherits the characteristics of the Tensor Representation binding and unbinding based on the mutually orthogonality. Our approach has surpassed some of the existing basic works related to image captioning.

</details>

<details>

<summary>2019-11-22 18:29:19 - Topical Phrase Extraction from Clinical Reports by Incorporating both Local and Global Context</summary>

- *Gabriele Pergola, Yulan He, David Lowe*

- `1911.10180v1` - [abs](http://arxiv.org/abs/1911.10180v1) - [pdf](http://arxiv.org/pdf/1911.10180v1)

> Making sense of words often requires to simultaneously examine the surrounding context of a term as well as the global themes characterizing the overall corpus. Several topic models have already exploited word embeddings to recognize local context, however, it has been weakly combined with the global context during the topic inference. This paper proposes to extract topical phrases corroborating the word embedding information with the global context detected by Latent Semantic Analysis, and then combine them by means of the P\'{o}lya urn model. To highlight the effectiveness of this combined approach the model was assessed analyzing clinical reports, a challenging scenario characterized by technical jargon and a limited word statistics available. Results show it outperforms the state-of-the-art approaches in terms of both topic coherence and computational cost.

</details>

<details>

<summary>2019-11-22 19:27:08 - Characterizing Developer Use of Automatically Generated Patches</summary>

- *José Pablo Cambronero, Jiasi Shen, Jürgen Cito, Elena Glassman, Martin Rinard*

- `1907.06535v2` - [abs](http://arxiv.org/abs/1907.06535v2) - [pdf](http://arxiv.org/pdf/1907.06535v2)

> We present a study that characterizes the way developers use automatically generated patches when fixing software defects. Our study tasked two groups of developers with repairing defects in C programs. Both groups were provided with the defective line of code. One was also provided with five automatically generated and validated patches, all of which modified the defective line of code, and one of which was correct. Contrary to our initial expectations, the group with access to the generated patches did not produce more correct patches and did not produce patches in less time. We characterize the main behaviors observed in experimental subjects: a focus on understanding the defect and the relationship of the patches to the original source code. Based on this characterization, we highlight various potentially productive directions for future developer-centric automatic patch generation systems.

</details>

<details>

<summary>2019-11-22 21:05:10 - Using link and content over time for embedding generation in Dynamic Attributed Networks</summary>

- *Ana Paula Appel, Renato L. F. Cunha, Charu C. Aggarwal, Marcela Megumi Terakado*

- `1807.06560v2` - [abs](http://arxiv.org/abs/1807.06560v2) - [pdf](http://arxiv.org/pdf/1807.06560v2)

> In this work, we consider the problem of combining link, content and temporal analysis for community detection and prediction in evolving networks. Such temporal and content-rich networks occur in many real-life settings, such as bibliographic networks and question answering forums. Most of the work in the literature (that uses both content and structure) deals with static snapshots of networks, and they do not reflect the dynamic changes occurring over multiple snapshots. Incorporating dynamic changes in the communities into the analysis can also provide useful insights about the changes in the network such as the migration of authors across communities. In this work, we propose Chimera, a shared factorization model that can simultaneously account for graph links, content, and temporal analysis. This approach works by extracting the latent semantic structure of the network in multidimensional form, but in a way that takes into account the temporal continuity of these embeddings. Such an approach simplifies temporal analysis of the underlying network by using the embedding as a surrogate. A consequence of this simplification is that it is also possible to use this temporal sequence of embeddings to predict future communities. We present experimental results illustrating the effectiveness of the approach.

</details>

<details>

<summary>2019-11-22 23:08:17 - Enhancing Cross-task Black-Box Transferability of Adversarial Examples with Dispersion Reduction</summary>

- *Yantao Lu, Yunhan Jia, Jianyu Wang, Bai Li, Weiheng Chai, Lawrence Carin, Senem Velipasalar*

- `1911.11616v1` - [abs](http://arxiv.org/abs/1911.11616v1) - [pdf](http://arxiv.org/pdf/1911.11616v1)

> Neural networks are known to be vulnerable to carefully crafted adversarial examples, and these malicious samples often transfer, i.e., they remain adversarial even against other models. Although great efforts have been delved into the transferability across models, surprisingly, less attention has been paid to the cross-task transferability, which represents the real-world cybercriminal's situation, where an ensemble of different defense/detection mechanisms need to be evaded all at once. In this paper, we investigate the transferability of adversarial examples across a wide range of real-world computer vision tasks, including image classification, object detection, semantic segmentation, explicit content detection, and text detection. Our proposed attack minimizes the ``dispersion'' of the internal feature map, which overcomes existing attacks' limitation of requiring task-specific loss functions and/or probing a target model. We conduct evaluation on open source detection and segmentation models as well as four different computer vision tasks provided by Google Cloud Vision (GCV) APIs, to show how our approach outperforms existing attacks by degrading performance of multiple CV tasks by a large margin with only modest perturbations linf=16.

</details>

<details>

<summary>2019-11-23 07:56:45 - RefNet: A Reference-aware Network for Background Based Conversation</summary>

- *Chuan Meng, Pengjie Ren, Zhumin Chen, Christof Monz, Jun Ma, Maarten de Rijke*

- `1908.06449v2` - [abs](http://arxiv.org/abs/1908.06449v2) - [pdf](http://arxiv.org/pdf/1908.06449v2)

> Existing conversational systems tend to generate generic responses. Recently, Background Based Conversations (BBCs) have been introduced to address this issue. Here, the generated responses are grounded in some background information. The proposed methods for BBCs are able to generate more informative responses, they either cannot generate natural responses or have difficulty in locating the right background information. In this paper, we propose a Reference-aware Network (RefNet) to address the two issues. Unlike existing methods that generate responses token by token, RefNet incorporates a novel reference decoder that provides an alternative way to learn to directly cite a semantic unit (e.g., a span containing complete semantic information) from the background. Experimental results show that RefNet significantly outperforms state-of-the-art methods in terms of both automatic and human evaluations, indicating that RefNet can generate more appropriate and human-like responses.

</details>

<details>

<summary>2019-11-23 21:33:31 - Using the Web as an Implicit Training Set: Application to Noun Compound Syntax and Semantics</summary>

- *Preslav Nakov*

- `1912.01113v1` - [abs](http://arxiv.org/abs/1912.01113v1) - [pdf](http://arxiv.org/pdf/1912.01113v1)

> An important characteristic of English written text is the abundance of noun compounds - sequences of nouns acting as a single noun, e.g., colon cancer tumor suppressor protein. While eventually mastered by domain experts, their interpretation poses a major challenge for automated analysis. Understanding noun compounds' syntax and semantics is important for many natural language applications, including question answering, machine translation, information retrieval, and information extraction. I address the problem of noun compounds syntax by means of novel, highly accurate unsupervised and lightly supervised algorithms using the Web as a corpus and search engines as interfaces to that corpus. Traditionally the Web has been viewed as a source of page hit counts, used as an estimate for n-gram word frequencies. I extend this approach by introducing novel surface features and paraphrases, which yield state-of-the-art results for the task of noun compound bracketing. I also show how these kinds of features can be applied to other structural ambiguity problems, like prepositional phrase attachment and noun phrase coordination. I address noun compound semantics by automatically generating paraphrasing verbs and prepositions that make explicit the hidden semantic relations between the nouns in a noun compound. I also demonstrate how these paraphrasing verbs can be used to solve various relational similarity problems, and how paraphrasing noun compounds can improve machine translation.

</details>

<details>

<summary>2019-11-23 21:37:19 - GAN-enhanced Conditional Echocardiogram Generation</summary>

- *Amir H. Abdi, Teresa Tsang, Purang Abolmaesumi*

- `1911.02121v2` - [abs](http://arxiv.org/abs/1911.02121v2) - [pdf](http://arxiv.org/pdf/1911.02121v2)

> Echocardiography (echo) is a common means of evaluating cardiac conditions. Due to the label scarcity, semi-supervised paradigms in automated echo analysis are getting traction. One of the most sought-after problems in echo is the segmentation of cardiac structures (e.g. chambers). Accordingly, we propose an echocardiogram generation approach using generative adversarial networks with a conditional patch-based discriminator. In this work, we validate the feasibility of GAN-enhanced echo generation with different conditions (segmentation masks), namely, the left ventricle, ventricular myocardium, and atrium. Results show that the proposed adversarial algorithm can generate high-quality echo frames whose cardiac structures match the given segmentation masks. This method is expected to facilitate the training of other machine learning models in a semi-supervised fashion as suggested in similar researches.

</details>

<details>

<summary>2019-11-23 21:42:23 - SemEval-2013 Task 4: Free Paraphrases of Noun Compounds</summary>

- *Iris Hendrickx, Preslav Nakov, Stan Szpakowicz, Zornitsa Kozareva, Diarmuid Ó Séaghdha, Tony Veale*

- `1911.10421v1` - [abs](http://arxiv.org/abs/1911.10421v1) - [pdf](http://arxiv.org/pdf/1911.10421v1)

> In this paper, we describe SemEval-2013 Task 4: the definition, the data, the evaluation and the results. The task is to capture some of the meaning of English noun compounds via paraphrasing. Given a two-word noun compound, the participating system is asked to produce an explicitly ranked list of its free-form paraphrases. The list is automatically compared and evaluated against a similarly ranked list of paraphrases proposed by human annotators, recruited and managed through Amazon's Mechanical Turk. The comparison of raw paraphrases is sensitive to syntactic and morphological variation. The "gold" ranking is based on the relative popularity of paraphrases among annotators. To make the ranking more reliable, highly similar paraphrases are grouped, so as to downplay superficial differences in syntax and morphology. Three systems participated in the task. They all beat a simple baseline on one of the two evaluation measures, but not on both measures. This shows that the task is difficult.

</details>

<details>

<summary>2019-11-23 21:49:10 - SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals</summary>

- *Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano, Stan Szpakowicz*

- `1911.10422v1` - [abs](http://arxiv.org/abs/1911.10422v1) - [pdf](http://arxiv.org/pdf/1911.10422v1)

> In response to the continuing research interest in computational semantic analysis, we have proposed a new task for SemEval-2010: multi-way classification of mutually exclusive semantic relations between pairs of nominals. The task is designed to compare different approaches to the problem and to provide a standard testbed for future research. In this paper, we define the task, describe the creation of the datasets, and discuss the results of the participating 28 systems submitted by 10 teams.

</details>

<details>

<summary>2019-11-24 00:08:09 - ScienceExamCER: A High-Density Fine-Grained Science-Domain Corpus for Common Entity Recognition</summary>

- *Hannah Smith, Zeyu Zhang, John Culnan, Peter Jansen*

- `1911.10436v1` - [abs](http://arxiv.org/abs/1911.10436v1) - [pdf](http://arxiv.org/pdf/1911.10436v1)

> Named entity recognition identifies common classes of entities in text, but these entity labels are generally sparse, limiting utility to downstream tasks. In this work we present ScienceExamCER, a densely-labeled semantic classification corpus of 133k mentions in the science exam domain where nearly all (96%) of content words have been annotated with one or more fine-grained semantic class labels including taxonomic groups, meronym groups, verb/action groups, properties and values, and synonyms. Semantic class labels are drawn from a manually-constructed fine-grained typology of 601 classes generated through a data-driven analysis of 4,239 science exam questions. We show an off-the-shelf BERT-based named entity recognition model modified for multi-label classification achieves an accuracy of 0.85 F1 on this task, suggesting strong utility for downstream tasks in science domain question answering requiring densely-labeled semantic classification.

</details>

<details>

<summary>2019-11-24 05:06:41 - Neural Storyboard Artist: Visualizing Stories with Coherent Image Sequences</summary>

- *Shizhe Chen, Bei Liu, Jianlong Fu, Ruihua Song, Qin Jin, Pingping Lin, Xiaoyu Qi, Chunting Wang, Jin Zhou*

- `1911.10460v1` - [abs](http://arxiv.org/abs/1911.10460v1) - [pdf](http://arxiv.org/pdf/1911.10460v1)

> A storyboard is a sequence of images to illustrate a story containing multiple sentences, which has been a key process to create different story products. In this paper, we tackle a new multimedia task of automatic storyboard creation to facilitate this process and inspire human artists. Inspired by the fact that our understanding of languages is based on our past experience, we propose a novel inspire-and-create framework with a story-to-image retriever that selects relevant cinematic images for inspiration and a storyboard creator that further refines and renders images to improve the relevancy and visual consistency. The proposed retriever dynamically employs contextual information in the story with hierarchical attentions and applies dense visual-semantic matching to accurately retrieve and ground images. The creator then employs three rendering steps to increase the flexibility of retrieved images, which include erasing irrelevant regions, unifying styles of images and substituting consistent characters. We carry out extensive experiments on both in-domain and out-of-domain visual story datasets. The proposed model achieves better quantitative performance than the state-of-the-art baselines for storyboard creation. Qualitative visualizations and user studies further verify that our approach can create high-quality storyboards even for stories in the wild.

</details>

<details>

<summary>2019-11-24 13:25:35 - A Quantitative Approach to Understanding Online Antisemitism</summary>

- *Savvas Zannettou, Joel Finkelstein, Barry Bradlyn, Jeremy Blackburn*

- `1809.01644v2` - [abs](http://arxiv.org/abs/1809.01644v2) - [pdf](http://arxiv.org/pdf/1809.01644v2)

> A new wave of growing antisemitism, driven by fringe Web communities, is an increasingly worrying presence in the socio-political realm. The ubiquitous and global nature of the Web has provided tools used by these groups to spread their ideology to the rest of the Internet. Although the study of antisemitism and hate is not new, the scale and rate of change of online data has impacted the efficacy of traditional approaches to measure and understand these troubling trends. In this paper, we present a large-scale, quantitative study of online antisemitism. We collect hundreds of million posts and images from alt-right Web communities like 4chan's Politically Incorrect board (/pol/) and Gab. Using scientifically grounded methods, we quantify the escalation and spread of antisemitic memes and rhetoric across the Web. We find the frequency of antisemitic content greatly increases (in some cases more than doubling) after major political events such as the 2016 US Presidential Election and the "Unite the Right" rally in Charlottesville. We extract semantic embeddings from our corpus of posts and demonstrate how automated techniques can discover and categorize the use of antisemitic terminology. We additionally examine the prevalence and spread of the antisemitic "Happy Merchant" meme, and in particular how these fringe communities influence its propagation to more mainstream communities like Twitter and Reddit. Taken together, our results provide a data-driven, quantitative framework for understanding online antisemitism. Our methods serve as a framework to augment current qualitative efforts by anti-hate groups, providing new insights into the growth and spread of hate online.

</details>

<details>

<summary>2019-11-24 17:21:37 - Emu: Enhancing Multilingual Sentence Embeddings with Semantic Specialization</summary>

- *Wataru Hirota, Yoshihiko Suhara, Behzad Golshan, Wang-Chiew Tan*

- `1909.06731v2` - [abs](http://arxiv.org/abs/1909.06731v2) - [pdf](http://arxiv.org/pdf/1909.06731v2)

> We present Emu, a system that semantically enhances multilingual sentence embeddings. Our framework fine-tunes pre-trained multilingual sentence embeddings using two main components: a semantic classifier and a language discriminator. The semantic classifier improves the semantic similarity of related sentences, whereas the language discriminator enhances the multilinguality of the embeddings via multilingual adversarial training. Our experimental results based on several language pairs show that our specialized embeddings outperform the state-of-the-art multilingual sentence embedding model on the task of cross-lingual intent classification using only monolingual labeled data.

</details>

<details>

<summary>2019-11-25 06:43:53 - TuNet: End-to-end Hierarchical Brain Tumor Segmentation using Cascaded Networks</summary>

- *Minh H. Vu, Tufve Nyholm, Tommy Löfstedt*

- `1910.05338v3` - [abs](http://arxiv.org/abs/1910.05338v3) - [pdf](http://arxiv.org/pdf/1910.05338v3)

> Glioma is one of the most common types of brain tumors; it arises in the glial cells in the human brain and in the spinal cord. In addition to having a high mortality rate, glioma treatment is also very expensive. Hence, automatic and accurate segmentation and measurement from the early stages are critical in order to prolong the survival rates of the patients and to reduce the costs of the treatment. In the present work, we propose a novel end-to-end cascaded network for semantic segmentation that utilizes the hierarchical structure of the tumor sub-regions with ResNet-like blocks and Squeeze-and-Excitation modules after each convolution and concatenation block. By utilizing cross-validation, an average ensemble technique, and a simple post-processing technique, we obtained dice scores of 88.06, 80.84, and 80.29, and Hausdorff Distances (95th percentile) of 6.10, 5.17, and 2.21 for the whole tumor, tumor core, and enhancing tumor, respectively, on the online test set.

</details>

<details>

<summary>2019-11-25 07:34:37 - End-to-End Trainable Non-Collaborative Dialog System</summary>

- *Yu Li, Kun Qian, Weiyan Shi, Zhou Yu*

- `1911.10742v1` - [abs](http://arxiv.org/abs/1911.10742v1) - [pdf](http://arxiv.org/pdf/1911.10742v1)

> End-to-end task-oriented dialog models have achieved promising performance on collaborative tasks where users willingly coordinate with the system to complete a given task. While in non-collaborative settings, for example, negotiation and persuasion, users and systems do not share a common goal. As a result, compared to collaborate tasks, people use social content to build rapport and trust in these non-collaborative settings in order to advance their goals. To handle social content, we introduce a hierarchical intent annotation scheme, which can be generalized to different non-collaborative dialog tasks. Building upon TransferTransfo (Wolf et al. 2019), we propose an end-to-end neural network model to generate diverse coherent responses. Our model utilizes intent and semantic slots as the intermediate sentence representation to guide the generation process. In addition, we design a filter to select appropriate responses based on whether these intermediate representations fit the designed task and conversation constraints. Our non-collaborative dialog model guides users to complete the task while simultaneously keeps them engaged. We test our approach on our newly proposed ANTISCAM dataset and an existing PERSUASIONFORGOOD dataset. Both automatic and human evaluations suggest that our model outperforms multiple baselines in these two non-collaborative tasks.

</details>

<details>

<summary>2019-11-25 09:21:17 - Filling Conversation Ellipsis for Better Social Dialog Understanding</summary>

- *Xiyuan Zhang, Chengxi Li, Dian Yu, Samuel Davidson, Zhou Yu*

- `1911.10776v1` - [abs](http://arxiv.org/abs/1911.10776v1) - [pdf](http://arxiv.org/pdf/1911.10776v1)

> The phenomenon of ellipsis is prevalent in social conversations. Ellipsis increases the difficulty of a series of downstream language understanding tasks, such as dialog act prediction and semantic role labeling. We propose to resolve ellipsis through automatic sentence completion to improve language understanding. However, automatic ellipsis completion can result in output which does not accurately reflect user intent. To address this issue, we propose a method which considers both the original utterance that has ellipsis and the automatically completed utterance in dialog act and semantic role labeling tasks. Specifically, we first complete user utterances to resolve ellipsis using an end-to-end pointer network model. We then train a prediction model using both utterances containing ellipsis and our automatically completed utterances. Finally, we combine the prediction results from these two utterances using a selection model that is guided by expert knowledge. Our approach improves dialog act prediction and semantic role labeling by 1.3% and 2.5% in F1 score respectively in social conversations. We also present an open-domain human-machine conversation dataset with manually completed user utterances and annotated semantic role labeling after manual completion.

</details>

<details>

<summary>2019-11-25 10:02:05 - Learning by Abstraction: The Neural State Machine</summary>

- *Drew A. Hudson, Christopher D. Manning*

- `1907.03950v4` - [abs](http://arxiv.org/abs/1907.03950v4) - [pdf](http://arxiv.org/pdf/1907.03950v4)

> We introduce the Neural State Machine, seeking to bridge the gap between the neural and symbolic views of AI and integrate their complementary strengths for the task of visual reasoning. Given an image, we first predict a probabilistic graph that represents its underlying semantics and serves as a structured world model. Then, we perform sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. In contrast to most neural architectures that are designed to closely interact with the raw sensory data, our model operates instead in an abstract latent space, by transforming both the visual and linguistic modalities into semantic concept-based representations, thereby achieving enhanced transparency and modularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets that involve compositionality, multi-step inference and diverse reasoning skills, achieving state-of-the-art results in both cases. We provide further experiments that illustrate the model's strong generalization capacity across multiple dimensions, including novel compositions of concepts, changes in the answer distribution, and unseen linguistic structures, demonstrating the qualities and efficacy of our approach.

</details>

<details>

<summary>2019-11-25 12:02:15 - Visual Summarization of Scholarly Videos using Word Embeddings and Keyphrase Extraction</summary>

- *Hang Zhou, Christian Otto, Ralph Ewerth*

- `1912.10809v1` - [abs](http://arxiv.org/abs/1912.10809v1) - [pdf](http://arxiv.org/pdf/1912.10809v1)

> Effective learning with audiovisual content depends on many factors. Besides the quality of the learning resource's content, it is essential to discover the most relevant and suitable video in order to support the learning process most effectively. Video summarization techniques facilitate this goal by providing a quick overview over the content. It is especially useful for longer recordings such as conference presentations or lectures. In this paper, we present an approach that generates a visual summary of video content based on semantic word embeddings and keyphrase extraction. For this purpose, we exploit video annotations that are automatically generated by speech recognition and video OCR (optical character recognition).

</details>

<details>

<summary>2019-11-25 13:59:05 - Discovering topics with neural topic models built from PLSA assumptions</summary>

- *Sileye 0. Ba*

- `1911.10924v1` - [abs](http://arxiv.org/abs/1911.10924v1) - [pdf](http://arxiv.org/pdf/1911.10924v1)

> In this paper we present a model for unsupervised topic discovery in texts corpora. The proposed model uses documents, words, and topics lookup table embedding as neural network model parameters to build probabilities of words given topics, and probabilities of topics given documents. These probabilities are used to recover by marginalization probabilities of words given documents. For very large corpora where the number of documents can be in the order of billions, using a neural auto-encoder based document embedding is more scalable then using a lookup table embedding as classically done. We thus extended the lookup based document embedding model to continuous auto-encoder based model. Our models are trained using probabilistic latent semantic analysis (PLSA) assumptions. We evaluated our models on six datasets with a rich variety of contents. Conducted experiments demonstrate that the proposed neural topic models are very effective in capturing relevant topics. Furthermore, considering perplexity metric, conducted evaluation benchmarks show that our topic models outperform latent Dirichlet allocation (LDA) model which is classically used to address topic discovery tasks.

</details>

<details>

<summary>2019-11-25 14:05:08 - Patch augmentation: Towards efficient decision boundaries for neural networks</summary>

- *Marcus D. Bloice, Peter M. Roth, Andreas Holzinger*

- `1911.07922v2` - [abs](http://arxiv.org/abs/1911.07922v2) - [pdf](http://arxiv.org/pdf/1911.07922v2)

> In this paper we propose a new augmentation technique, called patch augmentation, that, in our experiments, improves model accuracy and makes networks more robust to adversarial attacks. In brief, this data-independent approach creates new image data based on image/label pairs, where a patch from one of the two images in the pair is superimposed on to the other image, creating a new augmented sample. The new image's label is a linear combination of the image pair's corresponding labels. Initial experiments show a several percentage point increase in accuracy on CIFAR-10, from a baseline of approximately 81% to 89%. CIFAR-100 sees larger improvements still, from a baseline of 52% to 68% accuracy. Networks trained using patch augmentation are also more robust to adversarial attacks, which we demonstrate using the Fast Gradient Sign Method.

</details>

<details>

<summary>2019-11-25 15:00:11 - Data-Free Quantization Through Weight Equalization and Bias Correction</summary>

- *Markus Nagel, Mart van Baalen, Tijmen Blankevoort, Max Welling*

- `1906.04721v3` - [abs](http://arxiv.org/abs/1906.04721v3) - [pdf](http://arxiv.org/pdf/1906.04721v3)

> We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.

</details>

<details>

<summary>2019-11-25 17:32:06 - A Robust Approach for Securing Audio Classification Against Adversarial Attacks</summary>

- *Mohammad Esmaeilpour, Patrick Cardinal, Alessandro Lameiras Koerich*

- `1904.10990v2` - [abs](http://arxiv.org/abs/1904.10990v2) - [pdf](http://arxiv.org/pdf/1904.10990v2)

> Adversarial audio attacks can be considered as a small perturbation unperceptive to human ears that is intentionally added to the audio signal and causes a machine learning model to make mistakes. This poses a security concern about the safety of machine learning models since the adversarial attacks can fool such models toward the wrong predictions. In this paper we first review some strong adversarial attacks that may affect both audio signals and their 2D representations and evaluate the resiliency of the most common machine learning model, namely deep learning models and support vector machines (SVM) trained on 2D audio representations such as short time Fourier transform (STFT), discrete wavelet transform (DWT) and cross recurrent plot (CRP) against several state-of-the-art adversarial attacks. Next, we propose a novel approach based on pre-processed DWT representation of audio signals and SVM to secure audio systems against adversarial attacks. The proposed architecture has several preprocessing modules for generating and enhancing spectrograms including dimension reduction and smoothing. We extract features from small patches of the spectrograms using speeded up robust feature (SURF) algorithm which are further used to generate a codebook using the K-Means++ algorithm. Finally, codewords are used to train a SVM on the codebook of the SURF-generated vectors. All these steps yield to a novel approach for audio classification that provides a good trade-off between accuracy and resilience. Experimental results on three environmental sound datasets show the competitive performance of proposed approach compared to the deep neural networks both in terms of accuracy and robustness against strong adversarial attacks.

</details>

<details>

<summary>2019-11-25 20:45:13 - On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset</summary>

- *Muhammad Waleed Gondal, Manuel Wüthrich, Đorđe Miladinović, Francesco Locatello, Martin Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Schölkopf, Stefan Bauer*

- `1906.03292v3` - [abs](http://arxiv.org/abs/1906.03292v3) - [pdf](http://arxiv.org/pdf/1906.03292v3)

> Learning meaningful and compact representations with disentangled semantic aspects is considered to be of key importance in representation learning. Since real-world data is notoriously costly to collect, many recent state-of-the-art disentanglement models have heavily relied on synthetic toy data-sets. In this paper, we propose a novel data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position. In order to be able to control all the factors of variation precisely, we built an experimental platform where the objects are being moved by a robotic arm. In addition, we provide two more datasets which consist of simulations of the experimental setup. These datasets provide for the first time the possibility to systematically investigate how well different disentanglement methods perform on real data in comparison to simulation, and how simulated data can be leveraged to build better representations of the real world. We provide a first experimental study of these questions and our results indicate that learned models transfer poorly, but that model and hyperparameter selection is an effective means of transferring information to the real world.

</details>

<details>

<summary>2019-11-26 05:56:43 - Semantic Interior Mapology: A Toolbox For Indoor Scene Description From Architectural Floor Plans</summary>

- *Viet Trinh, Roberto Manduchi*

- `1911.11356v1` - [abs](http://arxiv.org/abs/1911.11356v1) - [pdf](http://arxiv.org/pdf/1911.11356v1)

> We introduce the Semantic Interior Mapology (SIM) toolbox for the conversion of a floor plan and its room contents (such as furnitures) to a vectorized form. The toolbox is composed of the Map Conversion toolkit and the Map Population toolkit. The Map Conversion toolkit allows one to quickly trace the layout of a floor plan, and to generate a GeoJSON file that can be rendered in 3D using web applications such as Mapbox. The Map Population toolkit takes the 3D scan of a room in the building (acquired from an RGB-D camera), and, through a semi-automatic process, populates individual objects of interest with a correct dimension and position in the GeoJSON representation of the building. SIM is easy to use and produces accurate results even in the case of complex building layouts.

</details>

<details>

<summary>2019-11-26 06:01:09 - Semantic Bottleneck Scene Generation</summary>

- *Samaneh Azadi, Michael Tschannen, Eric Tzeng, Sylvain Gelly, Trevor Darrell, Mario Lucic*

- `1911.11357v1` - [abs](http://arxiv.org/abs/1911.11357v1) - [pdf](http://arxiv.org/pdf/1911.11357v1)

> Coupling the high-fidelity generation capabilities of label-conditional image synthesis methods with the flexibility of unconditional generative models, we propose a semantic bottleneck GAN model for unconditional synthesis of complex scenes. We assume pixel-wise segmentation labels are available during training and use them to learn the scene structure. During inference, our model first synthesizes a realistic segmentation layout from scratch, then synthesizes a realistic scene conditioned on that layout. For the former, we use an unconditional progressive segmentation generation network that captures the distribution of realistic semantic scene layouts. For the latter, we use a conditional segmentation-to-image synthesis network that captures the distribution of photo-realistic images conditioned on the semantic layout. When trained end-to-end, the resulting model outperforms state-of-the-art generative models in unsupervised image synthesis on two challenging domains in terms of the Frechet Inception Distance and user-study evaluations. Moreover, we demonstrate the generated segmentation maps can be used as additional training data to strongly improve recent segmentation-to-image synthesis networks.

</details>

<details>

<summary>2019-11-26 06:02:33 - CAWA: An Attention-Network for Credit Attribution</summary>

- *Saurav Manchanda, George Karypis*

- `1911.11358v1` - [abs](http://arxiv.org/abs/1911.11358v1) - [pdf](http://arxiv.org/pdf/1911.11358v1)

> Credit attribution is the task of associating individual parts in a document with their most appropriate class labels. It is an important task with applications to information retrieval and text summarization. When labeled training data is available, traditional approaches for sequence tagging can be used for credit attribution. However, generating such labeled datasets is expensive and time-consuming. In this paper, we present "Credit Attribution With Attention (CAWA)", a neural-network-based approach, that instead of using sentence-level labeled data, uses the set of class labels that are associated with an entire document as a source of distant-supervision. CAWA combines an attention mechanism with a multilabel classifier into an end-to-end learning framework to perform credit attribution. CAWA labels the individual sentences from the input document using the resultant attention-weights. CAWA improves upon the state-of-the-art credit attribution approach by not constraining a sentence to belong to just one class, but modeling each sentence as a distribution over all classes, leading to better modeling of semantically-similar classes. Experiments on the credit attribution task on a variety of datasets show that the sentence class labels generated by CAWA outperform the competing approaches. Additionally, on the multilabel text classification task, CAWA performs better than the competing credit attribution approaches.

</details>

<details>

<summary>2019-11-26 12:36:47 - FCA2VEC: Embedding Techniques for Formal Concept Analysis</summary>

- *Dominik Dürrschnabel, Tom Hanika, Maximilian Stubbemann*

- `1911.11496v1` - [abs](http://arxiv.org/abs/1911.11496v1) - [pdf](http://arxiv.org/pdf/1911.11496v1)

> Embedding large and high dimensional data into low dimensional vector spaces is a necessary task to computationally cope with contemporary data sets. Superseding latent semantic analysis recent approaches like word2vec or node2vec are well established tools in this realm. In the present paper we add to this line of research by introducing fca2vec, a family of embedding techniques for formal concept analysis (FCA). Our investigation contributes to two distinct lines of research. First, we enable the application of FCA notions to large data sets. In particular, we demonstrate how the cover relation of a concept lattice can be retrieved from a computational feasible embedding. Secondly, we show an enhancement for the classical node2vec approach in low dimension. For both directions the overall constraint of FCA of explainable results is preserved. We evaluate our novel procedures by computing fca2vec on different data sets like, wiki44 (a dense part of the Wikidata knowledge graph), the Mushroom data set and a publication network derived from the FCA community.

</details>

<details>

<summary>2019-11-26 13:11:00 - Word-Class Embeddings for Multiclass Text Classification</summary>

- *Alejandro Moreo, Andrea Esuli, Fabrizio Sebastiani*

- `1911.11506v1` - [abs](http://arxiv.org/abs/1911.11506v1) - [pdf](http://arxiv.org/pdf/1911.11506v1)

> Pre-trained word embeddings encode general word semantics and lexical regularities of natural language, and have proven useful across many NLP tasks, including word sense disambiguation, machine translation, and sentiment analysis, to name a few. In supervised tasks such as multiclass text classification (the focus of this article) it seems appealing to enhance word representations with ad-hoc embeddings that encode task-specific information. We propose (supervised) word-class embeddings (WCEs), and show that, when concatenated to (unsupervised) pre-trained word embeddings, they substantially facilitate the training of deep-learning models in multiclass classification by topic. We show empirical evidence that WCEs yield a consistent improvement in multiclass classification accuracy, using four popular neural architectures and six widely used and publicly available datasets for multiclass text classification. Our code that implements WCEs is publicly available at https://github.com/AlexMoreo/word-class-embeddings

</details>

<details>

<summary>2019-11-26 18:52:33 - Chasing Ghosts: Instruction Following as Bayesian State Tracking</summary>

- *Peter Anderson, Ayush Shrivastava, Devi Parikh, Dhruv Batra, Stefan Lee*

- `1907.02022v2` - [abs](http://arxiv.org/abs/1907.02022v2) - [pdf](http://arxiv.org/pdf/1907.02022v2)

> A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) within the framework of Bayesian state tracking - learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet baseline when predicting the goal location on the map. On the full VLN task, i.e. navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.

</details>

<details>

<summary>2019-11-27 04:54:14 - Label Dependent Deep Variational Paraphrase Generation</summary>

- *Siamak Shakeri, Abhinav Sethy*

- `1911.11952v1` - [abs](http://arxiv.org/abs/1911.11952v1) - [pdf](http://arxiv.org/pdf/1911.11952v1)

> Generating paraphrases that are lexically similar but semantically different is a challenging task. Paraphrases of this form can be used to augment data sets for various NLP tasks such as machine reading comprehension and question answering with non-trivial negative examples. In this article, we propose a deep variational model to generate paraphrases conditioned on a label that specifies whether the paraphrases are semantically related or not. We also present new training recipes and KL regularization techniques that improve the performance of variational paraphrasing models. Our proposed model demonstrates promising results in enhancing the generative power of the model by employing label-dependent generation on paraphrasing datasets.

</details>

<details>

<summary>2019-11-27 11:25:43 - Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web as a Corpus</summary>

- *Su Nam Kim, Preslav Nakov*

- `1911.12085v1` - [abs](http://arxiv.org/abs/1911.12085v1) - [pdf](http://arxiv.org/pdf/1911.12085v1)

> Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., "orange juice", together with suitable fine-grained semantic interpretations, e.g., "squeezed from", which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun fixed yields both a higher number of semantically interpreted NCs and improved accuracy due to stronger semantic restrictions.

</details>

<details>

<summary>2019-11-27 12:40:08 - CMB-GAN: Fast Simulations of Cosmic Microwave background anisotropy maps using Deep Learning</summary>

- *Amit Mishra, Pranath Reddy, Rahul Nigam*

- `1908.04682v3` - [abs](http://arxiv.org/abs/1908.04682v3) - [pdf](http://arxiv.org/pdf/1908.04682v3)

> Cosmic Microwave Background (CMB) has been a cornerstone in many cosmology experiments and studies since it was discovered back in 1964. Traditional computational models like CAMB that are used for generating CMB temperature anisotropy maps are extremely resource intensive and act as a bottleneck in cosmology experiments that require a large amount of CMB data for analysis. In this paper, we present a new approach to the generation of CMB temperature maps using a specific class of neural networks called Generative Adversarial Network (GAN). We train our deep generative model to learn the complex distribution of CMB maps and efficiently generate new sets of CMB data in the form of 2D patches of anisotropy maps without losing much accuracy. We limit our experiment to the generation of 56$^{\circ}$ and 112$^{\circ}$ square patches of CMB maps. We have also trained a Multilayer perceptron model for estimation of baryon density from a CMB map, we will be using this model for the performance evaluation of our generative model using diagnostic measures like Histogram of pixel intensities, the standard deviation of pixel intensity distribution, Power Spectrum, Cross power spectrum, Correlation matrix of the power spectrum and Peak count. We show that the GAN model is able to efficiently generate CMB samples of multiple sizes and is sensitive to the cosmological parameters corresponding to the underlying distribution of the data. The primiary advantage of this method is the exponential reduction in the computational time needed to generate the CMB data, the GAN model is able to generate the samples within seconds as opposed to hours required by the CAMB package with an acceptable value to error and loss of information. We hope that future iterations of this methodology will replace traditional statistical methods of CMB data generation and help in large scale cosmological experiments.

</details>

<details>

<summary>2019-11-27 14:05:26 - Shearlets as Feature Extractor for Semantic Edge Detection: The Model-Based and Data-Driven Realm</summary>

- *Héctor Andrade-Loarca, Gitta Kutyniok, Ozan Öktem*

- `1911.12159v1` - [abs](http://arxiv.org/abs/1911.12159v1) - [pdf](http://arxiv.org/pdf/1911.12159v1)

> Semantic edge detection has recently gained a lot of attention as an image processing task, mainly due to its wide range of real-world applications. This is based on the fact that edges in images contain most of the semantic information. Semantic edge detection involves two tasks, namely pure edge detecion and edge classification. Those are in fact fundamentally distinct in terms of the level of abstraction that each task requires, which is known as the distracted supervision paradox that limits the possible performance of a supervised model in semantic edge detection. In this work, we will present a novel hybrid method to avoid the distracted supervision paradox and achieve high-performance in semantic edge detection. Our approach is based on a combination of the model-based concept of shearlets, which provides probably optimally sparse approximations of a model-class of images, and the data-driven method of a suitably designed convolutional neural netwok. Finally, we present several applications such as tomographic reconstruction and show that our approach signifiantly outperforms former methods, thereby indicating the value of such hybrid methods for the area in biomedical imaging.

</details>

<details>

<summary>2019-11-27 16:09:11 - Do Attention Heads in BERT Track Syntactic Dependencies?</summary>

- *Phu Mon Htut, Jason Phang, Shikha Bordia, Samuel R. Bowman*

- `1911.12246v1` - [abs](http://arxiv.org/abs/1911.12246v1) - [pdf](http://arxiv.org/pdf/1911.12246v1)

> We investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods---taking the maximum attention weight and computing the maximum spanning tree---to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type significantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT fine-tuned on two datasets---the syntax-oriented CoLA and the semantics-oriented MNLI---to investigate whether fine-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing significantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.

</details>

<details>

<summary>2019-11-27 19:51:19 - FT-SWRL: A Fuzzy-Temporal Extension of Semantic Web Rule Language</summary>

- *Abba Lawan, Abdur Rakib*

- `1911.12399v1` - [abs](http://arxiv.org/abs/1911.12399v1) - [pdf](http://arxiv.org/pdf/1911.12399v1)

> We present, FT-SWRL, a fuzzy temporal extension to the Semantic Web Rule Language (SWRL), which combines fuzzy theories based on the valid-time temporal model to provide a standard approach for modeling imprecise temporal domain knowledge in OWL ontologies. The proposal introduces a fuzzy temporal model for the semantic web, which is syntactically defined as a fuzzy temporal SWRL ontology (SWRL-FTO) with a new set of fuzzy temporal SWRL built-ins for defining their semantics. The SWRL-FTO hierarchically defines the necessary linguistic terminologies and variables for the fuzzy temporal model. An example model demonstrating the usefulness of the fuzzy temporal SWRL built-ins to model imprecise temporal information is also represented. Fuzzification process of interval-based temporal logic is further discussed as a reasoning paradigm for our FT-SWRL rules, with the aim of achieving a complete OWL-based fuzzy temporal reasoning. Literature review on fuzzy temporal representation approaches, both within and without the use of ontologies, led to the conclusion that the FT-SWRL model can authoritatively serve as a formal specification for handling imprecise temporal expressions on the semantic web.

</details>

<details>

<summary>2019-11-27 20:37:26 - Automatic Generation of Headlines for Online Math Questions</summary>

- *Ke Yuan, Dafang He, Zhuoren Jiang, Liangcai Gao, Zhi Tang, C. Lee Giles*

- `1912.00839v1` - [abs](http://arxiv.org/abs/1912.00839v1) - [pdf](http://arxiv.org/pdf/1912.00839v1)

> Mathematical equations are an important part of dissemination and communication of scientific information. Students, however, often feel challenged in reading and understanding math content and equations. With the development of the Web, students are posting their math questions online. Nevertheless, constructing a concise math headline that gives a good description of the posted detailed math question is nontrivial. In this study, we explore a novel summarization task denoted as geNerating A concise Math hEadline from a detailed math question (NAME). Compared to conventional summarization tasks, this task has two extra and essential constraints: 1) Detailed math questions consist of text and math equations which require a unified framework to jointly model textual and mathematical information; 2) Unlike text, math equations contain semantic and structural features, and both of them should be captured together. To address these issues, we propose MathSum, a novel summarization model which utilizes a pointer mechanism combined with a multi-head attention mechanism for mathematical representation augmentation. The pointer mechanism can either copy textual tokens or math tokens from source questions in order to generate math headlines. The multi-head attention mechanism is designed to enrich the representation of math equations by modeling and integrating both its semantic and structural features. For evaluation, we collect and make available two sets of real-world detailed math questions along with human-written math headlines, namely EXEQ-300k and OFEQ-10k. Experimental results demonstrate that our model (MathSum) significantly outperforms state-of-the-art models for both the EXEQ-300k and OFEQ-10k datasets.

</details>

<details>

<summary>2019-11-28 01:46:55 - Auxiliary Learning for Deep Multi-task Learning</summary>

- *Yifan Liu, Bohan Zhuang, Chunhua Shen, Hao Chen, Wei Yin*

- `1909.02214v2` - [abs](http://arxiv.org/abs/1909.02214v2) - [pdf](http://arxiv.org/pdf/1909.02214v2)

> Multi-task learning (MTL) is an efficient solution to solve multiple tasks simultaneously in order to get better speed and performance than handling each single-task in turn. The most current methods can be categorized as either: (i) hard parameter sharing where a subset of the parameters is shared among tasks while other parameters are task-specific; or (ii) soft parameter sharing where all parameters are task-specific but they are jointly regularized. Both methods suffer from limitations: the shared hidden layers of the former are difficult to optimize due to the competing objectives while the complexity of the latter grows linearly with the increasing number of tasks. To mitigate those drawbacks, this paper proposes an alternative, where we explicitly construct an auxiliary module to mimic the soft parameter sharing for assisting the optimization of the hard parameter sharing layers in the training phase. In particular, the auxiliary module takes the outputs of the shared hidden layers as inputs and is supervised by the auxiliary task loss. During training, the auxiliary module is jointly optimized with the MTL network, serving as a regularization by introducing an inductive bias to the shared layers. In the testing phase, only the original MTL network is kept. Thus our method avoids the limitation of both categories. We evaluate the proposed auxiliary module on pixel-wise prediction tasks, including semantic segmentation, depth estimation, and surface normal prediction with different network structures. The extensive experiments over various settings verify the effectiveness of our methods.

</details>

<details>

<summary>2019-11-28 02:14:57 - Dual-Attention Graph Convolutional Network</summary>

- *Xueya Zhang, Tong Zhang, Wenting Zhao, Zhen Cui, Jian Yang*

- `1911.12486v1` - [abs](http://arxiv.org/abs/1911.12486v1) - [pdf](http://arxiv.org/pdf/1911.12486v1)

> Graph convolutional networks (GCNs) have shown the powerful ability in text structure representation and effectively facilitate the task of text classification. However, challenges still exist in adapting GCN on learning discriminative features from texts due to the main issue of graph variants incurred by the textual complexity and diversity. In this paper, we propose a dual-attention GCN to model the structural information of various texts as well as tackle the graph-invariant problem through embedding two types of attention mechanisms, i.e. the connection-attention and hop-attention, into the classic GCN. To encode various connection patterns between neighbour words, connection-attention adaptively imposes different weights specified to neighbourhoods of each word, which captures the short-term dependencies. On the other hand, the hop-attention applies scaled coefficients to different scopes during the graph diffusion process to make the model learn more about the distribution of context, which captures long-term semantics in an adaptive way. Extensive experiments are conducted on five widely used datasets to evaluate our dual-attention GCN, and the achieved state-of-the-art performance verifies the effectiveness of dual-attention mechanisms.

</details>

<details>

<summary>2019-11-28 02:55:29 - MTab: Matching Tabular Data to Knowledge Graph using Probability Models</summary>

- *Phuc Nguyen, Natthawut Kertkeidkachorn, Ryutaro Ichise, Hideaki Takeda*

- `1910.00246v2` - [abs](http://arxiv.org/abs/1910.00246v2) - [pdf](http://arxiv.org/pdf/1910.00246v2)

> This paper presents the design of our system, namely MTab, for Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab 2019). MTab combines the voting algorithm and the probability models to solve critical problems of the matching tasks. Results on SemTab 2019 show that MTab obtains promising performance for the three matching tasks.

</details>

<details>

<summary>2019-11-28 03:35:57 - Rethinking Temporal Fusion for Video-based Person Re-identification on Semantic and Time Aspect</summary>

- *Xinyang Jiang, Yifei Gong, Xiaowei Guo, Qize Yang, Feiyue Huang, Weishi Zheng, Feng Zheng, Xing Sun*

- `1911.12512v1` - [abs](http://arxiv.org/abs/1911.12512v1) - [pdf](http://arxiv.org/pdf/1911.12512v1)

> Recently, the research interest of person re-identification (ReID) has gradually turned to video-based methods, which acquire a person representation by aggregating frame features of an entire video. However, existing video-based ReID methods do not consider the semantic difference brought by the outputs of different network stages, which potentially compromises the information richness of the person features. Furthermore, traditional methods ignore important relationship among frames, which causes information redundancy in fusion along the time axis. To address these issues, we propose a novel general temporal fusion framework to aggregate frame features on both semantic aspect and time aspect. As for the semantic aspect, a multi-stage fusion network is explored to fuse richer frame features at multiple semantic levels, which can effectively reduce the information loss caused by the traditional single-stage fusion. While, for the time axis, the existing intra-frame attention method is improved by adding a novel inter-frame attention module, which effectively reduces the information redundancy in temporal fusion by taking the relationship among frames into consideration. The experimental results show that our approach can effectively improve the video-based re-identification accuracy, achieving the state-of-the-art performance.

</details>

<details>

<summary>2019-11-28 05:14:23 - One-Shot Object Detection with Co-Attention and Co-Excitation</summary>

- *Ting-I Hsieh, Yi-Chen Lo, Hwann-Tzong Chen, Tyng-Luh Liu*

- `1911.12529v1` - [abs](http://arxiv.org/abs/1911.12529v1) - [pdf](http://arxiv.org/pdf/1911.12529v1)

> This paper aims to tackle the challenging problem of one-shot object detection. Given a query image patch whose class label is not included in the training data, the goal of the task is to detect all instances of the same class in a target image. To this end, we develop a novel {\em co-attention and co-excitation} (CoAE) framework that makes contributions in three key technical aspects. First, we propose to use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. Second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasize correlated feature channels to help uncover relevant proposals and eventually the target objects. Third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen in training. The resulting model is therefore a two-stage detector that yields a strong baseline on both VOC and MS-COCO under one-shot setting of detecting objects from both seen and never-seen classes. Codes are available at https://github.com/timy90022/One-Shot-Object-Detection.

</details>

<details>

<summary>2019-11-28 08:50:00 - TreeGen: A Tree-Based Transformer Architecture for Code Generation</summary>

- *Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, Lu Zhang*

- `1911.09983v2` - [abs](http://arxiv.org/abs/1911.09983v2) - [pdf](http://arxiv.org/pdf/1911.09983v2)

> A code generation system generates programming language code based on an input natural language description. State-of-the-art approaches rely on neural networks for code generation. However, these code generators suffer from two problems. One is the long dependency problem, where a code element often depends on another far-away code element. A variable reference, for example, depends on its definition, which may appear quite a few lines before. The other problem is structure modeling, as programs contain rich structural information. In this paper, we propose a novel tree-based neural architecture, TreeGen, for code generation. TreeGen uses the attention mechanism of Transformers to alleviate the long-dependency problem, and introduces a novel AST reader (encoder) to incorporate grammar rules and AST structures into the network. We evaluated TreeGen on a Python benchmark, HearthStone, and two semantic parsing benchmarks, ATIS and GEO. TreeGen outperformed the previous state-of-the-art approach by 4.5 percentage points on HearthStone, and achieved the best accuracy among neural network-based approaches on ATIS (89.1%) and GEO (89.6%). We also conducted an ablation test to better understand each component of our model.

</details>

<details>

<summary>2019-11-28 12:49:57 - Patch Reordering: a Novel Way to Achieve Rotation and Translation Invariance in Convolutional Neural Networks</summary>

- *Xu Shen, Xinmei Tian, Shaoyan Sun, Dacheng Tao*

- `1911.12682v1` - [abs](http://arxiv.org/abs/1911.12682v1) - [pdf](http://arxiv.org/pdf/1911.12682v1)

> Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art performance on many visual recognition tasks. However, the combination of convolution and pooling operations only shows invariance to small local location changes in meaningful objects in input. Sometimes, such networks are trained using data augmentation to encode this invariance into the parameters, which restricts the capacity of the model to learn the content of these objects. A more efficient use of the parameter budget is to encode rotation or translation invariance into the model architecture, which relieves the model from the need to learn them. To enable the model to focus on learning the content of objects other than their locations, we propose to conduct patch ranking of the feature maps before feeding them into the next layer. When patch ranking is combined with convolution and pooling operations, we obtain consistent representations despite the location of meaningful objects in input. We show that the patch ranking module improves the performance of the CNN on many benchmark tasks, including MNIST digit recognition, large-scale image recognition, and image retrieval. The code is available at https://github.com//jasonustc/caffe-multigpu/tree/TICNN .

</details>

<details>

<summary>2019-11-28 13:11:21 - An Introduction to Symbolic Artificial Intelligence Applied to Multimedia</summary>

- *Guilherme Lima, Rodrigo Costa, Marcio Ferreira Moreno*

- `1911.09606v2` - [abs](http://arxiv.org/abs/1911.09606v2) - [pdf](http://arxiv.org/pdf/1911.09606v2)

> In this chapter, we give an introduction to symbolic artificial intelligence (AI) and discuss its relation and application to multimedia. We begin by defining what symbolic AI is, what distinguishes it from non-symbolic approaches, such as machine learning, and how it can used in the construction of advanced multimedia applications. We then introduce description logic (DL) and use it to discuss symbolic representation and reasoning. DL is the logical underpinning of OWL, the most successful family of ontology languages. After discussing DL, we present OWL and related Semantic Web technologies, such as RDF and SPARQL. We conclude the chapter by discussing a hybrid model for multimedia representation, called Hyperknowledge. Throughout the text, we make references to technologies and extensions specifically designed to solve the kinds of problems that arise in multimedia representation.

</details>

<details>

<summary>2019-11-28 15:01:35 - Every Frame Counts: Joint Learning of Video Segmentation and Optical Flow</summary>

- *Mingyu Ding, Zhe Wang, Bolei Zhou, Jianping Shi, Zhiwu Lu, Ping Luo*

- `1911.12739v1` - [abs](http://arxiv.org/abs/1911.12739v1) - [pdf](http://arxiv.org/pdf/1911.12739v1)

> A major challenge for video semantic segmentation is the lack of labeled data. In most benchmark datasets, only one frame of a video clip is annotated, which makes most supervised methods fail to utilize information from the rest of the frames. To exploit the spatio-temporal information in videos, many previous works use pre-computed optical flows, which encode the temporal consistency to improve the video segmentation. However, the video segmentation and optical flow estimation are still considered as two separate tasks. In this paper, we propose a novel framework for joint video semantic segmentation and optical flow estimation. Semantic segmentation brings semantic information to handle occlusion for more robust optical flow estimation, while the non-occluded optical flow provides accurate pixel-level temporal correspondences to guarantee the temporal consistency of the segmentation. Moreover, our framework is able to utilize both labeled and unlabeled frames in the video through joint training, while no additional calculation is required in inference. Extensive experiments show that the proposed model makes the video semantic segmentation and optical flow estimation benefit from each other and outperforms existing methods under the same settings in both tasks.

</details>

<details>

<summary>2019-11-28 15:38:53 - Inducing Relational Knowledge from BERT</summary>

- *Zied Bouraoui, Jose Camacho-Collados, Steven Schockaert*

- `1911.12753v1` - [abs](http://arxiv.org/abs/1911.12753v1) - [pdf](http://arxiv.org/pdf/1911.12753v1)

> One of the most remarkable properties of word embeddings is the fact that they capture certain types of semantic and syntactic relationships. Recently, pre-trained language models such as BERT have achieved groundbreaking results across a wide range of Natural Language Processing tasks. However, it is unclear to what extent such models capture relational knowledge beyond what is already captured by standard word embeddings. To explore this question, we propose a methodology for distilling relational knowledge from a pre-trained language model. Starting from a few seed instances of a given relation, we first use a large text corpus to find sentences that are likely to express this relation. We then use a subset of these extracted sentences as templates. Finally, we fine-tune a language model to predict whether a given word pair is likely to be an instance of some relation, when given an instantiated template for that relation as input.

</details>

<details>

<summary>2019-11-28 21:42:21 - Region segmentation via deep learning and convex optimization</summary>

- *Matthias Sonntag, Veniamin I. Morgenshtern*

- `1911.12870v1` - [abs](http://arxiv.org/abs/1911.12870v1) - [pdf](http://arxiv.org/pdf/1911.12870v1)

> In this paper, we propose a method to segment regions in three-dimensional point clouds. We assume that (i) the shape and the number of regions in the point cloud are not known and (ii) the point cloud may be noisy. The method consists of two steps. In the first step we use a deep neural network to predict the probability that a pair of small patches from the point cloud belongs to the same region. In the second step, we use a convex-optimization based method to improve the predictions of the network by enforcing consistency constraints. We evaluate the accuracy of our method on a custom dataset of convex polyhedra, where the regions correspond to the faces of the polyhedra. The method can be seen as a robust and flexible alternative to the famous region growing segmentation algorithm. All reported results are reproducible and come with easy to use code that could serve as a baseline for future research.

</details>

<details>

<summary>2019-11-29 04:40:30 - Deep Object Co-segmentation via Spatial-Semantic Network Modulation</summary>

- *Kaihua Zhang, Jin Chen, Bo Liu, Qingshan Liu*

- `1911.12950v1` - [abs](http://arxiv.org/abs/1911.12950v1) - [pdf](http://arxiv.org/pdf/1911.12950v1)

> Object co-segmentation is to segment the shared objects in multiple relevant images, which has numerous applications in computer vision. This paper presents a spatial and semantic modulated deep network framework for object co-segmentation. A backbone network is adopted to extract multi-resolution image features. With the multi-resolution features of the relevant images as input, we design a spatial modulator to learn a mask for each image. The spatial modulator captures the correlations of image feature descriptors via unsupervised learning. The learned mask can roughly localize the shared foreground object while suppressing the background. For the semantic modulator, we model it as a supervised image classification task. We propose a hierarchical second-order pooling module to transform the image features for classification use. The outputs of the two modulators manipulate the multi-resolution features by a shift-and-scale operation so that the features focus on segmenting co-object regions. The proposed model is trained end-to-end without any intricate post-processing. Extensive experiments on four image co-segmentation benchmark datasets demonstrate the superior accuracy of the proposed method compared to state-of-the-art methods.

</details>

<details>

<summary>2019-11-29 07:48:19 - Merging Weak and Active Supervision for Semantic Parsing</summary>

- *Ansong Ni, Pengcheng Yin, Graham Neubig*

- `1911.12986v1` - [abs](http://arxiv.org/abs/1911.12986v1) - [pdf](http://arxiv.org/pdf/1911.12986v1)

> A semantic parser maps natural language commands (NLs) from the users to executable meaning representations (MRs), which are later executed in certain environment to obtain user-desired results. The fully-supervised training of such parser requires NL/MR pairs, annotated by domain experts, which makes them expensive to collect. However, weakly-supervised semantic parsers are learnt only from pairs of NL and expected execution results, leaving the MRs latent. While weak supervision is cheaper to acquire, learning from this input poses difficulties. It demands that parsers search a large space with a very weak learning signal and it is hard to avoid spurious MRs that achieve the correct answer in the wrong way. These factors lead to a performance gap between parsers trained in weakly- and fully-supervised setting. To bridge this gap, we examine the intersection between weak supervision and active learning, which allows the learner to actively select examples and query for manual annotations as extra supervision to improve the model trained under weak supervision. We study different active learning heuristics for selecting examples to query, and various forms of extra supervision for such queries. We evaluate the effectiveness of our method on two different datasets. Experiments on the WikiSQL show that by annotating only 1.8% of examples, we improve over a state-of-the-art weakly-supervised baseline by 6.4%, achieving an accuracy of 79.0%, which is only 1.3% away from the model trained with full supervision. Experiments on WikiTableQuestions with human annotators show that our method can improve the performance with only 100 active queries, especially for weakly-supervised parsers learnt from a cold start.

</details>

<details>

<summary>2019-11-29 08:08:28 - Investigations on the inference optimization techniques and their impact on multiple hardware platforms for Semantic Segmentation</summary>

- *Sethu Hareesh Kolluru*

- `1911.12993v1` - [abs](http://arxiv.org/abs/1911.12993v1) - [pdf](http://arxiv.org/pdf/1911.12993v1)

> In this work, the task of pixel-wise semantic segmentation in the context of self-driving with a goal to reduce the inference time is explored. Fully Convolutional Network (FCN-8s, FCN-16s, and FCN-32s) with a VGG16 encoder architecture and skip connections is trained and validated on the Cityscapes dataset. Numerical investigations are carried out for several inference optimization techniques built into TensorFlow and TensorRT to quantify their impact on the inference time and network size. Finally, the trained network is ported on to an embedded platform (Nvidia Jetson TX1) and the inference time, as well as the total energy consumed for inference across hardware platforms, are compared.

</details>

<details>

<summary>2019-11-29 11:45:20 - Sparsely Grouped Input Variables for Neural Networks</summary>

- *Beibin Li, Nicholas Nuechterlein, Erin Barney, Caitlin Hudac, Pamela Ventola, Linda Shapiro, Frederick Shic*

- `1911.13068v1` - [abs](http://arxiv.org/abs/1911.13068v1) - [pdf](http://arxiv.org/pdf/1911.13068v1)

> In genomic analysis, biomarker discovery, image recognition, and other systems involving machine learning, input variables can often be organized into different groups by their source or semantic category. Eliminating some groups of variables can expedite the process of data acquisition and avoid over-fitting. Researchers have used the group lasso to ensure group sparsity in linear models and have extended it to create compact neural networks in meta-learning. Different from previous studies, we use multi-layer non-linear neural networks to find sparse groups for input variables. We propose a new loss function to regularize parameters for grouped input variables, design a new optimization algorithm for this loss function, and test these methods in three real-world settings. We achieve group sparsity for three datasets, maintaining satisfying results while excluding one nucleotide position from an RNA splicing experiment, excluding 89.9% of stimuli from an eye-tracking experiment, and excluding 60% of image rows from an experiment on the MNIST dataset.

</details>

<details>

<summary>2019-11-30 10:32:52 - Latent Semantic Search and Information Extraction Architecture</summary>

- *Anton Kolonin*

- `1912.00180v1` - [abs](http://arxiv.org/abs/1912.00180v1) - [pdf](http://arxiv.org/pdf/1912.00180v1)

> The motivation, concept, design and implementation of latent semantic search for search engines have limited semantic search, entity extraction and property attribution features, have insufficient accuracy and response time of latent search, may impose privacy concerns and the search results are unavailable in offline mode for robotic search operations. The alternative suggestion involves autonomous search engine with adaptive storage consumption, configurable search scope and latent search response time with built-in options for entity extraction and property attribution available as open source platform for mobile, desktop and server solutions. The suggested architecture attempts to implement artificial general intelligence (AGI) principles as long as autonomous behaviour constrained by limited resources is concerned, and it is applied for specific task of enabling Web search for artificial agents implementing the AGI.

</details>

<details>

<summary>2019-11-30 23:23:09 - DAST Model: Deciding About Semantic Complexity of a Text</summary>

- *MohammadReza Besharati, Mohammad Izadi*

- `1908.09080v5` - [abs](http://arxiv.org/abs/1908.09080v5) - [pdf](http://arxiv.org/pdf/1908.09080v5)

> Measuring text complexity is an essential task in several fields and applications (such as NLP, semantic web, smart education, etc.). The semantic layer of text is more tacit than its syntactic structure and, as a result, calculation of semantic complexity is more difficult than syntactic complexity. While there are famous and powerful academic and commercial syntactic complexity measures, the problem of measuring semantic complexity is still a challenging one. In this paper, we introduce the DAST model, which stands for Deciding About Semantic Complexity of a Text. DAST proposes an intuitionistic approach to semantics that lets us have a well-defined model for the semantics of a text and its complexity: semantic is considered as a lattice of intuitions and, as a result, semantic complexity is defined as the result of a calculation on this lattice. A set theoretic formal definition of semantic complexity, as a 6-tuple formal system, is provided. By using this formal system, a method for measuring semantic complexity is presented. The evaluation of the proposed approach is done by a set of three human-judgment experiments. The results show that DAST model is capable of deciding about semantic complexity of text. Furthermore, the analysis of the results leads us to introduce a Markovian model for the process of common-sense, multiple-steps and semantic-complexity reasoning in people. The results of Experiments demonstrate that our method outperforms the random baseline with improvement in better precision and competes with other methods by less error percentage.

</details>


## 2019-12

<details>

<summary>2019-12-01 02:48:04 - Estimating localized complexity of white-matter wiring with GANs</summary>

- *Haraldur T. Hallgrimsson, Richika Sharan, Scott T. Grafton, Ambuj K. Singh*

- `1910.04868v2` - [abs](http://arxiv.org/abs/1910.04868v2) - [pdf](http://arxiv.org/pdf/1910.04868v2)

> In-vivo examination of the physical connectivity of axonal projections through the white matter of the human brain is made possible by diffusion weighted magnetic resonance imaging (dMRI) Analysis of dMRI commonly considers derived scalar metrics such as fractional anisotrophy as proxies for "white matter integrity," and differences of such measures have been observed as significantly correlating with various neurological diagnosis and clinical measures such as executive function, presence of multiple sclerosis, and genetic similarity. The analysis of such voxel measures is confounded in areas of more complicated fiber wiring due to crossing, kissing, and dispersing fibers. Recently, Volz et al. introduced a simple probabilistic measure of the count of distinct fiber populations within a voxel, which was shown to reduce variance in group comparisons. We propose a complementary measure that considers the complexity of a voxel in context of its local region, with an aim to quantify the localized wiring complexity of every part of white matter. This allows, for example, identification of particularly ambiguous regions of the brain for tractographic approaches of modeling global wiring connectivity. Our method builds on recent advances in image inpainting, in which the task is to plausibly fill in a missing region of an image. Our proposed method builds on a Bayesian estimate of heteroscedastic aleatoric uncertainty of a region of white matter by inpainting it from its context. We define the localized wiring complexity of white matter as how accurately and confidently a well-trained model can predict the missing patch. In our results, we observe low aleatoric uncertainty along major neuronal pathways which increases at junctions and towards cortex boundaries. This directly quantifies the difficulty of lesion inpainting of dMRI images at all parts of white matter.

</details>

<details>

<summary>2019-12-01 03:08:59 - Enhancing Item Response Theory for Cognitive Diagnosis</summary>

- *Song Cheng, Qi Liu*

- `1905.10957v3` - [abs](http://arxiv.org/abs/1905.10957v3) - [pdf](http://arxiv.org/pdf/1905.10957v3)

> Cognitive diagnosis is a fundamental and crucial task in many educational applications, e.g., computer adaptive test and cognitive assignments. Item Response Theory (IRT) is a classical cognitive diagnosis method which can provide interpretable parameters (i.e., student latent trait, question discrimination, and difficulty) for analyzing student performance. However, traditional IRT ignores the rich information in question texts, cannot diagnose knowledge concept proficiency, and it is inaccurate to diagnose the parameters for the questions which only appear several times. To this end, in this paper, we propose a general Deep Item Response Theory (DIRT) framework to enhance traditional IRT for cognitive diagnosis by exploiting semantic representation from question texts with deep learning. In DIRT, we first use a proficiency vector to represent students' proficiency in knowledge concepts and embed question texts and knowledge concepts to dense vectors by Word2Vec. Then, we design a deep diagnosis module to diagnose parameters in traditional IRT by deep learning techniques. Finally, with the diagnosed parameters, we input them into the logistic-like formula of IRT to predict student performance. Extensive experimental results on real-world data clearly demonstrate the effectiveness and interpretation power of DIRT framework.

</details>

<details>

<summary>2019-12-01 07:31:56 - Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder</summary>

- *Li Du, Xiao Ding, Ting Liu, Zhongyang Li*

- `1909.08824v3` - [abs](http://arxiv.org/abs/1909.08824v3) - [pdf](http://arxiv.org/pdf/1909.08824v3)

> Understanding event and event-centered commonsense reasoning are crucial for natural language processing (NLP). Given an observed event, it is trivial for human to infer its intents and effects, while this type of If-Then reasoning still remains challenging for NLP systems. To facilitate this, a If-Then commonsense reasoning dataset Atomic is proposed, together with an RNN-based Seq2Seq model to conduct such reasoning. However, two fundamental problems still need to be addressed: first, the intents of an event may be multiple, while the generations of RNN-based Seq2Seq models are always semantically close; second, external knowledge of the event background may be necessary for understanding events and conducting the If-Then reasoning. To address these issues, we propose a novel context-aware variational autoencoder effectively learning event background information to guide the If-Then reasoning. Experimental results show that our approach improves the accuracy and diversity of inferences compared with state-of-the-art baseline methods.

</details>

<details>

<summary>2019-12-01 15:06:44 - Semantic Enrichment of Streaming Healthcare Data</summary>

- *Daniel Cotter, V. K. Cody Bumgardner*

- `1912.00423v1` - [abs](http://arxiv.org/abs/1912.00423v1) - [pdf](http://arxiv.org/pdf/1912.00423v1)

> In the past decade, the healthcare industry has made significant advances in the digitization of patient information. However, a lack of interoperability among healthcare systems still imposes a high cost to patients, hospitals, and insurers. Currently, most systems pass messages using idiosyncratic messaging standards that require specialized knowledge to interpret. This increases the cost of systems integration and often puts more advanced uses of data out of reach. In this project, we demonstrate how two open standards, FHIR and RDF, can be combined both to integrate data from disparate sources in real-time and make that data queryable and susceptible to automated inference. To validate the effectiveness of the semantic engine, we perform simulations of real-time data feeds and demonstrate how they can be combined and used by client-side applications with no knowledge of the underlying sources.

</details>

<details>

<summary>2019-12-01 21:32:11 - Interpreting Context of Images using Scene Graphs</summary>

- *Himangi Mittal, Ajith Abraham, Anuja Arora*

- `1912.00501v1` - [abs](http://arxiv.org/abs/1912.00501v1) - [pdf](http://arxiv.org/pdf/1912.00501v1)

> Understanding a visual scene incorporates objects, relationships, and context. Traditional methods working on an image mostly focus on object detection and fail to capture the relationship between the objects. Relationships can give rich semantic information about the objects in a scene. The context can be conducive to comprehending an image since it will help us to perceive the relation between the objects and thus, give us a deeper insight into the image. Through this idea, our project delivers a model that focuses on finding the context present in an image by representing the image as a graph, where the nodes will the objects and edges will be the relation between them. The context is found using the visual and semantic cues which are further concatenated and given to the Support Vector Machines (SVM) to detect the relation between two objects. This presents us with the context of the image which can be further used in applications such as similar image retrieval, image captioning, or story generation.

</details>

<details>

<summary>2019-12-01 22:15:08 - Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark</summary>

- *Cody Coleman, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao, Jian Zhang, Peter Bailis, Kunle Olukotun, Chris Re, Matei Zaharia*

- `1806.01427v2` - [abs](http://arxiv.org/abs/1806.01427v2) - [pdf](http://arxiv.org/pdf/1806.01427v2)

> Researchers have proposed hardware, software, and algorithmic optimizations to improve the computational performance of deep learning. While some of these optimizations perform the same operations faster (e.g., increasing GPU clock speed), many others modify the semantics of the training procedure (e.g., reduced precision), and can impact the final model's accuracy on unseen data. Due to a lack of standard evaluation criteria that considers these trade-offs, it is difficult to directly compare these optimizations. To address this problem, we recently introduced DAWNBench, a benchmark competition focused on end-to-end training time to achieve near-state-of-the-art accuracy on an unseen dataset---a combined metric called time-to-accuracy (TTA). In this work, we analyze the entries from DAWNBench, which received optimized submissions from multiple industrial groups, to investigate the behavior of TTA as a metric as well as trends in the best-performing entries. We show that TTA has a low coefficient of variation and that models optimized for TTA generalize nearly as well as those trained using standard methods. Additionally, even though DAWNBench entries were able to train ImageNet models in under 3 minutes, we find they still underutilize hardware capabilities such as Tensor Cores. Furthermore, we find that distributed entries can spend more than half of their time on communication. We show similar findings with entries to the MLPERF v0.5 benchmark.

</details>

<details>

<summary>2019-12-02 01:18:37 - Probing Natural Language Inference Models through Semantic Fragments</summary>

- *Kyle Richardson, Hai Hu, Lawrence S. Moss, Ashish Sabharwal*

- `1909.07521v2` - [abs](http://arxiv.org/abs/1909.07521v2) - [pdf](http://arxiv.org/pdf/1909.07521v2)

> Do state-of-the-art models for language understanding already have, or can they easily learn, abilities such as boolean coordination, quantification, conditionals, comparatives, and monotonicity reasoning (i.e., reasoning about word substitutions in sentential contexts)? While such phenomena are involved in natural language inference (NLI) and go beyond basic linguistic understanding, it is unclear the extent to which they are captured in existing NLI benchmarks and effectively learned by models. To investigate this, we propose the use of semantic fragments---systematically generated datasets that each target a different semantic phenomenon---for probing, and efficiently improving, such capabilities of linguistic models. This approach to creating challenge datasets allows direct control over the semantic diversity and complexity of the targeted linguistic phenomena, and results in a more precise characterization of a model's linguistic behavior. Our experiments, using a library of 8 such semantic fragments, reveal two remarkable findings: (a) State-of-the-art models, including BERT, that are pre-trained on existing NLI benchmark datasets perform poorly on these new fragments, even though the phenomena probed here are central to the NLI task. (b) On the other hand, with only a few minutes of additional fine-tuning---with a carefully selected learning rate and a novel variation of "inoculation"---a BERT-based model can master all of these logic and monotonicity fragments while retaining its performance on established NLI benchmarks.

</details>

<details>

<summary>2019-12-02 08:50:27 - Meta-Path Constrained Random Walk Inference for Large-Scale Heterogeneous Information Networks</summary>

- *Chenguang Wang*

- `1912.00634v1` - [abs](http://arxiv.org/abs/1912.00634v1) - [pdf](http://arxiv.org/pdf/1912.00634v1)

> Heterogeneous information network (HIN) has shown its power of modeling real world data as a multi-typed entity-relation graph. Meta-path is the key contributor to this power since it enables inference by capturing the proximities between entities via rich semantic links. Previous HIN studies ask users to provide either 1) the meta-path(s) directly or 2) biased examples to generate the meta-path(s). However, lots of HINs (e.g., YAGO2 and Freebase) have rich schema consisting of a sophisticated and large number of types of entities and relations. It is impractical for users to provide the meta-path(s) to support the large scale inference, and biased examples will result in incorrect meta-path based inference, thus limit the power of the meta-path. In this paper, we propose a meta-path constrained inference framework to further release the ability of the meta-path, by efficiently learning the HIN inference patterns via a carefully designed tree structure; and performing unbiased random walk inference with little user guidance. The experiment results on YAGO2 and DBLP datasets show the state-of-the-art performance of the meta-path constrained inference framework.

</details>

<details>

<summary>2019-12-02 12:25:22 - Bridging the Gap between Semantics and Multimedia Processing</summary>

- *Marcio Ferreira Moreno, Guilherme Lima, Rodrigo Costa Mesquita Santos, Roberto Azevedo, Markus Endler*

- `1911.11631v2` - [abs](http://arxiv.org/abs/1911.11631v2) - [pdf](http://arxiv.org/pdf/1911.11631v2)

> In this paper, we give an overview of the semantic gap problem in multimedia and discuss how machine learning and symbolic AI can be combined to narrow this gap. We describe the gap in terms of a classical architecture for multimedia processing and discuss a structured approach to bridge it. This approach combines machine learning (for mapping signals to objects) and symbolic AI (for linking objects to meanings). Our main goal is to raise awareness and discuss the challenges involved in this structured approach to multimedia understanding, especially in the view of the latest developments in machine learning and symbolic AI.

</details>

<details>

<summary>2019-12-02 16:20:42 - SADA: Semantic Adversarial Diagnostic Attacks for Autonomous Applications</summary>

- *Abdullah Hamdi, Matthias Müller, Bernard Ghanem*

- `1812.02132v3` - [abs](http://arxiv.org/abs/1812.02132v3) - [pdf](http://arxiv.org/pdf/1812.02132v3)

> One major factor impeding more widespread adoption of deep neural networks (DNNs) is their lack of robustness, which is essential for safety-critical applications such as autonomous driving. This has motivated much recent work on adversarial attacks for DNNs, which mostly focus on pixel-level perturbations void of semantic meaning. In contrast, we present a general framework for adversarial attacks on trained agents, which covers semantic perturbations to the environment of the agent performing the task as well as pixel-level attacks. To do this, we re-frame the adversarial attack problem as learning a distribution of parameters that always fools the agent. In the semantic case, our proposed adversary (denoted as BBGAN) is trained to sample parameters that describe the environment with which the black-box agent interacts, such that the agent performs its dedicated task poorly in this environment. We apply BBGAN on three different tasks, primarily targeting aspects of autonomous navigation: object detection, self-driving, and autonomous UAV racing. On these tasks, BBGAN can generate failure cases that consistently fool a trained agent.

</details>

<details>

<summary>2019-12-02 20:18:54 - Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation</summary>

- *Janis Postels, Francesco Ferroni, Huseyin Coskun, Nassir Navab, Federico Tombari*

- `1908.00598v3` - [abs](http://arxiv.org/abs/1908.00598v3) - [pdf](http://arxiv.org/pdf/1908.00598v3)

> We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo sampling at inference time to estimate this quantity (e.g. Monte-Carlo dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead.

</details>

<details>

<summary>2019-12-02 23:00:13 - Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling</summary>

- *Jeremy Gordon, David Rawlinson, Subutai Ahmad*

- `1912.01116v1` - [abs](http://arxiv.org/abs/1912.01116v1) - [pdf](http://arxiv.org/pdf/1912.01116v1)

> In sequence learning tasks such as language modelling, Recurrent Neural Networks must learn relationships between input features separated by time. State of the art models such as LSTM and Transformer are trained by backpropagation of losses into prior hidden states and inputs held in memory. This allows gradients to flow from present to past and effectively learn with perfect hindsight, but at a significant memory cost. In this paper we show that it is possible to train high performance recurrent networks using information that is local in time, and thereby achieve a significantly reduced memory footprint. We describe a predictive autoencoder called bRSM featuring recurrent connections, sparse activations, and a boosting rule for improved cell utilization. The architecture demonstrates near optimal performance on a non-deterministic (stochastic) partially-observable sequence learning task consisting of high-Markov-order sequences of MNIST digits. We find that this model learns these sequences faster and more completely than an LSTM, and offer several possible explanations why the LSTM architecture might struggle with the partially observable sequence structure in this task. We also apply our model to a next word prediction task on the Penn Treebank (PTB) dataset. We show that a 'flattened' RSM network, when paired with a modern semantic word embedding and the addition of boosting, achieves 103.5 PPL (a 20-point improvement over the best N-gram models), beating ordinary RNNs trained with BPTT and approaching the scores of early LSTM implementations. This work provides encouraging evidence that strong results on challenging tasks such as language modelling may be possible using less memory intensive, biologically-plausible training regimes.

</details>

<details>

<summary>2019-12-03 06:46:20 - SemEval-2016 Task 4: Sentiment Analysis in Twitter</summary>

- *Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, Veselin Stoyanov*

- `1912.01973v1` - [abs](http://arxiv.org/abs/1912.01973v1) - [pdf](http://arxiv.org/pdf/1912.01973v1)

> This paper discusses the fourth year of the ``Sentiment Analysis in Twitter Task''. SemEval-2016 Task 4 comprises five subtasks, three of which represent a significant departure from previous editions. The first two subtasks are reruns from prior years and ask to predict the overall sentiment, and the sentiment towards a topic in a tweet. The three new subtasks focus on two variants of the basic ``sentiment classification in Twitter'' task. The first variant adopts a five-point scale, which confers an ordinal character to the classification task. The second variant focuses on the correct estimation of the prevalence of each class of interest, a task which has been called quantification in the supervised learning literature. The task continues to be very popular, attracting a total of 43 teams.

</details>

<details>

<summary>2019-12-03 07:02:38 - Modelling Semantic Categories using Conceptual Neighborhood</summary>

- *Zied Bouraoui, Jose Camacho-Collados, Luis Espinosa-Anke, Steven Schockaert*

- `1912.01220v1` - [abs](http://arxiv.org/abs/1912.01220v1) - [pdf](http://arxiv.org/pdf/1912.01220v1)

> While many methods for learning vector space embeddings have been proposed in the field of Natural Language Processing, these methods typically do not distinguish between categories and individuals. Intuitively, if individuals are represented as vectors, we can think of categories as (soft) regions in the embedding space. Unfortunately, meaningful regions can be difficult to estimate, especially since we often have few examples of individuals that belong to a given category. To address this issue, we rely on the fact that different categories are often highly interdependent. In particular, categories often have conceptual neighbors, which are disjoint from but closely related to the given category (e.g.\ fruit and vegetable). Our hypothesis is that more accurate category representations can be learned by relying on the assumption that the regions representing such conceptual neighbors should be adjacent in the embedding space. We propose a simple method for identifying conceptual neighbors and then show that incorporating these conceptual neighbors indeed leads to more accurate region based representations.

</details>

<details>

<summary>2019-12-03 10:45:53 - A Formal Approach to the Engineering of Domain-Specific Distributed Systems</summary>

- *Rocco De Nicola, Gianluigi Ferrari, Rosario Pugliese, Francesco Tiezzi*

- `1912.01289v1` - [abs](http://arxiv.org/abs/1912.01289v1) - [pdf](http://arxiv.org/pdf/1912.01289v1)

> We review some results regarding specification, programming and verification of different classes of distributed systems which stemmed from the research of the Concurrency and Mobility Group at University of Firenze. More specifically, we examine the distinguishing features of network-aware programming, service-oriented computing, autonomic computing, and collective adaptive systems programming. We then present an overview of four different languages, namely Klaim, Cows, Scel and AbC. For each language, we discuss design choices, present syntax and semantics, show how the different formalisms can be used to model and program a travel booking scenario, and describe programming environments and verification techniques.

</details>

<details>

<summary>2019-12-03 16:51:09 - Multiscale Self Attentive Convolutions for Vision and Language Modeling</summary>

- *Oren Barkan*

- `1912.01521v1` - [abs](http://arxiv.org/abs/1912.01521v1) - [pdf](http://arxiv.org/pdf/1912.01521v1)

> Self attention mechanisms have become a key building block in many state-of-the-art language understanding models. In this paper, we show that the self attention operator can be formulated in terms of 1x1 convolution operations. Following this observation, we propose several novel operators: First, we introduce a 2D version of self attention that is applicable for 2D signals such as images. Second, we present the 1D and 2D Self Attentive Convolutions (SAC) operator that generalizes self attention beyond 1x1 convolutions to 1xm and nxm convolutions, respectively. While 1D and 2D self attention operate on individual words and pixels, SAC operates on m-grams and image patches, respectively. Third, we present a multiscale version of SAC (MSAC) which analyzes the input by employing multiple SAC operators that vary by filter size, in parallel. Finally, we explain how MSAC can be utilized for vision and language modeling, and further harness MSAC to form a cross attentive image similarity machinery.

</details>

<details>

<summary>2019-12-03 17:14:53 - Quantifying Urban Canopy Cover with Deep Convolutional Neural Networks</summary>

- *Bill Cai, Xiaojiang Li, Carlo Ratti*

- `1912.02109v1` - [abs](http://arxiv.org/abs/1912.02109v1) - [pdf](http://arxiv.org/pdf/1912.02109v1)

> Urban canopy cover is important to mitigate the impact of climate change. Yet, existing quantification of urban greenery is either manual and not scalable, or use traditional computer vision methods that are inaccurate. We train deep convolutional neural networks (DCNNs) on datasets used for self-driving cars to estimate urban greenery instead, and find that our semantic segmentation and direct end-to-end estimation method are more accurate and scalable, reducing mean absolute error of estimating the Green View Index (GVI) metric from 10.1% to 4.67%. With the revised DCNN methods, the Treepedia project was able to scale and analyze canopy cover in 22 cities internationally, sparking interest and action in public policy and research fields.

</details>

<details>

<summary>2019-12-03 19:17:12 - Zero Shot Learning with the Isoperimetric Loss</summary>

- *Shay Deutsch, Andrea Bertozzi, Stefano Soatto*

- `1903.06781v2` - [abs](http://arxiv.org/abs/1903.06781v2) - [pdf](http://arxiv.org/pdf/1903.06781v2)

> We introduce the isoperimetric loss as a regularization criterion for learning the map from a visual representation to a semantic embedding, to be used to transfer knowledge to unknown classes in a zero-shot learning setting. We use a pre-trained deep neural network model as a visual representation of image data, a Word2Vec embedding of class labels, and linear maps between the visual and semantic embedding spaces. However, the spaces themselves are not linear, and we postulate the sample embedding to be populated by noisy samples near otherwise smooth manifolds. We exploit the graph structure defined by the sample points to regularize the estimates of the manifolds by inferring the graph connectivity using a generalization of the isoperimetric inequalities from Riemannian geometry to graphs. Surprisingly, this regularization alone, paired with the simplest baseline model, outperforms the state-of-the-art among fully automated methods in zero-shot learning benchmarks such as AwA and CUB. This improvement is achieved solely by learning the structure of the underlying spaces by imposing regularity.

</details>

<details>

<summary>2019-12-03 19:30:19 - LXMERT: Learning Cross-Modality Encoder Representations from Transformers</summary>

- *Hao Tan, Mohit Bansal*

- `1908.07490v3` - [abs](http://arxiv.org/abs/1908.07490v3) - [pdf](http://arxiv.org/pdf/1908.07490v3)

> Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert

</details>

<details>

<summary>2019-12-03 22:36:39 - Deep Metric Learning by Online Soft Mining and Class-Aware Attention</summary>

- *Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Neil M. Robertson*

- `1811.01459v3` - [abs](http://arxiv.org/abs/1811.01459v3) - [pdf](http://arxiv.org/pdf/1811.01459v3)

> Deep metric learning aims to learn a deep embedding that can capture the semantic similarity of data points. Given the availability of massive training samples, deep metric learning is known to suffer from slow convergence due to a large fraction of trivial samples. Therefore, most existing methods generally resort to sample mining strategies for selecting nontrivial samples to accelerate convergence and improve performance. In this work, we identify two critical limitations of the sample mining methods, and provide solutions for both of them. First, previous mining methods assign one binary score to each sample, i.e., dropping or keeping it, so they only selects a subset of relevant samples in a mini-batch. Therefore, we propose a novel sample mining method, called Online Soft Mining (OSM), which assigns one continuous score to each sample to make use of all samples in the mini-batch. OSM learns extended manifolds that preserve useful intraclass variances by focusing on more similar positives. Second, the existing methods are easily influenced by outliers as they are generally included in the mined subset. To address this, we introduce Class-Aware Attention (CAA) that assigns little attention to abnormal data samples. Furthermore, by combining OSM and CAA, we propose a novel weighted contrastive loss to learn discriminative embeddings. Extensive experiments on two fine-grained visual categorisation datasets and two video-based person re-identification benchmarks show that our method significantly outperforms the state-of-the-art.

</details>

<details>

<summary>2019-12-04 00:14:28 - Global Saliency: Aggregating Saliency Maps to Assess Dataset Artefact Bias</summary>

- *Jacob Pfau, Albert T. Young, Maria L. Wei, Michael J. Keiser*

- `1910.07604v2` - [abs](http://arxiv.org/abs/1910.07604v2) - [pdf](http://arxiv.org/pdf/1910.07604v2)

> In high-stakes applications of machine learning models, interpretability methods provide guarantees that models are right for the right reasons. In medical imaging, saliency maps have become the standard tool for determining whether a neural model has learned relevant robust features, rather than artefactual noise. However, saliency maps are limited to local model explanation because they interpret predictions on an image-by-image basis. We propose aggregating saliency globally, using semantic segmentation masks, to provide quantitative measures of model bias across a dataset. To evaluate global saliency methods, we propose two metrics for quantifying the validity of saliency explanations. We apply the global saliency method to skin lesion diagnosis to determine the effect of artefacts, such as ink, on model bias.

</details>

<details>

<summary>2019-12-04 04:39:32 - Towards Building a Multilingual Sememe Knowledge Base: Predicting Sememes for BabelNet Synsets</summary>

- *Fanchao Qi, Liang Chang, Maosong Sun, Sicong Ouyang, Zhiyuan Liu*

- `1912.01795v1` - [abs](http://arxiv.org/abs/1912.01795v1) - [pdf](http://arxiv.org/pdf/1912.01795v1)

> A sememe is defined as the minimum semantic unit of human languages. Sememe knowledge bases (KBs), which contain words annotated with sememes, have been successfully applied to many NLP tasks. However, existing sememe KBs are built on only a few languages, which hinders their widespread utilization. To address the issue, we propose to build a unified sememe KB for multiple languages based on BabelNet, a multilingual encyclopedic dictionary. We first build a dataset serving as the seed of the multilingual sememe KB. It manually annotates sememes for over $15$ thousand synsets (the entries of BabelNet). Then, we present a novel task of automatic sememe prediction for synsets, aiming to expand the seed dataset into a usable KB. We also propose two simple and effective models, which exploit different information of synsets. Finally, we conduct quantitative and qualitative analyses to explore important factors and difficulties in the task. All the source code and data of this work can be obtained on https://github.com/thunlp/BabelNet-Sememe-Prediction.

</details>

<details>

<summary>2019-12-04 07:22:32 - Towards Constructing a Corpus for Studying the Effects of Treatments and Substances Reported in PubMed Abstracts</summary>

- *Evgeni Stefchov, Galia Angelova, Preslav Nakov*

- `1912.01831v1` - [abs](http://arxiv.org/abs/1912.01831v1) - [pdf](http://arxiv.org/pdf/1912.01831v1)

> We present the construction of an annotated corpus of PubMed abstracts reporting about positive, negative or neutral effects of treatments or substances. Our ultimate goal is to annotate one sentence (rationale) for each abstract and to use this resource as a training set for text classification of effects discussed in PubMed abstracts. Currently, the corpus consists of 750 abstracts. We describe the automatic processing that supports the corpus construction, the manual annotation activities and some features of the medical language in the abstracts selected for the annotated corpus. It turns out that recognizing the terminology and the abbreviations is key for determining the rationale sentence. The corpus will be applied to improve our classifier, which currently has accuracy of 78.80% achieved with normalization of the abstract terms based on UMLS concepts from specific semantic groups and an SVM with a linear kernel. Finally, we discuss some other possible applications of this corpus.

</details>

<details>

<summary>2019-12-04 09:16:36 - Enhancing Relation Extraction Using Syntactic Indicators and Sentential Contexts</summary>

- *Qiongxing Tao, Xiangfeng Luo, Hao Wang*

- `1912.01858v1` - [abs](http://arxiv.org/abs/1912.01858v1) - [pdf](http://arxiv.org/pdf/1912.01858v1)

> State-of-the-art methods for relation extraction consider the sentential context by modeling the entire sentence. However, syntactic indicators, certain phrases or words like prepositions that are more informative than other words and may be beneficial for identifying semantic relations. Other approaches using fixed text triggers capture such information but ignore the lexical diversity. To leverage both syntactic indicators and sentential contexts, we propose an indicator-aware approach for relation extraction. Firstly, we extract syntactic indicators under the guidance of syntactic knowledge. Then we construct a neural network to incorporate both syntactic indicators and the entire sentences into better relation representations. By this way, the proposed model alleviates the impact of noisy information from entire sentences and breaks the limit of text triggers. Experiments on the SemEval-2010 Task 8 benchmark dataset show that our model significantly outperforms the state-of-the-art methods.

</details>

<details>

<summary>2019-12-04 13:59:48 - Self-Supervised Learning of Pretext-Invariant Representations</summary>

- *Ishan Misra, Laurens van der Maaten*

- `1912.01991v1` - [abs](http://arxiv.org/abs/1912.01991v1) - [pdf](http://arxiv.org/pdf/1912.01991v1)

> The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as "pearl") that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.

</details>

<details>

<summary>2019-12-04 14:26:08 - Rare Words: A Major Problem for Contextualized Embeddings And How to Fix it by Attentive Mimicking</summary>

- *Timo Schick, Hinrich Schütze*

- `1904.06707v4` - [abs](http://arxiv.org/abs/1904.06707v4) - [pdf](http://arxiv.org/pdf/1904.06707v4)

> Pretraining deep neural network architectures with a language modeling objective has brought large improvements for many natural language processing tasks. Exemplified by BERT, a recently proposed such architecture, we demonstrate that despite being trained on huge amounts of data, deep language models still struggle to understand rare words. To fix this problem, we adapt Attentive Mimicking, a method that was designed to explicitly learn embeddings for rare words, to deep language models. In order to make this possible, we introduce one-token approximation, a procedure that enables us to use Attentive Mimicking even when the underlying language model uses subword-based tokenization, i.e., it does not assign embeddings to all words. To evaluate our method, we create a novel dataset that tests the ability of language models to capture semantic properties of words without any task-specific fine-tuning. Using this dataset, we show that adding our adapted version of Attentive Mimicking to BERT does indeed substantially improve its understanding of rare words.

</details>

<details>

<summary>2019-12-04 14:27:34 - Using Sequence-to-Sequence Learning for Repairing C Vulnerabilities</summary>

- *Zimin Chen, Steve Kommrusch, Martin Monperrus*

- `1912.02015v1` - [abs](http://arxiv.org/abs/1912.02015v1) - [pdf](http://arxiv.org/pdf/1912.02015v1)

> Software vulnerabilities affect all businesses and research is being done to avoid, detect or repair them. In this article, we contribute a new technique for automatic vulnerability fixing. We present a system that uses the rich software development history that can be found on GitHub to train an AI system that generates patches. We apply sequence-to-sequence learning on a big dataset of code changes and we evaluate the trained system on real world vulnerabilities from the CVE database. The result shows the feasibility of using sequence-to-sequence learning for fixing software vulnerabilities.

</details>

<details>

<summary>2019-12-04 14:43:46 - Implicit Knowledge in Argumentative Texts: An Annotated Corpus</summary>

- *Maria Becker, Katharina Korfhage, Anette Frank*

- `1912.10161v1` - [abs](http://arxiv.org/abs/1912.10161v1) - [pdf](http://arxiv.org/pdf/1912.10161v1)

> When speaking or writing, people omit information that seems clear and evident, such that only part of the message is expressed in words. Especially in argumentative texts it is very common that (important) parts of the argument are implied and omitted. We hypothesize that for argument analysis it will be beneficial to reconstruct this implied information. As a starting point for filling such knowledge gaps, we build a corpus consisting of high-quality human annotations of missing and implied information in argumentative texts. To learn more about the characteristics of both the argumentative texts and the added information, we further annotate the data with semantic clause types and commonsense knowledge relations. The outcome of our work is a carefully de-signed and richly annotated dataset, for which we then provide an in-depth analysis by investigating characteristic distributions and correlations of the assigned labels. We reveal interesting patterns and intersections between the annotation categories and properties of our dataset, which enable insights into the characteristics of both argumentative texts and implicit knowledge in terms of structural features and semantic information. The results of our analysis can help to assist automated argument analysis and can guide the process of revealing implicit information in argumentative texts automatically.

</details>

<details>

<summary>2019-12-04 16:56:56 - On the Possibility of Rewarding Structure Learning Agents: Mutual Information on Linguistic Random Sets</summary>

- *Ignacio Arroyo-Fernández, Mauricio Carrasco-Ruíz, J. Anibal Arias-Aguilar*

- `1910.04023v4` - [abs](http://arxiv.org/abs/1910.04023v4) - [pdf](http://arxiv.org/pdf/1910.04023v4)

> We present a first attempt to elucidate a theoretical and empirical approach to design the reward provided by a natural language environment to some structure learning agent. To this end, we revisit the Information Theory of unsupervised induction of phrase-structure grammars to characterize the behavior of simulated actions modeled as set-valued random variables (random sets of linguistic samples) constituting semantic structures. Our results showed empirical evidence of that simulated semantic structures (Open Information Extraction triplets) can be distinguished from randomly constructed ones by observing the Mutual Information among their constituents. This suggests the possibility of rewarding structure learning agents without using pretrained structural analyzers (oracle actors/experts).

</details>

<details>

<summary>2019-12-04 16:59:40 - Keyword Aware Influential Community Search in Large Attributed Graphs</summary>

- *Md. Saiful Islam, Mohammed Eunus Ali, Yong-Bin Kang, Timos Sellis, Farhana M. Choudhury*

- `1912.02114v1` - [abs](http://arxiv.org/abs/1912.02114v1) - [pdf](http://arxiv.org/pdf/1912.02114v1)

> We introduce a novel keyword-aware influential community query KICQ that finds the most influential communities from an attributed graph, where an influential community is defined as a closely connected group of vertices having some dominance over other groups of vertices with the expertise (a set of keywords) matching with the query terms (words or phrases). We first design the KICQ that facilitates users to issue an influential CS query intuitively by using a set of query terms, and predicates (AND or OR). In this context, we propose a novel word-embedding based similarity model that enables semantic community search, which substantially alleviates the limitations of exact keyword based community search. Next, we propose a new influence measure for a community that considers both the cohesiveness and influence of the community and eliminates the need for specifying values of internal parameters of a network. Finally, we propose two efficient algorithms for searching influential communities in large attributed graphs. We present detailed experiments and a case study to demonstrate the effectiveness and efficiency of the proposed approaches.

</details>

<details>

<summary>2019-12-04 17:35:03 - AMUSED: A Multi-Stream Vector Representation Method for Use in Natural Dialogue</summary>

- *Gaurav Kumar, Rishabh Joshi, Jaspreet Singh, Promod Yenigalla*

- `1912.10160v1` - [abs](http://arxiv.org/abs/1912.10160v1) - [pdf](http://arxiv.org/pdf/1912.10160v1)

> The problem of building a coherent and non-monotonous conversational agent with proper discourse and coverage is still an area of open research. Current architectures only take care of semantic and contextual information for a given query and fail to completely account for syntactic and external knowledge which are crucial for generating responses in a chit-chat system. To overcome this problem, we propose an end to end multi-stream deep learning architecture which learns unified embeddings for query-response pairs by leveraging contextual information from memory networks and syntactic information by incorporating Graph Convolution Networks (GCN) over their dependency parse. A stream of this network also utilizes transfer learning by pre-training a bidirectional transformer to extract semantic representation for each input sentence and incorporates external knowledge through the the neighborhood of the entities from a Knowledge Base (KB). We benchmark these embeddings on next sentence prediction task and significantly improve upon the existing techniques. Furthermore, we use AMUSED to represent query and responses along with its context to develop a retrieval based conversational agent which has been validated by expert linguists to have comprehensive engagement with humans.

</details>

<details>

<summary>2019-12-05 01:20:56 - 3D Objectness Estimation via Bottom-up Regret Grouping</summary>

- *Zelin Ye, Yan Hao, Liang Xu, Rui Zhu, Cewu Lu*

- `1912.02332v1` - [abs](http://arxiv.org/abs/1912.02332v1) - [pdf](http://arxiv.org/pdf/1912.02332v1)

> 3D objectness estimation, namely discovering semantic objects from 3D scene, is a challenging and significant task in 3D understanding. In this paper, we propose a 3D objectness method working in a bottom-up manner. Beginning with over-segmented 3D segments, we iteratively group them into object proposals by learning an ingenious grouping predictor to determine whether two 3D segments can be grouped or not. To enhance robustness, a novel regret mechanism is presented to withdraw incorrect grouping operations. Hence the irreparable consequences brought by mistaken grouping in prior bottom-up works can be greatly reduced. Our experiments show that our method outperforms state-of-the-art 3D objectness methods with a small number of proposals in two difficult datasets, GMU-kitchen and CTD. Further ablation study also demonstrates the effectiveness of our grouping predictor and regret mechanism.

</details>

<details>

<summary>2019-12-05 04:52:24 - PhoneBit: Efficient GPU-Accelerated Binary Neural Network Inference Engine for Mobile Phones</summary>

- *Gang Chen, Shengyu He, Haitao Meng, Kai Huang*

- `1912.04050v1` - [abs](http://arxiv.org/abs/1912.04050v1) - [pdf](http://arxiv.org/pdf/1912.04050v1)

> Over the last years, a great success of deep neural networks (DNNs) has been witnessed in computer vision and other fields. However, performance and power constraints make it still challenging to deploy DNNs on mobile devices due to their high computational complexity. Binary neural networks (BNNs) have been demonstrated as a promising solution to achieve this goal by using bit-wise operations to replace most arithmetic operations. Currently, existing GPU-accelerated implementations of BNNs are only tailored for desktop platforms. Due to architecture differences, mere porting of such implementations to mobile devices yields suboptimal performance or is impossible in some cases. In this paper, we propose PhoneBit, a GPU-accelerated BNN inference engine for Android-based mobile devices that fully exploits the computing power of BNNs on mobile GPUs. PhoneBit provides a set of operator-level optimizations including locality-friendly data layout, bit packing with vectorization and layers integration for efficient binary convolution. We also provide a detailed implementation and parallelization optimization for PhoneBit to optimally utilize the memory bandwidth and computing power of mobile GPUs. We evaluate PhoneBit with AlexNet, YOLOv2 Tiny and VGG16 with their binary version. Our experiment results show that PhoneBit can achieve significant speedup and energy efficiency compared with state-of-the-art frameworks for mobile devices.

</details>

<details>

<summary>2019-12-05 05:17:05 - Pairwise Neural Machine Translation Evaluation</summary>

- *Francisco Guzman, Shafiq Joty, Lluis Marquez, Preslav Nakov*

- `1912.03135v1` - [abs](http://arxiv.org/abs/1912.03135v1) - [pdf](http://arxiv.org/pdf/1912.03135v1)

> We present a novel framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is compacted into relatively small distributed vector representations, and fed into a multi-layer neural network that models the interaction between each of the hypotheses and the reference, as well as between the two hypotheses. These compact representations are in turn based on word and sentence embeddings, which are learned using neural networks. The framework is flexible, allows for efficient learning and classification, and yields correlation with humans that rivals the state of the art.

</details>

<details>

<summary>2019-12-05 06:05:28 - Object-Oriented Dynamics Learning through Multi-Level Abstraction</summary>

- *Guangxiang Zhu, Jianhao Wang, Zhizhou Ren, Zichuan Lin, Chongjie Zhang*

- `1904.07482v4` - [abs](http://arxiv.org/abs/1904.07482v4) - [pdf](http://arxiv.org/pdf/1904.07482v4)

> Object-based approaches for learning action-conditioned dynamics has demonstrated promise for generalization and interpretability. However, existing approaches suffer from structural limitations and optimization difficulties for common environments with multiple dynamic objects. In this paper, we present a novel self-supervised learning framework, called Multi-level Abstraction Object-oriented Predictor (MAOP), which employs a three-level learning architecture that enables efficient object-based dynamics learning from raw visual observations. We also design a spatial-temporal relational reasoning mechanism for MAOP to support instance-level dynamics learning and handle partial observability. Our results show that MAOP significantly outperforms previous methods in terms of sample efficiency and generalization over novel environments for learning environment models. We also demonstrate that learned dynamics models enable efficient planning in unseen environments, comparable to true environment models. In addition, MAOP learns semantically and visually interpretable disentangled representations.

</details>

<details>

<summary>2019-12-05 08:57:20 - Fast Concurrent Data Sketches</summary>

- *Arik Rinberg, Alexander Spiegelman, Edward Bortnikov, Eshcar Hillel, Idit Keidar, Lee Rhodes, Hadar Serviansky*

- `1902.10995v2` - [abs](http://arxiv.org/abs/1902.10995v2) - [pdf](http://arxiv.org/pdf/1902.10995v2)

> Data sketches are approximate succinct summaries of long streams. They are widely used for processing massive amounts of data and answering statistical queries about it in real-time. Existing libraries producing sketches are very fast, but do not allow parallelism for creating sketches using multiple threads or querying them while they are being built. We present a generic approach to parallelising data sketches efficiently, while bounding the error that such parallelism introduces. Utilising relaxed semantics and the notion of strong linearisability we prove our algorithm's correctness and analyse the error it induces in two specific sketches. Our implementation achieves high scalability while keeping the error small.

</details>

<details>

<summary>2019-12-05 17:20:34 - Solving Advanced Argumentation Problems with Answer Set Programming</summary>

- *Gerhard Brewka, Martin Diller, Georg Heissenberger, Thomas Linsbichler, Stefan Woltran*

- `1912.02734v1` - [abs](http://arxiv.org/abs/1912.02734v1) - [pdf](http://arxiv.org/pdf/1912.02734v1)

> Powerful formalisms for abstract argumentation have been proposed, among them abstract dialectical frameworks (ADFs) that allow for a succinct and flexible specification of the relationship between arguments, and the GRAPPA framework which allows argumentation scenarios to be represented as arbitrary edge-labelled graphs. The complexity of ADFs and GRAPPA is located beyond NP and ranges up to the third level of the polynomial hierarchy. The combined complexity of Answer Set Programming (ASP) exactly matches this complexity when programs are restricted to predicates of bounded arity. In this paper, we exploit this coincidence and present novel efficient translations from ADFs and GRAPPA to ASP. More specifically, we provide reductions for the five main ADF semantics of admissible, complete, preferred, grounded, and stable interpretations, and exemplify how these reductions need to be adapted for GRAPPA for the admissible, complete and preferred semantics. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>

<details>

<summary>2019-12-05 18:17:45 - Urban Driving with Conditional Imitation Learning</summary>

- *Jeffrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma, Daniele Reda, Nikolay Nikolov, Przemyslaw Mazur, Sean Micklethwaite, Nicolas Griffiths, Amar Shah, Alex Kendall*

- `1912.00177v2` - [abs](http://arxiv.org/abs/1912.00177v2) - [pdf](http://arxiv.org/pdf/1912.00177v2)

> Hand-crafting generalised decision-making rules for real-world urban autonomous driving is hard. Alternatively, learning behaviour from easy-to-collect human driving demonstrations is appealing. Prior work has studied imitation learning (IL) for autonomous driving with a number of limitations. Examples include only performing lane-following rather than following a user-defined route, only using a single camera view or heavily cropped frames lacking state observability, only lateral (steering) control, but not longitudinal (speed) control and a lack of interaction with traffic. Importantly, the majority of such systems have been primarily evaluated in simulation - a simple domain, which lacks real-world complexities. Motivated by these challenges, we focus on learning representations of semantics, geometry and motion with computer vision for IL from human driving demonstrations. As our main contribution, we present an end-to-end conditional imitation learning approach, combining both lateral and longitudinal control on a real vehicle for following urban routes with simple traffic. We address inherent dataset bias by data balancing, training our final policy on approximately 30 hours of demonstrations gathered over six months. We evaluate our method on an autonomous vehicle by driving 35km of novel routes in European urban streets.

</details>

<details>

<summary>2019-12-05 19:45:30 - Information Privacy Opinions on Twitter: A Cross-Language Study</summary>

- *Felipe González, Andrea Figueroa, Claudia López, Cecilia Aragón*

- `1912.02852v1` - [abs](http://arxiv.org/abs/1912.02852v1) - [pdf](http://arxiv.org/pdf/1912.02852v1)

> The Cambridge Analytica scandal triggered a conversation on Twitter about data practices and their implications. Our research proposes to leverage this conversation to extend the understanding of how information privacy is framed by users worldwide. We collected tweets about the scandal written in Spanish and English between April and July 2018. We created a word embedding to create a reduced multi-dimensional representation of the tweets in each language. For each embedding, we conducted open coding to characterize the semantic contexts of key concepts: "information", "privacy", "company" and "users" (and their Spanish translations). Through a comparative analysis, we found a broader emphasis on privacy-related words associated with companies in English. We also identified more terms related to data collection in English and fewer associated with security mechanisms, control, and risks. Our findings hint at the potential of cross-language comparisons of text to extend the understanding of worldwide differences in information privacy perspectives.

</details>

<details>

<summary>2019-12-06 00:49:48 - End-to-end Training of CNN-CRF via Differentiable Dual-Decomposition</summary>

- *Shaofei Wang, Vishnu Lokhande, Maneesh Singh, Konrad Kording, Julian Yarkony*

- `1912.02937v1` - [abs](http://arxiv.org/abs/1912.02937v1) - [pdf](http://arxiv.org/pdf/1912.02937v1)

> Modern computer vision (CV) is often based on convolutional neural networks (CNNs) that excel at hierarchical feature extraction. The previous generation of CV approaches was often based on conditional random fields (CRFs) that excel at modeling flexible higher order interactions. As their benefits are complementary they are often combined. However, these approaches generally use mean-field approximations and thus, arguably, did not directly optimize the real problem. Here we revisit dual-decomposition-based approaches to CRF optimization, an alternative to the mean-field approximation. These algorithms can efficiently and exactly solve sub-problems and directly optimize a convex upper bound of the real problem, providing optimality certificates on the way. Our approach uses a novel fixed-point iteration algorithm which enjoys dual-monotonicity, dual-differentiability and high parallelism. The whole system, CRF and CNN can thus be efficiently trained using back-propagation. We demonstrate the effectiveness of our system on semantic image segmentation, showing consistent improvement over baseline models.

</details>

<details>

<summary>2019-12-06 01:57:07 - DSNet for Real-Time Driving Scene Semantic Segmentation</summary>

- *Wenfu Wang, Zhijie Pan*

- `1812.07049v2` - [abs](http://arxiv.org/abs/1812.07049v2) - [pdf](http://arxiv.org/pdf/1812.07049v2)

> We focus on the very challenging task of semantic segmentation for autonomous driving system. It must deliver decent semantic segmentation result for traffic critical objects real-time. In this paper, we propose a very efficient yet powerful deep neural network for driving scene semantic segmentation termed as Driving Segmentation Network (DSNet). DSNet achieves state-of-the-art balance between accuracy and inference speed through efficient units and architecture design inspired by ShuffleNet V2 and ENet. More importantly, DSNet highlights classes most critical with driving decision making through our novel Driving Importance-weighted Loss. We evaluate DSNet on Cityscapes dataset, our DSNet achieves 71.8% mean Intersection-over-Union (IoU) on validation set and 69.3% on test set. Class-wise IoU scores show that Driving Importance-weighted Loss could improve most driving critical classes by a large margin. Compared with ENet, DSNet is 18.9% more accurate and 1.1+ times faster which implies great potential for autonomous driving application.

</details>

<details>

<summary>2019-12-06 05:01:36 - Parameter-free Sentence Embedding via Orthogonal Basis</summary>

- *Ziyi Yang, Chenguang Zhu, Weizhu Chen*

- `1810.00438v2` - [abs](http://arxiv.org/abs/1810.00438v2) - [pdf](http://arxiv.org/pdf/1810.00438v2)

> We propose a simple and robust non-parameterized approach for building sentence representations. Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence. We model the semantic meaning of a word in a sentence based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is the word's novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace. Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representations. This approach requires zero parameters, along with efficient inference performance. We evaluate our approach on 11 downstream NLP tasks. Our model shows superior performance compared with non-parameterized alternatives and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time.

</details>

<details>

<summary>2019-12-06 06:35:21 - Machine Translation Evaluation Meets Community Question Answering</summary>

- *Francisco Guzmán, Lluís Màrquez, Preslav Nakov*

- `1912.02998v1` - [abs](http://arxiv.org/abs/1912.02998v1) - [pdf](http://arxiv.org/pdf/1912.02998v1)

> We explore the applicability of machine translation evaluation (MTE) methods to a very different problem: answer ranking in community Question Answering. In particular, we adopt a pairwise neural network (NN) architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings, and which efficiently models complex non-linear interactions. The evaluation results show state-of-the-art performance, with sizeable contribution from both the MTE features and from the pairwise NN architecture.

</details>

<details>

<summary>2019-12-06 09:36:15 - Decomposing predictability: Semantic feature overlap between words and the dynamics of reading for meaning</summary>

- *Markus J. Hofmann, Mareike A. Kleemann, Andre Roelke, Christian Vorstius, Ralph Radach*

- `1912.10164v1` - [abs](http://arxiv.org/abs/1912.10164v1) - [pdf](http://arxiv.org/pdf/1912.10164v1)

> The present study uses a computational approach to examine the role of semantic constraints in normal reading. This methodology avoids confounds inherent in conventional measures of predictability, allowing for theoretically deeper accounts of semantic processing. We start from a definition of associations between words based on the significant log likelihood that two words co-occur frequently together in the sentences of a large text corpus. Direct associations between stimulus words were controlled, and semantic feature overlap between prime and target words was manipulated by their common associates. The stimuli consisted of sentences of the form pronoun, verb, article, adjective and noun, followed by a series of closed class words, e. g. "She rides the grey elephant on one of her many exploratory voyages". The results showed that verb-noun overlap reduces single and first fixation durations of the target noun and adjective-noun overlap reduces go-past durations. A dynamic spreading of activation account suggests that associates of the prime words take some time to become activated: The verb can act on the target noun's early eye-movement measures presented three words later, while the adjective is presented immediately prior to the target, which induces sentence re-examination after a difficult adjective-noun semantic integration.

</details>

<details>

<summary>2019-12-06 09:57:22 - Can AI Generate Love Advice?: Toward Neural Answer Generation for Non-Factoid Questions</summary>

- *Makoto Nakatsuji*

- `1912.10163v1` - [abs](http://arxiv.org/abs/1912.10163v1) - [pdf](http://arxiv.org/pdf/1912.10163v1)

> Deep learning methods that extract answers for non-factoid questions from QA sites are seen as critical since they can assist users in reaching their next decisions through conversations with AI systems. The current methods, however, have the following two problems: (1) They can not understand the ambiguous use of words in the questions as word usage can strongly depend on the context. As a result, the accuracies of their answer selections are not good enough. (2) The current methods can only select from among the answers held by QA sites and can not generate new ones. Thus, they can not answer the questions that are somewhat different with those stored in QA sites. Our solution, Neural Answer Construction Model, tackles these problems as it: (1) Incorporates the biases of semantics behind questions into word embeddings while also computing them regardless of the semantics. As a result, it can extract answers that suit the contexts of words used in the question as well as following the common usage of words across semantics. This improves the accuracy of answer selection. (2) Uses biLSTM to compute the embeddings of questions as well as those of the sentences often used to form answers. It then simultaneously learns the optimum combination of those sentences as well as the closeness between the question and those sentences. As a result, our model can construct an answer that corresponds to the situation that underlies the question; it fills the gap between answer selection and generation and is the first model to move beyond the current simple answer selection model for non-factoid QAs. Evaluations using datasets created for love advice stored in the Japanese QA site, Oshiete goo, indicate that our model achieves 20% higher accuracy in answer creation than the strong baselines. Our model is practical and has already been applied to the love advice service in Oshiete goo.

</details>

<details>

<summary>2019-12-06 16:02:34 - FlakiMe: Laboratory-Controlled Test Flakiness Impact Assessment. A Case Study on Mutation Testing and Program Repair</summary>

- *Maxime Cordy, Renaud Rwemalika, Mike Papadakis, Mark Harman*

- `1912.03197v1` - [abs](http://arxiv.org/abs/1912.03197v1) - [pdf](http://arxiv.org/pdf/1912.03197v1)

> Much research on software testing makes an implicit assumption that test failures are deterministic such that they always witness the presence of the same defects. However, this assumption is not always true because some test failures are due to so-called flaky tests, i.e., tests with non-deterministic outcomes. Unfortunately, flaky tests have major implications for testing and test-dependent activities such as mutation testing and automated program repair. To deal with this issue, we introduce a test flakiness assessment and experimentation platform, called FlakiMe, that supports the seeding of a (controllable) degree of flakiness into the behaviour of a given test suite. Thereby, FlakiMe equips researchers with ways to investigate the impact of test flakiness on their techniques under laboratory-controlled conditions. We use FlakiME to report results and insights from case studies that assesses the impact of flakiness on mutation testing and program repair. These results indicate that a 5% of flakiness failures is enough to affect the mutation score, but the effect size is modest (2% - 4% ), while it completely annihilates the ability of program repair to patch 50% of the subject programs. We also observe that flakiness has case-specific effects, which mainly disrupts the repair of bugs that are covered by many tests. Moreover, we find that a minimal amount of user feedback is sufficient for alleviating the effects of flakiness.

</details>

<details>

<summary>2019-12-06 16:45:25 - Effectiveness of Data-Driven Induction of Semantic Spaces and Traditional Classifiers for Sarcasm Detection</summary>

- *Mattia Antonino Di Gangi, Giosué Lo Bosco, Giovanni Pilato*

- `1904.04019v4` - [abs](http://arxiv.org/abs/1904.04019v4) - [pdf](http://arxiv.org/pdf/1904.04019v4)

> Irony and sarcasm are two complex linguistic phenomena that are widely used in everyday language and especially over the social media, but they represent two serious issues for automated text understanding. Many labeled corpora have been extracted from several sources to accomplish this task, and it seems that sarcasm is conveyed in different ways for different domains. Nonetheless, very little work has been done for comparing different methods among the available corpora. Furthermore, usually, each author collects and uses their own datasets to evaluate his own method. In this paper, we show that sarcasm detection can be tackled by applying classical machine learning algorithms to input texts sub-symbolically represented in a Latent Semantic space. The main consequence is that our studies establish both reference datasets and baselines for the sarcasm detection problem that could serve the scientific community to test newly proposed methods.

</details>

<details>

<summary>2019-12-06 16:54:10 - Self-Supervised Visual Terrain Classification from Unsupervised Acoustic Feature Learning</summary>

- *Jannik Zürn, Wolfram Burgard, Abhinav Valada*

- `1912.03227v1` - [abs](http://arxiv.org/abs/1912.03227v1) - [pdf](http://arxiv.org/pdf/1912.03227v1)

> Mobile robots operating in unknown urban environments encounter a wide range of complex terrains to which they must adapt their planned trajectory for safe and efficient navigation. Most existing approaches utilize supervised learning to classify terrains from either an exteroceptive or a proprioceptive sensor modality. However, this requires a tremendous amount of manual labeling effort for each newly encountered terrain as well as for variations of terrains caused by changing environmental conditions. In this work, we propose a novel terrain classification framework leveraging an unsupervised proprioceptive classifier that learns from vehicle-terrain interaction sounds to self-supervise an exteroceptive classifier for pixel-wise semantic segmentation of images. To this end, we first learn a discriminative embedding space for vehicle-terrain interaction sounds from triplets of audio clips formed using visual features of the corresponding terrain patches and cluster the resulting embeddings. We subsequently use these clusters to label the visual terrain patches by projecting the traversed tracks of the robot into the camera images. Finally, we use the sparsely labeled images to train our semantic segmentation network in a weakly supervised manner. We present extensive quantitative and qualitative results that demonstrate that our proprioceptive terrain classifier exceeds the state-of-the-art among unsupervised methods and our self-supervised exteroceptive semantic segmentation model achieves a comparable performance to supervised learning with manually labeled data.

</details>

<details>

<summary>2019-12-06 23:36:01 - Multiple Futures Prediction</summary>

- *Yichuan Charlie Tang, Ruslan Salakhutdinov*

- `1911.00997v2` - [abs](http://arxiv.org/abs/1911.00997v2) - [pdf](http://arxiv.org/pdf/1911.00997v2)

> Temporal prediction is critical for making intelligent and robust decisions in complex dynamic environments. Motion prediction needs to model the inherently uncertain future which often contains multiple potential outcomes, due to multi-agent interactions and the latent goals of others. Towards these goals, we introduce a probabilistic framework that efficiently learns latent variables to jointly model the multi-step future motions of agents in a scene. Our framework is data-driven and learns semantically meaningful latent variables to represent the multimodal future, without requiring explicit labels. Using a dynamic attention-based state encoder, we learn to encode the past as well as the future interactions among agents, efficiently scaling to any number of agents. Finally, our model can be used for planning via computing a conditional probability density over the trajectories of other agents given a hypothetical rollout of the 'self' agent. We demonstrate our algorithms by predicting vehicle trajectories of both simulated and real data, demonstrating the state-of-the-art results on several vehicle trajectory datasets.

</details>

<details>

<summary>2019-12-08 01:05:17 - Formalizing Event-Driven Behavior of Serverless Applications</summary>

- *Matthew Obetz, Stacy Patterson, Ana Milanova*

- `1912.03584v1` - [abs](http://arxiv.org/abs/1912.03584v1) - [pdf](http://arxiv.org/pdf/1912.03584v1)

> We present new operational semantics for serverless computing that model the event-driven relationships between serverless functions, as well as their interaction with platforms services such as databases and object stores. These semantics precisely encapsulate how control transfers between functions, both directly and through reads and writes to platform services. We use these semantics to define the notion of the service call graph for serverless applications that captures program flows through functions and services. Finally, we construct service call graphs for twelve serverless JavaScript applications, using a prototype of our call graph construction algorithm, and we evaluate their accuracy.

</details>

<details>

<summary>2019-12-08 06:36:35 - Deep Bayesian Active Learning for Multiple Correct Outputs</summary>

- *Khaled Jedoui, Ranjay Krishna, Michael Bernstein, Li Fei-Fei*

- `1912.01119v2` - [abs](http://arxiv.org/abs/1912.01119v2) - [pdf](http://arxiv.org/pdf/1912.01119v2)

> Typical active learning strategies are designed for tasks, such as classification, with the assumption that the output space is mutually exclusive. The assumption that these tasks always have exactly one correct answer has resulted in the creation of numerous uncertainty-based measurements, such as entropy and least confidence, which operate over a model's outputs. Unfortunately, many real-world vision tasks, like visual question answering and image captioning, have multiple correct answers, causing these measurements to overestimate uncertainty and sometimes perform worse than a random sampling baseline. In this paper, we propose a new paradigm that estimates uncertainty in the model's internal hidden space instead of the model's output space. We specifically study a manifestation of this problem for visual question answer generation (VQA), where the aim is not to classify the correct answer but to produce a natural language answer, given an image and a question. Our method overcomes the paraphrastic nature of language. It requires a semantic space that structures the model's output concepts and that enables the usage of techniques like dropout-based Bayesian uncertainty. We build a visual-semantic space that embeds paraphrases close together for any existing VQA model. We empirically show state-of-art active learning results on the task of VQA on two datasets, being 5 times more cost-efficient on Visual Genome and 3 times more cost-efficient on VQA 2.0.

</details>

<details>

<summary>2019-12-08 12:52:35 - Data Exploration and Validation on dense knowledge graphs for biomedical research</summary>

- *Jens Dörpinghaus, Alexander Apke, Vanessa Lage-Rupprecht, Andreas Stefan*

- `1912.06194v1` - [abs](http://arxiv.org/abs/1912.06194v1) - [pdf](http://arxiv.org/pdf/1912.06194v1)

> Here we present a holistic approach for data exploration on dense knowledge graphs as a novel approach with a proof-of-concept in biomedical research. Knowledge graphs are increasingly becoming a vital factor in knowledge mining and discovery as they connect data using technologies from the semantic web. In this paper we extend a basic knowledge graph extracted from biomedical literature by context data like named entities and relations obtained by text mining and other linked data sources like ontologies and databases. We will present an overview about this novel network. The aim of this work was to extend this current knowledge with approaches from graph theory. This method will build the foundation for quality control, validation of hypothesis, detection of missing data and time series analysis of biomedical knowledge in general. In this context we tried to apply multiple-valued decision diagrams to these questions. In addition this knowledge representation of linked data can be used as FAIR approach to answer semantic questions. This paper sheds new lights on dense and very large knowledge graphs and the importance of a graph-theoretic understanding of these networks.

</details>

<details>

<summary>2019-12-08 13:04:06 - Detection of False Positive and False Negative Samples in Semantic Segmentation</summary>

- *Matthias Rottmann, Kira Maag, Robin Chan, Fabian Hüger, Peter Schlicht, Hanno Gottschalk*

- `1912.03673v1` - [abs](http://arxiv.org/abs/1912.03673v1) - [pdf](http://arxiv.org/pdf/1912.03673v1)

> In recent years, deep learning methods have outperformed other methods in image recognition. This has fostered imagination of potential application of deep learning technology including safety relevant applications like the interpretation of medical images or autonomous driving. The passage from assistance of a human decision maker to ever more automated systems however increases the need to properly handle the failure modes of deep learning modules. In this contribution, we review a set of techniques for the self-monitoring of machine-learning algorithms based on uncertainty quantification. In particular, we apply this to the task of semantic segmentation, where the machine learning algorithm decomposes an image according to semantic categories. We discuss false positive and false negative error modes at instance-level and review techniques for the detection of such errors that have been recently proposed by the authors. We also give an outlook on future research directions.

</details>

<details>

<summary>2019-12-08 14:15:02 - From semantics to execution: Integrating action planning with reinforcement learning for robotic causal problem-solving</summary>

- *Manfred Eppe, Phuong D. H. Nguyen, Stefan Wermter*

- `1905.09683v2` - [abs](http://arxiv.org/abs/1905.09683v2) - [pdf](http://arxiv.org/pdf/1905.09683v2)

> Reinforcement learning is an appropriate and successful method to robustly perform low-level robot control under noisy conditions. Symbolic action planning is useful to resolve causal dependencies and to break a causally complex problem down into a sequence of simpler high-level actions. A problem with the integration of both approaches is that action planning is based on discrete high-level action- and state spaces, whereas reinforcement learning is usually driven by a continuous reward function. However, recent advances in reinforcement learning, specifically, universal value function approximators and hindsight experience replay, have focused on goal-independent methods based on sparse rewards. In this article, we build on these novel methods to facilitate the integration of action planning with reinforcement learning by exploiting the reward-sparsity as a bridge between the high-level and low-level state- and control spaces. As a result, we demonstrate that the integrated neuro-symbolic method is able to solve object manipulation problems that involve tool use and non-trivial causal dependencies under noisy conditions, exploiting both data and knowledge.

</details>

<details>

<summary>2019-12-09 18:05:02 - Multi-level tree based approach for interactive graph visualization with semantic zoom</summary>

- *Felice De Luca, Iqbal Hossain, Kathryn Gray, Stephen Kobourov, Katy Börner*

- `1906.05996v2` - [abs](http://arxiv.org/abs/1906.05996v2) - [pdf](http://arxiv.org/pdf/1906.05996v2)

> Human subject studies that map-like visualizations are as good or better than standard node-link representations of graphs, in terms of task performance, memorization and recall of the underlying data, and engagement [SSKB14, SSKB15]. With this in mind, we propose the Zoomable Multi-Level Tree (ZMLT) algorithm for multi-level tree-based, map-like visualization of large graphs. We propose seven desirable properties that such visualization should maintain and an algorithm that accomplishes them. (1) The abstract trees represent the underlying graph appropriately at different level of details; (2) The embedded trees represent the underlying graph appropriately at different levels of details; (3) At every level of detail we show real vertices and real paths from the underlying graph; (4) If any node or edge appears in a given level, then they also appear in all deeper levels; (5) All nodes at the current level and higher levels are labeled and there are no label overlaps; (6) There are no edge crossings on any level; (7) The drawing area is proportional to the total area of the labels. This algorithm is implemented and we have a functional prototype for the interactive interface in a web browser.

</details>

<details>

<summary>2019-12-09 20:38:32 - A hierarchical loss and its problems when classifying non-hierarchically</summary>

- *Cinna Wu, Mark Tygert, Yann LeCun*

- `1709.01062v2` - [abs](http://arxiv.org/abs/1709.01062v2) - [pdf](http://arxiv.org/pdf/1709.01062v2)

> Failing to distinguish between a sheepdog and a skyscraper should be worse and penalized more than failing to distinguish between a sheepdog and a poodle; after all, sheepdogs and poodles are both breeds of dogs. However, existing metrics of failure (so-called "loss" or "win") used in textual or visual classification/recognition via neural networks seldom leverage a-priori information, such as a sheepdog being more similar to a poodle than to a skyscraper. We define a metric that, inter alia, can penalize failure to distinguish between a sheepdog and a skyscraper more than failure to distinguish between a sheepdog and a poodle. Unlike previously employed possibilities, this metric is based on an ultrametric tree associated with any given tree organization into a semantically meaningful hierarchy of a classifier's classes. An ultrametric tree is a tree with a so-called ultrametric distance metric such that all leaves are at the same distance from the root. Unfortunately, extensive numerical experiments indicate that the standard practice of training neural networks via stochastic gradient descent with random starting points often drives down the hierarchical loss nearly as much when minimizing the standard cross-entropy loss as when trying to minimize the hierarchical loss directly. Thus, this hierarchical loss is unreliable as an objective for plain, randomly started stochastic gradient descent to minimize; the main value of the hierarchical loss may be merely as a meaningful metric of success of a classifier.

</details>

<details>

<summary>2019-12-10 03:45:42 - Homograph Disambiguation Through Selective Diacritic Restoration</summary>

- *Sawsan Alqahtani, Hanan Aldarmaki, Mona Diab*

- `1912.04479v1` - [abs](http://arxiv.org/abs/1912.04479v1) - [pdf](http://arxiv.org/pdf/1912.04479v1)

> Lexical ambiguity, a challenging phenomenon in all natural languages, is particularly prevalent for languages with diacritics that tend to be omitted in writing, such as Arabic. Omitting diacritics leads to an increase in the number of homographs: different words with the same spelling. Diacritic restoration could theoretically help disambiguate these words, but in practice, the increase in overall sparsity leads to performance degradation in NLP applications. In this paper, we propose approaches for automatically marking a subset of words for diacritic restoration, which leads to selective homograph disambiguation. Compared to full or no diacritic restoration, these approaches yield selectively-diacritized datasets that balance sparsity and lexical disambiguation. We evaluate the various selection strategies extrinsically on several downstream applications: neural machine translation, part-of-speech tagging, and semantic textual similarity. Our experiments on Arabic show promising results, where our devised strategies on selective diacritization lead to a more balanced and consistent performance in downstream applications.

</details>

<details>

<summary>2019-12-10 07:35:06 - When redundancy is useful: A Bayesian approach to 'overinformative' referring expressions</summary>

- *Judith Degen, Robert D. Hawkins, Caroline Graf, Elisa Kreiss, Noah D. Goodman*

- `1903.08237v3` - [abs](http://arxiv.org/abs/1903.08237v3) - [pdf](http://arxiv.org/pdf/1903.08237v3)

> Referring is one of the most basic and prevalent uses of language. How do speakers choose from the wealth of referring expressions at their disposal? Rational theories of language use have come under attack for decades for not being able to account for the seemingly irrational overinformativeness ubiquitous in referring expressions. Here we present a novel production model of referring expressions within the Rational Speech Act framework that treats speakers as agents that rationally trade off cost and informativeness of utterances. Crucially, we relax the assumption that informativeness is computed with respect to a deterministic Boolean semantics, in favor of a non-deterministic continuous semantics. This innovation allows us to capture a large number of seemingly disparate phenomena within one unified framework: the basic asymmetry in speakers' propensity to overmodify with color rather than size; the increase in overmodification in complex scenes; the increase in overmodification with atypical features; and the increase in specificity in nominal reference as a function of typicality. These findings cast a new light on the production of referring expressions: rather than being wastefully overinformative, reference is usefully redundant.

</details>

<details>

<summary>2019-12-10 15:07:48 - Fraud detection in telephone conversations for financial services using linguistic features</summary>

- *Nikesh Bajaj, Tracy Goodluck Constance, Marvin Rajwadi, Julie Wall, Mansour Moniri, Cornelius Glackin, Nigel Cannings, Chris Woodruff, James Laird*

- `1912.04748v1` - [abs](http://arxiv.org/abs/1912.04748v1) - [pdf](http://arxiv.org/pdf/1912.04748v1)

> Detecting the elements of deception in a conversation is one of the most challenging problems for the AI community. It becomes even more difficult to design a transparent system, which is fully explainable and satisfies the need for financial and legal services to be deployed. This paper presents an approach for fraud detection in transcribed telephone conversations using linguistic features. The proposed approach exploits the syntactic and semantic information of the transcription to extract both the linguistic markers and the sentiment of the customer's response. We demonstrate the results on real-world financial services data using simple, robust and explainable classifiers such as Naive Bayes, Decision Tree, Nearest Neighbours, and Support Vector Machines.

</details>

<details>

<summary>2019-12-10 15:27:33 - SolarNet: A Deep Learning Framework to Map Solar Power Plants In China From Satellite Imagery</summary>

- *Xin Hou, Biao Wang, Wanqi Hu, Lei Yin, Haishan Wu*

- `1912.03685v2` - [abs](http://arxiv.org/abs/1912.03685v2) - [pdf](http://arxiv.org/pdf/1912.03685v2)

> Renewable energy such as solar power is critical to fight the ever more serious climate change. China is the world leading installer of solar panel and numerous solar power plants were built. In this paper, we proposed a deep learning framework named SolarNet which is designed to perform semantic segmentation on large scale satellite imagery data to detect solar farms. SolarNet has successfully mapped 439 solar farms in China, covering near 2000 square kilometers, equivalent to the size of whole Shenzhen city or two and a half of New York city. To the best of our knowledge, it is the first time that we used deep learning to reveal the locations and sizes of solar farms in China, which could provide insights for solar power companies, market analysts and the government.

</details>

<details>

<summary>2019-12-10 17:20:18 - Feature Relevance Determination for Ordinal Regression in the Context of Feature Redundancies and Privileged Information</summary>

- *Lukas Pfannschmidt, Jonathan Jakob, Fabian Hinder, Michael Biehl, Peter Tino, Barbara Hammer*

- `1912.04832v1` - [abs](http://arxiv.org/abs/1912.04832v1) - [pdf](http://arxiv.org/pdf/1912.04832v1)

> Advances in machine learning technologies have led to increasingly powerful models in particular in the context of big data. Yet, many application scenarios demand for robustly interpretable models rather than optimum model accuracy; as an example, this is the case if potential biomarkers or causal factors should be discovered based on a set of given measurements. In this contribution, we focus on feature selection paradigms, which enable us to uncover relevant factors of a given regularity based on a sparse model. We focus on the important specific setting of linear ordinal regression, i.e.\ data have to be ranked into one of a finite number of ordered categories by a linear projection. Unlike previous work, we consider the case that features are potentially redundant, such that no unique minimum set of relevant features exists. We aim for an identification of all strongly and all weakly relevant features as well as their type of relevance (strong or weak); we achieve this goal by determining feature relevance bounds, which correspond to the minimum and maximum feature relevance, respectively, if searched over all equivalent models. In addition, we discuss how this setting enables us to substitute some of the features, e.g.\ due to their semantics, and how to extend the framework of feature relevance intervals to the setting of privileged information, i.e.\ potentially relevant information is available for training purposes only, but cannot be used for the prediction itself.

</details>

<details>

<summary>2019-12-10 23:55:34 - Self-supervised Domain Adaptation for Computer Vision Tasks</summary>

- *Jiaolong Xu, Liang Xiao, Antonio M. Lopez*

- `1907.10915v3` - [abs](http://arxiv.org/abs/1907.10915v3) - [pdf](http://arxiv.org/pdf/1907.10915v3)

> Recent progress of self-supervised visual representation learning has achieved remarkable success on many challenging computer vision benchmarks. However, whether these techniques can be used for domain adaptation has not been explored. In this work, we propose a generic method for self-supervised domain adaptation, using object recognition and semantic segmentation of urban scenes as use cases. Focusing on simple pretext/auxiliary tasks (e.g. image rotation prediction), we assess different learning strategies to improve domain adaptation effectiveness by self-supervision. Additionally, we propose two complementary strategies to further boost the domain adaptation accuracy on semantic segmentation within our method, consisting of prediction layer alignment and batch normalization calibration. The experimental results show adaptation levels comparable to most studied domain adaptation methods, thus, bringing self-supervision as a new alternative for reaching domain adaptation. The code is available at https://github.com/Jiaolong/self-supervised-da.

</details>

<details>

<summary>2019-12-11 06:21:16 - Unsupervised Neural Dialect Translation with Commonality and Diversity Modeling</summary>

- *Yu Wan, Baosong Yang, Derek F. Wong, Lidia S. Chao, Haihua Du, Ben C. H. Ao*

- `1912.05134v1` - [abs](http://arxiv.org/abs/1912.05134v1) - [pdf](http://arxiv.org/pdf/1912.05134v1)

> As a special machine translation task, dialect translation has two main characteristics: 1) lack of parallel training corpus; and 2) possessing similar grammar between two sides of the translation. In this paper, we investigate how to exploit the commonality and diversity between dialects thus to build unsupervised translation models merely accessing to monolingual data. Specifically, we leverage pivot-private embedding, layer coordination, as well as parameter sharing to sufficiently model commonality and diversity among source and target, ranging from lexical, through syntactic, to semantic levels. In order to examine the effectiveness of the proposed models, we collect 20 million monolingual corpus for each of Mandarin and Cantonese, which are official language and the most widely used dialect in China. Experimental results reveal that our methods outperform rule-based simplified and traditional Chinese conversion and conventional unsupervised translation models over 12 BLEU scores.

</details>

<details>

<summary>2019-12-11 11:27:06 - BERT has a Moral Compass: Improvements of ethical and moral values of machines</summary>

- *Patrick Schramowski, Cigdem Turan, Sophie Jentzsch, Constantin Rothkopf, Kristian Kersting*

- `1912.05238v1` - [abs](http://arxiv.org/abs/1912.05238v1) - [pdf](http://arxiv.org/pdf/1912.05238v1)

> Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about "right" and "wrong" conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass? In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.

</details>

<details>

<summary>2019-12-11 12:20:20 - LINSPECTOR: Multilingual Probing Tasks for Word Representations</summary>

- *Gözde Gül Şahin, Clara Vania, Ilia Kuznetsov, Iryna Gurevych*

- `1903.09442v2` - [abs](http://arxiv.org/abs/1903.09442v2) - [pdf](http://arxiv.org/pdf/1903.09442v2)

> Despite an ever growing number of word representation models introduced for a large number of languages, there is a lack of a standardized technique to provide insights into what is captured by these models. Such insights would help the community to get an estimate of the downstream task performance, as well as to design more informed neural architectures, while avoiding extensive experimentation which requires substantial computational resources not all researchers have access to. A recent development in NLP is to use simple classification tasks, also called probing tasks, that test for a single linguistic feature such as part-of-speech. Existing studies mostly focus on exploring the linguistic information encoded by the continuous representations of English text. However, from a typological perspective the morphologically poor English is rather an outlier: the information encoded by the word order and function words in English is often stored on a morphological level in other languages. To address this, we introduce 15 type-level probing tasks such as case marking, possession, word length, morphological tag count and pseudoword identification for 24 languages. We present a reusable methodology for creation and evaluation of such tests in a multilingual setting. We then present experiments on several diverse multilingual word embedding models, in which we relate the probing task performance for a diverse set of languages to a range of five classic NLP tasks: POS-tagging, dependency parsing, semantic role labeling, named entity recognition and natural language inference. We find that a number of probing tests have significantly high positive correlation to the downstream tasks, especially for morphologically rich languages. We show that our tests can be used to explore word embeddings or black-box neural models for linguistic cues in a multilingual setting.

</details>

<details>

<summary>2019-12-11 12:50:48 - Two Birds with One Stone: Investigating Invertible Neural Networks for Inverse Problems in Morphology</summary>

- *Gözde Gül Şahin, Iryna Gurevych*

- `1912.05274v1` - [abs](http://arxiv.org/abs/1912.05274v1) - [pdf](http://arxiv.org/pdf/1912.05274v1)

> Most problems in natural language processing can be approximated as inverse problems such as analysis and generation at variety of levels from morphological (e.g., cat+Plural <-> cats) to semantic (e.g., (call + 1 2) <-> "Calculate one plus two."). Although the tasks in both directions are closely related, general approach in the field has been to design separate models specific for each task. However, having one shared model for both tasks, would help the researchers exploit the common knowledge among these problems with reduced time and memory requirements. We investigate a specific class of neural networks, called Invertible Neural Networks (INNs) (Ardizzone et al. 2019) that enable simultaneous optimization in both directions, hence allow addressing of inverse problems via a single model. In this study, we investigate INNs on morphological problems casted as inverse problems. We apply INNs to various morphological tasks with varying ambiguity and show that they provide competitive performance in both directions. We show that they are able to recover the morphological input parameters, i.e., predicting the lemma (e.g., cat) or the morphological tags (e.g., Plural) when run in the reverse direction, without any significant performance drop in the forward direction, i.e., predicting the surface form (e.g., cats).

</details>

<details>

<summary>2019-12-12 02:04:43 - Semantic segmentation of trajectories with improved agent models for pedestrian behavior analysis</summary>

- *Toru Tamaki, Daisuke Ogawa, Bisser Raytchev, Kazufumi Kaneda*

- `1912.05727v1` - [abs](http://arxiv.org/abs/1912.05727v1) - [pdf](http://arxiv.org/pdf/1912.05727v1)

> In this paper, we propose a method for semantic segmentation of pedestrian trajectories based on pedestrian behavior models, or agents. The agents model the dynamics of pedestrian movements in two-dimensional space using a linear dynamics model and common start and goal locations of trajectories. First, agent models are estimated from the trajectories obtained from image sequences. Our method is built on top of the Mixture model of Dynamic pedestrian Agents (MDA); however, the MDA's trajectory modeling and estimation are improved. Then, the trajectories are divided into semantically meaningful segments. The subsegments of a trajectory are modeled by applying a hidden Markov model using the estimated agent models. Experimental results with a real trajectory dataset show the effectiveness of the proposed method as compared to the well-known classical Ramer-Douglas-Peucker algorithm and also to the original MDA model.

</details>

<details>

<summary>2019-12-12 06:22:06 - TERA: the Toxicological Effect and Risk Assessment Knowledge Graph</summary>

- *Erik Bryhn Myklebust, Ernesto Jimenez-Ruiz, Jiaoyan Chen, Raoul Wolf, Knut Erik Tollefsen*

- `1908.10128v5` - [abs](http://arxiv.org/abs/1908.10128v5) - [pdf](http://arxiv.org/pdf/1908.10128v5)

> Ecological risk assessment requires large amounts of chemical effect data from laboratory experiments. Due to experimental effort and animal welfare concerns it is desired to extrapolate data from existing sources. To cover the required chemical effect data several data sources need to be integrated to enable their interoperability. In this paper we introduce the Toxicological Effect and Risk Assessment (TERA) knowledge graph, which aims at providing such integrated view, and the data preparation and steps followed to construct this knowledge graph.   We also present the applications of TERA for chemical effect prediction and the potential applications within the Semantic Web community.

</details>

<details>

<summary>2019-12-12 15:48:55 - Hue-Net: Intensity-based Image-to-Image Translation with Differentiable Histogram Loss Functions</summary>

- *Mor Avi-Aharon, Assaf Arbelle, Tammy Riklin Raviv*

- `1912.06044v1` - [abs](http://arxiv.org/abs/1912.06044v1) - [pdf](http://arxiv.org/pdf/1912.06044v1)

> We present the Hue-Net - a novel Deep Learning framework for Intensity-based Image-to-Image Translation. The key idea is a new technique termed network augmentation which allows a differentiable construction of intensity histograms from images. We further introduce differentiable representations of (1D) cyclic and joint (2D) histograms and use them for defining loss functions based on cyclic Earth Mover's Distance (EMD) and Mutual Information (MI). While the Hue-Net can be applied to several image-to-image translation tasks, we choose to demonstrate its strength on color transfer problems, where the aim is to paint a source image with the colors of a different target image. Note that the desired output image does not exist and therefore cannot be used for supervised pixel-to-pixel learning. This is accomplished by using the HSV color-space and defining an intensity-based loss that is built on the EMD between the cyclic hue histograms of the output and the target images. To enforce color-free similarity between the source and the output images, we define a semantic-based loss by a differentiable approximation of the MI of these images. The incorporation of histogram loss functions in addition to an adversarial loss enables the construction of semantically meaningful and realistic images. Promising results are presented for different datasets.

</details>

<details>

<summary>2019-12-13 04:29:41 - Improving Distant Supervised Relation Extraction by Dynamic Neural Network</summary>

- *Yanjie Gou, Yinjie Lei, Lingqiao Liu, Pingping Zhang, Xi Peng*

- `1911.06489v2` - [abs](http://arxiv.org/abs/1911.06489v2) - [pdf](http://arxiv.org/pdf/1911.06489v2)

> Distant Supervised Relation Extraction (DSRE) is usually formulated as a problem of classifying a bag of sentences that contain two query entities, into the predefined relation classes. Most existing methods consider those relation classes as distinct semantic categories while ignoring their potential connection to query entities. In this paper, we propose to leverage this connection to improve the relation extraction accuracy. Our key ideas are twofold: (1) For sentences belonging to the same relation class, the expression style, i.e. words choice, can vary according to the query entities. To account for this style shift, the model should adjust its parameters in accordance with entity types. (2) Some relation classes are semantically similar, and the entity types appear in one relation may also appear in others. Therefore, it can be trained cross different relation classes and further enhance those classes with few samples, i.e., long-tail classes. To unify these two arguments, we developed a novel Dynamic Neural Network for Relation Extraction (DNNRE). The network adopts a novel dynamic parameter generator that dynamically generates the network parameters according to the query entity types and relation classes. By using this mechanism, the network can simultaneously handle the style shift problem and enhance the prediction accuracy for long-tail classes. Through our experimental study, we demonstrate the effectiveness of the proposed method and show that it can achieve superior performance over the state-of-the-art methods.

</details>

<details>

<summary>2019-12-13 17:20:52 - From Shallow to Deep Interactions Between Knowledge Representation, Reasoning and Machine Learning (Kay R. Amel group)</summary>

- *Zied Bouraoui, Antoine Cornuéjols, Thierry Denœux, Sébastien Destercke, Didier Dubois, Romain Guillaume, João Marques-Silva, Jérôme Mengin, Henri Prade, Steven Schockaert, Mathieu Serrurier, Christel Vrain*

- `1912.06612v1` - [abs](http://arxiv.org/abs/1912.06612v1) - [pdf](http://arxiv.org/pdf/1912.06612v1)

> This paper proposes a tentative and original survey of meeting points between Knowledge Representation and Reasoning (KRR) and Machine Learning (ML), two areas which have been developing quite separately in the last three decades. Some common concerns are identified and discussed such as the types of used representation, the roles of knowledge and data, the lack or the excess of information, or the need for explanations and causal understanding. Then some methodologies combining reasoning and learning are reviewed (such as inductive logic programming, neuro-symbolic reasoning, formal concept analysis, rule-based representations and ML, uncertainty in ML, or case-based reasoning and analogical reasoning), before discussing examples of synergies between KRR and ML (including topics such as belief functions on regression, EM algorithm versus revision, the semantic description of vector representations, the combination of deep learning with high level inference, knowledge graph completion, declarative frameworks for data mining, or preferences and recommendation). This paper is the first step of a work in progress aiming at a better mutual understanding of research in KRR and ML, and how they could cooperate.

</details>

<details>

<summary>2019-12-13 22:10:16 - Unsupervised Detection of Sub-events in Large Scale Disasters</summary>

- *Chidubem Arachie, Manas Gaur, Sam Anzaroot, William Groves, Ke Zhang, Alejandro Jaimes*

- `1912.13332v1` - [abs](http://arxiv.org/abs/1912.13332v1) - [pdf](http://arxiv.org/pdf/1912.13332v1)

> Social media plays a major role during and after major natural disasters (e.g., hurricanes, large-scale fires, etc.), as people ``on the ground'' post useful information on what is actually happening. Given the large amounts of posts, a major challenge is identifying the information that is useful and actionable. Emergency responders are largely interested in finding out what events are taking place so they can properly plan and deploy resources. In this paper we address the problem of automatically identifying important sub-events (within a large-scale emergency ``event'', such as a hurricane). In particular, we present a novel, unsupervised learning framework to detect sub-events in Tweets for retrospective crisis analysis. We first extract noun-verb pairs and phrases from raw tweets as sub-event candidates. Then, we learn a semantic embedding of extracted noun-verb pairs and phrases, and rank them against a crisis-specific ontology. We filter out noisy and irrelevant information then cluster the noun-verb pairs and phrases so that the top-ranked ones describe the most important sub-events. Through quantitative experiments on two large crisis data sets (Hurricane Harvey and the 2015 Nepal Earthquake), we demonstrate the effectiveness of our approach over the state-of-the-art. Our qualitative evaluation shows better performance compared to our baseline.

</details>

<details>

<summary>2019-12-13 23:35:18 - Does AlphaGo actually play Go? Concerning the State Space of Artificial Intelligence</summary>

- *Holger Lyre*

- `1912.10005v1` - [abs](http://arxiv.org/abs/1912.10005v1) - [pdf](http://arxiv.org/pdf/1912.10005v1)

> The overarching goal of this paper is to develop a general model of the state space of AI. Given the breathtaking progress in AI research and technologies in recent years, such conceptual work is of substantial theoretical interest. The present AI hype is mainly driven by the triumph of deep learning neural networks. As the distinguishing feature of such networks is the ability to self-learn, self-learning is identified as one important dimension of the AI state space. Another main dimension lies in the possibility to go over from specific to more general types of problems. The third main dimension is provided by semantic grounding. Since this is a philosophically complex and controversial dimension, a larger part of the paper is devoted to it. We take a fresh look at known foundational arguments in the philosophy of mind and cognition that are gaining new relevance in view of the recent AI developments including the blockhead objection, the Turing test, the symbol grounding problem, the Chinese room argument, and general use-theoretic considerations of meaning. Finally, the AI state space, spanned by the main dimensions generalization, grounding and "selfx-ness", possessing self-x properties such as self-learning, is outlined.

</details>

<details>

<summary>2019-12-14 12:44:04 - Multi-label Detection and Classification of Red Blood Cells in Microscopic Images</summary>

- *Wei Qiu, Jiaming Guo, Xiang Li, Mengjia Xu, Mo Zhang, Ning Guo, Quanzheng Li*

- `1910.02672v2` - [abs](http://arxiv.org/abs/1910.02672v2) - [pdf](http://arxiv.org/pdf/1910.02672v2)

> Cell detection and cell type classification from biomedical images play an important role for high-throughput imaging and various clinical application. While classification of single cell sample can be performed with standard computer vision and machine learning methods, analysis of multi-label samples (region containing congregating cells) is more challenging, as separation of individual cells can be difficult (e.g. touching cells) or even impossible (e.g. overlapping cells). As multi-instance images are common in analyzing Red Blood Cell (RBC) for Sickle Cell Disease (SCD) diagnosis, we develop and implement a multi-instance cell detection and classification framework to address this challenge. The framework firstly trains a region proposal model based on Region-based Convolutional Network (RCNN) to obtain bounding-boxes of regions potentially containing single or multiple cells from input microscopic images, which are extracted as image patches. High-level image features are then calculated from image patches through a pre-trained Convolutional Neural Network (CNN) with ResNet-50 structure. Using these image features inputs, six networks are then trained to make multi-label prediction of whether a given patch contains cells belonging to a specific cell type. As the six networks are trained with image patches consisting of both individual cells and touching/overlapping cells, they can effectively recognize cell types that are presented in multi-instance image samples. Finally, for the purpose of SCD testing, we train another machine learning classifier to predict whether the given image patch contains abnormal cell type based on outputs from the six networks. Testing result of the proposed framework shows that it can achieve good performance in automatic cell detection and classification.

</details>

<details>

<summary>2019-12-14 14:59:38 - Knowledge-based Conversational Search</summary>

- *Svitlana Vakulenko*

- `1912.06859v1` - [abs](http://arxiv.org/abs/1912.06859v1) - [pdf](http://arxiv.org/pdf/1912.06859v1)

> Conversational interfaces that allow for intuitive and comprehensive access to digitally stored information remain an ambitious goal. In this thesis, we lay foundations for designing conversational search systems by analyzing the requirements and proposing concrete solutions for automating some of the basic components and tasks that such systems should support. We describe several interdependent studies that were conducted to analyse the design requirements for more advanced conversational search systems able to support complex human-like dialogue interactions and provide access to vast knowledge repositories. In the first two research chapters, we focus on analyzing the structures common to information-seeking dialogues by capturing recurrent patterns in terms of both domain-independent functional relations between utterances as well as domain-specific implicit semantic relations from shared background knowledge.   Our results show that question answering is one of the key components required for efficient information access but it is not the only type of dialogue interactions that a conversational search system should support. In the third research chapter, we propose a novel approach for complex question answering from a knowledge graph that surpasses the current state-of-the-art results in terms of both efficacy and efficiency. In the last research chapter, we turn our attention towards an alternative interaction mode, which we termed conversational browsing, in which, unlike question answering, the conversational system plays a more pro-active role in the course of a dialogue interaction. We show that this approach helps users to discover relevant items that are difficult to retrieve using only question answering due to the vocabulary mismatch problem.

</details>

<details>

<summary>2019-12-14 15:17:00 - Survivor: A Fine-Grained Intrusion Response and Recovery Approach for Commodity Operating Systems</summary>

- *Ronny Chevalier, David Plaquin, Chris Dalton, Guillaume Hiet*

- `1912.06863v1` - [abs](http://arxiv.org/abs/1912.06863v1) - [pdf](http://arxiv.org/pdf/1912.06863v1)

> Despite the deployment of preventive security mechanisms to protect the assets and computing platforms of users, intrusions eventually occur. We propose a novel intrusion survivability approach to withstand ongoing intrusions. Our approach relies on an orchestration of fine-grained recovery and per-service responses (e.g., privileges removal). Such an approach may put the system into a degraded mode. This degraded mode prevents attackers to reinfect the system or to achieve their goals if they managed to reinfect it. It maintains the availability of core functions while waiting for patches to be deployed. We devised a cost-sensitive response selection process to ensure that while the service is in a degraded mode, its core functions are still operating. We built a Linux-based prototype and evaluated the effectiveness of our approach against different types of intrusions. The results show that our solution removes the effects of the intrusions, that it can select appropriate responses, and that it allows services to survive when reinfected. In terms of performance overhead, in most cases, we observed a small overhead, except in the rare case of services that write many small files asynchronously in a burst, where we observed a higher but acceptable overhead.

</details>

<details>

<summary>2019-12-15 15:31:45 - One-Shot Induction of Generalized Logical Concepts via Human Guidance</summary>

- *Mayukh Das, Nandini Ramanan, Janardhan Rao Doppa, Sriraam Natarajan*

- `1912.07060v1` - [abs](http://arxiv.org/abs/1912.07060v1) - [pdf](http://arxiv.org/pdf/1912.07060v1)

> We consider the problem of learning generalized first-order representations of concepts from a single example. To address this challenging problem, we augment an inductive logic programming learner with two novel algorithmic contributions. First, we define a distance measure between candidate concept representations that improves the efficiency of search for target concept and generalization. Second, we leverage richer human inputs in the form of advice to improve the sample-efficiency of learning. We prove that the proposed distance measure is semantically valid and use that to derive a PAC bound. Our experimental analysis on diverse concept learning tasks demonstrates both the effectiveness and efficiency of the proposed approach over a first-order concept learner using only examples.

</details>

<details>

<summary>2019-12-15 19:55:45 - Intermittent Learning: On-Device Machine Learning on Intermittently Powered System</summary>

- *Seulki Lee, Bashima Islam, Yubo Luo, Shahriar Nirjon*

- `1904.09644v2` - [abs](http://arxiv.org/abs/1904.09644v2) - [pdf](http://arxiv.org/pdf/1904.09644v2)

> This paper introduces intermittent learning - the goal of which is to enable energy harvested computing platforms capable of executing certain classes of machine learning tasks effectively and efficiently. We identify unique challenges to intermittent learning relating to the data and application semantics of machine learning tasks, and to address these challenges, we devise 1) an algorithm that determines a sequence of actions to achieve the desired learning objective under tight energy constraints, and 2) propose three heuristics that help an intermittent learner decide whether to learn or discard training examples at run-time which increases the energy efficiency of the system. We implement and evaluate three intermittent learning applications that learn the 1) air quality, 2) human presence, and 3) vibration using solar, RF, and kinetic energy harvesters, respectively. We demonstrate that the proposed framework improves the energy efficiency of a learner by up to 100% and cuts down the number of learning examples by up to 50% when compared to state-of-the-art intermittent computing systems that do not implement the proposed intermittent learning framework.

</details>

<details>

<summary>2019-12-16 03:57:55 - Penalized-likelihood PET Image Reconstruction Using 3D Structural Convolutional Sparse Coding</summary>

- *Nuobei Xie, Kuang Gong, Ning Guo, Zhixin Qin, Zhifang Wu, Huafeng Liu, Quanzheng Li*

- `1912.07180v1` - [abs](http://arxiv.org/abs/1912.07180v1) - [pdf](http://arxiv.org/pdf/1912.07180v1)

> Positron emission tomography (PET) is widely used for clinical diagnosis. As PET suffers from low resolution and high noise, numerous efforts try to incorporate anatomical priors into PET image reconstruction, especially with the development of hybrid PET/CT and PET/MRI systems. In this work, we proposed a novel 3D structural convolutional sparse coding (CSC) concept for penalized-likelihood PET image reconstruction, named 3D PET-CSC. The proposed 3D PET-CSC takes advantage of the convolutional operation and manages to incorporate anatomical priors without the need of registration or supervised training. As 3D PET-CSC codes the whole 3D PET image, instead of patches, it alleviates the staircase artifacts commonly presented in traditional patch-based sparse coding methods. Moreover, we developed the residual-image and order-subset mechanisms to further reduce the computational cost and accelerate the convergence for the proposed 3D PET-CSC method. Experiments based on computer simulations and clinical datasets demonstrate the superiority of 3D PET-CSC compared with other reference methods.

</details>

<details>

<summary>2019-12-16 07:23:21 - Graph-based Neural Sentence Ordering</summary>

- *Yongjing Yin, Linfeng Song, Jinsong Su, Jiali Zeng, Chulun Zhou, Jiebo Luo*

- `1912.07225v1` - [abs](http://arxiv.org/abs/1912.07225v1) - [pdf](http://arxiv.org/pdf/1912.07225v1)

> Sentence ordering is to restore the original paragraph from a set of sentences. It involves capturing global dependencies among sentences regardless of their input order. In this paper, we propose a novel and flexible graph-based neural sentence ordering model, which adopts graph recurrent network \cite{Zhang:acl18} to accurately learn semantic representations of the sentences. Instead of assuming connections between all pairs of input sentences, we use entities that are shared among multiple sentences to make more expressive graph representations with less noise. Experimental results show that our proposed model outperforms the existing state-of-the-art systems on several benchmark datasets, demonstrating the effectiveness of our model. We also conduct a thorough analysis on how entities help the performance.

</details>

<details>

<summary>2019-12-16 14:45:56 - Semantic Similarity To Improve Question Understanding in a Virtual Patient</summary>

- *Fréjus A. A. Laleye, Antonia Blanié, Antoine Brouquet, Dan Behnamou, Gaël de Chalendar*

- `1912.07421v1` - [abs](http://arxiv.org/abs/1912.07421v1) - [pdf](http://arxiv.org/pdf/1912.07421v1)

> In medicine, a communicating virtual patient or doctor allows students to train in medical diagnosis and develop skills to conduct a medical consultation. In this paper, we describe a conversational virtual standardized patient system to allow medical students to simulate a diagnosis strategy of an abdominal surgical emergency. We exploited the semantic properties captured by distributed word representations to search for similar questions in the virtual patient dialogue system. We created two dialogue systems that were evaluated on datasets collected during tests with students. The first system based on hand-crafted rules obtains $92.29\%$ as $F1$-score on the studied clinical case while the second system that combines rules and semantic similarity achieves $94.88\%$. It represents an error reduction of $9.70\%$ as compared to the rules-only-based system.

</details>

<details>

<summary>2019-12-16 16:15:06 - Polynomial Rewritings from Expressive Description Logics with Closed Predicates to Variants of Datalog</summary>

- *Shqiponja Ahmetaj, Magdalena Ortiz, Mantas Simkus*

- `1912.07475v1` - [abs](http://arxiv.org/abs/1912.07475v1) - [pdf](http://arxiv.org/pdf/1912.07475v1)

> In many scenarios, complete and incomplete information coexist. For this reason, the knowledge representation and database communities have long shown interest in simultaneously supporting the closed- and the open-world views when reasoning about logic theories. Here we consider the setting of querying possibly incomplete data using logic theories, formalized as the evaluation of an ontology-mediated query (OMQ) that pairs a query with a theory, sometimes called an ontology, expressing background knowledge. This can be further enriched by specifying a set of closed predicates from the theory that are to be interpreted under the closed-world assumption, while the rest are interpreted with the open-world view. In this way we can retrieve more precise answers to queries by leveraging the partial completeness of the data.   The central goal of this paper is to understand the relative expressiveness of OMQ languages in which the ontology is written in the expressive Description Logic (DL) ALCHOI and includes a set of closed predicates. We consider a restricted class of conjunctive queries. Our main result is to show that every query in this non-monotonic query language can be translated in polynomial time into Datalog with negation under the stable model semantics. To overcome the challenge that Datalog has no direct means to express the existential quantification present in ALCHOI, we define a two-player game that characterizes the satisfaction of the ontology, and design a Datalog query that can decide the existence of a winning strategy for the game. If there are no closed predicates, that is in the case of querying a plain ALCHOI knowledge base, our translation yields a positive disjunctive Datalog program of polynomial size. To the best of our knowledge, unlike previous translations for related fragments with expressive (non-Horn) DLs, these are the first polynomial time translations.

</details>

<details>

<summary>2019-12-16 16:21:13 - Image Manipulation with Natural Language using Two-sidedAttentive Conditional Generative Adversarial Network</summary>

- *Dawei Zhu, Aditya Mogadala, Dietrich Klakow*

- `1912.07478v1` - [abs](http://arxiv.org/abs/1912.07478v1) - [pdf](http://arxiv.org/pdf/1912.07478v1)

> Altering the content of an image with photo editing tools is a tedious task for an inexperienced user. Especially, when modifying the visual attributes of a specific object in an image without affecting other constituents such as background etc. To simplify the process of image manipulation and to provide more control to users, it is better to utilize a simpler interface like natural language. Therefore, in this paper, we address the challenge of manipulating images using natural language description. We propose the Two-sidEd Attentive conditional Generative Adversarial Network (TEA-cGAN) to generate semantically manipulated images while preserving other contents such as background intact. TEA-cGAN uses fine-grained attention both in the generator and discriminator of Generative Adversarial Network (GAN) based framework at different scales. Experimental results show that TEA-cGAN which generates 128x128 and 256x256 resolution images outperforms existing methods on CUB and Oxford-102 datasets both quantitatively and qualitatively.

</details>

<details>

<summary>2019-12-16 17:12:00 - Scale-dependent Relationships in Natural Language</summary>

- *Aakash Sarkar, Marc Howard*

- `1912.07506v1` - [abs](http://arxiv.org/abs/1912.07506v1) - [pdf](http://arxiv.org/pdf/1912.07506v1)

> Natural language exhibits statistical dependencies at a wide range of scales. For instance, the mutual information between words in natural language decays like a power law with the temporal lag between them. However, many statistical learning models applied to language impose a sampling scale while extracting statistical structure. For instance, Word2Vec constructs a vector embedding that maximizes the prediction between a target word and the context words that appear nearby in the corpus. The size of the context is chosen by the user and defines a strong scale; relationships over much larger temporal scales would be invisible to the algorithm. This paper examines the family of Word2Vec embeddings generated while systematically manipulating the sampling scale used to define the context around each word. The primary result is that different linguistic relationships are preferentially encoded at different scales. Different scales emphasize different syntactic and semantic relations between words.Moreover, the neighborhoods of a given word in the embeddings change significantly depending on the scale. These results suggest that any individual scale can only identify a subset of the meaningful relationships a word might have, and point toward the importance of developing scale-free models of semantic meaning.

</details>

<details>

<summary>2019-12-16 21:40:16 - Average Individual Fairness: Algorithms, Generalization and Experiments</summary>

- *Michael Kearns, Aaron Roth, Saeed Sharifi-Malvajerdi*

- `1905.10607v2` - [abs](http://arxiv.org/abs/1905.10607v2) - [pdf](http://arxiv.org/pdf/1905.10607v2)

> We propose a new family of fairness definitions for classification problems that combine some of the best properties of both statistical and individual notions of fairness. We posit not only a distribution over individuals, but also a distribution over (or collection of) classification tasks. We then ask that standard statistics (such as error or false positive/negative rates) be (approximately) equalized across individuals, where the rate is defined as an expectation over the classification tasks. Because we are no longer averaging over coarse groups (such as race or gender), this is a semantically meaningful individual-level constraint. Given a sample of individuals and classification problems, we design an oracle-efficient algorithm (i.e. one that is given access to any standard, fairness-free learning heuristic) for the fair empirical risk minimization task. We also show that given sufficiently many samples, the ERM solution generalizes in two directions: both to new individuals, and to new classification tasks, drawn from their corresponding distributions. Finally we implement our algorithm and empirically verify its effectiveness.

</details>

<details>

<summary>2019-12-16 23:39:08 - Human-In-The-Loop Automatic Program Repair</summary>

- *Marcel Böhme, Charaka Geethal, Van-Thuan Pham*

- `1912.07758v1` - [abs](http://arxiv.org/abs/1912.07758v1) - [pdf](http://arxiv.org/pdf/1912.07758v1)

> We introduce Learn2fix, the first human-in-the-loop, semi-automatic repair technique when no bug oracle--except for the user who is reporting the bug--is available. Our approach negotiates with the user the condition under which the bug is observed. Only when a budget of queries to the user is exhausted, it attempts to repair the bug. A query can be thought of as the following question: "When executing this alternative test input, the program produces the following output; is the bug observed"? Through systematic queries, Learn2fix trains an automatic bug oracle that becomes increasingly more accurate in predicting the user's response. Our key challenge is to maximize the oracle's accuracy in predicting which tests are bug-exposing given a small budget of queries. From the alternative tests that were labeled by the user, test-driven automatic repair produces the patch.   Our experiments demonstrate that Learn2fix learns a sufficiently accurate automatic oracle with a reasonably low labeling effort (lt. 20 queries). Given Learn2fix's test suite, the GenProg test-driven repair tool produces a higher-quality patch (i.e., passing a larger proportion of validation tests) than using manual test suites provided with the repair benchmark.

</details>

<details>

<summary>2019-12-17 06:05:49 - Design and Implementation of Linked Planning Domain Definition Language</summary>

- *Michiaki Tatsubori, Asim Munawar, Takao Moriyama*

- `1912.07834v1` - [abs](http://arxiv.org/abs/1912.07834v1) - [pdf](http://arxiv.org/pdf/1912.07834v1)

> Planning is a critical component of any artificial intelligence system that concerns the realization of strategies or action sequences typically for intelligent agents and autonomous robots. Given predefined parameterized actions, a planning service should accept a query with the goal and initial state to give a solution with a sequence of actions applied to environmental objects. This paper addresses the problem by providing a repository of actions generically applicable to various environmental objects based on Semantic Web technologies. Ontologies are used for asserting constraints in common sense as well as for resolving compatibilities between actions and states. Constraints are defined using Web standards such as SPARQL and SHACL to allow conditional predicates. We demonstrate the usefulness of the proposed planning domain description language with our robotics applications.

</details>

<details>

<summary>2019-12-17 09:01:30 - Analyzing Structures in the Semantic Vector Space: A Framework for Decomposing Word Embeddings</summary>

- *Andreas Hanselowski, Iryna Gurevych*

- `1912.10434v1` - [abs](http://arxiv.org/abs/1912.10434v1) - [pdf](http://arxiv.org/pdf/1912.10434v1)

> Word embeddings are rich word representations, which in combination with deep neural networks, lead to large performance gains for many NLP tasks. However, word embeddings are represented by dense, real-valued vectors and they are therefore not directly interpretable. Thus, computational operations based on them are also not well understood. In this paper, we present an approach for analyzing structures in the semantic vector space to get a better understanding of the underlying semantic encoding principles. We present a framework for decomposing word embeddings into smaller meaningful units which we call sub-vectors. The framework opens up a wide range of possibilities analyzing phenomena in vector space semantics, as well as solving concrete NLP problems: We introduce the category completion task and show that a sub-vector based approach is superior to supervised techniques; We present a sub-vector based method for solving the word analogy task, which substantially outperforms different variants of the traditional vector-offset method.

</details>

<details>

<summary>2019-12-17 09:06:31 - Joint Interaction and Trajectory Prediction for Autonomous Driving using Graph Neural Networks</summary>

- *Donsuk Lee, Yiming Gu, Jerrick Hoang, Micol Marchetti-Bowick*

- `1912.07882v1` - [abs](http://arxiv.org/abs/1912.07882v1) - [pdf](http://arxiv.org/pdf/1912.07882v1)

> In this work, we aim to predict the future motion of vehicles in a traffic scene by explicitly modeling their pairwise interactions. Specifically, we propose a graph neural network that jointly predicts the discrete interaction modes and 5-second future trajectories for all agents in the scene. Our model infers an interaction graph whose nodes are agents and whose edges capture the long-term interaction intents among the agents. In order to train the model to recognize known modes of interaction, we introduce an auto-labeling function to generate ground truth interaction labels. Using a large-scale real-world driving dataset, we demonstrate that jointly predicting the trajectories along with the explicit interaction types leads to significantly lower trajectory error than baseline methods. Finally, we show through simulation studies that the learned interaction modes are semantically meaningful.

</details>

<details>

<summary>2019-12-17 10:15:39 - Gextext: Disease Network Extraction from Biomedical Literature</summary>

- *Robert O'Shea*

- `1911.02562v2` - [abs](http://arxiv.org/abs/1911.02562v2) - [pdf](http://arxiv.org/pdf/1911.02562v2)

> PURPOSE: We propose a fully unsupervised method to learn latent disease networks directly from unstructured biomedical text corpora. This method addresses current challenges in unsupervised knowledge extraction, such as the detection of long-range dependencies and requirements for large training corpora. METHODS: Let C be a corpus of n text chunks. Let V be a set of p disease terms occurring in the corpus. Let X indicate the occurrence of V in C. Gextext identifies disease similarities by positively correlated occurrence patterns. This information is combined to generate a graph on which geodesic distance describes dissimilarity. Diseasomes were learned by Gextext and GloVE on corpora of 100-1000 PubMed abstracts. Similarity matrix estimates were validated against biomedical semantic similarity metrics and gene profile similarity. RESULTS: Geodesic distance on Gextext-inferred diseasomes correlated inversely with external measures of semantic similarity. Gene profile similarity also correlated significant with proximity on the inferred graph. Gextext outperformed GloVE in our experiments. The information contained on the Gextext graph exceeded the explicit information content within the text. CONCLUSIONS: Gextext extracts latent relationships from unstructured text, enabling fully unsupervised modelling of diseasome graphs from PubMed abstracts.

</details>

<details>

<summary>2019-12-18 03:09:12 - DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog</summary>

- *Feilong Chen, Fandong Meng, Jiaming Xu, Peng Li, Bo Xu, Jie Zhou*

- `1912.08360v1` - [abs](http://arxiv.org/abs/1912.08360v1) - [pdf](http://arxiv.org/pdf/1912.08360v1)

> Visual Dialog is a vision-language task that requires an AI agent to engage in a conversation with humans grounded in an image. It remains a challenging task since it requires the agent to fully understand a given question before making an appropriate response not only from the textual dialog history, but also from the visually-grounded information. While previous models typically leverage single-hop reasoning or single-channel reasoning to deal with this complex multimodal reasoning task, which is intuitively insufficient. In this paper, we thus propose a novel and more powerful Dual-channel Multi-hop Reasoning Model for Visual Dialog, named DMRM. DMRM synchronously captures information from the dialog history and the image to enrich the semantic representation of the question by exploiting dual-channel reasoning. Specifically, DMRM maintains a dual channel to obtain the question- and history-aware image features and the question- and image-aware dialog history features by a mulit-hop reasoning process in each channel. Additionally, we also design an effective multimodal attention to further enhance the decoder to generate more accurate responses. Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that the proposed model is effective and outperforms compared models by a significant margin.

</details>

<details>

<summary>2019-12-18 22:00:19 - Enabling Smartphone-based Estimation of Heart Rate</summary>

- *Nutta Homdee, Mehdi Boukhechba, Yixue W. Feng, Natalie Kramer, John Lach, Laura E. Barnes*

- `1912.08910v1` - [abs](http://arxiv.org/abs/1912.08910v1) - [pdf](http://arxiv.org/pdf/1912.08910v1)

> Continuous, ubiquitous monitoring through wearable sensors has the potential to collect useful information about users' context. Heart rate is an important physiologic measure used in a wide variety of applications, such as fitness tracking and health monitoring. However, wearable sensors that monitor heart rate, such as smartwatches and electrocardiogram (ECG) patches, can have gaps in their data streams because of technical issues (e.g., bad wireless channels, battery depletion, etc.) or user-related reasons (e.g. motion artifacts, user compliance, etc.). The ability to use other available sensor data (e.g., smartphone data) to estimate missing heart rate readings is useful to cope with any such gaps, thus improving data quality and continuity. In this paper, we test the feasibility of estimating raw heart rate using smartphone sensor data. Using data generated by 12 participants in a one-week study period, we were able to build both personalized and generalized models using regression, SVM, and random forest algorithms. All three algorithms outperformed the baseline moving-average interpolation method for both personalized and generalized settings. Moreover, our findings suggest that personalized models outperformed the generalized models, which speaks to the importance of considering personal physiology, behavior, and life style in the estimation of heart rate. The promising results provide preliminary evidence of the feasibility of combining smartphone sensor data with wearable sensor data for continuous heart rate monitoring.

</details>

<details>

<summary>2019-12-19 03:48:05 - Discriminative Sentence Modeling for Story Ending Prediction</summary>

- *Yiming Cui, Wanxiang Che, Wei-Nan Zhang, Ting Liu, Shijin Wang, Guoping Hu*

- `1912.09008v1` - [abs](http://arxiv.org/abs/1912.09008v1) - [pdf](http://arxiv.org/pdf/1912.09008v1)

> Story Ending Prediction is a task that needs to select an appropriate ending for the given story, which requires the machine to understand the story and sometimes needs commonsense knowledge. To tackle this task, we propose a new neural network called Diff-Net for better modeling the differences of each ending in this task. The proposed model could discriminate two endings in three semantic levels: contextual representation, story-aware representation, and discriminative representation. Experimental results on the Story Cloze Test dataset show that the proposed model siginificantly outperforms various systems by a large margin, and detailed ablation studies are given for better understanding our model. We also carefully examine the traditional and BERT-based models on both SCT v1.0 and v1.5 with interesting findings that may potentially help future studies.

</details>

<details>

<summary>2019-12-19 10:06:09 - SAFE: Self-Attentive Function Embeddings for Binary Similarity</summary>

- *Luca Massarelli, Giuseppe Antonio Di Luna, Fabio Petroni, Leonardo Querzoni, Roberto Baldoni*

- `1811.05296v4` - [abs](http://arxiv.org/abs/1811.05296v4) - [pdf](http://arxiv.org/pdf/1811.05296v4)

> The binary similarity problem consists in determining if two functions are similar by only considering their compiled form. Advanced techniques for binary similarity recently gained momentum as they can be applied in several fields, such as copyright disputes, malware analysis, vulnerability detection, etc., and thus have an immediate practical impact. Current solutions compare functions by first transforming their binary code in multi-dimensional vector representations (embeddings), and then comparing vectors through simple and efficient geometric operations. However, embeddings are usually derived from binary code using manual feature extraction, that may fail in considering important function characteristics, or may consider features that are not important for the binary similarity problem. In this paper we propose SAFE, a novel architecture for the embedding of functions based on a self-attentive neural network. SAFE works directly on disassembled binary functions, does not require manual feature extraction, is computationally more efficient than existing solutions (i.e., it does not incur in the computational overhead of building or manipulating control flow graphs), and is more general as it works on stripped binaries and on multiple architectures. We report the results from a quantitative and qualitative analysis that show how SAFE provides a noticeable performance improvement with respect to previous solutions. Furthermore, we show how clusters of our embedding vectors are closely related to the semantic of the implemented algorithms, paving the way for further interesting applications (e.g. semantic-based binary function search).

</details>

<details>

<summary>2019-12-19 15:55:42 - Gaussianity and typicality in matrix distributional semantics</summary>

- *Sanjaye Ramgoolam, Mehrnoosh Sadrzadeh, Lewis Sword*

- `1912.10839v1` - [abs](http://arxiv.org/abs/1912.10839v1) - [pdf](http://arxiv.org/pdf/1912.10839v1)

> Constructions in type-driven compositional distributional semantics associate large collections of matrices of size $D$ to linguistic corpora. We develop the proposal of analysing the statistical characteristics of this data in the framework of permutation invariant matrix models. The observables in this framework are permutation invariant polynomial functions of the matrix entries, which correspond to directed graphs. Using the general 13-parameter permutation invariant Gaussian matrix models recently solved, we find, using a dataset of matrices constructed via standard techniques in distributional semantics, that the expectation values of a large class of cubic and quartic observables show high gaussianity at levels between 90 to 99 percent. Beyond expectation values, which are averages over words, the dataset allows the computation of standard deviations for each observable, which can be viewed as a measure of typicality for each observable. There is a wide range of magnitudes in the measures of typicality. The permutation invariant matrix models, considered as functions of random couplings, give a very good prediction of the magnitude of the typicality for different observables. We find evidence that observables with similar matrix model characteristics of Gaussianity and typicality also have high degrees of correlation between the ranked lists of words associated to these observables.

</details>

<details>

<summary>2019-12-19 16:33:42 - A deep learning framework for morphologic detail beyond the diffraction limit in infrared spectroscopic imaging</summary>

- *Kianoush Falahkheirkhah, Kevin Yeh, Shachi Mittal, Luke Pfister, Rohit Bhargava*

- `1911.04410v2` - [abs](http://arxiv.org/abs/1911.04410v2) - [pdf](http://arxiv.org/pdf/1911.04410v2)

> Infrared (IR) microscopes measure spectral information that quantifies molecular content to assign the identity of biomedical cells but lack the spatial quality of optical microscopy to appreciate morphologic features. Here, we propose a method to utilize the semantic information of cellular identity from IR imaging with the morphologic detail of pathology images in a deep learning-based approach to image super-resolution. Using Generative Adversarial Networks (GANs), we enhance the spatial detail in IR imaging beyond the diffraction limit while retaining their spectral contrast. This technique can be rapidly integrated with modern IR microscopes to provide a framework useful for routine pathology.

</details>

<details>

<summary>2019-12-19 17:29:15 - An Ensemble Method to Produce High-Quality Word Embeddings (2016)</summary>

- *Robyn Speer, Joshua Chin*

- `1604.01692v2` - [abs](http://arxiv.org/abs/1604.01692v2) - [pdf](http://arxiv.org/pdf/1604.01692v2)

> A currently successful approach to computational semantics is to represent words as embeddings in a machine-learned vector space. We present an ensemble method that combines embeddings produced by GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) with structured knowledge from the semantic networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al., 2013), merging their information into a common representation with a large, multilingual vocabulary. The embeddings it produces achieve state-of-the-art performance on many word-similarity evaluations. Its score of $\rho = .596$ on an evaluation of rare words (Luong et al., 2013) is 16% higher than the previous best known system.

</details>

<details>

<summary>2019-12-19 20:02:50 - Commonsense Knowledge Base Completion with Structural and Semantic Context</summary>

- *Chaitanya Malaviya, Chandra Bhagavatula, Antoine Bosselut, Yejin Choi*

- `1910.02915v2` - [abs](http://arxiv.org/abs/1910.02915v2) - [pdf](http://arxiv.org/pdf/1910.02915v2)

> Automatic KB completion for commonsense knowledge graphs (e.g., ATOMIC and ConceptNet) poses unique challenges compared to the much studied conventional knowledge bases (e.g., Freebase). Commonsense knowledge graphs use free-form text to represent nodes, resulting in orders of magnitude more nodes compared to conventional KBs (18x more nodes in ATOMIC compared to Freebase (FB15K-237)). Importantly, this implies significantly sparser graph structures - a major challenge for existing KB completion methods that assume densely connected graphs over a relatively smaller set of nodes. In this paper, we present novel KB completion models that can address these challenges by exploiting the structural and semantic context of nodes. Specifically, we investigate two key ideas: (1) learning from local graph structure, using graph convolutional networks and automatic graph densification and (2) transfer learning from pre-trained language models to knowledge graphs for enhanced contextual representation of knowledge. We describe our method to incorporate information from both these sources in a joint model and provide the first empirical results for KB completion on ATOMIC and evaluation with ranking metrics on ConceptNet. Our results demonstrate the effectiveness of language model representations in boosting link prediction performance and the advantages of learning from local graph structure (+1.5 points in MRR for ConceptNet) when training on subgraphs for computational efficiency. Further analysis on model predictions shines light on the types of commonsense knowledge that language models capture well.

</details>

<details>

<summary>2019-12-19 21:29:22 - Deep Exemplar Networks for VQA and VQG</summary>

- *Badri N. Patro, Vinay P. Namboodiri*

- `1912.09551v1` - [abs](http://arxiv.org/abs/1912.09551v1) - [pdf](http://arxiv.org/pdf/1912.09551v1)

> In this paper, we consider the problem of solving semantic tasks such as `Visual Question Answering' (VQA), where one aims to answers related to an image and `Visual Question Generation' (VQG), where one aims to generate a natural question pertaining to an image. Solutions for VQA and VQG tasks have been proposed using variants of encoder-decoder deep learning based frameworks that have shown impressive performance. Humans however often show generalization by relying on exemplar based approaches. For instance, the work by Tversky and Kahneman suggests that humans use exemplars when making categorizations and decisions. In this work, we propose the incorporation of exemplar based approaches towards solving these problems. Specifically, we incorporate exemplar based approaches and show that an exemplar based module can be incorporated in almost any of the deep learning architectures proposed in the literature and the addition of such a block results in improved performance for solving these tasks. Thus, just as the incorporation of attention is now considered de facto useful for solving these tasks, similarly, incorporating exemplars also can be considered to improve any proposed architecture for solving this task. We provide extensive empirical analysis for the same through various architectures, ablations, and state of the art comparisons.

</details>

<details>

<summary>2019-12-19 22:59:26 - BERTje: A Dutch BERT Model</summary>

- *Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, Malvina Nissim*

- `1912.09582v1` - [abs](http://arxiv.org/abs/1912.09582v1) - [pdf](http://arxiv.org/pdf/1912.09582v1)

> The transformer-based pre-trained language model BERT has helped to improve state-of-the-art performance on many natural language processing (NLP) tasks. Using the same architecture and parameters, we developed and evaluated a monolingual Dutch BERT model called BERTje. Compared to the multilingual BERT model, which includes Dutch but is only based on Wikipedia text, BERTje is based on a large and diverse dataset of 2.4 billion tokens. BERTje consistently outperforms the equally-sized multilingual BERT model on downstream NLP tasks (part-of-speech tagging, named-entity recognition, semantic role labeling, and sentiment analysis). Our pre-trained Dutch BERT model is made available at https://github.com/wietsedv/bertje.

</details>

<details>

<summary>2019-12-20 04:25:48 - Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</summary>

- *Wenhan Xiong, Jingfei Du, William Yang Wang, Veselin Stoyanov*

- `1912.09637v1` - [abs](http://arxiv.org/abs/1912.09637v1) - [pdf](http://arxiv.org/pdf/1912.09637v1)

> Recent breakthroughs of pretrained language models have shown the effectiveness of self-supervised learning for a wide range of natural language processing (NLP) tasks. In addition to standard syntactic and semantic NLP tasks, pretrained models achieve strong improvements on tasks that involve real-world knowledge, suggesting that large-scale language modeling could be an implicit method to capture knowledge. In this work, we further investigate the extent to which pretrained models such as BERT capture knowledge using a zero-shot fact completion task. Moreover, we propose a simple yet effective weakly supervised pretraining objective, which explicitly forces the model to incorporate knowledge about real-world entities. Models trained with our new objective yield significant improvements on the fact completion task. When applied to downstream tasks, our model consistently outperforms BERT on four entity-related question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard fine-grained entity typing dataset (i.e., FIGER) with 5.7 accuracy gains.

</details>

<details>

<summary>2019-12-20 06:14:18 - CORE: Automating Review Recommendation for Code Changes</summary>

- *JingKai Siow, Cuiyun Gao, Lingling Fan, Sen Chen, Yang Liu*

- `1912.09652v1` - [abs](http://arxiv.org/abs/1912.09652v1) - [pdf](http://arxiv.org/pdf/1912.09652v1)

> Code review is a common process that is used by developers, in which a reviewer provides useful comments or points out defects in the submitted source code changes via pull request. Code review has been widely used for both industry and open-source projects due to its capacity in early defect identification, project maintenance, and code improvement. With rapid updates on project developments, code review becomes a non-trivial and labor-intensive task for reviewers. Thus, an automated code review engine can be beneficial and useful for project development in practice. Although there exist prior studies on automating the code review process by adopting static analysis tools or deep learning techniques, they often require external sources such as partial or full source code for accurate review suggestion. In this paper, we aim at automating the code review process only based on code changes and the corresponding reviews but with better performance. The hinge of accurate code review suggestion is to learn good representations for both code changes and reviews. To achieve this with limited source, we design a multi-level embedding (i.e., word embedding and character embedding) approach to represent the semantics provided by code changes and reviews. The embeddings are then well trained through a proposed attentional deep learning model, as a whole named CORE. We evaluate the effectiveness of CORE on code changes and reviews collected from 19 popular Java projects hosted on Github. Experimental results show that our model CORE can achieve significantly better performance than the state-of-the-art model (DeepMem), with an increase of 131.03% in terms of Recall@10 and 150.69% in terms of Mean Reciprocal Rank. Qualitative general word analysis among project developers also demonstrates the performance of CORE in automating code review.

</details>

<details>

<summary>2019-12-20 22:53:18 - Modeling Intent, Dialog Policies and Response Adaptation for Goal-Oriented Interactions</summary>

- *Saurav Sahay, Shachi H Kumar, Eda Okur, Haroon Syed, Lama Nachman*

- `1912.10130v1` - [abs](http://arxiv.org/abs/1912.10130v1) - [pdf](http://arxiv.org/pdf/1912.10130v1)

> Building a machine learning driven spoken dialog system for goal-oriented interactions involves careful design of intents and data collection along with development of intent recognition models and dialog policy learning algorithms. The models should be robust enough to handle various user distractions during the interaction flow and should steer the user back into an engaging interaction for successful completion of the interaction. In this work, we have designed a goal-oriented interaction system where children can engage with agents for a series of interactions involving `Meet \& Greet' and `Simon Says' game play. We have explored various feature extractors and models for improved intent recognition and looked at leveraging previous user and system interactions in novel ways with attention models. We have also looked at dialog adaptation methods for entrained response selection. Our bootstrapped models from limited training data perform better than many baseline approaches we have looked at for intent recognition and dialog action prediction.

</details>

<details>

<summary>2019-12-20 22:56:54 - Exploring Context, Attention and Audio Features for Audio Visual Scene-Aware Dialog</summary>

- *Shachi H Kumar, Eda Okur, Saurav Sahay, Jonathan Huang, Lama Nachman*

- `1912.10132v1` - [abs](http://arxiv.org/abs/1912.10132v1) - [pdf](http://arxiv.org/pdf/1912.10132v1)

> We are witnessing a confluence of vision, speech and dialog system technologies that are enabling the IVAs to learn audio-visual groundings of utterances and have conversations with users about the objects, activities and events surrounding them. Recent progress in visual grounding techniques and Audio Understanding are enabling machines to understand shared semantic concepts and listen to the various sensory events in the environment. With audio and visual grounding methods, end-to-end multimodal SDS are trained to meaningfully communicate with us in natural language about the real dynamic audio-visual sensory world around us. In this work, we explore the role of `topics' as the context of the conversation along with multimodal attention into such an end-to-end audio-visual scene-aware dialog system architecture. We also incorporate an end-to-end audio classification ConvNet, AclNet, into our models. We develop and test our approaches on the Audio Visual Scene-Aware Dialog (AVSD) dataset released as a part of the DSTC7. We present the analysis of our experiments and show that some of our model variations outperform the baseline system released for AVSD.

</details>

<details>

<summary>2019-12-21 13:02:43 - Semantics Preserving Adversarial Learning</summary>

- *Ousmane Amadou Dia, Elnaz Barshan, Reza Babanezhad*

- `1903.03905v5` - [abs](http://arxiv.org/abs/1903.03905v5) - [pdf](http://arxiv.org/pdf/1903.03905v5)

> While progress has been made in crafting visually imperceptible adversarial examples, constructing semantically meaningful ones remains a challenge. In this paper, we propose a framework to generate semantics preserving adversarial examples. First, we present a manifold learning method to capture the semantics of the inputs. The motivating principle is to learn the low-dimensional geometric summaries of the inputs via statistical inference. Then, we perturb the elements of the learned manifold using the Gram-Schmidt process to induce the perturbed elements to remain in the manifold. To produce adversarial examples, we propose an efficient algorithm whereby we leverage the semantics of the inputs as a source of knowledge upon which we impose adversarial constraints. We apply our approach on toy data, images and text, and show its effectiveness in producing semantics preserving adversarial examples which evade existing defenses against adversarial attacks.

</details>

<details>

<summary>2019-12-22 17:40:19 - Recurrent Feedback Improves Feedforward Representations in Deep Neural Networks</summary>

- *Siming Yan, Xuyang Fang, Bowen Xiao, Harold Rockwell, Yimeng Zhang, Tai Sing Lee*

- `1912.10489v1` - [abs](http://arxiv.org/abs/1912.10489v1) - [pdf](http://arxiv.org/pdf/1912.10489v1)

> The abundant recurrent horizontal and feedback connections in the primate visual cortex are thought to play an important role in bringing global and semantic contextual information to early visual areas during perceptual inference, helping to resolve local ambiguity and fill in missing details. In this study, we find that introducing feedback loops and horizontal recurrent connections to a deep convolution neural network (VGG16) allows the network to become more robust against noise and occlusion during inference, even in the initial feedforward pass. This suggests that recurrent feedback and contextual modulation transform the feedforward representations of the network in a meaningful and interesting way. We study the population codes of neurons in the network, before and after learning with feedback, and find that learning with feedback yielded an increase in discriminability (measured by d-prime) between the different object classes in the population codes of the neurons in the feedforward path, even at the earliest layer that receives feedback. We find that recurrent feedback, by injecting top-down semantic meaning to the population activities, helps the network learn better feedforward paths to robustly map noisy image patches to the latent representations corresponding to important visual concepts of each object class, resulting in greater robustness of the network against noises and occlusion as well as better fine-grained recognition.

</details>

<details>

<summary>2019-12-22 19:21:05 - Bringing Belief Base Change into Dynamic Epistemic Logic</summary>

- *Marlo Souza, Álvaro Moreira*

- `1912.10515v1` - [abs](http://arxiv.org/abs/1912.10515v1) - [pdf](http://arxiv.org/pdf/1912.10515v1)

> AGM's belief revision is one of the main paradigms in the study of belief change operations. In this context, belief bases (prioritised bases) have been primarily used to specify the agent's belief state. While the connection of iterated AGM-like operations and their encoding in dynamic epistemic logics have been studied before, few works considered how well-known postulates from iterated belief revision theory can be characterised by means of belief bases and their counterpart in dynamic epistemic logic. Particularly, it has been shown that some postulates can be characterised through transformations in priority graphs, while others may not be represented that way. This work investigates changes in the semantics of Dynamic Preference Logic that give rise to an appropriate syntactic representation for its models that allow us to represent and reason about iterated belief base change in this logic.

</details>

<details>

<summary>2019-12-22 22:18:52 - Leveraging protection and efficiency of query answering in heterogenous RDF data using blockchain</summary>

- *Sara Hosseinzadeh Kassani, Ralph Deters*

- `1810.05292v2` - [abs](http://arxiv.org/abs/1810.05292v2) - [pdf](http://arxiv.org/pdf/1810.05292v2)

> The Semantic Web, an extension of the current web, provides a common framework that makes data machine understandable and also allows data to be shared and reused across various applications. Resource Description Framework (RDF), a graph-based data model for describing things (entities), facilitates data integration. Due to the explosion of the amount of RDF data, developing tools to support processing and answering of complex queries over the integrated data has become challenging. To overcome this challenge in query processing in semantic data integration frameworks, we provide a view layer inserted between the heterogeneous data sources and user interface layer while ensuring only authorized users are allowed access to the information. The view layer must provide a support in terms of access, integration, querying, management of data sources in a multi-user environment.

</details>

<details>

<summary>2019-12-23 05:40:20 - Deeply Integrating C11 Code Support into Isabelle/PIDE</summary>

- *Frédéric Tuong, Burkhart Wolff*

- `1912.10630v1` - [abs](http://arxiv.org/abs/1912.10630v1) - [pdf](http://arxiv.org/pdf/1912.10630v1)

> We present a framework for C code in C11 syntax deeply integrated into the Isabelle/PIDE development environment. Our framework provides an abstract interface for verification back-ends to be plugged-in independently. Thus, various techniques such as deductive program verification or white-box testing can be applied to the same source, which is part of an integrated PIDE document model. Semantic back-ends are free to choose the supported C fragment and its semantics. In particular, they can differ on the chosen memory model or the specification mechanism for framing conditions.   Our framework supports semantic annotations of C sources in the form of comments. Annotations serve to locally control back-end settings, and can express the term focus to which an annotation refers. Both the logical and the syntactic context are available when semantic annotations are evaluated. As a consequence, a formula in an annotation can refer both to HOL or C variables.   Our approach demonstrates the degree of maturity and expressive power the Isabelle/PIDE subsystem has achieved in recent years. Our integration technique employs Lex and Yacc style grammars to ensure efficient deterministic parsing. We present two case studies for the integration of (known) semantic back-ends in order to validate the design decisions for our back-end interface.

</details>

<details>

<summary>2019-12-23 05:40:43 - A Component-Based Formal Language Workbench</summary>

- *Peter D. Mosses*

- `1912.10631v1` - [abs](http://arxiv.org/abs/1912.10631v1) - [pdf](http://arxiv.org/pdf/1912.10631v1)

> The CBS framework supports component-based specification of programming languages. It aims to significantly reduce the effort of formal language specification, and thereby encourage language developers to exploit formal semantics more widely. CBS provides an extensive library of reusable language specification components, facilitating co-evolution of languages and their specifications.   After introducing CBS and its formal definition, this short paper reports work in progress on generating an IDE for CBS from the definition. It also considers the possibility of supporting component-based language specification in other formal language workbenches.

</details>

<details>

<summary>2019-12-23 12:21:44 - Physics-informed semantic inpainting: Application to geostatistical modeling</summary>

- *Qiang Zheng, Lingzao Zeng, Zhendan Cao, George Em Karniadakis*

- `1909.09459v2` - [abs](http://arxiv.org/abs/1909.09459v2) - [pdf](http://arxiv.org/pdf/1909.09459v2)

> A fundamental problem in geostatistical modeling is to infer the heterogeneous geological field based on limited measurements and some prior spatial statistics. Semantic inpainting, a technique for image processing using deep generative models, has been recently applied for this purpose, demonstrating its effectiveness in dealing with complex spatial patterns. However, the original semantic inpainting framework incorporates only information from direct measurements, while in geostatistics indirect measurements are often plentiful. To overcome this limitation, here we propose a physics-informed semantic inpainting framework, employing the Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) and jointly incorporating the direct and indirect measurements by exploiting the underlying physical laws. Our simulation results for a high-dimensional problem with 512 dimensions show that in the new method, the physical conservation laws are satisfied and contribute in enhancing the inpainting performance compared to using only the direct measurements.

</details>

<details>

<summary>2019-12-23 12:50:32 - A Survey of Deep Learning Applications to Autonomous Vehicle Control</summary>

- *Sampo Kuutti, Richard Bowden, Yaochu Jin, Phil Barber, Saber Fallah*

- `1912.10773v1` - [abs](http://arxiv.org/abs/1912.10773v1) - [pdf](http://arxiv.org/pdf/1912.10773v1)

> Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment. However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios. For these reasons, the use of deep learning for vehicle control is becoming increasingly popular. Although important advancements have been achieved in this field, these works have not been fully summarised. This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods. Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection. The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety. Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.

</details>

<details>

<summary>2019-12-23 14:46:46 - BioConceptVec: creating and evaluating literature-based biomedical concept embeddings on a large scale</summary>

- *Qingyu Chen, Kyubum Lee, Shankai Yan, Sun Kim, Chih-Hsuan Wei, Zhiyong Lu*

- `1912.10846v1` - [abs](http://arxiv.org/abs/1912.10846v1) - [pdf](http://arxiv.org/pdf/1912.10846v1)

> Capturing the semantics of related biological concepts, such as genes and mutations, is of significant importance to many research tasks in computational biology such as protein-protein interaction detection, gene-drug association prediction, and biomedical literature-based discovery. Here, we propose to leverage state-of-the-art text mining tools and machine learning models to learn the semantics via vector representations (aka. embeddings) of over 400,000 biological concepts mentioned in the entire PubMed abstracts. Our learned embeddings, namely BioConceptVec, can capture related concepts based on their surrounding contextual information in the literature, which is beyond exact term match or co-occurrence-based methods. BioConceptVec has been thoroughly evaluated in multiple bioinformatics tasks consisting of over 25 million instances from nine different biological datasets. The evaluation results demonstrate that BioConceptVec has better performance than existing methods in all tasks. Finally, BioConceptVec is made freely available to the research community and general public via https://github.com/ncbi-nlp/BioConceptVec.

</details>

<details>

<summary>2019-12-23 15:49:43 - Large Random Forests: Optimisation for Rapid Evaluation</summary>

- *Frederik Gossen, Bernhard Steffen*

- `1912.10934v1` - [abs](http://arxiv.org/abs/1912.10934v1) - [pdf](http://arxiv.org/pdf/1912.10934v1)

> Random Forests are one of the most popular classifiers in machine learning. The larger they are, the more precise is the outcome of their predictions. However, this comes at a cost: their running time for classification grows linearly with the number of trees, i.e. the size of the forest. In this paper, we propose a method to aggregate large Random Forests into a single, semantically equivalent decision diagram. Our experiments on various popular datasets show speed-ups of several orders of magnitude, while, at the same time, also significantly reducing the size of the required data structure.

</details>

<details>

<summary>2019-12-23 18:01:32 - Semantics- and Syntax-related Subvectors in the Skip-gram Embeddings</summary>

- *Maxat Tezekbayev, Zhenisbek Assylbekov, Rustem Takhanov*

- `1912.13413v1` - [abs](http://arxiv.org/abs/1912.13413v1) - [pdf](http://arxiv.org/pdf/1912.13413v1)

> We show that the skip-gram embedding of any word can be decomposed into two subvectors which roughly correspond to semantic and syntactic roles of the word.

</details>

<details>

<summary>2019-12-25 05:50:20 - A Logical Model for Supporting Social Commonsense Knowledge Acquisition</summary>

- *Zhenzhen Gu, Cungen Cao, Ya Wang, Yuefei Sui*

- `1912.11599v1` - [abs](http://arxiv.org/abs/1912.11599v1) - [pdf](http://arxiv.org/pdf/1912.11599v1)

> To make machine exhibit human-like abilities in the domains like robotics and conversation, social commonsense knowledge (SCK), i.e., common sense about social contexts and social roles, is absolutely necessarily. Therefor, our ultimate goal is to acquire large-scale SCK to support much more intelligent applications. Before that, we need to know clearly what is SCK and how to represent it, since automatic information processing requires data and knowledge are organized in structured and semantically related ways. For this reason, in this paper, we identify and formalize three basic types of SCK based on first-order theory. Firstly, we identify and formalize the interrelationships, such as having-role and having-social_relation, among social contexts, roles and players from the perspective of considering both contexts and roles as first-order citizens and not generating role instances. Secondly, we provide a four level structure to identify and formalize the intrinsic information, such as events and desires, of social contexts, roles and players, and illustrate the way of harvesting the intrinsic information of social contexts and roles from the exhibition of players in concrete contexts. And thirdly, enlightened by some observations of actual contexts, we further introduce and formalize the embedding of social contexts, and depict the way of excavating the intrinsic information of social contexts and roles from the embedded smaller and simpler contexts. The results of this paper lay the foundation not only for formalizing much more complex SCK but also for acquiring these three basic types of SCK.

</details>

<details>

<summary>2019-12-25 11:36:04 - RIMAX: Ranking Semantic Rhymes by calculating Definition Similarity</summary>

- *Alfonso Medina-Urrea, Juan-Manuel Torres-Moreno*

- `1912.09558v2` - [abs](http://arxiv.org/abs/1912.09558v2) - [pdf](http://arxiv.org/pdf/1912.09558v2)

> This paper presents RIMAX, a new system for detecting semantic rhymes, using a Comprehensive Mexican Spanish Dictionary (DEM) and its Rhyming Dictionary (REM). We use the Vector Space Model to calculate the similarity of the definition of a query with the definitions corresponding to the assonant and consonant rhymes of the query. The preliminary results using a manual evaluation are very encouraging.

</details>

<details>

<summary>2019-12-25 15:02:24 - Neural ODEs for Image Segmentation with Level Sets</summary>

- *Rafael Valle, Fitsum Reda, Mohammad Shoeybi, Patrick Legresley, Andrew Tao, Bryan Catanzaro*

- `1912.11683v1` - [abs](http://arxiv.org/abs/1912.11683v1) - [pdf](http://arxiv.org/pdf/1912.11683v1)

> We propose a novel approach for image segmentation that combines Neural Ordinary Differential Equations (NODEs) and the Level Set method. Our approach parametrizes the evolution of an initial contour with a NODE that implicitly learns from data a speed function describing the evolution. In addition, for cases where an initial contour is not available and to alleviate the need for careful choice or design of contour embedding functions, we propose a NODE-based method that evolves an image embedding into a dense per-pixel semantic label space. We evaluate our methods on kidney segmentation (KiTS19) and on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition to improving initial contours provided by deep learning models while using a fraction of their number of parameters, our approach achieves F scores that are higher than several state-of-the-art deep learning algorithms.

</details>

<details>

<summary>2019-12-25 16:25:29 - Unity in Diversity: Learning Distributed Heterogeneous Sentence Representation for Extractive Summarization</summary>

- *Abhishek Kumar Singh, Manish Gupta, Vasudeva Varma*

- `1912.11688v1` - [abs](http://arxiv.org/abs/1912.11688v1) - [pdf](http://arxiv.org/pdf/1912.11688v1)

> Automated multi-document extractive text summarization is a widely studied research problem in the field of natural language understanding. Such extractive mechanisms compute in some form the worthiness of a sentence to be included into the summary. While the conventional approaches rely on human crafted document-independent features to generate a summary, we develop a data-driven novel summary system called HNet, which exploits the various semantic and compositional aspects latent in a sentence to capture document independent features. The network learns sentence representation in a way that, salient sentences are closer in the vector space than non-salient sentences. This semantic and compositional feature vector is then concatenated with the document-dependent features for sentence ranking. Experiments on the DUC benchmark datasets (DUC-2001, DUC-2002 and DUC-2004) indicate that our model shows significant performance gain of around 1.5-2 points in terms of ROUGE score compared with the state-of-the-art baselines.

</details>

<details>

<summary>2019-12-26 02:52:47 - Multi-Label Graph Convolutional Network Representation Learning</summary>

- *Min Shi, Yufei Tang, Xingquan Zhu, Jianxun Liu*

- `1912.11757v1` - [abs](http://arxiv.org/abs/1912.11757v1) - [pdf](http://arxiv.org/pdf/1912.11757v1)

> Knowledge representation of graph-based systems is fundamental across many disciplines. To date, most existing methods for representation learning primarily focus on networks with simplex labels, yet real-world objects (nodes) are inherently complex in nature and often contain rich semantics or labels, e.g., a user may belong to diverse interest groups of a social network, resulting in multi-label networks for many applications. The multi-label network nodes not only have multiple labels for each node, such labels are often highly correlated making existing methods ineffective or fail to handle such correlation for node representation learning. In this paper, we propose a novel multi-label graph convolutional network (ML-GCN) for learning node representation for multi-label networks. To fully explore label-label correlation and network topology structures, we propose to model a multi-label network as two Siamese GCNs: a node-node-label graph and a label-label-node graph. The two GCNs each handle one aspect of representation learning for nodes and labels, respectively, and they are seamlessly integrated under one objective function. The learned label representations can effectively preserve the inner-label interaction and node label properties, and are then aggregated to enhance the node representation learning under a unified training framework. Experiments and comparisons on multi-label node classification validate the effectiveness of our proposed approach.

</details>

<details>

<summary>2019-12-26 03:13:59 - DAR-Net: Dynamic Aggregation Network for Semantic Scene Segmentation</summary>

- *Zongyue Zhao, Min Liu, Karthik Ramani*

- `1907.12022v2` - [abs](http://arxiv.org/abs/1907.12022v2) - [pdf](http://arxiv.org/pdf/1907.12022v2)

> Traditional grid/neighbor-based static pooling has become a constraint for point cloud geometry analysis. In this paper, we propose DAR-Net, a novel network architecture that focuses on dynamic feature aggregation. The central idea of DAR-Net is generating a self-adaptive pooling skeleton that considers both scene complexity and local geometry features. Providing variable semi-local receptive fields and weights, the skeleton serves as a bridge that connect local convolutional feature extractors and a global recurrent feature integrator. Experimental results on indoor scene datasets show advantages of the proposed approach compared to state-of-the-art architectures that adopt static pooling methods.

</details>

<details>

<summary>2019-12-27 05:25:39 - TBC-Net: A real-time detector for infrared small target detection using semantic constraint</summary>

- *Mingxin Zhao, Li Cheng, Xu Yang, Peng Feng, Liyuan Liu, Nanjian Wu*

- `2001.05852v1` - [abs](http://arxiv.org/abs/2001.05852v1) - [pdf](http://arxiv.org/pdf/2001.05852v1)

> Infrared small target detection is a key technique in infrared search and tracking (IRST) systems. Although deep learning has been widely used in the vision tasks of visible light images recently, it is rarely used in infrared small target detection due to the difficulty in learning small target features. In this paper, we propose a novel lightweight convolutional neural network TBC-Net for infrared small target detection. The TBCNet consists of a target extraction module (TEM) and a semantic constraint module (SCM), which are used to extract small targets from infrared images and to classify the extracted target images during the training, respectively. Meanwhile, we propose a joint loss function and a training method. The SCM imposes a semantic constraint on TEM by combining the high-level classification task and solve the problem of the difficulty to learn features caused by class imbalance problem. During the training, the targets are extracted from the input image and then be classified by SCM. During the inference, only the TEM is used to detect the small targets. We also propose a data synthesis method to generate training data. The experimental results show that compared with the traditional methods, TBC-Net can better reduce the false alarm caused by complicated background, the proposed network structure and joint loss have a significant improvement on small target feature learning. Besides, TBC-Net can achieve real-time detection on the NVIDIA Jetson AGX Xavier development board, which is suitable for applications such as field research with drones equipped with infrared sensors.

</details>

<details>

<summary>2019-12-27 07:46:29 - Visual Agreement Regularized Training for Multi-Modal Machine Translation</summary>

- *Pengcheng Yang, Boxing Chen, Pei Zhang, Xu Sun*

- `1912.12014v1` - [abs](http://arxiv.org/abs/1912.12014v1) - [pdf](http://arxiv.org/pdf/1912.12014v1)

> Multi-modal machine translation aims at translating the source sentence into a different language in the presence of the paired image. Previous work suggests that additional visual information only provides dispensable help to translation, which is needed in several very special cases such as translating ambiguous words. To make better use of visual information, this work presents visual agreement regularized training. The proposed approach jointly trains the source-to-target and target-to-source translation models and encourages them to share the same focus on the visual information when generating semantically equivalent visual words (e.g. "ball" in English and "ballon" in French). Besides, a simple yet effective multi-head co-attention model is also introduced to capture interactions between visual and textual features. The results show that our approaches can outperform competitive baselines by a large margin on the Multi30k dataset. Further analysis demonstrates that the proposed regularized training can effectively improve the agreement of attention on the image, leading to better use of visual information.

</details>

<details>

<summary>2019-12-27 11:25:56 - The Convolutional Tsetlin Machine</summary>

- *Ole-Christoffer Granmo, Sondre Glimsdal, Lei Jiao, Morten Goodwin, Christian W. Omlin, Geir Thore Berge*

- `1905.09688v5` - [abs](http://arxiv.org/abs/1905.09688v5) - [pdf](http://arxiv.org/pdf/1905.09688v5)

> Convolutional neural networks (CNNs) have obtained astounding successes for important pattern recognition tasks, but they suffer from high computational complexity and the lack of interpretability. The recent Tsetlin Machine (TM) attempts to address this lack by using easy-to-interpret conjunctive clauses in propositional logic to solve complex pattern recognition problems. The TM provides competitive accuracy in several benchmarks, while keeping the important property of interpretability. It further facilitates hardware-near implementation since inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on straightforward bit manipulation. In this paper, we exploit the TM paradigm by introducing the Convolutional Tsetlin Machine (CTM), as an interpretable alternative to CNNs. Whereas the TM categorizes an image by employing each clause once to the whole image, the CTM uses each clause as a convolution filter. That is, a clause is evaluated multiple times, once per image patch taking part in the convolution. To make the clauses location-aware, each patch is further augmented with its coordinates within the image. The output of a convolution clause is obtained simply by ORing the outcome of evaluating the clause on each patch. In the learning phase of the TM, clauses that evaluate to 1 are contrasted against the input. For the CTM, we instead contrast against one of the patches, randomly selected among the patches that made the clause evaluate to 1. Accordingly, the standard Type I and Type II feedback of the classic TM can be employed directly, without further modification. The CTM obtains a peak test accuracy of 99.4% on MNIST, 96.31% on Kuzushiji-MNIST, 91.5% on Fashion-MNIST, and 100.0% on the 2D Noisy XOR Problem, which is competitive with results reported for simple 4-layer CNNs, BinaryConnect, Logistic Circuits and an FPGA-accelerated Binary CNN.

</details>

<details>

<summary>2019-12-28 15:33:17 - Rule-Guided Compositional Representation Learning on Knowledge Graphs</summary>

- *Guanglin Niu, Yongfei Zhang, Bo Li, Peng Cui, Si Liu, Jingyang Li, Xiaowei Zhang*

- `1911.08935v2` - [abs](http://arxiv.org/abs/1911.08935v2) - [pdf](http://arxiv.org/pdf/1911.08935v2)

> Representation learning on a knowledge graph (KG) is to embed entities and relations of a KG into low-dimensional continuous vector spaces. Early KG embedding methods only pay attention to structured information encoded in triples, which would cause limited performance due to the structure sparseness of KGs. Some recent attempts consider paths information to expand the structure of KGs but lack explainability in the process of obtaining the path representations. In this paper, we propose a novel Rule and Path-based Joint Embedding (RPJE) scheme, which takes full advantage of the explainability and accuracy of logic rules, the generalization of KG embedding as well as the supplementary semantic structure of paths. Specifically, logic rules of different lengths (the number of relations in rule body) in the form of Horn clauses are first mined from the KG and elaborately encoded for representation learning. Then, the rules of length 2 are applied to compose paths accurately while the rules of length 1 are explicitly employed to create semantic associations among relations and constrain relation embeddings. Besides, the confidence level of each rule is also considered in optimization to guarantee the availability of applying the rule to representation learning. Extensive experimental results illustrate that RPJE outperforms other state-of-the-art baselines on KG completion task, which also demonstrate the superiority of utilizing logic rules as well as paths for improving the accuracy and explainability of representation learning.

</details>

<details>

<summary>2019-12-28 20:11:33 - Tha3aroon at NSURL-2019 Task 8: Semantic Question Similarity in Arabic</summary>

- *Ali Fadel, Ibraheem Tuffaha, Mahmoud Al-Ayyoub*

- `1912.12514v1` - [abs](http://arxiv.org/abs/1912.12514v1) - [pdf](http://arxiv.org/pdf/1912.12514v1)

> In this paper, we describe our team's effort on the semantic text question similarity task of NSURL 2019. Our top performing system utilizes several innovative data augmentation techniques to enlarge the training data. Then, it takes ELMo pre-trained contextual embeddings of the data and feeds them into an ON-LSTM network with self-attention. This results in sequence representation vectors that are used to predict the relation between the question pairs. The model is ranked in the 1st place with 96.499 F1-score (same as the second place F1-score) and the 2nd place with 94.848 F1-score (differs by 1.076 F1-score from the first place) on the public and private leaderboards, respectively.

</details>

<details>

<summary>2019-12-28 21:57:10 - MULE: Multimodal Universal Language Embedding</summary>

- *Donghyun Kim, Kuniaki Saito, Kate Saenko, Stan Sclaroff, Bryan A. Plummer*

- `1909.03493v2` - [abs](http://arxiv.org/abs/1909.03493v2) - [pdf](http://arxiv.org/pdf/1909.03493v2)

> Existing vision-language methods typically support two languages at a time at most. In this paper, we present a modular approach which can easily be incorporated into existing vision-language methods in order to support many languages. We accomplish this by learning a single shared Multimodal Universal Language Embedding (MULE) which has been visually-semantically aligned across all languages. Then we learn to relate MULE to visual data as if it were a single language. Our method is not architecture specific, unlike prior work which typically learned separate branches for each language, enabling our approach to easily be adapted to many vision-language methods and tasks. Since MULE learns a single language branch in the multimodal model, we can also scale to support many languages, and languages with fewer annotations can take advantage of the good representation learned from other (more abundant) language data. We demonstrate the effectiveness of MULE on the bidirectional image-sentence retrieval task, supporting up to four languages in a single model. In addition, we show that Machine Translation can be used for data augmentation in multilingual learning, which, combined with MULE, improves mean recall by up to 21.9% on a single-language compared to prior work, with the most significant gains seen on languages with relatively few annotations. Our code is publicly available.

</details>

<details>

<summary>2019-12-29 14:16:12 - Deep Self-representative Concept Factorization Network for Representation Learning</summary>

- *Yan Zhang, Zhao Zhang, Zheng Zhang, Mingbo Zhao, Li Zhang, Zhengjun Zha, Meng Wang*

- `1912.06444v4` - [abs](http://arxiv.org/abs/1912.06444v4) - [pdf](http://arxiv.org/pdf/1912.06444v4)

> In this paper, we investigate the unsupervised deep representation learning issue and technically propose a novel framework called Deep Self-representative Concept Factorization Network (DSCF-Net), for clustering deep features. To improve the representation and clustering abilities, DSCF-Net explicitly considers discovering hidden deep semantic features, enhancing the robustness proper-ties of the deep factorization to noise and preserving the local man-ifold structures of deep features. Specifically, DSCF-Net seamlessly integrates the robust deep concept factorization, deep self-expressive representation and adaptive locality preserving feature learning into a unified framework. To discover hidden deep repre-sentations, DSCF-Net designs a hierarchical factorization architec-ture using multiple layers of linear transformations, where the hierarchical representation is performed by formulating the prob-lem as optimizing the basis concepts in each layer to improve the representation indirectly. DSCF-Net also improves the robustness by subspace recovery for sparse error correction firstly and then performs the deep factorization in the recovered visual subspace. To obtain locality-preserving representations, we also present an adaptive deep self-representative weighting strategy by using the coefficient matrix as the adaptive reconstruction weights to keep the locality of representations. Extensive comparison results with several other related models show that DSCF-Net delivers state-of-the-art performance on several public databases.

</details>

<details>

<summary>2019-12-29 16:58:39 - Copy and Paste: A Simple But Effective Initialization Method for Black-Box Adversarial Attacks</summary>

- *Thomas Brunner, Frederik Diehl, Alois Knoll*

- `1906.06086v2` - [abs](http://arxiv.org/abs/1906.06086v2) - [pdf](http://arxiv.org/pdf/1906.06086v2)

> Many optimization methods for generating black-box adversarial examples have been proposed, but the aspect of initializing said optimizers has not been considered in much detail. We show that the choice of starting points is indeed crucial, and that the performance of state-of-the-art attacks depends on it. First, we discuss desirable properties of starting points for attacking image classifiers, and how they can be chosen to increase query efficiency. Notably, we find that simply copying small patches from other images is a valid strategy. We then present an evaluation on ImageNet that clearly demonstrates the effectiveness of this method: Our initialization scheme reduces the number of queries required for a state-of-the-art Boundary Attack by 81%, significantly outperforming previous results reported for targeted black-box adversarial examples.

</details>

<details>

<summary>2019-12-29 20:38:32 - SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory</summary>

- *Steven W. Chen, Guilherme V. Nardari, Elijah S. Lee, Chao Qu, Xu Liu, Roseli A. F. Romero, Vijay Kumar*

- `1912.12726v1` - [abs](http://arxiv.org/abs/1912.12726v1) - [pdf](http://arxiv.org/pdf/1912.12726v1)

> This paper describes an end-to-end pipeline for tree diameter estimation based on semantic segmentation and lidar odometry and mapping. Accurate mapping of this type of environment is challenging since the ground and the trees are surrounded by leaves, thorns and vines, and the sensor typically experiences extreme motion. We propose a semantic feature based pose optimization that simultaneously refines the tree models while estimating the robot pose. The pipeline utilizes a custom virtual reality tool for labeling 3D scans that is used to train a semantic segmentation network. The masked point cloud is used to compute a trellis graph that identifies individual instances and extracts relevant features that are used by the SLAM module. We show that traditional lidar and image based methods fail in the forest environment on both Unmanned Aerial Vehicle (UAV) and hand-carry systems, while our method is more robust, scalable, and automatically generates tree diameter estimations.

</details>

<details>

<summary>2019-12-29 23:44:06 - The continuous Bernoulli: fixing a pervasive error in variational autoencoders</summary>

- *Gabriel Loaiza-Ganem, John P. Cunningham*

- `1907.06845v5` - [abs](http://arxiv.org/abs/1907.06845v5) - [pdf](http://arxiv.org/pdf/1907.06845v5)

> Variational autoencoders (VAE) have quickly become a central tool in machine learning, applicable to a broad range of data types and latent variable models. By far the most common first step, taken by seminal papers and by core software libraries alike, is to model MNIST data using a deep network parameterizing a Bernoulli likelihood. This practice contains what appears to be and what is often set aside as a minor inconvenience: the pixel data is [0,1] valued, not {0,1} as supported by the Bernoulli likelihood. Here we show that, far from being a triviality or nuisance that is convenient to ignore, this error has profound importance to VAE, both qualitative and quantitative. We introduce and fully characterize a new [0,1]-supported, single parameter distribution: the continuous Bernoulli, which patches this pervasive bug in VAE. This distribution is not nitpicking; it produces meaningful performance improvements across a range of metrics and datasets, including sharper image samples, and suggests a broader class of performant VAE.

</details>

<details>

<summary>2019-12-30 02:45:42 - Low-Resource Response Generation with Template Prior</summary>

- *Ze Yang, Wei Wu, Jian Yang, Can Xu, Zhoujun Li*

- `1909.11968v3` - [abs](http://arxiv.org/abs/1909.11968v3) - [pdf](http://arxiv.org/pdf/1909.11968v3)

> We study open domain response generation with limited message-response pairs. The problem exists in real-world applications but is less explored by the existing work. Since the paired data now is no longer enough to train a neural generation model, we consider leveraging the large scale of unpaired data that are much easier to obtain, and propose response generation with both paired and unpaired data. The generation model is defined by an encoder-decoder architecture with templates as prior, where the templates are estimated from the unpaired data as a neural hidden semi-markov model. By this means, response generation learned from the small paired data can be aided by the semantic and syntactic knowledge in the large unpaired data. To balance the effect of the prior and the input message to response generation, we propose learning the whole generation model with an adversarial approach. Empirical studies on question response generation and sentiment response generation indicate that when only a few pairs are available, our model can significantly outperform several state-of-the-art response generation models in terms of both automatic and human evaluation.

</details>

<details>

<summary>2019-12-30 11:23:13 - Sem-LSD: A Learning-based Semantic Line Segment Detector</summary>

- *Yi Sun, Xushen Han, Kai Sun, Boren Li, Yongjiang Chen, Mingyang Li*

- `1909.06591v2` - [abs](http://arxiv.org/abs/1909.06591v2) - [pdf](http://arxiv.org/pdf/1909.06591v2)

> In this paper, we introduces a new type of line-shaped image representation, named semantic line segment (Sem-LS) and focus on solving its detection problem. Sem-LS contains high-level semantics and is a compact scene representation where only visually salient line segments with stable semantics are preserved. Combined with high-level semantics, Sem-LS is more robust under cluttered environment compared with existing line-shaped representations. The compactness of Sem-LS facilitates its use in large-scale applications, such as city-scale SLAM (simultaneously localization and mapping) and LCD (loop closure detection). Sem-LS detection is a challenging task due to its significantly different appearance from existing learning-based image representations such as wireframes and objects. For further investigation, we first label Sem-LS on two well-known datasets, KITTI and KAIST URBAN, as new benchmarks. Then, we propose a learning-based Sem-LS detector (Sem-LSD) and devise new module as well as metrics to address unique challenges in Sem-LS detection. Experimental results have shown both the efficacy and efficiency of Sem-LSD. Finally, the effectiveness of the proposed Sem-LS is supported by two experiments on detector repeatability and a city-scale LCD problem. Labeled datasets and code will be released shortly.

</details>

<details>

<summary>2019-12-30 17:21:22 - Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?</summary>

- *Ofir Nachum, Haoran Tang, Xingyu Lu, Shixiang Gu, Honglak Lee, Sergey Levine*

- `1909.10618v2` - [abs](http://arxiv.org/abs/1909.10618v2) - [pdf](http://arxiv.org/pdf/1909.10618v2)

> Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard "shallow" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.

</details>

<details>

<summary>2019-12-31 05:59:02 - Definitions and Semantic Simulations Based on Object-Oriented Analysis and Modeling</summary>

- *Robert B. Allen*

- `1912.13186v1` - [abs](http://arxiv.org/abs/1912.13186v1) - [pdf](http://arxiv.org/pdf/1912.13186v1)

> We have proposed going beyond traditional ontologies to use rich semantics implemented in programming languages for modeling. In this paper, we discuss the application of executable semantic models to two examples, first a structured definition of a waterfall and second the cardiopulmonary system. We examine the components of these models and the way those components interact. Ultimately, such models should provide the basis for direct representation.

</details>

<details>

<summary>2019-12-31 06:38:57 - CASE: Context-Aware Semantic Expansion</summary>

- *Jialong Han, Aixin Sun, Haisong Zhang, Chenliang Li, Shuming Shi*

- `1912.13194v1` - [abs](http://arxiv.org/abs/1912.13194v1) - [pdf](http://arxiv.org/pdf/1912.13194v1)

> In this paper, we define and study a new task called Context-Aware Semantic Expansion (CASE). Given a seed term in a sentential context, we aim to suggest other terms that well fit the context as the seed. CASE has many interesting applications such as query suggestion, computer-assisted writing, and word sense disambiguation, to name a few. Previous explorations, if any, only involve some similar tasks, and all require human annotations for evaluation. In this study, we demonstrate that annotations for this task can be harvested at scale from existing corpora, in a fully automatic manner. On a dataset of 1.8 million sentences thus derived, we propose a network architecture that encodes the context and seed term separately before suggesting alternative terms. The context encoder in this architecture can be easily extended by incorporating seed-aware attention. Our experiments demonstrate that competitive results are achieved with appropriate choices of context encoder and attention scoring function.

</details>

